1
 Rebuttal Testimony of 
Professor Douglas W. Oard
 U.S. et al. v. Google LLC
 United States District Court for the District of Columbia
 November 15, 2023
 REDACTED FOR PUBLIC FILING
My Assignment
 To provide my expert opinion of the analysis and 
opinions offered by Google’s expert, Prof. Edward A. Fox, 
in his June 3, 2022 expert report (the “Fox Report”).
 REDACTED FOR PUBLIC FILING
 2
 Expert Rebuttal Report of Douglas Oard, Aug. 19, 2022, ¶ 4.
My Overall Conclusion
 Prof. Fox substantially understates the 
beneficial effects of user-side data on 
search quality.
 3
 REDACTED FOR PUBLIC FILING
Prof. 
Fox’s Assignment
 Prof. Fox states he was asked by Google counsel to:
 “test the extent to which Google’s search quality 
is affected by the volume of user interaction data 
available to train its ranking algorithms”
 REDACTED FOR PUBLIC FILING
 4
 Expert Report of Edward A. Fox, Jun. 3, 2022, at 5.
Prof. 
Fox’s Conclusions
 REDACTED FOR PUBLIC FILING
 5
 DXD-26.002.
Prof. Fox’s Conclusions
 REDACTED FOR PUBLIC FILING
 6
 DXD-26.002 (modified).
Differences 
in IS4@5 
The Basis for Prof. Fox’s Central Conclusion
 DXD-26.009 (modified).
 70
 60
 50
 40
 30
 20
 10
 0
 7
 Bing v. Google
 3.924 ± 0.328
 Frozen v. Retrained Google
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
 0.113 ± 0.117 
70
 60
 50
 40
 30
 20
 10
 0
 REDACTED FOR PUBLIC FILING
My Response to Prof. Fox’s Central Conclusions
 Prof. Fox’s conclusions are unsupported because of:
  Unmeasured benefits
 experiment;
  Measurement errors
 of user-side data in this 
in the “quality gap”; and
  Important benefits
 cannot measure
 of user-side data that this experiment 
8
 REDACTED FOR PUBLIC FILING
Many Components Not Retrained
 Google only retrained 6 components, 
chosen based on their expected effect 
on web ranking (i.e., 10 blue links)
 Components were not chosen based 
on their effect on: 
• Indexing
 • Spelling Correction
 Measurable effect of 
user-side data
 Measured effect of 
user-side data
 • Search features like images, 
video…
 • Search advertising
 • Whole-page ranking
 9
 REDACTED FOR PUBLIC FILING
Glue’s Importance to Whole-Page Ranking
 10 UPX0262, at -989 (Mar. 7, 2016) (emphasis added).
 “User interaction data from Glue is already 
being used in Web, KE, and WebAnswers. 
More recently, it is one of the critical 
signals in Tetris.”
 2016 Redacted
 REDACTED FOR PUBLIC FILING
Prof. Edward 
Fox
 Google’s Expert 
Witness
 “In simpler terms, Glue aggregates diverse types 
of user interactions—such as clicks, hovers, 
scrolls, and swipes—and creates a common 
metric to compare web results and search 
features. This process determines both whether 
a search feature is triggered and where it 
triggers on the page.” 
Expert Reply Report of Edward A. Fox, Oct. 10, 2022, ¶ 46 (emphasis added).
 Glue Is Used to “Trigger” and Position Search Features 
11
 REDACTED FOR PUBLIC FILING
Prof.
 Fox Has Never Stated that Glue Was Retrained
 Component Name
 Navboost
 RankBrain
 DeepRank
 QBST
 Term Weighting
 RankEmbedBERT
 REDACTED FOR PUBLIC FILING
 12
 DXD-26.004 (emphasis added).
The IS4@5 Metric Evaluates Web and Search Features Results
 Prof. Edward 
Fox
 Google’s Expert 
Witness
 “Google rates the top five 
positions for IS4@5 counting both 
search features like OneBoxes and 
‘blue links.’”
 REDACTED FOR PUBLIC FILING
 Expert Report of Edward A. Fox, Jun. 3, 2022, App. A, n. 12 (emphasis added).
 13
“10 Blue Links” Ranking’s Effect on IS4@5 Can Be Small 
Expert Rebuttal Report of Douglas Oard, Aug. 19, 2022, Table 2, at 23; UPX2134, at -077 (Apr. 2019).
 IS4@5 Weighting
 0.50
 0.33
 0.25
 1.00
 0.20
 Frozen Google Retrained Google
 14
 REDACTED FOR PUBLIC FILING
Search Features “Trigger” for Many Results
 15 UPX1114, at -168 (May 3, 2017*) (emphasis added).
 2017
 “Small fraction of 
SERP is web 
results for many 
queries”
 Redacted
 Redacted
 REDACTED FOR PUBLIC FILING
My Response to Prof. Fox’s Central Conclusions
 Prof. Fox’s conclusions are unsupported because of:
  Unmeasured benefits
 experiment;
  Measurement errors
 of user-side data in this 
in the “quality gap”; and
  Important benefits
 cannot measure
 of user-side data that this experiment 
16
 REDACTED FOR PUBLIC FILING
Correcting for Measurement Errors
 17
 1. Effect of Google “teaching to the test”
 2. Google’s choice to “rate” all queries using based on mobile presentation
 3. Google’s difficulties accurately rating Bing’s results
 Actual IS4@5 “gap” is 
smaller than was measured
 Google IS4@5
 Bing IS4@5
 REDACTED FOR PUBLIC FILING
Teaching to the Test: Google Trains Using IS, Bing Does Not
 18 DXD-26.004 (modified).
 Component Name Data Used
 Navboost
 …Engineered functions have parameters that are 
learned by trying to maximize the IS ratings of the 
queryset result rankings they produce…
 RankBrain …Then fine-tuned on IS rating data…
 DeepRank …Then it is fine-tuned on rating IS data…
 QBST …Then the ranking integration is trained on rating 
data…
 Term Weighting …Then the ranking integration is trained on rating 
data.
 RankEmbedBERT Trained on documents, queries, click logs, and rating 
data…
 Redacted Redacted
 REDACTED FOR PUBLIC FILING
Mobile Evaluation Understates Bing’s Search Quality
 19 UPX0268, at -133 (Aug. 2020).
 “On Desktop, Google is comparable to Bing”
 2020
 Redacted
 Redacted
 REDACTED FOR PUBLIC FILING
20 Testimony of Edward Fox, Oct. 31, 2023, 7977:5–8 (emphasis added).
 Prof. Edward 
Fox
 Google’s Expert 
Witness
 Q. You don’t know what the IS gap would be if 
human raters were looking at desktop 
presentation; right?
 A. Google made a decision some years ago to do 
all the rater experiments with mobile. So that’s 
all I know.
 The Measured “Quality Gap” Does Not Account for This 
REDACTED FOR PUBLIC FILING
“Scraped” Results Can Understate Bing’s Search Quality
 21 UPX0220, at -882–883 (Jul. 15, 2020) (emphasis added).
 “Why are the two methods so different?”
 “In the full-page scrape, Bing has no user 
location so Google is much better.
 In live links SxS, both search engines utilize 
user location, but Bing shows rich features 
(map, listings) while Google only shows blue 
links.”
 2020
 REDACTED FOR PUBLIC FILING
Measureddifference
 between Bing and Google
 Correcting for
 measurement errors
 Correcting for
 Measurement Errors
 Accounting for 
Unmeasured Benefits
 Effect of retraining 
all components with 
less user-side data
 22
 Effect of retraining
 sixcomponents with 
less user-side data
 DXD-26.009 (modified).
 REDACTED FOR PUBLIC FILING
My Response to Prof. Fox’s Central Conclusions
 Prof. Fox’s conclusions are unsupported because of:
  Unmeasured benefits
 experiment;
  Measurement errors
 of user-side data in this 
in the “quality gap”; and
  Important benefits
 cannot measure
 of user-side data that this experiment 
23
 REDACTED FOR PUBLIC FILING
The Experiment Cannot Measure All Effects of User
Side Data
 1
 2
 3
 Effects on the Innovation Cycle
 Effects that the IS4@5 Metric Can’t Measure
 Effects that a Frozen System Can’t Measure
 REDACTED FOR PUBLIC FILING
 24
The Experiment Cannot Measure All Effects of User
Side Data
 1
 2
 3
 Effects on the Innovation Cycle
 Effects that the IS4@5 Metric Can’t Measure
 Effects that a Frozen System Can’t Measure
 25
 REDACTED FOR PUBLIC FILING
User-Side Data Benefits the Innovation Cycle
 Ideation and 
Implementation
 Evaluation and 
Decision
 People 
leverage
 user-side data
 26
 REDACTED FOR PUBLIC FILING
 26
User-Side Data Benefits the Innovation Cycle
 Ideation and 
Implementation
 Evaluation and 
Decision
 Engineers 
leverage
 user-side data
 27
 REDACTED FOR PUBLIC FILING
 27
Testimony of John Giannandrea (Apple), Sept. 22, 2023, 2257:11–2257:15 (emphasis added).
 John 
Giannandrea
 Apple SVP of Machine 
Learning and 
AI Strategy; Former 
Google Head of 
Search and AI
 Q. …So the more queries a search engine sees, 
the more opportunities it has to improve in this 
manner?
 A. The more opportunities the engineers have to 
look for patterns and improve the algorithm, 
yeah.
 28
 REDACTED FOR PUBLIC FILING
30(b)(6) Deposition of Pandu Nayak (Google), Apr. 7, 2022, 153:4–153:24 (emphasis added).
 Pandu Nayak
 VP, Search
 Q. …[O]ne thing that Google might do is look at queries for 
inspiration on what it might need to improve on. Does that 
sound familiar?
 A. Yes.
 Q. And what does that mean?
 A. So we create samples of queries that -- on which we evaluate 
how well we are doing overall using the IS metric, and we look 
at -- often we look at queries that have low IS to try and 
understand what is going on, what are we missing here…So 
that’s a way of figuring out how we can improve our algorithms.
 29
 REDACTED FOR PUBLIC FILING
User-Side Data Benefits the Innovation Cycle
 Ideation and 
Implementation
 Evaluation and 
Decision
 Engineers & 
Managers 
leverage
 user-side data
 30
 REDACTED FOR PUBLIC FILING
 30
User-Side Data Is Key to Launch Decisions
 UPX0876, at -158 (Dec. 10, 2018*).
 2018
 31
 REDACTED FOR PUBLIC FILING
On the Role of User Interaction Data in Innovation
 Testimony of Edward Fox, Oct. 30, 2023, 7850:3–7850:8 (emphasis added).
 Prof. Edward 
Fox
 Google’s Expert 
Witness
 Q.  Whether it’s innovation, better algorithms or the like 
you didn’t study, but that’s what accounts for the other 
97 percent, in your view?
 A. So, I don’t know what the other parts are. I have 
guesses because I’ve worked in the field for a long 
time, but it’s not from user interaction data. That's 
what I can tell.
 32
 REDACTED FOR PUBLIC FILING
The Experiment Cannot Measure All Effects of User
Side Data
 1
 2
 3
 Effects on the Innovation Cycle
 Effects that the IS4@5 Metric Can’t Measure
 Effects that a Frozen System Can’t Measure
 33
 REDACTED FOR PUBLIC FILING
Metrics Are 
Not
 Search Quality
 “IS4 is an approximation of user utility-- treat it 
as such.”
 “[A]lways look for real user value supported by 
thorough analysis and other metrics.”
 2021
 REDACTED FOR PUBLIC FILING
 34
 UPX0217, at -792 (Jan. 28, 2021) (emphasis added).
The IS4@5 Metric
 Is Only a Part of the Story
 2018
 Redacted
 Redacted
 Redacted
 2021
 Redacted
 “Raters may not understand technical queries”
 “Raters can not accurately the judge popularity of 
anything”
 “[I]n IS ratings, human raters don’t always pay enough 
attention to the freshness aspect of relevance or lack 
the time context for the query, thus undervaluing fresh 
results for fresh-seeking queries”
 REDACTED FOR PUBLIC FILING
 35
 UPX0204, at -223, -225 (Nov. 16, 2018*); UPX2133, at -420 (Aug. 11, 2021*) (emphasis added).
Google Uses Many Metrics to Evaluate Search Quality
 2016
 REDACTED FOR PUBLIC FILING
 36
 DX0080, at -738 (Jun. 17, 2016).
Live Experiment Metrics Provide Crucial Insights
 Redacted
 Redacted
 2016
 REDACTED FOR PUBLIC FILING
 37
 DX0080, at -743 (Jun. 17, 2016) (modified).
The Experiment Cannot Measure All Effects of User
Side Data
 1
 2
 3
 Effects on the Innovation Cycle
 Effects that the IS4@5 Metric Can’t Measure
 Effects that a Frozen System Can’t Measure
 REDACTED FOR PUBLIC FILING
 38
Frozen Systems Are Different from Live Systems
 Expert Report of Edward A. Fox, June 3, 2022, App. A, at 34.
 IS4@5  Difference
 39
 RedactedRedacted
 REDACTED FOR PUBLIC FILING
Frozen Systems Lack Fresh User-Side Data
 UPX2133, at -419 (Aug. 11, 2021*) (emphasis added).
 “One important aspect of freshness is ensuring 
that our ranking signals reflect the current state 
of the world. 
Instant Glue is a realtime pipeline aggregating 
the same fractions of user-interaction signals as 
Glue, but only from the last 24 hours of logs, 
with a latency of ~10 minutes.”
 2021
 40
 REDACTED FOR PUBLIC FILING
Freshness Benefits from User-Side Data
 DX0116, at -409 (May 24, 2018*) (emphasis added).
 2018
 41
 On the day of the attack, [nice pictures] 
has different interpretation. Instant 
Glue will suppress Image Universal.
 News and pictures from Nice, 
France is the main intent of the 
query.
 Redacted Redacted
 REDACTED FOR PUBLIC FILING
Expert Report of Edward A. Fox, June 3, 2022, App. A, at 10 (emphasis added).
 Prof. Edward 
Fox
 Google’s Expert 
Witness
 ThisExperiment Can’t Test Effects of User-Side Data on Freshness
 42
 Redacted
 REDACTED FOR PUBLIC FILING
Measured difference
 between Bing and Google
 Correcting for
 measurement errors
 Correcting for
 Measurement Errors
 Accounting for 
Unmeasured Benefits
 Effect of retraining 
all components with 
less user-side data
 Beneficial Effects 
of User-Side Data
 this Experiment 
Cannot Measure
 43
 Effect of retraining
 sixcomponents with 
less user-side data
 DXD-26.009 (modified).
 REDACTED FOR PUBLIC FILING
Prof. Fox’s Third Conclusion
 REDACTED FOR PUBLIC FILING
 44
 DXD-26.002 (modified).
The Results Show a Substantial Effect on Long-Tail Queries
 45 Expert Rebuttal Report of Douglas Oard, Aug. 19, 2022, Table 8 (results for “training” query set).
 IS4@5 Difference
 Long-Tail 
Queries
 Head & Torso 
Queries
 Beneficial effects of 
user-side data can be 
different for different 
queries
 Redacted
 REDACTED FOR PUBLIC FILING
Pandu Nayak
 VP, Search
 A. So we came up with the following way of thinking 
about it: Wikipedia is a really important source on the 
web, lots of great information. People like it a lot. If 
we took Wikipedia out of our index, completely out of 
our index, then that would lead to an IS loss of roughly 
about a half point. So that gives you a sense for what 
a point of IS is. A half point is a pretty significant 
difference if it represents the whole Wikipedia wealth 
of information there…
 Testimony of Pandu Nayak (Google), Oct. 18, 2023, 6323:8–18 (emphasis added). 46
 REDACTED FOR PUBLIC FILING
Prof. Fox’s Final Conclusion 
REDACTED FOR PUBLIC FILING
 47
 DXD-26.002 (modified).
Diminishing Returns Are Not Vanishing Returns
 Point where 
benefits begin 
to decrease
 Amount of Training Data 
 Benefits continue to accrue
  Benefits would be greater 
for tail queries, fine-grained 
location, etc.
 REDACTED FOR PUBLIC FILING
 48
 DXD-26.003 (modified).
49 Testimony of Pandu Nayak (Google), Oct. 18, 2023, 6337:6–6338:6.
 Pandu Nayak
 VP, Search
 Q. Google has a large collection of sessions logs.  Does 
each click, each piece of data have the same value to 
Google?
 A. …And so there is this trade-off in terms of amount of 
data that you use, the diminishing returns of the data, 
and the cost of processing the data. And so usually 
there’s a sweet spot along the way where the value has 
started diminishing, the costs have gone up, and that’s 
where you would stop.
 Google’s Choices Confirm Benefits Continue to Accrue
 REDACTED FOR PUBLIC FILING
Google Clearly Gets “Returns” 
from
 User
>13 Years
 Side Data
 of Bing’s User-Side Data
 13 Months of Google’s User-Side Data
 REDACTED FOR PUBLIC FILING
 50
 StatCounter GlobalStats (2020 Data).