CN115066689A - Fine-grained stochastic neural architecture search - Google Patents
Fine-grained stochastic neural architecture search Download PDFInfo
- Publication number
- CN115066689A CN115066689A CN202180012814.5A CN202180012814A CN115066689A CN 115066689 A CN115066689 A CN 115066689A CN 202180012814 A CN202180012814 A CN 202180012814A CN 115066689 A CN115066689 A CN 115066689A
- Authority
- CN
- China
- Prior art keywords
- neural network
- network
- architecture
- training
- operators
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/217—Validation; Performance evaluation; Active pattern learning techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for determining a neural network architecture. One of the methods includes receiving training data; receiving architecture data; assigning to each network operator of a plurality of network operators an utilization variable indicating a likelihood that the network operator is utilized in the neural network; generating an optimized neural network for performing a neural network task, comprising repeatedly performing the following operations: sampling the selected set of network operators; and training a neural network having an architecture defined by the selected set of network operators, wherein the training comprises: computing an objective function that evaluates (i) a measure of computational cost of the neural network and (ii) a measure of performance of the neural network on neural network tasks associated with the training data; and adjusting the respective current values of the utilization variables and the respective current values of the neural network parameters.
Description
Cross Reference to Related Applications
This application claims priority to U.S. provisional application No.62/971,866, filed on 7/2/2020. The disclosure of this prior application is considered to be part of the disclosure of the present application and is incorporated by reference into the disclosure of the present application.
Technical Field
The present specification relates to determining an architecture for a neural network.
Background
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict the output of received inputs. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as an input to the next layer in the network, i.e. the next hidden layer or output layer. Each layer of the network generates an output from the received input in accordance with the current values of the corresponding set of parameters.
Disclosure of Invention
The present specification describes a neural network architecture optimization system implemented as a computer program on one or more computers at one or more locations that determines an optimal network architecture for a neural network configured to perform a particular machine learning task. Depending on the task, the neural network can be configured to receive any type of digital data input and generate any type of score, classification, or regression output based on the input.
In general, one innovative aspect of the subject matter described in this specification can be embodied in a method that includes: receiving training data for training a neural network to perform a neural network task, the training data comprising a plurality of training examples and a respective target output for each training example; receiving architecture data defining a plurality of network operators; assigning to each network operator of a plurality of network operators an utilization variable indicating a likelihood that the network operator is utilized in the neural network; generating an optimized neural network for performing a neural network task, comprising repeatedly performing the following operations: sampling a selected set of network operators from a plurality of network operators and according to respective current values with the variables; and training a neural network having an architecture defined by the selected set of network operators on training data to perform the neural network task, wherein training comprises: computing an objective function that evaluates (i) a measure of computational cost of the neural network and (ii) a measure of performance of the neural network on neural network tasks associated with the training data; and adjusting the respective current values of the utilization variables and the respective current values of the neural network parameters based on the determined gradient of the objective function.
The architecture data may be initialized from one or more predetermined neural network architectures. The method may further comprise: redundant network operators are removed from the plurality of network operators. The plurality of operators may include a neural network layer. The neural network layer may include at least one of a convolutional layer, a fully-connected layer, a normalization layer, or an activation layer. The plurality of operators may also include filters in the convolutional layer, or neurons in the fully-connected layer. The metric of computational cost of the neural network may include at least one of size, number of floating point operations per second (FLOPS), or delay. Generating an optimized neural network for performing neural network tasks may further comprise: a zero masking layer is inserted after each operator that is not one of the selected set of network operators. Generating an optimized neural network for performing neural network tasks may further include: a concat aggregator is used to combine the respective outputs of the selected set of network operations. Each utilization variable may be defined by one or more distribution parameters. The method may further include calculating the determined gradient of the objective function relative to one or more distribution parameters. Adjusting the respective values of the utilization variables may include: the determined gradient of the objective function is back propagated into one or more distribution parameters by using the variable. The one or more distribution parameters may define a binary specific distribution.
Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. A system of one or more computers can be configured to perform particular operations or actions by software, firmware, hardware, or any combination thereof installed on the system that in operation can cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by including instructions that, when executed by a data processing apparatus, cause the apparatus to perform the actions.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
The techniques allow a neural network architecture optimization system to efficiently and automatically determine a neural network architecture from a search space, which will result in a small-sized (i.e., parameter efficient) but high-performance neural network for a particular task. In particular, during the search process, the system utilizes trainable random masks to encourage sparsity in candidate neural network architectures, thereby reducing runtime delay, memory footprint, or both of neural networks having the resulting architecture. Dedicated hardware can be optimized to perform sparse operations efficiently and with minimal runtime delay, e.g., by not computing multiplications involving 0 s. The sparse matrix can be efficiently stored in memory, i.e., by not explicitly storing a value as 0, the memory footprint of the resulting architecture is reduced.
The techniques also allow the system to collectively determine training parameter values for a neural network having a selected neural network architecture by training the neural network on a training data set associated with a particular task. More importantly, the specific task can be any neural network task, and the search space can be initialized from any existing neural network architecture. Thus, the system is able to automatically generate the resulting trained neural networks that can compete with or exceed state-of-the-art models for the performance of a wide range of tasks, while having a relatively small model size and thus being suitable for deployment on hardware platforms with limited computing resources, including, for example, mobile devices and embedded systems.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example neural architecture search system.
FIG. 2 is a flow diagram of an example process for searching an architecture of a neural network.
FIG. 3 is a flow diagram of an example process for training a neural network having an architecture defined by a selected set of network operators.
Fig. 4A-4B show example illustrations of search spaces.
FIG. 5 illustrates an example illustration of inserting a mask corresponding to a utilization variable into a network operator.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification describes a system, implemented as a computer program on one or more computers at one or more locations, that determines an architecture of a task neural network configured to perform a particular neural network task.
The neural network can be trained to perform any type of machine learning task, i.e., can be configured to receive any type of digital data input and generate any type of score, classification, or regression output based on the input.
In some cases, the neural network is a neural network configured to perform image processing tasks (i.e., receiving input images and processing the input images to generate network outputs of the input images). For example, the task may be image classification, and the output generated by the neural network for a given image may be a score for each class in a set of object classes, each score representing an estimated likelihood that the image contains an image of an object belonging to that class. As another example, the task can be image embedding generation and the output generated by the neural network can be digital embedding of the input image. As yet another example, the task can be object detection, and the output generated by the neural network can identify a location in the input image depicting a particular type of object. As yet another example, the task can be image segmentation, and the output generated by the neural network can assign each pixel of the input image to a class from a set of classes.
As another example, if the input to the neural network is an internet resource (e.g., a web page), a document, or a portion of a document or a feature extracted from an internet resource, document, or portion of a document, the task can be to classify the resource or document, i.e., the output generated by the neural network for a given internet resource, document, or portion of a document can be a score for each topic in a set of topics, each score representing an estimated likelihood that the internet resource, document, or portion of a document is about that topic.
As another example, if the input to the neural network is characteristic of the impression context of a particular advertisement, the output generated by the neural network may be a score representing an estimated likelihood that the particular advertisement will be clicked.
As another example, if the input to the neural network is a feature of a personalized recommendation for the user, such as a feature characterizing the context of the recommendation, such as a feature characterizing an action previously taken by the user, the output generated by the neural network may be a score for each content item in a set of content items, each score representing an estimated likelihood that the user will respond positively to the recommended content item.
As another example, if the input to the neural network is a text sequence in one language, the output generated by the neural network may be a score for each text segment in a set of text segments in another language, each score representing an estimated likelihood that a text segment in the other language is a correct translation of the input text to the other language.
As another example, the task may be an audio processing task. For example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a score for each text segment in a set of text segments, each score representing an estimated likelihood that the text segment is a correct transcription of the utterance. As another example, the task may be a keyword recognition task, wherein if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network can indicate whether a particular word or phrase ("hotword") was spoken in the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network is able to recognize the natural language in which the utterance was spoken.
As another example, the task can be a natural language processing or understanding task, such as an implication task, a paraphrase task, a text similarity task, an emotion task, a sentence completion task, and a grammar task, etc., that acts on text sequences in some natural languages.
As another example, the task can be a text-to-speech task, where the input is text in natural language or a feature of text in natural language, and the network output is a spectrogram or other data defining audio of the text being spoken in natural language.
As another example, the task can be a health prediction task, where the input is electronic health record data for a patient and the output is a prediction related to the patient's future health, e.g., a predicted treatment that should be prescribed for the patient, a likelihood of an adverse health event occurring for the patient, or a predicted diagnosis for the patient.
As another example, the task can be an agent control task, where the input is an observation characterizing a state of the environment and the output defines an action to be performed by the agent in response to the observation. The agent can be, for example, a real-world or simulated robot, a control system of an industrial installation, or a control system controlling different types of agents.
Fig. 1 illustrates an example neural architecture search system 100. The neural architecture search system 100 is an example of a system implemented as a computer program on one or more computers at one or more locations in which the systems, components, and techniques described below can be implemented.
The neural architecture search system 100 is a system that obtains training data 102 for training a neural network to perform a machine learning task and architecture data 104 defining a plurality of network operators, and uses the training data 102 and the architecture data 104 to determine an optimal neural network architecture for performing the machine learning task, and trains the neural network having the optimal neural network architecture to determine training values for parameters of the neural network.
The training data 102 can include a plurality of training examples and a respective target output for each training example. The target output for the given training example is the output that should be generated by the trained neural network by processing the given training example. In some embodiments, the system 100 partitions (e.g., randomly partitions) the received training data 102 into a training subset, a validation subset, and an optional test subset.
A CIFAR-10 data set consisting of 60000 training examples paired with a target output class selected from ten possible classes is one example of such training data. CIFAR-1000 is a relational data set in which the classification is one of 1000 possible categories. Another example of suitable training data is an ImageNet dataset consisting of more than 1400 million images paired with a target output classification selected from more than 20000 possible classes. Some or all of these images are also paired with bounding box data that specifies the boundaries of object presence areas that belong to one of the possible categories.
The architecture of a neural network typically defines the number of layers in the neural network, the operations performed by each layer, and the connectivity between layers in the neural network, i.e., which layers in the neural network receive inputs from which other layers.
In some implementations, the architecture data 104 includes data that specifies a set of candidate neural network architecture components. Each candidate architectural component can be in the form of a neural network unit or a neural network block. The architecture of the neural network generated by the system 100 from the architecture data 104, such as the training architecture 122 generated during the search process or the final architecture 150 generated at the end of the search process, can be in the form of a tower. A tower is a neural network that includes a sequence of neural network elements, a sequence of neural network blocks, or both, where each element (or block) after a first element (or block) in the sequence receives an input from one or more elements (or blocks) earlier in the sequence, receives a network input, or both. For example, each cell can be composed of multiple blocks, each block receiving input from one or more previous cells and one or more previous blocks within the same cell.
In some such implementations, the architecture generated by the system 100 from the architecture data 104 can have a plurality of neural network units, wherein each unit can have a plurality of neural network blocks, and wherein each block can be a directed graph that is used to arrange a plurality of neural network layers. Each neural network layer in the block can be configured to receive an input tensor from a previous layer and to generate an output tensor for the input tensor, the output tensor being fed as an input to a next neural network layer. The plurality of neural network layers included in each block can be of different types, i.e., different types of operations can be performed on different sizes of input tensors to generate different sizes of output tensors.
While different architectures can include different numbers of cells (or blocks), the sequence of cells (or blocks) in any given candidate includes at least one cell (or block) and at most a fixed maximum number of cells (or blocks). In addition to the sequence of one or more neural network elements or blocks, each tower can optionally include one or more predetermined components, e.g., one or more input layers before the first block in the sequence, one or more intermediate pooling layers, one or more output layers after the last block in the sequence, or a combination thereof.
In some implementations, the architecture data 104 can be initialized or otherwise derived from one or more baseline network architectures or search spaces. Examples of such baseline network architectures or search spaces are described in more detail below: gabriel Bender et al, understanding and simplifying one-shot architecture search; international conference on machine learning, 549-; IEEE computer vision and pattern recognition conference talk sets, 10734 and 10742 pages, 2019, and Ariel Gordon et al Morphnet Fast & simple resource-constrained structure learning of deep networks; IEEE computer vision and pattern recognition conference corpus, page 1586-1595, 2018.
In any of the above embodiments, the architecture data 104 includes data defining a plurality of network operators, each network operator configured to receive an operator input and generate a corresponding operator output based on processing the operator input according to current values of parameters associated with the network operator. For example, the operator can be a neural network layer. For example, each operator can be a convolutional layer, a fully-connected layer, a normalization layer, a pooling layer, or an activation layer that applies a series of operations (e.g., transformations) to a layer input to generate a plurality of layer outputs. As another example, the operator can be a component of a neural network layer. For example, each operator can be a filter in the convolutional layer, or a neuron in the fully-connected layer. As yet another example, an operator can be a combination of two or more neural network layers, or a combination of two or more neural network layer components, as described above.
FIG. 4A shows an example illustration of a search space. The example search space includes a plurality of neural network elements that in turn include a plurality of neural network blocks that in turn include a plurality of operators. Each operator includes a plurality of neural network layers including one or more of: a 1 × 1 convolutional layer ("1 × 1"), which effectively acts as a cascade aggregator; k × k depth convolution layers ("k × k DWs"); bulk normalization layer ("BN"); and a rectifying linear unit active layer ("ReLU"). An example architecture generated from the search space (as depicted on the left-hand side of fig. 4A) can be in the form of a tower that includes a sequence of neural network elements, where each element after the first element in the sequence receives input from one or more elements earlier in the sequence, receives network input, or both.
FIG. 4B illustrates an example illustration of another search space. The example search space includes a plurality of neural network blocks that, in turn, include a plurality of operators. Each operator includes a plurality of neural network layers including one or more of: a 1 × 1 convolutional layer ("1 × 1"), which effectively acts as a cascade aggregator; 3 × 3 deep convolutional layers ("3 × 3 DWs"); 5 × 5 deep convolutional layers ("5 × 5 DW"); bulk normalization layer ("BN"); and a rectifying linear unit active layer ("ReLU"). The dashed lines represent additional jump connections between different operators contained in the same block. An example architecture generated from the search space (as shown on the left-hand side of fig. 4B) can be in the form of a tower that includes a sequence of neural network blocks, each block after the first block in the sequence receiving input from an earlier block in the sequence.
In some implementations, the neural architecture search system 100 can additionally receive as input data one or more predetermined (e.g., user-specified) resource constraints that identify how much computing resources the neural network can consume in performing the inference. For example, the resource constraint may specify a target amount of computational resources used by the neural network having the final architecture. Target resource usage data specifies: (i) a target memory size indicating the maximum memory size allowed for creating the final architecture, i.e. the maximum memory that can be occupied by parameters and architecture data of the final architecture; and (ii) a target operand (e.g., number of floating point operations per second (FLOPS)) that indicates the maximum operand that the neural network with the final architecture is capable of performing to perform a particular machine learning task. As another example, the resource constraints may specify a target runtime delay for the neural network when performing a task and when deployed on one or more computing devices. Thus, the output of the neural architecture search system 100 may be further associated with specific technical details of the hardware on which the neural network is intended to act.
The neural architecture search system 100 can receive the training data 102, the architecture data 104, the additional input data, or a combination thereof in any of a variety of ways. For example, the system 100 can receive data from a remote user of the system as an upload over a data communication network, e.g., using an Application Programming Interface (API) provided by the system 100. As another example, the system 100 can receive input from a user specifying which data the system 100 has maintained should be used as training data 102 and architecture data 104.
To determine the final architecture, the neural architecture search system 100 repeatedly performs the search process using the variables distribution engine 110, the architecture generation engine 120, and the training engine 130.
The utilization variable assignment engine 110 can assign a utilization variable 112 to each of a plurality of network operators defined in the architecture data 104. Each utilization variable 112 is associated with a value that indicates the likelihood that the network operator is utilized in the final architecture of the neural network.
The utilization variable allocation engine 110 can determine the associated value of each utilization variable 112 according to the respective probabilities of one or more tunable parameter ("distribution parameters") parameterizations that can be maintained by the engine 110. For example, the variable assignment engine 110 can model the utilization variables as bernoulli random variables with continuous relaxation. Bernoulli random variables without continuous relaxation refer to discrete variables with values of 1 (probability p) or 0 (probability 1-p), and the continuous relaxation technique will be further described below. For each network operator, the assigned utilization variable 112 modeled in this manner can effectively be treated as a binary mask, e.g., including the network operator in the framework when the value is 1, and pruning the network operator from the framework when the value is 0.
FIG. 5 illustrates an example illustration of inserting a mask corresponding to a utilization variable into a network operator. As depicted, a binary masking layer corresponding to the utilization variable assigned to a network operator (e.g., the top "op 1") is inserted after that network operator in the architecture. The numbers next to the edges indicate the number of non-zero channels in the masking layer.
In response to sampling the all-zero masking layer inserted after the network operator, the network operator (e.g., the bottom "op 2") may be deselected, i.e., pruned from the current architecture. Thus, deselecting a network operator amounts to inserting a zero masking layer after operators not in the selected set of network operators.
A portion of a component of a network operator (e.g., the bottom "op 3") may be deselected in response to sampling a masking layer inserted after the network operator that has some but not all zero channels. As depicted at the bottom of fig. 5, if the network operator is a component of the convolutional neural network layer, i.e., a filter of the convolutional neural network layer, then in response to sampling the masking layer with 5 zero channels, 5 filters of the total of 8 filters of the convolutional neural network layer may be deselected. This amounts to modifying the width of the convolutional neural network layer.
Thus, the neural architecture search system 100 can act on any one of a plurality of search spaces by: the utilization variable assignment engine 110 is used to assign utilization variables to different network operators, i.e., to insert masks into different neural network layers and different components of the neural network layers. This facilitates fine-grained searching by the system 100 over a larger architectural space.
In particular, the system 100 can model the space of probability values with variables as a continuous space, and the corresponding probabilities of each network operator utilized in the final architecture can be modeled as continuous rather than discrete probabilities with the variable assignment engine 110. This allows the system to have fine grained control over the search process. For example, the neural architecture search system 100 assigns each network operator a utilization variable having a given value (e.g., 1) using the utilization variable assignment engine 110 according to a probability determined from a continuous distribution (e.g., Logistic-Sigmoid distribution or a specific distribution) over a set of probability values for the utilization variable (ranging from 0 to 1, inclusive).
The architecture generation engine 120 can generate a training candidate architecture 122 for the neural network based on the values of the utilization variables 112 assigned to each of the plurality of network operators. To generate a new training candidate architecture 122 at the beginning of the search process (or to update an existing training candidate architecture 122 during the search process), the architecture generation engine 120 can select a selected set of network operators from a plurality of network operators defined by the architecture data 104 based on current values of utilization variables assigned to the plurality of network operators. For example, the schema generation engine 120 can select network operators that have been assigned a utilization variable whose value falls within a particular range, e.g., greater than 0.9, e.g., equal to 1. The trained neural network architecture 122 can then be determined by using the engine 120 as a combination of a selected set of network operators. For example, the architecture generation engine 120 can do this by using an add aggregator that can be configured to combine the fixed shape operator outputs or a 1 × 1 convolutional layer (i.e., a cascade aggregator) that can be configured to combine the variable shape operator outputs.
In some implementations, during the search process, the system 100 maintains a set of distribution parameters used by the utilization variable allocation engine 110, e.g., at a memory device accessible to the system 100, to generate the utilization variables 112 and a set of parameters for the neural network. The set of parameters of the neural network in turn consists of different subsets of parameters associated with different operators of the neural network.
For the training architecture 122 generated using the architecture generation engine 120 and from the architecture data 104 and the utilization variables 112, the training engine 130 trains an instance of the neural network with the training architecture 122 on the training data 102 to iteratively update values of a set of parameters of the neural network, and additionally adjusts values of the utilization variables generated using the utilization variable assignment engine 110. Specifically, during training, training engine 130 jointly optimizes two objectives — a computational cost objective and a task execution objective.
The computational cost target can be derived from user input to the system or from some default computational cost target related to deploying a neural network on one or more computing devices to perform a particular machine learning task. For example, the computational cost target can include one or more of: (i) a target memory size of the final architecture, (ii) a target number of floating point operations per second (FLOPS) of the neural network having the final architecture when performing the particular machine learning task, or (iii) a target runtime delay of the neural network when performing the particular machine learning task.
The task execution goal is capable of evaluating an execution metric of the neural network at the training iteration that measures the execution of the trained neural network for a particular machine learning task. For example, the performance metric can be the loss of the trained neural network on the validation dataset, or the result of some other metric of model accuracy when computed on the validation dataset.
The training engine 130 determines the update by calculating the gradient of an objective function that evaluates the two objectives with respect to the network parameters. To adjust the values of the distribution parameters that in turn define the utilization variables, the training engine 130 can back-propagate the determined gradients into the distribution parameters by utilizing the variables.
During the search process, the system 100 can repeatedly use the architecture generation engine 120 to update the training neural network architecture 122 based on the updated values of the utilization variables assigned to each of the plurality of network operators. This allows the system to continuously and adaptively update the training architecture of the neural network to increase the diversity of the search process. At the same time, the set of network parameters associated with the updated training architecture can be updated based on the gradient of the objective function computed by the training engine 130.
After the search process has terminated, e.g., after a specified number of iterations have been performed or after the gradient of the objective function has converged to a specified value, the neural network search system 100 can then output the final architecture data 150 of the neural network. For example, the neural network search system 100 can output data specifying the final neural network architecture 150 to a user submitting the training data 102. For example, the architecture data can specify neural network operators that are part of the neural network, connectivity between neural network operations, and operations performed by the neural network operators.
In some embodiments, instead of or in addition to outputting the architecture data 150, the system 100 instantiates an instance of the neural network having the determined architecture and with trained parameters, e.g., either trained by the system from scratch after determining the final architecture using parameter values generated as a result of the search process, or generated by fine-tuning parameter values generated as a result of the search process, and then the system 100 processes the request received by the user using the trained neural network (e.g., through a system-provided API). That is, the system 100 is capable of receiving an input to be processed, processing the input using the trained neural network, and providing an output generated by the trained neural network or data derived from the generated output in response to the received input.
Fig. 2 is a flow diagram of an example process 200 for searching a neural network architecture. For convenience, process 200 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural architecture search system, such as neural architecture search system 100 of fig. 1, can perform process 200.
The system receives training data for training a neural network to perform a neural network task (202). The training data includes a plurality of training examples, and, for each training example, a respective target output that should be generated by the neural network to perform a particular task.
The system receives architecture data defining a plurality of network operators (204). In general, when used as part of the architecture of a neural network, each network operator is configured to receive an operator input and generate a corresponding operator output based on processing the operator input according to current values of parameters associated with the network operator. For example, an operator can include a neural network layer, a component of a neural network layer, or a combination of two or more neural network layers, or a combination of two or more components of a neural network layer.
The system assigns a utilization variable to each network operator in the plurality of network operators that indicates a likelihood that the network operator is utilized in the neural network (206). The values associated with the utilization variables can be determined from respective probabilities, which in turn can be parameterized by one or more distribution parameters. For example, the utilization variables can be modeled as bernoulli random variables with continuous slack using a continuous probability distribution (e.g., Logistic-Sigmoid distribution or a specific distribution).
The system generates a neural network (208) for performing a neural network task by jointly updating current values of the utilization variables and current values of parameters of the neural network by repeatedly performing the following two steps: (i) sampling a selected set of network operators from a plurality of network operators and according to respective current values of the utilization variables, and (ii) training a neural network having an architecture defined by the selected set of network operators on training data to perform a neural network task, as described in more detail below.
FIG. 3 is a flow diagram of an example process 300 for training a neural network having an architecture defined by a selected set of network operators. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural architecture search system, such as neural architecture search system 100 of fig. 1, can perform process 300.
In general, the system can repeatedly perform the process 300 to generate a neural network having an optimal architecture for performing neural network tasks.
The system samples (302) a selected set of network operators from a plurality of network operators and according to respective current values with variables.
The system trains a neural network having an architecture defined by the selected set of network operators on training data to perform a neural network task (304). Sampling a selected set of network operators and generating a neural network having an architecture defined by the selected set of network operators is described in more detail above with reference to fig. 1, but in brief this involves using a suitable aggregator (e.g., an add aggregator or a cascade aggregator) to generate a combination of network operators that have been assigned a utilization variable having a current value within a particular range (e.g., equal to 1).
During training, the system jointly updates the current values of the utilization variables and the current values of the neural network parameters to optimize an objective function that simultaneously evaluates a measure of the computational cost of the neural network and a measure of the performance of the neural network on the neural network task associated with the training data.
To do so, the system computes an objective function (306) that evaluates (i) a measure of computational cost of the neural network and (ii) a measure of performance of the neural network on neural network tasks associated with the training data.
For example, the objective function can be evaluated in the following form
Wherein the content of the first and second substances,
In this case, in the example shown,
Wherein, the first and the second end of the pipe are connected with each other,
The system adjusts the current values of the utilization variables and the current values of the neural network parameters based on the determined gradient of the objective function (308). The system can do this by: the gradient of the objective function relative to the neural network parameters is calculated, and then the determined gradient of the objective function is back propagated into the one or more distribution parameters by using the variables.
In some cases, m i Can be modeled as independent Bernoulli variables m i ～Bern(π i ) And the system can use a black-box method (e.g., a perturbation or log-derivative method) to determine an estimate of the gradient with respect to the distribution parameter pi.
In other cases, m can be expressed i Instead of modeling as consecutive samples from Logistic-Sigmoid distribution
The system can then use appropriate update rules, such as a stochastic gradient descent update rule, an Adam update rule, an rmsProp update rule, to apply gradients to adjust utilization variables and network parameters.
This description uses the term "configured" in relation to system and computer program components. For a system of one or more computers configured to perform particular operations or actions, it means that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operations or actions. By one or more computer programs being configured to perform certain operations or actions is meant that the one or more programs include instructions which, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, app, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way, or at all, and it can be stored on a storage device in one or more locations. Thus, for example, an index database can include multiple data sets, each of which can be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and run on the same or multiple computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in particular by, special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
A computer suitable for executing a computer program can be based on a general purpose or special purpose microprocessor or both, or any other type of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or carrying out instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device, such as a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can also be used to provide for interaction with the user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, the computer is able to interact with the user by: sending and receiving documents to and from a device used by a user; for example, by sending a web page to a web browser on the user's device in response to a request received from the web browser. Moreover, the computer is able to interact with the user by: a text message or other form of message is sent to a personal device (e.g., a smartphone running a messaging application) and a response message is in turn received from the user.
The data processing apparatus for implementing the machine learning model can also include, for example, a dedicated hardware accelerator unit for processing the common and computationally intensive portions of the machine learning training or production (i.e., reasoning) workload.
The machine learning model can be implemented and deployed using a machine learning framework (e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework).
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a Web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), e.g., the Internet.
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In some embodiments, the server sends data (e.g., HTML pages) to the user device, for example, for the purpose of displaying data to and receiving user input from a user interacting with the device, which may be a client. Data generated at the user device (e.g., a result of the user interaction) can be received at the server from the user device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings and are set forth in the claims below in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (15)
1. A method, comprising:
receiving training data for training a neural network to perform a neural network task, the training data comprising a plurality of training examples and a respective target output for each of the training examples;
receiving architecture data defining a plurality of network operators;
assigning, to each network operator of the plurality of network operators, an utilization variable indicating a likelihood that the network operator is utilized in a neural network;
generating an optimized neural network for performing the neural network task, comprising repeatedly performing the following operations:
sampling a selected set of network operators from the plurality of network operators and according to respective current values of the utilization variables; and
training the neural network having an architecture defined by the selected set of network operators on the training data to perform the neural network task, wherein the training comprises:
computing an objective function that evaluates (i) a measure of computational cost of the neural network and (ii) a measure of performance of the neural network on the neural network task associated with the training data; and
adjusting the respective current values of the utilization variables and the respective current values of the neural network parameters based on the determined gradient of the objective function.
2. The method of claim 1, wherein the architecture data is initialized from one or more predetermined neural network architectures.
3. The method of any of claims 1 to 2, further comprising:
removing redundant network operators from the plurality of network operators.
4. The method of any of claims 1 to 3, wherein the plurality of operators comprises a neural network layer.
5. The method of claim 4, wherein the neural network layer comprises at least one of: a convolutional layer, a fully connected layer, a normalization layer, or an active layer.
6. The method of any of claims 4 to 5, wherein the plurality of operators further comprises a filter in a convolutional layer or a neuron in a fully-connected layer.
7. The method of any one of the preceding claims, wherein the measure of computational cost of the neural network comprises at least one of: size, number of floating point operations per second (FLOPS), or delay.
8. The method of any one of the preceding claims, wherein generating the optimized neural network for performing the neural network task further comprises:
a zero masking layer is inserted after each operator that is not one of the selected set of network operators.
9. The method of any one of the preceding claims, wherein generating the optimized neural network for performing the neural network task further comprises:
the respective outputs of the selected set of network operations are combined using a concat aggregator.
10. The method of any one of the preceding claims, wherein each utilization variable is defined by one or more distribution parameters.
11. The method of any preceding claim, also dependent on claim 10, further comprising:
calculating the determined gradient of the objective function relative to the one or more distribution parameters.
12. The method of claim 10 or 11, wherein adjusting the respective values of the utilization variables comprises:
back propagating the determined gradient of the objective function into the one or more distribution parameters through the utilization variable.
13. The method of any of claims 10 to 12, wherein the one or more distribution parameters define a binary specific distribution.
14. A system comprising one or more computers and one or more storage devices storing instructions operable, when executed by the one or more computers, to cause the one or more computers to perform the operations of the respective methods of any preceding claim.
15. A computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective methods of any preceding claim.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062971866P | 2020-02-07 | 2020-02-07 | |
US62/971,866 | 2020-02-07 | ||
PCT/US2021/017128 WO2021159101A1 (en) | 2020-02-07 | 2021-02-08 | Fine-grained stochastic neural architecture search |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115066689A true CN115066689A (en) | 2022-09-16 |
Family
ID=74860399
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180012814.5A Pending CN115066689A (en) | 2020-02-07 | 2021-02-08 | Fine-grained stochastic neural architecture search |
Country Status (3)
Country | Link |
---|---|
US (1) | US20230063686A1 (en) |
CN (1) | CN115066689A (en) |
WO (1) | WO2021159101A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113821804B (en) * | 2021-11-24 | 2022-03-15 | 浙江君同智能科技有限责任公司 | Cross-architecture automatic detection method and system for third-party components and security risks thereof |
-
2021
- 2021-02-08 WO PCT/US2021/017128 patent/WO2021159101A1/en active Application Filing
- 2021-02-08 CN CN202180012814.5A patent/CN115066689A/en active Pending
- 2021-02-08 US US17/797,996 patent/US20230063686A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230063686A1 (en) | 2023-03-02 |
WO2021159101A1 (en) | 2021-08-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3711000B1 (en) | Regularized neural network architecture search | |
US10832139B2 (en) | Neural network acceleration and embedding compression systems and methods with activation sparsification | |
US10936949B2 (en) | Training machine learning models using task selection policies to increase learning progress | |
US20210004677A1 (en) | Data compression using jointly trained encoder, decoder, and prior neural networks | |
CN111406267A (en) | Neural architecture search using performance-predictive neural networks | |
US11803731B2 (en) | Neural architecture search with weight sharing | |
US20200104687A1 (en) | Hybrid neural architecture search | |
US20220092416A1 (en) | Neural architecture search through a graph search space | |
US11922281B2 (en) | Training machine learning models using teacher annealing | |
US20220092429A1 (en) | Training neural networks using learned optimizers | |
CN115066689A (en) | Fine-grained stochastic neural architecture search | |
US20230107409A1 (en) | Ensembling mixture-of-experts neural networks | |
US20230121404A1 (en) | Searching for normalization-activation layer architectures | |
US20220383195A1 (en) | Machine learning algorithm search | |
CN115398446A (en) | Machine learning algorithm search using symbolic programming | |
CN114492758A (en) | Training neural networks using layer-by-layer losses | |
CN116324807A (en) | Neural architecture and hardware accelerator search | |
US20240152809A1 (en) | Efficient machine learning model architecture selection | |
US20230107247A1 (en) | Neural networks with transformed activation function layers | |
US20230124177A1 (en) | System and method for training a sparse neural network whilst maintaining sparsity | |
WO2022076933A1 (en) | Efficient hardware accelerator architecture exploration | |
JP2024519265A (en) | Neural network with feedforward spatial transformation units | |
CN115516466A (en) | Hyper-parametric neural network integration |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |