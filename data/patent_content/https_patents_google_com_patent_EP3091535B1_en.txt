EP3091535B1 - Multi-modal input on an electronic device - Google Patents
Multi-modal input on an electronic device Download PDFInfo
- Publication number
- EP3091535B1 EP3091535B1 EP16001249.8A EP16001249A EP3091535B1 EP 3091535 B1 EP3091535 B1 EP 3091535B1 EP 16001249 A EP16001249 A EP 16001249A EP 3091535 B1 EP3091535 B1 EP 3091535B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- input
- user
- text
- speech
- touchscreen
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims description 66
- 238000006243 chemical reaction Methods 0.000 claims description 14
- 230000004044 response Effects 0.000 claims description 3
- 230000015654 memory Effects 0.000 description 35
- 230000008569 process Effects 0.000 description 21
- 238000004891 communication Methods 0.000 description 18
- 238000010586 diagram Methods 0.000 description 14
- 238000012545 processing Methods 0.000 description 11
- 238000004590 computer program Methods 0.000 description 8
- 230000001413 cellular effect Effects 0.000 description 5
- 238000001514 detection method Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 5
- 230000007246 mechanism Effects 0.000 description 5
- 238000013519 translation Methods 0.000 description 5
- 230000014616 translation Effects 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 3
- 238000012360 testing method Methods 0.000 description 3
- 235000010627 Phaseolus vulgaris Nutrition 0.000 description 2
- 244000046052 Phaseolus vulgaris Species 0.000 description 2
- 230000009471 action Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- PEDCQBHIVMGVHV-UHFFFAOYSA-N Glycerine Chemical compound OCC(O)CO PEDCQBHIVMGVHV-UHFFFAOYSA-N 0.000 description 1
- 241000042032 Petrocephalus catostoma Species 0.000 description 1
- 230000001133 acceleration Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000005055 memory storage Effects 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000007781 pre-processing Methods 0.000 description 1
- 239000000047 product Substances 0.000 description 1
- 238000010079 rubber tapping Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/005—Language recognition
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
Definitions
- This document relates to systems and techniques for multi-modal input into an electronic device and conversion of spoken input to text.
- keyboards are common input devices, and they typically include single-digit numbers (e.g., in a cellular telephone) each of the letters in the alphabet, and some characters (e.g., in Qwerty or Dvorak keyboards).
- keyboards are frequently "virtual" in form, and are displayed on a touch screen of a device.
- Such keyboards may be made available to various different applications running on a device, using a program known as an Input Method Editor, or IME, so that the IME receives the user input and then passes it to whatever application is currently active on the device.
- IME Input Method Editor
- An IME can also translate user input, such as when a user enters Roman characters in a written language like Pinyin, and the IME generates Chinese characters that correspond to the typed Pinyin. Where the Pinyin corresponds to multiple possible characters, the IME can display all such characters, the user can tap the intended character, and the IME can pass that character to the operating application.
- keyboards Users of computer devices, and particularly of mobile computing devices, may be constrained in their use of a keyboard.
- the keyboard itself may be constrained in size because mobile device displays are small, so that only a sub-set of relevant characters can be displayed or the keys may be too small to press accurately.
- the user may be constrained, in that they cannot easily type on a keyboard while walking through a crowded airport or driving a car. In such situations, spoken input may be preferred over typed input.
- speech-to-text conversion or translation typically requires lots of computer processing power, and mobile devices typically do not have much processing power. Also, such conversion often requires a particular user to "train" the system so that it better understands the user's voice and speech patterns.
- US 2009/0216531 A1 presents systems, methods and computer readable media providing a speech input interface.
- the interface can receive speech input and non-speech input from a user through a user interface.
- Further prior art is known from US2004/243415A1 disclosing an input method editor.
- This document describes techniques and systems that may be used to provide speech-to-text conversion for a user of a computing device, such as a smartphone.
- the speech input may be handled in a manner similar to other input (e.g., typed input) using an application such as an IME, where the IME can be switched into modes depending on the manner that the user chooses to enter data.
- an application such as an IME
- the input may in certain circumstances be transmitted (either in a raw or converted form) to a server system remote from the computing device that may be programmed to pass a transformed input back to the device, such as by providing text in response to receiving spoken inputs.
- the computing device may also provide the server system with meta data that is passed with, or at essentially the same time as, the spoken inputs, and the meta data may be used by the server system to identify a context in which the user is entering the spoken input.
- the server system may then use that meta data to identify a language model to be used and/or to build a language model on the fly, such as by dynamically applying particular weightings to different language models (which may each be derived from different input corpuses).
- FIG. 1A is a conceptual diagram of an example system 100 that includes a multi-modal input method editor (IME) 102.
- the IME 102 is implemented in a mobile electronic device 104, though it should be understood that the IME 102 can be implemented in a different electronic device, e.g., a PC, laptop computer, PDA, etc.
- the electronic device 104 includes multiple user input devices 106, including a microphone 105 to receive spoken user input.
- Other user input mechanisms include a keyboard, which can include a soft or virtual keyboard (e.g., a touchscreen keyboard 108) or a hard or physical keyboard, a mouse, a trackball, and the like.
- the user input mechanisms 106 are capable of receiving spoken input (i.e., by the microphone) and written input (i.e., by the keyboard 108).
- the user input can be received by the electronic device 104 for use as input into one of various applications 110 that can execute on the electronic device 104, e.g., a web browser, an e-mail application, a word processing application, a contacts book, and/or a calendar.
- the user input is an input into a web form on a particular web page of a particular web site.
- the IME is generally application-independent, i.e., can be used for most any of the applications 110.
- the spoken input can be provided to a remote server for conversion to text.
- the speech utterance 111 can be transmitted over the network 112 to a remote server 114 that includes a speech service 116 and speech recognizer system 118.
- the network 112 can include one or more local area networks (LANs), a wide area network (WAN), such as the Internet, a wireless network, such as a cellular network, or a combination of all of the above.
- LANs local area networks
- WAN wide area network
- wireless network such as a cellular network
- the speech recognizer system 118 can use one or more language models 120 to recognize text from the speech utterance.
- the text 113 which can be a selected best candidate or can be a list of n-best candidates that correspond to the speech utterance, is provided back to the electronic device 104 over the network 112.
- the text 113 can be displayed to the user on a display 122 of the electronic device 104.
- the user 101 can select a candidate from the list that corresponds to the user's spoken input, for example, using the keyboard 108 or another input mechanism, such as touching the touch screen over one of the candidates, to navigate the list and make a selection.
- the user can also provide written input, and can provide input using a combination of written and spoken input.
- the user can begin a search query in a web browser by speaking one or more words and can then add to the query string by typing additional input using the keyboard 108.
- the IME 102 can provide the combined user input to the relevant application, i.e., the web browser application in this example.
- the language that the written input is written in can be determined and then provided as a language indicator to the remote server 114.
- the remote server 114 can use the language indicator when converting the speech utterance 111 to the text 113. For example, by knowing the language in which the speech is spoken, an appropriate language model 120 can be selected for use by the speech recognizer 118.
- FIG. 1B is a block diagram of an example system 130 that can be used to implement a multi-modal IME.
- the example system 130 can be implemented, for example, in a computer device, such as a personal computer device, or other electronic devices, such as a mobile phone, mobile communication device, personal digital assistant (PDA), Global Positioning System (GPS) navigation device, and the like.
- a computer device such as a personal computer device
- PDA personal digital assistant
- GPS Global Positioning System
- the example system 130 includes a processing device 132, a first data store 134, a second data store 136, input devices 138, output devices 140, and a network interface 142.
- a bus system 144 including, for example, a data bus and a motherboard, can be used to establish and control data communication between the components 132, 134, 136, 138, 140 and 142.
- Other system architectures can also be used.
- the processing device 132 can, for example, include one or more microprocessors.
- the first data store 134 can, for example, include a random access memory storage device, such as a dynamic random access memory, or other types of computer-readable medium memory devices.
- the second data store 136 can, for example, include one or more hard drives, a flash memory, and/or a read only memory, or other types of computer-readable medium memory devices.
- the input devices 138 include at least one input device that is configured to receive spoken input and at least one input device configured to receive written input.
- Example input devices 138 can include a microphone, keyboard, a mouse, a stylus, etc.
- example output devices 140 can include a display device, an audio device, etc.
- the network interface 142 can, for example, include a wired or wireless network device operable to communicate data to and from a network 146.
- the network 146 can include one or more local area networks (LANs), a wide area network (WAN), such as the Internet, a wireless network, such as a cellular network, or a combination of all of the above.
- LANs local area networks
- WAN wide area network
- wireless network such as a cellular network
- the system 130 can include input method editor (IME) code 131 from a data store, such as the data store 136.
- the input method editor code 131 can be defined by instructions that upon execution cause the processing device 132 to carry out input method editing functions.
- the input method editor code 131 can, for example, include interpreted instructions, such as script instructions, e.g., JavaScript or ECMAScript instructions, that can be executed in a web browser environment.
- Other implementations can also be used, e.g., a stand-alone application, an applet, a plug-in module, etc., for use in a user interface, such as a display that displays user inputs received by use of keypad mapping for a mobile device or keyboard mapping for a mobile device or personal computer.
- Execution of the input method editor code 131 generates or launches an input method editor instance (IMEI) 133.
- the input method editor instance 133 facilitates the processing of one or more input methods at the system 130, during which time the system 130 can receive inputs for characters or symbols, such as, for example, spoken or written input.
- the user can use one or more of the input devices 138, e.g., a microphone for spoken input or a keyboard for written input.
- the user input can be Roman characters that represent input in a first writing system, e.g., Pinyin, and the input method editor can convert the input to a second writing system, e.g., Hanzi terms.
- a Hanzi term can be composed of more than one Pinyin input.
- the first data store 134 and/or the second data store 136 can store an association of inputs. Based on a user input, the input method editor instance 133 can use information in the data store 134 and/or the data store 136 to identify one or more candidate selections represented by the input. In some implementations, if more than one candidate selection is identified, the candidate selections are displayed on an output device 140. For example, if the user input is spoken input, then a list of candidate selections showing written text representations of the spoken input can be presented to the user on the output device 140. In another example, if the user input is Pinyin inputs, the user can select from the candidate selections a Hanzi term, for example, that the user desires to input.
- a remote computing system 148 having access to the system 130 can be used to convert spoken user input to written user input.
- the remote system 148 can be a server that provides a speech recognition service via the network 146.
- One or more speech utterances forming the spoken input can be transmitted to the remote system 148 over the network 146.
- the remote system 148 can determine a text conversion of the spoken input, for example, using a convenient form of speech recognizer system, and transmit the text conversion to the system 130.
- the text conversion can be a best candidate for text corresponding to the spoken input or can be a list of n-best candidate selections for presentation to the user for selection as the input.
- the speech recognizer system can include Hidden Markov Modeling (HMM) encoded in a finite state transducer (FST). Other configurations of speech recognizer can be used by the remote system 148.
- HMM Hidden Markov Modeling
- FST finite state transducer
- the remote system 148 can also be used to edit a logographic script.
- the remote system 148 may be a server that provides logographic script editing capability via the network 146.
- a user can edit a logographic script stored in the data store 134 and/or the data store 136 using a remote computing system, e.g., a client computer.
- the system 130 can, for example, select a character and receive an input from a user over the network interface 142.
- the processing device 132 can, for example, identify one or more characters adjacent to the selected character, and identify one or more candidate selections based on the received input and the adjacent characters.
- the system 130 can transmit a data communication that includes the candidate selections back to the remote computing system.
- FIG. 1C includes a block diagram of example software that can be used to implement an input method editor in FIG. 1B (e.g., IMEI 133).
- the system 160 includes a user interface 162 and software 164.
- a user 166 can access system 160 through the user interface 162.
- the software 164 includes applications 165, IME engine 166, an operating system (OS) 167, a speech recognition system 169 including a language model 168, and a detection engine 170.
- the operating system 167 is a particular piece of software that can provide the user interface 162 between the software 164 (e.g., applications 165 and IME engine 166) and the user 166.
- the speech recognition system 169 and language model 168 are separate from IME engine 166.
- the speech recognition system 169 and language model 168 (which can include two or more language models) are included within software 164 as a separate software component.
- the speech recognition system 169 and language model 168 can be located remotely (e.g., at the remote system 148 of FIG. 1B ).
- the speech recognition system 169 and language model 168 can be included within the IME engine 166.
- the language model 168 can define one or more language sub-models, each sub-model tailored to a particular application, or webpage, or webform on a particular webpage, or website, to name a few examples.
- Each language sub-model can, for example, define a particular rule set, e.g., grammar particular to a language, phrase sets, verbals, etc., that can be used to determine a user's likely intent in entering a set of inputs (e.g., inputs for generating candidates that are translations, transliterations, or other types of phonetic representations).
- each language sub-model can also include a user history of a particular user, e.g., a dictionary of words and phrased often used by a particular user.
- the detection engine 170 includes an input module 172 and can include a timing module 174.
- the input module 172 can, for example, receive input (e.g., keystrokes representing characters or a speech utterance) to particular applications 165 and send the received input to the IME engine 166.
- the detection engine 170 is a component of the IME engine 166.
- the detection engine 170 can detect input and determine whether or not to send the input to the IME engine 166.
- the IME engine 166 can, for example, be implemented using the input method editor code 131 and associated data stores 134 and 136, and provide output candidates in text converted from speech to an interface (e.g., user interface 162) as the input (e.g., speech utterances) is detected, as described with reference to FIGS. 2 and 3A-E below.
- the components of system 160 can be communicatively coupled to one or more of each other. Though the components identified above are described as being separate or distinct form each other, one or more of the components may be combined in a single system, or to perform a single process or routine.
- the functional description provided herein including separation of responsibility for distinct functions is by way of example.
- Other storage architectures can also be used. In particular, other groupings or other divisions of functional responsibilities can be made as necessary or in accordance with design preferences.
- IME engine 166 can perform the functions of detection engine 170.
- input module 172 and timing module 174 can be combined into a single module.
- FIG. 2 is a flowchart of an inventive process 200 for using an input method editor to receive spoken input from a user input device and to provide written, or textual,input to a corresponding application.
- a request is received from a user for an application-independent input method editor that has written and spoken input capabilities (Step 202).
- the request is received by a mobile electronic device that has a touchscreen keyboard.
- Example screenshots from such a mobile electronic device are shown in FIGS. 3A-E . These screenshots can be used to illustrate the example process 200; however, it should be understood that other devices can implement the process 200, and the screenshots shown are not intended to be limiting.
- FIG. 3A shows a screenshot 300 where a user has selected to activate a web browser application. Through the web browser application, the user has selected to navigate to the Google search page at the URL www.google.com 302.
- FIG. 3B shows a screen shot 304 with a soft touchscreen keyboard 306 displayed in a lower portion of the display screen. For example, the user can touch or tap the screen in the search query field 308 to automatically have the keyboard 306 displayed, although other mechanisms can be used to trigger the display of the keyboard 306.
- the keyboard 306 shown includes a microphone key 310. A request that is received from the user includes the user selecting the microphone key 310.
- Another example includes the user selecting a graphical entity, such as a microphone icon or button, displayed next to or in an input field, e.g., in search query field 308.
- a graphical entity such as a microphone icon or button
- Another example includes the user swiping his/her finger across the input field, e.g., in a left to right motion, or tapping the input field.
- Yet another example includes the user picking up the device in a manner that is consistent with raising a microphone included in the device to the proximity of the user's mouth, which can be detected, for example, by an accelerometer reading.
- Other forms of request can be received from the user for an application-independent input method editor having written and spoken input capabilities, and the above are but some examples.
- a user's intention to provide spoken input to the application-independent input method editor is then identified in the process (Step 204). For example, receiving a speech utterance from the user can be used to identify that the user intends to provide spoken input. In other implementations, receiving the request from the user for the input method editor with written and spoken input capabilities can also be used to identify that the user intends to provide spoken input, i.e., the same user action can provide both the request and be used to identify the user's intention. As shown in the screenshot 312 in FIG. 3C , a graphical element is displayed that prompts the user to speak, such as the microphone graphic 314 and the instructions "Speak now" 316.
- a spoken input i.e., a speech utterance
- the user provides the spoken input as input to an application that is executing on the device (Step 206).
- the spoken input is provided to a remote server that includes a speech recognition system configured to recognize text based on the spoken input (Step 208).
- the spoken input can be sent over the network 146 to the remote system 148, where the remote system 148 includes a speech recognition system to recognize text from a speech utterance. Because processing the speech to text conversion can take some time, a graphic is displayed to the user to indicate that the process is in progress, such as the "Working" graphic 320 shown in the screenshot 322 in FIG. 3D .
- Text is then received from the remote server, where the text represents the spoken input (Step 210).
- the remote server e.g., remote system 148
- the corresponding text is sent back to the user's device and is displayed for the user.
- the best candidate for representation of the speech utterance is selected by the speech recognition system at the remote server and provided to the device.
- an n-best list of candidates can be provided and presented to the user for selection of the correct candidate. For example, referring to FIG. 3E , a screen shot 324 shows a list of suggestions, with the best candidate "the man in the moon" displayed at the top of the list as the default selection.
- the text i.e., the spoken input converted to written input
- the application as user input (Step 212). That is, once the correct text conversion is selected, if a list of candidates was provided, or once the best candidate has been received, if only one was sent from the remote server, the written input can be passed to the application as the user input for processing by the application.
- a context indicator can be sent with the spoken input to the remote system for conversion to text.
- the remote system can use the context indicator to facilitate the speech-to-text conversion.
- the context indicator can be used as a basis for selecting an appropriate language model to use by the speech recognition system.
- the context indicator can specify the context in which the spoken user input was received.
- the context indicator can specify a name of a field, e.g., in a web form, the name of the application in which the input was received, and/or identify a web page if the user input was received in a web browser application.
- the context indicator can include metadata relating to a field in which the user input was received.
- the metadata can specify that the field requires a one-word answer, or a date, or a name, and the like.
- the context indicator information can be obtained by the input method editor from the operating system of the electronic device.
- FIG. 4 is a block diagram of an example system 400 for receiving speech input and training language models to interpret the speech input.
- users of client devices enter data into text input fields, and a speech server analyzes that data to determine the type of text the users entered.
- the speech server builds language models from this information, and uses the language models to recognize speech input associated with similar input fields.
- Clients 402 can receive text input for web forms. When this input is used as part of a process to browse to another page, such as typing in a search field or order form field before pressing a submit button, the text and destination information can be saved in a toolbar log 404.
- Clients 406, such as computers and mobile computing devices, can receive text based web search queries from users. These queries can be resolved by a web search server (not shown) and the queries and search results can be stored in query logs 408.
- the client 406 can receive text input to applications, such as an email client, a messaging client, and/or a word processor and spoken input to applications.
- the text input and transcriptions of spoken input can be stored in input logs 409.
- a speech recognition server 410 can retrieve the data in the toolbar logs 404, the query logs 408, and/or the input logs 409.
- a speech recognition system 412 can group this data into categories or classifications.
- the speech recognition system 412 creates a series of language models 414a-414n.
- the language models can contain words, phrases, sentences, etc. from the toolbar logs 404, query logs 408, and/or the input logs 409 based on a particular topic.
- the language model A 414a is a language model of British last name, it can contain “Churchill,” “Bean,” and “Pigou.”
- the language model B 414b contains technology-related language, it can contain "object oriented programming,” “reverse polish notation,” and “garbage in, garbage out.”
- the speech recognition system 412 can build interpolated language models from the language models 414a-414n. For example, the speech recognition system 412 can use an utterance with a known text result, and poll the language models 414a-414n to determine the confidence level that each language model 414a-414n would associate with the utterance/known text pair. The confidence level returned by each language model 414a-414n can be used by the speech recognition system 412 to create an interpolated language model. For example, using a test utterance and text of "Mr.
- the language model A 414a gives a confidence level of 50%
- the language model B 414b give a confidence level of 65%
- the other language models 414c-414n give substantially 0% confidence levels.
- the speech recognition system 412 can create an interpolated language model that heavily weighs the language model A and B 414a-b. The particular weightings may be equal to or proportional to the confidence levels, in certain nimplementations.
- a client device 416 can execute an application that accepts text input and can receive an utterance from a user for that text input.
- the client device 416 can transmit the utterance, along with the context in which the utterance was made or received, to the speech recognition server 410 for translation into text.
- the context in which an utterance is received is determined by the client device 416 with the field name or label associated with the text input.
- some applications include metadata for an application input field such as package name, field number or name, and/or attribute flags (e.g. long sentence, email, street address, etc.).
- a text label associated with or displayed near an input box is used to determine the context.
- a speech service 418 can receive the utterance and context.
- the speech service 418 can map the context to broader categories or to the categories of the language models 414a-414n.
- the speech service 418 can maintain a cache of utterances and resultant texts, and if a received utterance substantially matches a cached utterance, the speech service 418 can return the appropriate cached resultant texts.
- the speech recognition system 412 uses or creates an interpolated language model to recognize the text in the utterance.
- a single candidate text is identified.
- the top n candidates are identified either wherein all candidates meet a certain confidence threshold, or wherein the top n candidates are selected.
- the text or texts identified by the speech recognition system 412 is returned to the client 416, where it is, for example, displayed in a text input field that has focus.
- FIG. 5A is a block diagram 500 of queries and associated websites in search results.
- a group of queries 502 is a collection of example queries that can be collected from a browser executing a toolbar.
- the queries can be submitted to a web search engine, and a list of search results can be returned. For example, a search for "mike lebeau" can result in search results including facebook.com and linkedin.com.
- a search for "blink" can result in search results including amazon.com.
- the queries and associated websites can be used, for example, in determining a relationship between queries and related topics or categories.
- facebook.com and linkedin.com are social network websites, it can be determined that the queries “bill byrne,” “will rusch,” “mike lebeau,” and “brian stope” may be the names of people.
- amazon.com is a retailer with a reputation as a book retailer, it can be determined that "blink” and "great gatsby” may be the names or titles of a retail products, perhaps books.
- a search for "william gibson” returns links to both facebook.com and amazon.com, it can be determined that "william gibson” may be an author.
- FIG. 5B is a URL showing search terms in a query.
- the URL in this example can be recorded by a web browser executing a toolbar.
- a user submits a form via a GET request, the contents of the form can be encoded and logged in the URL parameters.
- a search on the webpage facebook.com can generate the URL shown.
- the name parameter 550 in this example "bill%20byrne", indicates that "bill byrne" was entered into a field in a web form.
- FIG. 6 is a block diagram of an example system 600 for recognizing speech input to a computing device.
- a user enters speech input to an application via an IME.
- the speech input is analyzed at a speech recognition server and text is returned to the application.
- An application 604 executes on a computing device 602, for example a mobile computing device.
- An input method manager 608 can manage input methods that the application 604 accepts, including speech input from a user.
- a speech IME 610 in the operating system of the computing device 602 records speech input and collects context data from an input context 612.
- Speech IME 160 can transmit the speech input and context to a voice data center 614.
- a speech service front end 616 can receive the speech input and context and prepare the speech input and context for analysis. For example, statistics can be collected, recognition jobs can be created and sorted, etc.
- a speech recognition system 618 examines the context, and selects an associated language model from a big table of language models 622.
- the associated language model in the big table of language models 622 can contain a reference to a language model in a ProdLM 620.
- the speech recognition system 618 uses the referenced language model in the ProdLM 620 to analyze the speech input and determine a text string.
- the text string is returned to the speech input method 610, for example by way of the speech service front end 616.
- the text string can be presented to the user of the computing device 602, and if approved by the user, can be sent to the application 604 as text input.
- FIG. 7A shows a flowchart of an example process 700 of building an interpolated language model.
- pairs of queries and results are collected, base language models are created, and interpolated language models are created.
- pairs containing web queries and result sites are extracted from web search logs.
- a web browser or web browser add on can report queries, query result URLs, or search results to a central repository.
- a web search server can create a log of search terms and website domain names that were returned in response to those search terms.
- a cluster bipartite graph also known as a bigraph, is formed by the pairs.
- a bipartite graph is a graph whose vertices can be divided into two disjoint sets such that every edge connects a vertex in the first set to one in the second set.
- the first set can contain nodes representing queries and the second set can contain nodes representing results.
- Clusters are identified in the bipartite graph, for example so that each cluster consists of queries and results that may by semantically or syntactically similar.
- step 706 language models are trained based on the queries in the clusters.
- sample utterances are recorded by a range of speakers and associated with the queries.
- the utterances for each query can be aggregated, such as by determining an average waveform and a deviation factor indicating how much deviation from the mean was found for most utterances.
- web form sample data is obtained from toolbar logs.
- a web browser toolbar can record and transmit the URLs of websites visited by the browser.
- the URLs can include input text that has been entered into a web form and submitted. This information can be used to associate a particular web site, domain, or web page with a particular topic or idea.
- step 710 the K most significant clusters for a query sample are identified.
- a list of cluster names is collected and ranked in order of significance or similarity to the data collected in the step 708.
- a predetermined number (K, in this case) of the most significant clusters are identified.
- an interpolated model is built.
- a data structure is created that contains links to the K most significant language models. This data structure represents a combined language model that delegates speech recognition functionality to the referenced language models.
- each referenced language model can have a weight, such as a percentage, integer in a particular range, or probability, associated with the referenced language model in the interpolated language model.
- this weight can be determined by testing a known pairs of utterances and text. The pairs can be submitted to the referenced language models, which can return a confidence level representing the likelihood or accuracy of the language model to correctly identify the text in the utterance. The confidence levels, optionally normalized or otherwise converted, can be used as weighting values in the interpolated language model. Thus, reference language models likely to correctly identify text in a particular classification are most heavily weighted.
- step 704. input field flags and descriptions are obtained in the step 708.
- step 702 and step 714 can be combined using a process that builds an interpolation model and interpolation weights in one step.
- FIG. 7B shows a flowchart of an example process 750 of building an interpolated language model.
- text input and associated metadata is collected, base language models are created, and interpolated language models are created using the base models and interpolation factors such as weightings to be provided to each of multiple base models.
- step 752 text input and input metadata pairs are extracted from input logs.
- an application can report to a central repository input entered into an input field and metadata for the input field.
- the text input can include voice input that is transcribed into a text format.
- application wide or operating system wide metadata schemes can define or describe the type of input field, such as an address field, a free form text field, a search field, or a social status field.
- a cluster bipartite graph also known as a bigraph, is formed by the pairs.
- a bipartite graph is a graph whose vertices can be divided into two disjoint sets such that every edge connects a vertex in the first set to one in the second set.
- the first set can contain nodes representing input metadata and the second set can contain nodes representing text input.
- Clusters are identified in the bipartite graph, for example so that each cluster consists of similar input metadata.
- step 756 language models are trained based on the text input in the clusters.
- sample utterances are recorded by a range of speakers and associated with the metadata.
- the utterances for each text input can be aggregated, such as by determining an average waveform and a deviation factor indicating how much deviation from the mean was found for most utterances.
- input metadata is categorized.
- the input metadata can be used to associate a particular input field or application with a particular topic or idea.
- Example topics or ideas include, but are not limited to, address fields, free form text fields, search field, social status fields, and numeric fields.
- step 760 the K most significant clusters for a category are identified.
- a list of cluster names is collected and ranked in order of significance or similarity to the data collected in the step 708.
- a predetermined number (K, in this case) of the most significant clusters are then identified.
- an interpolated model is built.
- a data structure is created that contains links to the K most significant language models. This data structure represents a combined language model that delegates speech recognition functionality to the referenced language models.
- K interpolation weights are optimized.
- Each referenced language model that is referenced in the data structure can have a weight, such as a percentage, integer in a particular range, or probability, associated with it. In some implementations, this weight can be determined by testing a known pair or pairs of utterances and text. The pairs can be submitted to the referenced language models, which can return a confidence level representing the likelihood or accuracy of the language model to correctly identify the text in the utterance. The confidence levels, optionally normalized or otherwise converted, can be used as weighting values in the interpolated language model. Thus, reference language models that are likely to identify text in a particular classification correctly are most heavily weighted.
- a data structure with weighted links to the K most significant language models can be selected from a collection of some or all possible data structures with weighted links to language models.
- FIG. 8 shows a flowchart of an example process 800 for recognizing text in an utterance.
- an utterance is received, the speech in the utterance is recognized, and text, or a list of text options, is returned.
- an utterance is received.
- a speech recognition server can receive translation requests from clients.
- the translation request can include an utterance (e.g. an audio file) and a context (e.g. text or other data describing how the utterance may be used or categorized).
- a relevant language model is determined. For example, an interpolated language model for web search, free form text input, or social status can be determined.
- language models including interpolated language models, can be indexed by keyword, web domain, application type, or other criteria. Using metadata associated with the utterance, such as context or source information, a relevant language model can be determined.
- step 806 speech recognition is performed.
- the utterance can used by a speech recognition application using the language model.
- the speech recognition application can calculate one or more candidate text strings from the utterance.
- a confidence level can be associated with each text string.
- step 808 one a K best list of text strings representing the utterance is returned.
- the text string with the highest confidence value is selected and returned.
- a particular number (K, in this case) of text strings with the highest confidence level are returned.
- step 808 all text strings with a confidence level above a particular threshold can be returned.
- an utterance can be preprocessed before step 806 to improve recognition.
- FIG. 9 shows an example of a generic computer device 900 and a generic mobile computer device 950, which may be used with the techniques described here.
- Computing device 900 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 950 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Computing device 900 includes a processor 902, memory 904, a storage device 906, a high-speed interface 908 connecting to memory 904 and high-speed expansion ports 910, and a low speed interface 912 connecting to low speed bus 914 and storage device 906.
- Each of the components 902, 904, 906, 908, 910, and 912 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 902 may process instructions for execution within the computing device 900, including instructions stored in the memory 904 or on the storage device 906 to display graphical information for a GUI on an external input/output device, such as display 916 coupled to high speed interface 908.
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 900 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 904 stores information within the computing device 900.
- the memory 904 is a volatile memory unit or units.
- the memory 904 is a non-volatile memory unit or units.
- the memory 904 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 906 is capable of providing mass storage for the computing device 900.
- the storage device 906 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product may be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 904, the storage device 906, memory on processor 902, or a propagated signal.
- the high speed controller 908 manages bandwidth-intensive operations for the computing device 900, while the low speed controller 912 manages lower bandwidth-intensive operations.
- the high-speed controller 908 is coupled to memory 904, display 916 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 910, which may accept various expansion cards (not shown).
- low-speed controller 912 is coupled to storage device 906 and low-speed expansion port 914.
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 900 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 920, or multiple times in a group of such servers. It may also be implemented as part of a rack server system 924. In addition, it may be implemented in a personal computer such as a laptop computer 922. Alternatively, components from computing device 900 may be combined with other components in a mobile device (not shown), such as device 950. Each of such devices may contain one or more of computing device 900, 950, and an entire system may be made up of multiple computing devices 900, 950 communicating with each other.
- Computing device 950 includes a processor 952, memory 964, an input/output device such as a display 954, a communication interface 966, and a transceiver 968, among other components.
- the device 950 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 950, 952, 964, 954, 966, and 968 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 952 may execute instructions within the computing device 950, including instructions stored in the memory 964.
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 950, such as control of user interfaces, applications run by device 950, and wireless communication by device 950.
- Processor 952 may communicate with a user through control interface 958 and display interface 956 coupled to a display 954.
- the display 954 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 956 may comprise appropriate circuitry for driving the display 954 to present graphical and other information to a user.
- the control interface 958 may receive commands from a user and convert them for submission to the processor 952.
- an external interface 962 may be provide in communication with processor 952, so as to enable near area communication of device 950 with other devices.
- External interface 962 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 964 stores information within the computing device 950.
- the memory 964 may be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 974 may also be provided and connected to device 950 through expansion interface 972, which may include, for instance, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 974 may provide extra storage space for device 950, or may also store applications or other information for device 950.
- expansion memory 974 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 974 may be provide as a security module for device 950, and may be programmed with instructions that permit secure use of device 950.
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 964, expansion memory 974, memory on processor 952, or a propagated signal that may be received, for example, over transceiver 968 or external interface 962.
- Device 950 may communicate wirelessly through communication interface 966, which may include digital signal processing circuitry where necessary. Communication interface 966 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 968. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 970 may provide additional navigation- and location-related wireless data to device 950, which may be used as appropriate by applications running on device 950.
- GPS Global Positioning System
- Device 950 may also communicate audibly using audio codec 960, which may receive spoken information from a user and convert it to usable digital information. Audio codec 960 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 950. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 950.
- Audio codec 960 may receive spoken information from a user and convert it to usable digital information. Audio codec 960 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 950. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 950.
- the computing device 950 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 980. It may also be implemented as part of a smartphone 982, personal digital assistant, or other similar mobile device.
- Device 950 may also include one or more different devices that are capable of sensing motion. Examples include, but are not limited to, accelerometers and compasses. Accelerometers and compasses, or other devices that are capable of detecting motion or position are available from any number of vendors and may sense motion in a variety of ways. For example, accelerometers may detect changes in acceleration while compasses may detect changes in orientation respective to the magnetic North or South Pole. These changes in motion may be detected by the device 950 and used to update the display of the respective devices 950 according to processes and techniques described herein.
- accelerometers may detect changes in acceleration while compasses may detect changes in orientation respective to the magnetic North or South Pole.
- implementations of the systems and techniques described here may be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations may include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here may be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user may be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here may be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- FIG. 10 is a block diagram of an example data structure 1000 of a language model.
- the data structure 1000 can weightedly link a semantic idea or category with language models, for example, for use in speech recognition.
- a semantic category 1002 such as a search query or type of input field, can be linked to one or more of a collection of interpolated language models 1004.
- the interpolated language models 1004 can be linked to one or more base language models 1006.
- the links between the interpolated language models 1004 and base language models 1006 can be weighted. In some examples, the sum of the weights of the links from one interpolated language model can be 1 or can be normalized to 1.
- interpolated language models 1004 may exist that every weighted combination of base language models 1006 has a linked interpolated language model 1004.
- interpolated language model can be linked to more or fewer base language models 1006.
- different link weights between interpolated language models 1004 and base language modesl 1006 may exist, such as possitive integers, probabilities, or dimensional distance (e.g. W,X,Y,Z values for four dimensional space.)
- multiple semantic categories 1002 can be linked to a single interpolated language model 1004.
- interpolated language models 1004 can be created on demand, such as when a semantic category 1002 is added to the data structure.
- the interpolated language models 1004 can persists after the removal of semantic categories (not shown). For example, previously removed semantic categories (not shown) may have prompted the creation of the interpolated language models 1004a, 1004b, and 1004c.
- a new semantica category 1008 can be added to the data structure 1004, and linked to any of the interpolated language models 1004.
- every possible interpolated language model 1004 can be pre-created for use by a new semantic category 1008. Some of these examples can be used in situations, such as when many new semantic categories 1008 are expected, when pre-processing time is available, and/or when few base language models 1006 are expected.
Description
- This document relates to systems and techniques for multi-modal input into an electronic device and conversion of spoken input to text.
- Computer users employ a number of mechanisms to provide input to their computing devices. Keyboards are common input devices, and they typically include single-digit numbers (e.g., in a cellular telephone) each of the letters in the alphabet, and some characters (e.g., in Qwerty or Dvorak keyboards). On mobile devices, keyboards are frequently "virtual" in form, and are displayed on a touch screen of a device. Such keyboards may be made available to various different applications running on a device, using a program known as an Input Method Editor, or IME, so that the IME receives the user input and then passes it to whatever application is currently active on the device. An IME can also translate user input, such as when a user enters Roman characters in a written language like Pinyin, and the IME generates Chinese characters that correspond to the typed Pinyin. Where the Pinyin corresponds to multiple possible characters, the IME can display all such characters, the user can tap the intended character, and the IME can pass that character to the operating application.
- Users of computer devices, and particularly of mobile computing devices, may be constrained in their use of a keyboard. For example, the keyboard itself may be constrained in size because mobile device displays are small, so that only a sub-set of relevant characters can be displayed or the keys may be too small to press accurately. Also, the user may be constrained, in that they cannot easily type on a keyboard while walking through a crowded airport or driving a car. In such situations, spoken input may be preferred over typed input. However, speech-to-text conversion or translation typically requires lots of computer processing power, and mobile devices typically do not have much processing power. Also, such conversion often requires a particular user to "train" the system so that it better understands the user's voice and speech patterns. United States patent application publication no.
US 2009/0216531 A1 presents systems, methods and computer readable media providing a speech input interface. The interface can receive speech input and non-speech input from a user through a user interface. Further prior art is known fromUS2004/243415A1 disclosing an input method editor. - It is an object of the invention to overcome the shortcomings in the prior art. This object of the invention is solved by the independent claims. Specific embodiments are defined in the dependent claims.
-
-
FIG. 1A is a conceptual diagram of an example system including a multi-modal input method editor. -
FIG. 1B is a block diagram of an example system that can be used to implement the multi-modal input method editor. -
FIG. 1C is a block diagram of example software that can be used to implement the input method editor. -
FIG. 2 is a flow chart of an example process for generating text using a multi-modal input method editor. -
FIGS. 3A-E show example screen shots of an electronic device including a multi-modal input method editor. -
FIG. 4 is a block diagram of an example system for receiving speech input and training language models to interpret the speech input. -
FIG. 5A is a block diagram of queries and associated websites in search results -
FIG. 5B is a URL showing search terms in a query -
FIG. 6 is a block diagram of an example system for recognizing speech input to a computing device. -
FIG. 7A and7B show flowcharts of example processes of building an interpolated language model. -
FIG. 8 shows a flowchart of an example process for recognizing text in an utterance. -
FIG. 9 shows examples of generic computer devices that may be used to execute the actions discussed in this document. -
FIG. 10 is a block diagram of an example data structure of a language model. - Like reference symbols in the various drawings indicate like elements.
- This document describes techniques and systems that may be used to provide speech-to-text conversion for a user of a computing device, such as a smartphone. In certain instances, the speech input may be handled in a manner similar to other input (e.g., typed input) using an application such as an IME, where the IME can be switched into modes depending on the manner that the user chooses to enter data. Where transformation of the input is needed, the input may in certain circumstances be transmitted (either in a raw or converted form) to a server system remote from the computing device that may be programmed to pass a transformed input back to the device, such as by providing text in response to receiving spoken inputs. The computing device may also provide the server system with meta data that is passed with, or at essentially the same time as, the spoken inputs, and the meta data may be used by the server system to identify a context in which the user is entering the spoken input. The server system may then use that meta data to identify a language model to be used and/or to build a language model on the fly, such as by dynamically applying particular weightings to different language models (which may each be derived from different input corpuses).
-
FIG. 1A is a conceptual diagram of anexample system 100 that includes a multi-modal input method editor (IME) 102. In this example, theIME 102 is implemented in a mobileelectronic device 104, though it should be understood that theIME 102 can be implemented in a different electronic device, e.g., a PC, laptop computer, PDA, etc. Theelectronic device 104 includes multipleuser input devices 106, including amicrophone 105 to receive spoken user input. Other user input mechanisms include a keyboard, which can include a soft or virtual keyboard (e.g., a touchscreen keyboard 108) or a hard or physical keyboard, a mouse, a trackball, and the like. Theuser input mechanisms 106 are capable of receiving spoken input (i.e., by the microphone) and written input (i.e., by the keyboard 108). - The user input can be received by the
electronic device 104 for use as input into one ofvarious applications 110 that can execute on theelectronic device 104, e.g., a web browser, an e-mail application, a word processing application, a contacts book, and/or a calendar. In some implementations, the user input is an input into a web form on a particular web page of a particular web site. The IME is generally application-independent, i.e., can be used for most any of theapplications 110. - If the user input is spoken input, i.e., a speech utterance, the spoken input can be provided to a remote server for conversion to text. For example, the
speech utterance 111 can be transmitted over thenetwork 112 to aremote server 114 that includes aspeech service 116 andspeech recognizer system 118. Thenetwork 112 can include one or more local area networks (LANs), a wide area network (WAN), such as the Internet, a wireless network, such as a cellular network, or a combination of all of the above. - The
speech recognizer system 118 can use one ormore language models 120 to recognize text from the speech utterance. Thetext 113, which can be a selected best candidate or can be a list of n-best candidates that correspond to the speech utterance, is provided back to theelectronic device 104 over thenetwork 112. Thetext 113 can be displayed to the user on adisplay 122 of theelectronic device 104. - If the
text 113 includes a list of n-best candidates, the user 101 can select a candidate from the list that corresponds to the user's spoken input, for example, using thekeyboard 108 or another input mechanism, such as touching the touch screen over one of the candidates, to navigate the list and make a selection. - The user can also provide written input, and can provide input using a combination of written and spoken input. For example, the user can begin a search query in a web browser by speaking one or more words and can then add to the query string by typing additional input using the
keyboard 108. TheIME 102 can provide the combined user input to the relevant application, i.e., the web browser application in this example. In some implementations, the language that the written input is written in can be determined and then provided as a language indicator to theremote server 114. Theremote server 114 can use the language indicator when converting thespeech utterance 111 to thetext 113. For example, by knowing the language in which the speech is spoken, anappropriate language model 120 can be selected for use by thespeech recognizer 118. -
FIG. 1B is a block diagram of an example system 130 that can be used to implement a multi-modal IME. The example system 130 can be implemented, for example, in a computer device, such as a personal computer device, or other electronic devices, such as a mobile phone, mobile communication device, personal digital assistant (PDA), Global Positioning System (GPS) navigation device, and the like. - The example system 130 includes a processing device 132, a first data store 134, a second data store 136, input devices 138, output devices 140, and a network interface 142. A bus system 144, including, for example, a data bus and a motherboard, can be used to establish and control data communication between the components 132, 134, 136, 138, 140 and 142. Other system architectures can also be used.
- The processing device 132 can, for example, include one or more microprocessors. The first data store 134 can, for example, include a random access memory storage device, such as a dynamic random access memory, or other types of computer-readable medium memory devices. The second data store 136 can, for example, include one or more hard drives, a flash memory, and/or a read only memory, or other types of computer-readable medium memory devices.
- The input devices 138 include at least one input device that is configured to receive spoken input and at least one input device configured to receive written input. Example input devices 138 can include a microphone, keyboard, a mouse, a stylus, etc., and example output devices 140 can include a display device, an audio device, etc. The network interface 142 can, for example, include a wired or wireless network device operable to communicate data to and from a network 146. The network 146 can include one or more local area networks (LANs), a wide area network (WAN), such as the Internet, a wireless network, such as a cellular network, or a combination of all of the above.
- In some implementations, the system 130 can include input method editor (IME) code 131 from a data store, such as the data store 136. The input method editor code 131 can be defined by instructions that upon execution cause the processing device 132 to carry out input method editing functions. The input method editor code 131 can, for example, include interpreted instructions, such as script instructions, e.g., JavaScript or ECMAScript instructions, that can be executed in a web browser environment. Other implementations can also be used, e.g., a stand-alone application, an applet, a plug-in module, etc., for use in a user interface, such as a display that displays user inputs received by use of keypad mapping for a mobile device or keyboard mapping for a mobile device or personal computer.
- Execution of the input method editor code 131 generates or launches an input method editor instance (IMEI) 133. The input method editor instance 133 facilitates the processing of one or more input methods at the system 130, during which time the system 130 can receive inputs for characters or symbols, such as, for example, spoken or written input. For example, the user can use one or more of the input devices 138, e.g., a microphone for spoken input or a keyboard for written input. In some implementations, the user input can be Roman characters that represent input in a first writing system, e.g., Pinyin, and the input method editor can convert the input to a second writing system, e.g., Hanzi terms. In some examples, a Hanzi term can be composed of more than one Pinyin input.
- The first data store 134 and/or the second data store 136 can store an association of inputs. Based on a user input, the input method editor instance 133 can use information in the data store 134 and/or the data store 136 to identify one or more candidate selections represented by the input. In some implementations, if more than one candidate selection is identified, the candidate selections are displayed on an output device 140. For example, if the user input is spoken input, then a list of candidate selections showing written text representations of the spoken input can be presented to the user on the output device 140. In another example, if the user input is Pinyin inputs, the user can select from the candidate selections a Hanzi term, for example, that the user desires to input.
- In some implementations, a remote computing system 148 having access to the system 130 can be used to convert spoken user input to written user input. For example, the remote system 148 can be a server that provides a speech recognition service via the network 146. One or more speech utterances forming the spoken input can be transmitted to the remote system 148 over the network 146. The remote system 148 can determine a text conversion of the spoken input, for example, using a convenient form of speech recognizer system, and transmit the text conversion to the system 130. The text conversion can be a best candidate for text corresponding to the spoken input or can be a list of n-best candidate selections for presentation to the user for selection as the input. In an example implementation, the speech recognizer system can include Hidden Markov Modeling (HMM) encoded in a finite state transducer (FST). Other configurations of speech recognizer can be used by the remote system 148.
- In some implementations, the remote system 148 can also be used to edit a logographic script. For example, the remote system 148 may be a server that provides logographic script editing capability via the network 146. In one example, a user can edit a logographic script stored in the data store 134 and/or the data store 136 using a remote computing system, e.g., a client computer. The system 130 can, for example, select a character and receive an input from a user over the network interface 142. The processing device 132 can, for example, identify one or more characters adjacent to the selected character, and identify one or more candidate selections based on the received input and the adjacent characters. The system 130 can transmit a data communication that includes the candidate selections back to the remote computing system.
-
FIG. 1C includes a block diagram of example software that can be used to implement an input method editor inFIG. 1B (e.g., IMEI 133). The system 160 includes a user interface 162 and software 164. A user 166 can access system 160 through the user interface 162. The software 164 includes applications 165, IME engine 166, an operating system (OS) 167, a speech recognition system 169 including a language model 168, and a detection engine 170. The operating system 167 is a particular piece of software that can provide the user interface 162 between the software 164 (e.g., applications 165 and IME engine 166) and the user 166. - As shown in
FIG. 1C , the speech recognition system 169 and language model 168 are separate from IME engine 166. In particular, the speech recognition system 169 and language model 168 (which can include two or more language models) are included within software 164 as a separate software component. Other implementations are possible. For example, the speech recognition system 169 and language model 168 can be located remotely (e.g., at the remote system 148 ofFIG. 1B ). As another example, the speech recognition system 169 and language model 168 can be included within the IME engine 166. - The language model 168 can define one or more language sub-models, each sub-model tailored to a particular application, or webpage, or webform on a particular webpage, or website, to name a few examples. Each language sub-model can, for example, define a particular rule set, e.g., grammar particular to a language, phrase sets, verbals, etc., that can be used to determine a user's likely intent in entering a set of inputs (e.g., inputs for generating candidates that are translations, transliterations, or other types of phonetic representations). In some implementations, each language sub-model can also include a user history of a particular user, e.g., a dictionary of words and phrased often used by a particular user.
- The detection engine 170 includes an input module 172 and can include a timing module 174. The input module 172 can, for example, receive input (e.g., keystrokes representing characters or a speech utterance) to particular applications 165 and send the received input to the IME engine 166. In some implementations, the detection engine 170 is a component of the IME engine 166.
- The detection engine 170 can detect input and determine whether or not to send the input to the IME engine 166. The IME engine 166 can, for example, be implemented using the input method editor code 131 and associated data stores 134 and 136, and provide output candidates in text converted from speech to an interface (e.g., user interface 162) as the input (e.g., speech utterances) is detected, as described with reference to
FIGS. 2 and3A-E below. - The components of system 160 can be communicatively coupled to one or more of each other. Though the components identified above are described as being separate or distinct form each other, one or more of the components may be combined in a single system, or to perform a single process or routine. The functional description provided herein including separation of responsibility for distinct functions is by way of example. Other storage architectures can also be used. In particular, other groupings or other divisions of functional responsibilities can be made as necessary or in accordance with design preferences. For example, IME engine 166 can perform the functions of detection engine 170. As another example, input module 172 and timing module 174 can be combined into a single module.
-
FIG. 2 is a flowchart of aninventive process 200 for using an input method editor to receive spoken input from a user input device and to provide written, or textual,input to a corresponding application. A request is received from a user for an application-independent input method editor that has written and spoken input capabilities (Step 202). The request is received by a mobile electronic device that has a touchscreen keyboard. Example screenshots from such a mobile electronic device are shown inFIGS. 3A-E . These screenshots can be used to illustrate theexample process 200; however, it should be understood that other devices can implement theprocess 200, and the screenshots shown are not intended to be limiting. -
FIG. 3A shows a screenshot 300 where a user has selected to activate a web browser application. Through the web browser application, the user has selected to navigate to the Google search page at the URL www.google.com 302.FIG. 3B shows a screen shot 304 with a soft touchscreen keyboard 306 displayed in a lower portion of the display screen. For example, the user can touch or tap the screen in the search query field 308 to automatically have the keyboard 306 displayed, although other mechanisms can be used to trigger the display of the keyboard 306. The keyboard 306 shown includes a microphone key 310. A request that is received from the user includes the user selecting the microphone key 310. Another example includes the user selecting a graphical entity, such as a microphone icon or button, displayed next to or in an input field, e.g., in search query field 308. Another example includes the user swiping his/her finger across the input field, e.g., in a left to right motion, or tapping the input field. Yet another example includes the user picking up the device in a manner that is consistent with raising a microphone included in the device to the proximity of the user's mouth, which can be detected, for example, by an accelerometer reading. Other forms of request can be received from the user for an application-independent input method editor having written and spoken input capabilities, and the above are but some examples. - A user's intention to provide spoken input to the application-independent input method editor is then identified in the process (Step 204). For example, receiving a speech utterance from the user can be used to identify that the user intends to provide spoken input. In other implementations, receiving the request from the user for the input method editor with written and spoken input capabilities can also be used to identify that the user intends to provide spoken input, i.e., the same user action can provide both the request and be used to identify the user's intention. As shown in the screenshot 312 in
FIG. 3C , a graphical element is displayed that prompts the user to speak, such as the microphone graphic 314 and the instructions "Speak now" 316. - A spoken input, i.e., a speech utterance, is then received from the user. The user provides the spoken input as input to an application that is executing on the device (Step 206). The spoken input is provided to a remote server that includes a speech recognition system configured to recognize text based on the spoken input (Step 208). For example, referring again to
FIG. 1B , the spoken input can be sent over the network 146 to the remote system 148, where the remote system 148 includes a speech recognition system to recognize text from a speech utterance. Because processing the speech to text conversion can take some time, a graphic is displayed to the user to indicate that the process is in progress, such as the "Working" graphic 320 shown in the screenshot 322 inFIG. 3D . - Text is then received from the remote server, where the text represents the spoken input (Step 210). Once the remote server, e.g., remote system 148, has processed the speech utterance, the corresponding text is sent back to the user's device and is displayed for the user. In some implementations, the best candidate for representation of the speech utterance is selected by the speech recognition system at the remote server and provided to the device. However, in some implementations, an n-best list of candidates can be provided and presented to the user for selection of the correct candidate. For example, referring to
FIG. 3E , a screen shot 324 shows a list of suggestions, with the best candidate "the man in the moon" displayed at the top of the list as the default selection. - The text, i.e., the spoken input converted to written input, is then provided to the application as user input (Step 212). That is, once the correct text conversion is selected, if a list of candidates was provided, or once the best candidate has been received, if only one was sent from the remote server, the written input can be passed to the application as the user input for processing by the application.
- In some implementations, a context indicator can be sent with the spoken input to the remote system for conversion to text. The remote system can use the context indicator to facilitate the speech-to-text conversion. For example, the context indicator can be used as a basis for selecting an appropriate language model to use by the speech recognition system. The context indicator can specify the context in which the spoken user input was received. For example, the context indicator can specify a name of a field, e.g., in a web form, the name of the application in which the input was received, and/or identify a web page if the user input was received in a web browser application. As another example, the context indicator can include metadata relating to a field in which the user input was received. For example, the metadata can specify that the field requires a one-word answer, or a date, or a name, and the like. In some implementations, the context indicator information can be obtained by the input method editor from the operating system of the electronic device.
-
FIG. 4 is a block diagram of an example system 400 for receiving speech input and training language models to interpret the speech input. In the system 400, users of client devices enter data into text input fields, and a speech server analyzes that data to determine the type of text the users entered. The speech server builds language models from this information, and uses the language models to recognize speech input associated with similar input fields. - Clients 402, such as computers executing a web browser with an optional toolbar, can receive text input for web forms. When this input is used as part of a process to browse to another page, such as typing in a search field or order form field before pressing a submit button, the text and destination information can be saved in a
toolbar log 404. Clients 406, such as computers and mobile computing devices, can receive text based web search queries from users. These queries can be resolved by a web search server (not shown) and the queries and search results can be stored in query logs 408. The client 406 can receive text input to applications, such as an email client, a messaging client, and/or a word processor and spoken input to applications. The text input and transcriptions of spoken input can be stored in input logs 409. - A
speech recognition server 410 can retrieve the data in the toolbar logs 404, the query logs 408, and/or the input logs 409. A speech recognition system 412 can group this data into categories or classifications. The speech recognition system 412 creates a series oflanguage models 414a-414n. The language models can contain words, phrases, sentences, etc. from the toolbar logs 404, query logs 408, and/or the input logs 409 based on a particular topic. For example, if thelanguage model A 414a is a language model of British last name, it can contain "Churchill," "Bean," and "Pigou." In another example, if the language model B 414b contains technology-related language, it can contain "object oriented programming," "reverse polish notation," and "garbage in, garbage out." - In some implementations, the speech recognition system 412 can build interpolated language models from the
language models 414a-414n. For example, the speech recognition system 412 can use an utterance with a known text result, and poll thelanguage models 414a-414n to determine the confidence level that eachlanguage model 414a-414n would associate with the utterance/known text pair. The confidence level returned by eachlanguage model 414a-414n can be used by the speech recognition system 412 to create an interpolated language model. For example, using a test utterance and text of "Mr. Bean writes in reverse polish notation," thelanguage model A 414a gives a confidence level of 50%, the language model B 414b give a confidence level of 65%, and the other language models 414c-414n give substantially 0% confidence levels. In this example, the speech recognition system 412 can create an interpolated language model that heavily weighs the language model A andB 414a-b. The particular weightings may be equal to or proportional to the confidence levels, in certain nimplementations. - A client device 416 can execute an application that accepts text input and can receive an utterance from a user for that text input. The client device 416 can transmit the utterance, along with the context in which the utterance was made or received, to the
speech recognition server 410 for translation into text. In some implementations, the context in which an utterance is received is determined by the client device 416 with the field name or label associated with the text input. For example, some applications include metadata for an application input field such as package name, field number or name, and/or attribute flags (e.g. long sentence, email, street address, etc.). In some applications, a text label associated with or displayed near an input box is used to determine the context. - A
speech service 418 can receive the utterance and context. In some implementations, thespeech service 418 can map the context to broader categories or to the categories of thelanguage models 414a-414n. In some implementations, thespeech service 418 can maintain a cache of utterances and resultant texts, and if a received utterance substantially matches a cached utterance, thespeech service 418 can return the appropriate cached resultant texts. - The speech recognition system 412 uses or creates an interpolated language model to recognize the text in the utterance. In some implementations, a single candidate text is identified. In some implementations, the top n candidates are identified either wherein all candidates meet a certain confidence threshold, or wherein the top n candidates are selected.
- The text or texts identified by the speech recognition system 412 is returned to the client 416, where it is, for example, displayed in a text input field that has focus.
-
FIG. 5A is a block diagram 500 of queries and associated websites in search results. A group ofqueries 502 is a collection of example queries that can be collected from a browser executing a toolbar. The queries can be submitted to a web search engine, and a list of search results can be returned. For example, a search for "mike lebeau" can result in search results including facebook.com and linkedin.com. A search for "blink" can result in search results including amazon.com. - The queries and associated websites can be used, for example, in determining a relationship between queries and related topics or categories. In this example, if it is known that facebook.com and linkedin.com are social network websites, it can be determined that the queries "bill byrne," "will rusch," "mike lebeau," and "brian stope" may be the names of people. Similarly, if it is known that amazon.com is a retailer with a reputation as a book retailer, it can be determined that "blink" and "great gatsby" may be the names or titles of a retail products, perhaps books. Similarly, if a search for "william gibson" returns links to both facebook.com and amazon.com, it can be determined that "william gibson" may be an author.
-
FIG. 5B is a URL showing search terms in a query. The URL in this example can be recorded by a web browser executing a toolbar. When a user submits a form via a GET request, the contents of the form can be encoded and logged in the URL parameters. In this example, a search on the webpage facebook.com can generate the URL shown. Thename parameter 550, in this example "bill%20byrne", indicates that "bill byrne" was entered into a field in a web form. -
FIG. 6 is a block diagram of anexample system 600 for recognizing speech input to a computing device. In thesystem 600, a user enters speech input to an application via an IME. The speech input is analyzed at a speech recognition server and text is returned to the application. - An
application 604 executes on acomputing device 602, for example a mobile computing device. Aninput method manager 608 can manage input methods that theapplication 604 accepts, including speech input from a user. Aspeech IME 610 in the operating system of thecomputing device 602 records speech input and collects context data from aninput context 612. - Speech IME 160 can transmit the speech input and context to a voice data center 614. A speech service
front end 616 can receive the speech input and context and prepare the speech input and context for analysis. For example, statistics can be collected, recognition jobs can be created and sorted, etc. Aspeech recognition system 618 examines the context, and selects an associated language model from a big table oflanguage models 622. The associated language model in the big table oflanguage models 622 can contain a reference to a language model in aProdLM 620. Thespeech recognition system 618 uses the referenced language model in theProdLM 620 to analyze the speech input and determine a text string. The text string is returned to thespeech input method 610, for example by way of the speech servicefront end 616. The text string can be presented to the user of thecomputing device 602, and if approved by the user, can be sent to theapplication 604 as text input. -
FIG. 7A shows a flowchart of anexample process 700 of building an interpolated language model. In theprocess 700, pairs of queries and results are collected, base language models are created, and interpolated language models are created. - In step 702, pairs containing web queries and result sites are extracted from web search logs. For example, a web browser or web browser add on can report queries, query result URLs, or search results to a central repository. In another example, a web search server can create a log of search terms and website domain names that were returned in response to those search terms.
- In
step 704, a cluster bipartite graph, also known as a bigraph, is formed by the pairs. A bipartite graph is a graph whose vertices can be divided into two disjoint sets such that every edge connects a vertex in the first set to one in the second set. In some implementations, the first set can contain nodes representing queries and the second set can contain nodes representing results. Clusters are identified in the bipartite graph, for example so that each cluster consists of queries and results that may by semantically or syntactically similar. - In step 706, language models are trained based on the queries in the clusters. In some implementations, sample utterances are recorded by a range of speakers and associated with the queries. The utterances for each query can be aggregated, such as by determining an average waveform and a deviation factor indicating how much deviation from the mean was found for most utterances.
- In step 708, web form sample data is obtained from toolbar logs. In some implementations, a web browser toolbar can record and transmit the URLs of websites visited by the browser. The URLs can include input text that has been entered into a web form and submitted. This information can be used to associate a particular web site, domain, or web page with a particular topic or idea.
- In step 710, the K most significant clusters for a query sample are identified. A list of cluster names is collected and ranked in order of significance or similarity to the data collected in the step 708. A predetermined number (K, in this case) of the most significant clusters are identified.
- In step 712, an interpolated model is built. In one example, a data structure is created that contains links to the K most significant language models. This data structure represents a combined language model that delegates speech recognition functionality to the referenced language models.
- In step 714, K interpolation weights are optimized. Each referenced language model can have a weight, such as a percentage, integer in a particular range, or probability, associated with the referenced language model in the interpolated language model. In some implementations, this weight can be determined by testing a known pairs of utterances and text. The pairs can be submitted to the referenced language models, which can return a confidence level representing the likelihood or accuracy of the language model to correctly identify the text in the utterance. The confidence levels, optionally normalized or otherwise converted, can be used as weighting values in the interpolated language model. Thus, reference language models likely to correctly identify text in a particular classification are most heavily weighted.
- Although a particular number, type, and order of steps are shown, it will be understood by one skilled in the art that other number, types, and orders are possible. For example, other methods of clustering or graph creation can be used in the
step 704. In another example, input field flags and descriptions are obtained in the step 708. In another example, step 702 and step 714 can be combined using a process that builds an interpolation model and interpolation weights in one step. -
FIG. 7B shows a flowchart of anexample process 750 of building an interpolated language model. In theprocess 750, text input and associated metadata is collected, base language models are created, and interpolated language models are created using the base models and interpolation factors such as weightings to be provided to each of multiple base models. - In
step 752, text input and input metadata pairs are extracted from input logs. For example, an application can report to a central repository input entered into an input field and metadata for the input field. In some examples, the text input can include voice input that is transcribed into a text format. In some examples, application wide or operating system wide metadata schemes can define or describe the type of input field, such as an address field, a free form text field, a search field, or a social status field. - In step 754, a cluster bipartite graph, also known as a bigraph, is formed by the pairs. A bipartite graph is a graph whose vertices can be divided into two disjoint sets such that every edge connects a vertex in the first set to one in the second set. In some implementations, the first set can contain nodes representing input metadata and the second set can contain nodes representing text input. Clusters are identified in the bipartite graph, for example so that each cluster consists of similar input metadata.
- In
step 756, language models are trained based on the text input in the clusters. In some implementations, sample utterances are recorded by a range of speakers and associated with the metadata. The utterances for each text input can be aggregated, such as by determining an average waveform and a deviation factor indicating how much deviation from the mean was found for most utterances. - In
step 758, input metadata is categorized. The input metadata can be used to associate a particular input field or application with a particular topic or idea. Example topics or ideas include, but are not limited to, address fields, free form text fields, search field, social status fields, and numeric fields. - In
step 760, the K most significant clusters for a category are identified. A list of cluster names is collected and ranked in order of significance or similarity to the data collected in the step 708. A predetermined number (K, in this case) of the most significant clusters are then identified. - In
step 762, an interpolated model is built. In one example, a data structure is created that contains links to the K most significant language models. This data structure represents a combined language model that delegates speech recognition functionality to the referenced language models. - In
step 764, K interpolation weights are optimized. Each referenced language model that is referenced in the data structure can have a weight, such as a percentage, integer in a particular range, or probability, associated with it. In some implementations, this weight can be determined by testing a known pair or pairs of utterances and text. The pairs can be submitted to the referenced language models, which can return a confidence level representing the likelihood or accuracy of the language model to correctly identify the text in the utterance. The confidence levels, optionally normalized or otherwise converted, can be used as weighting values in the interpolated language model. Thus, reference language models that are likely to identify text in a particular classification correctly are most heavily weighted. - Although a particular number, type, and order of steps are shown for the process in this figure, it will be understood by one skilled in the art that other number, types, and orders are possible. For example, in
step 762 and step 764, a data structure with weighted links to the K most significant language models can be selected from a collection of some or all possible data structures with weighted links to language models. -
FIG. 8 shows a flowchart of anexample process 800 for recognizing text in an utterance. In theprocess 800, an utterance is received, the speech in the utterance is recognized, and text, or a list of text options, is returned. - In
step 802, an utterance is received. For example, a speech recognition server can receive translation requests from clients. The translation request can include an utterance (e.g. an audio file) and a context (e.g. text or other data describing how the utterance may be used or categorized). - In
step 804, a relevant language model is determined. For example, an interpolated language model for web search, free form text input, or social status can be determined. In some implementations, language models, including interpolated language models, can be indexed by keyword, web domain, application type, or other criteria. Using metadata associated with the utterance, such as context or source information, a relevant language model can be determined. - In
step 806, speech recognition is performed. The utterance can used by a speech recognition application using the language model. The speech recognition application can calculate one or more candidate text strings from the utterance. A confidence level can be associated with each text string. - In step 808, one a K best list of text strings representing the utterance is returned. In some implementations, the text string with the highest confidence value is selected and returned. In some implementations, a particular number (K, in this case) of text strings with the highest confidence level are returned.
- Although a particular number, type, and order of steps are shown, it will be understood by one skilled in the art that other number, types, and orders are possible. For example, in step 808, all text strings with a confidence level above a particular threshold can be returned. In another example, an utterance can be preprocessed before
step 806 to improve recognition. -
FIG. 9 shows an example of ageneric computer device 900 and a genericmobile computer device 950, which may be used with the techniques described here.Computing device 900 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.Computing device 950 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. -
Computing device 900 includes a processor 902,memory 904, astorage device 906, a high-speed interface 908 connecting tomemory 904 and high-speed expansion ports 910, and alow speed interface 912 connecting tolow speed bus 914 andstorage device 906. Each of thecomponents computing device 900, including instructions stored in thememory 904 or on thestorage device 906 to display graphical information for a GUI on an external input/output device, such asdisplay 916 coupled tohigh speed interface 908. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 900 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 904 stores information within thecomputing device 900. In one implementation, thememory 904 is a volatile memory unit or units. In another implementation, thememory 904 is a non-volatile memory unit or units. Thememory 904 may also be another form of computer-readable medium, such as a magnetic or optical disk. - The
storage device 906 is capable of providing mass storage for thecomputing device 900. In one implementation, thestorage device 906 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 904, thestorage device 906, memory on processor 902, or a propagated signal. - The
high speed controller 908 manages bandwidth-intensive operations for thecomputing device 900, while thelow speed controller 912 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller 908 is coupled tomemory 904, display 916 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 910, which may accept various expansion cards (not shown). In the implementation, low-speed controller 912 is coupled tostorage device 906 and low-speed expansion port 914. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 900 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 920, or multiple times in a group of such servers. It may also be implemented as part of arack server system 924. In addition, it may be implemented in a personal computer such as alaptop computer 922. Alternatively, components fromcomputing device 900 may be combined with other components in a mobile device (not shown), such asdevice 950. Each of such devices may contain one or more ofcomputing device multiple computing devices -
Computing device 950 includes aprocessor 952,memory 964, an input/output device such as adisplay 954, acommunication interface 966, and atransceiver 968, among other components. Thedevice 950 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of thecomponents - The
processor 952 may execute instructions within thecomputing device 950, including instructions stored in thememory 964. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of thedevice 950, such as control of user interfaces, applications run bydevice 950, and wireless communication bydevice 950. -
Processor 952 may communicate with a user throughcontrol interface 958 anddisplay interface 956 coupled to adisplay 954. Thedisplay 954 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. Thedisplay interface 956 may comprise appropriate circuitry for driving thedisplay 954 to present graphical and other information to a user. Thecontrol interface 958 may receive commands from a user and convert them for submission to theprocessor 952. In addition, anexternal interface 962 may be provide in communication withprocessor 952, so as to enable near area communication ofdevice 950 with other devices.External interface 962 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used. - The
memory 964 stores information within thecomputing device 950. Thememory 964 may be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.Expansion memory 974 may also be provided and connected todevice 950 throughexpansion interface 972, which may include, for instance, a SIMM (Single In Line Memory Module) card interface.Such expansion memory 974 may provide extra storage space fordevice 950, or may also store applications or other information fordevice 950. Specifically,expansion memory 974 may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example,expansion memory 974 may be provide as a security module fordevice 950, and may be programmed with instructions that permit secure use ofdevice 950. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner. - The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the
memory 964,expansion memory 974, memory onprocessor 952, or a propagated signal that may be received, for example, overtransceiver 968 orexternal interface 962. -
Device 950 may communicate wirelessly throughcommunication interface 966, which may include digital signal processing circuitry where necessary.Communication interface 966 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 968. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System)receiver module 970 may provide additional navigation- and location-related wireless data todevice 950, which may be used as appropriate by applications running ondevice 950. -
Device 950 may also communicate audibly usingaudio codec 960, which may receive spoken information from a user and convert it to usable digital information.Audio codec 960 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset ofdevice 950. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating ondevice 950. - The
computing device 950 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as acellular telephone 980. It may also be implemented as part of asmartphone 982, personal digital assistant, or other similar mobile device. -
Device 950 may also include one or more different devices that are capable of sensing motion. Examples include, but are not limited to, accelerometers and compasses. Accelerometers and compasses, or other devices that are capable of detecting motion or position are available from any number of vendors and may sense motion in a variety of ways. For example, accelerometers may detect changes in acceleration while compasses may detect changes in orientation respective to the magnetic North or South Pole. These changes in motion may be detected by thedevice 950 and used to update the display of therespective devices 950 according to processes and techniques described herein. - Various implementations of the systems and techniques described here may be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations may include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" "computer-readable medium" refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
- To provide for interaction with a user, the systems and techniques described here may be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user may be received in any form, including acoustic, speech, or tactile input.
- The systems and techniques described here may be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system may be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
- The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
-
FIG. 10 is a block diagram of anexample data structure 1000 of a language model. Thedata structure 1000 can weightedly link a semantic idea or category with language models, for example, for use in speech recognition. - A
semantic category 1002, such as a search query or type of input field, can be linked to one or more of a collection of interpolatedlanguage models 1004. The interpolatedlanguage models 1004 can be linked to one or morebase language models 1006. The links between the interpolatedlanguage models 1004 andbase language models 1006 can be weighted. In some examples, the sum of the weights of the links from one interpolated language model can be 1 or can be normalized to 1. - It will be understood that, although a particular number and configuration of of interpolated
language models 1004,base language models 1006, and links are shown, other numbers and conffigurations are possible. For example, suffieicent interpolatedlanguage models 1004 may exist that every weighted combination ofbase language models 1006 has a linked interpolatedlanguage model 1004. In some examples, interpolated language model can be linked to more or fewerbase language models 1006. In some examples, different link weights between interpolatedlanguage models 1004 andbase language modesl 1006 may exist, such as possitive integers, probabilities, or dimensional distance (e.g. W,X,Y,Z values for four dimensional space.) In some implementations, multiplesemantic categories 1002 can be linked to a single interpolatedlanguage model 1004. - In some examples, interpolated
language models 1004 can be created on demand, such as when asemantic category 1002 is added to the data structure. The interpolatedlanguage models 1004 can persists after the removal of semantic categories (not shown). For example, previously removed semantic categories (not shown) may have prompted the creation of the interpolatedlanguage models data structure 1004, and linked to any of the interpolatedlanguage models 1004. - In some examples, every possible interpolated
language model 1004 can be pre-created for use by a new semantic category 1008. Some of these examples can be used in situations, such as when many new semantic categories 1008 are expected, when pre-processing time is available, and/or when fewbase language models 1006 are expected. - A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the scope of what is described here.
- In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.
Claims (3)
- A computer-implemented method, comprising:displaying, on a touchscreen of a mobile electronic device (104), a touchscreen keyboard (108, 306) in a lower portion of the touchscreen, the touchscreen keyboard including a microphone key (310);receiving a request for an application-independent input method editor that has written and spoken input capabilities based on a selection of the microphone key (310) on the touchscreen keyboard (108, 306);identifying a user's intention to provide spoken input based on the request;in response to the request:
displaying, in the lower portion of the touchscreen, in place of the touchscreen keyboard, a graphical element prompting the user to speak, so that the touchscreen keyboard is not displayed on the touchscreen;receiving, from the user using a microphone, a speech utterance as input to the mobile electronic device (104);providing the speech utterance to a remote server that includes a speech recognition system configured to recognize text based on the speech utterance;displaying, in the lower portion of the touchscreen, in place of the graphical element prompting the user to speak, a graphical element indicating that conversion of the speech utterance to at least one text candidate is in progress;receiving the at least one text candidate from the remote server, each text candidate of the at least one text candidate corresponding to the speech utterance;displaying the at least one text candidate on the touchscreen;when displaying the at least one text candidate, displaying, in the lower portion of the touchscreen, the keyboard in place of the graphical element indicating that conversion of the speech utterance to at least one text candidate is in progress; andproviding a text candidate of the at least one text candidate to an application executing on the mobile electronic device as input. - One or more non-transitory computer-readable media having instructions stored thereon that, when executed by one or more processors, cause performance of the method according to claim 1.
- A mobile electronic device (104), comprising:an electronic display (954);a microphone;one or more processors; andone or more computer-readable media having instructions stored thereon that, when executed by the one or more processors, cause performance of:
the method according to claim 1.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP23202787.0A EP4318463A3 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US28996809P | 2009-12-23 | 2009-12-23 | |
US33021910P | 2010-04-30 | 2010-04-30 | |
EP10165480.4A EP2339576B1 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
Related Parent Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP10165480.4A Division-Into EP2339576B1 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
EP10165480.4A Division EP2339576B1 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP23202787.0A Division EP4318463A3 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
Publications (3)
Publication Number | Publication Date |
---|---|
EP3091535A2 EP3091535A2 (en) | 2016-11-09 |
EP3091535A3 EP3091535A3 (en) | 2017-03-01 |
EP3091535B1 true EP3091535B1 (en) | 2023-10-11 |
Family
ID=43858442
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP10165480.4A Active EP2339576B1 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
EP23202787.0A Pending EP4318463A3 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
EP16001249.8A Active EP3091535B1 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
Family Applications Before (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP10165480.4A Active EP2339576B1 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
EP23202787.0A Pending EP4318463A3 (en) | 2009-12-23 | 2010-06-10 | Multi-modal input on an electronic device |
Country Status (2)
Country | Link |
---|---|
US (12) | US9495127B2 (en) |
EP (3) | EP2339576B1 (en) |
Families Citing this family (376)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7516190B2 (en) | 2000-02-04 | 2009-04-07 | Parus Holdings, Inc. | Personal voice-based information retrieval system |
US8181205B2 (en) | 2002-09-24 | 2012-05-15 | Russ Samuel H | PVR channel and PVR IPG information |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US8996376B2 (en) | 2008-04-05 | 2015-03-31 | Apple Inc. | Intelligent text-to-speech conversion |
US20130275899A1 (en) * | 2010-01-18 | 2013-10-17 | Apple Inc. | Application Gateway for Providing Different User Interfaces for Limited Distraction and Non-Limited Distraction Contexts |
US20100030549A1 (en) | 2008-07-31 | 2010-02-04 | Lee Michael M | Mobile device having human language translation capability with positional feedback |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
USD625733S1 (en) | 2009-03-04 | 2010-10-19 | Apple Inc. | Graphical user interface for a display screen or portion thereof |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US9431006B2 (en) | 2009-07-02 | 2016-08-30 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
JP5310389B2 (en) * | 2009-08-27 | 2013-10-09 | ソニー株式会社 | Information processing apparatus, information processing method, and program |
US11416214B2 (en) | 2009-12-23 | 2022-08-16 | Google Llc | Multi-modal input on an electronic device |
EP2339576B1 (en) * | 2009-12-23 | 2019-08-07 | Google LLC | Multi-modal input on an electronic device |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US8626511B2 (en) * | 2010-01-22 | 2014-01-07 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US20110184723A1 (en) * | 2010-01-25 | 2011-07-28 | Microsoft Corporation | Phonetic suggestion engine |
CN116312567A (en) * | 2010-02-18 | 2023-06-23 | 株式会社尼康 | Portable device and information processing system |
US8682667B2 (en) | 2010-02-25 | 2014-03-25 | Apple Inc. | User profiling for selecting user specific voice input processing information |
US9009040B2 (en) * | 2010-05-05 | 2015-04-14 | Cisco Technology, Inc. | Training a transcription system |
US20110314003A1 (en) * | 2010-06-17 | 2011-12-22 | Microsoft Corporation | Template concatenation for capturing multiple concepts in a voice query |
US8442827B2 (en) * | 2010-06-18 | 2013-05-14 | At&T Intellectual Property I, L.P. | System and method for customized voice response |
US8239366B2 (en) * | 2010-09-08 | 2012-08-07 | Nuance Communications, Inc. | Method and apparatus for processing spoken search queries |
US10002608B2 (en) | 2010-09-17 | 2018-06-19 | Nuance Communications, Inc. | System and method for using prosody for voice-enabled search |
US8401853B2 (en) * | 2010-09-22 | 2013-03-19 | At&T Intellectual Property I, L.P. | System and method for enhancing voice-enabled search based on automated demographic identification |
KR20120066530A (en) * | 2010-12-14 | 2012-06-22 | 한국전자통신연구원 | Method of estimating language model weight and apparatus for the same |
US9063931B2 (en) * | 2011-02-16 | 2015-06-23 | Ming-Yuan Wu | Multiple language translation system |
US9674328B2 (en) * | 2011-02-22 | 2017-06-06 | Speak With Me, Inc. | Hybridized client-server speech recognition |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US9679561B2 (en) * | 2011-03-28 | 2017-06-13 | Nuance Communications, Inc. | System and method for rapid customization of speech recognition models |
US9263045B2 (en) * | 2011-05-17 | 2016-02-16 | Microsoft Technology Licensing, Llc | Multi-mode text input |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
CA2839265A1 (en) | 2011-06-19 | 2012-12-27 | Mmodal Ip Llc | Speech recognition using context-aware recognition models |
AU2012272977A1 (en) | 2011-06-20 | 2014-01-16 | Tandemseven, Inc. | System and method for building and managing user experience for computer software interfaces |
GB2493413B (en) | 2011-07-25 | 2013-12-25 | Ibm | Maintaining and supplying speech models |
US8650031B1 (en) * | 2011-07-31 | 2014-02-11 | Nuance Communications, Inc. | Accuracy improvement of spoken queries transcription using co-occurrence information |
KR20130016644A (en) * | 2011-08-08 | 2013-02-18 | 삼성전자주식회사 | Voice recognition apparatus, voice recognition server, voice recognition system and voice recognition method |
US8676580B2 (en) * | 2011-08-16 | 2014-03-18 | International Business Machines Corporation | Automatic speech and concept recognition |
US9411970B2 (en) * | 2011-08-19 | 2016-08-09 | Microsoft Technology Licensing, Llc | Sealing secret data with a policy that includes a sensor-based constraint |
US8589160B2 (en) * | 2011-08-19 | 2013-11-19 | Dolbey & Company, Inc. | Systems and methods for providing an electronic dictation interface |
US9576573B2 (en) * | 2011-08-29 | 2017-02-21 | Microsoft Technology Licensing, Llc | Using multiple modality input to feedback context for natural language understanding |
EP2758958A4 (en) * | 2011-09-21 | 2015-04-08 | Nuance Communications Inc | Efficient incremental modification of optimized finite-state transducers (fsts) for use in speech applications |
US9053087B2 (en) * | 2011-09-23 | 2015-06-09 | Microsoft Technology Licensing, Llc | Automatic semantic evaluation of speech recognition results |
US9129606B2 (en) * | 2011-09-23 | 2015-09-08 | Microsoft Technology Licensing, Llc | User query history expansion for improving language model adaptation |
USD671558S1 (en) * | 2011-10-04 | 2012-11-27 | Apple Inc. | Display screen or portion thereof with icon |
US8924853B2 (en) * | 2011-10-07 | 2014-12-30 | Blackberry Limited | Apparatus, and associated method, for cognitively translating media to facilitate understanding |
KR101185354B1 (en) * | 2011-10-10 | 2012-09-21 | 한화에스앤씨주식회사 | Apparatus for providing linked service among closed user groups based on smart television and smart set-top box |
US8930189B2 (en) | 2011-10-28 | 2015-01-06 | Microsoft Corporation | Distributed user input to text generated by a speech to text transcription service |
US9159236B2 (en) | 2011-12-01 | 2015-10-13 | Elwha Llc | Presentation of shared threat information in a transportation-related context |
US10875525B2 (en) | 2011-12-01 | 2020-12-29 | Microsoft Technology Licensing Llc | Ability enhancement |
US9064152B2 (en) | 2011-12-01 | 2015-06-23 | Elwha Llc | Vehicular threat detection based on image analysis |
US8934652B2 (en) | 2011-12-01 | 2015-01-13 | Elwha Llc | Visual presentation of speaker-related information |
US9053096B2 (en) * | 2011-12-01 | 2015-06-09 | Elwha Llc | Language translation based on speaker-related information |
US9107012B2 (en) | 2011-12-01 | 2015-08-11 | Elwha Llc | Vehicular threat detection based on audio signals |
US9368028B2 (en) | 2011-12-01 | 2016-06-14 | Microsoft Technology Licensing, Llc | Determining threats based on information from road-based devices in a transportation-related context |
US8811638B2 (en) | 2011-12-01 | 2014-08-19 | Elwha Llc | Audible assistance |
US9245254B2 (en) | 2011-12-01 | 2016-01-26 | Elwha Llc | Enhanced voice conferencing with history, language translation and identification |
US9214157B2 (en) * | 2011-12-06 | 2015-12-15 | At&T Intellectual Property I, L.P. | System and method for machine-mediated human-human conversation |
US9348479B2 (en) | 2011-12-08 | 2016-05-24 | Microsoft Technology Licensing, Llc | Sentiment aware user interface customization |
US8903824B2 (en) * | 2011-12-09 | 2014-12-02 | International Business Machines Corporation | Vertex-proximity query processing |
US8788269B2 (en) | 2011-12-15 | 2014-07-22 | Microsoft Corporation | Satisfying specified intent(s) based on multimodal request(s) |
US9378290B2 (en) | 2011-12-20 | 2016-06-28 | Microsoft Technology Licensing, Llc | Scenario-adaptive input method editor |
US20130173265A1 (en) * | 2012-01-03 | 2013-07-04 | Chiaka Chukwuma Okoroh | Speech-to-online-text system |
US9324323B1 (en) * | 2012-01-13 | 2016-04-26 | Google Inc. | Speech recognition using topic-specific language models |
US11544750B1 (en) | 2012-01-17 | 2023-01-03 | Google Llc | Overlaying content items with third-party reviews |
USD692910S1 (en) * | 2012-02-07 | 2013-11-05 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US10209954B2 (en) * | 2012-02-14 | 2019-02-19 | Microsoft Technology Licensing, Llc | Equal access to speech and touch input |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
USD703231S1 (en) | 2012-03-06 | 2014-04-22 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US8775177B1 (en) | 2012-03-08 | 2014-07-08 | Google Inc. | Speech recognition process |
US9317605B1 (en) | 2012-03-21 | 2016-04-19 | Google Inc. | Presenting forked auto-completions |
US8521539B1 (en) * | 2012-03-26 | 2013-08-27 | Nuance Communications, Inc. | Method for chinese point-of-interest search |
USD705808S1 (en) * | 2012-03-27 | 2014-05-27 | Apple Inc. | Display screen or portion thereof with animated graphical user interface |
WO2013169759A2 (en) * | 2012-05-07 | 2013-11-14 | Citrix Systems, Inc. | Speech recognition support for remote applications and desktops |
US9280610B2 (en) | 2012-05-14 | 2016-03-08 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
CN103426089B (en) * | 2012-05-17 | 2019-05-03 | 北京京东尚科信息技术有限公司 | Commodity purchase guiding system and method |
US20130326347A1 (en) * | 2012-05-31 | 2013-12-05 | Microsoft Corporation | Application language libraries for managing computing environment languages |
US9639676B2 (en) | 2012-05-31 | 2017-05-02 | Microsoft Technology Licensing, Llc | Login interface selection for computing environment user login |
US11023520B1 (en) | 2012-06-01 | 2021-06-01 | Google Llc | Background audio identification for query disambiguation |
US9123338B1 (en) | 2012-06-01 | 2015-09-01 | Google Inc. | Background audio identification for speech disambiguation |
US9081814B1 (en) | 2012-06-01 | 2015-07-14 | Google Inc. | Using an entity database to answer entity-triggering questions |
US9721563B2 (en) | 2012-06-08 | 2017-08-01 | Apple Inc. | Name recognition system |
USD703695S1 (en) | 2012-06-10 | 2014-04-29 | Apple Inc. | Display screen or portion thereof with graphical user interface |
USD711408S1 (en) * | 2012-06-10 | 2014-08-19 | Apple Inc. | Display screen or portion thereof with graphical user interface |
USD754159S1 (en) | 2012-06-11 | 2016-04-19 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US9734839B1 (en) * | 2012-06-20 | 2017-08-15 | Amazon Technologies, Inc. | Routing natural language commands to the appropriate applications |
US9043205B2 (en) * | 2012-06-21 | 2015-05-26 | Google Inc. | Dynamic language model |
US9594831B2 (en) | 2012-06-22 | 2017-03-14 | Microsoft Technology Licensing, Llc | Targeted disambiguation of named entities |
CN110488991A (en) | 2012-06-25 | 2019-11-22 | 微软技术许可有限责任公司 | Input Method Editor application platform |
US9959340B2 (en) * | 2012-06-29 | 2018-05-01 | Microsoft Technology Licensing, Llc | Semantic lexicon-based input method editor |
KR101605862B1 (en) | 2012-06-29 | 2016-03-24 | 삼성전자주식회사 | Display apparatus, electronic device, interactive system and controlling method thereof |
KR20140004515A (en) * | 2012-07-03 | 2014-01-13 | 삼성전자주식회사 | Display apparatus, interactive server and method for providing response information |
US9747895B1 (en) | 2012-07-10 | 2017-08-29 | Google Inc. | Building language models for a user in a social network from linguistic information |
US20140039893A1 (en) * | 2012-07-31 | 2014-02-06 | Sri International | Personalized Voice-Driven User Interfaces for Remote Multi-User Services |
US8831957B2 (en) * | 2012-08-01 | 2014-09-09 | Google Inc. | Speech recognition models based on location indicia |
US8959109B2 (en) | 2012-08-06 | 2015-02-17 | Microsoft Corporation | Business intelligent in-document suggestions |
KR101911999B1 (en) | 2012-08-30 | 2018-10-25 | 마이크로소프트 테크놀로지 라이센싱, 엘엘씨 | Feature-based candidate selection |
US20140074466A1 (en) | 2012-09-10 | 2014-03-13 | Google Inc. | Answering questions using environmental context |
US9547647B2 (en) | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
RU2530268C2 (en) * | 2012-11-28 | 2014-10-10 | Общество с ограниченной ответственностью "Спиктуит" | Method for user training of information dialogue system |
US9190057B2 (en) * | 2012-12-12 | 2015-11-17 | Amazon Technologies, Inc. | Speech model retrieval in distributed speech recognition systems |
BR112015014830B1 (en) * | 2012-12-28 | 2021-11-16 | Sony Corporation | DEVICE AND METHOD OF INFORMATION PROCESSING, AND MEMORY STORAGE MEANS. |
AU349920S (en) * | 2013-01-05 | 2013-07-29 | Samsung Electronics Co Ltd | Display screen for an electronic device |
CN103942230B (en) * | 2013-01-21 | 2017-03-29 | 上海智臻智能网络科技股份有限公司 | A kind of methods, devices and systems for carrying out voice web page navigation |
US9697821B2 (en) * | 2013-01-29 | 2017-07-04 | Tencent Technology (Shenzhen) Company Limited | Method and system for building a topic specific language model for use in automatic speech recognition |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
USD736255S1 (en) * | 2013-02-23 | 2015-08-11 | Samsung Electronics Co., Ltd. | Display screen or portion thereof with graphical user interface |
US10424292B1 (en) * | 2013-03-14 | 2019-09-24 | Amazon Technologies, Inc. | System for recognizing and responding to environmental noises |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US9875237B2 (en) * | 2013-03-14 | 2018-01-23 | Microsfot Technology Licensing, Llc | Using human perception in building language understanding models |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US9953630B1 (en) * | 2013-05-31 | 2018-04-24 | Amazon Technologies, Inc. | Language recognition for device settings |
WO2014197334A2 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
WO2014197335A1 (en) | 2013-06-08 | 2014-12-11 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
USD755240S1 (en) | 2013-06-09 | 2016-05-03 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
KR101959188B1 (en) | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
USD819649S1 (en) | 2013-06-09 | 2018-06-05 | Apple Inc. | Display screen or portion thereof with graphical user interface |
USD744529S1 (en) | 2013-06-09 | 2015-12-01 | Apple Inc. | Display screen or portion thereof with icon |
USD738889S1 (en) | 2013-06-09 | 2015-09-15 | Apple Inc. | Display screen or portion thereof with animated graphical user interface |
US9063636B2 (en) | 2013-06-10 | 2015-06-23 | International Business Machines Corporation | Management of input methods |
USD739873S1 (en) * | 2013-06-10 | 2015-09-29 | Huawei Technologies Co., Ltd. | Display screen with icon |
US9728184B2 (en) | 2013-06-18 | 2017-08-08 | Microsoft Technology Licensing, Llc | Restructuring deep neural network acoustic models |
US9589565B2 (en) | 2013-06-21 | 2017-03-07 | Microsoft Technology Licensing, Llc | Environmentally aware dialog policies and response generation |
US9311298B2 (en) | 2013-06-21 | 2016-04-12 | Microsoft Technology Licensing, Llc | Building conversational understanding systems using a toolset |
US9640182B2 (en) * | 2013-07-01 | 2017-05-02 | Toyota Motor Engineering & Manufacturing North America, Inc. | Systems and vehicles that provide speech recognition system notifications |
US9646606B2 (en) | 2013-07-03 | 2017-05-09 | Google Inc. | Speech recognition using domain knowledge |
WO2015018055A1 (en) | 2013-08-09 | 2015-02-12 | Microsoft Corporation | Input method editor providing language assistance |
CN112989840A (en) * | 2013-08-30 | 2021-06-18 | 英特尔公司 | Extensible context-aware natural language interaction for virtual personal assistants |
USD748140S1 (en) * | 2013-09-03 | 2016-01-26 | Samsung Electronics Co., Ltd. | Display screen portion with icon |
KR102065409B1 (en) * | 2013-09-04 | 2020-01-13 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
USD746831S1 (en) | 2013-09-10 | 2016-01-05 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US9229680B2 (en) * | 2013-09-20 | 2016-01-05 | Oracle International Corporation | Enhanced voice command of computing devices |
US10296160B2 (en) | 2013-12-06 | 2019-05-21 | Apple Inc. | Method for extracting salient dialog usage from live data |
US10873616B1 (en) | 2013-12-10 | 2020-12-22 | Google Llc | Providing content to co-located devices with enhanced presentation characteristics |
US9767803B1 (en) | 2013-12-16 | 2017-09-19 | Aftershock Services, Inc. | Dynamically selecting speech functionality on client devices |
JP6543460B2 (en) * | 2013-12-18 | 2019-07-10 | ハーマン インターナショナル インダストリーズ インコーポレイテッド | Voice recognition inquiry response system |
US9324321B2 (en) | 2014-03-07 | 2016-04-26 | Microsoft Technology Licensing, Llc | Low-footprint adaptation and personalization for a deep neural network |
US20150261968A1 (en) * | 2014-03-12 | 2015-09-17 | Ebay Inc. | Visualizing random characters for keyboard-less devices |
USD758442S1 (en) * | 2014-03-26 | 2016-06-07 | Samsung Electronics Co., Ltd. | Display screen or portion thereof with graphical user interface |
US9529794B2 (en) | 2014-03-27 | 2016-12-27 | Microsoft Technology Licensing, Llc | Flexible schema for language model customization |
US9614724B2 (en) | 2014-04-21 | 2017-04-04 | Microsoft Technology Licensing, Llc | Session-based device configuration |
US9520127B2 (en) | 2014-04-29 | 2016-12-13 | Microsoft Technology Licensing, Llc | Shared hidden layer combination for speech recognition systems |
US9384334B2 (en) | 2014-05-12 | 2016-07-05 | Microsoft Technology Licensing, Llc | Content discovery in managed wireless distribution networks |
US9430667B2 (en) | 2014-05-12 | 2016-08-30 | Microsoft Technology Licensing, Llc | Managed wireless distribution network |
US9384335B2 (en) | 2014-05-12 | 2016-07-05 | Microsoft Technology Licensing, Llc | Content delivery prioritization in managed wireless distribution networks |
US10111099B2 (en) | 2014-05-12 | 2018-10-23 | Microsoft Technology Licensing, Llc | Distributing content in managed wireless distribution networks |
US9874914B2 (en) | 2014-05-19 | 2018-01-23 | Microsoft Technology Licensing, Llc | Power management contracts for accessory devices |
USD772257S1 (en) * | 2014-05-28 | 2016-11-22 | Ricoh Company, Ltd. | Display screen with graphical user interface |
US9437189B2 (en) * | 2014-05-29 | 2016-09-06 | Google Inc. | Generating language models |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
USD771112S1 (en) | 2014-06-01 | 2016-11-08 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US10037202B2 (en) | 2014-06-03 | 2018-07-31 | Microsoft Technology Licensing, Llc | Techniques to isolating a portion of an online computing service |
US9509799B1 (en) | 2014-06-04 | 2016-11-29 | Grandios Technologies, Llc | Providing status updates via a personal assistant |
US8995972B1 (en) | 2014-06-05 | 2015-03-31 | Grandios Technologies, Llc | Automatic personal assistance between users devices |
US10140981B1 (en) * | 2014-06-10 | 2018-11-27 | Amazon Technologies, Inc. | Dynamic arc weights in speech recognition models |
US9367490B2 (en) | 2014-06-13 | 2016-06-14 | Microsoft Technology Licensing, Llc | Reversible connector for accessory devices |
US9717006B2 (en) | 2014-06-23 | 2017-07-25 | Microsoft Technology Licensing, Llc | Device quarantine in a wireless network |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
USD753696S1 (en) | 2014-09-01 | 2016-04-12 | Apple Inc. | Display screen or portion thereof with graphical user interface |
USD765114S1 (en) | 2014-09-02 | 2016-08-30 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US20170294420A1 (en) * | 2014-09-02 | 2017-10-12 | Philips Lighting Holding B.V. | A method of applying a lighting arrangement to a surface and a lighting surface |
US9953646B2 (en) | 2014-09-02 | 2018-04-24 | Belleau Technologies | Method and system for dynamic speech recognition and tracking of prewritten script |
USD753697S1 (en) | 2014-09-02 | 2016-04-12 | Apple Inc. | Display screen or portion thereof with graphical user interface |
USD791143S1 (en) * | 2014-09-03 | 2017-07-04 | Apple Inc. | Display screen or portion thereof with graphical user interface |
USD767595S1 (en) * | 2014-09-03 | 2016-09-27 | Apple Inc. | Display screen or portion thereof with graphical user interface |
CN105469793A (en) * | 2014-09-11 | 2016-04-06 | 苗码信息科技(上海)股份有限公司 | Full-automatic onsite driving control method via foreign language speech |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
CN105469792A (en) * | 2014-09-11 | 2016-04-06 | 苗码信息科技(上海)股份有限公司 | Remote automatic navigating and driving automobile system via speaker-independent foreign language speech |
US9606986B2 (en) * | 2014-09-29 | 2017-03-28 | Apple Inc. | Integrated word N-gram and class M-gram language models |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US20160094491A1 (en) * | 2014-09-30 | 2016-03-31 | Genesys Telecommunications Laboratories, Inc. | Pattern-controlled automated messaging system |
US9502032B2 (en) | 2014-10-08 | 2016-11-22 | Google Inc. | Dynamically biasing language models |
CN104462262B (en) * | 2014-11-21 | 2017-10-31 | 北京奇虎科技有限公司 | A kind of method for realizing phonetic search, device and browser client |
CN105827878B (en) * | 2015-01-04 | 2019-06-25 | 中国移动通信集团公司 | Voice messaging conversion method and voice transfer gateway |
RU2646350C2 (en) * | 2015-01-27 | 2018-03-02 | Общество С Ограниченной Ответственностью "Яндекс" | Method of entering data to electronic device, method of processing voice request, machine-readable media (options), electronic device, server and system |
US10152299B2 (en) | 2015-03-06 | 2018-12-11 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
USD771670S1 (en) | 2015-03-09 | 2016-11-15 | Apple Inc. | Display screen or portion thereof with animated graphical user interface |
US10460227B2 (en) | 2015-05-15 | 2019-10-29 | Apple Inc. | Virtual assistant in a communication session |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US9966073B2 (en) * | 2015-05-27 | 2018-05-08 | Google Llc | Context-sensitive dynamic update of voice to text model in a voice-enabled electronic device |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
KR102394202B1 (en) * | 2015-05-29 | 2022-05-04 | 삼성전자주식회사 | Method for processing input between devices and electronic device thereof |
USD760746S1 (en) | 2015-06-04 | 2016-07-05 | Apple Inc. | Display screen or portion thereof with animated graphical user interface |
US9578173B2 (en) | 2015-06-05 | 2017-02-21 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US10403271B2 (en) | 2015-06-11 | 2019-09-03 | Nice Ltd. | System and method for automatic language model selection |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
US9704483B2 (en) * | 2015-07-28 | 2017-07-11 | Google Inc. | Collaborative language model biasing |
US9576578B1 (en) * | 2015-08-12 | 2017-02-21 | Google Inc. | Contextual improvement of voice query recognition |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US9858923B2 (en) * | 2015-09-24 | 2018-01-02 | Intel Corporation | Dynamic adaptation of language models and semantic tracking for automatic speech recognition |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US9946437B2 (en) * | 2015-11-05 | 2018-04-17 | International Business Machines Corporation | Modifying an appearance of a GUI to improve GUI usability |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10468016B2 (en) | 2015-11-24 | 2019-11-05 | International Business Machines Corporation | System and method for supporting automatic speech recognition of regional accents based on statistical information and user corrections |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10896681B2 (en) * | 2015-12-29 | 2021-01-19 | Google Llc | Speech recognition with selective use of dynamic language models |
US10049744B2 (en) | 2016-01-08 | 2018-08-14 | Samsung Electronics Co., Ltd. | Three-dimensional (3D) semiconductor memory devices and methods of manufacturing the same |
US20170206899A1 (en) * | 2016-01-20 | 2017-07-20 | Fitbit, Inc. | Better communication channel for requests and responses having an intelligent agent |
US20170235724A1 (en) * | 2016-02-11 | 2017-08-17 | Emily Grewal | Systems and methods for generating personalized language models and translation using the same |
US10282417B2 (en) | 2016-02-19 | 2019-05-07 | International Business Machines Corporation | Conversational list management |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
DK201670539A1 (en) * | 2016-03-14 | 2017-10-02 | Apple Inc | Dictation that allows editing |
US9978367B2 (en) | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
US20170308289A1 (en) * | 2016-04-20 | 2017-10-26 | Google Inc. | Iconographic symbol search within a graphical keyboard |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
DK179309B1 (en) | 2016-06-09 | 2018-04-23 | Apple Inc | Intelligent automated assistant in a home environment |
USD815649S1 (en) | 2016-06-10 | 2018-04-17 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
DK179343B1 (en) | 2016-06-11 | 2018-05-14 | Apple Inc | Intelligent task discovery |
DK179049B1 (en) | 2016-06-11 | 2017-09-18 | Apple Inc | Data driven natural language event detection and classification |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
USD804502S1 (en) | 2016-06-11 | 2017-12-05 | Apple Inc. | Display screen or portion thereof with graphical user interface |
USD798893S1 (en) | 2016-06-11 | 2017-10-03 | Apple Inc. | Display screen or portion thereof with animated graphical user interface |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
US10409488B2 (en) * | 2016-06-13 | 2019-09-10 | Microsoft Technology Licensing, Llc | Intelligent virtual keyboards |
US10937415B2 (en) * | 2016-06-15 | 2021-03-02 | Sony Corporation | Information processing device and information processing method for presenting character information obtained by converting a voice |
CN109313895A (en) | 2016-06-23 | 2019-02-05 | 索尼公司 | Information processing unit and information processing method |
US10481863B2 (en) * | 2016-07-06 | 2019-11-19 | Baidu Usa Llc | Systems and methods for improved user interface |
USD817337S1 (en) | 2016-07-07 | 2018-05-08 | Baidu Usa Llc | Display screen or portion thereof with graphical user interface |
USD812635S1 (en) * | 2016-07-07 | 2018-03-13 | Baidu Usa Llc. | Display screen or portion thereof with graphical user interface |
USD815110S1 (en) | 2016-07-07 | 2018-04-10 | Baidu Usa Llc | Display screen or portion thereof with graphical user interface |
US9691384B1 (en) * | 2016-08-19 | 2017-06-27 | Google Inc. | Voice action biasing system |
US10832664B2 (en) | 2016-08-19 | 2020-11-10 | Google Llc | Automated speech recognition using language models that selectively use domain-specific model components |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10366918B2 (en) * | 2016-10-04 | 2019-07-30 | International Business Machines Corporation | Self-aligned trench metal-alloying for III-V nFETs |
US9959864B1 (en) | 2016-10-27 | 2018-05-01 | Google Llc | Location-based voice query recognition |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10831366B2 (en) | 2016-12-29 | 2020-11-10 | Google Llc | Modality learning on mobile devices |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10517975B2 (en) * | 2017-01-12 | 2019-12-31 | Industrial Technology Research Institute | Light source apparatus and method of using the same |
US10741174B2 (en) * | 2017-01-24 | 2020-08-11 | Lenovo (Singapore) Pte. Ltd. | Automatic language identification for speech |
US10268669B1 (en) * | 2017-01-27 | 2019-04-23 | John C. Allen | Intelligent graphical word processing system and method |
US11010784B2 (en) | 2017-01-31 | 2021-05-18 | Walmart Apollo, Llc | Systems and methods for search query refinement |
US10628458B2 (en) * | 2017-01-31 | 2020-04-21 | Walmart Apollo, Llc | Systems and methods for automated recommendations |
US10554779B2 (en) | 2017-01-31 | 2020-02-04 | Walmart Apollo, Llc | Systems and methods for webpage personalization |
US11609964B2 (en) | 2017-01-31 | 2023-03-21 | Walmart Apollo, Llc | Whole page personalization with cyclic dependencies |
US10592577B2 (en) | 2017-01-31 | 2020-03-17 | Walmart Apollo, Llc | Systems and methods for updating a webpage |
US10311860B2 (en) | 2017-02-14 | 2019-06-04 | Google Llc | Language model biasing system |
USD866575S1 (en) * | 2017-03-20 | 2019-11-12 | Exfo Inc. | Display screen, or portion thereof, with graphical user interface for multi-fiber connector, fiber inspection probe testing |
KR102398649B1 (en) * | 2017-03-28 | 2022-05-17 | 삼성전자주식회사 | Electronic device for processing user utterance and method for operation thereof |
US20180316634A1 (en) * | 2017-04-26 | 2018-11-01 | Microsoft Technology Licensing, Llc | Extending application functionality via conversational interfaces |
KR102380717B1 (en) * | 2017-04-30 | 2022-03-31 | 삼성전자주식회사 | Electronic apparatus for processing user utterance and controlling method thereof |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK201770439A1 (en) | 2017-05-11 | 2018-12-13 | Apple Inc. | Offline personal assistant |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK201770432A1 (en) | 2017-05-15 | 2018-12-21 | Apple Inc. | Hierarchical belief states for digital assistants |
DK201770431A1 (en) | 2017-05-15 | 2018-12-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
DK179560B1 (en) | 2017-05-16 | 2019-02-18 | Apple Inc. | Far-field extension for digital assistant services |
WO2018213415A1 (en) * | 2017-05-16 | 2018-11-22 | Apple Inc. | Far-field extension for digital assistant services |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10616036B2 (en) * | 2017-06-07 | 2020-04-07 | Accenture Global Solutions Limited | Integration platform for multi-network integration of service platforms |
US11900072B1 (en) * | 2017-07-18 | 2024-02-13 | Amazon Technologies, Inc. | Quick lookup for speech translation |
CN109471537A (en) * | 2017-09-08 | 2019-03-15 | 腾讯科技（深圳）有限公司 | Pronunciation inputting method, device, computer equipment and storage medium |
USD843442S1 (en) | 2017-09-10 | 2019-03-19 | Apple Inc. | Type font |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10395647B2 (en) * | 2017-10-26 | 2019-08-27 | Harman International Industries, Incorporated | System and method for natural language processing |
US10621282B1 (en) * | 2017-10-27 | 2020-04-14 | Interactions Llc | Accelerating agent performance in a natural language processing system |
CN107808007A (en) * | 2017-11-16 | 2018-03-16 | 百度在线网络技术（北京）有限公司 | Information processing method and device |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US20190214013A1 (en) * | 2018-01-05 | 2019-07-11 | Ca, Inc. | Speech-to-text conversion based on user interface state awareness |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
JP7056185B2 (en) * | 2018-01-31 | 2022-04-19 | トヨタ自動車株式会社 | Information processing equipment and information processing method |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
WO2019161229A1 (en) | 2018-02-15 | 2019-08-22 | DMAI, Inc. | System and method for reconstructing unoccupied 3d space |
WO2019161193A2 (en) * | 2018-02-15 | 2019-08-22 | DMAI, Inc. | System and method for adaptive detection of spoken language via multiple speech models |
WO2019161207A1 (en) | 2018-02-15 | 2019-08-22 | DMAI, Inc. | System and method for conversational agent via adaptive caching of dialogue tree |
DE112018007127T5 (en) * | 2018-02-20 | 2020-11-05 | Lg Electronics Inc. | Display device |
US10832657B2 (en) * | 2018-03-01 | 2020-11-10 | International Business Machines Corporation | Use of small unit language model for training large unit language models |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US11106868B2 (en) * | 2018-03-06 | 2021-08-31 | Samsung Electronics Co., Ltd. | System and method for language model personalization |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10623246B1 (en) * | 2018-03-27 | 2020-04-14 | Amazon Technologies, Inc. | Device configuration by natural language processing system |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10782986B2 (en) | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
US11886473B2 (en) | 2018-04-20 | 2024-01-30 | Meta Platforms, Inc. | Intent identification for agent matching by assistant systems |
US11676220B2 (en) | 2018-04-20 | 2023-06-13 | Meta Platforms, Inc. | Processing multimodal user input for assistant systems |
US11715042B1 (en) | 2018-04-20 | 2023-08-01 | Meta Platforms Technologies, Llc | Interpretability of deep reinforcement learning models in assistant systems |
US11307880B2 (en) | 2018-04-20 | 2022-04-19 | Meta Platforms, Inc. | Assisting users with personalized and contextual communication content |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK201870355A1 (en) | 2018-06-01 | 2019-12-16 | Apple Inc. | Virtual assistant operation in multi-device environments |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
USD863337S1 (en) | 2018-06-03 | 2019-10-15 | Apple Inc. | Electronic device with animated graphical user interface |
US10832678B2 (en) | 2018-06-08 | 2020-11-10 | International Business Machines Corporation | Filtering audio-based interference from voice commands using interference information |
USD900830S1 (en) | 2018-09-10 | 2020-11-03 | Apple Inc. | Electronic device with graphical user interface |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
KR20210052563A (en) * | 2018-11-02 | 2021-05-10 | 주식회사 엘솔루 | Method and apparatus for providing context-based voice recognition service |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US10949224B2 (en) | 2019-01-29 | 2021-03-16 | Walmart Apollo Llc | Systems and methods for altering a GUI in response to in-session inferences |
USD902221S1 (en) | 2019-02-01 | 2020-11-17 | Apple Inc. | Electronic device with animated graphical user interface |
USD900871S1 (en) | 2019-02-04 | 2020-11-03 | Apple Inc. | Electronic device with animated graphical user interface |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
KR20200113349A (en) * | 2019-03-25 | 2020-10-07 | 삼성전자주식회사 | Electronic Device and the Method for Supporting Multitasking thereof |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
KR20210016739A (en) | 2019-08-05 | 2021-02-17 | 삼성전자주식회사 | Electronic device and input method of the same |
USD924912S1 (en) | 2019-09-09 | 2021-07-13 | Apple Inc. | Display screen or portion thereof with graphical user interface |
US11568146B2 (en) | 2019-09-10 | 2023-01-31 | Google Llc | Location-based mode(s) for biasing provisioning of content when an automated assistant is responding to condensed natural language inputs |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11308265B1 (en) * | 2019-10-11 | 2022-04-19 | Wells Fargo Bank, N.A. | Digitally aware neural dictation interface |
KR20220010034A (en) | 2019-10-15 | 2022-01-25 | 구글 엘엘씨 | Enter voice-controlled content into a graphical user interface |
KR20210055387A (en) | 2019-11-07 | 2021-05-17 | 삼성전자주식회사 | Context based application providing server and controlling method thereof |
US11675842B1 (en) * | 2020-02-03 | 2023-06-13 | Amazon Technologies, Inc. | Generation of recommendation results using a verbal query |
CN113555009A (en) * | 2020-04-21 | 2021-10-26 | 京东数字科技控股有限公司 | Method and apparatus for training a model |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11810578B2 (en) | 2020-05-11 | 2023-11-07 | Apple Inc. | Device arbitration for digital assistant-based intercom systems |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
CN114205665B (en) | 2020-06-09 | 2023-05-09 | 抖音视界有限公司 | Information processing method, device, electronic equipment and storage medium |
CN111738023A (en) * | 2020-06-24 | 2020-10-02 | 宋万利 | Automatic image-text audio translation method and system |
US20220215833A1 (en) * | 2021-01-07 | 2022-07-07 | Lenovo (Singapore) Pte. Ltd. | Method and device for converting spoken words to text form |
CN113066480B (en) * | 2021-03-26 | 2023-02-17 | 北京达佳互联信息技术有限公司 | Voice recognition method and device, electronic equipment and storage medium |
US11967306B2 (en) | 2021-04-14 | 2024-04-23 | Honeywell International Inc. | Contextual speech recognition methods and systems |
US11810558B2 (en) * | 2021-05-26 | 2023-11-07 | International Business Machines Corporation | Explaining anomalous phonetic translations |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040243415A1 (en) * | 2003-06-02 | 2004-12-02 | International Business Machines Corporation | Architecture for a speech input method editor for handheld portable devices |
Family Cites Families (226)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4820059A (en) * | 1985-10-30 | 1989-04-11 | Central Institute For The Deaf | Speech processing apparatus and methods |
US5477451A (en) * | 1991-07-25 | 1995-12-19 | International Business Machines Corp. | Method and system for natural language translation |
US5267345A (en) | 1992-02-10 | 1993-11-30 | International Business Machines Corporation | Speech recognition apparatus which predicts word classes from context and words from word classes |
DE69326431T2 (en) * | 1992-12-28 | 2000-02-03 | Toshiba Kawasaki Kk | Voice recognition interface system that can be used as a window system and voice mail system |
TW323364B (en) * | 1993-11-24 | 1997-12-21 | At & T Corp | |
US5638487A (en) * | 1994-12-30 | 1997-06-10 | Purespeech, Inc. | Automatic speech recognition |
US5715367A (en) * | 1995-01-23 | 1998-02-03 | Dragon Systems, Inc. | Apparatuses and methods for developing and using models for speech recognition |
DE19533541C1 (en) * | 1995-09-11 | 1997-03-27 | Daimler Benz Aerospace Ag | Method for the automatic control of one or more devices by voice commands or by voice dialog in real time and device for executing the method |
US6397180B1 (en) * | 1996-05-22 | 2002-05-28 | Qwest Communications International Inc. | Method and system for performing speech recognition based on best-word scoring of repeated speech attempts |
US5819225A (en) * | 1996-05-30 | 1998-10-06 | International Business Machines Corporation | Display indications of speech processing states in speech recognition system |
US6021403A (en) * | 1996-07-19 | 2000-02-01 | Microsoft Corporation | Intelligent user assistance facility |
US5822730A (en) | 1996-08-22 | 1998-10-13 | Dragon Systems, Inc. | Lexical tree pre-filtering in speech recognition |
US6167377A (en) | 1997-03-28 | 2000-12-26 | Dragon Systems, Inc. | Speech recognition language models |
US6119186A (en) | 1997-05-30 | 2000-09-12 | Texas Instruments Incorporated | Computer system with environmental manager for detecting and responding to changing environmental conditions |
JP2001507482A (en) * | 1997-10-08 | 2001-06-05 | コーニンクレッカ フィリップス エレクトロニクス エヌ ヴィ | Vocabulary and / or language model training |
US6182038B1 (en) * | 1997-12-01 | 2001-01-30 | Motorola, Inc. | Context dependent phoneme networks for encoding speech information |
US6317712B1 (en) | 1998-02-03 | 2001-11-13 | Texas Instruments Incorporated | Method of phonetic modeling using acoustic decision tree |
AU2981099A (en) * | 1998-03-09 | 1999-09-27 | Trustees Of Tufts College | Treatment of compulsive behaviours in man and animals |
US6418431B1 (en) | 1998-03-30 | 2002-07-09 | Microsoft Corporation | Information retrieval and speech recognition based on language models |
US7137126B1 (en) * | 1998-10-02 | 2006-11-14 | International Business Machines Corporation | Conversational computing via conversational virtual machine |
US6356866B1 (en) * | 1998-10-07 | 2002-03-12 | Microsoft Corporation | Method for converting a phonetic character string into the text of an Asian language |
US7881936B2 (en) * | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
US8938688B2 (en) | 1998-12-04 | 2015-01-20 | Nuance Communications, Inc. | Contextual prediction of user words and user actions |
US6922669B2 (en) | 1998-12-29 | 2005-07-26 | Koninklijke Philips Electronics N.V. | Knowledge-based strategies applied to N-best lists in automatic speech recognition systems |
US6851115B1 (en) | 1999-01-05 | 2005-02-01 | Sri International | Software-based architecture for communication and cooperation among distributed electronic agents |
US7030863B2 (en) | 2000-05-26 | 2006-04-18 | America Online, Incorporated | Virtual keyboard system with automatic correction |
US6912499B1 (en) * | 1999-08-31 | 2005-06-28 | Nortel Networks Limited | Method and apparatus for training a multilingual speech model set |
JP4292646B2 (en) | 1999-09-16 | 2009-07-08 | 株式会社デンソー | User interface device, navigation system, information processing device, and recording medium |
FI112978B (en) * | 1999-09-17 | 2004-02-13 | Nokia Corp | Entering Symbols |
US6789231B1 (en) * | 1999-10-05 | 2004-09-07 | Microsoft Corporation | Method and system for providing alternatives for text derived from stochastic input sources |
US6581033B1 (en) * | 1999-10-19 | 2003-06-17 | Microsoft Corporation | System and method for correction of speech recognition mode errors |
US6778959B1 (en) | 1999-10-21 | 2004-08-17 | Sony Corporation | System and method for speech verification using out-of-vocabulary models |
US6446041B1 (en) | 1999-10-27 | 2002-09-03 | Microsoft Corporation | Method and system for providing audio playback of a multi-source document |
US20020111990A1 (en) | 1999-11-01 | 2002-08-15 | Wood Christopher Noah | Internet based message management system |
US7403888B1 (en) * | 1999-11-05 | 2008-07-22 | Microsoft Corporation | Language input user interface |
US20020055844A1 (en) * | 2000-02-25 | 2002-05-09 | L'esperance Lauren | Speech user interface for portable personal devices |
US7203731B1 (en) * | 2000-03-03 | 2007-04-10 | Intel Corporation | Dynamic replication of files in a network storage system |
AU4869601A (en) * | 2000-03-20 | 2001-10-03 | Robert J. Freeman | Natural-language processing system using a large corpus |
US7107204B1 (en) * | 2000-04-24 | 2006-09-12 | Microsoft Corporation | Computer-aided writing system and method with cross-language writing wizard |
US6678415B1 (en) * | 2000-05-12 | 2004-01-13 | Xerox Corporation | Document image decoding using an integrated stochastic language model |
US6539358B1 (en) * | 2000-05-24 | 2003-03-25 | Delphi Technologies, Inc. | Voice-interactive docking station for a portable computing device |
US7149970B1 (en) | 2000-06-23 | 2006-12-12 | Microsoft Corporation | Method and system for filtering and selecting from a candidate list generated by a stochastic input method |
US7623648B1 (en) * | 2004-12-01 | 2009-11-24 | Tellme Networks, Inc. | Method and system of generating reference variations for directory assistance data |
US7219058B1 (en) | 2000-10-13 | 2007-05-15 | At&T Corp. | System and method for processing speech recognition results |
US7457750B2 (en) | 2000-10-13 | 2008-11-25 | At&T Corp. | Systems and methods for dynamic re-configurable speech recognition |
US7043422B2 (en) * | 2000-10-13 | 2006-05-09 | Microsoft Corporation | Method and apparatus for distribution-based language model adaptation |
US6876966B1 (en) * | 2000-10-16 | 2005-04-05 | Microsoft Corporation | Pattern recognition training method and apparatus using inserted noise followed by noise reduction |
ATE297588T1 (en) * | 2000-11-14 | 2005-06-15 | Ibm | ADJUSTING PHONETIC CONTEXT TO IMPROVE SPEECH RECOGNITION |
US7600014B2 (en) | 2000-11-16 | 2009-10-06 | Symantec Corporation | Method and system for monitoring the performance of a distributed application |
ATE391986T1 (en) * | 2000-11-23 | 2008-04-15 | Ibm | VOICE NAVIGATION IN WEB APPLICATIONS |
US6915262B2 (en) | 2000-11-30 | 2005-07-05 | Telesector Resources Group, Inc. | Methods and apparatus for performing speech recognition and using speech recognition results |
JP2002197118A (en) * | 2000-12-15 | 2002-07-12 | Internatl Business Mach Corp <Ibm> | Information access method, information access system and storage medium |
US20020087325A1 (en) * | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Dialogue application computer platform |
US20020087315A1 (en) * | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Computer-implemented multi-scanning language method and system |
US20020087309A1 (en) | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Computer-implemented speech expectation-based probability method and system |
DE10100725C1 (en) | 2001-01-10 | 2002-01-24 | Philips Corp Intellectual Pty | Automatic dialogue system for speech interrogation of databank entries uses speech recognition system assisted by speech model obtained before beginning of dialogue |
US7027987B1 (en) * | 2001-02-07 | 2006-04-11 | Google Inc. | Voice interface for a search engine |
US7076738B2 (en) * | 2001-03-02 | 2006-07-11 | Semantic Compaction Systems | Computer device, method and article of manufacture for utilizing sequenced symbols to enable programmed application and commands |
US7778816B2 (en) | 2001-04-24 | 2010-08-17 | Microsoft Corporation | Method and system for applying input mode bias |
US7203645B2 (en) * | 2001-04-27 | 2007-04-10 | Intel Corporation | Speech recognition system loading different recognition engines for different applications |
US6714778B2 (en) * | 2001-05-15 | 2004-03-30 | Nokia Corporation | Context sensitive web services |
US20030008680A1 (en) | 2001-05-24 | 2003-01-09 | Huh Stephen S. | Using identification information obtained from a portable phone |
US7225130B2 (en) * | 2001-09-05 | 2007-05-29 | Voice Signal Technologies, Inc. | Methods, systems, and programming for performing speech recognition |
US7526431B2 (en) * | 2001-09-05 | 2009-04-28 | Voice Signal Technologies, Inc. | Speech recognition using ambiguous or phone key spelling and/or filtering |
US6901364B2 (en) * | 2001-09-13 | 2005-05-31 | Matsushita Electric Industrial Co., Ltd. | Focused language models for improved speech input of structured documents |
US6959276B2 (en) | 2001-09-27 | 2005-10-25 | Microsoft Corporation | Including the category of environmental noise when processing speech signals |
US6950796B2 (en) | 2001-11-05 | 2005-09-27 | Motorola, Inc. | Speech recognition by dynamical noise model adaptation |
US7610556B2 (en) * | 2001-12-28 | 2009-10-27 | Microsoft Corporation | Dialog manager for interactive dialog with computer user |
US6999931B2 (en) | 2002-02-01 | 2006-02-14 | Intel Corporation | Spoken dialog system using a best-fit language model and best-fit grammar |
US7149694B1 (en) | 2002-02-13 | 2006-12-12 | Siebel Systems, Inc. | Method and system for building/updating grammars in voice access systems |
US8213917B2 (en) * | 2006-05-05 | 2012-07-03 | Waloomba Tech Ltd., L.L.C. | Reusable multimodal application |
US7174288B2 (en) * | 2002-05-08 | 2007-02-06 | Microsoft Corporation | Multi-modal entry of ideogrammatic languages |
US7403890B2 (en) | 2002-05-13 | 2008-07-22 | Roushar Joseph C | Multi-dimensional method and apparatus for automated language interpretation |
US7224981B2 (en) | 2002-06-20 | 2007-05-29 | Intel Corporation | Speech recognition of mobile devices |
DE10230983A1 (en) * | 2002-07-10 | 2004-01-22 | Bayer Ag | Multi-layer product containing polycarbonate |
US7570943B2 (en) * | 2002-08-29 | 2009-08-04 | Nokia Corporation | System and method for providing context sensitive recommendations to digital services |
JP4109063B2 (en) | 2002-09-18 | 2008-06-25 | パイオニア株式会社 | Speech recognition apparatus and speech recognition method |
US7328155B2 (en) | 2002-09-25 | 2008-02-05 | Toyota Infotechnology Center Co., Ltd. | Method and system for speech recognition using grammar weighted based upon location information |
JP4352790B2 (en) | 2002-10-31 | 2009-10-28 | セイコーエプソン株式会社 | Acoustic model creation method, speech recognition device, and vehicle having speech recognition device |
US7149688B2 (en) | 2002-11-04 | 2006-12-12 | Speechworks International, Inc. | Multi-lingual speech recognition with cross-language context modeling |
US6993615B2 (en) * | 2002-11-15 | 2006-01-31 | Microsoft Corporation | Portable computing device-integrated appliance |
US7457745B2 (en) | 2002-12-03 | 2008-11-25 | Hrl Laboratories, Llc | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
WO2004053836A1 (en) | 2002-12-10 | 2004-06-24 | Kirusa, Inc. | Techniques for disambiguating speech input using multimodal interfaces |
US7373300B1 (en) * | 2002-12-18 | 2008-05-13 | At&T Corp. | System and method of providing a spoken dialog interface to a website |
DE60231844D1 (en) * | 2002-12-20 | 2009-05-14 | Nokia Corp | NEW RELEASE INFORMATION WITH META INFORMATION |
WO2004061796A1 (en) * | 2002-12-31 | 2004-07-22 | Burlingtonspeech Limited | Comprehensive spoken language learning system |
US7698136B1 (en) * | 2003-01-28 | 2010-04-13 | Voxify, Inc. | Methods and apparatus for flexible speech recognition |
US7805299B2 (en) | 2004-03-01 | 2010-09-28 | Coifman Robert E | Method and apparatus for improving the transcription accuracy of speech recognition software |
CA2428821C (en) * | 2003-05-15 | 2009-03-17 | Ibm Canada Limited - Ibm Canada Limitee | Accessing a platform independent input method editor from an underlying operating system |
US7392188B2 (en) * | 2003-07-31 | 2008-06-24 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method enabling acoustic barge-in |
JP4548646B2 (en) | 2003-09-12 | 2010-09-22 | 株式会社エヌ・ティ・ティ・ドコモ | Noise model noise adaptation system, noise adaptation method, and speech recognition noise adaptation program |
US7634720B2 (en) * | 2003-10-24 | 2009-12-15 | Microsoft Corporation | System and method for providing context to an input method |
FI20031566A (en) * | 2003-10-27 | 2005-04-28 | Nokia Corp | Select a language for word recognition |
CA2486128C (en) | 2003-10-30 | 2011-08-23 | At&T Corp. | System and method for using meta-data dependent language modeling for automatic speech recognition |
US20050114474A1 (en) * | 2003-11-20 | 2005-05-26 | International Business Machines Corporation | Automatic configuration of the network devices via connection to specific switch ports |
US7634095B2 (en) | 2004-02-23 | 2009-12-15 | General Motors Company | Dynamic tuning of hands-free algorithm for noise and driving conditions |
US7400878B2 (en) | 2004-02-26 | 2008-07-15 | Research In Motion Limited | Computing device with environment aware features |
US7696136B2 (en) * | 2004-03-11 | 2010-04-13 | Crompton Corporation | Lubricant compositions containing hydroxy carboxylic acid and hydroxy polycarboxylic acid esters |
US7624018B2 (en) | 2004-03-12 | 2009-11-24 | Microsoft Corporation | Speech recognition using categories and speech prefixing |
US20050246325A1 (en) | 2004-04-30 | 2005-11-03 | Microsoft Corporation | Method and system for recording and accessing usage of an item in a computer system |
WO2005122145A1 (en) * | 2004-06-08 | 2005-12-22 | Metaphor Solutions, Inc. | Speech recognition dialog management |
JP3923513B2 (en) | 2004-06-08 | 2007-06-06 | 松下電器産業株式会社 | Speech recognition apparatus and speech recognition method |
US7299181B2 (en) * | 2004-06-30 | 2007-11-20 | Microsoft Corporation | Homonym processing in the context of voice-activated command systems |
US7562069B1 (en) * | 2004-07-01 | 2009-07-14 | Aol Llc | Query disambiguation |
US20060009974A1 (en) * | 2004-07-09 | 2006-01-12 | Matsushita Electric Industrial Co., Ltd. | Hands-free voice dialing for portable and remote devices |
US7580363B2 (en) * | 2004-08-16 | 2009-08-25 | Nokia Corporation | Apparatus and method for facilitating contact selection in communication devices |
US20060048055A1 (en) * | 2004-08-25 | 2006-03-02 | Jun Wu | Fault-tolerant romanized input method for non-roman characters |
US7698124B2 (en) * | 2004-11-04 | 2010-04-13 | Microsoft Corporaiton | Machine translation system incorporating syntactic dependency treelets into a statistical framework |
US7418387B2 (en) * | 2004-11-24 | 2008-08-26 | Microsoft Corporation | Generic spelling mnemonics |
JP3955880B2 (en) | 2004-11-30 | 2007-08-08 | 松下電器産業株式会社 | Voice recognition device |
US7409344B2 (en) | 2005-03-08 | 2008-08-05 | Sap Aktiengesellschaft | XML based architecture for controlling user interfaces with contextual voice commands |
US8009678B2 (en) | 2005-03-17 | 2011-08-30 | Microsoft Corporation | System and method for generating a dynamic prioritized contact list |
US7739286B2 (en) | 2005-03-17 | 2010-06-15 | University Of Southern California | Topic specific language models built from large numbers of documents |
US20060277466A1 (en) * | 2005-05-13 | 2006-12-07 | Anderson Thomas G | Bimodal user interaction with a simulated object |
ATE550756T1 (en) | 2005-08-04 | 2012-04-15 | Nuance Communications Inc | VOICE DIALOGUE SYSTEM |
US20070060114A1 (en) * | 2005-09-14 | 2007-03-15 | Jorey Ramer | Predictive text completion for a mobile communication facility |
US7672833B2 (en) * | 2005-09-22 | 2010-03-02 | Fair Isaac Corporation | Method and apparatus for automatic entity disambiguation |
US7895193B2 (en) * | 2005-09-30 | 2011-02-22 | Microsoft Corporation | Arbitration of specialized content using search results |
US8620667B2 (en) * | 2005-10-17 | 2013-12-31 | Microsoft Corporation | Flexible speech-activated command and control |
US20070106685A1 (en) * | 2005-11-09 | 2007-05-10 | Podzinger Corp. | Method and apparatus for updating speech recognition databases and reindexing audio and video content using the same |
EP1791114B1 (en) | 2005-11-25 | 2009-08-12 | Swisscom AG | A method for personalization of a service |
US20070124507A1 (en) * | 2005-11-28 | 2007-05-31 | Sap Ag | Systems and methods of processing annotations and multimodal user inputs |
DE102005061365A1 (en) * | 2005-12-21 | 2007-06-28 | Siemens Ag | Background applications e.g. home banking system, controlling method for use over e.g. user interface, involves associating transactions and transaction parameters over universal dialog specification, and universally operating applications |
JP4961755B2 (en) | 2006-01-23 | 2012-06-27 | 富士ゼロックス株式会社 | Word alignment device, word alignment method, word alignment program |
US7818315B2 (en) | 2006-03-13 | 2010-10-19 | Microsoft Corporation | Re-ranking search results based on query log |
US7818279B2 (en) * | 2006-03-13 | 2010-10-19 | Microsoft Corporation | Event detection based on evolution of click-through data |
US8301448B2 (en) | 2006-03-29 | 2012-10-30 | Nuance Communications, Inc. | System and method for applying dynamic contextual grammars and language models to improve automatic speech recognition accuracy |
JP5218052B2 (en) * | 2006-06-26 | 2013-06-26 | 日本電気株式会社 | Language model generation system, language model generation method, and language model generation program |
US20080005067A1 (en) * | 2006-06-28 | 2008-01-03 | Microsoft Corporation | Context-based search, retrieval, and awareness |
US8001130B2 (en) | 2006-07-25 | 2011-08-16 | Microsoft Corporation | Web object retrieval based on a language model |
JP4957110B2 (en) | 2006-08-03 | 2012-06-20 | 日亜化学工業株式会社 | Light emitting device |
US8564544B2 (en) * | 2006-09-06 | 2013-10-22 | Apple Inc. | Touch screen device, method, and graphical user interface for customizing display of content category icons |
US7907705B1 (en) * | 2006-10-10 | 2011-03-15 | Intuit Inc. | Speech to text for assisted form completion |
US8041568B2 (en) * | 2006-10-13 | 2011-10-18 | Google Inc. | Business listing search |
US7890326B2 (en) * | 2006-10-13 | 2011-02-15 | Google Inc. | Business listing search |
US8073681B2 (en) | 2006-10-16 | 2011-12-06 | Voicebox Technologies, Inc. | System and method for a cooperative conversational voice user interface |
WO2008067562A2 (en) * | 2006-11-30 | 2008-06-05 | Rao Ashwin P | Multimodal speech recognition system |
US20080131851A1 (en) * | 2006-12-04 | 2008-06-05 | Dimitri Kanevsky | Context-sensitive language learning |
US7827033B2 (en) | 2006-12-06 | 2010-11-02 | Nuance Communications, Inc. | Enabling grammars in web page frames |
JP4348361B2 (en) * | 2006-12-20 | 2009-10-21 | 株式会社日立製作所 | Distribution system, communication apparatus, and distribution method |
US7856351B2 (en) | 2007-01-19 | 2010-12-21 | Microsoft Corporation | Integrated speech recognition and semantic classification |
US7941189B2 (en) * | 2007-02-07 | 2011-05-10 | Denso Corporation | Communicating road noise control system, in-vehicle road noise controller, and server |
US20100325109A1 (en) | 2007-02-09 | 2010-12-23 | Agency For Science, Technology And Rearch | Keyword classification and determination in language modelling |
US20080215541A1 (en) * | 2007-03-01 | 2008-09-04 | Microsoft Corporation | Techniques for searching web forums |
US20080221884A1 (en) | 2007-03-07 | 2008-09-11 | Cerra Joseph P | Mobile environment speech processing facility |
US8949266B2 (en) * | 2007-03-07 | 2015-02-03 | Vlingo Corporation | Multiple web-based content category searching in mobile search application |
US8949130B2 (en) * | 2007-03-07 | 2015-02-03 | Vlingo Corporation | Internal and external speech recognition use with a mobile communication facility |
US20110060587A1 (en) * | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US8838457B2 (en) * | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US20090030687A1 (en) * | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Adapting an unstructured language model speech recognition system based on usage |
US20080221901A1 (en) | 2007-03-07 | 2008-09-11 | Joseph Cerra | Mobile general search environment speech processing facility |
US7584294B2 (en) | 2007-03-12 | 2009-09-01 | Citrix Systems, Inc. | Systems and methods for prefetching objects for caching using QOS |
US7945851B2 (en) * | 2007-03-14 | 2011-05-17 | Nuance Communications, Inc. | Enabling dynamic voiceXML in an X+V page of a multimodal application |
US20080228903A1 (en) | 2007-03-16 | 2008-09-18 | Yahoo! Inc. | System and method of serving advertisements for web applications |
US8670987B2 (en) * | 2007-03-20 | 2014-03-11 | Nuance Communications, Inc. | Automatic speech recognition with dynamic grammar rules |
US8060373B2 (en) | 2007-03-21 | 2011-11-15 | At&T Intellectual Property I, L.P. | System and method of identifying contact information |
EP1978704A1 (en) * | 2007-04-02 | 2008-10-08 | British Telecommunications Public Limited Company | Content delivery |
US20080256056A1 (en) * | 2007-04-10 | 2008-10-16 | Yahoo! Inc. | System for building a data structure representing a network of users and advertisers |
CN101286094A (en) * | 2007-04-10 | 2008-10-15 | 谷歌股份有限公司 | Multi-mode input method editor |
US7895177B2 (en) | 2007-05-29 | 2011-02-22 | Yahoo! Inc. | Enabling searching of user ratings and reviews using user profile location, and social networks |
WO2008151465A1 (en) | 2007-06-14 | 2008-12-18 | Google Inc. | Dictionary word and phrase determination |
US7831427B2 (en) | 2007-06-20 | 2010-11-09 | Microsoft Corporation | Concept monitoring in spoken-word audio |
US8275615B2 (en) | 2007-07-13 | 2012-09-25 | International Business Machines Corporation | Model weighting, selection and hypotheses combination for automatic speech recognition and machine translation |
CN105045777A (en) * | 2007-08-01 | 2015-11-11 | 金格软件有限公司 | Automatic context sensitive language correction and enhancement using an internet corpus |
US8244534B2 (en) * | 2007-08-20 | 2012-08-14 | Microsoft Corporation | HMM-based bilingual (Mandarin-English) TTS techniques |
US8321424B2 (en) * | 2007-08-30 | 2012-11-27 | Microsoft Corporation | Bipartite graph reinforcement modeling to annotate web images |
US7877385B2 (en) * | 2007-09-21 | 2011-01-25 | Microsoft Corporation | Information retrieval using query-document pair information |
US8321219B2 (en) * | 2007-10-05 | 2012-11-27 | Sensory, Inc. | Systems and methods of performing speech recognition using gestures |
US7953692B2 (en) * | 2007-12-07 | 2011-05-31 | Microsoft Corporation | Predicting candidates using information sources |
US8423362B2 (en) * | 2007-12-21 | 2013-04-16 | General Motors Llc | In-vehicle circumstantial speech recognition |
TWI399966B (en) | 2007-12-31 | 2013-06-21 | Htc Corp | The mobile phone and the dialing method thereof |
US8473276B2 (en) * | 2008-02-19 | 2013-06-25 | Google Inc. | Universal language input |
US8065143B2 (en) * | 2008-02-22 | 2011-11-22 | Apple Inc. | Providing text input using speech data and non-speech data |
US8224656B2 (en) * | 2008-03-14 | 2012-07-17 | Microsoft Corporation | Speech recognition disambiguation on mobile devices |
EP2101250B1 (en) | 2008-03-14 | 2014-06-11 | BlackBerry Limited | Character selection on a device using offset contact-zone |
US8831950B2 (en) | 2008-04-07 | 2014-09-09 | Nuance Communications, Inc. | Automated voice enablement of a web page |
US8121837B2 (en) | 2008-04-24 | 2012-02-21 | Nuance Communications, Inc. | Adjusting a speech engine for a mobile computing device based on background noise |
TWI352970B (en) | 2008-04-30 | 2011-11-21 | Delta Electronics Inc | Voice input system and voice input method |
US8090738B2 (en) | 2008-05-14 | 2012-01-03 | Microsoft Corporation | Multi-modal search wildcards |
US8589161B2 (en) | 2008-05-27 | 2013-11-19 | Voicebox Technologies, Inc. | System and method for an integrated, multi-modal, multi-device natural language voice services environment |
US8364481B2 (en) * | 2008-07-02 | 2013-01-29 | Google Inc. | Speech recognition with parallel recognition tasks |
US20100030549A1 (en) * | 2008-07-31 | 2010-02-04 | Lee Michael M | Mobile device having human language translation capability with positional feedback |
US8027973B2 (en) | 2008-08-04 | 2011-09-27 | Microsoft Corporation | Searching questions based on topic and focus |
US8385971B2 (en) * | 2008-08-19 | 2013-02-26 | Digimarc Corporation | Methods and systems for content processing |
WO2010021368A1 (en) | 2008-08-20 | 2010-02-25 | 日本電気株式会社 | Language model creation device, language model creation method, voice recognition device, voice recognition method, program, and storage medium |
US20100045611A1 (en) * | 2008-08-21 | 2010-02-25 | Microsoft Corporation | Touch screen mobile device as graphics tablet input |
US7979415B2 (en) * | 2008-09-04 | 2011-07-12 | Microsoft Corporation | Predicting future queries from log data |
US8775154B2 (en) * | 2008-09-18 | 2014-07-08 | Xerox Corporation | Query translation through dictionary adaptation |
US8326785B2 (en) * | 2008-09-30 | 2012-12-04 | Microsoft Corporation | Joint ranking model for multilingual web search |
US8407236B2 (en) * | 2008-10-03 | 2013-03-26 | Microsoft Corp. | Mining new words from a query log for input method editors |
GB2477653B (en) * | 2008-10-10 | 2012-11-14 | Nuance Communications Inc | Generating and processing forms for receiving speech data |
US9798720B2 (en) | 2008-10-24 | 2017-10-24 | Ebay Inc. | Hybrid machine translation |
US9043209B2 (en) | 2008-11-28 | 2015-05-26 | Nec Corporation | Language model creation device |
US8352321B2 (en) * | 2008-12-12 | 2013-01-08 | Microsoft Corporation | In-text embedded advertising |
US20100153370A1 (en) * | 2008-12-15 | 2010-06-17 | Microsoft Corporation | System of ranking search results based on query specific position bias |
US8156129B2 (en) * | 2009-01-15 | 2012-04-10 | Microsoft Corporation | Substantially similar queries |
US9330165B2 (en) | 2009-02-13 | 2016-05-03 | Microsoft Technology Licensing, Llc | Context-aware query suggestion by mining log data |
US8509398B2 (en) | 2009-04-02 | 2013-08-13 | Microsoft Corporation | Voice scratchpad |
US20100257171A1 (en) | 2009-04-03 | 2010-10-07 | Yahoo! Inc. | Techniques for categorizing search queries |
US20100318531A1 (en) | 2009-06-10 | 2010-12-16 | Microsoft Corporation | Smoothing clickthrough data for web search ranking |
US20100315266A1 (en) * | 2009-06-15 | 2010-12-16 | Microsoft Corporation | Predictive interfaces with usability constraints |
US9892730B2 (en) * | 2009-07-01 | 2018-02-13 | Comcast Interactive Media, Llc | Generating topic-specific language models |
US8364612B2 (en) * | 2009-09-15 | 2013-01-29 | Microsoft Corporation | Machine learning using relational databases |
US8255217B2 (en) * | 2009-10-16 | 2012-08-28 | At&T Intellectual Property I, Lp | Systems and methods for creating and using geo-centric language models |
US8589163B2 (en) * | 2009-12-04 | 2013-11-19 | At&T Intellectual Property I, L.P. | Adapting language models with a bit mask for a subset of related words |
US8612206B2 (en) * | 2009-12-08 | 2013-12-17 | Microsoft Corporation | Transliterating semitic languages including diacritics |
EP2339576B1 (en) * | 2009-12-23 | 2019-08-07 | Google LLC | Multi-modal input on an electronic device |
US11416214B2 (en) * | 2009-12-23 | 2022-08-16 | Google Llc | Multi-modal input on an electronic device |
US20110162035A1 (en) * | 2009-12-31 | 2011-06-30 | Apple Inc. | Location-based dock for a computing device |
US8494852B2 (en) * | 2010-01-05 | 2013-07-23 | Google Inc. | Word-level correction of speech input |
US8782556B2 (en) * | 2010-02-12 | 2014-07-15 | Microsoft Corporation | User-centric soft keyboard predictive technologies |
KR101477530B1 (en) * | 2010-03-12 | 2014-12-30 | 뉘앙스 커뮤니케이션즈, 인코포레이티드 | Multimodal text input system, such as for use with touch screens on mobile phones |
US8428759B2 (en) * | 2010-03-26 | 2013-04-23 | Google Inc. | Predictive pre-recording of audio for voice input |
US8694304B2 (en) * | 2010-03-26 | 2014-04-08 | Virtuoz Sa | Semantic clustering and user interfaces |
US8265928B2 (en) | 2010-04-14 | 2012-09-11 | Google Inc. | Geotagged environmental audio for enhanced speech recognition accuracy |
US8694313B2 (en) | 2010-05-19 | 2014-04-08 | Google Inc. | Disambiguation of contact information using historical data |
US8468012B2 (en) | 2010-05-26 | 2013-06-18 | Google Inc. | Acoustic model adaptation using geographic information |
US8359311B2 (en) * | 2010-06-01 | 2013-01-22 | Microsoft Corporation | Federated implicit search |
US20120060113A1 (en) * | 2010-09-08 | 2012-03-08 | Nuance Communications, Inc. | Methods and apparatus for displaying content |
US20120143611A1 (en) * | 2010-12-07 | 2012-06-07 | Microsoft Corporation | Trajectory Tiling Approach for Text-to-Speech |
US20120191745A1 (en) | 2011-01-24 | 2012-07-26 | Yahoo!, Inc. | Synthesized Suggestions for Web-Search Queries |
US8813060B2 (en) * | 2011-06-17 | 2014-08-19 | Microsoft Corporation | Context aware application model for connected devices |
US8847775B2 (en) | 2012-11-30 | 2014-09-30 | Panasonic Corporation | Tangible charge level awareness method and apparatus using augmented batteries |
US9075846B2 (en) * | 2012-12-12 | 2015-07-07 | King Fahd University Of Petroleum And Minerals | Method for retrieval of arabic historical manuscripts |
US20140173440A1 (en) | 2012-12-13 | 2014-06-19 | Imimtek, Inc. | Systems and methods for natural interaction with operating systems and application graphical user interfaces using gestural and vocal input |
US9626960B2 (en) * | 2013-04-25 | 2017-04-18 | Nuance Communications, Inc. | Systems and methods for providing metadata-dependent language models |
-
2010
- 2010-06-10 EP EP10165480.4A patent/EP2339576B1/en active Active
- 2010-06-10 EP EP23202787.0A patent/EP4318463A3/en active Pending
- 2010-06-10 EP EP16001249.8A patent/EP3091535B1/en active Active
- 2010-12-22 US US12/976,920 patent/US9495127B2/en active Active
- 2010-12-22 US US12/976,972 patent/US20110161080A1/en not_active Abandoned
- 2010-12-22 US US12/977,003 patent/US9031830B2/en active Active
- 2010-12-22 US US12/977,017 patent/US20110161081A1/en not_active Abandoned
-
2011
- 2011-09-29 US US13/249,172 patent/US8751217B2/en active Active
- 2011-09-29 US US13/249,181 patent/US9047870B2/en active Active
- 2011-09-29 US US13/249,175 patent/US20120022866A1/en not_active Abandoned
- 2011-09-29 US US13/249,180 patent/US20120022873A1/en not_active Abandoned
-
2014
- 2014-06-09 US US14/299,837 patent/US9251791B2/en active Active
-
2016
- 2016-01-05 US US14/988,408 patent/US10157040B2/en active Active
-
2018
- 2018-10-24 US US16/169,279 patent/US10713010B2/en active Active
-
2022
- 2022-07-13 US US17/812,320 patent/US11914925B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040243415A1 (en) * | 2003-06-02 | 2004-12-02 | International Business Machines Corporation | Architecture for a speech input method editor for handheld portable devices |
Also Published As
Publication number | Publication date |
---|---|
EP4318463A3 (en) | 2024-02-28 |
EP2339576B1 (en) | 2019-08-07 |
US20120022873A1 (en) | 2012-01-26 |
US20110153324A1 (en) | 2011-06-23 |
US20120022853A1 (en) | 2012-01-26 |
US8751217B2 (en) | 2014-06-10 |
US20140288929A1 (en) | 2014-09-25 |
EP2339576A2 (en) | 2011-06-29 |
US20220405046A1 (en) | 2022-12-22 |
US20160132293A1 (en) | 2016-05-12 |
EP2339576A3 (en) | 2011-11-23 |
US11914925B2 (en) | 2024-02-27 |
EP3091535A3 (en) | 2017-03-01 |
US20120022866A1 (en) | 2012-01-26 |
US10713010B2 (en) | 2020-07-14 |
US9031830B2 (en) | 2015-05-12 |
US20120022867A1 (en) | 2012-01-26 |
US9047870B2 (en) | 2015-06-02 |
US20110161081A1 (en) | 2011-06-30 |
US20110153325A1 (en) | 2011-06-23 |
US9251791B2 (en) | 2016-02-02 |
EP3091535A2 (en) | 2016-11-09 |
US20190056909A1 (en) | 2019-02-21 |
US20110161080A1 (en) | 2011-06-30 |
EP4318463A2 (en) | 2024-02-07 |
US10157040B2 (en) | 2018-12-18 |
US9495127B2 (en) | 2016-11-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11914925B2 (en) | Multi-modal input on an electronic device | |
KR102596446B1 (en) | Modality learning on mobile devices | |
US11416214B2 (en) | Multi-modal input on an electronic device | |
US10395654B2 (en) | Text normalization based on a data-driven learning network | |
KR102072730B1 (en) | Determining hotword suitability | |
CN111710333B (en) | Method and system for generating speech transcription | |
JP5599662B2 (en) | System and method for converting kanji into native language pronunciation sequence using statistical methods | |
JP7200405B2 (en) | Context Bias for Speech Recognition | |
US10643603B2 (en) | Acoustic model training using corrected terms | |
EP2875509A1 (en) | Speech and gesture recognition enhancement | |
JP7400112B2 (en) | Biasing alphanumeric strings for automatic speech recognition | |
US20240160403A1 (en) | Multi-modal input on an electronic device | |
CN1965349A (en) | Multimodal disambiguation of speech recognition | |
US11482214B1 (en) | Hypothesis generation and selection for inverse text normalization for search |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20160602 |
|
AC | Divisional application: reference to earlier application |
Ref document number: 2339576Country of ref document: EPKind code of ref document: P |
|
AK | Designated contracting states |
Kind code of ref document: A2Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO SE SI SK SM TR |
|
PUAL | Search report despatched |
Free format text: ORIGINAL CODE: 0009013 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G10L 15/30 20130101ALN20170109BHEPIpc: G10L 15/22 20060101AFI20170109BHEPIpc: G10L 15/183 20130101ALN20170109BHEP |
|
AK | Designated contracting states |
Kind code of ref document: A3Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO SE SI SK SM TR |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G10L 15/183 20130101ALN20170125BHEPIpc: G10L 15/22 20060101AFI20170125BHEPIpc: G10L 15/30 20130101ALN20170125BHEP |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20190326 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20230424 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AC | Divisional application: reference to earlier application |
Ref document number: 2339576Country of ref document: EPKind code of ref document: P |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602010069084Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20231011 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1621050Country of ref document: ATKind code of ref document: TEffective date: 20231011 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231011 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240112 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240211 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231011 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231011 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231011 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231011Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240211Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240112Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231011Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240111Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231011Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240212 |