CN110520925A - End of inquiry detection - Google Patents
End of inquiry detection Download PDFInfo
- Publication number
- CN110520925A CN110520925A CN201880021601.7A CN201880021601A CN110520925A CN 110520925 A CN110520925 A CN 110520925A CN 201880021601 A CN201880021601 A CN 201880021601A CN 110520925 A CN110520925 A CN 110520925A
- Authority
- CN
- China
- Prior art keywords
- language
- complete
- confidence
- audio data
- microphone
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L25/87—Detection of discrete points within a voice signal
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/065—Adaptation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/187—Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/03—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L2025/783—Detection of presence or absence of voice signals based on threshold decision
Abstract
Disclosing for detect end of inquiry includes the mthods, systems and devices for encoding the computer program in computer storage medium.On the one hand, a kind of method includes receiving the movement of audio.Movement further includes that application queries terminate model.Movement further includes determining the confidence for a possibility that reflection language is complete language.Movement further includes being compared confidence with threshold value.Movement further includes whether determining language is to be likely to complete.Movement further includes providing microphone instruction.This method, which can increase, calculates equipment to the effectiveness of user, especially for voice is out-of-sequence or the user of obstacle.This method can also save power, because microphone does not need to be activated.Can also to avoid based on microphone detection to additional audio come explain and execution task in computing resource use.
Description
Cross reference to related applications
This application claims U.S. Provisional Application No. 62/515,771 equity submitted on June 6th, 2017, pass through reference
Merged.
Technical field
This disclosure relates to natural language processing.
Background technique
Natural language processing is related to the interaction between computer and Human Natural Language.Specifically, natural language processing collection
In in the natural language data for how handling computer programming different number.Natural language processing may include that voice is known
Not, natural language understanding and spatial term.
Summary of the invention
Natural language processing system determined usually using end indicator (endpointer) user when start with it is complete
At speaking.Once the part for representing the audio of user speech is transferred to another component of system for further by delimiting
Processing or other assemblies can be activated or be powered for extra process.Some traditional end indicators are determining words
Language when start or at the end of assess word between pause duration.For example, if user says that " what is < length is paused
> for dinner (what dinner eats) ", then traditional end indicator may cut speech input in the long punishment that pauses, and can
Natural language processing system can be instructed to attempt to handle incomplete phrase " what is (what is) ", rather than complete phrase
" what is for dinner (what dinner eats) ".If end indicator be speech input specify it is incorrect beginning or
End point, then the result that speech input is handled using natural language processing system may inaccuracy or undesirable.Instruction terminates
(endpoint) another component that inaccurate language may cause system unnecessarily activation system is further processed audio
Data.This unnecessary activation may waste computing resource and/or battery capacity because user may finally repeat it is identical
Language, it is desirable to system suitably delimit the voice of user and activates component appropriate or execute extra process appropriate.
Some natural language processing systems may include Voice decoder.Voice decoder can be configured as using language
Model handles audio data corresponding with user spoken utterances, to generate the transcription of audio data, and determines when user very may be used
It can complete to speak.Voice decoder can be implemented in remote server, which may need the reception when user speaks
The equipment of language passes through the part of network transmission audio data, and the receiving device uses microphone detection language.
Network speed can determine that (dictate) receiving device can transmit audio data to remote server by network
Mode.Fast network permission receiving device transmits the audio number of small grouping with the frequency every about 100 milliseconds of groupings
According to each small grouping may include about 100 milliseconds of audio data.Slow speed network may prevent receiving device with similar
Frequency transmits audio data.When transmitting audio data by slow speed network, receiving device can be every about 800 milliseconds one
The frequency of grouping transmits larger packet audio data, and each larger grouping may include 800 milliseconds of audio data.
In the case where Voice decoder receives the audio of larger grouping, the efficiency of Voice decoder may be reduced.In
In this case, Voice decoder possibly can not determine in time whether user is likely to complete to speak.In doing so, it examines
The microphone for surveying the equipment of language may stay open, and detect and be not intended to received sound for the equipment.Language
The execution of specified movement may also be delayed by.It makes a phone call if user says to mother, phone initiates calling may
There is delay, because Voice decoder is very slow when determining when user is likely to pipe down, this causes phone executing language
It is very slow when specified movement.Phone is also possible to detection additional audio, this may cause movement that phone or server execute not
It is same as the movement of user's intention, this may cause the waste of the computing resource when explaining and acting on the additional audio detected.
The problem of in order to correct above-mentioned identification, natural language processing system may include end of inquiry (end of query)
Detector, quickly determines whether user is likely to pipe down using machine learning and neural network.End of inquiry
Detector can be applied and be configured to determine that received audio data likely corresponds to complete language or imperfect language
Model.The training data of the label including complete language and imperfect language can be used to train the model.The model can
To use the various acoustics including tone, loudness, intonation, acutance, clarity, roughness, unstability and voice rate
Characteristics of speech sounds determines whether user is likely to complete to speak.Determining whether user is likely to complete using voice
When at speaking, which is also conceivable to other acoustics being likely to occur during pause prompt (acoustic cue).
End of inquiry detector can quickly determine that the received audio data of institute is to be likely to corresponding than Voice decoder
In complete language or imperfect language, especially when audio data of the remote server with the big grouping of slow frequency reception.Cause
This, remote server can not postpone when determining when user is likely to complete to speak, and can transmit deactivated wheat
The instruction of gram wind, determines that user is likely to complete to speak without waiting voice decoder.Therefore, calculating money can be saved
Source and power.
According to the novel aspects of theme described in this application, a kind of method for detecting end of inquiry includes following dynamic
Make: receiving audio data corresponding with the language that user says；End of inquiry model is applied to audio data, the inquiry knot
Beam model (i) be configured to determine that reflection language a possibility that being complete language (likelihood) confidence and
(ii) it is trained to using the audio data from complete language and from imperfect language；Terminate model based on application queries come really
Surely the confidence for a possibility that reflection language is complete language, the end of inquiry model (i) are configured to determine that reflection words
The confidence for a possibility that language is complete language and (ii) use the sound from complete language and from imperfect language
Frequency evidence is trained to；The confidence for reflecting a possibility that language is complete language is compared with confidence threshold value
Compared with；Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Language is likely to complete or is likely to incomplete；And it is likely to complete based on determining language or is likely to
It is incomplete, provide that the microphone for receiving language is maintained at active state with (i) by the instruction for output or (ii) is deactivated
Receive the microphone of language.
These and other embodiments can optionally include the one or more of following characteristics with each.Movement further includes,
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, confidence is determined
Degree score meets confidence threshold value.Determine that language is likely to complete or is likely to incomplete movement include base
It is to be likely to complete in determining that confidence meets confidence threshold value and determining language.Instruction for output is provided
Include by the movement that the microphone for receiving language is maintained at the microphone that active state or (ii) deactivate reception language with (i),
The instruction for output is provided to deactivate the microphone for receiving language.Movement further includes generating the transcription of audio data, and provide
Transcription is for exporting.Movement further includes receiving the data that confirmation user completes to speak from user；And it is used based on confirmation is received
The data spoken are completed at family, update end of inquiry model.
Movement further includes, based on the confidence and confidence threshold that will reflect a possibility that language is complete language
Value is compared, and determines that confidence is unsatisfactory for confidence threshold value.Determine that language is likely to complete or very may be used
Incomplete can act includes being unsatisfactory for confidence threshold value and determining language to be to be likely to not based on determining confidence
Completely.The microphone for receiving language is maintained at active state with (i) for instruction of the offer for output or (ii) is deactivated and connect
The movement for receiving the microphone of language includes providing the instruction for output so that microphone is maintained at active state.Movement is also wrapped
Include the audio data for receiving multiple complete language and multiple imperfect language；And it uses machine learning, use multiple complete words
The audio data of language and multiple imperfect language trains end of inquiry model.End of inquiry model is configured as based on language
Acoustic voice characteristic reflects the confidence for a possibility that language is complete language to determine, acoustic voice characteristic includes sound
Tune, loudness, intonation, acutance, clarity, roughness, unstability and voice rate.
Movement further includes that determining Voice decoder determines that language is likely to complete or is likely to imperfect not yet
, Voice decoder be configurable to generate the transcription of audio data and be configured to determine that language be likely to it is complete or
It is likely to incomplete.Determine language be likely to complete or be likely to it is incomplete movement be based only upon will reflection language be
The confidence of a possibility that complete language is compared with confidence threshold value.Voice decoder uses language model
Determine that language is likely to complete or is likely to incomplete.Movement further includes that determining Voice decoder has determined language
It is likely to complete or is likely to incomplete, Voice decoder is configurable to generate the transcription of audio data and is matched
Determining language is set to be likely to complete or be likely to incomplete.Determine that language is likely to complete or is likely to
It is incomplete movement based on (i) by reflect language be complete language a possibility that confidence and confidence threshold value into
Row is relatively and (ii) Voice decoder determines that language is likely to complete or is likely to incomplete.
The other embodiments of this aspect include corresponding system, device and computer program, these systems, device and meter
Calculation machine program includes the instruction that can recorde on one or more computer memory devices, each is configured to execute these sides
The operation of method.
Theme described in this application can have the one or more of following advantages.
The speech input capability for calculating equipment can be used in user, and is spoken with the comfortable speed of user.This can increase
Add and calculate equipment to the effectiveness of user, especially for voice is out-of-sequence or the user of obstacle.It can be in the intention knot of language
It is instructed to terminate at beam, leads to more acurrate or desired natural language processing output and natural language processing system faster
Processing.This can reduce the use of computing resource, and can save power.In addition, can more suitably putting mute microphone (MIC)
Power is used and saves be further reduced computing resource, because microphone does not need to be activated, and can be to avoid in base
In microphone detection to additional audio explain and execution task in use computing resource.
The details of one or more embodiments of subject content described in this specification is in the accompanying drawings and the description below
It illustrates.From description, drawings and claims, other features, aspects and advantages of subject content be will become obvious.
Detailed description of the invention
Figures 1 and 2 show that terminating the example system of (end of an utterance) for detecting language.
Fig. 3 shows the example system for training language end detector.
Fig. 4 shows the instantiation procedure for detecting end of inquiry model.
Fig. 5 shows the directed graph of the calculating and conditional independence structure that show the exemplary classifier based on LSTM
Model.
Fig. 6 is shown for after the housebroken voice activity classifier of single language and the example of end of inquiry classifier
Test (posterior).
Fig. 7 shows the example for calculating equipment and mobile computing device.
In the accompanying drawings, identical reference number represents corresponding part always.
Specific embodiment
Fig. 1 shows the example system 100 terminated for detecting language.In brief, and more detailed description as follows
, user 102 says language 104.Calculate the microphone detection language 104 of equipment 106.When user 102 speaks, equipment is calculated
106 by the fractional transmission of the audio data 108 of language 104 to server 110.When server 110 receives audio data 108
When part, server 110 handles the part of audio data 108, and determines when user 102 may complete to speak.Server 110
To the transmission of equipment 106 instruction 112 is calculated, to deactivate the microphone for calculating equipment 106.
In stage A, and in the time 0, user 102 starts to say language 104.For example, user can be by saying " what
(what) " starts language 104.It calculates equipment 106 and passes through microphone detection language 104.Calculating equipment 106 can be and can examine
Survey any kind of calculating equipment of sound.For example, calculating equipment 106 can be phone, tablet computer, smartwatch, intelligence
Loudspeaker, laptop computer, desktop computer or any other similar type calculating equipment.
When user 102 loquiturs, calculates equipment 106 and receive and process language 104.Equipment 106 is calculated to microphone
The audio detected is sampled, and converts analog signals into digital signal using analog-digital converter.Calculating equipment 106 can be with
In a buffer by digitized audio storage, so that calculating equipment 106 is further processed or arrives digitized audio transmission
Server 106.
In the example depicted in fig. 1, equipment 106 and server 110 is calculated to be communicated by fast network 114.With such as
The case where fruit calculating equipment 106 is communicated with server 110 by slower network is compared, and fast network 114 allows to calculate
Equipment 106 is with the smaller portions of higher frequency transmission audio data 108.It will describe that there is slower network in Fig. 2 below
Example.
In stage B, calculates equipment 106 and start the part for transmitting audio data 108 to server 110.For example, being equal to
200 milliseconds of time calculates the part 116 that equipment 106 transmits audio data 108.Part 116 can correspond to audio data
The initial part of first 100 milliseconds of 108 or " what (what) ".Because network 114 is sufficiently fast, calculating equipment 106 can be with
Continue the extra section for transmitting audio data 108 at short intervals.For example, in stage C, and in the time for being equal to 300 milliseconds, meter
Calculate the part 118 that equipment 106 transmits audio data 108.The part 118 of audio data 108 can correspond to audio data 108
The remainder of second 100 milliseconds or " what (what) ".In stage D, and in the time for being equal to 400 milliseconds, calculating is set
The part 120 of audio data 108 is transferred to server 110 by standby 106.The part 120 of audio data 108 can correspond to audio
Third 100 milliseconds or the silence period between " what (what) " and " is (YES) " of data 108.
Server 110 receives the part 116,118 and 120 of audio data 108, and uses Voice decoder 122 and inquiry
End detector 124 handles them.It includes in the part of audio data 108 that Voice decoder 122, which can be configured as identification,
In different phonemes.The part 116,118 and 120 of audio data 108 may not correspond to the different phonemes of language 104.For example,
The part 116 of audio data 108 may include some of " wh " sound and " a " sound.The part 118 of audio data 108 may include
The remainder and " t " sound of " a " sound.Voice decoder 122 handles part 116 and 118, and identifies " wh " sound, " a " sound and " t "
The phoneme of sound.Voice decoder 122 can handle part 120 and identify silencing phoneme.In some embodiments, server
Different disposal engine, to identify phoneme, and phoneme and timing data is mentioned based on the part 116,118 and 120 of audio data 108
Supply Voice decoder 122.Timing data may include the time quantum that user 102 is used to say each phoneme.In some implementations
In mode, calculates equipment 106 and identify phoneme, and each of part 116,118 and 120 corresponds to different phonemes.In
In this case, each of part 116,118 and 120 may include designated user 102 be used for say each phoneme when
The timing data of the area of a room.
Voice decoder 122 handles the phoneme of audio data 108, and generates the transcription of audio data 108
(transcription)128.In some embodiments, Voice decoder 122 generates transcription using language model 126
128.Voice decoder 122 can also determine when user 102 pipes down using language model 126.For example, by by language
Say that model 126 is applied to the initial phoneme of audio data 108, Voice decoder 122 can determine user after " what is "
It is likely to have not been completed and speak, because " what is " may be incomplete language.
When Voice decoder 122 receives part 116,118 and 120 of audio data 108, the processing of Voice decoder 122
Part 116,118 and 120.Voice decoder 122 receive audio data 108 with " California (California) " phase
It can determine that user 102 may say " what is " when corresponding part.
End of inquiry detector 124 also receives the part 116,118 and 120 of audio data 108, and they are applied to warp
Trained end of inquiry model 130.When server 110 receives part 116,118 and 120 of audio data 108, end of inquiry
Detector 124 handles these parts.End of inquiry detector 124 generates confidence, and confidence has reflected user 102
Through complete speak (i.e. language is complete) a possibility that.In other words, confidence reflection language 104 is complete general
Rate.Confidence is compared by end of inquiry detector 124 with threshold score.If confidence meets threshold value, that
End of inquiry detector 124 determines that user 102 is likely to complete to speak, and language is likely to complete.
As an example, end of inquiry detector 124 handles the part 116,118 and 120 of audio data 108, these parts
" what " is said, followed by the silencing between " what " and " is " corresponding to user 102.End of inquiry detector 124 receives first
Part 116, and part 116 is handled using housebroken end of inquiry model 130.End of inquiry detector 124 generates confidence
Score 0.01 is spent, and the confidence and confidence threshold value 0.8 are compared.End of inquiry detector 124 determines in portion
Divide after 116, language is likely to incomplete, and user 102 has not been completed and speaks.
124 receiving portion 118 of end of inquiry detector, and part is handled using housebroken end of inquiry model 130
118.End of inquiry detector 124 generates confidence 0.01, and the confidence and confidence threshold value 0.8 are compared
Compared with.End of inquiry detector 124 determines that after part 116 and 118, language is likely to incomplete, and user 102 is also
It does not complete to speak.
124 receiving portion 120 of end of inquiry detector, and part is handled using housebroken end of inquiry model 130
120.End of inquiry detector 124 generates confidence 0.2, and the confidence and confidence threshold value 0.8 are compared
Compared with.End of inquiry detector 124 determines that after part 116,118 and 120, language is likely to incomplete, and user
102 have not been completed and speak.In this example, confidence may be higher, because part 120 includes silencing.
In stage E, and in the time for being equal to 1500 milliseconds, user completes to say language 104.At this point, user 102 may
There are no provide any instruction for having completed to speak of user 102 to calculating equipment 106.Therefore, server 110 is not used by oneself
Any information instruction user 102 at family 102 has completed to speak.The microphone for calculating equipment 106 keeps active, and can connect
Receive and handle any additional sound.
After user completes to speak soon, audio number of the equipment 106 to voice of the transmission of server 110 including user is calculated
According to 108 decline.After the decline of audio data 108 for including the voice of user, calculates the transmission of equipment 106 and refer to
Show the part of the audio data of silencing, is similar to part 120.When server 110 continues to the part of audio data, voice
Decoder 122 and end of inquiry detector 124 continue with the part of audio data.Voice decoder 122 analyzes phoneme, and makes
Transcription 128 is generated with language model, and determines when user may complete to speak.End of inquiry detector 124 will be through
Trained end of inquiry model 130 is applied to the part of audio data, to determine when user may complete to speak.
In stage F, and in the time for being equal to 1600 milliseconds, Voice decoder 122 determines that user 102 may complete
It speaks.Voice decoder 122 generates transcription 128, and output 132 is being equal to 1800 milliseconds of time for calculating equipment 106
(endpoint) is terminated to the instruction of language 104 of user.Voice decoder 122 can be configured as to be likely to user 102
Point through completing to speak increases by 300 milliseconds, actually has not been completed and speaks to prevent user 102.If user 102 starts again at
It speaks, then the process that Voice decoder 122 analyzes audio data portion continues.
In stage G, and in the time for being equal to 1600 milliseconds, end of inquiry detector 124 determines that user 102 is likely to
Through completing to speak.End of inquiry detector 124 generates output 134 and is being equal to 1800 milliseconds of time pair for calculating equipment 106
The instruction of language 104 of user terminates.End of inquiry detector 124 can be configured as to be likely to complete to user 102
The point of words increases by 300 milliseconds, actually has not been completed and speaks to prevent user 102.It speaks if user 102 starts again at, that
The process that end of inquiry detector 124 analyzes audio data portion continues.
Because network 114 is fast network, Voice decoder 122 and end of inquiry detector 124 are almost true simultaneously
Determine user to be likely to complete to speak, and the instruction of language 104 should be terminated in the time equal to 1800 milliseconds.By right
The instruction of language 104 terminates, and server 110 determines that user is likely to the time point for completing to speak.Server will not end point it
It is handled afterwards using additional audio as user speech.
In stage H, server 110 is to the transmission of equipment 106 instruction 112 is calculated, to deactivate in the time equal to 1800 milliseconds
Microphone.It calculates equipment 106 and receives instruction 112, and deactivate microphone in the commanded time.
In stage I, server 110 exports the transcription 128 of language 104.In some embodiments, server 110 can be with
Transcription 128 is transferred to and calculates equipment 106.In this case, calculating equipment 106 can be in the display for calculating equipment 106
Upper display transcription 128.In some embodiments, server 110 can execute movement based on transcription 128, such as initiation phone
Calling sends message, opens application, initiates search inquiry or any other similar action.
Fig. 2 shows the example systems 200 terminated for detecting language.In brief, and more detailed description as follows
, user 202 says language 204.Calculate the microphone detection language 204 of equipment 206.When user 202 speaks, due to connection
The slow speed network 214 of equipment 206 and server 210 is calculated, the small of audio data 208 cannot continuously be transmitted by calculating equipment 206
Part.But calculate the larger grouping 216 and 220 that equipment 206 transmits audio data 208 with less frequent interval.When
When server 210 receives the grouping 216 and 220 of audio data 208, server 210 handles 216 He of grouping of audio data 208
220, and determine when user 202 is likely to complete to speak.Server 210 is to the transmission of equipment 206 instruction 212 is calculated, to deactivate
Calculate the microphone of equipment 206.
In stage A, and in the time 0, user 202 starts to say language 204.For example, user can be by saying " what
(what) " starts language 204.It calculates equipment 206 and passes through microphone detection language 204.Meter can be similar to by calculating equipment 206
Equipment 106 is calculated, and can be any kind of calculating equipment for being able to detect sound.For example, calculating equipment 206 can be electricity
Words, tablet computer, smartwatch, smart speakers, laptop computer, desktop computer or any other similar type meter
Calculate equipment.
When user 202 loquiturs, calculates equipment 206 and receive and process language 204.Equipment 206 is calculated to microphone
The audio detected is sampled, and converts analog signals into digital signal using analog-digital converter.Calculating equipment 206 can be with
In a buffer by digitized audio storage, so that calculating equipment 206 is further processed or arrives digitized audio transmission
Server 206.
In the illustrated example shown in fig. 2, equipment 206 and server 210 is calculated to be communicated by slow speed network 214.At a slow speed
Network 214, which prevents, calculates equipment 206 to be similar to the frequency of calculating 106 hop 116,118 and 120 of equipment in Fig. 1
Frequency transmits the part of audio data 208.On the contrary, calculate equipment 206 be merely able to than frequency lower in Fig. 1 to server
The biggish grouping 216 and 220 of 210 transmission.
In stage B, equipment 206 is calculated to the first of server transport audio data 208 and is grouped 216.For example, being equal to
800 milliseconds of time calculates the grouping 216 that equipment 206 transmits audio data 208.Grouping 216 can correspond to audio data
" (how is weather by what is the weather. for first 800 milliseconds of 208 or word.) " because network 214 is too slow, meter
Any additional audio packet cannot be transmitted before stage D by calculating equipment 206.
Before calculating equipment 206 and sending server 210 for next grouping 220 of audio data 208, in stage C (example
Such as, the time equal to 1500 milliseconds), user 202 completes to speak.Similar to the example in Fig. 1, user 202 may be to meter
The offer user 202 of equipment 206 is provided and has completed any instruction spoken.Therefore, the appointing not from user 202 of server 210
What information instruction user 202 has completed to speak.The microphone for calculating equipment 206 keeps active, and can receive and handle
Any additional sound.
In stage D, next grouping 220 that equipment 206 sends audio data 208 to server 210 is calculated.For example, audio
The grouping 220 of data 208 may include next 800 milliseconds of the audio data 208 after grouping 216.Grouping 220 can be with
Corresponding to word " inCalifornia (in California) " and then about 100 milliseconds of silencing.It is being waited in user 202
In the case that 1000 milliseconds of times complete to speak, grouping 220 may include additional silencing (for example, 600 milliseconds).
Server 210 receives the grouping 216 of audio data, and uses Voice decoder 222 and end of inquiry detector 224
To handle them.It includes the different phonemes in the part of audio data 208 that Voice decoder, which can be configured as identification,.Dividing
Group 216 includes tone decoding in the example of audio data corresponding with " what is the weather (how is weather) "
The processing of device 222 grouping 216 simultaneously identifies corresponding phoneme.Voice decoder 222 generates grouping 216 using language model 226
It may transcription.Voice decoder handles biggish grouping 216 rather than the smaller portions of the audio data in Fig. 1 may be without that
Efficiently.This slower processing may make Voice decoder 222 and generate transcription using more times, and determine user
When 202 may complete to speak.
End of inquiry detector 224 receives the grouping 216 of audio data 208, and by audio data applied to housebroken
End of inquiry model 230.End of inquiry detector 224 generates the confidence level that reflection user 202 has completed a possibility that speaking
Score.In other words, confidence reflection is complete probability with 216 corresponding language of grouping.End of inquiry detector
224 are compared confidence with threshold score.If confidence meets threshold value, end of inquiry detector 224
Determine that user 202 is likely to complete to speak.
As an example, end of inquiry detector 224 handles the grouping 216 of audio data 208, grouping 216 is likely corresponded to
User 202 says " what is the weather (how is weather) ".Grouping 216 may include some additional silencings or
It can not include " weather (weather) " all voices.End of inquiry detector 224 uses housebroken end of inquiry model
230 handle grouping 216.End of inquiry detector 224 generates confidence 0.4, and by the confidence and confidence level
Threshold value 0.8 is compared.End of inquiry detector 224 determines that after part 216, language is likely to incomplete, and
User 202, which has not been completed, to speak.
Unlike Voice decoder 222, end of inquiry detector 224 can quickly handle grouping 216, even if 216 packet of grouping
Include than in Fig. 1 part 116,118 and 120 more audio datas.Therefore, end of inquiry detector 224 can be than voice solution
Code device 222 quickly determines whether be likely to complete with 216 corresponding language of grouping.
In stage E, end of inquiry detector 224 is by being applied to 220 Hes of grouping for housebroken end of inquiry model 230
216 are grouped to handle grouping 220.Because the processing that end of inquiry detector 224 executes is quickly, end of inquiry detector
224 can quickly calculate confidence, and confidence reflection is complete with 216 and 220 corresponding language of grouping
Possibility.In this example, it is 0.9 that end of inquiry detector 224, which can calculate confidence,.End of inquiry detector
224 are compared the confidence and confidence threshold value 0.8, and determine that language is likely to complete.It is being equal to 1700
The time of millisecond, 224 output order of end of inquiry detector is to terminate the instruction of language 204 in the time equal to 1800 milliseconds.
In stage G, Voice decoder 222 handles the grouping 220 of audio data 208.As described above, Voice decoder 222
220 may not be able to be grouped greatly to handle with speed identical with the smaller portions of audio data 108 shown in FIG. 1.Tone decoding
Device 222 handles grouping 220 and grouping 216 using language model 226.Voice decoder 222 cannot be such as end of inquiry detector
224 determine whether be likely to complete with 216 and 220 corresponding language of grouping so fast.For example, being equal to 1750 millis
The time of second, output 232 do not indicate whether language 208 is likely to complete.The audio being grouped greatly is received in server 210
In the case where data (this may be caused by being connected as slow speed network), end of inquiry detector 224 can compare Voice decoder
222, which quickly export language, terminates to determine.
In order to safeguard user experience and the microphone for calculating equipment 206 holding be prevented actively to be longer than the necessary time, in rank
Section F, server 210 can transmit instruction 212, be equal to 1800 milliseconds of time mute microphone (MIC) for calculating equipment 206.This
Can by prevent detect and realize additional detections to audio come avoid unnecessary computing resource use and safeguard user
Experience.Even if the timely instruction about deactivated microphone can also be received by calculating equipment 206 using slower network 214
212, and in response to instructing 212 deactivated microphones.
Voice decoder 222 continues with the grouping 216 and 222 of audio data 208.Voice decoder 222 is by language mould
Type 226 is applied to grouping 216 and 222, and generates transcription 228.In stage H, server 210 exports the transcription 228 of language 204.
In some embodiments, transcription 228 can be transferred to by server 210 calculates equipment 206.In this case, calculating is set
Standby 202 can show transcription 228 on the display for calculating equipment 202.In some embodiments, server 210 can be with base
Movement is executed in transcription 228, such as initiation call sends message, opens application, initiates search inquiry or any other class
Apparent movement is made.
Fig. 3 shows the example system 300 for training end of inquiry model 302.In brief, and it is as follows more detailed
It carefully describes, system 300 terminates training data 304 using the instruction of label to train end of inquiry model 302.Housebroken inquiry
Asking, which terminates model 302, can determine whether language is likely to complete.End of inquiry model 302 can be similar to the inquiry of Fig. 1
Ask the end of inquiry model 230 for terminating model 130 and Fig. 2.
System 300 includes that the instruction of label terminates training data 304.It includes more that the instruction of label, which terminates training data 304,
A audio sample, multiple audio sample include both complete language and imperfect language.Each audio sample includes label,
The label indicates that the audio sample is complete or incomplete.For example, label instruction terminate training data 304 include with
The corresponding audio data of language " what is the score ... of the game (game ... score be how many) "
306.Audio data 306 includes the label 308 that instruction audio data 306 indicates complete language.The instruction of label terminates to train number
It include audio data 310 corresponding with language " call mom (making a phone call to mother) " according to 304.Audio data 310 includes referring to
Show that audio data 310 indicates the label 312 of complete language.The instruction of label terminates training data 304
The corresponding audio data 314 of a (order) ".Audio data 314 includes the mark that instruction audio data 314 indicates imperfect language
Label 316.
In some embodiments, the instruction of label terminate training data 304 can be specific to user, user type, ring
The variable in border, particular device or any other type.For example, it can only include coming from that the instruction of label, which terminates training data 304,
The audio sample of user 102 in Fig. 1.As another example, the instruction of label terminate training data 304 can only be included in it is specific
The audio sample collected in equipment, the phone of such as specific model.As another example, the instruction of label terminates training data
304 can only include the audio sample from the user to drive when speaking.
System 300 includes end of inquiry model trainer 318.End of inquiry model trainer 318 is come using machine learning
Training end of inquiry model 302.End of inquiry model 302 can be the neural network trained by end model trainer.One
In a little embodiments, neural network is recurrent neural network or convolutional neural networks.Neural network can have certain amount of
Hidden layer, or the unilateral network based on LSTM (long short-term memory, shot and long term memory).
Housebroken end of inquiry model 302 includes confidence generator 320, and confidence generator 320 is raw
At confidence, confidence indicates a possibility that received audio data corresponds to complete language.Housebroken inquiry
Terminate model 302 and carries out the confidence of generation with the confidence threshold value being stored in confidence threshold value 322
Compare.
Housebroken end of inquiry model 302 is configurable to generate for received audio data sample and for audio number
According to the confidence of each further part of sample.The confidence being subsequently generated is based not only on the first of audio data sample
Initial portion, also each further part based on audio data sample.Follow the example of Fig. 1, housebroken end of inquiry model 302
Confidence can be generated based on the part 116 of audio data 108.Housebroken end of inquiry model 302 can receive sound
Frequency and generates another confidence based on both parts 116 and 118 according to 108 part 118.Once receiving audio number
According to 108 part 120, housebroken end of inquiry model 302 can generate another confidence level based on part 116,118 and 120
Score.In other words, housebroken end of inquiry model 302 determines the words using the received all data of particular utterance are directed to
Whether language is likely to complete.
In some embodiments, various audio speech characteristics can be considered to determine confidence level in end of inquiry model 302
Score.For example, end of inquiry model 302 can be used tone, loudness, intonation, acutance, clarity, roughness, unstability and
Any combination of voice rate or these features.With the silencing that only considers whether to detect fixed intervals after user spoken utterances
Conventional method is compared, and can provide improved end of inquiry model using these characteristics.In the slow-spoken situation of user, ask
Asking, which terminates model 302, may generate lower confidence.For example, if user says " what is the weather
(weather is how) ", and the pronunciation of " weather (weather) " is elongated to indicate that user does not complete to speak, then end of inquiry
The characteristics of speech sounds and other characteristics of speech sounds can be used to generate confidence in model 302.For example, model can be improved in this
For there is the effectiveness of the user of voice disorder.Audio speech characteristic used in end of inquiry model 302 can depend on label
Instruction terminate training data 304.
In some embodiments, confidence threshold value 322 may include the different confidences for varying environment condition
Spend score threshold.For example, for road noise (for example, driving in the car), background conversation (for example, bar or restaurant) or minimum
Ambient noise (for example, office environment), confidence threshold value 322 can be different.
In some embodiments, the instruction that system 300 receives additional label terminates training data 304.Additional mark
The instruction of note, which terminates training data 304, can come from user feedback.For example, calculating equipment out of use microphone and handling user's
User can indicate that user does not complete to speak before inquiry.As another example, user can indicate that calculating equipment captures
The language of entire user.The instruction that user feedback audio sample can be added to label terminates in training data 304 for asking
Asking, which terminates model trainer 318, updates housebroken end of inquiry model 302.In the example depicted in fig. 3, user confirms user
Say " what is the weather inCalifornia (how is the weather in California) ".System 300 can be by phase
The instruction that the audio data 324 and complete tag 326 answered are added to label terminates training data 304.
Fig. 4 shows the instantiation procedure 400 terminated for detecting language.In general, process 400 receives the language that user says
Audio data.Process 400 determines that user is likely to the point for having completed to speak, and deactivates microphone.Process 400 will be described
To be executed by the computer system for including one or more computers, for example, system 100 or such as institute in Fig. 2 as shown in Figure 1
The system 200 shown.
System receives audio data (410) corresponding with the language that user says.For example, user can be against mobile electricity
The microphone of words is spoken, and is started: " Order a large cheese pizza. (orders big cheese's Pizza.) " mobile electric
Words can start to generate audio data corresponding with language, and the portion of audio data is transmitted when mobile phone receives language
Point.The size and frequency of the transmission of audio data portion can be related to the network connection speed between mobile phone and system.
Slower network connection may result in mobile phone and less frequent transmit the larger of audio data than being connected to the network faster
Part.
System will use the end of inquiry model from the training of the audio data of complete language and imperfect language to be applied to
Audio data (420).For example, system receives and " order a large cheese pizza. (orders big cheese's Pizza.) " phase
The part of corresponding audio data.When system receives the initial part of audio data, system is by end of inquiry model application
In the initial part of audio data.System continues the further part that end of inquiry model is applied to audio data.In some realities
It applies in mode, system receives the audio data sample including multiple complete language and multiple imperfect language.System uses machine
Study trains end of inquiry model using the audio data of multiple complete language and multiple imperfect language.This training can
To be carried out before the audio data for receiving user spoken utterances.End of inquiry model can be based on neural network, and be configured as
Pass through analytical acoustics characteristics of speech sounds (such as tone, loudness, intonation, acutance, clarity, roughness, unstability and voice speed
Rate) determine whether language is likely to complete.The acoustic voice characteristic that end of inquiry model uses can depend on being used for
The audio sample of training end of inquiry model.
System is based on the end of inquiry using the audio data training from complete language and from imperfect language
Model determines the confidence (430) for a possibility that reflection language is complete language.For example, system is based on and " order a
Large cheese pizza. (orders big cheese's Pizza.) " initial part of corresponding audio data generates confidence level point
Number, also referred to as posteriority (posterior).When system receives the further part of audio data, system will initial and further part
Applied to end of inquiry model, and generate confidence.For example, receiving sound corresponding with " order a (order) "
For frequency after, confidence can be 0.1.It is opposite with " large cheese pizza (big cheese's Pizza) " receiving
After the audio data answered, confidence can be 0.9.
The confidence for reflecting a possibility that language is complete language is compared by system with confidence threshold value
(440).For example, confidence 0.1 and confidence threshold value 0.8 are compared by system, or by confidence 0.9
It is compared with confidence threshold value 0.8.Confidence threshold value can depend on the environment that user speaks and change.Example
Such as, the noisy environment of the automobile such as moved may have lower confidence threshold than the quiet environment of such as office
Value.
System is carried out based on the confidence and confidence threshold value that will reflect a possibility that language is complete language
Compare, determines that language is likely to completely still be likely to incomplete (450).For example, confidence 0.1 can refer to
Show that language is likely to incomplete, and confidence 0.9 can indicate that language is likely to complete.Work as confidence
When lower than confidence threshold value, system can instruct mobile phone that microphone is maintained active state, additional to ensure
Voice is detected.If confidence is equal to or more than confidence threshold value, system can instruct mobile phone to deactivate
Microphone, to ensure that additional audio is not detected.In some embodiments, mobile phone can not deactivate microphone
Instruction in the case where keep microphone open.
The transcription of audio data can be generated in system.For example, text " order a large cheese can be generated in system
Pizza. (big cheese's Pizza is ordered.) " system can by transcription be output to mobile phone or other calculating equipment.System can be with
Movement is executed based on transcription.For example, system can order big cheese's Piza for user.
In some embodiments, it is correctly to which the instruction of language terminates to be just that system can receive transcription from user
True confirmation.For example, system can instruct mobile phone to show the confirmation to big cheese's Piza is ordered.If user's confirmation is ordered
Audio data is labeled as completely, and update end of inquiry mould by purchase then the audio data of corresponding language can be used in system
Type.If the option that mobile phone is presented does not include full content described in user, user cancels the option and repetition
Language.For example, user is it may be said that " Text mom, I ' ll be home soon. (sends short messages, I gets home soon to mother
.) " if system deactivated microphone after " home (family) " before " soon (quickly) ", and option would be presented to send short messages
" I ' ll be home (I will get home) ", then user can cancel the option.System can be used and " text mom, I ' ll
The corresponding audio data of be home (sending short messages to mother, I can go home) " by audio data labeled as imperfect, and more
New end of inquiry model.It is updated by this dynamic, the improved learning process of end of inquiry model can be provided.
In some embodiments, system includes Voice decoder, and Voice decoder is configurable to generate audio data
Transcription, and determine when user is likely to complete to speak.Voice decoder or another part of system can will be received
Audio data is converted into phoneme.Language model can be used to generate the transcription of audio data in Voice decoder, and determines user
When it is likely to complete to speak.
Almost determine whether user is likely to the feelings for having completed to speak simultaneously in Voice decoder and end of inquiry model
Under condition, system can be used the two and determine to generate the instruction of mute microphone (MIC).If Voice decoder and end of inquiry mould
The instruction of type terminates to determine mismatch, then system can choose more than respective threshold larger number or bigger relative populations certainly
It is fixed.
In the case where end of inquiry model determines that user is likely to complete to speak before Voice decoder, system
The instruction for generating mute microphone (MIC) can be determined based on end of inquiry model.When the network connection between mobile phone and system compared with
When slow, in fact it could happen that such case.
In some embodiments, system can determine the network connection speed between system and client device.System
Can be used network speed as determine whether based on Voice decoder or end of inquiry model come to language instruction terminate because
Element.If network speed is 100 kilobit for example per second, once end of inquiry model, which generates instruction, to be terminated to determine, system
Language can be indicated to terminate.If network speed be it is 50 megabits for example per second, system can be decoded with waiting voice
Device, which generates instruction, to be terminated to determine.In some embodiments, system can be after inquiry model generates instruction end decision, In
Maximum amount is waited before terminating to language instruction.For example, maximum amount can be 10 milliseconds.System can determine maximum
Time quantum, so that user experience keeps identical during slow speed network connects when Voice decoder may be delayed by.
In more detail, it is identified in application (such as voice search) in some Streaming voices, quickly and accurately determines user
The inquiry for when having completed them is helpful.In some embodiments, whenever being trained to each frame classification
The silencing of fixed intervals is detected for the speech activity detector (voice activity detector, VAD) of voice or silencing
When, speech recognition application can state end of inquiry.Silencing detection and end of inquiry detection are considered being different task, and
The standard used during VAD training may not be optimal.In some embodiments, silencing detection method may be ignored latent
Acoustics prompt, such as filling sound (filler sound) and passing rate of speaking can indicate that given pause is temporary
When or inquire final (query-final).The present disclosure proposes a kind of modifications, so that silencing detects VAD training standard
It is detected with end of inquiry more closely related.Unidirectional shot and long term memory architecture allows the passing acoustic events of systematic memory, and training
Pattern field systematic learning use any acoustics relevant to prediction future customer intention prompts.As described below, this method changes
It has been apt to delay of the end of inquiry detection of voice search under given accuracy.
In some Streaming voices identification application of such as voice search and conversational system, system is quickly and accurately determined
When user, which completes to speak, is likely to helpful.The task is executed by instruction finisher (endpointer), can be by
Referred to as microphone closer (microphone closer) or end of inquiry detector.Microphone closer or end of inquiry inspection
It surveys when device identification user is likely to complete to speak, and deactivates microphone at that.System receives audio stream and makes a series of
Binary system determines: waiting further voice, or stops listening to and submitting the audio up to the present received for subsequent
Processing.Each of the closing of these microphones or stopping decision are all irrevocable, and are based only upon and receive so far
The audio arrived.In some embodiments, it may be desirable that there is small delay and do not cut off (cut-off) user, wherein prolonging
It is defined as user late to complete the time spoken between system mute microphone (MIC), it is complete in user that cutting user is defined as system
At mute microphone (MIC) before speaking.There may be natural tight relationship (tension) between the two targets.Microphone closes
Close the perception that device performance can influence strongly user to system.For example, microphone closer performance is for the nature in conversational system
Turn-taking (turn-taking) is most important, and user satisfaction is low to be attributed to bad microphone closer performance.It is longer
Delay, which also will increase, to be calculated equipment used in operation microphone and explains the electricity in incoherent background audio, for example, this can shadow
Ring battery life.
Voice activity detection (VAD), otherwise referred to as instruction terminate, and can be every frame audio classification is voice or silencing
The task of (non-voice).In audios all when making all decisions all to system in available offline setting, VAD and microphone
Closing is in practice likely to be same task, because the end of final stage voice is exactly the end of user's inquiry.However, each
The classification of frame is based only upon in the online or streaming setting of preceding audio, and microphone closing may be more difficult: VAD system only needs to examine
Any current silencing is surveyed, and microphone closer can predict whether to have subsequent voice.
A kind of method that microphone is closed is, once VAD system observes the silencing of fixed intervals after voice, just states
End of inquiry (end-of-query, EOQ).In some embodiments, VAD system is by from probability voice activity point
The posteriority of class device is arranged threshold value to obtain.Human listener seem most likely with additional acoustics prompt (such as filling sound,
Speak rhythm or fundamental frequency) know: whether human speakers are intended to continues to speak after given pause.Wheat based on VAD
Gram wind closer ignores the prompt of these end of inquiry acoustics.
In some embodiments, the basis that probability end of inquiry classifier is closed as microphone can be used in system.
Classifier is trained to predict whether user has completed to speak in given time, and is come using the unidirectional framework of unidirectional LSTM
Allow to inform its prediction by passing acoustic events.LSTM and the loss function of modification be it is complementary, this combination provides
The potentiality of the prompt of sound and passing rate of speaking such as are filled in automatic study, these prompts may temporarily predict EOQ with them
The most useful frame separation, and be difficult to understand (pick up on) with simpler model.
In some embodiments, system can be by improving the accuracy of speech activity detector, improving to current heavy
The estimation of silent duration or surmounts the microphone based on VAD and close and consider end of inquiry (sometimes referred to as language knot
Beam) various trials improve microphone closer performance.In some embodiments, the system for executing end of inquiry detection can
To benefit from used EOQ information acoustics and decoder characteristic.System can be by using the sequence model of such as LSTM
(sequential model) extracts better EOQ relevant information from existing acoustic feature.
The following contents includes to training voice activity classifier and being used for microphone closing, training end of inquiry classification
Device and the description for being used for microphone closing, and the discussion of measurement relevant to assessment microphone closer.
Microphone based on voice activity (VAD type) classifier is closed.
In some embodiments, system can execute wheat based on online or streaming probability voice activity classifier is trained
Gram wind is closed.Given acoustic feature sequence vector and model parameter λ, the conditional probability model P used during the training period (y | x, λ)
Specified speech/silencing label sequences y=[yt] (t=1 to T) probability.In some embodiments, in the mark of different time
Signing y1, y2 ... is conditional sampling, although this may not be genuine.Probability P (yt| x, λ), commonly referred to as " posteriority ", by with
The output of acoustic feature sequence vector neural network as input provides.It includes that one or more shot and long terms are remembered that system, which uses,
(LSTM) the recurrence framework of layer is remembered and predicts that present frame is voice or the relevant passing acoustic information of silencing.Recurrence layer is
Unidirectional, to allow whole system to run in streaming fashion.The last layer is 2 grades softmax layers, it is exported by frame sequential
(framewise) voice and silencing posteriority.The directed graph model of display model structure is shown in Fig. 5.In Fig. 5, give
Its father node, solid node is deterministic, and circular node is random, and the node observed is shade.In Fig. 5
In, xt is acoustic feature vector, and yt is binary label.For some voice activity classifiers, yt is voice or silencing, and right
In the end of inquiry classifier proposed, yt is to inquire imperfect (query-not-complete) or the complete (query- of inquiry
complete).Maximum likelihood (i.e. cross entropy) Lai Xunlian can be used in probabilistic model.For trained reference voice/silencing
Sequence label can be by forcing the alignment mankind to refer to transcript (transcript), being voice by all non-silencing phoneme notations
To obtain.Specifically, system can be used 1 as voice label, 0 and be used as silencing.
In order to use housebroken probability voice activity classifier to close for microphone, the posteriority and threshold of frame sequential will be pressed
Value is compared, and is determined with obtaining hard voice/silencing, once and system observe some voices after be Fixed Time Interval
Silencing, microphone is turned off.
Above-mentioned training process can be detected with excitation system and prompt the acoustics that present voice and present silencing distinguish,
But it may ignore and potentially contribute to predict the prompt whether current silencing will have subsequent voice later.Therefore, above-mentioned side
It is considered complete that method, which may cause when language is actually imperfect,.
Microphone based on end of inquiry (EOQ) classifier is closed
In some embodiments of end of inquiry model described herein, system can be based on training probability end of inquiry
Classifier is closed to execute microphone, directly to predict whether user has completed to speak in given time.
Probabilistic model P (y | x, λ) it can have above-mentioned identical structure, but different labels is used during the training period；
Label is to inquire imperfect (label 1) or inquire complete (label 0) now.The reference label sequence used during training can be by
1 sequence and subsequent 0 sequence composition, wherein appearing in the time that ideal microphone is closed for first 0, user is just complete at this time
At speaking.The example of these VAD types and EOQ type sequence label is as shown in table 1.
(inquiry is completely 0, inquires that imperfect is 1) index for table 1:VAD type (silencing 0, voice 1) and EOQ type
(target) example of the difference between, the index use during the classifier training of the language with 10 frames, wherein user
It speaks in the completion of the 8th frame.
In order to use housebroken probability end of inquiry classifier to close for microphone, the posteriority and threshold of frame sequential will be pressed
Value is compared, and is determined with obtaining hard end of inquiry, once and the system complete tag 0 of output inquiry for the first time, microphone is just
It is closed.Hard -threshold (hard thresholding) is a kind of heuristic process, and in some embodiments, just " maximum
It may be suboptimum for change effectiveness ".Hard -threshold can also provide simple effective method, to calculate money used in reducing
Source.
This variation of training data can be detected with excitation system and be helpful in indicate whether user is intended to say more more voice
Any acoustics prompt.For example, end of inquiry classifier has energy if user says " um (uh) " during slightly longer pause
Power (due to LSTM) and tendency (due to the loss function of modification) come remember the acoustic events and reduce in subsequent silencing frame
Inquire complete probability.
The posteriority or probability of sample utterance are as shown in Figure 6.As can be seen that during the period of non-initial silencing, inquiry knot
Beam classifier increases the conviction (belief) of " inquiry is complete " (for example, a possibility that language is complete), but rate may not
It is linear: for example, system terminates language to be relatively uncertain, and posteriority increases in pausing for the first time shown in
It is long slow.The difference of training standard can also from the fact that in find out: voice activity (or VAD) classifier is in an identical manner
The silencing near the beginning and end of language is treated, and end of inquiry classifier treats them then very differently.
Microphone closer measurement
In some embodiments, one group of 4 measurement can be used to see clearly microphone closer performance in system.Measurement is total
Knot is in table 2.Word error probability (word error rate) may be the primary metric of accuracy of speech recognition.It is by microphone
Closer influences, because cutting would generally cut off many words.EP cutoff (cutting) is the language ratio that user is cut off,
For example, system mute microphone (MIC) before the inquiry that user's completion says them.This may be useful measurement amount, because by cutting
Disconnected is negative user experience, and in the application of such as voice search, is cut off and may require repeating entire inquiry.WER
Measure the accuracy of microphone closer together with EP cutoff.EP50 is the intermediate value of all or almost all language
(median) postpone.The concept of this typical user experience when can provide using system.EP90 is the 90% of all language
Delay.This tail portion delay provides the concept that the user experience when system mute microphone (MIC) is slow has more bad lucks.EP50 and
EP90 measures the speed of microphone closer together.For EP cutoff, EP50 and EP90, it is aligned with reference to the pressure of transcript
For determining when user completes to speak.
Table 2: for assessing the measurement of microphone closer performance.
Near the preferred operations point that WER is 11.7, VAD type classifier EP cutoff, 460ms with 4.9%
The EP90 of EP50 and 940ms.EOQ classifier (WER 11.7) has 5.1% EP cutoff, the EP50 and 820ms of 350ms
EP90.Compared with VAD type classifier, typical delay is reduced 110ms by EOQ system, tail portion delay reduces 120ms.
In some embodiments, for assessing the measurement of speech activity detector, such as rate of false alarm (false alarm
Rate it) and false rejection rate (false reject rate) or accuracy and recalls, is closed for assessing microphone as described herein
It may not be highly useful for closing the performance of device.
Fig. 7 shows the example of the calculating equipment 700 and mobile computing device 750 that can be used for implementing technique described herein.
It calculates equipment 700 and is intended to represent various forms of digital computers, such as laptop computer, desktop computer, work station, a
Personal digital assistant, server, blade server, mainframe and other suitable computers.Mobile computing device 750 is intended to represent
Various forms of mobile devices, such as personal digital assistant, cellular phone, smart phone and other similar calculating equipment.This
Component, their connection and relationship and their function shown in text are only intended as example, without being intended to limitation.
Equipment 700 is calculated to include processor 702, memory 704, storage equipment 706, be connected to memory 704 and multiple
The high-speed interface 708 of high-speed expansion ports 710 and the low-speed interface for being connected to low-speed expansion port 714 and storage equipment 706
712.Processor 702, memory 704, storage equipment 706, high-speed interface 708, high-speed expansion ports 710 and low-speed interface 712
Each of use various bus interconnections, and may be mounted on public mainboard or optionally otherwise install.
Processor 702 can handle the instruction for executing in calculating equipment 700, including stores in memory 704 or store and set
Instruction on standby 706, to be shown on external input/output device (display 716 for being such as coupled to high-speed interface 708)
The graphical information of GUI.In other embodiments, multiple processors and/or multiple buses and multiple can optionally be used
Memory and a plurality of types of memories.Furthermore, it is possible to connect multiple calculating equipment, each equipment provides one of necessary operation
Divide (for example, as server group, one group of blade server or multicomputer system).
Memory 704 is calculating 700 inner storag information of equipment.In some embodiments, memory 704 is one or more
A volatile memory cell.In some embodiments, memory 704 is one or more non-volatile memory cells.Storage
Device 704 is also possible to another form of computer-readable medium, such as disk or CD.
Storage equipment 706 can provide massive store to calculate equipment 700.In some embodiments, equipment is stored
706 can be or comprising computer-readable medium, such as floppy device, hard disc apparatus, compact disk equipment or tape unit, flash memory or
Other similar solid storage device or equipment array, including the equipment or other configurations in storage area network.Instruction can
To store in the information carrier.When being run by one or more processing equipments (for example, processor 702), instruction execution one
Or multiple methods, such as those described above method.Instruction can also be such as computer-readable by one or more storage equipment storage
Or machine readable media (for example, memory on memory 704, storage equipment 706 or processor 702).
The management of high-speed interface 708 calculates bandwidth intensive (bandwidth-intensive) operation of equipment 700, and low
Quick access mouth 712 manages lower bandwidth intensive.This function distribution is only example.In some embodiments, high quick access
Mouth 708 is coupled to memory 704, display 716 (for example, by graphics processor or accelerator) and can receive various
The high-speed expansion ports 710 of expansion card.In this embodiment, low-speed interface 712 is coupled to storage equipment 706 and low-speed expansion
Port 714.It may include the low-speed expansion port of various communication port (for example, USB, bluetooth, Ethernet, wireless ethernet)
714 for example can be coupled to one or more input-output apparatus, such as keyboard, pointing device, scanning by network adapter
The network equipment of instrument or such as switch or router.
As shown, calculating equipment 700 can be implemented with many different forms.For example, it may be embodied as standard clothes
Multiple (the multiple times) being engaged in server as device 720 or one group.In addition, it can be such as on knee
Implement in the personal computer of computer 722.It also may be implemented as a part of rack server system 724.It can replace
Change ground, from calculate equipment 700 component can the other assemblies in the mobile device with such as mobile computing device 750 mutually tie
It closes.Each such equipment, which may include, calculates one or more of equipment 700 and mobile computing device 750, and entire
System can be made of the multiple calculating equipment to communicate with one another.
Mobile computing device 750 include processor 752, memory 764, such as display 754 input-output apparatus,
Communication interface 766 and transceiver 768 and other assemblies.Mobile computing device 750 may be equipped with storage equipment, such as micro- drive
Dynamic device or other equipment, to provide additional storage.Processor 752, memory 764, display 754, communication interface 766 and receipts
It sends out each of device 768 and uses various bus interconnections, and several components may be mounted on public mainboard or optionally
Otherwise install.
Processor 752 can run the instruction in mobile computing device 750, including the instruction being stored in memory 764.
Processor 752 may be embodied as the chipset of chip comprising independent multiple analog- and digital- processors.Processor 752 can
To provide the coordination of such as other assemblies of mobile computing device 750, the control of such as user interface, mobile computing device 750
The application of operation and the wireless communication of mobile computing device 750.
Processor 752 can be communicated by the control interface 758 and display interface 756 for being coupled to display 754 with user.
Display 754 can be such as TFT (Thin-Film-Transistor Liquid Crystal Display, thin film transistor (TFT)
Liquid crystal display) display or OLED (Organic Light Emitting Diode, Organic Light Emitting Diode) display, or
Other suitable display technologies.Display interface 756 may include for driving display 754 that figure and other letters is presented to user
The proper circuit of breath.Control interface 758 can be received from user and be ordered, and convert them to submit to processor 752.In addition,
External interface 762 can provide the communication with processor 752 so that mobile computing device 750 can with other equipment into
Row near-field communication.External interface 762 can for example be used for wire communication in some embodiments, or in other embodiments
In for wirelessly communicating, and multiple interfaces also can be used.
Memory 764 stores the information within mobile computing device 750.Memory 764 may be embodied as computer-readable
One or more of medium, one or more volatile memory cells, one or more non-volatile memory cells.It can be with
Extended menory 774 is provided, and mobile computing device 750 is connected to by expansion interface 772, expansion interface 772 may include
Such as SIMM (Single In Line Memory Module, single-in-line memory module) card interface.Extended menory 774
Additional memory space can be provided for mobile computing device 750, or may be the storage of mobile computing device 750 application or
Other information.Specifically, extended menory 774 may include the instruction for executing or supplementing the above process, and can also include
Security information.Thus, for example, extended menory 774 may be provided as the security module of mobile computing device 750, and can
To be programmed with the instruction for using mobile computing device 750 safely is allowed.Furthermore, it is possible to via SIMM card provide security application with
And additional information, such as identification information is placed on SIMM card in a manner of it can not crack.
As described below, memory may include such as flash memory and or NVRAM memory (non-volatile random access storage
Device).In some embodiments, instruction stores in the information carrier.When by one or more processing equipments (for example, processor
752) when running, instruction execution one or more method, such as those described above method.Instruction can also be stored by one or more
Equipment storage, such as one or more computer-readable or machine readable media is (for example, memory 764, extended menory 774
Or the memory on processor 752).In some embodiments, instruction can for example pass through transceiver 768 or external interface
762 are received in the form of transmitting signal.
Mobile computing device 750 can be carried out wireless communication by communication interface 766, and communication interface 766 if necessary may be used
To include digital signal processing circuit.Communication interface 766 can provide the communication under various modes or agreement, such as GSM speech
Call (global system for mobile communications), SMS (short message service), EMS (enhancing messaging service) or MMS message (Multimedia Message
Service), CDMA (CDMA), TDMA (time division multiple acess), PDC (personal digital cellular), WCDMA (wideband code division multiple access),
CDMA2000 or GPRS (general packet radio service) and other.This communication can be for example by using the transceiver of radio frequency
768 carry out.Furthermore, it is possible to carry out short haul connection, such as using bluetooth, Wi-Fi or other such transceivers.In addition, GPS
(global positioning system) receiver module 770 can provide the additional relevant nothing of navigation and position to mobile computing device 750
Line number evidence, these data can optionally be run on mobile computing device 750 using.
Mobile computing device 750 can also use audio codec 760 to carry out audible communication, audio codec 760
Verbal information can be received from user and converts thereof into available digital information.Audio codec 760 equally can be such as
Audible sound is generated for user by loudspeaker, such as in the earpiece (handset) of mobile computing device 750.This sound
It may include the sound from speech phone call, may include the sound (for example, speech information, music file etc.) of record,
It and can also include by being operated on mobile computing device 750 using sound generated.
As shown, mobile computing device 750 can be implemented with many different forms.For example, it may be implemented as
Cellular phone 780.It also may be implemented as one of smart phone 782, personal digital assistant or other similar mobile device
Point.
The various embodiments of system described herein and technology can in Fundamental Digital Circuit, integrated circuit, specially set
Implement in the ASIC (specific integrated circuit) of meter, computer hardware, firmware, software and/or combination thereof.These different embodiment party
Formula may include the embodiments in one or more computer programs that can be run and/or can be explained on programmable systems,
Programmable system includes at least one programmable processor, programmable processor can be it is dedicated or general, be coupled into from
Storage system, at least one input equipment and at least one output equipment receive data and instruction, and to storage system, at least
One input equipment and at least one output equipment send data and instruction.
These computer programs (also referred to as program, software, software application or code) include for programmable processor
Machine instruction, and can with high level procedural and/or object-oriented programming language and/or compilation/machine language come
Implement.As it is used herein, term machine readable media and computer-readable medium refer to for mentioning to programmable processor
For any computer program product of machine instruction and/or data, device and/or equipment (for example, disk, CD, memory,
Programmable logic device (PLD)), including receive machine instruction machine readable medium as a machine-readable signal.Term machine
Readable signal refers to for providing any signal of machine instruction and/or data to programmable processor.
In order to provide the interaction with user, system described herein and technology can be implemented on computers, the computer
With for showing the display equipment (for example, CRT (cathode-ray tube) or LCD (liquid crystal display) monitor) of information to user
And user can provide the keyboard and pointing device (for example, mouse or trackball) of input to computer by it.It can also be with
Interaction with user is provided using other kinds of equipment；For example, the feedback for being supplied to user may be any type of sense
Feel feedback (for example, visual feedback, audio feedback or touch feedback)；And it can receive in any form from the user defeated
Enter, including sound, voice or tactile input.
System described herein and technology can be implemented in computing systems, the computing system include aft-end assembly (for example,
As data server), perhaps including middleware component (for example, application server) or including front end assemblies (for example, tool
There is the client computer of graphic user interface or web browser, user can pass through itself and system described herein and technology
Embodiment interact) or this aft-end assembly, middleware component or front end assemblies any combination.The group of system
Part can be interconnected by any form or the digital data communications (for example, communication network) of medium.The example of communication network includes
Local area network (LAN), wide area network (WAN) and internet.
Computing system may include client and server.Client and server is generally remote from each other, and usually logical
Communication network is crossed to interact.The relationship of client and server be due on respective computer operation and each other it
Between with client-server relation computer program generate.
Although some embodiments are described in detail above, other modifications are also possible.Although for example, visitor
The application of family end is described as (multiple) agencies of access, but in other embodiments, (multiple) agencies can by by one or
The other application that multiple processors are implemented uses, the application that such as runs on one or more servers.In addition, in figure
The logic flow of description do not require shown in specific order or the order of sequence realize desired result.Furthermore, it is possible to mention
For other movements, or elimination can be acted from described process, and can be added to described system or from being retouched
Other assemblies are removed in the system stated.Therefore, other embodiments show also within the scope of the claims.
Claims (20)
1. a kind of the method implemented by computer, comprising:
Receive audio data corresponding with the language that user says；
End of inquiry model is applied to the audio data, the end of inquiry model (i) is configured to determine that reflection language
The confidence for a possibility that being complete language and (ii) use the audio from complete language and from imperfect language
Data are trained to；
Based on the end of inquiry model is applied, the confidence for a possibility that reflection language is complete language is determined, it is described
End of inquiry model (i) is configured to determine that the confidence for a possibility that reflection language is complete language and (ii) use
Audio data from complete language and from imperfect language is trained to；
The confidence for reflecting a possibility that language is complete language is compared with confidence threshold value；
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Language is likely to complete or is likely to incomplete；And
It is likely to complete based on determining language or is likely to incomplete, providing will be connect for the instruction of output with (i)
The microphone for receiving language is maintained at active state or (ii) deactivates the microphone for receiving language.
2. according to the method described in claim 1, including:
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Confidence meets confidence threshold value,
Wherein it is determined that language be likely to complete or be likely to incomplete step include, based on determine confidence
Meet confidence threshold value and determine that language is to be likely to complete,
Wherein, provide that the microphone for receiving language is maintained at active state with (i) by the instruction for output or (ii) is deactivated
The step of receiving the microphone of language includes providing the instruction for output to deactivate the microphone for receiving language,
The transcription of audio data is generated, and
The transcription is provided for exporting.
3. method according to claim 1 or 2, comprising:
Confirmation user, which is received, from user completes the data spoken；And
The data spoken are completed based on confirmation user is received, update the end of inquiry model.
4. according to the method described in claim 1, including:
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Confidence is unsatisfactory for confidence threshold value,
Wherein it is determined that language be likely to complete or be likely to incomplete step include, based on determine confidence
Be unsatisfactory for confidence threshold value and determine language be likely to incomplete, and
Wherein, provide that the microphone for receiving language is maintained at active state with (i) by the instruction for output or (ii) is deactivated
The step of receiving the microphone of language includes providing the instruction for output so that microphone is maintained at active state.
5. method according to any of claims 1-4, comprising:
Receive the audio data of multiple complete language and multiple imperfect language；And
It is trained using machine learning, using the audio data of the multiple complete language and the multiple imperfect language described
End of inquiry model.
6. method according to any one of claims 1-5, wherein the end of inquiry model is configured as based on language
Acoustic voice characteristic determine that the confidence for reflecting a possibility that language is complete language, the acoustic voice are special
Property includes tone, loudness, intonation, acutance, clarity, roughness, unstability and voice rate.
7. method according to claim 1 to 6, comprising:
Determine that Voice decoder determines that language is likely to complete or is likely to incomplete not yet, the tone decoding
Device is configurable to generate the transcription of audio data and is configured to determine that language is likely to complete or is likely to endless
Whole,
Wherein it is determined that language is likely to complete or is likely to incomplete be based only upon reflection language be complete language
The confidence of possibility is compared with confidence threshold value.
8. according to the method described in claim 7, wherein, the Voice decoder determines that language is very may be used using language model
Can completely is still likely to incomplete.
9. method according to claim 1 to 6, comprising:
Determine that Voice decoder has determined that language is likely to complete or is likely to incomplete, the Voice decoder
It is configurable to generate the transcription of audio data and is configured to determine that language is likely to complete or is likely to imperfect
,
Wherein it is determined that language is likely to complete or is likely to incomplete be that will reflect that language is completely to talk about based on (i)
The confidence of a possibility that language is compared with confidence threshold value and (ii) described Voice decoder determines language
It is likely to complete or is likely to incomplete.
10. a kind of system, comprising:
One or more computers；And
One or more storage equipment, store instruction, described instruction are running Shi Kecao by one or more of computers
Make so that it includes operation below that one or more of computers, which execute:
Receive audio data corresponding with the language that user says；
End of inquiry model is applied to audio data, the end of inquiry model (i) is configured to determine that reflection language has been
The confidence of a possibility that whole language and (ii) use the audio data from complete language and from imperfect language
It is trained to；
The confidence for a possibility that reflection language is complete language is determined based on the application end of inquiry model, it is described
End of inquiry model (i) is configured to determine that the confidence for a possibility that reflection language is complete language and (ii) use
Audio data from complete language and from imperfect language is trained to；
The confidence for reflecting a possibility that language is complete language is compared with confidence threshold value；
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Language is likely to complete or is likely to incomplete；And
It is likely to complete based on determining language or is likely to incomplete, providing will be connect for the instruction of output with (i)
The microphone for receiving language is maintained at active state or (ii) deactivates the microphone for receiving language.
11. system according to claim 10, wherein the operation further include:
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Confidence meets confidence threshold value,
Wherein it is determined that language is likely to complete or is likely to incomplete include meeting based on determining confidence
Confidence threshold value and determine that language is to be likely to complete,
Wherein, provide that the microphone for receiving language is maintained at active state with (i) by the instruction for output or (ii) is deactivated
The microphone for receiving language includes providing the instruction for output to deactivate the microphone for receiving language,
The transcription of audio data is generated, and
The transcription is provided for exporting.
12. according to claim 10 or claim 11 described in system, wherein the operation further include:
Confirmation user, which is received, from user completes the data spoken；And
The data spoken are completed based on confirmation user is received, update end of inquiry model.
13. system according to claim 10, wherein the operation further include:
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Confidence is unsatisfactory for confidence threshold value,
Wherein it is determined that language is likely to complete or is likely to incomplete include being discontented with based on determining confidence
Sufficient confidence threshold value and determine language be likely to incomplete, and
Wherein, provide that the microphone for receiving language is maintained at active state with (i) by the instruction for output or (ii) is deactivated
The microphone for receiving language includes providing the instruction for output so that microphone is maintained at active state.
14. system described in any one of 0-13 according to claim 1, wherein the operation further include:
Receive the audio data of multiple complete language and multiple imperfect language；And
It is trained using machine learning, using the audio data of the multiple complete language and the multiple imperfect language described
End of inquiry model.
15. system described in any one of 0-14 according to claim 1, wherein the end of inquiry model is configured as being based on
The acoustic voice characteristic of language reflects that the confidence for a possibility that language is complete language, the acoustic voice are special to determine
Property includes tone, loudness, intonation, acutance, clarity, roughness, unstability and voice rate.
16. system described in any one of 0-15 according to claim 1, wherein the operation further include:
Determine that Voice decoder determines that language is likely to complete or is likely to incomplete not yet, the tone decoding
Device is configurable to generate the transcription of audio data and is configured to determine that language is likely to complete or is likely to endless
Whole,
Wherein it is determined that language is likely to complete or is likely to incomplete be based only upon reflection language be complete language
The confidence of possibility is compared with confidence threshold value.
17. system according to claim 16, wherein the Voice decoder determines that language is very using language model
Can completely be still likely to incomplete.
18. system described in any one of 0-15 according to claim 1, wherein the operation further include:
Determine that Voice decoder has determined that language is likely to complete or is likely to incomplete, the Voice decoder
It is configurable to generate the transcription of audio data and is configured to determine that language is likely to complete or is likely to imperfect
,
Wherein it is determined that language is likely to complete or is likely to incomplete be that will reflect that language is completely to talk about based on (i)
The confidence of a possibility that language is compared with confidence threshold value and (ii) described Voice decoder determines language
It is likely to complete or is likely to incomplete.
19. a kind of non-transitory computer-readable medium for storing software, the software includes can be by one or more computers
The instruction of operation, it includes operation below that described instruction, which executes one or more of computers:
Receive audio data corresponding with the language that user says；
End of inquiry model is applied to audio data, the end of inquiry model (i) is configured to determine that reflection language has been
The confidence of a possibility that whole language and (ii) use the audio data from complete language and from imperfect language
It is trained to；
The confidence for a possibility that reflection language is complete language is determined based on the application end of inquiry model, it is described
End of inquiry model (i) is configured to determine that the confidence for a possibility that reflection language is complete language and (ii) use
Audio data from complete language and from imperfect language is trained to；
The confidence for reflecting a possibility that language is complete language is compared with confidence threshold value；
Based on the confidence for reflecting a possibility that language is complete language to be compared with confidence threshold value, determine
Language is likely to complete or is likely to incomplete；And
It is likely to complete based on determining language or is likely to incomplete, providing will be connect for the instruction of output with (i)
The microphone for receiving language is maintained at active state or (ii) deactivates the microphone for receiving language.
20. medium according to claim 19, wherein the operation further include:
Determine that Voice decoder determines that language is likely to complete or is likely to incomplete not yet, the tone decoding
Device is configurable to generate the transcription of audio data and is configured to determine that language is likely to complete or is likely to endless
Whole,
Wherein it is determined that language is likely to complete or is likely to incomplete be based only upon reflection language be complete language
The confidence of possibility is compared with confidence threshold value.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202011326900.7A CN112581982A (en) | 2017-06-06 | 2018-06-06 | End of query detection |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762515771P | 2017-06-06 | 2017-06-06 | |
US62/515,771 | 2017-06-06 | ||
PCT/US2018/036188 WO2018226779A1 (en) | 2017-06-06 | 2018-06-06 | End of query detection |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202011326900.7A Division CN112581982A (en) | 2017-06-06 | 2018-06-06 | End of query detection |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110520925A true CN110520925A (en) | 2019-11-29 |
CN110520925B CN110520925B (en) | 2020-12-15 |
Family
ID=62815131
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202011326900.7A Pending CN112581982A (en) | 2017-06-06 | 2018-06-06 | End of query detection |
CN201880021601.7A Active CN110520925B (en) | 2017-06-06 | 2018-06-06 | End of query detection |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202011326900.7A Pending CN112581982A (en) | 2017-06-06 | 2018-06-06 | End of query detection |
Country Status (4)
Country | Link |
---|---|
US (2) | US10593352B2 (en) |
EP (2) | EP3577645B1 (en) |
CN (2) | CN112581982A (en) |
WO (1) | WO2018226779A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111326140A (en) * | 2020-03-12 | 2020-06-23 | 科大讯飞股份有限公司 | Speech recognition result discrimination method, correction method, device, equipment and storage medium |
Families Citing this family (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR102441063B1 (en) * | 2017-06-07 | 2022-09-06 | 현대자동차주식회사 | Apparatus for detecting adaptive end-point, system having the same and method thereof |
KR102429498B1 (en) * | 2017-11-01 | 2022-08-05 | 현대자동차주식회사 | Device and method for recognizing voice of vehicle |
US10777189B1 (en) * | 2017-12-05 | 2020-09-15 | Amazon Technologies, Inc. | Dynamic wakeword detection |
US11416207B2 (en) * | 2018-06-01 | 2022-08-16 | Deepmind Technologies Limited | Resolving time-delays using generative models |
CN108962227B (en) * | 2018-06-08 | 2020-06-30 | 百度在线网络技术（北京）有限公司 | Voice starting point and end point detection method and device, computer equipment and storage medium |
US10950223B2 (en) * | 2018-08-20 | 2021-03-16 | Accenture Global Solutions Limited | System and method for analyzing partial utterances |
US10951635B2 (en) * | 2018-09-20 | 2021-03-16 | Cisco Technology, Inc. | System and method to estimate network disruption index |
US11138334B1 (en) * | 2018-10-17 | 2021-10-05 | Medallia, Inc. | Use of ASR confidence to improve reliability of automatic audio redaction |
US10957317B2 (en) * | 2018-10-18 | 2021-03-23 | Ford Global Technologies, Llc | Vehicle language processing |
US11140110B2 (en) * | 2018-10-26 | 2021-10-05 | International Business Machines Corporation | Adaptive dialog strategy for multi turn conversation systems using interaction sequences |
US11133026B2 (en) * | 2019-01-04 | 2021-09-28 | International Business Machines Corporation | Natural language processor for using speech to cognitively detect and analyze deviations from a baseline |
CA3147589A1 (en) * | 2019-07-15 | 2021-01-21 | Axon Enterprise, Inc. | Methods and systems for transcription of audio data |
US11495240B1 (en) * | 2019-07-23 | 2022-11-08 | Amazon Technologies, Inc. | Management of local devices |
US11392401B1 (en) | 2019-07-23 | 2022-07-19 | Amazon Technologies, Inc. | Management of and resource allocation for local devices |
CN112581938B (en) * | 2019-09-30 | 2024-04-09 | 华为技术有限公司 | Speech breakpoint detection method, device and equipment based on artificial intelligence |
CN112863496B (en) * | 2019-11-27 | 2024-04-02 | 阿里巴巴集团控股有限公司 | Voice endpoint detection method and device |
US11769004B2 (en) * | 2020-01-02 | 2023-09-26 | International Business Machines Corporation | Goal-oriented conversation with code-mixed language |
CN112287860B (en) * | 2020-11-03 | 2022-01-07 | 北京京东乾石科技有限公司 | Training method and device of object recognition model, and object recognition method and system |
CN112749509B (en) * | 2020-12-30 | 2022-06-10 | 西华大学 | Intelligent substation fault diagnosis method based on LSTM neural network |
US20220207392A1 (en) * | 2020-12-31 | 2022-06-30 | International Business Machines Corporation | Generating summary and next actions in real-time for multiple users from interaction records in natural language |
US11705125B2 (en) * | 2021-03-26 | 2023-07-18 | International Business Machines Corporation | Dynamic voice input detection for conversation assistants |
KR102570908B1 (en) * | 2021-04-13 | 2023-08-28 | 주식회사 카카오 | Speech end point detection apparatus, program and control method thereof |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2014134675A (en) * | 2013-01-10 | 2014-07-24 | Ntt Docomo Inc | Function execution system and speech example output method |
CN104050256A (en) * | 2014-06-13 | 2014-09-17 | 西安蒜泥电子科技有限责任公司 | Initiative study-based questioning and answering method and questioning and answering system adopting initiative study-based questioning and answering method |
WO2015073071A1 (en) * | 2013-11-13 | 2015-05-21 | Google Inc. | Envelope comparison for utterance detection |
CN105190746A (en) * | 2013-05-07 | 2015-12-23 | 高通股份有限公司 | Method and apparatus for detecting a target keyword |
US20150371665A1 (en) * | 2014-06-19 | 2015-12-24 | Apple Inc. | Robust end-pointing of speech signals using speaker recognition |
Family Cites Families (84)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4713777A (en) | 1984-05-27 | 1987-12-15 | Exxon Research And Engineering Company | Speech recognition method having noise immunity |
US4980918A (en) | 1985-05-09 | 1990-12-25 | International Business Machines Corporation | Speech recognition system with efficient storage and rapid assembly of phonological graphs |
JPH07104676B2 (en) | 1988-02-29 | 1995-11-13 | 日本電信電話株式会社 | Adaptive voicing end detection method |
JP2764343B2 (en) | 1990-09-07 | 1998-06-11 | 富士通株式会社 | Clause / phrase boundary extraction method |
JP2924555B2 (en) | 1992-10-02 | 1999-07-26 | 三菱電機株式会社 | Speech recognition boundary estimation method and speech recognition device |
JP3004883B2 (en) | 1994-10-18 | 2000-01-31 | ケイディディ株式会社 | End call detection method and apparatus and continuous speech recognition method and apparatus |
JP3611223B2 (en) | 1996-08-20 | 2005-01-19 | 株式会社リコー | Speech recognition apparatus and method |
JP3069531B2 (en) | 1997-03-14 | 2000-07-24 | 日本電信電話株式会社 | Voice recognition method |
US6018708A (en) | 1997-08-26 | 2000-01-25 | Nortel Networks Corporation | Method and apparatus for performing speech recognition utilizing a supplementary lexicon of frequently used orthographies |
US6453292B2 (en) | 1998-10-28 | 2002-09-17 | International Business Machines Corporation | Command boundary identifier for conversational natural language |
US6321197B1 (en) | 1999-01-22 | 2001-11-20 | Motorola, Inc. | Communication device and method for endpointing speech utterances |
WO2000046789A1 (en) | 1999-02-05 | 2000-08-10 | Fujitsu Limited | Sound presence detector and sound presence/absence detecting method |
US6324509B1 (en) | 1999-02-08 | 2001-11-27 | Qualcomm Incorporated | Method and apparatus for accurate endpointing of speech in the presence of noise |
JP4341111B2 (en) | 1999-08-18 | 2009-10-07 | ソニー株式会社 | Recording / reproducing apparatus and recording / reproducing method |
JP2001219893A (en) | 2000-02-08 | 2001-08-14 | Nkk Corp | Icebreaker |
IT1315917B1 (en) | 2000-05-10 | 2003-03-26 | Multimedia Technologies Inst M | VOICE ACTIVITY DETECTION METHOD AND METHOD FOR LASEGMENTATION OF ISOLATED WORDS AND RELATED APPARATUS. |
US6810375B1 (en) | 2000-05-31 | 2004-10-26 | Hapax Limited | Method for segmentation of text |
JP4105841B2 (en) | 2000-07-11 | 2008-06-25 | インターナショナル・ビジネス・マシーンズ・コーポレーション | Speech recognition method, speech recognition apparatus, computer system, and storage medium |
US7277853B1 (en) | 2001-03-02 | 2007-10-02 | Mindspeed Technologies, Inc. | System and method for a endpoint detection of speech for improved speech recognition in noisy environments |
US7177810B2 (en) | 2001-04-10 | 2007-02-13 | Sri International | Method and apparatus for performing prosody-based endpointing of a speech signal |
US7668718B2 (en) | 2001-07-17 | 2010-02-23 | Custom Speech Usa, Inc. | Synchronized pattern recognition source data processed by manual or automatic means for creation of shared speaker-dependent speech user profile |
JP3557605B2 (en) | 2001-09-19 | 2004-08-25 | インターナショナル・ビジネス・マシーンズ・コーポレーション | Sentence segmentation method, sentence segmentation processing device using the same, machine translation device, and program |
US20050108011A1 (en) | 2001-10-04 | 2005-05-19 | Keough Steven J. | System and method of templating specific human voices |
AU2002240872A1 (en) | 2001-12-21 | 2003-07-09 | Telefonaktiebolaget Lm Ericsson (Publ) | Method and device for voice recognition |
US7035807B1 (en) | 2002-02-19 | 2006-04-25 | Brittain John W | Sound on sound-annotations |
US7665024B1 (en) | 2002-07-22 | 2010-02-16 | Verizon Services Corp. | Methods and apparatus for controlling a user interface based on the emotional state of a user |
JP4433704B2 (en) | 2003-06-27 | 2010-03-17 | 日産自動車株式会社 | Speech recognition apparatus and speech recognition program |
US7756709B2 (en) | 2004-02-02 | 2010-07-13 | Applied Voice & Speech Technologies, Inc. | Detection of voice inactivity within a sound stream |
US7610199B2 (en) | 2004-09-01 | 2009-10-27 | Sri International | Method and apparatus for obtaining complete speech signals for speech recognition applications |
US7809569B2 (en) | 2004-12-22 | 2010-10-05 | Enterprise Integration Group, Inc. | Turn-taking confidence |
US7689423B2 (en) | 2005-04-13 | 2010-03-30 | General Motors Llc | System and method of providing telematically user-optimized configurable audio |
US20080294433A1 (en) | 2005-05-27 | 2008-11-27 | Minerva Yeung | Automatic Text-Speech Mapping Tool |
US8170875B2 (en) | 2005-06-15 | 2012-05-01 | Qnx Software Systems Limited | Speech end-pointer |
US7518631B2 (en) | 2005-06-28 | 2009-04-14 | Microsoft Corporation | Audio-visual control system |
JP4732030B2 (en) | 2005-06-30 | 2011-07-27 | キヤノン株式会社 | Information processing apparatus and control method thereof |
US8756057B2 (en) | 2005-11-02 | 2014-06-17 | Nuance Communications, Inc. | System and method using feedback speech analysis for improving speaking ability |
US7831425B2 (en) | 2005-12-15 | 2010-11-09 | Microsoft Corporation | Time-anchored posterior indexing of speech |
US7603633B2 (en) | 2006-01-13 | 2009-10-13 | Microsoft Corporation | Position-based multi-stroke marking menus |
KR100762636B1 (en) | 2006-02-14 | 2007-10-01 | 삼성전자주식회사 | System and nethod for controlling voice detection of network terminal |
JP4906379B2 (en) * | 2006-03-22 | 2012-03-28 | 富士通株式会社 | Speech recognition apparatus, speech recognition method, and computer program |
WO2007121548A1 (en) | 2006-04-24 | 2007-11-01 | Bce Inc. | Method, system and apparatus for conveying an event reminder |
JP5030534B2 (en) | 2006-11-01 | 2012-09-19 | 出光興産株式会社 | Aminodibenzofluorene derivative and organic electroluminescence device using the same |
CN101197131B (en) | 2006-12-07 | 2011-03-30 | 积体数位股份有限公司 | Accidental vocal print password validation system, accidental vocal print cipher lock and its generation method |
CN101636784B (en) | 2007-03-20 | 2011-12-28 | 富士通株式会社 | Speech recognition system, and speech recognition method |
US7881933B2 (en) | 2007-03-23 | 2011-02-01 | Verizon Patent And Licensing Inc. | Age determination using speech |
US8364485B2 (en) | 2007-08-27 | 2013-01-29 | International Business Machines Corporation | Method for automatically identifying sentence boundaries in noisy conversational data |
KR100925479B1 (en) | 2007-09-19 | 2009-11-06 | 한국전자통신연구원 | The method and apparatus for recognizing voice |
WO2009101837A1 (en) | 2008-02-13 | 2009-08-20 | Nec Corporation | Mark insertion device and mark insertion method |
CA2680304C (en) | 2008-09-25 | 2017-08-22 | Multimodal Technologies, Inc. | Decoding-time prediction of non-verbalized tokens |
DE102008058883B4 (en) | 2008-11-26 | 2023-07-27 | Lumenvox Corporation | Method and arrangement for controlling user access |
US8494857B2 (en) | 2009-01-06 | 2013-07-23 | Regents Of The University Of Minnesota | Automatic measurement of speech fluency |
EP2243734B1 (en) | 2009-04-23 | 2011-10-26 | Müller Martini Holding AG | Method for turning printed products transported on a conveyor path in a supply stream |
US9173582B2 (en) | 2009-04-24 | 2015-11-03 | Advanced Brain Monitoring, Inc. | Adaptive performance trainer |
US8412525B2 (en) | 2009-04-30 | 2013-04-02 | Microsoft Corporation | Noise robust speech classifier ensemble |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
CN102073635B (en) | 2009-10-30 | 2015-08-26 | 索尼株式会社 | Program endpoint time detection apparatus and method and programme information searching system |
KR101622111B1 (en) | 2009-12-11 | 2016-05-18 | 삼성전자 주식회사 | Dialog system and conversational method thereof |
KR101377459B1 (en) | 2009-12-21 | 2014-03-26 | 한국전자통신연구원 | Apparatus for interpreting using utterance similarity measure and method thereof |
EP2561508A1 (en) | 2010-04-22 | 2013-02-27 | Qualcomm Incorporated | Voice activity detection |
WO2011151502A1 (en) | 2010-06-02 | 2011-12-08 | Nokia Corporation | Enhanced context awareness for speech recognition |
US8762150B2 (en) | 2010-09-16 | 2014-06-24 | Nuance Communications, Inc. | Using codec parameters for endpoint detection in speech recognition |
US20120089392A1 (en) | 2010-10-07 | 2012-04-12 | Microsoft Corporation | Speech recognition user interface |
US8650029B2 (en) * | 2011-02-25 | 2014-02-11 | Microsoft Corporation | Leveraging speech recognizer feedback for voice activity detection |
US9763617B2 (en) | 2011-08-02 | 2017-09-19 | Massachusetts Institute Of Technology | Phonologically-based biomarkers for major depressive disorder |
ES2409530B1 (en) | 2011-10-14 | 2014-05-14 | Telefónica, S.A. | METHOD FOR MANAGING THE RECOGNITION OF THE AUDIO CALL SPEAK |
US9043413B2 (en) | 2011-11-15 | 2015-05-26 | Yahoo! Inc. | System and method for extracting, collecting, enriching and ranking of email objects |
US9202086B1 (en) | 2012-03-30 | 2015-12-01 | Protegrity Corporation | Tokenization in a centralized tokenization environment |
TWI474317B (en) | 2012-07-06 | 2015-02-21 | Realtek Semiconductor Corp | Signal processing apparatus and signal processing method |
US8600746B1 (en) | 2012-09-10 | 2013-12-03 | Google Inc. | Speech recognition parameter adjustment |
CN102982811B (en) * | 2012-11-24 | 2015-01-14 | 安徽科大讯飞信息科技股份有限公司 | Voice endpoint detection method based on real-time decoding |
US9015048B2 (en) | 2012-11-30 | 2015-04-21 | At&T Intellectual Property I, L.P. | Incremental speech recognition for dialog systems |
US20150348538A1 (en) | 2013-03-14 | 2015-12-03 | Aliphcom | Speech summary and action item generation |
US10121493B2 (en) | 2013-05-07 | 2018-11-06 | Veveo, Inc. | Method of and system for real time feedback in an incremental speech input interface |
US9437186B1 (en) | 2013-06-19 | 2016-09-06 | Amazon Technologies, Inc. | Enhanced endpoint detection for speech recognition |
GB2519117A (en) | 2013-10-10 | 2015-04-15 | Nokia Corp | Speech processing |
US8843369B1 (en) | 2013-12-27 | 2014-09-23 | Google Inc. | Speech endpointing based on voice profile |
US9418660B2 (en) | 2014-01-15 | 2016-08-16 | Cisco Technology, Inc. | Crowd sourcing audio transcription via re-speaking |
US9311932B2 (en) | 2014-01-23 | 2016-04-12 | International Business Machines Corporation | Adaptive pause detection in speech recognition |
US9607613B2 (en) | 2014-04-23 | 2017-03-28 | Google Inc. | Speech endpointing based on word comparisons |
US9530412B2 (en) | 2014-08-29 | 2016-12-27 | At&T Intellectual Property I, L.P. | System and method for multi-agent architecture for interactive machines |
US9520128B2 (en) * | 2014-09-23 | 2016-12-13 | Intel Corporation | Frame skipping with extrapolation and outputs on demand neural network for automatic speech recognition |
US9666192B2 (en) | 2015-05-26 | 2017-05-30 | Nuance Communications, Inc. | Methods and apparatus for reducing latency in speech recognition applications |
US10121471B2 (en) * | 2015-06-29 | 2018-11-06 | Amazon Technologies, Inc. | Language model speech endpointing |
CN105261357B (en) * | 2015-09-15 | 2016-11-23 | 百度在线网络技术（北京）有限公司 | Sound end detecting method based on statistical model and device |
-
2018
- 2018-06-06 US US16/001,140 patent/US10593352B2/en active Active
- 2018-06-06 CN CN202011326900.7A patent/CN112581982A/en active Pending
- 2018-06-06 WO PCT/US2018/036188 patent/WO2018226779A1/en unknown
- 2018-06-06 EP EP18737063.0A patent/EP3577645B1/en active Active
- 2018-06-06 EP EP22178779.9A patent/EP4083998A1/en active Pending
- 2018-06-06 CN CN201880021601.7A patent/CN110520925B/en active Active
-
2020
- 2020-01-31 US US16/778,222 patent/US11551709B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2014134675A (en) * | 2013-01-10 | 2014-07-24 | Ntt Docomo Inc | Function execution system and speech example output method |
CN105190746A (en) * | 2013-05-07 | 2015-12-23 | 高通股份有限公司 | Method and apparatus for detecting a target keyword |
WO2015073071A1 (en) * | 2013-11-13 | 2015-05-21 | Google Inc. | Envelope comparison for utterance detection |
CN104050256A (en) * | 2014-06-13 | 2014-09-17 | 西安蒜泥电子科技有限责任公司 | Initiative study-based questioning and answering method and questioning and answering system adopting initiative study-based questioning and answering method |
US20150371665A1 (en) * | 2014-06-19 | 2015-12-24 | Apple Inc. | Robust end-pointing of speech signals using speaker recognition |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111326140A (en) * | 2020-03-12 | 2020-06-23 | 科大讯飞股份有限公司 | Speech recognition result discrimination method, correction method, device, equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
WO2018226779A1 (en) | 2018-12-13 |
CN110520925B (en) | 2020-12-15 |
US20180350395A1 (en) | 2018-12-06 |
EP4083998A1 (en) | 2022-11-02 |
EP3577645A1 (en) | 2019-12-11 |
CN112581982A (en) | 2021-03-30 |
US10593352B2 (en) | 2020-03-17 |
US11551709B2 (en) | 2023-01-10 |
US20200168242A1 (en) | 2020-05-28 |
EP3577645B1 (en) | 2022-08-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110520925A (en) | End of inquiry detection | |
US11361768B2 (en) | Utterance classifier | |
US11922095B2 (en) | Device selection for providing a response | |
US10699702B2 (en) | System and method for personalization of acoustic models for automatic speech recognition | |
JP6435312B2 (en) | Speech recognition using parallel recognition tasks. | |
US20230230572A1 (en) | End-to-end speech conversion | |
US11355098B1 (en) | Centralized feedback service for performance of virtual assistant | |
US10854186B1 (en) | Processing audio data received from local devices | |
US20220310082A1 (en) | Contextual tagging and biasing of grammars inside word lattices | |
CN116417003A (en) | Voice interaction system, method, electronic device and storage medium | |
US20240071408A1 (en) | Acoustic event detection | |
KR20210114480A (en) | automatic call system | |
US11915690B1 (en) | Automatic speech recognition | |
US20210350802A1 (en) | Method and system for performing speech recognition in an electronic device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |