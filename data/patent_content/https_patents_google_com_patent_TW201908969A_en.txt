TW201908969A - Code conversion to improve image processor execution time efficiency - Google Patents
Code conversion to improve image processor execution time efficiency Download PDFInfo
- Publication number
- TW201908969A TW201908969A TW107104240A TW107104240A TW201908969A TW 201908969 A TW201908969 A TW 201908969A TW 107104240 A TW107104240 A TW 107104240A TW 107104240 A TW107104240 A TW 107104240A TW 201908969 A TW201908969 A TW 201908969A
- Authority
- TW
- Taiwan
- Prior art keywords
- image
- image processing
- data
- image data
- kernel
- Prior art date
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5005—Allocation of resources, e.g. of the central processing unit [CPU] to service a request
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
- G06F9/4881—Scheduling strategies for dispatcher, e.g. round robin, multi-level priority queues
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/20—Processor architectures; Processor configuration, e.g. pipelining
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/60—Memory management
Abstract
Description
本發明之領域大體上係關於影像處理器且更具體言之，係關於用以改善影像處理器執行時間效率之程式碼轉換。The field of the present invention relates generally to image processors and, more specifically, to code conversion to improve the execution time efficiency of image processors.
影像處理通常涉及處理組織成一陣列之像素值。此處，一空間組織二維陣列擷取影像之二維性質(額外維度可包含時間(例如，二維影像之一序列)及資料類型(例如，色彩))。在一典型案例中，由已產生一靜止影像或一系列圖框之一攝影機提供陣列式像素值以擷取運動影像。傳統影像處理器通常落於兩個極端之任一端上。 一第一極端執行如在一通用處理器或似通用處理器(例如，具有向量指令增強之一通用處理器)上執行之軟體程式之影像處理任務。儘管第一極端通常提供一高度通用的應用程式軟體開發平台，但其使用與相關聯額外負擔(例如，指令提取及解碼、晶片上及晶片外資料之處置、臆測執行)組合之細粒化資料結構最終導致在執行程式碼期間每單位資料消耗較大量能量。 一第二、對置極端將固定功能固線式電路應用於大得多之資料區塊。使用直接應用於客製化設計電路之較大資料區塊(與細粒化資料單元相比)大大地降低每單位資料之電力消耗。然而，使用客製化設計之固定功能電路一般導致處理器能夠執行之任務集受限。因而，在第二極端中缺乏用途廣泛的程式化環境(與第一極端相關聯)。 提供高度通用的應用程式軟體開發機會與每單位資料之經改善電力效率兩者之一技術平台仍係一種有待找尋之解決方案。Image processing usually involves processing pixel values organized into an array. Here, the two-dimensional nature of the image captured by a two-dimensional array of spatial organization (extra dimensions may include time (e.g., a sequence of two-dimensional images) and data type (e.g., color)). In a typical case, an array of pixel values is provided by a camera that has generated a still image or a series of frames to capture a moving image. Traditional image processors usually fall on either end. A first extreme performs image processing tasks such as software programs executed on a general-purpose processor or a general-purpose processor (eg, a general-purpose processor with vector instruction enhancements). Although the first extreme usually provides a highly versatile application software development platform, it uses fine-grained data combined with additional burdens associated with it (e.g., instruction fetching and decoding, processing of on-chip and off-chip data, speculative execution) The structure ultimately causes a large amount of energy to be consumed per unit of data during code execution. First, the opposite pole applies fixed-function fixed-line circuits to much larger data blocks. The use of larger data blocks (compared to fine-grained data units) applied directly to custom-designed circuits greatly reduces power consumption per unit of data. However, the use of custom-designed fixed-function circuits generally results in a limited set of tasks that the processor can perform. Thus, a widely used stylized environment (associated with the first extreme) is lacking in the second extreme. A technology platform that provides one of the most versatile application software development opportunities and improved power efficiency per unit of data is still a solution to be found.
描述一種方法。該方法包含建構一影像處理軟體資料流，其中一緩衝器儲存及轉送自一生產內核傳送至一或多個消費內核之影像資料。該方法亦包含辨識該緩衝器不具有足以儲存及轉送該影像資料之資源。該方法亦包含修改該影像處理軟體資料流以包含多個緩衝器，該多個緩衝器在該影像資料自該生產內核至該一或多個消費內核之該傳送期間儲存及轉送該影像資料。Describe a method. The method includes constructing an image processing software data stream, wherein a buffer stores and forwards image data transmitted from a production core to one or more consumer cores. The method also includes identifying that the buffer does not have sufficient resources to store and forward the image data. The method also includes modifying the image processing software data stream to include multiple buffers that store and forward the image data during the transfer of the image data from the production core to the one or more consumer cores.
i. 介紹 下文描述係描述關於一新影像處理技術平台之眾多實施例，該新影像處理技術平台提供使用較大資料區塊(例如，如下文進一步描述之線群組及圖表)以提供改善電力效率之廣泛通用之應用程式軟體開發環境。 1.0 硬體架構實施例 a. 影像處理器硬體架構及操作 圖1展示在硬體中實施之一影像處理器的一架構100之一實施例。影像處理器可例如由一編譯器標定，該編譯器將針對一模擬環境內之一虛擬處理器寫入之程式碼轉換成實際上由硬體處理器執行之程式碼。如圖1中所觀察，架構100包含複數個線緩衝器單元101_1至101_M (後文中「線緩衝器」、「線緩衝器單元」或類似者)，其等透過一網路104 (例如，一網路晶片(NOC)，包含一晶片上交換網路、一晶片上環狀網路或其他類型之網路)互連至複數個模板處理器單元102_1至102_N (後文中「模板處理器」、「模板處理器單元」、「影像處理核心」、「核心」及類似者)及對應圖表產生器單元103_1至103_N (後文中「圖表產生器」、「圖表產生器單元」或類似者)。在一實施例中，任何線緩衝器單元可透過網路104連接至任何圖表產生器及對應模板處理器。 在一實施例中，編譯程式碼且將其載入至一對應模板處理器102上以執行早先由一軟體開發者定義之影像處理操作(例如，取決於設計及實施方案，亦可將程式碼載入至模板處理器之相關聯圖表產生器103上)。在至少一些例項中，可藉由將一第一管線級之一第一內核程式載入至一第一模板處理器102_1中、將一第二管線級之一第二內核程式載入至一第二模板處理器102_2中等而實現一影像處理管線，其中第一內核執行管線之第一級之函數，第二內核執行管線之第二級之函數等，且額外控制流程方法經安裝以將輸出影像資料自管線之一個級傳遞至管線之下一級。 在其他組態中，影像處理器可經實現為具有操作相同內核程式碼之兩個或兩個以上模板處理器102_1、102_2之一平行機。例如，可藉由跨多個模板處理器(其各者執行相同功能)散佈圖框而處理一高度密集且高資料速率之影像資料串流。 在又其他組態中，可藉由組態各自模板處理器及其自身之各自程式碼內核且將適當控制流程鉤組態至硬體中以將來自一個內核之輸出影像引導至DAG設計中之下一內核之輸入而將內核之基本上任何DAG載入至硬體處理器上。 作為一般流程，影像資料圖框由一巨集I/O單元105接收且逐圖框傳遞至線緩衝器單元101之一或多者。一特定線緩衝器單元將其影像資料圖框剖析成影像資料之一較小區域(稱為「線群組」)，且接著透過網路104將該線群組傳遞至一特定圖表產生器。一完整或「全」單個線群組可例如由一圖框之多個連續完整列或行之資料組成(為簡潔起見，本說明書將主要係指連續列)。圖表產生器將影像資料之線群組進一步剖析成影像資料一較小區域(稱為「圖表」)，且將圖表呈現給其對應模板處理器。 就具有一單一輸入之一影像處理管線或一DAG流程而言，將輸入圖框引導至相同線緩衝器單元101_1，線緩衝器單元101_1將影像資料剖析成線群組且將線群組引導至圖表產生器103_1，圖表產生器103_1之對應模板處理器102_1執行管線/DAG中之第一內核之程式碼。在完成由模板處理器102_1操作其處理之線群組之後，圖表產生器103_1將輸出線群組發送至一「下游」線緩衝器單元101_2 (在一些使用情況中，可將輸出線群組發送回至早先已發送輸入線群組之相同線緩衝器單元101_1)。 接著，一或多個「消費者」內核(其表示在其自身之各自其他圖表產生器及模板處理器(例如圖表產生器103_2及模板處理器102_2)上執行之管線/DAG中之下一級/操作)自下游線緩衝器單元101_2接收由第一模板處理器102_1產生之影像資料。依此方式，操作一第一模板處理器之一「生產者」內核使其輸出資料轉送至操作一第二模板處理器之一「消費者」內核，其中消費者內核在與總管線或DAG之設計一致之生產者內核之後執行下一組任務。此處，線緩衝器單元101_2儲存及轉送由生產者內核產生之影像資料作為將影像資料自生產者內核傳送至消費者內核之部分。 一模板處理器102經設計以同時操作影像資料之多個重疊模板。多個重疊模板及模板處理器之內部硬體處理容量有效地判定一圖表之大小。此處，在模板處理器102內，執行道陣列一齊操作以同時處理由多個重疊模板覆蓋之影像資料表面區域。 如下文將更詳細描述，在各項實施例中，將影像資料圖表載入至模板處理器102內之二維暫存器陣列結構中。據信，使用圖表及二維位移暫存器陣列結構可藉由將大量資料移動至大量暫存器空間(作為(例如)一單一載入操作)中且此後由一執行道陣列即時對資料直接執行處理任務來有效地提供電力消耗改善方案。另外，使用一執行道陣列及對應暫存器陣列提供可容易程式化/組態之不同模板大小。 圖2a至圖2e繪示一線緩衝器單元101之剖析活動、一圖表產生器單元103之細粒化剖析活動兩者以及耦合至圖表產生器單元103之模板處理器102之模板處理活動的一高階實施例。 圖2a描繪影像資料201之一輸入圖框之一實施例。圖2a亦描繪一模板處理器經設計以在其上操作之三個重疊模板202之一輪廓(各模板具有3個像素x3個像素之尺寸)。以加粗黑色突顯各模板分別針對其產生輸出影像資料之輸出像素。為簡潔起見，三個重疊模板202被描繪為僅在垂直方向上重疊。應切實認識到，一模板處理器實際上可經設計以具有垂直方向及水平方向兩者上之重疊模板。 由於模板處理器內之垂直重疊模板202 (如圖2a中所觀察)，故單個模板處理器可在其上操作之圖框內存在一寬頻帶影像資料。如下文將更詳細論述，在一實施例中，模板處理器依一從左至右方式跨影像資料處理其重疊模板內之資料(且接著依從上至下順序對下一組線重複)。因此，隨著模板處理器使其操作繼續向前，加粗黑色輸出像素區塊之數目將水平向右增長。如上文所論述，一線緩衝器單元101負責自足以使模板處理器在擴大數目之未來週期內於其上操作之一傳入圖框剖析輸入影像資料之一線群組。一線群組之一例示性描繪經繪示為一陰影區域203。在一實施例中，如下文進一步描述，線緩衝器單元101可綜合不同動態表現以將一線群組發送至一圖表產生器/自一圖表產生器接收一線群組。例如，根據一個模式(稱為「全群組(full group)」)，在一線緩衝器單元與一圖表產生器之間傳遞影像資料之完整全寬線。根據一第二模式(稱為「實際上高(virtually tall)」)，首先使用全寬列之一子集來傳遞一線群組。接著，以較小(非全寬)件依序傳遞剩餘列。 在輸入影像資料之線群組203已由線緩衝器單元界定且被傳遞至圖表產生器單元之情況下，圖表產生器單元進一步將線群組剖析成更精確適應模板處理器之硬體限制的精細圖表。更具體而言，如下文將進一步更詳細描述，在一實施例中，各模板處理器由一二維位移暫存器陣列組成。二維位移暫存器陣列基本上在一執行道陣列「下方」位移影像資料，其中位移之型樣引起各執行道操作其自身各自模板內之資料(即，各執行道處理其自身之資訊模板以產生該模板之一輸出)。在一實施例中，圖表係「填充」二維位移暫存器陣列或以其他方式載入至二維位移暫存器陣列中之輸入影像資料之表面區域。 因此，如圖2b中所觀察，圖表產生器自線群組203剖析一初始圖表204且將其提供至模板處理器(此處，資料之例示性圖表對應於一般由元件符號204識別之5×5陰影區域)。如圖2c及圖2d中所觀察，模板處理器藉由在圖表上依一從左至右方式有效地移動重疊模板202來操作輸入影像資料之圖表。截至圖2d，已耗盡可自圖表內之資料計算其之一輸出值的像素之數目(在一塗黑3×3陣列中係9) (其他像素位置不可具有自圖表內之資訊判定之一輸出值)。為簡便起見，已忽略影像之邊界區域。 接著，如圖2e中所觀察，圖表產生器提供下一圖表205來供模板處理器繼續操作。應注意，模板之初始位置隨著其開始操作下一圖表而自第一圖表上之耗盡點接著向右前進(如先前圖2d中所描繪)。就新圖表205而言，模板將僅在模板處理器依相同於處理第一圖表之方式的方式操作新圖表時繼續向右移動。 應注意，由於包圍一輸出像素位置之模板之邊界區域，第一圖表204之資料與第二圖表205之資料之間存在某一重疊。可僅藉由圖表產生器重新傳輸重疊資料兩次來處置重疊。在替代實施方案中，為將下一圖表饋送至模板處理器，圖表產生器可繼續將新資料僅發送至模板處理器且模板處理器重新使用來自前一圖表之重疊資料。 b. 模板處理器設計及操作 圖3a展示一模板處理器單元架構300之一實施例。如圖3a中所觀察，模板處理器包含一資料運算單元301、一純量處理器302及相關聯記憶體303及一I/O單元304。資料運算單元301包含一執行道陣列305、一二維位移陣列結構306及與陣列之特定列或行相關聯之單獨各自隨機存取記憶體307。 I/O單元304負責將自圖表產生器接收之「輸入」資料圖表載入至資料運算單元301中及將來自模板處理器之「輸出」資料圖表儲存至圖表產生器中。在一實施例中，將圖表資料載入至資料運算單元301中需要將一接收圖表剖析成影像資料列/行且將影像資料列/行載入至二維位移暫存器結構306或執行道陣列之列/行之各自隨機存取記憶體307中(下文將更詳細描述)。若首先將圖表載入至記憶體307中，則執行道陣列305內之個別執行道可視情況將圖表資料自隨機存取記憶體307載入至二維位移暫存器結構306中(例如，作為恰好在操作圖表之資料之前的一載入指令)。在完成將一資料圖表載入至暫存器結構306中(無論是否直接來自一圖表產生器或記憶體307)之後，執行道陣列305之執行道操作資料且最終「寫回」完成資料作為直接返回至圖表產生器或進入隨機存取記憶體307之一圖表。若執行道寫回至隨機存取記憶體307，則I/O單元304自隨機存取記憶體307提取資料以形成接著轉送至圖表產生器之一輸出圖表。 純量處理器302包含一程式控制器309，其自純量記憶體303讀取模板處理器之程式碼之指令且將指令發出至執行道陣列305中之執行道。在一實施例中，將一單一相同指令廣播至陣列305內之所有執行道以實現來自資料運算單元301之似單指令多資料(SIMD)行為。在一實施例中，自純量記憶體303讀取且發出至執行道陣列305之執行道的指令之指令格式包含一極長指令字(VLIW)型格式，其包含每指令之一個以上運算碼。在另一實施例中，VLIW格式包含一ALU運算碼(其指導由各執行道之ALU (如下文將描述，在一實施例中，其可指定一個以上傳統ALU運算)執行之一數學函數)及一記憶體運算碼(其指導一特定執行道或執行道集合之一記憶體運算)兩者。 術語「執行道」係指能夠執行一指令之一組一或多個執行單元(例如可執行一指令之邏輯電路)。然而，在各項實施例中，一執行道可包含不只是執行單元之更多似處理器功能性。例如，除一或多個執行單元之外，一執行道亦可包含解碼一接收指令之邏輯電路，或就更多似多指令多資料(MIMD)設計而言，包含提取及解碼一指令之邏輯電路。關於似MIMD方法，儘管本文已主要描述一集中式程式控制方法，但可在各種替代實施例中實施一更分散方法(例如，其包含陣列305之各執行道內之程式碼及一程式控制器)。 一執行道陣列305、程式控制器309及二維位移暫存器結構306之組合對一寬範圍之可程式化功能提供一可廣泛適應/組態之硬體平台。例如，應用程式軟體開發者能夠程式化具有一寬範圍之不同功能性能及尺寸(例如模板大小)之內核，假定個別執行道能夠執行各種功能且能夠容易地存取接近任何輸出陣列位置之輸入影像資料。 除充當由執行道陣列305操作之影像資料之一資料儲存器之外，隨機存取記憶體307亦可保存一或多個查找表。在各項實施例中，一或多個純量查找表亦可樣例化於純量記憶體303內。查找表通常由影像處理任務用於(例如)獲得不同陣列位置之濾波或變換係數，實施複變函數(例如伽瑪曲線、正弦、餘弦)(其中查找表提供一輸入指標值之函數輸出)，等等。此處，可預期，SIMD影像處理序列通常將在一相同時脈週期期間對一相同查找表執行一查找。類似地，可將一或多個常數表儲存於純量記憶體303中。此處，例如，可預期，不同執行道可在相同時脈週期上需要一相同常數或其他值(例如，對一整個影像應用一特定乘法器)。因此，至一常數查找表中之存取將一相同純量值傳回至執行道之各者。通常使用一指標值存取查找表。 一純量查找涉及：將來自相同查找表之相同資料值自相同指標傳遞至執行道陣列305內之各執行道。在各項實施例中，上文所描述之VLIW指令格式經擴展以亦包含使由純量處理器執行之一查找操作針對一純量查找表之一純量運算碼。經指定與運算碼一起使用之指標可為一立即運算元或自某一其他資料儲存位置提取。無論如何，在一實施例中，自純量記憶體內之一純量查找表之一查找基本上涉及：在相同時脈週期期間，將相同資料值廣播至執行道陣列305內之所有執行道。下文將進一步提供關於查找表之使用及操作的額外細節。 圖3b概述上文所論述之(若干) VLIW指令字實施例。如圖3b中所觀察，VLIW指令字格式包含用於以下三個單獨指令之欄位：1)一純量指令351，其由純量處理器執行；2)一ALU指令352，其經廣播且由執行道陣列內之各自ALU依SIMD方式執行；及3)一記憶體指令353，其經廣播且依一部分SIMD方式執行(例如，若沿執行道陣列中之一相同列之執行道共用一相同隨機存取記憶體，則來自不同列之各者之一個執行道實際上執行指令(記憶體指令353之格式可包含識別來自各列之何種執行道執行指令之一運算元))。 亦包含用於一或多個立即運算元之一欄位354。可以指令格式識別指令351、352、353之何者使用何種立即運算元資訊。指令351、352、353之各者亦包含其自身之各自輸入運算元及所得資訊(例如用於ALU運算之局部暫存器及用於記憶體存取指令之一局部暫存器及一記憶體位址)。在一實施例中，在執行道陣列內之執行道執行其他兩個指令352、353之任一者之前，由純量處理器執行純量指令351。即，VLIW字之執行包含其中執行純量指令351之一第一週期及接著其中可執行其他指令352、353之一第二週期(應注意，在各項實施例中，可並行執行指令352及353)。 在一實施例中，由純量處理器302執行之純量指令包含發出至圖表產生器103以自資料運算單元301之記憶體或2D位移暫存器306載入圖表/將圖表儲存至資料運算單元301之記憶體或2D位移暫存器306中的命令。此處，圖表產生器之操作可取決於線緩衝器單元101之操作或其他變數(其防止預執行時間包含圖表產生器103完成由純量處理器302發出之任何命令將所花費之週期數)。因而，在一實施例中，任何VLIW字(其純量指令351對應於待發出至圖表產生器103之一命令或以其他方式引起一命令發出至圖表產生器103)亦包含其他兩個指令欄位352、353中之無操作(NOOP)指令。接著，程式碼進入指令欄位352、353之NOOP指令之一迴路，直至圖表產生器完成其至資料運算單元之載入/來自資料運算單元之儲存。此處，在將一命令發出至圖表產生器之後，純量處理器可設定一互鎖暫存器之一位元，圖表產生器在完成命令之後重設該位元。在NOOP迴路期間，純量處理器監測互鎖暫存器之位元。當純量處理器偵測到圖表產生器已完成其命令時，正常執行再次開始。 圖4展示一資料運算單元401之一實施例。如圖4中所觀察，資料運算單元401包含邏輯上定位於一二維位移暫存器陣列結構406「上方」之一執行道陣列405。如上文所論述，在各項實施例中，將由一圖表產生器提供之一影像資料圖表載入至二維位移暫存器406中。接著，執行道操作來自暫存器結構406之圖表資料。 執行道陣列405及位移暫存器結構406相對於彼此固定位置。然而，位移暫存器陣列406內之資料依一策略性協調方式位移以引起執行道陣列中之各執行道處理資料內之一不同模板。因而，各執行道判定所產生之輸出圖表中之一不同像素之輸出影像值。應自圖4之架構明白，由於執行道陣列405包含垂直相鄰執行道及水平相鄰執行道，所以重疊模板不僅經垂直配置，且亦經水平配置。 資料運算單元401之一些顯著架構特徵包含具有比執行道陣列405寬之尺寸的位移暫存器結構406。即，執行道陣列405外存在暫存器之一「暈圈」409。儘管暈圈409經展示為存在於執行道陣列之兩側上，但取決於實施方案，暈圈可存在於執行道陣列405之更少側(一側)或更多側(三個或四個側)上。暈圈409用於對資料提供「外溢(spill-over)」空間，其隨著資料在執行道405「下方」位移而溢出至執行道陣列405之界限外。簡單而言，當處理模板之最左像素時，居中於執行道陣列405之右邊緣上的一5×5模板將需要進一步向右之四個暈圈暫存器位置。為便於繪製，圖4將暈圈之右側之暫存器展示為僅具有水平位移連接且將暈圈之底側之暫存器展示為僅具有垂直位移連接，但在一名義實施例中，任一側(右側、底側)上之暫存器將具有水平連接及垂直連接兩者。 額外外溢空間由耦合至陣列中之各列及/或各行的隨機存取記憶體407或其部分提供(例如，可將一隨機存取記憶體指派給跨越逐列4個執行道及逐行2個執行道之執行道陣列之一「區域」。為簡便起見，應用程式之剩餘部分將主要係指基於列及/或行之分配方案)。此處，若一執行道之內核運算需要其處理二維位移暫存器陣列406外之像素值(一些影像處理常式需要該等像素值)，則影像資料之平面能夠(例如)自暈圈區域409進一步外溢至隨機存取記憶體407中。例如，考量一6×6模板，其中硬體包含執行道陣列之右邊緣上之一執行道右側的僅四個儲存元件之一暈圈區域。在此情況中，需要使資料自暈圈409之右邊緣進一步向右位移以完全處理模板。接著，經位移至暈圈區域409外之資料將外溢至隨機存取記憶體407。下文將進一步提供隨機存取記憶體407及圖3之模板處理器之其他應用。 圖5a至圖5k演示在執行道陣列「下方」之二維位移暫存器陣列內位移影像資料(如上文所提及)之方式之一工作實例。如圖5a中所觀察，一第一陣列507中描繪二維位移陣列之資料內容且由一圖框505描繪執行道陣列。又，簡單地描繪執行道陣列內之兩個鄰近執行道510。在此簡化描繪510中，各執行道包含一暫存器R1，其可自位移暫存器接受資料，自一ALU輸出接受資料(例如，充當一跨週期累加器)，或將輸出資料寫入至一輸出目的地中。 各執行道亦可使用其「下方」之二維位移陣列中之一局部暫存器R2中之內容。因此，R1係執行道之一實體暫存器，而R2係二維位移暫存器陣列之一實體暫存器。執行道包含可操作由R1及/或R2提供之運算元的一ALU。如下文將進一步更詳細描述，在一實施例中，位移暫存器實際上由每陣列位置之多個儲存器/暫存器元件(儲存器/暫存器元件之一「深度」)實施，但位移活動受限於儲存元件之一個平面(例如儲存元件之僅一個平面可每週期位移)。圖5a至圖5k將此等較深暫存器位置之一者描繪為用於儲存來自各自執行道之結果X。為便於繪示，較深結果暫存器經繪製成與其配對暫存器R2並排而非位於其配對暫存器R2下方。 圖5a至圖5k著重於兩個模板(其中央位置與執行道陣列505內所描繪之執行道位置對511對準)之計算。為便於圖解，執行道對510經繪製為水平鄰近者，但事實上，根據以下實例，執行道對510係垂直鄰近者。 首先，如圖5a中所觀察，執行道511居中於其中央模板位置上。圖5b展示由兩個執行道511執行之目標碼。如圖5b中所觀察，兩個執行道511之程式碼引起位移暫存器陣列507內之資料下移一個位置且右移一個位置。此使兩個執行道511對準於其各自模板之左上角。接著，程式碼引起定位於其各自位置(R2)中之資料載入至R1中。 接著，如圖5c中所觀察，程式碼引起執行道對511使位移暫存器陣列507內之資料左移一個單位，其引起至各執行道之各自位置右側之值位移至各執行道之位置中。接著，將R1中之值(先前值)與已位移至執行道之位置(R2)中之新值相加。將結果寫入至R1中。如圖5d中所觀察，重複相同於上文針對圖5c所描述之程序的程序，其引起所得R1現包含上執行道中之值A+B+C及下執行道中之值F+G+H。此時，兩個執行道511已處理其各自模板之上列。應注意至執行道陣列505之左側上之一暈圈區域中(若左手側上存在暈圈區域)或至隨機存取記憶體中(若執行道陣列505之左手側上不存在一暈圈區域)的外溢。 接著，如圖5e中所觀察，程式碼引起位移暫存器陣列內之資料上移一個單位，其引起兩個執行道511與其各自模板之中間列之右邊緣對準。兩個執行道511之暫存器R1當前包含模板之頂列及中間列之最右值之總和。圖5f及圖5g演示跨兩個執行道之模板之中間列向左移動之繼續進展。累加繼續，使得在圖5g之處理結束時，兩個執行道511包含其各自模板之頂列及中間列之值之總和。 圖5h展示用於使各執行道與其對應模板之最下列對準的另一位移。圖5i及圖5j展示用於完成兩個執行道之整個模板之處理的繼續位移。圖5k展示用於使各執行道與資料陣列中之其校正位置對準且將結果寫入至校正位置中的額外位移。 應注意，在圖5a至圖5k之實例中，用於位移運算之目標碼可包含識別以(X, Y)座標所表達之位移之方向及量值的一指令格式。例如，用於上移一個位置之目標碼可以目標碼表達為SHIFT 0, +1。作為另一實例，右移一個位置可以目標碼表達為SHIFT+1, 0。在各項實施例中，亦可在目標碼中指定較大量值之位移(例如SHIFT 0, +2)。此處，若2D位移暫存器硬體僅支援每週期位移一個位置，則指令可由機器解譯為需要多週期執行，或2D位移暫存器硬體可經設計以支援每週期位移一個以上位置。下文將進一步更詳細描述後者之實施例。 圖6展示陣列執行道及位移暫存器結構之單位胞元之另一更詳細描繪(暈圈區域中之暫存器不包含一對應執行道)。在一實施例中，藉由樣例化在執行道陣列之各節點處圖6中所觀察之電路而實施執行道及與執行道陣列中之各位置相關聯之暫存器空間。如圖6中所觀察，單位胞元包含一執行道601，執行道601經耦合至由四個暫存器R1至R4組成之一暫存器檔案602。在任何週期期間，執行道601可自暫存器R0至R4之任一者讀取或寫入至暫存器R0至R4之任一者。對於需要兩個輸入運算元之指令，執行道可自R0至R4之任一者擷取兩個運算元。 在一實施例中，藉由以下操作來實施二維位移暫存器結構：容許暫存器R1至R3之任何(僅)一者之內容在一單一週期期間透過輸出多工器603移「出」至其鄰近者之暫存器檔案之一者且使暫存器R1至R3之任何(僅)一者之內容由透過輸入多工器604自其鄰近者之一對應者移「入」的內容替換，使得鄰近者之間的位移係在一相同方向上(例如，所有執行道左移，所有執行道右移，等等)。儘管一相同暫存器在一相同週期上使其內容移出且由移入之內容替換可較為常見，但多工器配置603、604容許一相同週期期間之一相同暫存器檔案內之不同位移源及位移目標暫存器。 應注意，如圖6中所描繪，在一位移序列期間，一執行道將使內容自其暫存器檔案602移出至其左、右、頂部及底部之各鄰近者。結合相同位移序列，執行道亦將使內容自其左、右、頂部及底部之一特定鄰近者移入至其暫存器檔案中。又，移出目標及移入源應與所有執行道之一相同位移方向一致(例如，若移出係朝向右鄰近者，則移入應來自左鄰近者)。 儘管在一項實施例中，容許每週期每執行道位移僅一個暫存器之內容，但其他實施例可容許移入/移出一個以上暫存器之內容。例如，若將圖6中所觀察之多工器電路603、604之一第二例項併入至圖6之設計中，則可在一相同週期期間移出/移入兩個暫存器之內容。當然，在容許每週期位移僅一個暫存器之內容的實施例中，自多個暫存器之位移可藉由使數學運算之間的位移消耗更多時脈週期來發生於數學運算之間(例如，可藉由在數學運算之間消耗兩個位移運算來在數學運算之間位移兩個暫存器之內容)。 若在一位移序列期間移出一執行道之暫存器檔案之非所有內容，則應注意，各執行道之未經移出暫存器之內容保留於適當位置中(未位移)。因而，未由移入內容替換之任何未位移內容跨位移週期存留於執行道之局部處。各執行道中所觀察之記憶體單元(「M」)用於自與執行道陣列內之執行道之列及/或行相關聯之隨機存取記憶體空間載入資料/將資料儲存至與執行道陣列內之執行道之列及/或行相關聯之隨機存取記憶體空間。此處，M單元充當一標準M單元，此係因為其常用於載入/儲存無法自執行道自身之暫存器空間載入/儲存至執行道自身之暫存器空間的資料。在各項實施例中，M單元之主要操作係將資料自一局部暫存器寫入至記憶體中及自記憶體讀取資料且將其寫入至一局部暫存器中。 關於由硬體執行道601之ALU單元支援之指令集架構(ISA)運算碼，在各項實施例中，由硬體ALU支援之數學運算碼與由一虛擬執行道支援之數學運算碼(例如ADD、SUB、MOV、MUL、MAD、ABS、DIV、SHL、SHR、MIN/MAX、SEL、AND、OR、XOR、NOT)整合在一起(例如，實質上相同)。恰如上文所描述，記憶體存取指令可由執行道601執行以自其相關聯之隨機存取記憶體提取資料/將資料儲存至其相關聯之隨機存取記憶體。另外，硬體執行道601支援用於位移二維位移暫存器結構內之資料的位移運算指令(向右、向左、向上、向下)。如上文所描述，程式控制指令主要由模板處理器之純量處理器執行。 2.0 用以改善執行時間效率之程式碼轉換 如上文詳細描述，可藉由將較小、細粒化軟體程式(本文中稱為內核)組合至一較大總體結構(諸如一有向非循環圖)中而定義針對影像處理器開發之應用程式軟體。定義一般包含將不同內核耦合至一特定資料流型樣中，其中數個「生產」內核將其輸出影像資料供給至一或多個「消費」內核。至少一個內核接收應用程式軟體程式操作之總體輸入影像且通常，該等內核之一者產生應用程式軟體之總體輸出影像。 接著將各內核映射至一特定模板處理器。各模板處理器具有一相關聯圖表產生器，該相關聯圖表產生器接收其相關聯模板處理器之內核欲操作之影像資料。在各項實施例中，由圖表產生器以線群組方式接收影像資料。例如，圖表產生器可接收影像資料作為跨一輸入影像圖框之全寬之數個列。圖表產生器接著形成二維影像資料「圖表」，其等經提供至該模板處理器且最終經載入至該模板處理器之二維位移暫存器陣列中。 在各項實施例中，使用專用硬體邏輯電路(例如，特定應用積體電路(ASIC)邏輯電路)、可程式化邏輯電路(例如，場可程式化閘陣列邏輯電路)、嵌入式處理器邏輯電路或此等任何組合實施圖表產生器以實施圖表產生器之功能性。專用硬體邏輯電路(若有)具有經設定有由應用程式軟體之編譯程序產生的資訊之相關聯組態暫存器，該資訊引起圖表產生器針對映射至與圖表產生器相關聯之模板處理器之內核執行圖表產生活動。可程式化邏輯電路(若有)經程式化有由應用程式軟體之編譯程序產生之資訊，該資訊引起可程式化邏輯電路針對已映射至圖表產生器之相關聯模板處理器、待在模板處理器上執行之內核實施圖表產生器功能性。嵌入式處理器電路(若有)具備由應用程式軟體之編譯程序產生之程式碼，該程式碼在由嵌入式處理器執行時引起嵌入式處理器針對已映射至圖表產生器之相關聯模板處理器、待在模板處理器上執行之內核實施圖表產生器功能性。模板處理器之純量處理器亦可經程式化以執行、輔助或以其他方式參與各種圖表產生活動任務。相同種類之電路實施概率及相關聯編譯程式碼及/或資訊亦可相對於線緩衝器電路而存在。 應用程式軟體開發程序因此不僅包含將一內核映射至一特定模板處理器而且包含產生用以針對內核執行圖表產生活動之相關聯組態資訊及/或程式碼。 在各種應用程式軟體程式開發環境中，負責接受一應用程式軟體程式之一高階描述且據此回應而產生供影像處理器執行之低階程式碼(例如，目標碼)及任何相關聯組態資訊之一編譯器將辨識應用程式軟體之各種低效且改變所編譯之程式碼以改善或以其他方式減少低效。經改變程式碼可為用於一或多個圖表產生器及/或待由其等及/或線緩衝器單元饋送之內核之程式碼及/或組態資訊。 圖7a係關於一第一潛在低效。如圖7a中所觀察，由一圖表產生器接收一輸入影像701作為例如由一線緩衝器單元發送之數個線群組。如圖7a中所觀察，輸入影像702在由於一圖表產生器所耦合之模板處理器上執行的內核K1處理之前例如由圖表產生器降低取樣。替代地，內核K1可經程式化以執行降低取樣。 在各項實施例中，模板處理器自然地建立具有相同於模板處理器之執行道陣列的尺寸之輸出影像圖表。例如，在其中執行道陣列尺寸係16個像素x16個像素之一實施例中，模板處理器之內核程式碼K1之構造最初預設為產生16個像素x16個像素輸出影像圖表。 若模板處理器經組態以自已經降低取樣之輸入影像產生尺寸相同於其執行道陣列之輸出圖表，則需要大量緩衝空間。例如，參考圖7a，若由圖表產生器執行降低取樣702以建立一16個像素x16個像素降低取樣圖表703以載入至模板處理器之二維位移暫存器中，則圖表產生器將需要佇列化一整個32像素x32像素輸入影像701以便形成16像素x16像素降低取樣輸入影像703以供內核K1消費。分配此佇列化所需之大量記憶體係低效之一形式。 因而，在一實施例中，一編譯器將重組應用程式軟體程式(包含例如任何相關組態資訊)，如圖7b中所描繪。具體言之，該編譯器將結構化程式碼使得內核K1不在充分利用其執行道陣列之情況下操作。繼續本實例，內核K1代替地經設計以操作8像素x8像素輸入圖表703b，此引起內核K1產生一8像素x8像素輸出圖表704b。 藉由組態內核K1以操作較小8像素x8像素輸入圖表703b，相較於圖7a之輸入影像資料701a，降低取樣活動702b (例如，如由圖表產生器執行)僅需要佇列化一半量之輸入影像資料701b。圖7b之輸入影像資料701b對應於輸入影像資料之僅16列，而相比之下圖7a之輸入影像資料701a對應於影像資料之32列。在輸入影像資料701b之僅16列之情況下，降低取樣活動702b能夠執行2:1降低取樣，此產生將跨越影像全寬之一系列8像素x8像素輸入圖表703b。 圖8a及圖8b展示另一低效，其中對一內核K1之輸出影像資料801執行增加取樣且接著在由K1之消費內核K2對影像資料執行之前執行一相同量之降低取樣。此處，如圖8a中所觀察，生產內核K1產生輸出圖表A0至A3之一系列801。接著使此等輸出圖表801之影像資料交錯以有效地增加取樣K1之輸出。即，如圖8a中所觀察，例如，使輸出圖表A0至A3之各者之一頂線交錯以形成儲存於線緩衝器802中之增加取樣K1輸出803之一頂輸出線，線緩衝器802在由K1之消費內核K2消費K1之輸出資料之前暫時佇列化該輸出資料。在各項實施例中，由K1、耦合至K1在其上執行之模板處理器之圖表產生器或K1發送其輸出所至之線緩衝器802的任一者執行增加取樣。 如圖8b中所觀察，針對消費K1之輸出的內核K2之輸入處理經組態以降低取樣其輸入達增加取樣K1之輸出之一相同倍數。因而，將適當大小輸入資料饋送給K2之程序需要反轉對K1之輸出執行之增加取樣程序。即，參考圖8b，最終使線緩衝器802中之交錯佇列化資料803解交錯以重新形成最初由K1形成之輸出影像A0至A3。可由線緩衝器802、耦合至K2在其上執行之模板處理器之圖表產生器或K2自身之任一者執行降低取樣。 在一實施例中，一編譯器經設計以辨識一生產內核之增加取樣輸出何時針對將消費該生產內核之輸出(其可包含多個此等內核)之一內核降低取樣達一相同倍數(例如，1:2增加取樣及2:1降低取樣)。據此回應，該編譯器將進一步重組所開發程式碼以便消除沿生產者至消費者資料路徑之增加取樣及降低取樣兩者。圖8c中描繪此解決方案。此處，K1之非增加取樣輸出僅佇列化在耦合於K1與K2連接之間的線緩衝器802中。接著在無任何降低取樣之情況下將非增加取樣K1輸出直接饋送至K2。因而，避免圖8a之增加取樣活動及圖8b之降低取樣活動兩者。 圖9a及圖9b係關於例如可在多分量輸出影像之情況下發生之另一低效。如此項技術中已知，數位影像可具有多個分量(例如，RGB、YUV等)。各種應用程式軟體程式可經設計/經組態以將不同分量處理為不同資料平面。此處，例如，可完全由一生產內核K1藉由以下步驟而產生一完整輸出影像901：產生僅由第一分量(R)組成之一或多個資料圖表；產生僅由第二分量(G)組成之一或多個資料圖表；及產生僅由一第三分量(B)組成之一或多個資料圖表。在各項實施例中，將在一生產內核與一消費內核之間傳遞的一影像之所有資料佇列化於一相同線緩衝器902中可係自然的或標準預設。因此，圖9a展示佇列化於一相同線緩衝器單元902中之所有三個分量901之影像資料。 然而，在例如大輸出影像之情況下，將所有三個分量之影像資料儲存於一相同線緩衝器單元中可耗用(strain)或以其他方式消耗大量線緩衝器記憶體資源。因此在一實施例中，參考圖9b，編譯一應用程式軟體程式之一編譯器將自動辨識儲存多分量影像之不同分量何時可耗用線緩衝器記憶體資源。例如，編譯程式最初可分配固定量之緩衝器記憶體資源以儲存及轉送影像或可分配與待傳送之資料大小及/或量相關之一定量的緩衝器記憶體資源且鑑於該分配，可判定自動分配量不足或達到某個最大臨限值。在其他方法中，編譯程序可包含模擬應用程式軟體程式及辨識線緩衝器單元係一瓶頸(例如，其常常不具有用以儲存已由一生產內核產生之一線群組之記憶體空間或其不具有用以對自一消費內核讀取請求作出回應之頻寬)。據此回應，編譯程序自動修改應用程式軟體及/或重新組態影像處理器使得生產內核K1之輸出影像之不同分量經佇列化於不同線緩衝器單元中。 此處，圖9b展示R、G及B影像資料分別經佇列化於不同線緩衝器單元902_1、902_2及902_3中。 圖9b之解決方案亦可用於其中生產內核K1具有諸多消費者之情況。在此情況下，若採用圖9a之預設解決方案，則儲存影像資料901之所有分量之單個線緩衝器單元可變為一系統瓶頸，因為大量消費者將需要自線緩衝器載入/讀取多次以便接收單個輸入影像之所有資訊。因此在一實施例中，採用圖9b之方法，其中各線緩衝器僅保存一相同分量類型之資料。在所論述實例中，相較於圖9a之預設方法，此將使消費者對單個線緩衝器資源所提出之讀取請求減少達66%。即，圖9b之線緩衝器單元902_1、902_2及902_3之各者將僅需要支援圖9a之線緩衝器單元902之33%消費讀取負載。對於生產內核之影像資料至線緩衝器資源中之寫入活動，亦發生一相同減小需求影響。 其中圖9b之方法可減少低效之另一情況係特定消費者僅消費分量之一子集。例如，在一極端情況下，一個消費者消費R分量，另一消費者消費G分量且另一消費者消費B分量。在此情況下，各不同消費者經組態有其自身專用線緩衝器資源，該線緩衝器資源流線化沿不同資料路徑之基於不同分量之資料流(透過不同線緩衝器單元連接)。相比之下，若使用圖9a之方法，則基於不同分量之資料流將會聚於圖9a之線緩衝器902之單個點處，在此情況下，一個分量之資料流可因正轉送其他分量之線緩衝器單元902處之大量讀取及寫入活動而遲緩。 圖10a及圖10b展示基於單個消費者下游線緩衝器資源之散佈之另一效率改善。此處，過多消費者之存在可強制使用多個線緩衝器單元轉送單個生產內核之輸出影像資料。圖10a展示其中四個不同消費者K2至K5消費來自單個線緩衝器單元1002之單個生產內核K1之輸出之潛在低效。又，單個線緩衝器單元1002可為瓶頸，因為其無法清除其佇列化資料直至所有消費者已消費該資料。在此情況下，來自線緩衝器單元1002之總體資料流將最小減小為其最慢消費者之輸入速率。此外，鑑於線緩衝器單元1002支援之大量消費者可壓倒線緩衝器單元1002之資源，線緩衝器單元1002將接收重負載之讀取請求。 因而，如圖10b中所描繪，一第一消費者子集K2、K3經指派至一第一線緩衝器單元1002_1且一第二消費者子集K4、K5經指派至一第二線緩衝器單元1002_2。生產內核K1之輸出影像串流經饋送至兩個線緩衝器單元1002_1、1002_2。在多個線緩衝器單元資源1002_1、1002_2當中散佈總消費者負載幫助降低對任何特定線緩衝器單元資源之總需求(相較於圖10a之方法)。另外，編譯器可能夠使用一相同線緩衝器單元饋送較快輸入串流消費內核(及/或使用一不同線緩衝器單元饋送較慢輸入串流消費內核)使得較快消費內核不因一較慢輸入速率消費內核之消費速率較慢而遲緩。 圖11a展示可起因於設計為一DAG之應用程式軟體程式(或其分量)之一「分裂與接合(split and join)」低效。如圖11a中所觀察，一源內核K1之輸出經饋送至兩個不同消費內核K2及K3。另外，內核K3消費內核K2之輸出。內核K3與內核K1之輸出之雙重相依性可引起執行時間運算低效及模型化/設計低效兩者。關於執行時間低效，LB2線緩衝器1102_2可需要製成極大以便佇列化大量K1之輸出資料。通常，內核K3不會請求來自LB2 1102_2之下一線群組直至大約當與來自LB2 1102_2之下一線群組一起被內核K3處理的來自LB3 1102_3之下一線群組可用時。歸因於透過K2之可能大傳播延遲，LB2 1102_2可變得極大。在應用程式軟體之設計期間，在LB2 1102_2中之資料準備好被消費時與在自內核K2至K3之其同層級輸入資料可用於LB3 1102_3中時之間的前述差異亦可使模型化或最佳化程序變得更難。 圖11b展示其中一編譯器將一管線結構強加於分裂與接合結構上之一解決方案。此處，圖11a之K2內核經擴展成一不同內核K2’，其包含原始K2內核外加僅消費來自LB1 1102_1之內容且將其轉送至LB4 1102_4之一載入/儲存演算法1103。重要的是，載入/儲存演算法1103可引發來自K1之未處理串流之某一傳播延遲，此消除在來自K1之原始輸出資料準備好被K3消費時與在LB3 1102_3中來自K2之輸出資料準備好被K3消費時之間的差異。 自圖3a之論述回想到，在各項實施例中，純量記憶體303可經組態以保存一查找表或常數表。在特定應用中，由一內核處理之輸入影像資料係一固定常數而非可變資訊(例如，如由操作變化輸入資料之一源內核產生)。一實例係透鏡陰影校正，其中例如針對透鏡表面上之不同相當大粒度區域記錄透鏡之校正值。相當大粒度對應於一低解析度影像資料(若所記錄資料經實施為不同項目，其中各項目對應於一不同晶粒，所記錄資料不含諸多項目)。 當影像處理器處理來自包含透鏡之一攝影機之影像時，此等所記錄校正值之一者對應於由執行道陣列處理之影像區域。所記錄值因此作為一輸入值應用於各執行道。在此意義上，透鏡校正值類似地經實施為一查找表或常數表。另外，在實施校正值所需之總資料量受限之情況下，校正值不消耗大量記憶體空間。因而，如圖12中所觀察，在各項實施例中，固定且足夠小以適合純量記憶體1203之輸入影像資料1210經載入至純量記憶體1203中(例如，作為應用程式軟體之一初始組態)且被在執行時間期間於純量處理器之執行道陣列上執行之內核引用為查找表或常數表(而非例如由一源內核產生且透過一線緩衝器單元饋送至該內核)。 圖13a展示可潛在地導致較大量資料佇列化於一線緩衝器單元及/或圖表產生器中之另一執行時間問題。此處，圖13a描繪例如在自一線緩衝器單元提供之後佇列化於一圖表產生器中之三個線群組1301、1302、1303。例如，假定線群組1301、1302、1303之各者含有影像資料之16列且圖表產生器之對應模板處理器的執行道陣列之尺寸1305亦係16個像素x16個像素。另外，假定二維位移暫存器陣列之尺寸1306係24個像素x24個像素以支援圍繞執行道陣列之周邊形成一4像素寬邊界之一暈圈區域。至少在此等境況下，一自然組態可使執行道陣列1305之16列與一特定線群組之16列對準。即，圖表產生器形成居中於一特定線群組之圖表。圖13a展示其中執行道1305經對準以在第二線群組1302之高度內操作之此方法。 一問題係如圖13a中所描繪，由於暈圈1306之存在，故饋送至二維位移暫存器陣列中之完整圖表將需要來自第一線群組1301之一下區域及第三線群組1303之一上區域的資料(暈圈區域亦覆蓋此等線群組)。因而，在一實施例中，如圖13b中所描繪，更改對準使得需要存在最小數目之線群組以形成一全尺寸圖表。在此實例中，圖13b之對準相對於圖13a之對準上移達四個像素值使得僅兩個線群組1301、1302需要存在於圖表產生器中以形成一全尺寸圖表。如此做，不僅在圖表產生器中需要較少記憶體空間(及亦潛在地線緩衝器)，而且圖表僅需要等待兩個線群組開始處理而非等待三個線群組開始處理。 圖14係關於由一圖表產生器執行為被饋送輸入影像資料之一內核的一輸入程序之一解交錯程序，該輸入影像資料每資料道包含多個像素或換言之，待由該圖表產生器之內核處理之基本資料單位包含多個像素。作為一實例，圖14展示在經結構化以含有不同色彩像素之一馬賽克1401(例如呈一拜耳圖案格式)時由一圖表產生器接收的一輸入影像之像素。此處，由圖表產生器接收輸入影像作為由一線緩衝器單元提供之線群組。因而，例如，由圖表產生器接收之各線群組之各列含有R、G及B像素。此處，輸入影像之基本資料單位包含四個像素之一單位胞元1402，其包含一R像素、一B像素及兩個G像素。 圖表產生器代替地對輸入影像資料結構1401執行一解交錯程序以針對包含四種不同類型之圖表之內核產生一新輸入結構1403，而非直接僅剖析來自所接收輸入影像結構1401之圖表(此將建立具有拜耳圖案之圖表)。即，如圖14中所觀察，新輸入結構1403包含：1)僅由輸入影像之R像素組成或以其他方式僅自該等R像素導出之圖表；2)僅由定位於單位胞元輸入影像之單位胞元的一相同第一位置中之G像素組成或以其他方式僅自該等G像素導出的圖表；3)僅由定位於單位胞元輸入影像之單位胞元的一相同第二位置中之G像素組成或以其他方式僅自該等G像素導出的圖表；4)僅由輸入影像之B像素組成或以其他方式僅自該等B像素導出的圖表。該等圖表可僅由輸入影像像素組成或可例如藉由將值內插至不同色彩所在之輸入影像位置中而增加取樣。 接著將新結構化圖表提供至圖表產生器之相關聯內核，該內核處理該等圖表且產生提供回至圖表產生器之相同結構1403之輸出圖表(每圖表一種色彩)。圖表產生器接著對單色結構1403執行一交錯程序以產生一輸出影像以供消費，該輸出影像具有包含混合色彩之一單位胞元之原始結構1401。 在各項實施例中，前述線緩衝器或線緩衝器單元可更一般地經特性化為在生產內核與消費內核之間儲存及轉送影像資料之緩衝器。即，在各項實施例中，一緩衝器不必佇列化線群組。另外，影像處理器之硬體平台包含具有相關聯記憶體資源之複數個線緩衝器單元且一或多個線緩衝器可經組態以自單個線緩衝器單元操作。即，硬體中之單個線緩衝器單元可經組態以在不同生產內核/消費內核對之間儲存及轉送不同影像資料流。 圖15展示如上文所描述之一方法。該方法包含建構一影像處理軟體資料流1501，其中一緩衝器儲存及轉送自一生產內核傳送至一或多個消費內核之影像資料。該方法亦包含辨識該緩衝器不具有足以儲存及轉送該影像資料之資源1502。該方法亦包含修改該影像處理軟體資料流以包含在該影像資料自該生產內核至該一或多個消費內核之該傳送期間儲存及轉送該影像資料之多個緩衝器1503。 3.0 低階程式碼之構造 圖16展示一預執行時間開發環境，其中一程式設計師設計一高階影像處理功能且應用程式開發環境提供章節2.0之任何/所有前述變換使得開發者不必從頭開始識別低效及/或寫入變換。 此處，開發環境自動辨識上文所描述之任何低效且藉由例如參考含有低效(為包含低效，開發環境掃描所開發程式碼)及對應修復(若發現一低效，則強加修復)之描述之一程式庫1601而自動強加對應變換改善。即，開發環境自動插入來自程式庫1601之程式碼，此執行更有效程序(例如，作為一編譯程序之部分)或以其他方式修改程式碼以用包含對低效之修復之新碼替換低效碼。 因此，執行上文所描述之操作或其替代實施例之程式碼可以較高階程式碼或較低階目標碼表達。在各項實施例中，較高階虛擬指令集架構(ISA)碼可指定待操作資料值作為具有x,y位址座標之記憶體讀數，而目標碼可代替地綜合此等資料存取作為二維位移暫存器操作(諸如上文所描述之位移操作或類似實施例之任一者)。 一編譯器可將開發環境中之x,y讀數轉換成被指定目標碼之二維位移暫存器之對應位移(例如，在開發環境中具有x,y座標(+2, +2)之一讀數可以目標碼實現為左移兩個空間及下移兩個空間)。取決於環境，開發者可見此等兩個階(或例如僅較高虛擬ISA階)。在又其他實施例中，可在執行時間(例如，由一即時編譯器)而非預執行時間期間調用此等預寫入常式。 4.0 總結陳述 應自上述章節切實認識到，上文章節1.0中所描述之一影像處理器可體現於一電腦系統上之硬體中(例如，作為一手持器件之系統單晶片(SOC)之部分，其處理來自該手持器件之攝影機之資料)。 需要指出的是，上文所描述之各種影像處理器架構特徵未必僅限於傳統意義上之影像處理且因此可應用於可(或可不)引起影像處理器重新特性化之其他應用。例如，若上文所描述之各種影像處理器架構特徵之任何者用於建立及/或產生及/或渲染動畫而非處理實際攝影機影像，則影像處理器可經特性化為一圖形處理單元。另外，上文所描述之影像處理器架構特徵可應用於其他技術應用，諸如視訊處理、視覺處理、影像辨識及/或機器學習。依此方式應用之影像處理器可與一更通用處理器(例如，即為或係運算系統之一CPU之部分)整合(例如，作為一共處理器)或可為一運算系統內之一獨立處理器。 上文所論述之硬體設計實施例可體現於一半導體晶片內及/或體現為用於最終針對一半導體製程之一電路設計之一描述。就後者而言，此等電路描述可呈以下形式：一(例如VHDL或Verilog)暫存器轉移層次(RTL)電路描述、一閘級電路描述、一電晶體級電路描述或遮罩描述或其各種組合。電路描述通常體現於一電腦可讀儲存媒體(諸如一CD-ROM或其他類型之儲存技術)上。 應自上述章節切實認識到，上文所描述之一影像處理器可體現於一電腦系統上之硬體中(例如，作為一手持器件之系統單晶片(SOC)之部分，其處理來自該手持器件之攝影機之資料)。應注意，在影像處理器體現為一硬體電路之情況中，可自一攝影機直接接收由影像處理器處理之影像資料。此處，影像處理器可為一離散攝影機之部分或具有一整合式攝影機之一運算系統之部分。就後者而言，可自攝影機或運算系統之系統記憶體直接接收影像資料(例如，攝影機將其影像資料發送至系統記憶體而非影像處理器)。亦應注意，上述章節中所描述之諸多特徵可應用於一圖形處理器單元(其渲染動畫)。 圖17提供一運算系統之一例示性描繪。下文所描述之運算系統之諸多組件可適用於具有一整合式攝影機及相關聯影像處理器之一運算系統(例如，一手持器件，諸如一智慧型電話或平板電腦)。一般技術者將能夠容易區分兩者。 如圖17中所觀察，基本運算系統可包含一中央處理單元1701 (其可包含(例如)安置於一多核心處理器或應用處理器上之複數個通用處理核心1715_1至1715_N及一主記憶體控制器1717)、系統記憶體1702、一顯示器1703 (例如觸控螢幕、平板顯示裝置)、一區域有線點對點鏈接(例如USB)介面1704、各種網路I/O功能1705 (諸如乙太網路介面及/或蜂巢式數據機子系統)、一無線區域網路(例如WiFi)介面1706、一無線點對點鏈接(例如Bluetooth)介面1707及一全球定位系統介面1708、各種感測器1709_1至1709_N、一或多個攝影機1710、一電池1711、一電力管理控制單元1712、一揚聲器及麥克風1713以及一音訊編碼器/解碼器1714。 一應用處理器或多核心處理器1750可包含其CPU 1701內之一或多個通用處理核心1715、一或多個圖形處理單元1716、一記憶體管理功能1717 (例如一記憶體控制器)、一I/O控制功能1718及一影像處理單元1719。通用處理核心1715通常執行運算系統之作業系統及應用程式軟體。圖形處理單元1716通常執行圖形密集功能以(例如)產生呈現於顯示器1703上之圖形資訊。記憶體控制功能1717與系統記憶體1702介接以將資料寫入至系統記憶體1702/自系統記憶體1702讀取資料。電力管理控制單元1712一般控制系統1700之電力消耗。 可根據上述章節中所詳細描述之影像處理單元實施例之任何者來實施影像處理單元1719。替代地或組合地，IPU 1719可耦合至GPU 1716及CPU 1701之任一者或兩者作為其之一共處理器。另外，在各項實施例中，可使用上文所詳細描述之影像處理器特徵之任何者來實施GPU 1716。 觸控螢幕顯示器1703、通信介面1704至1707、GPS介面1708、感測器1709、攝影機1710及揚聲器/麥克風編解碼器1713、1714之各者可全部被視為相對於總運算系統(其視情況亦包含一整合式周邊器件(例如一或多個攝影機1710))之各種形式之I/O (輸入及/或輸出)。取決於實施方案，此等I/O組件之各者可整合於應用處理器/多核心處理器1750上或可定位成離開晶粒或定位於應用處理器/多核心處理器1750之封裝外。 在一實施例中，一或多個攝影機1710包含能夠量測攝影機與其視野中之一物件之間的深度的一深度攝影機。在一應用處理器或其他處理器之一通用CPU核心(或具有用於執行程式碼之一指令執行管線之其他功能區塊)上執行之應用程式軟體、作業系統軟體、器件驅動器軟體及/或韌體可執行上文所描述之功能之任何者。此處，圖17之運算系統之諸多組件可存在於執行對應於圖16之應用程式開發環境的程式碼之一較高效能運算系統(例如，一伺服器)內，包含執行上文所描述之所有/任何變換之一編譯器。 本發明之實施例可包含上文所闡述之各種程序。程序可體現於機器可執行指令中。指令可用於引起一通用或專用處理器執行特定程序。替代地，此等程序可由含有用於執行程序之固線式邏輯的特定硬體組件或由程式化電腦組件及客製化硬體組件之任何組合執行。 本發明之元件亦可提供為用於儲存機器可執行指令之一機器可讀媒體。機器可讀媒體可包含(但不限於)軟碟、光碟、CD-ROM及磁光碟、FLASH記憶體、ROM、RAM、EPROM、EEPROM、磁卡或光卡、傳播媒介或適合於儲存電子指令之其他類型之媒體/機器可讀媒體。例如，該等元件可下載為藉由以一載波或其他傳播媒介體現之資料信號經由一通信鏈路(例如一數據機或網路連接)自一遠端電腦(例如一伺服器)傳送至一請求電腦(例如一用戶端)之一電腦程式。 上述說明中已描述特定例示性實施例。然而，應明白，可在不背離隨附申請專利範圍中所闡述之本發明之較廣泛精神及範疇的情況下對本發明作出各種修改及改變。相應地，本說明書及圖式應被視為闡釋性而非限制性。i. The following description describes various embodiments of a new image processing technology platform that provides the use of larger data blocks (eg, line groups and diagrams as described further below) to provide improved power Efficient and versatile application software development environment. 1.0 Hardware Architecture Example a. Image processor hardware architecture and operation FIG. 1 illustrates an embodiment of an architecture 100 for implementing an image processor in hardware. The image processor may be calibrated, for example, by a compiler that converts code written for a virtual processor in a simulated environment into code that is actually executed by a hardware processor. As viewed in FIG. 1, the architecture 100 includes a plurality of line buffer units 101_1 to 101_M (hereinafter, “line buffer”, “line buffer unit”, or the like), which are transmitted through a network 104 (for example, a Network chip (NOC), including a chip-on-chip switching network, a chip-on-chip ring network, or other type of network, interconnected to a plurality of template processor units 102_1 to 102_N (hereinafter referred to as "template processor", "Template processor unit", "image processing core", "core" and the like) and corresponding graph generator units 103_1 to 103_N (hereinafter "graph generator", "graph generator unit" or the like). In one embodiment, any line buffer unit can be connected to any graph generator and corresponding template processor through the network 104. In an embodiment, the code is compiled and loaded onto a corresponding template processor 102 to perform image processing operations previously defined by a software developer (for example, depending on design and implementation, the code may also be To the associated graph generator 103 of the template processor). In at least some examples, a first kernel program of a first pipeline stage can be loaded into a first template processor 102_1, and a second kernel program of a second pipeline stage can be loaded into a The second template processor 102_2 implements an image processing pipeline, wherein the first kernel executes a function of a first stage of the pipeline, the second kernel executes a function of a second stage of the pipeline, and an additional control flow method is installed to output The image data is passed from one stage of the pipeline to the next stage below the pipeline. In other configurations, the image processor may be implemented as a parallel machine having two or more template processors 102_1, 102_2 operating the same kernel code. For example, a highly dense and high data rate image data stream can be processed by distributing frames across multiple template processors, each of which performs the same function. In yet other configurations, the output image from one core can be directed to the DAG design by configuring the respective template processor and its own respective code core and hooking the appropriate control flow into the hardware. The next kernel input loads substantially any DAG of the kernel onto the hardware processor. As a general process, the image data frame is received by a macro I / O unit 105 and is transmitted to one or more of the line buffer units 101 frame by frame. A specific line buffer unit parses its image data frame into a smaller area of image data (referred to as a "line group"), and then passes the line group to a specific chart generator via the network 104. A complete or "full" single line group may, for example, consist of multiple consecutive complete columns or rows of data in a frame (for the sake of brevity, this description will mainly refer to consecutive columns). The chart generator further analyzes the line group of image data into a smaller area of image data (referred to as a "chart"), and presents the chart to its corresponding template processor. For an image processing pipeline or a DAG process with a single input, the input frame is guided to the same line buffer unit 101_1, and the line buffer unit 101_1 analyzes the image data into line groups and leads the line group to The chart generator 103_1 and the corresponding template processor 102_1 of the chart generator 103_1 execute the code of the first kernel in the pipeline / DAG. After the line group processed by the template processor 102_1 is completed, the chart generator 103_1 sends the output line group to a "downstream" line buffer unit 101_2 (in some use cases, the output line group can be sent Go back to the same line buffer unit 101_1) that had previously sent the input line group. Next, one or more "consumer" kernels (which represent the next / middle stage of the pipeline / DAG executed on their own respective other graph generator and template processor (e.g., the graph generator 103_2 and the template processor 102_2) Operation) The image data generated by the first template processor 102_1 is received from the downstream buffer unit 101_2. In this way, a "producer" core that operates a first template processor causes its output data to be transferred to a "consumer" core that operates a second template processor, where the consumer core is in communication with the main pipeline or DAG. After designing a consistent producer kernel, perform the next set of tasks. Here, the line buffer unit 101_2 stores and forwards the image data generated by the producer kernel as a part of transmitting the image data from the producer kernel to the consumer kernel. A template processor 102 is designed to simultaneously operate multiple overlapping templates of image data. The internal hardware processing capacity of multiple overlapping templates and template processors effectively determines the size of a chart. Here, in the template processor 102, a track array operation is performed to simultaneously process the image data surface area covered by multiple overlapping templates. As described in more detail below, in various embodiments, the image data graph is loaded into the two-dimensional register array structure in the template processor 102. It is believed that the use of charts and a two-dimensional displacement register array structure can be used to move a large amount of data into a large amount of register space (as, for example, a single load operation) and then directly to the data directly by an execution track array Perform processing tasks to effectively provide power consumption improvement programs. In addition, using a track array and corresponding register array provides different template sizes that can be easily programmed / configured. 2a to 2e show a high-order analysis activity of a line buffer unit 101, a fine-grained analysis activity of a chart generator unit 103, and a template processing activity of a template processor 102 coupled to the chart generator unit 103 Examples. FIG. 2 a depicts an embodiment of an input frame of the image data 201. Figure 2a also depicts the outline of one of the three overlapping templates 202 that a template processor is designed to operate on (each template has a size of 3 pixels x 3 pixels). The output pixels for which each template generates output image data are highlighted with bold black. For brevity, the three overlapping templates 202 are depicted as overlapping only in the vertical direction. It should be practically recognized that a template processor may actually be designed to have overlapping templates in both vertical and horizontal directions. Due to the vertically overlapping template 202 in the template processor (as viewed in Figure 2a), the frame on which a single template processor can operate stores a wide range of image data. As will be discussed in more detail below, in one embodiment, the template processor processes the data in its overlapping templates across the image data from left to right (and then repeats the next set of lines in order from top to bottom). Therefore, as the template processor continues its operation forward, the number of bold black output pixel blocks will increase horizontally to the right. As discussed above, the line buffer unit 101 is responsible for analyzing a line group of input image data from an incoming frame that is sufficient for the template processor to operate on in an extended number of future cycles. An exemplary depiction of one line group is depicted as a shaded area 203. In an embodiment, as further described below, the line buffer unit 101 may synthesize different dynamic performances to send / receive a line group to a chart generator / receive a line group from a chart generator. For example, according to a mode (called a "full group"), a full-width line of image data is transferred between a line buffer unit and a chart generator. According to a second pattern (called "virtually tall"), a subset of full-width columns is first used to pass a one-line group. Next, the remaining columns are passed in order in smaller (not full-width) pieces. In the case where the line group 203 of the input image data has been defined by the line buffer unit and passed to the chart generator unit, the chart generator unit further analyzes the line group into a more accurate adaptation to the hardware constraints of the template processor. Fine charts. More specifically, as will be described in more detail below, in one embodiment, each template processor is composed of a two-dimensional displacement register array. The two-dimensional displacement register array basically displaces image data "below" an execution track array, where the displacement pattern causes each execution track to operate its own data in its own template (that is, each execution track processes its own information template) To produce one of the templates output). In one embodiment, the chart is "filled" with the surface area of the input image data in the two-dimensional displacement register array or otherwise loaded into the two-dimensional displacement register array. Therefore, as observed in FIG. 2b, the graph generator analyzes an initial graph 204 from the line group 203 and provides it to the template processor (here, the exemplary graph of the data corresponds to a 5 × generally identified by the component symbol 204 5 shaded area). As observed in FIG. 2c and FIG. 2d, the template processor operates the chart of the input image data by effectively moving the overlapping template 202 from left to right on the chart. As of Figure 2d, the number of pixels that have been exhausted and can be calculated from one of the output values in the chart (9 in a blacked out 3 × 3 array) (Other pixel positions must not have one of the information determined from the chart output value). For simplicity, the border area of the image has been ignored. Next, as observed in Figure 2e, the chart generator provides the next chart 205 for the template processor to continue operation. It should be noted that the initial position of the template proceeds to the right from the depletion point on the first chart as it starts to operate on the next chart (as previously depicted in Figure 2d). For the new chart 205, the template will continue to move to the right only when the template processor operates the new chart in the same manner as the first chart. It should be noted that there is some overlap between the data of the first graph 204 and the data of the second graph 205 due to the boundary area of the template surrounding an output pixel position. Overlap can be handled only by the chart generator retransmitting the overlapping data twice. In an alternative embodiment, to feed the next chart to the template processor, the chart generator may continue to send new data only to the template processor and the template processor reuses the overlapping data from the previous chart. b. Template processor design and operation Figure 3a shows an embodiment of a template processor unit architecture 300. As seen in FIG. 3a, the template processor includes a data operation unit 301, a scalar processor 302, an associated memory 303, and an I / O unit 304. The data operation unit 301 includes an execution track array 305, a two-dimensional displacement array structure 306, and a separate individual random access memory 307 associated with a specific column or row of the array. The I / O unit 304 is responsible for loading the "input" data chart received from the chart generator into the data operation unit 301 and storing the "output" data chart from the template processor into the chart generator. In an embodiment, loading the chart data into the data operation unit 301 requires analyzing a receiving chart into image data rows / rows and loading the image data rows / rows into the two-dimensional displacement register structure 306 or the execution path. The columns / rows of the array are in respective random access memories 307 (described in more detail below). If the chart is first loaded into the memory 307, the individual track in the execution track array 305 may load the chart data from the random access memory 307 into the two-dimensional displacement register structure 306 (for example, as A load instruction just before the data of the chart). After loading a data chart into the register structure 306 (whether directly from a chart generator or the memory 307), the track operation data of the track array 305 is executed and finally the "write back" completed data is used as a direct Return to the graph generator or enter one of the random access memory 307 graphs. If the track is written back to the random access memory 307, the I / O unit 304 extracts data from the random access memory 307 to form an output chart which is then transferred to one of the chart generators. The scalar processor 302 includes a program controller 309 that reads the instructions of the template processor's code from the scalar memory 303 and sends the instructions to the execution tracks in the execution track array 305. In one embodiment, a single identical instruction is broadcast to all execution lanes in the array 305 to implement a single instruction multiple data (SIMD) behavior from the data operation unit 301. In one embodiment, the instruction format of the instruction read from the scalar memory 303 and issued to the execution track of the execution track array 305 includes a very long instruction word (VLIW) type format, which includes more than one operation code per instruction. . In another embodiment, the VLIW format includes an ALU operation code (which is guided by the ALU of each execution channel (as described below, in one embodiment, it can specify more than one traditional ALU operation) to perform a mathematical function) And a memory operation code that directs a memory operation on a particular execution path or a set of execution paths. The term "execution path" refers to a group of one or more execution units capable of executing an instruction (for example, a logic circuit that can execute an instruction). However, in various embodiments, an execution lane may include more processor-like functionality than just execution units. For example, in addition to one or more execution units, an execution path may also include logic circuits for decoding and receiving instructions, or for more MIMD designs, logic for extracting and decoding an instruction Circuit. Regarding the MIMD-like method, although a centralized program control method has been mainly described herein, a more decentralized method may be implemented in various alternative embodiments (for example, it includes code in each execution path of the array 305 and a program controller ). The combination of a track array 305, a program controller 309, and a two-dimensional displacement register structure 306 provides a hardware platform that can be widely adapted / configured for a wide range of programmable functions. For example, application software developers can program kernels with a wide range of different functional performance and sizes (such as template sizes), assuming that individual execution lanes can perform various functions and can easily access input images near any output array location data. In addition to serving as a data store for image data operated by the track array 305, the random access memory 307 can also store one or more lookup tables. In various embodiments, one or more scalar lookup tables can also be instantiated in the scalar memory 303 by way of example. Lookup tables are usually used by image processing tasks to, for example, obtain filtering or transformation coefficients for different array positions, and implement complex variable functions (such as gamma curve, sine, cosine) (where the lookup table provides a function output of an input index value), and many more. Here, it is expected that the SIMD image processing sequence will typically perform a lookup on an identical lookup table during a same clock cycle. Similarly, one or more constant tables may be stored in the scalar memory 303. Here, for example, it is expected that different execution paths may require the same constant or other value on the same clock cycle (for example, applying a specific multiplier to an entire image). Therefore, accesses to a constant lookup table return a same scalar value to each of the execution paths. A lookup table is usually accessed using an index value. A scalar lookup involves passing the same data values from the same lookup table from the same index to each execution track in the execution track array 305. In various embodiments, the VLIW instruction format described above is extended to also include a scalar opcode that causes a lookup operation performed by a scalar processor to target a scalar lookup table. An indicator designated for use with an opcode can be an immediate operand or be retrieved from some other data storage location. Anyway, in one embodiment, the lookup from one of the scalar lookup tables in the scalar memory basically involves: broadcasting the same data value to all execution tracks in the execution track array 305 during the same clock cycle. Additional details on the use and operation of lookup tables are provided below. Figure 3b summarizes the VLIW instruction word embodiment (s) discussed above. As observed in Figure 3b, the VLIW instruction word format contains fields for three separate instructions: 1) a scalar instruction 351, which is executed by a scalar processor; 2) an ALU instruction 352, which is broadcast and Executed by the respective ALUs in the execution track array in SIMD mode; and 3) a memory instruction 353, which is broadcast and executed in a part of the SIMD mode (for example, if an execution track in the same row in an execution track array shares the same In random access memory, one execution lane from each of the different rows actually executes the instruction (the format of the memory instruction 353 may include an operand that identifies which execution lane from each row executes the instruction). A field 354 for one or more immediate operands is also included. The command format can be used to identify which of the commands 351, 352, and 353 uses which immediate operand information. Each of the instructions 351, 352, 353 also contains its own input operator and the obtained information (such as a local register for ALU operations and a local register for memory access instructions and a memory location). site). In one embodiment, the scalar processor executes the scalar instruction 351 before any one of the other two instructions 352, 353 is executed by the execution track in the execution track array. That is, the execution of the VLIW word includes a first cycle in which a scalar instruction 351 is executed and then a second cycle in which other instructions 352, 353 are executable (note that in various embodiments, the instructions 352 and 353). In an embodiment, the scalar instruction executed by the scalar processor 302 includes sending to the chart generator 103 to load the chart from the memory of the data operation unit 301 or the 2D displacement register 306 / store the chart to the data operation. The command in the memory of the unit 301 or the 2D displacement register 306. Here, the operation of the chart generator may depend on the operation of the line buffer unit 101 or other variables (which prevents the pre-execution time from including the number of cycles the chart generator 103 will take to complete any command issued by the scalar processor 302) . Therefore, in one embodiment, any VLIW word (whose scalar instruction 351 corresponds to a command to be issued to the chart generator 103 or otherwise causes a command to be issued to the chart generator 103) also includes two other instruction columns No operation (NOOP) instruction in bits 352, 353. Then, the code enters a loop of the NOOP instruction in the instruction fields 352 and 353, until the chart generator finishes loading / storing from the data operation unit. Here, after sending a command to the chart generator, the scalar processor can set a bit of an interlock register, and the chart generator resets the bit after completing the command. During the NOOP loop, the scalar processor monitors the bits of the interlock register. When the scalar processor detects that the chart generator has completed its command, normal execution begins again. FIG. 4 shows an embodiment of a data operation unit 401. As viewed in FIG. 4, the data operation unit 401 includes an execution track array 405 which is logically positioned on one of the “upper” two-dimensional displacement register array structures 406. As discussed above, in various embodiments, an image data chart provided by a chart generator is loaded into the two-dimensional displacement register 406. Then, the track data from the register structure 406 is executed. The track array 405 and the displacement register structure 406 are fixed relative to each other. However, the data in the shift register array 406 is shifted in a strategically coordinated manner to cause each track in the track array to process a different template in the data. Therefore, each execution path determines the output image value of a different pixel in the output chart. It should be understood from the architecture of FIG. 4 that, because the execution track array 405 includes vertically adjacent execution tracks and horizontally adjacent execution tracks, the overlapping templates are not only configured vertically but also horizontally. Some notable architectural features of the data operation unit 401 include a displacement register structure 406 having a size wider than the track array 405. That is, there is a "halo" 409, one of the registers, outside the track array 405. Although the halo 409 is shown as being present on both sides of the execution track array, depending on the implementation, the halo may be present on fewer (one side) or more sides (three or four) of the execution track array 405 Side). The halo 409 is used to provide "spill-over" space for the data, which overflows beyond the boundaries of the execution path array 405 as the data is displaced "below" the execution path 405. To put it simply, when processing the leftmost pixel of a template, a 5 × 5 template centered on the right edge of the execution track array 405 will require four halo register locations further to the right. For ease of drawing, FIG. 4 shows the register on the right side of the halo as only having a horizontal displacement connection and the register on the bottom side of the halo as only having a vertical displacement connection. However, in a nominal embodiment, any The registers on one side (right side, bottom side) will have both horizontal and vertical connections. The extra overflow space is provided by random access memory 407 or portions thereof coupled to the columns and / or rows in the array (e.g., a random access memory can be assigned to 4 execution lanes and 2 One "area" of the execution track array of each execution track. For simplicity, the remainder of the application will primarily refer to the row and / or row based allocation scheme). Here, if the kernel operation of an execution channel needs to process pixel values outside the two-dimensional displacement register array 406 (some image processing routines require such pixel values), the plane of the image data can (for example) be from halo The area 409 further overflows into the random access memory 407. For example, consider a 6 × 6 template, where the hardware contains a halo area of only one of the four storage elements on the right side of an execution track on the right edge of the execution track array. In this case, the data needs to be further shifted to the right from the right edge of halo 409 to fully process the template. Then, the data shifted out of the halo area 409 will overflow to the random access memory 407. Other applications of the random access memory 407 and the template processor of FIG. 3 are further provided below. 5a to 5k illustrate one working example of a method of displacing image data (as mentioned above) in a two-dimensional displacement register array "below" the execution track array. As seen in FIG. 5a, the data content of the two-dimensional displacement array is depicted in a first array 507 and the execution track array is depicted by a frame 505. Also, two adjacent execution tracks 510 in the execution track array are simply depicted. In this simplified depiction 510, each execution path includes a register R1, which can receive data from a shift register, receive data from an ALU output (for example, act as a cross-cycle accumulator), or write output data to To an output destination. Each execution track can also use the contents of a local register R2 in its two-dimensional displacement array "below". Therefore, R1 is a physical register of the execution path, and R2 is a physical register of the two-dimensional displacement register array. The execution path contains an ALU operable with the operands provided by R1 and / or R2. As will be described in more detail below, in one embodiment, the displacement register is actually implemented by a plurality of memory / register elements ("depth" of one of the memory / register elements) per array position, However, the displacement activity is limited to one plane of the storage element (for example, only one plane of the storage element can be displaced per cycle). Figures 5a to 5k depict one of these deeper register locations as used to store the results X from the respective execution lanes. For ease of illustration, the deeper results register is drawn side-by-side with its paired register R2 instead of below its paired register R2. 5a to 5k focus on the calculation of two templates whose center positions are aligned with the execution track position pair 511 depicted in the execution track array 505. For ease of illustration, the execution track pair 510 is drawn as a horizontal neighbor, but in fact, according to the following example, the execution track pair 510 is a vertical neighbor. First, as viewed in Fig. 5a, the execution path 511 is centered on its central template position. Figure 5b shows the object code executed by the two execution lanes 511. As observed in FIG. 5b, the code in the two execution lanes 511 causes the data in the displacement register array 507 to move down one position and to the right by one position. This aligns the two execution lanes 511 to the upper left corner of their respective templates. The code then causes the data located in its respective position (R2) to be loaded into R1. Next, as observed in FIG. 5c, the code causes the execution track pair 511 to shift the data in the displacement register array 507 to the left by one unit, which causes the values to the right of the respective positions of the execution tracks to shift to the positions of the execution tracks in. Next, the value (previous value) in R1 is added to the new value in the position (R2) that has been shifted to the execution path. Write the result to R1. As observed in FIG. 5d, the same procedure as described above for FIG. 5c is repeated, which results in that the resulting R1 now includes the value A + B + C in the upper execution lane and the value F + G + H in the lower execution lane. At this point, the two execution lanes 511 have processed their respective templates. It should be noted that to one of the halo areas on the left side of the track array 505 (if there is a halo area on the left hand side) or to the random access memory (if there is no halo area on the left hand side of the track array 505 ) Spillage. Next, as observed in FIG. 5e, the code causes the data in the shift register array to move up by one unit, which causes the two execution lanes 511 to align with the right edges of the middle rows of their respective templates. The register R1 of the two execution lanes 511 currently contains the sum of the rightmost values of the top and middle columns of the template. Figures 5f and 5g illustrate the progress of moving left across the middle column of the template of the two execution lanes. The accumulation continues so that at the end of the processing of FIG. 5g, the two execution lanes 511 contain the sum of the values of the top and middle columns of their respective templates. Fig. 5h shows another displacement for aligning each execution track with the bottom of its corresponding template. Figures 5i and 5j show the continued displacement of the process for completing the entire template of the two execution lanes. Fig. 5k shows additional displacements used to align each execution track with its correction position in the data array and write the result into the correction position. It should be noted that in the examples of FIGS. 5a to 5k, the target code used for the displacement operation may include an instruction format identifying the direction and magnitude of the displacement expressed in the (X, Y) coordinates. For example, the target code for moving up one position can be expressed as SHIFT 0, +1. As another example, shifting one position to the right can be expressed as SHIFT + 1, 0. In various embodiments, a larger amount of displacement (eg, SHIFT 0, +2) can also be specified in the target code. Here, if the 2D displacement register hardware only supports one position shift per cycle, the instructions can be interpreted by the machine as requiring multiple cycles to execute, or the 2D displacement register hardware can be designed to support more than one position shift per cycle. . The latter embodiment will be described in further detail below. FIG. 6 shows another more detailed depiction of the unit cell of the array execution path and the displacement register structure (the register in the halo area does not include a corresponding execution path). In one embodiment, the execution track and the register space associated with each position in the execution track array are implemented by instantiating the circuit observed in FIG. 6 at each node of the execution track array. As observed in FIG. 6, the unit cell includes an execution track 601, which is coupled to a register file 602 composed of four registers R1 to R4. During any period, the execution path 601 can be read from or written to any of the registers R0 to R4 to any of the registers R0 to R4. For instructions that require two input operands, the execution path can retrieve two operands from any of R0 to R4. In one embodiment, the two-dimensional displacement register structure is implemented by the following operations: Allow the content of any (only) one of the registers R1 to R3 to be shifted out by the output multiplexer 603 during a single cycle To one of its neighbor's register files and to move the contents of any (only) one of the registers R1 to R3 from the counterpart of one of its neighbors via the input multiplexer 604 Content replacement so that displacements between neighbors are in the same direction (eg, all execution lanes are shifted left, all execution lanes are shifted right, etc.). Although it is more common for an identical register to have its content moved out of and replaced by the moved-in content, multiplexer configurations 603, 604 allow different sources of displacement within the same register file during the same period And displacement target register. It should be noted that, as depicted in FIG. 6, during a displacement sequence, an execution track will move content from its register file 602 to its neighbors on the left, right, top, and bottom. Combined with the same displacement sequence, the execution path will also move the content from one of its specific neighbors to the left, right, top, and bottom to its register file. Also, the moving target and moving source should be in the same displacement direction as one of all the execution paths (for example, if the moving system is toward the right neighbor, the moving should be from the left neighbor). Although in one embodiment, the contents of only one register are allowed to be shifted per execution track per cycle, other embodiments may allow the contents of more than one register to be moved in / out. For example, if a second example of one of the multiplexer circuits 603, 604 observed in FIG. 6 is incorporated into the design of FIG. 6, the contents of the two registers can be moved in / out during the same cycle. Of course, in an embodiment that allows the contents of only one register to be shifted per cycle, displacements from multiple registers can occur between mathematical operations by making the displacement between mathematical operations consume more clock cycles. (For example, the contents of two registers can be shifted between mathematical operations by consuming two shift operations between mathematical operations). If not all the contents of the scratchpad file of an execution track are removed during a displacement sequence, it should be noted that the contents of the execution track without being removed from the scratchpad are kept in place (unshifted). Thus, any undisplaced content that has not been replaced by the moved-in content persists across the displacement period at a portion of the execution path. The memory unit ("M") observed in each execution lane is used to load data from / store data to and from the random access memory space associated with the rows and / or rows of execution lanes within the execution lane array. Random access memory space associated with the row and / or row of execution lanes within a track array. Here, the M unit serves as a standard M unit because it is commonly used to load / store data that cannot be loaded / stored from the register space of the execution track itself. In various embodiments, the main operation of the M unit is to write data from a local register into the memory and read data from the memory and write it into a local register. Regarding the instruction set architecture (ISA) operation code supported by the ALU unit of the hardware execution path 601, in various embodiments, the mathematical operation code supported by the hardware ALU and the mathematical operation code supported by a virtual execution path (for example, ADD, SUB, MOV, MUL, MAD, ABS, DIV, SHL, SHR, MIN / MAX, SEL, AND, OR, XOR, NOT) are integrated together (for example, substantially the same). As described above, the memory access instruction may be executed by execution channel 601 to extract data from its associated random access memory / store data to its associated random access memory. In addition, the hardware execution path 601 supports displacement calculation instructions (rightward, leftward, upward, downward) for displacement of data in the two-dimensional displacement register structure. As described above, the program control instructions are mainly executed by the scalar processor of the template processor. 2.0 Code conversion to improve execution time efficiency As described in detail above, applications developed for image processors can be defined by combining smaller, fine-grained software programs (referred to herein as kernels) into a larger overall structure, such as a directed acyclic graph. Program software. The definition typically involves coupling different cores into a particular data stream pattern, with several "production" cores supplying their output image data to one or more "consumer" cores. At least one kernel receives an overall input image of the application software program operation and typically, one of these kernels produces an overall output image of the application software. Each core is then mapped to a specific template processor. Each template processor has an associated chart generator that receives image data to be operated by the kernel of its associated template processor. In various embodiments, the image generator receives the image data in a line group manner. For example, the chart generator may receive image data as a number of rows across the full width of an input image frame. The chart generator then forms a two-dimensional image data "chart", which is provided to the template processor and finally loaded into the two-dimensional displacement register array of the template processor. In various embodiments, dedicated hardware logic circuits (e.g., application specific integrated circuit (ASIC) logic circuits), programmable logic circuits (e.g., field programmable gate array logic circuits), embedded processors are used The logic circuit or any combination of these implements the chart generator to implement the functionality of the chart generator. The dedicated hardware logic circuit (if any) has an associated configuration register that is set with information generated by the compiler of the application software, which information causes the chart generator to process the mapping to the template associated with the chart generator The kernel of the processor performs chart generation activities. Programmable logic circuits (if any) are programmed with information generated by the compiler of the application software, which causes the programmable logic circuits to process the template templates that are mapped to the graph generator and wait for template processing. The kernel executing on the implements the graph generator functionality. The embedded processor circuit (if any) has code generated by a compiler of the application software, which when executed by the embedded processor causes the embedded processor to process the associated template that has been mapped to the chart generator The generator, the kernel to be executed on the template processor, implements the graph generator functionality. The scalar processor of the template processor can also be programmed to execute, assist or otherwise participate in various chart generation activity tasks. The same kind of circuit implementation probability and associated compiled code and / or information may also exist with respect to line buffer circuits. The application software development process therefore includes not only mapping a kernel to a specific template processor but also generating associated configuration information and / or code that is used to perform graphics generation activities against the kernel. In various application software program development environments, it is responsible for accepting a high-level description of an application software program and responding to generate low-level code (for example, object code) and any associated configuration information for execution by the image processor. One compiler will recognize various inefficiencies in the application software and change the compiled code to improve or otherwise reduce inefficiencies. The changed code may be code and / or configuration information for one or more chart generators and / or kernels to be fed by their and / or line buffer units. Figure 7a relates to a first potential inefficiency. As viewed in Fig. 7a, an input image 701 is received by a chart generator as a number of line groups sent, for example, by a line buffer unit. As observed in Figure 7a, the input image 702 is, for example, downsampled by the graph generator before the kernel K1 processing performed on a template processor coupled to a graph generator. Alternatively, kernel K1 may be programmed to perform downsampling. In various embodiments, the template processor naturally creates an output image chart having the same size as the execution track array of the template processor. For example, in one embodiment in which the size of the execution track array is 16 pixels × 16 pixels, the kernel processor K1 of the template processor is initially configured to generate a 16-pixel × 16-pixel output image chart. If the template processor is configured to produce an output chart with the same size as its execution track array from the input image that has been downsampled, a large amount of buffer space is required. For example, referring to FIG. 7a, if the down-sampling 702 is performed by the chart generator to create a 16-pixel x 16-pixel down-sampling chart 703 to load into the two-dimensional displacement register of the template processor, the chart generator will need Queue a whole 32 pixel x 32 pixel input image 701 to form a 16 pixel x 16 pixel down-sampled input image 703 for consumption by the kernel K1. One of the inefficient forms of allocating the massive memory system required for this queueing. Thus, in one embodiment, a compiler will reassemble the application software program (including, for example, any relevant configuration information), as depicted in Figure 7b. Specifically, the compiler will structure the code so that the kernel K1 does not operate without fully utilizing its execution track array. Continuing this example, kernel K1 is instead designed to operate an 8-pixel x 8-pixel input chart 703b, which causes kernel K1 to generate an 8-pixel x 8-pixel output chart 704b. By configuring the kernel K1 to operate a smaller 8-pixel x 8-pixel input chart 703b, compared to the input image data 701a of FIG. 7a, reducing the sampling activity 702b (e.g., as performed by a chart generator) requires only half the amount of enqueuing The input image data 701b. The input image data 701b of FIG. 7b corresponds to only 16 rows of the input image data, while the input image data 701a of FIG. 7a corresponds to 32 rows of the image data. In the case of only 16 columns of input image data 701b, downsampling activity 702b can perform 2: 1 downsampling, which results in a series of 8-pixel x 8-pixel input charts 703b that will span the full width of the image. Figures 8a and 8b show another inefficiency in which upsampling is performed on the output image data 801 of a kernel K1 and then a same amount of downsampling is performed before the image data is executed by the consumption kernel K2 of K1. Here, as observed in Fig. 8a, the production kernel K1 generates a series 801 of output charts A0 to A3. The image data of these output charts 801 are then interleaved to effectively increase the output of sample K1. That is, as viewed in FIG. 8 a, for example, one of the top lines of the output graphs A0 to A3 is interleaved to form one of the top output lines of the upsampling K1 output 803 stored in the line buffer 802, the line buffer 802. The output data of K1 is temporarily queued before it is consumed by the consumption kernel K2 of K1. In various embodiments, any one of K1, a graph generator coupled to the template processor on which K1 executes, or line buffer 802 to which K1 sends its output performs upsampling. As observed in Fig. 8b, the input processing of the kernel K2 for the output of the consumption K1 is configured to down-sample its input by an equal multiple of the output of the up-sample K1. Therefore, the process of feeding the input data of appropriate size to K2 needs to reverse the upsampling process performed on the output of K1. That is, referring to FIG. 8b, the interleaved dequeuing data 803 in the line buffer 802 is finally de-interleaved to re-form the output images A0 to A3 originally formed by K1. Downsampling can be performed by any of the line buffer 802, the graph generator coupled to the template processor on which K2 executes, or K2 itself. In one embodiment, a compiler is designed to identify when the increased sampling output of a production core is downsampled by a same multiple of a core that will consume the output of the production core (which may include multiple such cores) (e.g., , 1: 2 increase sampling and 2: 1 decrease sampling). In response, the compiler will further restructure the developed code to eliminate both upsampling and downsampling along the producer-to-consumer data path. This solution is depicted in Figure 8c. Here, the non-upsampling output of K1 is only queued in a line buffer 802 coupled between the K1 and K2 connections. The non-upsampling K1 output is then fed directly to K2 without any downsampling. Thus, both the increased sampling activity of Figure 8a and the reduced sampling activity of Figure 8b are avoided. Figures 9a and 9b relate to another inefficiency that can occur, for example, in the case of multi-component output images. As is known in the art, digital images can have multiple components (eg, RGB, YUV, etc.). Various application software programs can be designed / configured to process different components into different data planes. Here, for example, a complete output image 901 can be generated entirely by a production kernel K1 by the following steps: generating one or more data graphs consisting of only the first component (R); generating only the second component (G ) To form one or more data charts; and generate one or more data charts consisting of only a third component (B). In various embodiments, enqueuing all the data of an image transmitted between a production core and a consumer core in a same line buffer 902 may be a natural or standard preset. Therefore, FIG. 9a shows image data of all three components 901 queued in a same line buffer unit 902. However, in the case of, for example, a large output image, storing all three components of image data in a same line buffer unit may strain or otherwise consume a large amount of line buffer memory resources. Therefore, in an embodiment, referring to FIG. 9b, a compiler that compiles an application software program will automatically recognize when different components of a multi-component image can consume line buffer memory resources. For example, the compiler may initially allocate a fixed amount of buffer memory resources to store and forward images or may allocate a quantitative amount of buffer memory resources related to the size and / or amount of data to be transmitted and, given that allocation, may determine The amount of automatic allocation is insufficient or reaches a certain maximum threshold. In other methods, the compiler may include a simulation application software program and an identification line buffer unit that is a bottleneck. Has bandwidth to respond to read requests from a consumer core). According to this response, the compiler automatically modifies the application software and / or reconfigures the image processor so that different components of the output image of the production core K1 are queued in different line buffer units. Here, FIG. 9b shows that the R, G, and B image data are queued in different line buffer units 902_1, 902_2, and 902_3, respectively. The solution of Fig. 9b can also be used in a situation where the production kernel K1 has many consumers. In this case, if the default solution of FIG. 9a is adopted, a single line buffer unit storing all components of the image data 901 can become a system bottleneck, because a large number of consumers will need to load / read from the line buffer Fetch multiple times to receive all the information for a single input image. Therefore, in an embodiment, the method of FIG. 9b is adopted, in which each line buffer only stores data of a same component type. In the example discussed, this will reduce the number of read requests made by a consumer to a single line buffer resource by up to 66% compared to the default method of FIG. 9a. That is, each of the line buffer units 902_1, 902_2, and 902_3 of FIG. 9b will only need to support 33% of the consumption read load of the line buffer unit 902 of FIG. 9a. For the writing activity of the image data in the production core to the line buffer resource, a similar reduction in demand impact occurs. Another situation where the method of FIG. 9b can reduce inefficiency is that a specific consumer consumes only a subset of the components. For example, in one extreme case, one consumer consumes the R component, another consumer consumes the G component, and the other consumer consumes the B component. In this case, each different consumer is configured with its own dedicated line buffer resource that streamlines different component-based data streams (connected through different line buffer units) along different data paths. In contrast, if the method of FIG. 9a is used, the data streams based on different components will be gathered at a single point of the line buffer 902 of FIG. 9a. In this case, the data stream of one component can be forwarded to other components. The large amount of read and write activity at the line buffer unit 902 is sluggish. Figures 10a and 10b show another efficiency improvement based on the distribution of a single consumer downstream buffer resource. Here, the presence of too many consumers can force the use of multiple line buffer units to forward output image data from a single production core. Figure 10a shows the potential inefficiency of four different consumers K2 to K5 consuming the output of a single production core K1 from a single line buffer unit 1002. Also, a single line buffer unit 1002 can be a bottleneck because it cannot clear its queued data until all consumers have consumed the data. In this case, the overall data stream from the line buffer unit 1002 will be minimized to the input rate of its slowest consumer. In addition, given that a large number of consumers supported by the line buffer unit 1002 can overwhelm the resources of the line buffer unit 1002, the line buffer unit 1002 will receive a heavy load read request. Thus, as depicted in Figure 10b, a first consumer subset K2, K3 is assigned to a first line buffer unit 1002_1 and a second consumer subset K4, K5 is assigned to a second line buffer Unit 1002_2. The output image stream of the production core K1 is fed to the two line buffer units 1002_1 and 1002_2. Spreading the total consumer load among multiple line buffer unit resources 1002_1, 1002_2 helps reduce the overall demand for any particular line buffer unit resource (compared to the method of Figure 10a). In addition, the compiler may be able to use a same line buffer unit to feed the faster input stream consumption core (and / or use a different line buffer unit to feed the slower input stream consumption core) so that the faster consumption core does not The consumption rate of the slow input rate consumption core is slower and slower. Fig. 11a shows that "split and join" inefficiency, which can be caused by one of the application software programs (or components thereof) designed as a DAG. As observed in Figure 11a, the output of a source kernel K1 is fed to two different consumer kernels K2 and K3. In addition, kernel K3 consumes the output of kernel K2. The dual dependency of the outputs of kernel K3 and kernel K1 can cause both inefficient execution time calculations and inefficient modeling / design. Regarding inefficient execution time, the LB2 line buffer 1102_2 may need to be made extremely large in order to queue a large amount of K1 output data. Normally, kernel K3 will not request the first-line group from LB2 1102_2 until approximately when the first-line group from LB3 1102_3 is processed by kernel K3 together with the first-line group from LB2 1102_2. Due to the possible large propagation delay through K2, LB2 1102_2 can become extremely large. During the design of the application software, the aforementioned differences between when the data in LB2 1102_2 is ready to be consumed and when its input data at the same level from kernel K2 to K3 are available in LB3 1102_3 can also be modeled or optimized. The optimization process becomes more difficult. Figure 11b shows a solution where one of the compilers imposes a pipeline structure on the split and join structure. Here, the K2 kernel of Fig. 11a is expanded into a different kernel K2 ', which contains the original K2 kernel plus a load / store algorithm 1103 that consumes only content from LB1 1102_1 and forwards it to one of LB4 1102_4. Importantly, the load / store algorithm 1103 can cause a certain propagation delay of the unprocessed stream from K1, which eliminates the need for the original output data from K1 to be consumed by K3 and the output from K2 in LB3 1102_3 The difference between when data is ready to be consumed by K3. As recalled from the discussion of FIG. 3a, in various embodiments, the scalar memory 303 may be configured to store a lookup table or a constant table. In certain applications, the input image data processed by a kernel is a fixed constant rather than variable information (for example, generated by a source kernel that operates on input data that changes). One example is lens shading correction, where the correction value of the lens is recorded, for example, for regions of different considerable granularity on the lens surface. The considerable granularity corresponds to a low-resolution image data (if the recorded data is implemented as different items, where each item corresponds to a different die, the recorded data does not include many items). When the image processor processes an image from a camera containing a lens, one of these recorded correction values corresponds to the image area processed by the track array. The recorded value is therefore applied as an input value to each execution track. In this sense, the lens correction values are similarly implemented as a lookup table or a constant table. In addition, when the total amount of data required to implement the correction value is limited, the correction value does not consume a large amount of memory space. Thus, as observed in FIG. 12, in various embodiments, the input image data 1210 that is fixed and small enough to fit the scalar memory 1203 is loaded into the scalar memory 1203 (for example, as one of the application software) (Initial configuration) and is referenced by the kernel executing on the execution track array of the scalar processor during the execution time as a lookup table or a constant table (rather than, for example, generated by a source kernel and fed to the kernel through a line buffer unit) . Figure 13a shows another execution time issue that can potentially cause large amounts of data to be queued in a first-line buffer unit and / or a graph generator. Here, FIG. 13a depicts three line groups 1301, 1302, 1303 that are queued in a chart generator, for example, after a line buffer unit is provided. For example, assume that each of the line groups 1301, 1302, and 1303 contains 16 rows of image data and the size 1305 of the execution track array of the template processor corresponding to the chart generator is also 16 pixels x 16 pixels. In addition, it is assumed that the size of the two-dimensional displacement register array 1306 is 24 pixels × 24 pixels to support forming a halo region with a 4-pixel wide boundary around the periphery of the execution track array. In at least these circumstances, a natural configuration can align the 16 columns of the track array 1305 with the 16 columns of a particular line group. That is, the chart generator forms a chart centered on a specific line group. FIG. 13a shows this method in which the execution lanes 1305 are aligned to operate within the height of the second line group 1302. One problem is as depicted in Figure 13a. Due to the presence of halo 1306, the complete chart fed into the two-dimensional displacement register array will need to come from the area below one of the first line group 1301 and the third line group 1303. The data of the upper area (the halo area also covers these line groups). Thus, in one embodiment, as depicted in FIG. 13b, changing the alignment requires that there be a minimum number of line groups to form a full-size chart. In this example, the alignment of FIG. 13b is shifted up to four pixel values relative to the alignment of FIG. 13a so that only two line groups 1301, 1302 need to exist in the chart generator to form a full-size chart. Doing so not only requires less memory space (and potentially a ground line buffer) in the graph generator, but the graph only needs to wait for two line groups to begin processing rather than waiting for three line groups to begin processing. FIG. 14 is a de-interlacing procedure performed by a chart generator as an input procedure of a kernel fed with input image data. Each input image data channel contains a plurality of pixels or in other words, to be processed by the chart generator. The basic data unit processed by the kernel contains multiple pixels. As an example, FIG. 14 shows the pixels of an input image received by a chart generator when structured to contain a mosaic 1401 (eg, in a Bayer pattern format) of pixels of different colors. Here, the input image is received by the chart generator as a line group provided by a line buffer unit. Thus, for example, each column of each line group received by the chart generator contains R, G, and B pixels. Here, the basic data unit of the input image includes a unit cell 1402 of four pixels, which includes one R pixel, one B pixel, and two G pixels. The chart generator instead performs a de-interlacing process on the input image data structure 1401 to generate a new input structure 1403 for a kernel containing four different types of charts, rather than directly parsing only the charts from the received input image structure 1401 (this A chart with a Bayer pattern will be created). That is, as observed in FIG. 14, the new input structure 1403 includes: 1) a graph composed of or otherwise derived only from the R pixels of the input image; 2) an input image located only from the unit cell A graph of G pixels in a same first position of a unit cell or otherwise derived from only those G pixels; 3) a same second position of a unit cell located only in a unit cell input image A graph composed of G pixels in or otherwise derived from only those G pixels; 4) a graph composed of B pixels of the input image or otherwise derived from only those B pixels. The graphs may consist of only input image pixels or may increase sampling, for example, by interpolating values into input image locations where different colors are located. The new structured chart is then provided to the associated kernel of the chart generator, which processes the charts and generates output charts (one color per chart) of the same structure 1403 provided back to the chart generator. The chart generator then performs an interleaving process on the monochrome structure 1403 to generate an output image for consumption, the output image having an original structure 1401 containing a unit cell of mixed colors. In various embodiments, the aforementioned line buffer or line buffer unit may be more generally characterized as a buffer that stores and transfers image data between the production core and the consumer core. That is, in various embodiments, a buffer does not need to queue groups of lines. In addition, the hardware platform of the image processor includes a plurality of line buffer units with associated memory resources and one or more line buffers can be configured to operate from a single line buffer unit. That is, a single line buffer unit in hardware can be configured to store and forward different image data streams between different production core / consumer core pairs. Figure 15 shows one method as described above. The method includes constructing an image processing software data stream 1501, in which a buffer stores and forwards image data transmitted from a production core to one or more consumer cores. The method also includes identifying that the buffer does not have sufficient resources 1502 to store and forward the image data. The method also includes modifying the image processing software data stream to include a plurality of buffers 1503 that store and forward the image data during the transmission of the image data from the production core to the one or more consumer cores. 3.0 Construction of low-level code Figure 16 shows a pre-execution time development environment in which a programmer designs a high-level image processing function and the application development environment provides any / all of the aforementioned transformations of section 2.0 so that developers do not have to identify inefficient and / or write transformations from scratch . Here, the development environment automatically recognizes any inefficiencies described above and includes, for example, references containing inefficiencies (to scan the code developed for inclusion of inefficiencies, the development environment) and corresponding repairs (if an inefficiency is found, imposes a repair ) Describes a library 1601 and automatically imposes corresponding transformation improvements. That is, the development environment automatically inserts code from the library 1601, which executes more efficient procedures (for example, as part of a compilation process) or otherwise modifies the code to replace the inefficient with new code that includes fixes to the inefficient code. Therefore, the code that performs the operations described above or an alternative embodiment thereof may be expressed in a higher-order code or a lower-order object code. In various embodiments, the higher-level virtual instruction set architecture (ISA) code may specify the data value to be operated as a memory reading with x, y address coordinates, and the target code may instead synthesize these data accesses as two Dimensional displacement register operations (such as the displacement operations described above or any of the similar embodiments). A compiler can convert the x, y readings in the development environment to the corresponding displacements of the two-dimensional displacement register of the specified target code (for example, one of the x, y coordinates (+2, +2) in the development environment The reading can be realized by the target code being shifted to the left by two spaces and shifted down by two spaces). Depending on the environment, developers can see these two stages (or only higher virtual ISA stages, for example). In yet other embodiments, these pre-write routines may be called during execution time (e.g., by a just-in-time compiler) rather than pre-execution time. 4.0 Summary statement It should be effectively recognized from the above sections that one of the image processors described in section 1.0 above may be embodied in hardware on a computer system (for example, as part of a system-on-chip (SOC) as a handheld device, its processing Information from the camera of the handheld device). It should be pointed out that the various image processor architecture features described above are not necessarily limited to image processing in the traditional sense and therefore can be applied to other applications that may (or may not) cause image processor re-characterization. For example, if any of the various image processor architecture features described above are used to create and / or generate and / or render animations instead of processing actual camera images, the image processor may be characterized as a graphics processing unit. In addition, the image processor architecture features described above can be applied to other technical applications, such as video processing, visual processing, image recognition, and / or machine learning. An image processor applied in this manner may be integrated with a more general-purpose processor (e.g., being part of or a part of a CPU of a computing system) (e.g., as a co-processor) or may be independently processed within a computing system Device. The hardware design embodiments discussed above may be embodied in a semiconductor wafer and / or embodied as a description of a circuit design for a semiconductor process. In the latter case, these circuit descriptions can take the form of: a (such as VHDL or Verilog) register transfer level (RTL) circuit description, a gate-level circuit description, a transistor-level circuit description or mask description, or Various combinations. The circuit description is typically embodied on a computer-readable storage medium, such as a CD-ROM or other type of storage technology. It should be effectively recognized from the above sections that one of the image processors described above may be embodied in hardware on a computer system (e.g., part of a system-on-chip (SOC) as a handheld device whose processing comes from the handheld Device camera information). It should be noted that in the case where the image processor is embodied as a hardware circuit, the image data processed by the image processor can be directly received from a camera. Here, the image processor may be part of a discrete camera or part of a computing system with an integrated camera. In the latter case, the image data can be received directly from the system memory of the camera or computing system (for example, the camera sends its image data to the system memory instead of the image processor). It should also be noted that many of the features described in the above section can be applied to a graphics processor unit (which renders animations). FIG. 17 provides an exemplary depiction of a computing system. Many of the components of the computing system described below are applicable to a computing system (eg, a handheld device such as a smart phone or tablet) with an integrated camera and associated image processor. A person of ordinary skill will be able to easily distinguish the two. As observed in FIG. 17, the basic computing system may include a central processing unit 1701 (which may include, for example, a plurality of general-purpose processing cores 1715_1 to 1715_N and a main memory disposed on a multi-core processor or an application processor Controller 1717), system memory 1702, a display 1703 (such as a touch screen, a flat panel display device), an area wired point-to-point link (such as USB) interface 1704, various network I / O functions 1705 (such as Ethernet Interface and / or cellular modem subsystem), a wireless local area network (e.g. WiFi) interface 1706, a wireless point-to-point link (e.g. Bluetooth) interface 1707 and a global positioning system interface 1708, various sensors 1709_1 to 1709_N, One or more cameras 1710, a battery 1711, a power management control unit 1712, a speaker and microphone 1713, and an audio encoder / decoder 1714. An application processor or multi-core processor 1750 may include one or more general-purpose processing cores 1715, one or more graphics processing units 1716, a memory management function 1717 (e.g., a memory controller) within its CPU 1701, An I / O control function 1718 and an image processing unit 1719. The general-purpose processing core 1715 usually executes an operating system and application software of a computing system. The graphics processing unit 1716 typically performs graphics-intensive functions to, for example, generate graphics information presented on the display 1703. The memory control function 1717 interfaces with the system memory 1702 to write data to the system memory 1702 / to read data from the system memory 1702. The power management control unit 1712 generally controls the power consumption of the system 1700. The image processing unit 1719 may be implemented according to any of the image processing unit embodiments described in detail in the above section. Alternatively or in combination, the IPU 1719 may be coupled to either or both of the GPU 1716 and the CPU 1701 as one of its coprocessors. Additionally, in various embodiments, GPU 1716 may be implemented using any of the image processor features described in detail above. Each of the touch screen display 1703, the communication interfaces 1704 to 1707, the GPS interface 1708, the sensor 1709, the camera 1710, and the speaker / microphone codec 1713, 1714 can all be regarded as relative to the total computing system (which depends on the situation) It also includes various forms of I / O (input and / or output) of an integrated peripheral device (such as one or more cameras 1710). Depending on the implementation, each of these I / O components may be integrated on the application processor / multi-core processor 1750 or may be positioned off the die or located outside the package of the application processor / multi-core processor 1750. In one embodiment, the one or more cameras 1710 include a depth camera capable of measuring the depth between the camera and an object in its field of view. Application software, operating system software, device driver software, and / or software running on a general-purpose CPU core (or other functional block with an instruction execution pipeline for executing code) on an application processor or other processor The firmware can perform any of the functions described above. Here, many components of the computing system of FIG. 17 may exist in a higher-performance computing system (for example, a server) that executes code corresponding to the application development environment of FIG. 16, including performing the operations described above. All / any transform one compiler. Embodiments of the invention may include various procedures described above. Programs may be embodied in machine-executable instructions. Instructions can be used to cause a general-purpose or special-purpose processor to execute a particular program. Alternatively, these programs may be executed by specific hardware components containing fixed-line logic for executing the programs or by any combination of programmed computer components and custom hardware components. Elements of the present invention may also be provided as a machine-readable medium for storing machine-executable instructions. Machine-readable media may include (but is not limited to) floppy disks, optical disks, CD-ROMs and magneto-optical disks, flash memory, ROM, RAM, EPROM, EEPROM, magnetic or optical cards, transmission media, or other suitable for storing electronic instructions Type of media / machine-readable media. For example, the components can be downloaded to be transmitted from a remote computer (e.g., a server) to a computer via a communication link (e.g., a modem or network connection) by a data signal embodied in a carrier wave or other media. A computer program that requests a computer (such as a client). Specific exemplary embodiments have been described in the above description. It should be understood, however, that various modifications and changes may be made to the present invention without departing from the broader spirit and scope of the invention as set forth in the scope of the accompanying patent application. Accordingly, the description and drawings are to be regarded as illustrative rather than restrictive.
100‧‧‧架構100‧‧‧ architecture
101_1至101_M 線緩衝器單元101_1 to 101_M line buffer units
102_1至102_N‧‧‧模板處理器/模板處理器單元102_1 to 102_N‧‧‧ template processor / template processor unit
103_1至103_N‧‧‧圖表產生器單元/圖表產生器103_1 to 103_N‧‧‧‧Chart generator unit / chart generator
104‧‧‧網路104‧‧‧Internet
105‧‧‧巨集I/O單元105‧‧‧Macro I / O Unit
201‧‧‧影像資料201‧‧‧Image data
202‧‧‧重疊模板202‧‧‧ Overlay Template
203‧‧‧線群組203‧‧‧line group
204‧‧‧圖表204‧‧‧ chart
205‧‧‧圖表205‧‧‧ chart
300‧‧‧模板處理器單元架構300‧‧‧Template processor unit architecture
301‧‧‧資料運算單元301‧‧‧Data Operation Unit
302‧‧‧純量處理器302‧‧‧ scalar processor
303‧‧‧記憶體303‧‧‧Memory
304‧‧‧I/O單元304‧‧‧I / O unit
305‧‧‧執行道陣列305‧‧‧ Execution Road Array
306‧‧‧二維位移陣列結構306‧‧‧Two-dimensional displacement array structure
307‧‧‧隨機存取記憶體307‧‧‧ Random Access Memory
309‧‧‧程式控制器309‧‧‧Program Controller
351‧‧‧純量指令351‧‧‧scalar instruction
352‧‧‧ALU指令/指令欄位352‧‧‧ALU Instruction / Instruction Field
353‧‧‧記憶體指令/指令欄位353‧‧‧Memory Command / Command Field
354‧‧‧欄位354‧‧‧field
401‧‧‧資料運算單元401‧‧‧Data Operation Unit
405‧‧‧執行道陣列405‧‧‧ Execution Road Array
406‧‧‧二維位移暫存器陣列結構406‧‧‧Two-dimensional displacement register array structure
407‧‧‧隨機存取記憶體407‧‧‧RAM
409‧‧‧暈圈/暈圈區域409‧‧‧Halo / Halo area
505‧‧‧執行道陣列505‧‧‧Executive track array
507‧‧‧位移暫存器陣列507‧‧‧shift register array
510‧‧‧執行道510‧‧‧Execution Road
511‧‧‧執行道位置/執行道511‧‧‧Execution Road Location / Execution Road
601‧‧‧執行道601‧‧‧Execution Road
602‧‧‧暫存器檔案602‧‧‧Register file
603‧‧‧輸出多工器/多工器配置/多工器電路603‧‧‧Output Multiplexer / Multiplexer Configuration / Multiplexer Circuit
604‧‧‧輸入多工器/多工器配置/多工器電路604‧‧‧Input Multiplexer / Multiplexer Configuration / Multiplexer Circuit
701a‧‧‧輸入影像資料701a‧‧‧Input image data
701b‧‧‧輸入影像資料701b‧‧‧Input image data
702a‧‧‧降低取樣702a‧‧‧ downsampling
702b‧‧‧降低取樣活動702b‧‧‧Sampling down
703a‧‧‧降低取樣圖表/降低取樣輸入影像703a‧‧‧‧Sampling down graph / downsampling input image
703b‧‧‧降低取樣圖表/降低取樣輸入影像703b ‧‧‧ downsampling graph / downsampling input image
704a‧‧‧輸出圖表704a‧‧‧ output chart
704b‧‧‧輸出圖表704b‧‧‧ output chart
801‧‧‧輸出影像資料/輸出圖表801‧‧‧ Output image data / output chart
802‧‧‧線緩衝器802‧‧‧line buffer
803‧‧‧增加取樣K1輸出/交錯佇列化資料803‧‧‧Increase sampling K1 output / interleaved queue data
901‧‧‧輸出影像/影像資料901‧‧‧ output image / image data
902‧‧‧線緩衝器/線緩衝器單元902‧‧‧line buffer / line buffer unit
1002‧‧‧線緩衝器單元1002‧‧‧line buffer unit
1002_1‧‧‧線緩衝器單元1002_1‧‧‧line buffer unit
1002_2‧‧‧線緩衝器單元1002_2‧‧‧line buffer unit
1102_1‧‧‧線緩衝器1102_1‧‧‧line buffer
1102_2‧‧‧線緩衝器1102_2‧‧‧line buffer
1102_3‧‧‧線緩衝器1102_3‧‧‧line buffer
1102_4‧‧‧線緩衝器1102_4‧‧‧line buffer
1103‧‧‧載入/儲存演算法1103‧‧‧Load / Save Algorithm
1203‧‧‧純量記憶體1203‧‧‧ scalar memory
1210‧‧‧輸入影像資料1210‧‧‧ Input image data
1301‧‧‧線群組1301‧‧‧line group
1302‧‧‧線群組1302‧‧‧line group
1303‧‧‧線群組1303‧‧‧line group
1305‧‧‧執行道陣列1305‧‧‧ Execution Road Array
1306‧‧‧暈圈1306‧‧‧Halo
1401‧‧‧馬賽克/輸入影像資料結構1401‧‧‧Mosaic / input image data structure
1402‧‧‧單位胞元1402‧‧‧unit cells
1403‧‧‧新輸入結構1403‧‧‧New input structure
1501‧‧‧步驟1501‧‧‧step
1502‧‧‧步驟1502‧‧‧step
1503‧‧‧步驟1503‧‧‧ steps
1601‧‧‧程式庫1601‧‧‧Library
1700‧‧‧系統1700‧‧‧System
1701‧‧‧中央處理單元1701‧‧‧Central Processing Unit
1702‧‧‧系統記憶體1702‧‧‧System Memory
1703‧‧‧顯示器1703‧‧‧Display
1704‧‧‧區域有線點對點鏈接介面1704‧‧‧ Regional Wired Point-to-Point Link Interface
1705‧‧‧網路I/O功能1705‧‧‧Network I / O Function
1706‧‧‧無線區域網路介面1706‧‧‧Wireless LAN Interface
1707‧‧‧無線點對點鏈接介面1707‧‧‧Wireless point-to-point link interface
1708‧‧‧全球定位系統介面1708‧‧‧Global Positioning System Interface
1709_1至1709_N‧‧‧感測器1709_1 to 1709_N‧‧‧ sensors
1710‧‧‧攝影機1710‧‧‧Camera
1711‧‧‧電池1711‧‧‧ Battery
1712‧‧‧電力管理控制單元1712‧‧‧Power Management Control Unit
1713‧‧‧揚聲器及麥克風1713‧‧‧Speaker and microphone
1714‧‧‧音訊編碼器/解碼器1714‧‧‧Audio encoder / decoder
1715_1至1715_N‧‧‧通用處理核心1715_1 to 1715_N‧‧‧ General Processing Core
1716‧‧‧圖形處理單元1716‧‧‧Graphics Processing Unit
1717‧‧‧主記憶體控制器/記憶體管理功能1717‧‧‧Main memory controller / memory management function
1750‧‧‧應用處理器/多核心處理器1750‧‧‧Application Processor / Multi-Core Processor
1718‧‧‧I/O控制功能1718‧‧‧I / O control function
1719‧‧‧影像處理單元1719‧‧‧Image Processing Unit
R0‧‧‧暫存器R0‧‧‧Register
R1‧‧‧暫存器R1‧‧‧Register
R2‧‧‧暫存器R2‧‧‧Register
R3‧‧‧暫存器R3‧‧‧Register
R4‧‧‧暫存器R4‧‧‧Register
下文描述及隨附圖式用以繪示本發明之實施例。在圖式中： 圖1展示一影像處理器硬體架構之一實施例； 圖2a、圖2b、圖2c、圖2d及圖2e描繪將影像資料剖析成一線群組，將一線群組剖析成一圖表及對具有重疊模板之一圖表執行之操作； 圖3a展示一模板處理器之一實施例； 圖3b展示模板處理器之一指令字之一實施例； 圖4展示一模板處理器內之一資料運算單元之一實施例； 圖5a、圖5b、圖5c、圖5d、圖5e、圖5f、圖5g、圖5h、圖5i、圖5j及圖5k描繪使用二維位移陣列及一執行道陣列以判定具有重疊模板之一對鄰近輸出像素值之一實例； 圖6展示一整合式執行道陣列及二維位移陣列之一單位胞元之一實施例； 圖7a及圖7b係關於一第一程式碼變換； 圖8a、圖8b及圖8c係關於一第二程式碼變換； 圖9a及圖9b係關於一第三程式碼變換； 圖10a及圖10b係關於一第四程式碼變換； 圖11a及圖11b係關於一第五程式碼變換； 圖12係關於一第六程式碼變換； 圖13a及圖13b係關於一第七程式碼變換； 圖14係關於一第八程式碼變換； 圖15展示一程式碼變換方法； 圖16係關於一軟體開發環境； 圖17係關於一運算系統。The following description and accompanying drawings are used to illustrate embodiments of the present invention. In the drawings: Figure 1 shows an embodiment of an image processor hardware architecture; Figures 2a, 2b, 2c, 2d, and 2e depict parsing image data into a line group, and parsing a line group into a Diagrams and operations performed on a diagram with overlapping templates; Figure 3a shows an embodiment of a template processor; Figure 3b shows an embodiment of an instruction word of a template processor; Figure 4 shows one of a template processor An embodiment of the data operation unit; Figures 5a, 5b, 5c, 5d, 5e, 5f, 5g, 5h, 5i, 5j, and 5k depict the use of a two-dimensional displacement array and an execution path; An example of an array to determine a pair of adjacent output pixel values with overlapping templates; Figure 6 shows an embodiment of an integrated execution track array and a unit cell of a two-dimensional displacement array; Figures 7a and 7b are related to a first A code transformation; Figures 8a, 8b and 8c are about a second code transformation; Figures 9a and 9b are about a third code transformation; Figures 10a and 10b are about a fourth code transformation; Figures 11a and 11b are about a fifth code change Figure 12 is about a sixth code transformation; Figures 13a and 13b are about a seventh code transformation; Figure 14 is about an eighth code transformation; Figure 15 shows a code transformation method; and Figure 16 is about A software development environment; Figure 17 relates to a computing system.
Claims (20)
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/594,517 | 2017-05-12 | ||
US15/594,517 US10489199B2 (en) | 2017-05-12 | 2017-05-12 | Program code transformations to improve image processor runtime efficiency |
Publications (2)
Publication Number | Publication Date |
---|---|
TW201908969A true TW201908969A (en) | 2019-03-01 |
TWI690850B TWI690850B (en) | 2020-04-11 |
Family
ID=61868846
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
TW107104240A TWI690850B (en) | 2017-05-12 | 2018-02-07 | Non-transitory computer storage medium, computing system and method performed by one or more computers |
Country Status (7)
Country | Link |
---|---|
US (2) | US10489199B2 (en) |
EP (1) | EP3622474A1 (en) |
JP (1) | JP6775088B2 (en) |
KR (1) | KR102278021B1 (en) |
CN (1) | CN110192220B (en) |
TW (1) | TWI690850B (en) |
WO (1) | WO2018208341A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10915319B2 (en) * | 2017-05-15 | 2021-02-09 | Google Llc | Two dimensional masked shift instruction |
CN111382094B (en) * | 2018-12-29 | 2021-11-30 | 深圳云天励飞技术有限公司 | Data processing method and device |
KR102611722B1 (en) * | 2019-01-07 | 2023-12-07 | 삼성전자주식회사 | Image processor and image processing method |
CN113703998A (en) * | 2021-08-25 | 2021-11-26 | 深圳市慧鲤科技有限公司 | Image conversion method, image conversion device, electronic equipment and computer readable storage medium |
US11474720B1 (en) * | 2022-04-04 | 2022-10-18 | Illuscio, Inc. | Systems and methods for implementing a custom heap memory manager to optimize compute kernel performance |
Family Cites Families (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2267194B (en) | 1992-05-13 | 1995-10-04 | Sony Broadcast & Communication | Apparatus and method for processing image data |
US5523854A (en) | 1993-11-08 | 1996-06-04 | Transdata International, Inc. | Store and forward data transmission |
JP4146654B2 (en) * | 2002-02-28 | 2008-09-10 | 株式会社リコー | Image processing circuit, composite image processing circuit, and image forming apparatus |
JP2005259113A (en) * | 2004-02-12 | 2005-09-22 | Ricoh Co Ltd | Process editing apparatus, process management apparatus, process editing program, process management program, recording medium, process editing method and process management method |
TWI328201B (en) * | 2006-10-30 | 2010-08-01 | Ind Tech Res Inst | Method and system for object detection in an image plane |
US7995067B2 (en) | 2007-03-29 | 2011-08-09 | Mobileye Technologies Limited | Cyclical image buffer |
KR100793286B1 (en) * | 2007-05-02 | 2008-01-10 | 주식회사 코아로직 | Digital video codec using small size buffer memory, and method for controlling the same |
US9772852B2 (en) * | 2015-04-23 | 2017-09-26 | Google Inc. | Energy efficient processor core architecture for image processor |
US9756268B2 (en) * | 2015-04-23 | 2017-09-05 | Google Inc. | Line buffer unit for image processor |
US9965824B2 (en) | 2015-04-23 | 2018-05-08 | Google Llc | Architecture for high performance, power efficient, programmable image processing |
US9785423B2 (en) * | 2015-04-23 | 2017-10-10 | Google Inc. | Compiler for translating between a virtual image processor instruction set architecture (ISA) and target hardware having a two-dimensional shift array structure |
US9769356B2 (en) * | 2015-04-23 | 2017-09-19 | Google Inc. | Two dimensional shift array for image processor |
US10095479B2 (en) * | 2015-04-23 | 2018-10-09 | Google Llc | Virtual image processor instruction set architecture (ISA) and memory model and exemplary target hardware having a two-dimensional shift array structure |
US10291813B2 (en) * | 2015-04-23 | 2019-05-14 | Google Llc | Sheet generator for image processor |
-
2017
- 2017-05-12 US US15/594,517 patent/US10489199B2/en active Active
-
2018
- 2018-01-16 CN CN201880007442.5A patent/CN110192220B/en active Active
- 2018-01-16 KR KR1020197021659A patent/KR102278021B1/en active IP Right Grant
- 2018-01-16 WO PCT/US2018/013801 patent/WO2018208341A1/en active Application Filing
- 2018-01-16 JP JP2019539188A patent/JP6775088B2/en active Active
- 2018-01-16 EP EP18715147.7A patent/EP3622474A1/en active Pending
- 2018-02-07 TW TW107104240A patent/TWI690850B/en active
-
2019
- 2019-10-21 US US16/658,989 patent/US10996988B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
JP6775088B2 (en) | 2020-10-28 |
TWI690850B (en) | 2020-04-11 |
KR20190101409A (en) | 2019-08-30 |
WO2018208341A1 (en) | 2018-11-15 |
CN110192220A (en) | 2019-08-30 |
CN110192220B (en) | 2023-09-12 |
US10489199B2 (en) | 2019-11-26 |
US10996988B2 (en) | 2021-05-04 |
EP3622474A1 (en) | 2020-03-18 |
JP2020519976A (en) | 2020-07-02 |
US20180329745A1 (en) | 2018-11-15 |
KR102278021B1 (en) | 2021-07-15 |
US20200050488A1 (en) | 2020-02-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7202987B2 (en) | Architecture for High Performance, Power Efficient, Programmable Image Processing | |
TWI614689B (en) | Compiler techniques for mapping program code to a high performance, power efficient, programmable image processing hardware platform | |
TWI690850B (en) | Non-transitory computer storage medium, computing system and method performed by one or more computers | |
CN107133908B (en) | Compiler managed memory for image processor | |
TW201812641A (en) | Statistics operations on two dimensional image processor | |
TWI670968B (en) | Image processor i/o unit | |
TW201901612A (en) | Active CPU with configurable number and image processor supporting internal network | |
TW201901608A (en) | Configuration of application software on multi-core image processor |