US9311299B1 - Weakly supervised part-of-speech tagging with coupled token and type constraints - Google Patents
Weakly supervised part-of-speech tagging with coupled token and type constraints Download PDFInfo
- Publication number
- US9311299B1 US9311299B1 US13/955,491 US201313955491A US9311299B1 US 9311299 B1 US9311299 B1 US 9311299B1 US 201313955491 A US201313955491 A US 201313955491A US 9311299 B1 US9311299 B1 US 9311299B1
- Authority
- US
- United States
- Prior art keywords
- token
- word
- parts
- language
- level set
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G06F17/28—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/211—Syntactic parsing, e.g. based on context-free grammar [CFG] or unification grammars
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/45—Example-based machine translation; Alignment
-
- G06F17/2785—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
Definitions
- POS taggers are available for more than twenty languages and achieve accuracies of around 95% on in-domain data.
- Supervised taggers are routinely employed in many natural language processing (“NLP”) applications, such as syntactic and semantic parsing, named-entity recognition, and machine translation.
- NLP natural language processing
- the resources required to train supervised taggers are expensive to create and unlikely to exist for the majority of written languages.
- the necessity of building NLP tools for these resource-poor languages has been part of the motivation for research on unsupervised learning of POS taggers.
- Tag dictionaries noisily projected via word-aligned bitext have bridged the gap between purely unsupervised and fully supervised taggers, resulting in an average accuracy of over 83% on a benchmark of eight Indo-European languages.
- a further improvement employs a tag dictionary source, resulting in the hitherto best published result of almost 85% on the same setup.
- At least one test word in a first language may be received.
- Token level data for the at least one test word may be obtained.
- the token level data may be associated with the at least one test word in the first language.
- Type level data may be obtained for the at least one test word.
- the type level data may be associated with the at least one test word in the first language.
- a POS tagger algorithm may be trained based on the token level data and the type level data.
- a sentence may be received in the first language.
- the trained POS tagger algorithm may be applied to the sentence to determine a tag for one or more words in the sentence.
- the tagged one or more words in the sentence may be provided.
- Training the POS algorithm for the at least one test word in the first language may include allowing a first tag set for each token level data whose type level data is not in a tag dictionary.
- a second tag set may be pruned.
- the second tag set may include token level data that is present in the type level data.
- the step of pruning may refer to removing any tag from the second tag set for the at least one test word that does not appear in the tag dictionary.
- a bidirectional word alignment may be generated based on the first tag set and the second tag set.
- a tag for the at least one test word in the first language may be projected using the bidirectional word alignment. Any unprojected tag in the bidirectional word alignment may be pruned. In some configurations, any projected tag that is not present in the bidirectional word alignment may be ignored.
- a system includes a database for storing type level data and a processor connected to the database.
- the processor may be configured to receive at least one test word in a first language. It may obtain token level data for the at least one test word and associate the token level data with the at least one test word in the first language.
- the processor may be configured to obtain type level data for the at least one test word and associate the type level data with the at least one test word in the first language. It may train a part-of-speech tagger algorithm based on the token level data and the type level data.
- FIG. 1 shows a computer according to an implementation of the disclosed subject matter.
- FIG. 2 shows a network configuration according to an implementation of the disclosed subject matter.
- FIG. 3 shows an example process for training a POS tagger algorithm according to an implementation disclosed herein.
- FIG. 4 is an example lattice representation of an inference search space Y(x) for a sentence after pruning with tag dictionary type constraints according to an implementation disclosed herein.
- FIG. 5 is an example POS tagging according to an implementation disclosed herein.
- FIG. 6 shows an example of a tag dictionary and projection dictionary coverage according to an implementation disclosed herein.
- FIG. 7 shows an example of the average number of licensed tags per token on the target side of the bitext, for types in tag dictionary according to an implementation disclosed herein.
- FIG. 8 shows an example system according to an implementation disclosed herein.
- FIG. 9 is a table that shows the tagging accuracies for the models tested according to an implementation disclosed herein.
- FIG. 10 is a table that shows tagging accuracies for models with token constraints and coupled token and type constraints.
- Treebank data may refer to a text corpus in which each sentence has been parsed (i.e., annotated with syntactic structure).
- Such dictionaries may provide in-depth coverage of the test domain and also list all inflected forms of items included in the tag dictionary. Further, such resources often are difficult to obtain and, realistically, often may be unavailable for resource-poor languages.
- type-level tag dictionaries may be generated automatically by aggregating over projected token-level information extracted from bitext.
- label propagation may be used on a similarity graph to smooth (and also expand) the label distributions. While this approach produces good results and is applicable to resource-poor languages, it requires a complex multi-stage training procedure including the construction of a large distributional similarity graph. While noisy and sparse in nature, tag dictionaries are available for almost 200 languages. Furthermore, the quality and coverage of these dictionaries is growing continuously.
- Type constraints from a tag dictionary may be incorporated into a feature-based Hidden Markov Model (“HMM”) for improved results.
- HMM Hidden Markov Model
- a POS tag may refer to an identification of at least one function of a word (e.g., noun, verb, adjective, adverb, article, etc.).
- a word e.g., noun, verb, adjective, adverb, article, etc.
- token- and type-level information offer different and complementary signals.
- high confidence token-level projections offer precise constraints on a tag in a particular context.
- manually created type-level dictionaries can have broad coverage and do not suffer from word-alignment errors; they can therefore be used to filter systematic as well as random noise in token-level projections.
- conditional random field (“CRF”) model couples token and type constraints in order to guide learning.
- the model is given the freedom to push probability mass towards hypotheses consistent with both types of information.
- noisy projected or manually constructed dictionaries may be used according to the implementations disclosed herein to generate type constraints.
- adding features from a monolingual word clustering can significantly improve accuracy. While most of these features can also be used in a generative feature-based hidden Markov model (“HMM”), accuracy may be improved with a globally normalized discriminative CRF model as disclosed herein.
- HMM generative feature-based hidden Markov model
- FIG. 1 is an example computer 20 suitable for implementations of the presently disclosed subject matter.
- the computer 20 includes a bus 21 which interconnects major components of the computer 20 , such as a central processor 24 , a memory 27 (typically RAM, but which may also include ROM, flash RAM, or the like), an input/output controller 28 , a user display 22 , such as a display screen via a display adapter, a user input interface 26 , which may include one or more controllers and associated user input devices such as a keyboard, mouse, and the like, and may be closely coupled to the I/O controller 28 , fixed storage 23 , such as a hard drive, flash storage, Fibre Channel network, SAN device, SCSI device, and the like, and a removable media component 25 operative to control and receive an optical disk, flash drive, and the like.
- a bus 21 which interconnects major components of the computer 20 , such as a central processor 24 , a memory 27 (typically RAM, but which may also include ROM, flash RAM,
- the bus 21 allows data communication between the central processor 24 and the memory 27 , which may include read-only memory (ROM) or flash memory (neither shown), and random access memory (RAM) (not shown), as previously noted.
- the RAM is generally the main memory into which the operating system and application programs are loaded.
- the ROM or flash memory can contain, among other code, the Basic Input-Output system (BIOS) which controls basic hardware operation such as the interaction with peripheral components.
- BIOS Basic Input-Output system
- Applications resident with the computer 20 are generally stored on and accessed via a computer readable medium, such as a hard disk drive (e.g., fixed storage 23 ), an optical drive, floppy disk, or other storage medium 25 .
- a network interface 29 may provide a direct connection to a remote server via a telephone link, to the Internet via an internet service provider (ISP), or a direct connection to a remote server via a direct network link to the Internet via a POP (point of presence) or other technique.
- the network interface 29 may provide such connection using wireless techniques, including digital cellular telephone connection, Cellular Digital Packet Data (CDPD) connection, digital satellite data connection or the like.
- CDPD Cellular Digital Packet Data
- the network interface 29 may allow the computer to communicate with other computers via one or more local, wide-area, or other networks, as shown in FIG. 2 .
- FIG. 1 Many other devices or components (not shown) may be connected in a similar manner (e.g., document scanners, digital cameras and so on). Conversely, all of the components shown in FIG. 1 need not be present to practice the present disclosure. The components can be interconnected in different ways from that shown. The operation of a computer such as that shown in FIG. 1 is readily known in the art and is not discussed in detail in this application. Code to implement the present disclosure can be stored in computer-readable storage media such as one or more of the memory 27 , fixed storage 23 , removable media 25 , or on a remote storage location.
- FIG. 2 shows an example network arrangement according to an implementation of the disclosed subject matter.
- One or more clients 10 , 11 such as local computers, smart phones, tablet computing devices, and the like may connect to other devices via one or more networks 7 .
- the network may be a local network, wide-area network, the Internet, or any other suitable communication network or networks, and may be implemented on any suitable platform including wired and/or wireless networks.
- the clients may communicate with one or more servers 13 and/or databases 15 .
- the devices may be directly accessible by the clients 10 , 11 , or one or more other devices may provide intermediary access such as where a server 13 provides access to resources stored in a database 15 .
- the clients 10 , 11 also may access remote platforms 17 or services provided by remote platforms 17 such as cloud computing arrangements and services.
- the remote platform 17 may include one or more servers 13 and/or databases 15 .
- implementations of the presently disclosed subject matter may include or be implemented in the form of computer-implemented processes and apparatuses for practicing those processes. Implementations also may be implemented in the form of a computer program product having computer program code containing instructions implemented in non-transitory and/or tangible media, such as floppy diskettes, CD-ROMs, hard drives, USB (universal serial bus) drives, or any other machine readable storage medium, wherein, when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing implementations of the disclosed subject matter.
- Implementations also may be implemented in the form of computer program code, for example, whether stored in a storage medium, loaded into and/or executed by a computer, or transmitted over some transmission medium, such as over electrical wiring or cabling, through fiber optics, or via electromagnetic radiation, wherein when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing implementations of the disclosed subject matter.
- the computer program code segments configure the microprocessor to create specific logic circuits.
- a set of computer-readable instructions stored on a computer-readable storage medium may be implemented by a general-purpose processor, which may transform the general-purpose processor or a device containing the general-purpose processor into a special-purpose device configured to implement or carry out the instructions.
- Implementations may be implemented using hardware that may include a processor, such as a general purpose microprocessor and/or an Application Specific Integrated Circuit (ASIC) that implements all or part of the techniques according to implementations of the disclosed subject matter in hardware and/or firmware.
- the processor may be coupled to memory, such as RAM, ROM, flash memory, a hard disk or any other device capable of storing electronic information.
- the memory may store instructions adapted to be executed by the processor to perform the techniques according to implementations of the disclosed subject matter.
- bitext may refer to a word alignment between two languages (e.g., between a resource-poor language and a resource-rich language).
- An example of a resource-rich language is English. In the examples described below, it is utilized as the source language.
- Other examples of factors that make a language resource-rich may include the presence of one or more of the following: an annotated corpus, a dictionary, and a translation text (e.g., where the resource rich language is translated/aligned with another language). Examples of resource rich languages may include Hindi, French, German, and Russian.
- resource poor languages may include Assamese, Tamili, Maithili, Oriya, Bengali, Punjabi, and Santali. Some languages may be difficult to characterize as resource-rich or -poor. In such instances, they may be deemed as a resource-poor language.
- a supervised POS tagger may be used to predict POS tags for the English side of a bitext. These predicted tags can be projected subsequently to the target side via automatic word alignments. However, due to the automatic nature of the word alignments and the POS tags, there may be significant noise in the projected tags.
- CRF and HMM based models may be utilized to perform a POS tagging. Smoothing techniques may be employed when training a HMM to mitigate the noise. Noise may be mitigated by combining projections from multiple source languages to filter out random projection noise as well as the systematic noise arising from different source language annotations and syntactic divergences.
- At least one test word in a first language may be received at 310 .
- the first language may be a resource poor language as described above.
- the at least one test word may be received as one or more sentences or phrases in the first language. Phrases and/or sentences may enable collection of token level data.
- Token level data (e.g., a token constraint) for the at least one test word may be obtained at 320 .
- Token level data may refer to a tag associated with a word as it may be used in a particular context. For example, in the sentence, “Kate read the book,” the word “book” is used as a noun.
- the token level data may completely disambiguate a particular token (e.g., word) as it is used in a sentence. However, it may not indicate other possible uses of the word. Thus it may be beneficial to combine token level data with type level data, as discussed below.
- the token level data may refer to a linkage between a second language (e.g., English) that has been tagged for POS and the first language.
- the word “book” may be tagged in a resource rich language such as English as a noun (e.g., based on a translation of the sentence from the first language to the English language).
- a linkage may be represented in the form of a lattice as shown in FIG. 4 and discussed below.
- the token level data may be associated with the at least one test word in the first language at 330 .
- a table may be generated and stored that associates a particular word in the first language with a particular POS tag based on the token level data.
- Type level data (e.g., a type constraint) for the at least one test word may be obtained at 340 .
- Type level data may refer to data about a particular word as obtained from a tag dictionary.
- a tag dictionary may refer to any collection of words in which at least one word has one or more POS tags associated with it. For example, many online or print dictionaries identify a POS tag for the word as a component of a definition (e.g., an annotated dictionary entry).
- the type level data may be associated with the at least one test word in the first language at 350 .
- a database table may be generated that contains the word in the first language and the one or more type level data obtained from the tag dictionary.
- a POS tagger algorithm may be trained based on the token level data and the type level data at 360 . Training the POS tagger algorithm is discussed in further detail below.
- a training set may include, for example, one or more training words for which a POS tag is known, and/or for which token and/or type level data are known or available.
- a partially constrained lattice of tag sequences may be constructed as follows:
- FIG. 4 provides an example of the logic disclosed above as applied to a Swedish sentence 410 . It shows a lattice representation 420 of an inference search space Y(x) for a sentence in Swedish (which translates in English to, “The farming products must be pure and must not contain any additives.”) after pruning with tag dictionary type constraints according to an implementation disclosed herein. The correct parts of speech are listed underneath each word. Bold nodes show projected token constraints ⁇ tilde over (y) ⁇ . Underlined text indicates incorrect tags. The coupled constraints lattice ⁇ (x, ⁇ tilde over (y) ⁇ ) consists of the bold nodes together with nodes for words that are lacking token constraints; in this case, the coupled constraints lattice thus defines exactly one valid path.
- the lattice 420 shows tags permitted after constraining the words to tags that appear in the dictionary (up until Step 2 from above). There is only a single token “Jordbruks isterna” (which translates in English to, “the farming products”) not in the dictionary; in this case the lattice 420 permits the full set of tags. With token-level projections (e.g., Step 3, nodes with bold border in FIG. 4 ), the lattice can be further pruned. In most cases, the projected tag is both correct and is in the dictionary-pruned lattice. Thus, such tokens may be disambiguated successfully and substantially shrink the search space.
- the model may not adequately tag a POS.
- the erroneously projected tag “ADJ” may eliminate all other tags from the lattice, including the correct tag “NOUN.” That is, in the example shown in FIG. 4 , “ADJ” was predicted from the token level data. Based on step 1 of the logic described above, the projected token tag would be accepted if there was not type level data.
- the token “n ⁇ gra” (which translates in English to “any”) has a single dictionary entry “PRON” and is missing the correct tag “DET.” In the case where “DET” is the projected tag, it may not be added to the lattice 420 and may be ignored.
- the tag dictionary has an entry for this word (See step 2 of the logic disclosed earlier). In such cases, the tag dictionary can be trusted more than the tags projected via noisy word alignments. Taking the union of tags may cause the POS tagger algorithm to perform worse.
- FIG. 5 provides an example POS tagging according to an implementation disclosed herein.
- the token- and type-data are hypothetical and do not necessarily reflect a tags that would be obtained from a tag dictionary or from a projection.
- the example is merely illustrative of an implementation of the process to obtain POS tags.
- the original language for which tags are sought is German 510 .
- An English translation of the sentence is provided at 530 as is token level data 540 and type level data 550 .
- the type level data 550 may be obtained from an English tag dictionary.
- the alignment between the German sentence and the English sentence is shown by lines connecting the German word to its English translation. Following the steps disclosed above, the tag set for the word “the” is allowed because its type is not in the tag dictionary.
- any tag that does not appear in the dictionary may be pruned.
- the token level data 540 indicates that “book” may be a NOUN or ADJ.
- ADJ does not appear in the type level data 550 and may be pruned.
- any projected tag that is still present after the pruning process may be accepted (i.e., has not been pruned from a prior step).
- the tags predicted 520 based on the model for the sample sentence 510 may be determined.
- the most likely label sequence may be determined utilizing a Viterbi decoding.
- two lattices may need to be defined: one that the model moves probability mass towards and another one defining the overall search space (or partition function).
- the former is a trivial lattice containing the “gold standard” (i.e., best-known) tag sequence and the latter is the set of all possible tag sequences spanning the tokens.
- mass may be moved towards the coupled token- and type-constrained lattice, such that the model can freely distribute mass across all paths consistent with these constraints.
- the lattice defining the partition function will be the full set of possible tag sequences when no dictionary is used; when a dictionary is used it will consist of all dictionary pruned tag sequences (sans Step 3 above; the full set of possibilities shown in FIG. 4 for the running example).
- FIGS. 6 and 7 provide examples of statistics regarding the supervision coverage and remaining ambiguity.
- FIG. 6 shows an example of a tag dictionary and projection dictionary coverage. It shows the percentage of tokens in the target side of the bitext that are covered by the tag dictionary (black colored bars), that have a projected tag (cross-hatched bars), and that have a projected tag after intersecting the two (white bars).
- FIG. 6 shows that more than two thirds of all tokens in the training data may be in the tag dictionary.
- Spanish has the highest coverage with over 90%
- Turkish an agglutinative language with a vast number of word forms, has less than 50% coverage.
- FIG. 7 shows that there is substantial uncertainty left after pruning with the tag dictionary, since tokens are rarely fully disambiguated: 1.3 tags per token are allowed on average for types in the tag dictionary in the provided example.
- FIG. 6 further shows that high-confidence alignments may be available for about half of the tokens for most languages (Japanese is a notable exception with less than 30% of the tokens covered in this example). Intersecting the dictionary tags and the projected tags (Steps 2 and 3 above) may filter out some of the potentially erroneous tags, but may preserve the majority of the projected tags. The remaining, presumably more accurate projected tags cover almost half of all tokens, greatly reducing the search space that the learner needs to explore.
- Token and type constraints may be coupled and utilized to train a probabilistic training model.
- (x x 1 x 2 . . . x
- the lattice of all admissible tag sequences for the sentence x may be denoted by Y(x). This is the inference search space in which the tagger operates.
- a tag dictionary maps a word type x j ⁇ V to a set of admissible tags T(x j ) T. For word types not in the dictionary the full set of tags T may be allowed.
- ⁇ may be defined that couples ⁇ tilde over (y) ⁇ and Y(x) with respect to every sentence index, which results in a token- and type-constrained lattice. The operator behaves as follows,
- ⁇ (x, ⁇ tilde over (y) ⁇ ) ⁇ circumflex over (T) ⁇ (x 1 , ) ⁇ circumflex over (T) ⁇ (x 2 , ) ⁇ . . .
- a first-order HMM specifies the joint distribution of a sentence x ⁇ X and a tag-sequence y ⁇ Y(x) as:
- a log-linear parameterization of the emission and the transition distributions may be utilized instead of a multinomial parameterization. This may allow model parameters to be shared across categorical events.
- the categorical emission and transition events are represented by feature vectors ⁇ (x i , y i ) and ⁇ (x i , y i ⁇ 1 ). Each element of the parameter vector ⁇ corresponds to a particular feature; the component log-linear distributions are:
- , and thus p ⁇ (x, ⁇ (x, ⁇ tilde over (y) ⁇ )) p ⁇ (x), which reduces to fully unsupervised learning.
- the Expectation-Maximization algorithm may be employed to optimize this objective. Note that since the marginal likelihood is non-concave, a local maximum of only Equation 6 may be determined.
- the tag sequence y* ⁇ Y(x) for a sentence x ⁇ X may be predicted by choosing the one with maximal joint probability: y * ⁇ arg max p ⁇ ( x,y ) y ⁇ Y ( x ) Equation 7
- a CRF may model the probability of the output conditioned on the input as a globally normalized log-linear distribution:
- ⁇ is a parameter vector.
- Y(x) is not necessarily the full space of possible tag-sequences. It may be the dictionary-pruned lattice without the token constraints.
- a marginal conditional probability may be modeled, given by the total probability of all tag sequences consistent with the lattice ⁇ (x, ⁇ tilde over (y) ⁇ ): p ⁇ ( ⁇ circumflex over ( Y ) ⁇ ( x, ⁇ tilde over (y) ⁇ )
- x ) ⁇ y ⁇ (x, ⁇ tilde over (y) ⁇ ) p ⁇ ( y
- the tag-sequence y* ⁇ Y(x) for a sentence x ⁇ X is chosen as the sequence with the maximal conditional probability: y * ⁇ arg max p ⁇ ( y
- the POS algorithm may be applied to one or more words from the first language.
- a sentence in the first language may be received.
- the trained POS tagger algorithm may be applied to the sentence to determine a tag for the one or more words that make up the sentence.
- the POS tagger algorithm may not be able to tag a particular word in the sentence.
- the tagged one or more words may be provided, for example, via a database or by any other user interface.
- a system is disclosed.
- An example of the system is provided in FIG. 8 that includes a database 810 that is connected to a processor 840 .
- the database may be used for storage of token level data 820 and/or type level data 830 .
- the processor 840 may be configured to receive at least one test word in a first language from the database 810 , for example.
- Token level data 820 may obtained and associated with the at least one word in the first language as described above.
- Type level data 830 may be obtained for the at least one test word and associated with the at least one test word in the first language.
- the associations may be stored, for example, as one or more database tables.
- the processor 840 may be configured to train a part-of-speech tagger algorithm based on the token level data 820 and the type level data 830 as disclosed above.
- the processor 840 may be used to apply the trained POS algorithm to a sentence that was not a component of the training set of sentences.
- the table shown in FIG. 9 compares the performance of the disclosed models with the best results of a first prior work (“PW 1 ”) and a second prior work (“PW 2 ”).
- the “With LP” model is from PW 1 in the table shown in FIG. 10 while the “SHMM-ME” model is from PW 2 in FIG. 10 .
- y proj. HMM , y wik. HMM , and y union HMM are HMMs trained solely with type constraints derived from the projected dictionary, a tag dictionary, and the union of the projected dictionary and the tag dictionary.
- y union HMM +C is equivalent to y union HMM with additional cluster features. All models are trained on the Treebank of each language, stripped of “gold labels.” Results are averaged over the 8 languages described earlier, denoted as “avg(8), as well as over the full set of 15 languages, denoted as “avg.”
- y union HMM +C Monolingual cluster features were added to the model with the union dictionary.
- This model referred to as y union HMM +C, significantly outperforms all other type-constrained models, which may indicate the importance of word-cluster features.
- the same model was trained on datasets containing 500K tokens sampled from the target side of the parallel data (the model is y union HMM C+L. Training on large datasets resulted in an average accuracy of 87.2% which is comparable to the 87.3% reported for y union HMM +C in FIG. 9 . This may indicate that the different source domain and amount of training data does not influence the performance of the HMM significantly.
- CRF models were trained where type constraints were treated as a partially observed lattice and that used the full un-pruned lattice for computing the partition function (see Equations 8-12 and corresponding description). Similar trends were observed in these results as those for the HMM models. But, on average, accuracies were much lower compared to the type-constrained HMM models; the CRF model with the union dictionary along with cluster features achieved an average accuracy of 79.3% when trained on the same data. This may be explained by the CRF model's search space being fully unconstrained and the dictionary providing a weak set of observation constraints, which do not provide sufficient information to successfully train a discriminative model. However, as described below, coupling the dictionary constraints with token-level information solves this problem.
- Token-level information was added to the above models, focusing in particular on coupled token and type constraints. Since it may not be possible to generate projected token constraints for the monolingual treebanks utilized, all models below were trained on the 500K-tokens datasets sampled from the bitext. As a baseline, the HMM and CRF models were trained using only projected token constraints, referred to as ⁇ tilde over (y) ⁇ ⁇ HMM +C+L and ⁇ tilde over (y) ⁇ ⁇ CRF +C+L, respectively.
- the table shown in FIG. 10 indicates these models underperform the type-level model y union HMM +C+L, which indicates that projected token constraints may not be reliable on their own.
- FIG. 10 shows a table for tagging accuracies for models with token constraints and coupled token and type constraints. All models in FIG. 10 use cluster features ( . . . +C) and are trained on large training sets each containing 500 k tokens with (partial) token-level projections ( . . . +L). The best type-constrained model, trained on the larger datasets, y union HMM +C+L, is included for comparison. The remaining columns correspond to HMM and CRF models trained only with token constraints ( ⁇ tilde over (y) ⁇ . . . ) and with coupled token and type constraints ( ⁇ tilde over (y) ⁇ . . . ).
- the latter are trained using the projected dictionary (proj.), a tag dictionary (wik.), and the union of these dictionaries (union), respectively.
- the search spaces of the models trained with coupled constraints ( ⁇ tilde over (y) ⁇ . . . ) are each pruned with the respective tag dictionary used to derive the coupled constraints.
- CRF +C+L and y union HMM +C+L is statistically significant at p ⁇ 0.01 (**) and p ⁇ 0.015 (*) according to a paired bootstrap test. Significance was not assessed for avg or avg (8).
- Models with coupled token and type constraints were examined as well (results shown in FIG. 10 ). These models use the same dictionaries as used earlier, but they also couple the derived type constraints with projected token constraints. Note that since only projected tags that are licensed by the dictionary were allowed (Step 3 from above), the actual token constraints used in these models vary with the different dictionaries.
- FIG. 10 shows that coupled constraints may be superior to token constraints, when used both with the HMM and the CRF.
- coupled constraints may not provide any benefit over type constraints alone, in particular when the projected dictionary or the union dictionary is used to derive the coupled constraints, ⁇ tilde over (y) ⁇ proj.
- the CRF model is able to take advantage of the complementary information in the coupled constraints, provided that the dictionary is able to filter out the systematic token-level errors.
- the dictionary With the tag dictionary that was used and projected token-level constraints, ⁇ tilde over (y) ⁇ wik.
- CRF +C+L performs better than all the remaining models, with an average accuracy of 88.8% across the eight Indo-European languages available to PW 1 and PW 2 . Averaged over all 15 languages, its accuracy is 84.5%.
Abstract
Description
-
- 1. For each token having a type not in the tag dictionary, a complete tag set may be allowed for potential matches to the training word (i.e., a first tag set is allowed for each token level data whose type level data is not in the tag dictionary).
- 2. For each token whose type is in the tag dictionary, all tags that do not appear in the dictionary may be pruned and the token may be marked as dictionary-pruned. For example, a second tag set may be pruned; the second tag set may be token level data that is present in the type level data (e.g., where token and type level data exists for the at least one word). The pruning function may refer to the removal of any tag from the second tag set for the at least one test word that does not appear in the tag dictionary. For example, a word may have multiple tags associated with it based on the token level data. Some of the tags associated with the word may not appear in the tag dictionary. Those tags may be removed or pruned.
- 3. A bidirectional word alignment based on the first tag set and the second tag set may be generated. A tag may be projected for the at least one test word in the first language utilizing the bidirectional word alignment. For each token that has a tag projected via a high-confidence bidirectional word alignment: if the projected tag is still present in the lattice, then every tag may be pruned but the projected tag for that token; if the projected tag is not present in the lattice, which can only happen for dictionary-pruned tokens, then the projected tag may be ignored.
The token- and type-constrained lattice may be denoted as Ŷ(x, {tilde over (y)})={circumflex over (T)}(x1,
A log-linear parameterization of the emission and the transition distributions may be utilized instead of a multinomial parameterization. This may allow model parameters to be shared across categorical events. The categorical emission and transition events are represented by feature vectors φ(xi, yi) and φ(xi, yi−1). Each element of the parameter vector β corresponds to a particular feature; the component log-linear distributions are:
and
p β(x,Ŷ(x,{tilde over (y)}))=ΣyεŶ(x,{tilde over (y)}) p β(x,y) Equation 5
If there are no projections and no tag dictionary, then Ŷ(x, {tilde over (y)})=T|x|, and thus pβ(x, Ŷ(x, {tilde over (y)}))=pβ(x), which reduces to fully unsupervised learning. The l2-regularized marginal joint log-likelihood of the constrained training data D={(x(i),{tilde over (y)}(i))}i=1 n is:
L(β;D)=Σi=1 n log p β(x (i) ,Ŷ(x (i) ,ŷ (i))−γ∥β∥2 2 Equation 6
A direct gradient approach may be applied to optimize Equation 6 with L-BFGS. γ=1 may be set and 100 iterations of L-BFGS may be run. In some configurations, the Expectation-Maximization algorithm may be employed to optimize this objective. Note that since the marginal likelihood is non-concave, a local maximum of only Equation 6 may be determined.
y*←arg max p β(x,y)
yεY(x)
where θ is a parameter vector. As for the HMM, Y(x) is not necessarily the full space of possible tag-sequences. It may be the dictionary-pruned lattice without the token constraints.
Φ(x,y)=Σi=1 |x|φ(x,y i ,y i−1) Equation 9
p θ({circumflex over (Y)}(x,{tilde over (y)})|x)=ΣyεŶ(x,{tilde over (y)}) p θ(y|x)
The parameters of this constrained CRF are estimated by maximizing the l2-regularized marginal conditional log-likelihood of the constrained data:
L(θ;D)=Σi=1 n log p θ({circumflex over (Y)}(x (i) ,{tilde over (y)} (i))|x (i))−γ∥θ∥2 2
y*←arg max p θ(y|x)
yεY(x) Equation 12
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/955,491 US9311299B1 (en) | 2013-07-31 | 2013-07-31 | Weakly supervised part-of-speech tagging with coupled token and type constraints |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/955,491 US9311299B1 (en) | 2013-07-31 | 2013-07-31 | Weakly supervised part-of-speech tagging with coupled token and type constraints |
Publications (1)
Publication Number | Publication Date |
---|---|
US9311299B1 true US9311299B1 (en) | 2016-04-12 |
Family
ID=55643206
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/955,491 Active 2034-06-14 US9311299B1 (en) | 2013-07-31 | 2013-07-31 | Weakly supervised part-of-speech tagging with coupled token and type constraints |
Country Status (1)
Country | Link |
---|---|
US (1) | US9311299B1 (en) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109408828A (en) * | 2018-11-08 | 2019-03-01 | 四川长虹电器股份有限公司 | Words partition system for television field semantic analysis |
US10380263B2 (en) * | 2016-11-15 | 2019-08-13 | International Business Machines Corporation | Translation synthesizer for analysis, amplification and remediation of linguistic data across a translation supply chain |
US10404703B1 (en) * | 2016-12-02 | 2019-09-03 | Worldpay, Llc | Systems and methods for third-party interoperability in secure network transactions using tokenized data |
US10402808B1 (en) * | 2016-12-02 | 2019-09-03 | Worldpay, Llc | Systems and methods for linking high-value tokens using a low-value token |
CN110956018A (en) * | 2019-11-22 | 2020-04-03 | 腾讯科技（深圳）有限公司 | Training method of text processing model, text processing method, text processing device and storage medium |
CN111950278A (en) * | 2019-05-14 | 2020-11-17 | 株式会社理光 | Sequence labeling method and device and computer readable storage medium |
US20210081597A1 (en) * | 2018-04-03 | 2021-03-18 | Nippon Telegraph And Telephone Corporation | Tag assignment model generation apparatus, tag assignment apparatus, methods and programs therefor |
US11397862B2 (en) * | 2020-07-23 | 2022-07-26 | International Business Machines Corporation | Configuring metrics and recall levels for natural language processing annotator |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5146405A (en) * | 1988-02-05 | 1992-09-08 | At&T Bell Laboratories | Methods for part-of-speech determination and usage |
US20090089058A1 (en) * | 2007-10-02 | 2009-04-02 | Jerome Bellegarda | Part-of-speech tagging using latent analogy |
US8560477B1 (en) * | 2010-10-08 | 2013-10-15 | Google Inc. | Graph-based semi-supervised learning of structured tagging models |
-
2013
- 2013-07-31 US US13/955,491 patent/US9311299B1/en active Active
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5146405A (en) * | 1988-02-05 | 1992-09-08 | At&T Bell Laboratories | Methods for part-of-speech determination and usage |
US20090089058A1 (en) * | 2007-10-02 | 2009-04-02 | Jerome Bellegarda | Part-of-speech tagging using latent analogy |
US8560477B1 (en) * | 2010-10-08 | 2013-10-15 | Google Inc. | Graph-based semi-supervised learning of structured tagging models |
Non-Patent Citations (6)
Title |
---|
Das et al.,"Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections", In Proceedings of ACL-HLT, 2011, 10 pages. |
Garrette et al.,"Learning a Part-of-Speech Tagger from Two Hours of Annotation", The University of Texas at Austin, 2013, Retrieved on Apr. 10, 2013 from http://www.cs.utexas.edu/users/dhg/papers/garrette-baldridge-naacl2013.pdf, Apr. 10, 2013, 10 pages. |
Lee et al.,"Simple Type-Level Unsupervised POS Tagging", EMNLP '10 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing; pp. 853-861, Oct. 11, 2010. |
Li et al.,"Wiki-ly Supervised Part-of-Speech Tagging", In Proceedings of EMNLP-CoNLL, 2012, 10 pages. |
Tackstrom et al.,"Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging", Google Research, 2013, Retrieved on Apr. 10, 2013 from http://static.googleusercontent.com/external-content/untrusted-dlcp/research.google.com/en/us/pubs/archive/40758.pdf, Apr. 10, 2013, 12 pages. |
Yarowsky et al.,"Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection across Aligned Corpora", In Proceedings of NAACL, 2001, 8 pages. |
Cited By (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190325030A1 (en) * | 2016-11-15 | 2019-10-24 | International Business Machines Corporation | Translation synthesizer for analysis, amplification and remediation of linguistic data across a translation supply chain |
US10380263B2 (en) * | 2016-11-15 | 2019-08-13 | International Business Machines Corporation | Translation synthesizer for analysis, amplification and remediation of linguistic data across a translation supply chain |
US11256879B2 (en) * | 2016-11-15 | 2022-02-22 | International Business Machines Corporation | Translation synthesizer for analysis, amplification and remediation of linguistic data across a translation supply chain |
US11100486B2 (en) | 2016-12-02 | 2021-08-24 | Worldpay, Llc | Systems and methods for linking high-value tokens using a low-value token |
US10404703B1 (en) * | 2016-12-02 | 2019-09-03 | Worldpay, Llc | Systems and methods for third-party interoperability in secure network transactions using tokenized data |
US11836696B2 (en) | 2016-12-02 | 2023-12-05 | Worldpay, Llc | Systems and methods for linking high-value tokens using a low-value token |
US11777937B2 (en) | 2016-12-02 | 2023-10-03 | Worldpay, Llc | Systems and methods for third-party interoperability in secure network transactions using tokenized data |
US10402808B1 (en) * | 2016-12-02 | 2019-09-03 | Worldpay, Llc | Systems and methods for linking high-value tokens using a low-value token |
US11361297B2 (en) | 2016-12-02 | 2022-06-14 | Wordpay, LLC | Systems and methods for linking high-value tokens using a low-value token |
US11108778B2 (en) | 2016-12-02 | 2021-08-31 | Worldpay, Llc | Systems and methods for third-party interoperability in secure network transactions using tokenized data |
US20210081597A1 (en) * | 2018-04-03 | 2021-03-18 | Nippon Telegraph And Telephone Corporation | Tag assignment model generation apparatus, tag assignment apparatus, methods and programs therefor |
US11531806B2 (en) * | 2018-04-03 | 2022-12-20 | Nippon Telegraph And Telephone Corporation | Tag assignment model generation apparatus, tag assignment apparatus, methods and programs therefor using probability of a plurality of consecutive tags in predetermined order |
CN109408828A (en) * | 2018-11-08 | 2019-03-01 | 四川长虹电器股份有限公司 | Words partition system for television field semantic analysis |
CN111950278A (en) * | 2019-05-14 | 2020-11-17 | 株式会社理光 | Sequence labeling method and device and computer readable storage medium |
CN110956018B (en) * | 2019-11-22 | 2023-04-18 | 腾讯科技（深圳）有限公司 | Training method of text processing model, text processing method, text processing device and storage medium |
CN110956018A (en) * | 2019-11-22 | 2020-04-03 | 腾讯科技（深圳）有限公司 | Training method of text processing model, text processing method, text processing device and storage medium |
US11397862B2 (en) * | 2020-07-23 | 2022-07-26 | International Business Machines Corporation | Configuring metrics and recall levels for natural language processing annotator |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9311299B1 (en) | Weakly supervised part-of-speech tagging with coupled token and type constraints | |
JP4694121B2 (en) | Statistical method and apparatus for learning translation relationships between phrases | |
Tiedemann | Recycling translations: Extraction of lexical data from parallel corpora and their application in natural language processing | |
US9798720B2 (en) | Hybrid machine translation | |
Wang et al. | Joint word alignment and bilingual named entity recognition using dual decomposition | |
US20130103390A1 (en) | Method and apparatus for paraphrase acquisition | |
US8874433B2 (en) | Syntax-based augmentation of statistical machine translation phrase tables | |
US20030023422A1 (en) | Scaleable machine translation system | |
US20090326912A1 (en) | Means and a method for training a statistical machine translation system | |
US9400787B2 (en) | Language segmentation of multilingual texts | |
Unnikrishnan et al. | A novel approach for English to South Dravidian language statistical machine translation system | |
Maučec et al. | Slavic languages in phrase-based statistical machine translation: a survey | |
Mansouri et al. | State-of-the-art english to persian statistical machine translation system | |
Liu et al. | Language model augmented relevance score | |
Slayden et al. | Thai sentence-breaking for large-scale SMT | |
Venkatapathy et al. | A discriminative approach for dependency based statistical machine translation | |
Horvat | Hierarchical statistical semantic translation and realization | |
Khenglawt | Machine translation and its approaches | |
Srivastava et al. | A hybrid approach for word alignment in english-hindi parallel corpora with scarce resources | |
US11657229B2 (en) | Using a joint distributional semantic system to correct redundant semantic verb frames | |
John | Improving Statistical Machine Translation with Target-Side Dependency Syntax | |
Dinh | Building an annotated English-Vietnamese parallel corpus | |
Delmonte | Getting Past the Language Gap: Innovations in Machine Translation | |
Lee et al. | IBM Chinese-to-English PatentMT System for NTCIR-9. | |
Khalilov | New statistical and syntactic models for machine translation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PETROV, SLAV;DAS, DIPANJAN;MCDONALD, RYAN;AND OTHERS;REEL/FRAME:031075/0179Effective date: 20130821 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |