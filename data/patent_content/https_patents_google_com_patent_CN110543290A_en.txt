CN110543290A - Multimodal response - Google Patents
Multimodal response Download PDFInfo
- Publication number
- CN110543290A CN110543290A CN201910826487.1A CN201910826487A CN110543290A CN 110543290 A CN110543290 A CN 110543290A CN 201910826487 A CN201910826487 A CN 201910826487A CN 110543290 A CN110543290 A CN 110543290A
- Authority
- CN
- China
- Prior art keywords
- client device
- multimodal
- modality
- output
- user interface
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000004044 response Effects 0.000 title claims abstract description 153
- 230000003993 interaction Effects 0.000 claims abstract description 107
- 238000000034 method Methods 0.000 claims abstract description 70
- 230000000007 visual effect Effects 0.000 claims abstract description 70
- 230000001419 dependent effect Effects 0.000 claims abstract description 35
- 230000009471 action Effects 0.000 claims description 38
- 238000009877 rendering Methods 0.000 claims description 18
- 238000001228 spectrum Methods 0.000 abstract description 8
- 230000002452 interceptive effect Effects 0.000 description 14
- 230000008569 process Effects 0.000 description 12
- 230000001413 cellular effect Effects 0.000 description 9
- 239000003795 chemical substances by application Substances 0.000 description 8
- 238000010586 diagram Methods 0.000 description 8
- 230000015654 memory Effects 0.000 description 8
- 230000008859 change Effects 0.000 description 7
- 239000000463 material Substances 0.000 description 5
- 238000012545 processing Methods 0.000 description 5
- 238000004891 communication Methods 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000003442 weekly effect Effects 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000007717 exclusion Effects 0.000 description 1
- 235000013305 food Nutrition 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000005055 memory storage Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000010399 physical interaction Effects 0.000 description 1
- 238000004611 spectroscopical analysis Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1694—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being a single or a set of motion sensors for pointer control or gesture input obtained by sensing movements of the portable computer
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/162—Interface to dedicated audio devices, e.g. audio drivers, interface to CODECs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2203/00—Indexing scheme relating to G06F3/00 - G06F3/048
- G06F2203/038—Indexing scheme relating to G06F3/038
- G06F2203/0381—Multimodal input, i.e. interface arrangements enabling the user to issue commands by simultaneous use of input devices of different nature, e.g. voice plus gesture on digitizer
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/225—Feedback of the input speech
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2203/00—Aspects of automatic or semi-automatic exchanges
- H04M2203/25—Aspects of automatic or semi-automatic exchanges related to user interface aspects of the telephonic communication service
- H04M2203/251—Aspects of automatic or semi-automatic exchanges related to user interface aspects of the telephonic communication service where a voice mode or a visual mode can be used interchangeably
- H04M2203/253—Aspects of automatic or semi-automatic exchanges related to user interface aspects of the telephonic communication service where a voice mode or a visual mode can be used interchangeably where a visual mode is used instead of a voice mode
Abstract
Systems, methods, and apparatuses for using multimodal responses in dynamically generating client device output tailored to a current modality of a client device are disclosed herein. The multimodal client device can engage in various interactions across a multimodal spectrum including voice-only interactions, voice-dominant interactions, multimodal interactions, visual-dominant interactions, visual-only interactions, and the like. The multi-modal response may include: a core message to be rendered for all interaction types and one or more modality-dependent components for providing additional information to the user.
Description
Technical Field
The present disclosure relates to multimodal responses, and more particularly to systems, methods, and apparatuses for using multimodal responses in dynamically generating client device output tailored to a current modality of a client device.
Background
Automated assistants (also referred to as "personal assistants," "mobile assistants," and the like) can be implemented on the multimodal client device and/or on one or more remote computing devices, such as computing devices in a "cloud" connected to the multimodal client device via a network. The multimodal client device can provide a user with multiple modes of interaction with the device. More specifically, the multimodal device has several modes that can be used for a user to input data to the device and/or several modes that can be used for the device to render output to the user.
For example, user inputs to the multimodal device can include spoken inputs (e.g., a user speaking), text inputs (e.g., a user typing on a keyboard, a user entering text on a touch-sensitive screen, etc.), gesture inputs (e.g., a user hand action, a user gaze, a user head motion, etc.), tactile inputs (e.g., applying various forces to a client device, such as squeezing the client device, picking up the client device, shaking the client device, placing the client device in a particular orientation, etc.), as well as other types of inputs (e.g., pressing a physical button on the client device, clicking a button in a user interface with a mouse, performing an action with a mouse, such as moving information on a screen up and/or down using a mouse wheel, etc.), and so forth. Similarly, the multimodal device can render output to the user in a number of ways including audio output (e.g., using speakers to generate output for the user to listen to), visual output (e.g., displaying text, images, video, etc. on a screen, flashing lights on the client device on and off, changing the color of lights on the device, etc.), tactile output (e.g., vibrating the client device), etc.
Disclosure of Invention
Systems, methods, and apparatuses for using multimodal responses in dynamically generating client device output tailored to a current modality of a client device are described herein. The current modality of the client device may be one of a plurality of candidate modalities within the multi-modal spectrum and may indicate a current method of user interface input and/or client device output to be utilized. In some implementations, the multimodal spectrum of candidate modalities includes voice-only (voice only) interactions, voice-dominated (voice forward) interactions, multimodal interactions (also referred to as "inter-modality interactions"), visual-dominated (visual forward) interactions, visual-only (visual only) interactions, and so on. The client device can utilize one or more modalities, such as a voice-only interaction, a voice-dominant interaction, a multi-modal interaction, and a visual-only interaction. In some implementations, the current modality of the client device may be determined based on sensor data from one or more sensors of the client device and/or based on a selection of a modality input via a user interface of a user of the client device. For example, the multimodal client device may determine a current modality of the multimodal client device using sensor data from one or more sensors, such as a camera, an inertial measurement unit ("IMU"), a gyroscope, a positioning system (e.g., global positioning system ("GPS"), a microphone, a proximity sensor, a pressure sensor, and/or the like.
The client device may use its modality, as well as various other information, including: a user interface input, one or more modalities of user interface input, a type of client device, a location of the user relative to the client device, and/or the like. According to many embodiments, the multimodal response may include a core message as well as additional information that is modality dependent.
For example, a spoken user interface input to the multimodal client device may indicate a client action, such as adjusting a set/desired temperature of a networked smart thermostat (e.g., a user interface input such as "increase temperature by five degrees"). The multimodal response to render the output for the client device action that adjusts the thermostat temperature may include a core message, such as confirming that the desired temperature has been adjusted. The current modality of the client device, as well as various other information, may determine whether this confirmation is rendered as audio output, visual output, both audio and visual output, and so forth. Similarly, additional information that is modality dependent may provide additional output to be rendered via the client device. For example, the visual output may indicate the newly set temperature as well as the current temperature of the room. Additional visual outputs for use in one or more device modalities may indicate various information, such as an estimated time to reach a new temperature, a name of an associated intelligent thermostat, a temperature outside, and so forth. Similarly, additional audio output for use in one or more device modalities may indicate various information to be rendered by the client device, such as a new temperature, a current temperature, an estimate of how long it will take to reach a desired temperature, and so forth.
In many implementations, the client device can dynamically select portions of the multimodal response to render output for respective ones of the various modalities. For example, a portion of the multi-modal response may be selected for voice-only interaction, and a different portion of the multi-modal response may be selected for visual-only interaction. As another example, one or more visual components of the multi-modal response can be rendered via the client device in a voice-dominant interaction, while, conversely, the visual components of the multi-modal response will not be rendered via the client device in a voice-only interaction.
In many implementations, a single multi-modal response that may be used to generate client device output for any of a plurality of candidate modalities may provide storage efficiency relative to a plurality of individual responses that are each tailored to a single corresponding modality. For example, a single multi-modal response to dynamically generate output for many device modalities may be stored in a smaller portion of memory or other data structure than individual device responses for each modality type. For example, the voice-dominant interaction may contain the same rendered audio output as the multi-modal interaction. The audio output extracted from the same multi-modal response for both the voice-dominant interaction and the multi-modal interaction may reduce memory storage requirements as compared to storing the same audio output in both the voice-dominant interaction response and the multi-modal interaction response.
Additionally or alternatively, a single multi-modal response may provide computational efficiency and/or reduced latency in client device output generation relative to multiple individual responses that are each tailored to a single corresponding modality. For example, having a single multi-modal response rather than separate responses that are each tailored to a single corresponding modality may reduce the size of the index and/or other data structures utilized in determining output for rendering in response to user input, thereby enabling faster and/or more efficient retrieval of responses. Also, for example, in some implementations, a multimodal response can be transmitted from the remote server to the client device, and the client device itself can generate output based on the multimodal response for rendering in response to user input. Transmitting a single multimodal response (as compared to a response for each modal interaction type) may enable the client device to quickly switch between interaction types without requesting and waiting to receive responses for additional interaction types. For example, a multimodal client device such as a cellular telephone may be placed screen side down when a user is engaged in voice-only interactions. If the user picks up the cell phone and looks at the screen, the current device modality may change to multimodal interaction, for example. The multimodal client device can select different portions of the multimodal response and continue to render output without waiting for a separate multimodal interactive response to be received from the remote server.
Moreover, dynamic generation of client device outputs tailored to the current modality of the client device may additionally or alternatively directly result in various other efficiencies. For example, by generating client device outputs that are customized specifically for the current modality, client device resources are not wasted rendering various content that is not needed for the current modality unnecessarily. For example, assume that the client device is a mobile phone and has a current modality of "voice only" based on sensor data indicating that the phone is "face down". With the techniques disclosed herein, in a "speech-only" modality, only audible output may be provided, thereby preventing unnecessary simultaneous rendering of related visual output. Also, for example, dynamic generation of client device output tailored to the current modality may reduce the amount of user input and/or enable an automated assistant to assist the user more efficiently in the performance of a technical task. For example, during multiple rounds of a dialog session between a user and an automated assistant in which output is tailored to a current modality, the output can be dynamically tailored to most efficiently convey information to the user in view of the modality/modalities during the dialog session.
Additionally or alternatively, client device errors may be handled differently depending on the current modality of the device. For example, in voice-only interaction, the client device may render output that instructs the user to repeat their spoken input and/or render output that indicates that the system encountered an error. Similarly, error handling may be rendered visually using a visual-only-interactive client device, such as rendering visual output requesting a user to repeat user input and/or rendering visual output indicating that a backend server error has occurred. The multi-modal interactions, voice-dominant interactions, and/or visual-dominant interactions may use a combination of voice-only and visual-only error handling to handle errors. In many implementations, error handling can be included as part of a multimodal response to a client action that would initiate an error. In other implementations, error handling may be included as part of a separate multimodal response.
The above description is provided as an overview of some embodiments disclosed herein. Additional descriptions of these and other embodiments are set forth in greater detail herein.
In some embodiments, there is provided a method comprising: the client device action is determined based on one or more instances of user interface input provided by a user of the multimodal client device. The method further comprises the following steps: determining a current client device modality of the multimodal client device based at least in part on sensor data from one or more sensors of the multimodal client device, wherein the current client device modality is one of a plurality of discrete client device modalities of the multimodal client device, and wherein the sensor data on which the current client device modality is determined is sensor data other than any sensor data generated by one or more instances of user interface input. The method further comprises the following steps: generating a client device output for a client device action using a multimodal response, wherein the multimodal response includes components of the output for the client device action for a plurality of discrete client device modalities, and wherein generating the client device output includes: one or more of the components of the multi-modal response that are relevant to the current client device modality are selected. The method further comprises the following steps: causing the client device output to be rendered by one or more user interface output devices of the multimodal client device.
These and other implementations of the techniques described herein may include one or more of the following features.
In some implementations, the multimodal response is received by the multimodal client device from the remote server and generating the client device output is by the multimodal client device. In some of those embodiments, the method further comprises: in response to a request transmitted by the client device to the remote server, receiving, by the multimodal client device, a multimodal response from the remote server, the request being based on the user interface input, and wherein determining that a current client modality of the multimodal client device is by the multimodal client device and occurs after transmitting the request. In some versions of those embodiments, the method further comprises: while at least a portion of the client device output is being rendered by one or more user interface output devices of the multimodal client device, detecting a switch of the multimodal client device from a current client device modality to a separate new client device modality, detecting a switch of the multimodal client device from the current client device modality to the separate new client device modality. The method further comprises the following steps: in response to detecting the switch, an alternate client device output is generated using the multimodal response, wherein the alternate client device output includes additional content or less content relative to the client device output. The method further comprises the following steps: causing the alternative client device output to be rendered by the multimodal response client device. In some of those versions, the method further comprises: the client device output includes audible output rendered via at least one speaker of one or more user interface output devices of the multimodal client device and visual output rendered via at least one display of the one or more user interface output devices, the alternative client device lacks the visual output, and causing the alternative client device output to be rendered by the multimodal client device includes: ceasing rendering of the visual output by the at least one display.
In some of those versions, the components of the multi-modal response include: a core message component and one or more modality-dependent components. In some additional or alternative versions, generating the client device output further comprises: at least the core message component of the multimodal response is selected. The method further comprises the following steps: one or more components of the multimodal response that are relevant to the current client device modality are selected by selecting one or more of the modality-dependent components. In some of those versions, the current client device modality is voice-only interaction, and the client device output is rendered only via one or more speakers in the one or more user interface output devices.
in additional or alternative versions, the current client device modality is voice-only interaction, and the client device output is rendered only via one or more speakers of the one or more user interface output devices. In an additional or alternative version, the current client device modality is voice-dominated interaction, core message components output by the client device are rendered only via one or more speakers in the one or more user interface output devices, and one or more modality-dependent components output by the client device are rendered via a touchscreen in the one or more user interface output devices. In an additional or alternative version, the current client device modality is multi-modal interaction and the client device output is rendered via one or more speakers in one or more user interface output devices and via a touchscreen. In an additional or alternative version, the current device modality is a visually dominant interaction, core message components of the client device output are rendered only via a touchscreen in the one or more user interface output devices, and one or more modality-dependent components of the client device output are rendered via one or more speakers in the one or more user interface output devices. In additional or alternative versions, the current device modality is visual interaction only, and the client device output is rendered only via a touchscreen in the one or more user interface output devices.
In some implementations, determining the current client device modality based at least in part on the sensor data includes: the method further includes determining an orientation of the multimodal client device, and selecting a current client device modality based on the orientation of the multimodal client device.
In various embodiments, a method comprises: an error in generating an output for the client device action is determined, wherein the client device action is determined from one or more instances of user interface input provided by a user of the multimodal client device. The method further comprises the following steps: determining a current client device modality of the multimodal client device based at least in part on sensor data from one or more sensors of the multimodal client device, wherein the current device modality is one of a plurality of discrete client device modalities available to the multimodal client device, and wherein the sensor data on which the current client device modality is determined is sensor data other than any sensor data generated by one or more instances of user interface input. The method further comprises the following steps: for an error in generating output for a client device action, generating an error message using a multi-modal response, wherein the multi-modal response includes components of the output for the error for a plurality of discrete client device modalities, and wherein generating the error message includes: one or more of the components of the multi-modal response that are relevant to the current device modality are selected. The method further comprises the following steps: causing an error message to be rendered by one or more user interface output devices of the multimodal client device.
In many embodiments, a method comprises: client device actions and a current client device modality are received via a network interface at one or more server devices remote from the multimodal client device. The method further comprises the following steps: the client device action is determined based on one or more instances of user interface input provided by a user of the multimodal client device. The method further comprises the following steps: a current client device modality is determined based at least in part on sensor data from one or more sensors of the multimodal client device. The method further comprises the following steps: the current client device modality is one of a plurality of discrete client device modalities available to the multimodal client device. The method further comprises the following steps: the sensor data on which the current client device modality is determined is sensor data other than any sensor data generated by one or more instances of user interface input. The method further comprises the following steps: generating a client device output for a client device action using a multimodal response, wherein the multimodal response includes components of the output for the client device action for a plurality of discrete client device modalities, and wherein generating the client device output includes: one or more of the components of the multi-modal response that are relevant to the current client device modality are selected. The method further comprises the following steps: the client device output is transmitted via the network interface to the multimodal client device for rendering by one or more user interface output devices of the multimodal client device.
Furthermore, some embodiments include one or more processors (e.g., Central Processing Units (CPUs), Graphics Processing Units (GPUs), and/or Tensor Processing Units (TPUs) of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the methods described herein.
It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of claimed subject matter that are appended to this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram illustrating an example environment in which various embodiments disclosed herein may be implemented.
Fig. 2 illustrates an example multi-modal spectrum in accordance with embodiments disclosed herein.
FIG. 3 illustrates an example multi-modal response in accordance with embodiments disclosed herein.
FIG. 4 illustrates an example of user interaction with a client device in accordance with embodiments disclosed herein.
FIG. 5 illustrates another example of user interaction with a client device in accordance with embodiments disclosed herein.
Fig. 6A and 6B illustrate another example of user interaction with a client device in accordance with embodiments disclosed herein.
FIG. 7 is a flow chart illustrating a process in which various embodiments disclosed herein may be implemented.
Fig. 8 is a flow diagram illustrating another process in which various embodiments disclosed herein may be implemented.
Fig. 9 is a flow diagram illustrating another process in which various embodiments disclosed herein may be implemented.
Fig. 10 is a block diagram illustrating an example architecture of a computing device.
Detailed Description
FIG. 1 illustrates an example environment 100 in which various embodiments may be implemented. The example environment 100 includes a client device 102. One or more cloud-based remote server components 116, such as a natural language processor 122 and/or a multimodal response module 124, may be implemented on one or more computing systems (collectively "cloud" computing systems) communicatively coupled to the client device 102 via one or more local and/or wide area networks (e.g., the internet), generally indicated as 114.
The client device 102 may include, for example, one or more of the following: a desktop computing device, a laptop computing device, a tablet computing device, a touch-sensitive computing device (e.g., a computing device that may receive input via touch from a user), a mobile phone computing device, a computing device in a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart appliance such as a smart television, and/or a wearable apparatus of a user that includes a computing device (e.g., a watch of a user having a computing device, glasses of a user having a computing device, a virtual reality or augmented reality computing device). In many implementations, the client device 102 may be a multimodal client device. Additional and/or alternative client computing devices may be provided.
In various implementations, the client device 102 may include one or more sensors 108 that may have various forms. The sensors 108 may sense different types of inputs to the client device 102, such as spoken, text, graphical, physical (e.g., a touch on a display device including a touch-sensitive projector and/or a touch-sensitive screen of a computing device), and/or visual (e.g., a gesture) based inputs. Some client devices 102 may be equipped with one or more digital cameras configured to capture and provide signals indicative of detected motion in their field of view. Additionally or alternatively, some client devices may be equipped with sensors that detect acoustic (or pressure) waves, such as one or more microphones.
The sensors 108 may collect various sensor data used in part to determine a current modality of the client device 102, the sensors 108 including: one or more cameras, IMUs, gyroscopes, GPS, microphones, one or more pressure sensors, one or more proximity sensors, and the like. In some implementations, different sensors receiving user interface inputs can be used to collect sensor data used to determine device modalities. For example, a microphone may be used to collect user interface input and IMU data indicative of the location and/or pose of the client device may be used to determine the modality. In other embodiments, sensors may be used to collect user interface input data and determine device modalities. For example, a microphone may determine the user interface input and the same microphone may determine the ambient noise surrounding the client device. In other words, the same sensor (e.g., microphone) may have a set of sensor data corresponding to the user interface input and a second set of sensor data unrelated to the user interface input for use in determining the device modality.
The client device 102 and/or the cloud-based remote server component 116 may communicate with one or more devices 104. The device 104 may comprise any of a variety of devices, including: internet of things devices such as intelligent appliances, intelligent thermostats, intelligent coffee machines, intelligent lights, intelligent locks, intelligent lights, and the like. The devices 104 are linked with the client device 102 (and/or a particular user of the client device 102) and are interconnected with each other. For example, the device 104 may link to a profile assigned to the client device 102 (and optionally to other client devices) and/or may link to a profile assigned to a user of the client device 102. In general, client device 102, other client devices, and device 104 may define an ecosystem of coordinated devices. In various embodiments, devices are linked to one another via device topology representations, which may be user created and/or automatically created and which may define various client devices, various smart devices, identifiers for each device, and/or attributes for each device. For example, the identifier of the device may specify a room (and/or other area) of the structure in which the device is located (e.g., living room, kitchen) and/or may specify a nickname and/or alias for the device (e.g., sofa light, front door lock, bedroom speaker, kitchen assistant, etc.). In this manner, the identifier of a device may be the name, alias, and/or location of the respective device that the user may associate with the respective device.
In many implementations, the device 104 may be controlled by the client device 102 under various modalities. For example, the smart thermostat may be controlled through voice interaction (e.g., a user giving a command to a stand-alone interactive speaker and/or a multi-modal device) as well as physical interaction (e.g., controlling a display on the smart thermostat itself and/or generating a new command input on the multi-modal device). After receiving user input to change the temperature, the multimodal response can generate various combinations of spoken and/or visual output by selecting components of the multimodal response that correspond to the modality of the client device. A multi-modal response according to various embodiments is illustrated in fig. 3.
The device 104 may be controlled directly by the client device 102, and/or the device 104 may be controlled by one or more third party agents 106 hosted by a remote device (e.g., another cloud-based component). Further, one or more third party agents 106 may also perform functions other than controlling the device 104 and/or controlling other hardware devices. For example, the client device 102 may interact with the third party agent 106 to cause a service to be performed, a transaction to be initiated, and so on. In some implementations, the third party agent 106 can provide one or more multimodal responses for generating client device output in response to receiving a command to initiate an action controlled by the third party agent. For example, the third party agent 106 may receive a user input command to turn on a smart light. In addition to turning on the smart light, the third party agent may transmit a multimodal response to the cloud-based remote server component 116 and/or the client device 102 such that additional output may be rendered on the client device according to the modality of the client device to confirm that the light has been turned on.
in many implementations, the client device 102 may conduct a dialog session with one or more users via user interface input devices and/or output devices of one or more client devices. In some implementations, the client device 102 may conduct a dialog session with the user in response to user interface input provided by the user via one or more user interface input devices of one of the client devices 102. In some of these embodiments, the user interface input is explicitly directed to an automated assistant (not shown). For example, the user can speak a predetermined invocation phrase, such as "OK, Assistant" or "Hey, Assistant" to cause the automated Assistant to begin listening actively.
In some implementations, the client device 102 can conduct a conversation session in response to the user interface input even when the user interface input is not explicitly directed to the automated assistant. In many implementations, the client device 102 can utilize speech recognition to convert an utterance from a user into text and respond to the text accordingly, e.g., by providing visual information, by providing search results, by providing general information, and/or taking one or more responsive actions (e.g., playing media, starting a game, ordering food, etc.). In some implementations, the client device 102 can additionally or alternatively respond to an utterance without converting the utterance to text. For example, the client device 102 may convert the speech input into embedding (embedding), into an entity representation (indicating the entity/entities present in the speech input), and/or other "non-text" representations and operate on such non-text representations. Accordingly, embodiments described herein as operating based on text converted from speech input may additionally and/or alternatively operate directly on speech input and/or other non-text representations of speech input.
The client device 102 and cloud-based remote server components 116 may include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. Operations performed by one or more computing devices 102 may be distributed across multiple computer systems.
in various implementations, the client device 102 may include a corresponding voice capture/text-to-voice ("TTS")/voice-to-text ("STT") module 110. In other implementations, one or more aspects of the voice capture/TTS/STT module 110 may be implemented separately from the client device 102. The voice capture/TTS/STT module 110 may be configured to perform one or more functions: capturing a user's voice, e.g., via a microphone; converting the captured audio to text (and/or other representation or embedding); and/or converting text to speech. For example, in some implementations, because the client device 102 may be relatively limited in computing resources (e.g., processor cycles, memory, battery, etc.), the voice capture/TTS/STT module 110 located at the client device 102 may be configured to convert a limited number of different spoken phrases, particularly phrases that invoke automated assistants, into text (or other forms, such as lower-dimensional embedding). Other voice inputs may be sent to the cloud-based remote server component 116, which cloud-based remote server component 116 may include a cloud-based TTS module 118 and/or a cloud-based STT module 120.
The cloud-based STT module 120 may be configured to utilize the nearly unlimited resources of the cloud to convert audio data captured by the voice capture/TTS/STT module 110 to text (which may then be provided to the natural language processor 122). Similarly, cloud-based TTS module 118 may be configured to utilize the nearly unlimited resources of the cloud to convert text data into computer-generated speech output. In some implementations, TTS module 118 can provide computer-generated speech output to client device 102 to be directly output, e.g., using one or more speakers. In other implementations, the text data (e.g., natural language response) generated by the STT module 120 can be provided to the voice capture/TTS/STT module 110, which the voice capture/TTS/STT module 110 can then convert the text data to computer-generated voice that is output locally.
The cloud-based remote server component 116 can include a natural language processor 122, a multimodal response module 124, the aforementioned TTS module 118, the aforementioned STT module 120, and other components, some of which are described in more detail below. In some implementations, one or more of the engines and/or modules of the client device 102 and/or the cloud-based remote server component 116 may be omitted, combined, and/or implemented in a component separate from the client device 102. In some implementations, to protect privacy, one or more of the components, such as the natural language processor 122, the voice capture/TTS/STT module 110, the multimodal response module 124, etc., may be implemented at least in part on the client device 102 (e.g., to the exclusion of the cloud).
in some implementations, the client device 102 generates responsive content in response to various inputs generated by a user during a human-to-computer dialog session. Additionally or alternatively, the client device 102 may provide responsive content for presentation to the user as part of the conversation session. For example, responsive content may be generated in response to free-form natural language input provided via the client device 102. As used herein, free-form input is input conceived by a user that is not limited to a set of options presented for selection by the user.
The natural language processor 122 processes natural language input generated by a user via the client device 102 and may generate annotated output. For example, the natural language processor 122 may process natural language free-form input generated by a user via one or more user interface input devices of the client device 102. The generated labeled output includes one or more labels of the natural language input and optionally one or more (e.g., all) of the terms in the natural language input.
In some implementations, the natural language processor 122 is configured to identify and label various types of grammatical information in the natural language input. For example, the natural language processor 122 may include a portion of a part-of-speech tagger configured to tag terms using their grammatical roles. Also, for example, in some implementations, the natural language processor 122 can additionally and/or alternatively include a dependency parser (not shown) configured to determine syntactic relationships between terms in the natural language input.
In some implementations, the natural language processor 122 can additionally and/or alternatively include an entity tagger (not shown) configured to tag entity references, such as references to persons (e.g., including literary characters, celebrities, public characters, etc.), organizations, (real and fictional) places, etc., in one or more segments. The entity tagger of the natural language processor 122 may tag references to entities at a high level of granularity (e.g., to enable identification of full references to a class of entities such as a person) and/or tag references to entities at a lower level of granularity (e.g., to enable identification of full references to a particular entity such as a particular person). The entity tagger may rely on the content of the natural language input to resolve (resolve) a particular entity and/or may optionally communicate with a knowledge graph or other entity database to resolve a particular entity.
In some implementations, the natural language processor 122 can additionally and/or alternatively include a coreference resolver (not shown) configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, "there" in the natural language input "I liked Artificial cafe where we liked the Hypothetical cafe where we took the last meal" can be resolved into "Artificial cafe" using a coreference resolver.
In many implementations, one or more components of the natural language processor 122 may rely on annotations from one or more other components in the natural language processor 122. For example, in some implementations, the referenced entity tagger may rely on annotations from the coreference resolver and/or dependency resolver in all references to a particular entity by annotation. Also for example, in some embodiments, the coreference resolver may rely on annotations from dependent resolvers in clustering references to the same entity. In many implementations, in processing a particular natural language input, one or more components of the natural language processor 122 may determine one or more annotations using related previous inputs and/or other related data in addition to the particular natural language input.
The multimodal response module 124 can retrieve multimodal responses related to the user input, receive multimodal responses from third party agents, determine a modality of the client device 102, select one or more portions of the multimodal responses for inclusion in the client device output, and so forth.
In many implementations, upon receiving user interface input from a client device, the multimodal response module 124 can retrieve one or more multimodal responses related to the user interface input. For example, the user may ask the client device "how the current weather (what is the current weather)", and the multimodal response module 124 may retrieve the corresponding multimodal response to render the current weather output on the client device. Additionally or alternatively, in response to user interface input received at the client device, the multimodal response may be retrieved from and/or transmitted to the cloud-based remote server component and/or the client device itself.
The multimodal response module 124 may additionally determine a current modality of the client device. Various data collected by client device 102 and/or sensors 108 may be indicative of a client device modality, including a type of client device, a state of visual and/or audio components of the client device, a location of the client device, a pose of the client device, a location of the user relative to the client device, and so forth.
In many implementations, the multimodal response module 124 can determine the type of client device, such as a stand-alone interactive speaker, a cellular telephone, a touch screen interface on a hardware device such as a smart thermostat, and so forth. For example, the smart thermostat may be controlled by various client devices, including a stand-alone interactive speaker, a cellular telephone, and/or a display screen on the smart thermostat itself. Some client devices are inherently limited to a particular modality due to one or more hardware components not included in the client device (e.g., a device without a speaker may be limited to only visual interaction, and similarly, a device without a display screen may be limited to only voice interaction). While the stand-alone interactive speaker, the display screen on the cell phone and/or the smart thermostat may all utilize the same multi-modal response in controlling the thermostat, the stand-alone interactive speaker is typically limited to voice-only interaction due to the lack of a visual display. Similarly, display screens on smart thermostats that lack microphones and/or speakers are typically limited to visual interaction only. Cellular phones (and other multimodal client devices) may take additional modalities and may therefore utilize additional components of the multimodal response. In other words, a multimodal client device that is voice-only interactive may use similar (and often identical) components of a multimodal response to a client device that is voice-only interactive only, such as a stand-alone interactive speaker. Similarly, a multimodal client device that conducts only visual interactions may use similar (and often identical) components of a multimodal response as a visual-only client device.
In many implementations, the location of the client device can be used to determine one or more components of the multimodal response for rendering. For example, a GPS unit may determine the location of the mobile device. The location known to the client device may indicate the type of interaction the user may want to have. For example, the user may not want to render sound when in a known "work" position and the multi-modal response module 124 may determine that only visual interactions and/or visual dominant interactions. Similarly, the user may be less concerned about rendering sound at home, and a known "home" location may be indicated to the multimodal response module 124 to determine multimodal interactions. In some implementations, the client device can determine the location of the client device by connecting to a known Wi-Fi network (e.g., the client device knows when it is connected to a "home" Wi-Fi network).
Many client devices may be placed in various poses by a user. For example, a cellular telephone may be placed in a "face down" position, which would hinder the user's ability to see the telephone screen. In some such implementations, a gesture that prevents the user's ability to see the display can indicate to the multimodal response module 124 that the client device is in voice-only or voice-dominant interaction. Various sensors may be used to determine the pose of the client device, including an IMU, one or more cameras, and the like.
additionally or alternatively, the device modality may be determined by a location of the user relative to the client device. For example, one or more cameras may determine a location of a user relative to a client device. Similarly, the proximity sensor may determine when the user is within a threshold range of the client device. In many implementations, the multimodal client device can be in different modalities depending on the position of the user relative to the screen of the multimodal client device. For example, a user holding a client device is typically very close to the screen and can see more detailed modality-dependent visual components of the multimodal response. In other embodiments, the client device may determine that the user is on the other side of the room relative to the client device. While a user located on the other side of the room may still see some information on the client device display, the multimodal response module 124 may determine that the client device is engaged in a voice-dominant interaction and render modality-dependent visual components on the display using less dense visual information.
in determining the device modality, the multimodal response module 124 can utilize sensor data specific to various other client devices. For example, an in-vehicle navigation system may utilize various sensors to determine when a vehicle is traveling. In some such embodiments, the multimodal response module 124 may determine that the in-vehicle navigation system may engage in voice-only or voice-dominant interactions while the vehicle is traveling, and may engage in any of the discrete types of interactions within the multimodal spectrum while the vehicle is stopped.
After determining the current modality of the client device 102, the multimodal response module 124 can select one or more components of the multimodal response to render output to the user via the client device. The components of a multimodal response including a core message component and one or more modality-dependent components are illustrated in fig. 3.
Fig. 2 illustrates an image of a multi-modal spectrum 200 according to various embodiments described herein. The multimodal spectrum includes a plurality of discrete modalities of multimodal client device interaction. In many embodiments, multimodal spectroscopy 200 can include: voice-only interaction 202, voice-dominant interaction 204, multi-modal interaction 206 (also referred to as "inter-modality interaction"), visual-dominant interaction 208, visual-only interaction 210, and so forth.
The voice-only interaction 202 may include a user speaking into the client device and/or listening from the client device (i.e., an audio interaction). For example, a stand-alone interactive speaker may be voice-only interactive. Additionally or alternatively, the multimodal client device may engage in voice-only interaction when, for example, the screen is not visible to the user. As one illustrative example, a cellular telephone may engage in voice-only interaction when the cellular telephone screen is placed side down on a surface, the screen is closed, the user is too far away to see the screen, etc. In many embodiments, the voice-only interaction 202 includes: the user provides spoken input to the client device via the microphone along with rendering output by the client device through the speaker. An example of voice-only interaction according to many embodiments is depicted in FIG. 4.
Additionally or alternatively, the visual-only interaction 210 includes: a user provides physical input to the client device (e.g., typing, clicking on a physical button, clicking on a button rendered by a user interface display, shaking the client device, etc.) along with rendering output on the display by the client device. Some client devices that engage in only visual interaction can lack a microphone and/or speaker. In other implementations, the multimodal client device may engage in only visual interaction while the audio interface is disabled. For example, if the client device speaker is muted, the smartphone may engage in only visual interaction. Examples of visual interaction only are depicted in fig. 6A-6B.
the multimodal client device conducting the voice-dominant interaction 204, the multimodal interaction 206, and the visual-dominant interaction 208 may use various input interfaces and/or output interfaces, such as a microphone and speaker for voice interaction and a physical input and display screen for visual interaction. In some implementations, the voice-dominant interaction 204 can include rendering the core message component of the multi-modal response as audio and rendering one or more additional modality-dependent audio and/or visual components of the multi-modal response. For example, the multimodal client device may render an output informing the user of the current weather (i.e., the core message component of the multimodal response) and instruct the user via the speaker to view a screen for the weekly weather report (i.e., the modality-dependent audio component of the multimodal response) and render the weekly weather report (i.e., the modality-dependent visual component of the multimodal response) on the client device display.
Additionally or alternatively, in the multimodal interaction 206, the client device may render the current weather as audio output and the current weather as visual output (i.e., the core message components may be rendered via speakers and screen) and may visually render one or more of a variety of additional weather information on the screen, such as an interactive weather report for the next week, where the user may select a single day to fetch more detailed weather information for that day (i.e., the modality-dependent visual components). In some such implementations, one or more additional audio components (i.e., modality-dependent audio components) may be rendered after a user selects a single day to retrieve more detailed weather information.
Similarly, the visually dominant interaction 208 may be generated, for example, when one or more sensors determine that the screen is visible to the user but that the user is at a threshold distance away from the screen and is generally unable to read detailed information on the screen. In some such implementations, the current temperature (i.e., the core message component) may be rendered via a speaker, and the current temperature may be rendered as a large graphic, such that the user may see the current temperature (i.e., the modality-dependent visual component) from the other end of the room.
FIG. 3 illustrates an example multi-modal response in accordance with various embodiments described herein. In many implementations, multimodal response 300 may include a core message component 302, modality-dependent audio components 304, 306, and modality-dependent visual components 308, 310. The core message component 302 can include information that will generate a multimodal response for all modalities. In some implementations, the client device can determine how to render the core message component 302 at the client device. In other words, the core message component may be converted by the client device into an audio output or a visual output (e.g., text may be visually rendered by the client device and converted into a spoken output). In some other implementations, the core message component 302 may also include an audio core message component (not shown) and a visual core message component (not shown), which may enable the client device to render the same information for any modality. Additionally or alternatively, certain actions performed by the multimodal client device cannot be performed in each device modality and the core message component 302 may include a visual component or an audio component. For example, video cannot be played in audio-only interactions, so the core message components comprising video will not be rendered via audio-only interactions. In some such implementations, the multimodal response can generate an error message to render to the user indicating that the video cannot be played.
In various embodiments, the multi-modal response can be structured such that the multi-modal response has a preferred device modality. For example, the client device can switch to a voice-dominant interaction to render information in the multimodal response whenever possible. In other words, the multimodal response may push the client device into a particular modality when several modalities are available for the client device.
the modality-dependent audio components 304, 306 may include different audio information that may be rendered by the client device. Similarly, modality-dependent visual components 308, 310 may include different visual information that may be rendered by the client device. In many implementations, the multimodal response module 124 (as illustrated in fig. 1) can select a combination of modality-dependent components (such as 304, 306, 308, 310) for a particular client device in a particular modality. In other words, the one or more modality-dependent components selected for a first client device in one modality may be different from the one or more modality-dependent components selected for a second client device in the same modality. Further, different components of the multimodal response can be selected at different times for the same client device in the same modality. For example, the modality-dependent visual component may change depending on the detected distance of the user from the client device.
In many implementations, the multimodal responses can be generated separately for each client action. For example, a user may provide output for rendering for several modalities, which may be consolidated into a multimodal response by one or more server devices.
FIG. 4 illustrates an example image of a user interacting with a client device in a voice-only interaction. The image 400 includes a client device 402 and a user 404. In many implementations, the client device 402 may include a stand-alone interactive speaker. Additionally or alternatively, the multimodal client device may engage in voice-only interactions based on the unavailability of physical input and/or visual output. For example, the user 404 may change the temperature of a networked smart thermostat (not shown) by saying "Assistant, please turn up the temperature by three degrees". The core message component of the multimodal response may indicate a change in temperature. For example, in voice-only interaction, the speaker may render "OK, the temperature has been set to 75 degrees". In many implementations, one or more additional modality-dependent audio components of The multimodal response may be additionally rendered, such as "The current temperature is 73 degrees", "It with take to approximate one hour to reach 75 degrees", and so forth.
FIG. 5 illustrates an example image of a user interacting with a multimodal client device. The image 500 includes a multimodal client device 502 (such as a cellular telephone) and a user 504. In many implementations, the multimodal client device can conduct any one of the separate modalities in the multimodal spectrum. The visual-dominant interactions, multi-modal interactions, and/or voice-dominant interactions may share one or more visual and/or voice components of the multi-modal response. For example, user input generated by user 504 may change a temperature setting on a networked intelligent thermostat. The core message components may be audibly rendered and shown on a display screen. For example, the speaker may render "OK, the temperature has been left set to 75 degrees" while the display screen renders a text message indicating the new temperature. One or more modality-dependent voice components and/or one or more modality-dependent visual components of the multi-modal response may be rendered by the client device to enhance the core message component.
Fig. 6A illustrates an image of a client device prior to engaging in visual-only interactions. The image 600 includes a display 602 of the intelligent thermostat, the display 602 indicating that the current temperature is 72 degrees. The user may change the temperature by interacting with the touch surface of the smart thermostat display to increase the temperature by three degrees. Fig. 6B shows an image 610 illustrating the same intelligent thermostat display 612 after the temperature has been increased by three degrees. For example, the core message "Temperature included to 75 degrees" may be rendered on the display. One or more modality-dependent visual components of the multi-modal response, such as the Current temperature (e.g., "Current-72 degrees"), may also be rendered on the display. In many implementations, a multimodal client device (not shown) that performs only visual interactions may control the intelligent thermostat. In some such implementations, upon receiving a user input to increase the Temperature, the same core message component of the multimodal response (e.g., "Temperature increased to 75 degrees") and the same modality-dependent visual component (e.g., "Current-72 degrees") may be displayed on the smart thermostat display. In many implementations, a multimodal client device such as a cellular telephone (which typically has a larger screen for rendering content than a hardware device display) may present additional modality-dependent visual components such as "it will take prompt alert one way to reach 75 definitions". In other words, a particular client device may affect: which components of the multi-modal response are rendered for the same action performed in the same modality.
Fig. 7 is a flow diagram illustrating an example process 700 for generating client device output using multimodal responses in accordance with various embodiments disclosed herein. For convenience, the operations of fig. 7 are described with reference to a system performing the operations. This system may include various components of various computer systems, such as one or more components of client device 102. Further, while the operations of process 700 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 702, the system receives user interface input provided by a user of the multimodal client device. For example, the system may receive spoken user interface input, typed user interface input, gesture-based input, and/or other input.
at block 704, the system determines sensor data related to a device modality of the multimodal client device.
At block 706, the system transmits the user interface input and the sensor data to one or more remote server devices via a network.
At block 708, the system receives an instance of the client device output from one or more remote server devices via a network. In many embodiments, one or more remote server devices identify an action corresponding to a user interface input and a device modality corresponding to sensor data. Additionally or alternatively, the client device output may include one or more components of a multimodal response related to a device modality. In many implementations, the multimodal response module 124 can determine a current device modality and select one or more components of the multimodal response to generate the client device output.
At block 710, the system renders client device output using one or more user interface output devices of the multimodal client device. For example, the system may render the client device output using a user interface output device that corresponds to the client device output and thus also corresponds to the current modality of the client device.
fig. 8 is a flow diagram illustrating an example process 800 for generating client device output using multimodal responses in accordance with various embodiments disclosed herein. For convenience, the operations of fig. 8 are described with reference to a system performing the operations. This system may include various components of various computer systems, such as one or more components of client device 102. Further, while the operations of process 800 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
At block 802, the system determines a client device action based on one or more instances of user interface input provided by a user of the multimodal client device. In many implementations, the client device action may be determined from user interface input. For example, the client device itself may determine the client device action from the received user interface input (optionally interfacing with a remote server when making the determination) and/or the client device action may be determined by the remote system based on the user interface input (and/or a translation thereof) transmitted to the remote system.
At block 804, the system determines a current client device modality of the multimodal client device based at least in part on sensor data from one or more sensors of the multimodal client device. In many embodiments, the device modality may be determined by a multi-modal response module 124 as described in fig. 1.
At block 806, the system receives a multimodal response from the one or more remote server devices via the network, wherein the multimodal response includes components of the output for the client device action for a plurality of client device modalities.
at block 808, the system generates client device output for the client device action using the multimodal response. The client device output may include one or more components of a multimodal response related to the current device modality. In many implementations, the multimodal response module 124 can select one or more portions of the multimodal response to generate the client device output.
At block 810, the system renders client device output using one or more user interface output devices of the multimodal client device.
Fig. 9 is a flow diagram illustrating an example process 900 for generating an error message for a client device using a multimodal response in accordance with various embodiments disclosed herein. For convenience, the operations of fig. 9 are described with reference to a system performing the operations. This system may include various components of various computer systems, such as one or more components of client device 102. Further, while the operations of process 900 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
at block 902, the system determines an error in generating an output of the multimodal client device from one or more instances of user interface input provided by a user of the multimodal client device.
At block 904, the system determines a current client device modality of the multimodal client device based at least in part on sensor data from one or more sensors of the multimodal client device.
At block 906, the system generates an error message for the error in generating the output using the multimodal response by: one or more components of a multimodal response that are related to a current client device modality are selected.
At block 908, the system renders an error message using one or more user interface output devices of the multimodal client device. For example, the error message may be rendered via a client device speaker and/or as a text output on a client device display screen.
Fig. 10 is a block diagram of an example computing device 1010 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing device, user-controlled resource modules, and/or other components can include one or more components of the example computing device 1010.
the computing device 1010 typically includes at least one processor 1014, the at least one processor 1014 communicating with a plurality of peripheral devices via a bus subsystem 1012. These peripheral devices may include: storage subsystem 1024, including for example memory subsystem 1025 and file storage subsystem 1026, user interface output device 1020, user interface input device 1022, and network interface subsystem 1016. The input and output devices allow user interaction with computing device 1010. Network interface subsystem 1016 provides an interface to an external network and is coupled to corresponding interface devices in other computing devices.
The user interface input devices 1022 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or drawing tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and methods for inputting information to computing device 1010 or onto a communication network.
user interface output devices 1020 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and methods for outputting information from computing device 1010 to a user or to another machine or computing device.
Storage subsystem 1024 stores programming and data structures that provide the functionality of some or all of the modules described herein. For example, storage subsystem 1024 may include logic to perform selected aspects of the processes of fig. 7, 8, and/or 9 and to implement various components depicted in fig. 1.
These software modules are typically executed by the processor 1014, either alone or in combination with other processors. Memory 1025 used in storage subsystem 1024 may include a number of memories including a main Random Access Memory (RAM)1030 for storing instructions and data during program execution and a Read Only Memory (ROM)1032 in which fixed instructions are stored. File storage subsystem 1026 may provide persistent storage for program and data files and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 1026 in storage subsystem 1024 or in other machines accessible to processor 1014.
Bus subsystem 1012 provides a mechanism for the various components and subsystems of computing device 1010 to communicate with one another as intended. Although the bus subsystem 1012 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
Computing device 1010 can be of various types, including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 1010 depicted in FIG. 10 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 1010 are possible with more or fewer components than the computing device depicted in fig. 10.
Where the systems described herein collect or otherwise monitor personal information about a user or personal and/or monitored information may be utilized, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. Also, certain data may be processed in one or more ways before being stored or used, such that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or in the case of obtaining geographic location information, the user's geographic location may be generalized (such as to a city, zip code, or state level). Such that the user's particular geographic location cannot be determined. Thus, the user may control how information about the user is collected and/or used.
while several embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized and each of such variations and/or modifications is deemed to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the particular application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure relate to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (20)
1. A method implemented by one or more processors, the method comprising:
Determining a client device action based on one or more instances of user interface input provided by a user of the multimodal client device;
Determine a current client device modality of the multimodal client device based at least in part on sensor data from one or more sensors of the multimodal client device,
Wherein the current client device modality is one of a plurality of discrete client device modalities available to the multimodal client device, an
Wherein the sensor data on which the current client device modality is determined is sensor data other than any sensor data generated by the one or more instances of user interface input;
Generating a client device output for the client device action using a multimodal response,
Wherein the multi-modal response comprises a component of the output for the client device action for the plurality of discrete client device modalities, an
Wherein generating the client device output comprises: selecting one or more of the components of the multimodal response that are relevant to the current client device modality; and
Causing the client device output to be rendered by one or more user interface output devices of the multimodal client device.
2. The method of claim 1, wherein the multimodal response is received by the multimodal client device from a remote server, and wherein generating the client device output is by the multimodal client device.
3. the method of claim 2, wherein the multimodal response is received by the multimodal client device from the remote server in response to a request transmitted by the multimodal client device to the remote server, the request being based on the user interface input, and wherein determining that the current client device modality of the multimodal client device is by the multimodal client device occurs after transmitting the request.
4. The method of claim 2, further comprising:
While at least a portion of the client device output is being rendered by the one or more user interface output devices of the multimodal client device:
Detecting a switch of the multimodal client device from the current client device modality to a separate new client device modality;
in response to detecting the switch, generating an alternate client device output using the multimodal response, wherein the alternate client device output includes additional or less content relative to the client device output; and
Causing the alternate client device output to be rendered by the multimodal client device.
5. The method of claim 4, wherein the first and second light sources are selected from the group consisting of,
Wherein the client device output comprises: an audible output rendered via at least one speaker of the one or more user interface output devices of the multimodal client device and a visual output rendered via at least one display of the one or more user interface output devices,
Wherein the alternative client device output lacks the visual output, an
Wherein causing the alternate client device output to be rendered by the multimodal client device comprises: ceasing rendering of the visual output by the at least one display.
6. The method of claim 1, wherein determining the current client device modality based at least in part on the sensor data comprises:
Determining an orientation of the multimodal client device; and
Selecting the current client device modality based on the orientation of the multimodal client device.
7. The method of claim 5, wherein the components of the multi-modal response comprise: a core message component and one or more modality-dependent components.
8. The method of claim 7, wherein generating the client device output further comprises: selecting at least the core message component of the multi-modal response, and wherein selecting the one or more of the components of the multi-modal response that are relevant to the current client device modality comprises: selecting one or more of the modality-dependent components.
9. The method of claim 8, wherein the current client device modality is voice-only interaction, and the client device output is rendered only via one or more speakers of the one or more user interface output devices.
10. The method of claim 8, wherein the current client device modality is voice-dominated interaction, the core message components output by the client device are rendered only via one or more speakers in the one or more user interface output devices, and the one or more modality-dependent components output by the client device are rendered via a touchscreen in the one or more user interface output devices.
11. The method of claim 8, wherein the current client device modality is a multi-modal interaction, the client device output being rendered via one or more speakers and via a touchscreen in the one or more user interface output devices.
12. The method of claim 8, wherein the current client device modality is a visually-dominant interaction, the core message components of the client device output are rendered only via a touchscreen in the one or more user interface output devices, and the one or more modality-dependent components of the client device output are rendered via one or more speakers in the one or more user interface output devices.
13. the method of claim 8, wherein the current client device modality is visual-only interaction, and the client device output is rendered only via a touchscreen of the one or more user interface output devices.
14. A method implemented by one or more processors, the method comprising:
Determining an error in generating output for a client device action, wherein the client device action is determined from one or more instances of user interface input provided by a user of a multimodal client device;
Determine a current client device modality of the multimodal client device based at least in part on sensor data from one or more sensors of the multimodal client device,
wherein the current device modality is one of a plurality of discrete client device modalities available to the multimodal client device, an
Wherein the sensor data on which the current client device modality is determined is sensor data other than any sensor data generated by the one or more instances of user interface input;
generating an error message using a multimodal response for the error in generating the output for the client device action,
Wherein the multi-modal response includes a component of the output for the error for the plurality of discrete client device modalities, an
Wherein generating the error message comprises: selecting one or more of the components of the multimodal response that are relevant to the current client device modality; and
Causing the error message to be rendered by one or more user interface output devices of the multimodal client device.
15. The method of claim 14, wherein the current client device modality is voice-only interaction, and the error message is rendered only via one or more speakers of the one or more user interface output devices.
16. The method of claim 14, wherein the current client device modality is voice-dominated interaction, and the error message is rendered via one or more speakers and via a touchscreen in the one or more user interface output devices.
17. the method of claim 14, wherein the current client device modality is a multi-modal interaction, and the error message is rendered via one or more speakers and via a touchscreen in the one or more user interface output devices.
18. the method of claim 14, wherein the current client device modality is a visually dominant interaction, and the error message is rendered via one or more speakers and via a touchscreen in the one or more user interface output devices.
19. The method of claim 14, wherein the current client device modality is visual-only interaction, and the error message is rendered only via a touchscreen in the one or more user interface output devices.
20. A method implemented by one or more processors, the method comprising:
Receiving, via a network interface at one or more server devices remote from the multimodal client device, a client device action and a current client device modality,
wherein the client device actions are determined based on one or more instances of user interface input provided by a user of the multimodal client device,
Wherein the current client device modality is determined based at least in part on sensor data from one or more sensors of the multimodal client device,
Wherein the current client device modality is one of a plurality of discrete client device modalities available to the multimodal client device, an
Wherein the sensor data on which the current client device modality is determined is sensor data other than any sensor data generated by the one or more instances of user interface input;
Generating a client device output for the client device action using a multimodal response,
Wherein the multi-modal response comprises a component of the output for the client device action for the plurality of discrete client device modalities, an
Wherein generating the client device output comprises: selecting one or more of the components of the multimodal response that are relevant to the current client device modality; and
Transmit, via the network interface, the client device output to the multimodal client device for rendering by one or more user interface output devices of the multimodal client device.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202410158700.7A CN117971154A (en) | 2018-09-04 | 2019-09-03 | Multimodal response |
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862726947P | 2018-09-04 | 2018-09-04 | |
US62/726,947 | 2018-09-04 | ||
US16/251,982 US11164576B2 (en) | 2018-09-04 | 2019-01-18 | Multimodal responses |
US16/251,982 | 2019-01-18 |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202410158700.7A Division CN117971154A (en) | 2018-09-04 | 2019-09-03 | Multimodal response |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110543290A true CN110543290A (en) | 2019-12-06 |
CN110543290B CN110543290B (en) | 2024-03-05 |
Family
ID=68711100
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202410158700.7A Pending CN117971154A (en) | 2018-09-04 | 2019-09-03 | Multimodal response |
CN201910826487.1A Active CN110543290B (en) | 2018-09-04 | 2019-09-03 | Multimodal response |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202410158700.7A Pending CN117971154A (en) | 2018-09-04 | 2019-09-03 | Multimodal response |
Country Status (2)
Country | Link |
---|---|
US (1) | US11935530B2 (en) |
CN (2) | CN117971154A (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113778580A (en) * | 2021-07-28 | 2021-12-10 | 赤子城网络技术（北京）有限公司 | Modal user interface display method, electronic device and storage medium |
CN117351331A (en) * | 2023-10-24 | 2024-01-05 | 北京云上曲率科技有限公司 | Method and device for adding adapter for large visual model |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101689187A (en) * | 2007-05-29 | 2010-03-31 | 智思博公司 | Multi-modal smartpen computing system |
CN101911064A (en) * | 2007-12-31 | 2010-12-08 | 摩托罗拉公司 | Methods and apparatus for implementing distributed multi-modal applications |
CN102340649A (en) * | 2011-10-24 | 2012-02-01 | 华为技术有限公司 | Call control method for cordless video telephone and cordless video telephone |
CN102818913A (en) * | 2012-07-31 | 2012-12-12 | 宋子健 | Detection device and detection method for human motion information |
CN203135171U (en) * | 2012-09-07 | 2013-08-14 | 苹果公司 | Connector adapter |
CN104423829A (en) * | 2013-09-09 | 2015-03-18 | 联想(北京)有限公司 | Information processing method and electronic equipment |
CN104618206A (en) * | 2015-02-09 | 2015-05-13 | 河南九洲计算机有限公司 | Family core accessing device for realizing intelligent community service and system thereof |
CN105721666A (en) * | 2014-12-18 | 2016-06-29 | Lg电子株式会社 | Mobile Terminal And Controlling Method Thereof |
CN106025679A (en) * | 2011-11-07 | 2016-10-12 | 苹果公司 | Dual orientation electronic connector with external contacts |
US20160299959A1 (en) * | 2011-12-19 | 2016-10-13 | Microsoft Corporation | Sensor Fusion Interface for Multiple Sensor Input |
CN107423809A (en) * | 2017-07-07 | 2017-12-01 | 北京光年无限科技有限公司 | The multi-modal exchange method of virtual robot and system applied to net cast platform |
CN108106541A (en) * | 2017-12-21 | 2018-06-01 | 浙江大学 | A kind of bridge cable force measuring method based on video image identification |
CN108197329A (en) * | 2011-11-01 | 2018-06-22 | 微软技术许可有限责任公司 | It is synchronized by the Real-time document demonstration data of generic service |
Family Cites Families (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7602748B2 (en) | 2004-08-13 | 2009-10-13 | Verizon Business Global Llc | Fixed-mobile communications with mid-session mode switching |
US8977965B1 (en) | 2005-08-19 | 2015-03-10 | At&T Intellectual Property Ii, L.P. | System and method for controlling presentations using a multimodal interface |
US8386260B2 (en) * | 2007-12-31 | 2013-02-26 | Motorola Mobility Llc | Methods and apparatus for implementing distributed multi-modal applications |
US20090182562A1 (en) * | 2008-01-14 | 2009-07-16 | Garmin Ltd. | Dynamic user interface for automated speech recognition |
US10031549B2 (en) * | 2008-07-10 | 2018-07-24 | Apple Inc. | Transitioning between modes of input |
US10382509B2 (en) | 2011-01-28 | 2019-08-13 | Amazon Technologies, Inc. | Audio-based application architecture |
KR102158843B1 (en) * | 2013-08-05 | 2020-10-23 | 삼성전자주식회사 | Method for user input by using mobile device and mobile device |
WO2016049080A1 (en) | 2014-09-22 | 2016-03-31 | Dexcom, Inc. | System and method for mode switching |
US9904450B2 (en) | 2014-12-19 | 2018-02-27 | At&T Intellectual Property I, L.P. | System and method for creating and sharing plans through multimodal dialog |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US9443519B1 (en) * | 2015-09-09 | 2016-09-13 | Google Inc. | Reducing latency caused by switching input modalities |
KR102307976B1 (en) | 2016-05-10 | 2021-09-30 | 구글 엘엘씨 | Implementations for voice assistant on devices |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
DK201770411A1 (en) | 2017-05-15 | 2018-12-20 | Apple Inc. | Multi-modal interfaces |
CN108062213A (en) * | 2017-10-20 | 2018-05-22 | 沈阳美行科技有限公司 | A kind of methods of exhibiting and device at quick search interface |
WO2019195799A1 (en) * | 2018-04-05 | 2019-10-10 | Synaptics Incorporated | Context-aware control for smart devices |
-
2019
- 2019-09-03 CN CN202410158700.7A patent/CN117971154A/en active Pending
- 2019-09-03 CN CN201910826487.1A patent/CN110543290B/en active Active
-
2021
- 2021-11-01 US US17/515,901 patent/US11935530B2/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101689187A (en) * | 2007-05-29 | 2010-03-31 | 智思博公司 | Multi-modal smartpen computing system |
CN101911064A (en) * | 2007-12-31 | 2010-12-08 | 摩托罗拉公司 | Methods and apparatus for implementing distributed multi-modal applications |
CN102340649A (en) * | 2011-10-24 | 2012-02-01 | 华为技术有限公司 | Call control method for cordless video telephone and cordless video telephone |
CN108197329A (en) * | 2011-11-01 | 2018-06-22 | 微软技术许可有限责任公司 | It is synchronized by the Real-time document demonstration data of generic service |
CN106025679A (en) * | 2011-11-07 | 2016-10-12 | 苹果公司 | Dual orientation electronic connector with external contacts |
US20160299959A1 (en) * | 2011-12-19 | 2016-10-13 | Microsoft Corporation | Sensor Fusion Interface for Multiple Sensor Input |
CN102818913A (en) * | 2012-07-31 | 2012-12-12 | 宋子健 | Detection device and detection method for human motion information |
CN203135171U (en) * | 2012-09-07 | 2013-08-14 | 苹果公司 | Connector adapter |
CN104423829A (en) * | 2013-09-09 | 2015-03-18 | 联想(北京)有限公司 | Information processing method and electronic equipment |
CN105721666A (en) * | 2014-12-18 | 2016-06-29 | Lg电子株式会社 | Mobile Terminal And Controlling Method Thereof |
CN104618206A (en) * | 2015-02-09 | 2015-05-13 | 河南九洲计算机有限公司 | Family core accessing device for realizing intelligent community service and system thereof |
CN107423809A (en) * | 2017-07-07 | 2017-12-01 | 北京光年无限科技有限公司 | The multi-modal exchange method of virtual robot and system applied to net cast platform |
CN108106541A (en) * | 2017-12-21 | 2018-06-01 | 浙江大学 | A kind of bridge cable force measuring method based on video image identification |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113778580A (en) * | 2021-07-28 | 2021-12-10 | 赤子城网络技术（北京）有限公司 | Modal user interface display method, electronic device and storage medium |
CN113778580B (en) * | 2021-07-28 | 2023-12-08 | 赤子城网络技术（北京）有限公司 | Modal user interface display method, electronic device and storage medium |
CN117351331A (en) * | 2023-10-24 | 2024-01-05 | 北京云上曲率科技有限公司 | Method and device for adding adapter for large visual model |
Also Published As
Publication number | Publication date |
---|---|
US11935530B2 (en) | 2024-03-19 |
CN110543290B (en) | 2024-03-05 |
US20220051675A1 (en) | 2022-02-17 |
CN117971154A (en) | 2024-05-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6789320B2 (en) | Providing a state machine personal assistant module that can be traced selectively | |
KR102222421B1 (en) | Save metadata related to captured images | |
CN107430626B (en) | The Action query based on speech suggested is provided | |
US11735182B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
KR102043365B1 (en) | Local maintenance of data for voice actions that can be selectively performed offline on a speech recognition electronic device | |
JP2023103313A (en) | Invoking automated assistant functions based on detected gesture and gaze | |
CN112119379B (en) | Transferring an auto-assistant routine between client devices during execution of the routine | |
US11200893B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
US11790004B2 (en) | Systems, methods, and apparatuses for providing assistant deep links to effectuate third-party dialog session transfers | |
US11238868B2 (en) | Initializing non-assistant background actions, via an automated assistant, while accessing a non-assistant application | |
KR102591555B1 (en) | Selective detection of visual cues for automated assistants | |
JP7017643B2 (en) | Text-independent speaker recognition | |
US11935530B2 (en) | Multimodal responses | |
EP3714355B1 (en) | Expanding physical motion gesture lexicon for an automated assistant | |
US20230336521A1 (en) | Sending messages from smart speakers and smart displays via smartphones | |
US11164576B2 (en) | Multimodal responses | |
CN110688011B (en) | Dynamic list construction based on modalities of a multi-modality client device | |
US20230343336A1 (en) | Multi-modal interaction between users, automated assistants, and other computing services |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |