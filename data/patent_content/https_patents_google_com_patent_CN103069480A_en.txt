CN103069480A - Speech and noise models for speech recognition - Google Patents
Speech and noise models for speech recognition Download PDFInfo
- Publication number
- CN103069480A CN103069480A CN2011800263904A CN201180026390A CN103069480A CN 103069480 A CN103069480 A CN 103069480A CN 2011800263904 A CN2011800263904 A CN 2011800263904A CN 201180026390 A CN201180026390 A CN 201180026390A CN 103069480 A CN103069480 A CN 103069480A
- Authority
- CN
- China
- Prior art keywords
- user
- sound signal
- audio
- model
- noise
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/20—Speech recognition techniques specially adapted for robustness in adverse environments, e.g. in noise, of stress induced speech
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
Abstract
An audio signal generated by a device based on audio input from a user may be received. The audio signal may include at least a user audio portion that corresponds to one or more user utterances recorded by the device. A user speech model associated with the user may be accessed and a determination may be made background audio in the audio signal is below a defined threshold. In response to determining that the background audio in the audio signal is below the defined threshold, the accessed user speech model may be adapted based on the audio signal to generate an adapted user speech model that models speech characteristics of the user. Noise compensation may be performed on the received audio signal using the adapted user speech model to generate a filtered audio signal with reduced background audio compared to the received audio signal.
Description
The cross reference of related application
That the application requires is that submit on June 14th, 2010, title is the right of priority of the U. S. application sequence number 12/814,665 of " SPEECH ANDNOISE MODELS FOR SPEECH RECOGNITION ", and its disclosure is incorporated into this by reference.
Technical field
This instructions relates to speech recognition.
Background technology
Speech recognition can be used for voice search query.Usually, search inquiry comprises one or more query term that the user submits to search engine when the user asks search engine to carry out search.In other modes, the user can be by coming the query term of typing search inquiry by oral account query term in the microphone of for example mobile device in keyboard key entry or the situation in voice queries.
When submitting voice queries to by mobile device for example, the microphone of mobile device also may record neighbourhood noise or sound except user's spoken utterance, be known as in other respects " environment audio frequency " or " background audio ".For example, the environment audio frequency can comprise be positioned at around the user other people the background chat or the noise of talking or being generated by nature (for example, barking) or culture (for example, office, airport or highway noise or construction activity).The environment audio frequency may partly cover user's speech, thereby so that automated voice identification (" ASR ") engine is difficult to accurately identify spoken utterance.
Summary of the invention
In one aspect, one or more memory devices that a kind of system comprises one or more treatment facilities and stores instruction, when instruction is carried out by one or more treatment facilities, so that one or more treatment facilities receive the sound signal that is generated based on the audio frequency input from the user by equipment, sound signal comprises at least corresponding to the audio user part by one or more user spoken utterances of equipment records; The user speech model that access is associated with the user; Determine that the background audio in the sound signal is defining below the threshold value; In response to determining background audio in the sound signal below the threshold value of definition, based on the user speech model of the adaptive access of sound signal to generate the adaptive user speech model to the modeling of user speech characteristic; And use adaptive user speech model that the sound signal that receives is carried out noise compensation to generate the filtering audio signals of comparing the background audio with minimizing with the sound signal that receives.
Implementation can comprise one or more following characteristics.For example, sound signal only can comprise corresponding to around the environment audio-frequency unit of user's background audio to determine that background audio in the sound signal is under the definition threshold value, instruction can comprise as giving an order, when being performed, so that one or more treatment facility is determined the amount of the energy in the environment audio-frequency unit; And in definite environment audio-frequency unit the amount of energy under threshold energy.In order to determine that the background audio in the sound signal is defining under the threshold value, instruction comprises as giving an order, when being performed, so that described one or more treatment facility is determined the signal to noise ratio (S/N ratio) of sound signal; And determine that this signal to noise ratio (S/N ratio) is under threshold signal-to-noise ratio.Sound signal only can comprise corresponding to around the environment audio-frequency unit of user's background audio to determine the signal to noise ratio (S/N ratio) of sound signal, instruction comprises as giving an order, when being performed, so that one or more treatment facility is determined the amount of the energy in the audio user part of sound signal; Determine the amount of the energy in the environment audio-frequency unit of sound signal; And determine signal to noise ratio (S/N ratio) by the ratio between the amount of determining the energy in audio user part and the environment audio-frequency unit.
The user speech model of access can comprise the alternate user speech model that not yet is adapted for user's characteristics of speech sounds modeling.Instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility is selected the alternate user speech model; And will substitute speech model and be associated with the user.In order to select the alternate user speech model, instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility is determined user's sex; And among a plurality of alternate user speech models, select the alternate user speech model based on user's sex.In order to select the alternate user speech model, instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility is determined the position of user when the one or more language of record; And among a plurality of alternate user speech models, select the alternate user speech model based on the position record user during one or more language.In order to select the alternate user speech model, instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility is determined user's language or accent; And among a plurality of alternate user speech models, select the alternate user speech model based on language or accent.In order to select the alternate user speech model, instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility receives the initial sound signal that comprises at least corresponding to by the initial user audio-frequency unit of one or more user spoken utterances of equipment records; Similarity measurement between the user's who determines a plurality of alternate user speech models and determine based on described initial sound signal the desired user speech model; And select the alternate user speech model among a plurality of alternate user speech models based on similarity measurement.
Instruction can comprise as giving an order, when being performed, so that the noise model that the access of one or more treatment facility is associated with the user; And wherein in order to carry out noise compensation, instruction may further include as giving an order, and it is so that one or more treatment facility uses adaptive user speech model and access noise model that the sound signal that receives is carried out noise compensation.In order to carry out noise compensation, instruction may further include as giving an order, itself so that one or more treatment facility based on the adaptive access noise model of sound signal that receives to generate the adaptive noise model around the characteristic modeling of user's background audio; And come the sound signal that receives is carried out noise compensation with adaptive user speech model and adaptive noise model.Instruction can comprise as giving an order, when being performed, so that one or more treatment facility receives the second sound signal that comprises at least corresponding to by the second audio user part of one or more user spoken utterances of equipment records; Determine that the background audio in the second sound signal is defining on the threshold value; And in response to determining background audio in the second sound signal on the definition threshold value, based on the adaptive noise model that is associated with the user of the second sound signal to generate the adaptive noise model to the characteristic modeling of the background audio that centers on the user.The access noise model can comprise the alternative noise model that not yet is adapted for around the characteristic modeling of user's background audio.
Instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility selects to substitute noise model; And will substitute noise model and be associated with the user.In order to select to substitute noise model, instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility receives the initial sound signal that comprises at least corresponding to by the initial user audio-frequency unit of one or more user spoken utterances of equipment records; Determine the position record user during corresponding to one or more language of initial user audio-frequency unit; And based on user's position among a plurality of alternative noise models, selecting to substitute noise model at record during corresponding to one or more language of initial user audio-frequency unit.
In order to select to substitute noise model, instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility receives the initial sound signal that comprises at least corresponding to by the initial user audio-frequency unit of one or more user spoken utterances of equipment records; Similarity measurement between the user's who determines a plurality of alternative noise models and determine based on initial sound signal the expectation noise model; And select to substitute noise model among a plurality of alternative noise models based on similarity measurement.In a plurality of alternative noise models each can be to the characteristic modeling of the background audio in the ad-hoc location.In a plurality of alternative noise models each can be to the characteristic modeling of the background audio in the environmental baseline of particular types.
In order to access noise model, instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility is determined the position of user when the one or more language of record; And among a plurality of noise models, select noise model based on user's position.
Sound signal can be corresponding to voice search query, and instruction can comprise as giving an order, when being carried out by one or more treatment facilities, so that one or more treatment facility is carried out the speech recognition of filtering audio signals is transcribed with the one or more candidates that generate one or more user spoken utterances; Use one or more candidates to transcribe and carry out search inquiry to generate Search Results; And to equipment transmission Search Results.
On the other hand, system comprises client device and automated voice recognition system.Client device is configured to send the sound signal comprise at least corresponding to by the audio user part of one or more user spoken utterances of equipment records to the automated voice recognition system.The automated voice recognition system is configured to from the client device received audio signal; The user speech model that access is associated with the user; Determine that the background audio in the sound signal is defining under the threshold value; In response to determining background audio in the sound signal under the definition threshold value, based on the user speech model of the adaptive access of sound signal to generate the adaptive user speech model to user's characteristics of speech sounds modeling; And use adaptive user speech model that the sound signal that receives is carried out noise compensation to generate the filtering audio signals of comparing the background audio with minimizing with the sound signal that receives.
Implementation can comprise following characteristics.For example, the automated voice recognition system can be configured to filtering audio signals execution speech recognition is transcribed with the one or more candidates that generate one or more user spoken utterances.System can comprise search engine system, and it is configured to transcribe to carry out search inquiry to generate Search Results with one or more candidates; And to client device transmission Search Results.
On the other hand, method comprises and receives the sound signal that is generated based on the audio frequency input from the user by equipment, and sound signal comprises at least corresponding to the audio user part by one or more user spoken utterances of equipment records; The user speech model that access is associated with the user; Determine that the background audio in the sound signal is defining below the threshold value; In response to determining that the background audio in the sound signal is limiting below the threshold value, based on the user speech model of the adaptive access of sound signal to generate the adaptive user speech model to user's characteristics of speech sounds modeling; And use adaptive user speech model that the sound signal that receives is carried out noise compensation to generate the filtering audio signals of comparing the background audio with minimizing with the sound signal that receives.
The implementation of described technology can comprise the computer software on hardware, method or process or the computer accessible.
At accompanying drawing with set forth the details of one or more implementation in hereinafter describing.Other features will and become obvious from description, accompanying drawing from claim.
At accompanying drawing with set forth the details of one or more implementation in hereinafter describing.Other potential features, aspect and advantage will become obvious from description, accompanying drawing and claim.
Description of drawings
Fig. 1 is the synoptic diagram of supporting the example system of voice search query.
Fig. 2 is the process flow diagram that the example of process is shown.
Fig. 3 is the process flow diagram that another example of process is shown.
Fig. 4 is swimming lane (swim lane) figure that the example of process is shown.
Embodiment
Fig. 1 shows the synoptic diagram of the example of the system 100 that supports voice search query.System 100 comprises search engine 106 and automatic speech recognition (ASR) engine 108, they are connected with mobile device with one group of mobile device 102a-102c by one or more networks 110 and are connected, such as in some embodiments, described one or more networks 110 are wireless cellular network, WLAN (wireless local area network) (WLAN) or Wi-Fi network, the third generation (3G) mobile telecom network, dedicated network such as Intranet, common network such as the Internet or its any appropriate combination.
Usually, the user of equipment (such as mobile device 104) can be to the microphone oral account search inquiry of mobile device 104.In the application of mobile device 104 operation user's oral account search inquiry is recorded as sound signal, and sends these sound signals as the part of voice search query to ASR engine 108.After the sound signal that receives corresponding to voice search query, ASR engine 108 can translate or be transcribed into one or more text candidates with the user spoken utterances in the sound signal and transcribe, and these candidates can be transcribed as query term and offer search engine 106, thereby the audio search function of support mobile device 104.Query term can comprise one or more complete or part word, character or character strings.
As the example of the operation of system 100, sound signal 138 is included in by network 110 from mobile device 104 to the ASR engine 108 voice search query that send.Sound signal 138 comprises language 140 " Gym New York ".ASR engine 108 receives the voice search query that comprises sound signal 138.One or more text candidates that ASR engine 108 audio signal 138 are mated with the language that generates and detect in sound signal 138 are transcribed or are transcribed 146 through one group of text candidates of rank.For example, the language in the sound signal 138 can produce " Gym New York " and " Jim Newark " transcribes 146 as the candidate.
The one or more candidates that generated by speech recognition system 118 transcribe 146 and are used as search query term and are delivered to search engine 106 from ASR engine 108.Search engine 106 provides search query term 146 to generate one or more Search Results to searching algorithm.Search engine 106 provides last set result 152 (for example, the Uniform Resource Identifier of webpage (URI), image, document, multimedia file etc.) to mobile device 104.
Transcribe for one or more text candidates are translated or be transcribed into to the user spoken utterances in the sound signal, ASR engine 108 comprises the database 111 of noise compensation system 116, speech recognition system 118 and storage noise model 112 and user speech model 114.118 pairs of sound signals of speech recognition system are carried out speech recognition with the user spoken utterances in the identification sound signal and these language are translated into one or more text candidates and transcribe.In some implementation, speech recognition system 118 can generate a plurality of candidates for given language and transcribe.For example, speech recognition system 118 can be transcribed into language a plurality of and can assign with each of language and transcribe the confidence levels that is associated.
In some implementation, the specific change of speech recognition system 118 can be selected for given sound signal based on the additional contextual information relevant with sound signal, and the change of selecting can be used for transcribing the language of sound signal.For example, in some implementation, together with the sound signal that comprises user spoken utterances, voice search query can comprise zone or the language message for the variation of selecting speech recognition system 118.In particular example, the language of the zone of registration of mobile devices 104 or mobile device 104 arranges possible language or the accent of user that language can be provided for ASR engine 108 and be used for determining mobile device 104 for ASR engine 108 therein.The variation of speech recognition system 118 can movement-based equipment 104 user's expection language or accent carry out choice and operation.
For given sound signal, one of noise model 112 of storage removes with one of user speech model or reduces background or environment audio frequency in the sound signal in the noise compensation system 116 usage data storehouses 111.Noise model 112 comprises alternative noise model 120 and adaptive noise model 120b.Similarly, the user speech model comprises alternate user speech model 126a and adaptive user speech model 126b.Usually, adaptive noise model 120b and adaptive user speech model 126b are exclusively used in the specific user and based on being adapted to this user by previous voice search query from the sound signal that this user receives.When not having adaptive noise model or adaptive user speech model for the specific user who submits the current voice search inquiry to, use respectively to substitute noise model 120a and alternate user speech model 126a.
In some instances, the performance of noise compensation system 116 can be improved by using adaptive user speech model, and the specific user's who submits voice search query to concrete sound characteristic had been trained or otherwise adapted to this adaptive user speech model.Yet, adapt to the specific user in order to make speech model, may need the sampling of this user's voice.In the environment such as system 100, those samplings at first may be easily not available.Therefore, in an implementation, if during not for user's adaptive user speech model, ASR 108 selects the alternate user speech models from one or more alternate user speech model 126a when the user sends voice search query at first or for some other reasons.Selected alternate user speech model can be the user speech model that rationally is similar to that is confirmed as user's characteristics of speech sounds.Selected alternate user speech model is used for initial sound signal is carried out noise compensation.Along with user's submission voice search query subsequently, with described those inquire about subsequently some or all sound signals that send be used for selected alternate user speech model training or adapt to be exclusively used in this user adaptive user speech model (namely, characteristics of speech sounds modeling to the user), it is used for the noise compensation of those sound signals subsequently.
For example, in an implementation, when the sound signal that receives subsequently, ASR 108 determines whether environment or background audio are under the specific threshold.If under specific threshold, then this sound signal be used for the alternate user speech model adapted to or further with adaptive user speech model adaptation in the specific user.If background audio is on threshold value, then sound signal is not used in adaptive user speech model (but can be used for adaptive noise model, as mentioned below).
User speech model (no matter being alternate user speech model 126a or adaptive user speech model 126b) for example may be implemented as hidden Markov model (HMM) or gauss hybrid models (GMM).Can use expectation maximization Algorithm for Training or adaptive user speech model otherwise.
In some implementation, the user can be identified clearly.For example, some implementation can be pointed out sign to the user before accepting search inquiry.Other implementations can be used other available information implicit identification users, such as the pattern of keying in the user or user's Move Mode when accelerator forming device a part of (for example, when).When the user can specifically be identified, adaptive user speech model can carry out index by the user identifier corresponding to identifying user.
In other implementations, the user may not specifically be identified.In the case, the equipment (such as mobile device 104) that is used for the typing voice search query can be used as the identifier of particular user, and can be based on the device identifier index adaptive user speech model corresponding with the equipment that is used for the submission voice search query.Usually only exist in single or major equipment user's the environment therein, for example when mobile phone was used as input equipment, adaptive user speech model can provide acceptable speech model to reach the performance constraints of forcing on noise compensation system 116 (particularly) or the ASR 108 (more general) take equipment as foundational development.
Can improve the same procedure of the performance of noise compensation system 116 by adaptive user speech model, the performance of noise compensation system 116 can also be modified by using the noise model that trained or otherwise adapt to usually around user's environment audio frequency.As speech sample, in the environment such as system 100, the sampling that usually centers on user's environment audio frequency at first may be easily not available.Therefore, in an implementation, if during not for user's adaptive user speech model, ASR 108 selects to substitute noise model from one or more alternative noise model 126b when the user sends voice search query at first or for some other reasons.Selected alternative noise model can be based on the noise model that rationally is similar to that known or definite information is determined to be in user's expectation environment audio frequency on every side.Selected alternative noise model is used for initial sound signal is carried out noise compensation.Along with user's submission voice search query subsequently, some or all sound signals that send with those inquiries be used for selected alternative noise model adapt to be exclusively used in this user adaptive noise model (namely, when submitting search inquiry to the characteristic modeling around user's typical environment sound), it is used for the noise compensation of those sound signals subsequently.
For example, in an implementation, when the sound signal that receives subsequently, ASR 108 determines whether environment or background audio are under the specific threshold.If not under specific threshold, then noise model adapts to this sound signal or further adaptive noise model is adapted to the specific user for substituting.In some implementation, no matter on specific threshold, the sound signal of reception may be used to adaptive alternative noise model or adaptive noise model to background audio.
In some implementation; sampling and this sampling in order to ensure the environment audio frequency that obtains not have user spoken utterances can be used for adaptive noise model, and the voice search query on the mobile device 104 is used and can be begun to record before the user says search inquiry and/or can finish the user and continue record after saying search inquiry.For example, voice search query use can be captured in the user say before the search inquiry and/or two seconds afterwards audio frequency to guarantee to obtain the sampling of environment audio frequency.
In some implementation, single alternative noise model can be selected and adapt to strides the single adaptive noise model for this user that the user uses the varying environment that voice search uses.Yet in other were realized, when using voice search to use, adaptive noise model can be developed for the various positions that the user often goes.For example, can develop different noise models and be stored as alternative noise model 120a for diverse location.User's position can send to ASR 108 by mobile device 104 when submitting voice search query to, and perhaps user's position can be determined by other means when submitting voice search query to.When the initial sound signal that receives for given position, then can select the alternative noise model for this position, and when receiving other voice search query from this position, the sound signal that is associated can be used for adaptive this particular noise model.This can occur for each position in the residing diverse location of user when carrying out voice search query, produces thus a plurality of adaptive noise model for the user, and wherein each model is exclusively used in certain position.After the non-usage time interval of definition (for example, the user does not carry out voice search in this position special time), can the delete position particular noise model.
User's position, the position that is associated with given noise model and the position that is associated with given speech model all can define by various granularity ranks when submitting voice search query to, longitude and latitude navigation coordinate or closely defined the zone of (for example, 1/4th miles or less) by navigation coordinate the most specifically.Alternatively, the position can use realm identifier to provide, identifier (for example, " cell/region ABC 123 ") such as state name or identifier, city name, trivial name (for example, " Central Park "), country name or any defined range.In some implementation, the position can locative type, such as in some examples seabeach, big city, amusement park, the mobile traffic, on the ship, in the buildings, open air, countryside, underground position (for example, subway, parking lot etc.), in the street in position, high building (skyscraper) inside or the forest, rather than geographical assigned address.Granularity rank and the customer location when submitting voice search query to, the position that is associated with given noise model and with position that given speech model is associated between can be identical or different.
Noise model (no matter being to substitute 120a or adaptive 120b) for example may be implemented as hidden Markov model (HMM) or gauss hybrid models (GMM).The user speech model can use expectation maximization Algorithm for Training or otherwise adaptive.
As indicated above, in some implementation, the user can specifically be identified and in other implementations equipment can be used as substituting of user.Therefore, be similar to the index to speech model, adaptive noise model can carry out index by user's the user identifier corresponding to sign when the user can specifically be identified, perhaps can be by based on the device identifier index corresponding to the equipment that is used for the submission voice search query when the user can't specifically be identified.
Fig. 2 shows can be at the process flow diagram of the example of the process 200 of carrying out when user or equipment receive initial voice search query, and Fig. 3 shows the process flow diagram of the example of the process 300 that can carry out when the voice search query that receives from user or equipment subsequently.Hereinafter the component description with system 100 is implementation 200 and process 300, but other assemblies of system 100 or another system also can implementation 200 or process 300.
With reference to figure 2, ASR 108 receives initial voice search query (202) from equipment (such as mobile device 104).Initial voice search query can be initial, because this voice search query is first voice search query for particular user or equipment reception; Because this voice search query is first that receives from the ad-hoc location of submitting this voice search query to; Perhaps (or both) do not exist for user or equipment for some other reasons (for example, deleted because this model does not use in special time period) because adaptive user speech model or adaptive noise model.
Voice search query comprises sound signal, and this sound signal comprises audio user signal and environmental audio signal.The audio user signal comprises by the user to be given an oral account to one or more language of the microphone of mobile device 104 and potential environment audio frequency.Environmental audio signal only comprises the environment audio frequency.As mentioned below, voice search query can also comprise contextual information.
When being used, ASR 108 access are about the contextual information (204) of voice search query.This contextual information for example can provide the indication about the condition of the sound signal in the voice search query.This contextual information can comprise temporal information, date and time information, quote the data, other device senses device data, device status data (for example, bluetooth headset, speaker-phone or traditional input method) of the speed measured by specific mobile device or amount of movement the information of the user identifier when if user selection provides or sign mobile device type or model during recording.
This contextual information can also be included in its position of submitting voice search query to.This position for example can be determined by user's schedule, from user preference (for example, be stored in the user account of ASR engine 108 or search engine 106) or the default location derivation, based on the past position (for example, by the equipment that is used for submit Query (for example, mobile device 104) proximal most position that GPS (GPS) module is calculated), when submitting voice queries to, provide by the user is explicit, determine from language, based on the launching tower trigonometric calculations, provide (for example, voice search is used and can be accessed GPS equipment to determine the position and to send this position with voice search query) by the GPS module in the mobile device 104, perhaps use dead reckoning to estimate.If sent by equipment, then positional information can comprise the accuracy information of the levels of precision of indicating this positional information.
ASR 108 can use this type of contextual information to help speech recognition, for example by selecting the particular variant of speech recognition system with contextual information or selecting suitable alternate user speech model or alternative noise model.ASR 108 can be delivered to this type of contextual information search engine 106 to improve Search Results.Some or all contextual informations can receive with voice search query.
If the adaptive user speech model for the user does not exist, then ASR 108 selects initial or alternate user speech model and this initial user speech model is associated with user or equipment (for example, depending on whether the user can specifically be identified) (206).For example, as indicated above, ASR 108 can select in some available alternate user speech models.
Selected alternate user speech model can be based on the rationally approximate user speech model that known or definite information is confirmed as user's characteristics of speech sounds, although alternate user speech model that should be selected is not yet adaptive by any sampling institute with user's voice.For example, in an implementation, can have two alternate user speech models: one be used for male sex's speech and one be used for women's speech.User's sex can be determined and suitable alternate user speech model (sex) can be selected based on user's possible sex.User's sex for example can by analyze the sound signal that receives with initial voice search query or based on for example by the user voluntarily the information in the information in that submit to and the profile that be included in the user determine.
Additionally or alternatively, the adaptive user speech model for other users (such as the user of mobile device 102a-102c) can be used as the alternate user speech model.When receiving initial voice search query, expression can be determined based on inquiring about the initial sound signal that comprises with initial searches for the measuring similarity of the similarity between the user's who submits the initial searches inquiry to the adaptive user speech model (corresponding to other users) of expectational model in being stored in database 111.For example, if the linear regression technique of model Constraint-based maximum likelihood, then measuring similarity can be the L2 norm (for the summation of the difference of two squares of each coefficient) of the difference between the model.Use therein in the situation of GMM technology, measuring similarity can be two Kullback-Leibler entropys between the probability density function, if perhaps model is GMM and be spatial point from the expectational model of single language, then may be that the probability density of GMM is positioned at this spatial point.In using other implementations of GMM, measuring similarity for example can be the distance between each GMM average, or by the normalized distance between average of some norm of covariance matrix.
Can be selected as alternate user speech model for the user who submits initial voice search query near the adaptive user speech model of user's expectational model (as by shown in the measuring similarity).For example, when the user of equipment 104 submitted initial voice search query to, ASR 108 can determine to represent for the user's of equipment 104 desired user speech model and for the measuring similarity of the similarity between the user's of equipment 102a the adaptive user speech model.Similarly, ASR 108 can determine to represent for the user's of equipment 104 desired user speech model and for the measuring similarity of the similarity between the user's of equipment 102b the adaptive user speech model.If the measuring similarity pointer more is similar to model for the user of equipment 102a to the user's of equipment 104 expectational model than the model for the user of equipment 102b, then can be used as alternate user speech model for the user of equipment 104 for the user's of equipment 102a model.
As the particular example of the implementation that adopts GMM, voice search query can comprise the language that comprises voice and ambient signal.This inquiry can be segmented into for example segmentation of 25ms, and wherein each segmentation is voice or pure environment.For each segmentation, the calculated characteristics vector x
t, wherein the vector corresponding to voice is designated as x
sFor each the potential alternative model M that in database, has
i, calculate each vectorial likelihood score:
This is that likelihood score calculating and the p (i) of GMM are the priori of this alternative model.Suppose the independence of observation, speech vector x
sThe probability of set can be expressed as:
X wherein
sIt is the set of speech vector.
Given observation x
sThe conditional probability of class i be:
p(i|x
s)＝p(x
s，i)/p(x
s)
Wherein
This conditional probability can be used as current language and certain alternative speech model M
iBetween measuring similarity.
Having, the alternative model of high conditional probability can be selected:
model
index＝ArgMax(p(i|x
s))i
Contextual information (such as the language of user's accent or expectation) can be used alone or is used in combination to select the alternate user speech model with other technologies mentioned above.For example, a plurality of alternate user speech models can be stored for different language and/or accent.User's position can be used for determining for ASR 108 language or the accent of expectation when submitting voice search query to, and can be selected corresponding to the alternate user speech model of expectation language and/or accent.Similarly, can be stored in for user's language and/or positional information in user's for example the profile, and be used for selecting corresponding to user's language and/or the alternate user speech model of accent.
If adaptive user speech model (for example is saved as, because voice search query is for the original position of ad-hoc location but is not for user or equipment), then move and 206 can be skipped, perhaps can be by other adaptive the substituting with adaptive user speech model.For example, the sound signal that receives by initial voice search query can be evaluated to determine that background audio is whether under specific threshold, if and under specific threshold, then this sound signal can be used to further training or by other means adaptive this adaptive user speech model.
ASR 108 selects initial or alternative noise models and should initial noise model be associated with user or equipment (for example, depending on whether the user can specifically be identified) (208).Selected alternative noise model can be based on known or definite information and be confirmed as around the noise model that rationally is similar to of user's expectation environment audio frequency.For example, alternative noise model can for the environmental baseline of various criterion kind (such as, in automobile, on the airport, be in or in bar/dining room) develop.Data from other users in the system can be used to develop alternative noise model.For example, if some duration of low noise data (for example, 10 minutes) is collected from the user, then these data can be used to generate alternative model.When receiving initial sound signal, expression expectation noise model and standard substitute the measuring similarity of the similarity between the noise model and can determine based on initial sound signal, and this standard substitutes one of noise model can select based on this measuring similarity (for example, use and be similar to above about selecting the described technology of alternate user model).For example, the expectation noise model can be determined based on environmental audio signal.(for example exceed specific dissimilar threshold value, determine based on KL distance) alternative noise model (for example, 100) set can be retained as the standard alternative model, and employed alternative model can use measuring similarity as described to select from this set.When selecting to substitute noise model, this can minimization calculation.
Additionally or alternatively, different noise models can develop and be stored as for diverse location and substitute noise model 120a.For example, the noise model for position A 132a and position B 132b can be developed and be stored as alternative noise model 120a.Noise model for particular location can be based on being developed by the previous voice search query of other Client-initiateds in those positions.For example can develop based on being received as from the sound signal 130b of the part of the user's of equipment 102b voice search query with at position B 132b the time to be received as the sound signal 130c from the part of the user's of equipment 102c voice search query by ASR 108 by ASR 108 when the B 132b of position for the noise model of position B 132b.For the noise model of position A 132a for example can based at position A by ASR
108 reception conducts are developed from the sound signal 130a of the part of the user's of equipment 102a voice search query.
When receiving initial sound signal, alternative noise model can be selected based on user's position.For example, when the user of mobile device 104 submitted initial voice search to from position B 132b, ASR 108 can select the alternative noise model for position B.In some implementation, the voice search on the mobile device 104 is used the GPS that can access on this mobile device and is sent positional informations with definite user's position and with voice search query to ASR 108.Positional information can be used with based on the suitable alternative noise model of this location positioning for ASR 108 then.In other implementations, when receiving initial sound signal, the measuring similarity of similarity can be determined based on this initial sound signal between the distinctive alternative noise model in position of having stored in expression expectation noise model and the database 111, and one of this distinctive alternative noise model in position can be selected based on this measuring similarity.
Use initial (or adaptive) user speech model and initial noise model, the sound signals that 116 pairs of the noise compensation systems of ASR 108 receive with voice search query are carried out noise compensation to remove or to reduce background audio in the sound signal, produce thus filtering audio signals (210).For example, at for example ALGONQUIN:Iterating Laplace ' s Methodto Remove Multiple Types of Acoustic Distortion for Robust Speech Recognition, the algorithm such as the Algonquin algorithm of describing among the Eurospeech 2001-Scandinavia can be used for carrying out noise compensation with initial user speech model and initial noise model.
Speech recognition system is carried out speech recognition to filtering audio signals and is transcribed (210) so that the language in the sound signal is transcribed into one or more candidates.Search inquiry can use one or more candidates to transcribe execution.In some implementation, ASR 108 can select particular variant for the speech recognition system of carrying out speech recognition with contextual information.For example, user's accent and/or expectation or known language can be used for selecting suitable speech recognition system.User's position can be used for determining user's expectation language when submitting voice search query to, and perhaps user's language can be included in this user's the profile.
With reference to figure 3, ASR 108 is from equipment (such as mobile device 104) reception voice search query (302) subsequently.This voice search query subsequently can be subsequently, this is because this voice search query receives after for the previous voice search query of particular user or equipment, perhaps because there be substituting or adaptive user speech model or noise model for user or equipment.
Voice search query subsequently comprises sound signal, and this sound signal comprises audio user signal and environmental audio signal.The audio user signal comprises by the user gives an oral account one or more language to the microphone of mobile device 104 and potential environment audio frequency.Environmental audio signal only comprises the environment audio frequency.As mentioned below, voice search query can also comprise contextual information.
When being used, ASR 108 access are about the contextual information (304) of voice search query.ASR 108 can use this type of contextual information to help speech recognition, for example, and by select the particular variant of speech recognition system with this contextual information.Additionally or alternatively, contextual information can be used for helping substituting or the selection of adaptive user speech model and/or adaptive or alternative noise model and/or adaptive.ASR 108 can transmit this type of contextual information to improve Search Results to search engine 106.Some or all contextual informations can receive with voice search query.
If the environment audio frequency in the sound signal that voice search query receives not under the definition threshold value, then uses adaptive (or adaptive) noise model that substitutes of this sound signal to generate adaptive noise model (312).In some implementation, treat that adaptive particular noise model selects based on user's position.For example, when different noise models are used for the user from its frequent diverse location of submitting voice search query to, ASR 108 can the user or the position of equipment to select substituting or adaptive noise model for this position.
Noise model can be adaptive on whole sound signal, and perhaps environmental audio signal can be extracted and be used for adaptive noise model, depends on that the specific implementation mode of noise model and voice strengthen or Speech separation algorithm.Technology such as hidden Markov model or gauss hybrid models can be used for realizing the user speech model, and the technology such as expectation maximization can be used for adaptive user speech model.
If the environment audio frequency in the sound signal that voice search query receives is under the definition threshold value, then this sound signal is used for alternate user speech model (before not yet having adapted to adaptive user speech model if should substitute) or the adaptive user speech model (308) of adaptive previous selection.The user speech model can be adaptive on whole sound signal, and perhaps the audio user signal can be extracted and be used for adaptive user speech model, depends on the specific implementation mode of user speech model.Be similar to noise model, technology such as hidden Markov model or gauss hybrid models can be used for realize the user speech model, and the technology such as expectation maximization or maximum a posteriori (MAP) are adaptive can be used for adaptive user speech model.
In some implementation, ASR 108 is also based on sound signal training or otherwise adaptive alternative noise model or the adaptive noise model (310) under threshold value of background audio wherein.Although in some implementation, the user speech model only uses sound signal training under the definition threshold value of background audio wherein or adaptive, but in some instances, noise model can be based on this type of sound signal and the wherein sound signal training of background audio on threshold value or adaptive, and this depends on for the particular technology of realizing noise model.For example, some noise model can comprise reflect the environment of background audio under threshold value wherein aspect parameter, and therefore this class model can be benefited from the sound signal of adaptive wherein background audio under threshold value.
Use and substitute or adaptive user speech model (depending on whether substitute speech model is adapted) and alternative or adaptive noise model (depending on whether substitute noise model is adapted), the sound signal that the noise compensation system 116 of ASR 108 receives with voice search query in identical mode as indicated above pair is carried out noise compensation removing or to reduce background audio in the sound signal, thereby produces filtering audio signals (314).Speech recognition system is carried out speech recognition in identical mode as indicated above to filtering audio signals and is transcribed (316) so that the speech in the sound signal is transcribed into one or more candidates.
Although process 300 illustrates adaptive noise model and/or user speech model before being used for noise compensation, but adaptive can the generation after carrying out noise compensation, and noise compensation can be based on noise and/or further adaptive noise and/or the user speech model before of user speech model quilt.This can be following situation, for example, and when adaptive when being computation-intensive.In the case, to Expected Response time of voice search query can by use for the current noise of noise compensation and user speech model and based on after new sound signal to its realization of more newly arriving.
Fig. 4 shows the swimming lane figure of the example of the process 400 of carrying out by mobile device 104, ASR 108 with for the treatment of the search engine 106 of voice search query.Mobile device 104 sends voice search query (402) to ASR 108.As indicated above, voice search query comprises the sound signal that comprises environmental audio signal and audio user signal, environmental audio signal comprises the environment audio frequency with user spoken utterances, and the audio user signal comprises user spoken utterances (and potentially environment audio frequency).Voice search query can also comprise contextual information, all contextual informations as indicated above.
Do not exist in the event of adaptive user speech model for user or equipment, ASR 108 for example selects alternate user speech model (404) with technology mentioned above.Similarly, if do not have adaptive noise model for user or equipment, perhaps at least not for the ad-hoc location of user when submitting voice search query to, then ASR 108 for example selects to substitute noise model with technology mentioned above.
404 pairs of filtering audio signals of ASR engine are carried out speech recognition 416 and are transcribed (412) so that the one or more language in the sound signal are transcribed into text candidates.ASR engine 404 is transmitted transcribe (414) of 418 generations to search engine 406.If ASR engine 404 generates a plurality of transcribing, then can transcribe ordering take degree of confidence as ordered pair alternatively.ASR engine 404 can provide context data to search engine 406 alternatively, and such as the geographic position, search engine 406 can use this context data that Search Results is filtered or sort.
A plurality of implementations have been described.Yet, will understand, can carry out various modifications and do not break away from Spirit Essence and the scope of disclosure.For example, above technology is described about the sound signal in the voice search query is carried out speech recognition, and this technology can be used for other system, such as the computerize speech dictation system or the conversational system that realize at mobile or other equipment.In addition, the various forms of flow process shown in can when rearrangement, interpolation or removal step, using above.Thereby other implementations within the scope of the appended claims.
The embodiment and all functions operation that realize describing in this instructions in can be in Fundamental Digital Circuit or in the computer software, firmware or the hardware that comprise disclosed in this manual structure and structural equivalents thereof or in them one or the multinomial combination.Embodiment may be implemented as one or more computer program, is namely carried out by data processing equipment or one or more module of the computer program instructions of the operation of control data processing equipment in computer-readable medium being used for of encoding.Computer-readable medium can be that the material of machine readable storage device, machine readable storage substrate, memory devices, realization machine readable transmitting signal forms or one or multinomial combination in them.All devices, equipment and the machine for the treatment of data contained in term " data processing equipment ", for example comprises a programmable processor, computing machine or a plurality of processor or computing machine.Device can also be included as discussion except comprising hardware computer program creates the code of execution environment, for example consists of the code of processor firmware, protocol stack, data base management system (DBMS), operating system or in them or multinomial combination.Transmitting signal is the signal that artificially generates, and for example, by electricity, optics or the electromagnetic signal that machine generates, this signal is generated to be used for that information is encoded to be used for to suitable acceptor device transmission.
Can write computer program (being also referred to as program, software, software application, script or code) with any type of programming language that comprises compiling or interpretative code, and can dispose it with any form, comprise as stand-alone program or as the module, parts, subroutine or other unit that are suitable in computing environment, using.Computer program is not necessarily corresponding to the file in the file system.Program (for example can be stored in the part of the file that keeps other program or data, be stored in one or more script in the marking language document), in the Single document of the program that is exclusively used in discussion or in a plurality of coordinated files (for example, storing the file of one or more module, subroutine or code section).Computer program can be deployed on a computing machine or be positioned at the three unities or be distributed in a plurality of places and carried out by a plurality of computing machines of interconnection of telecommunication network.
The process of describing in this manual and logic flow can be carried out by one or more programmable processor, and this processor is carried out one or more computer program with by to the input data manipulation and generate output and carry out function.Process and logic flow also can by dedicated logic circuit for example FPGA (field programmable gate array) or ASIC (special IC) carry out, and device also can be implemented as this dedicated logic circuit.
The processor that is suitable for computer program for example comprises any one or a plurality of processor of the digital machine of general and special microprocessor and any kind.Generally speaking, processor will be from ROM (read-only memory) or random access memory or these two reception instruction and data.The elementary cell of computing machine is for the processor of carrying out instruction and for one or more memory devices of storing instruction and data.Generally speaking, computing machine also will comprise for one or more mass memory unit (for example, disk, photomagneto disk or CD) of storage data or operatively be coupled into from this mass memory unit receive data or to this mass memory unit and transmit data or these two.Yet computing machine need not to have such equipment.In addition, computing machine can be embedded in another equipment, only lifts numerical example, and this another equipment for example is flat computer, mobile phone, personal digital assistant (PDA), Mobile audio player, GPS (GPS) receiver.The computer-readable medium that is suitable for storing computer program instructions and data comprises nonvolatile memory, medium and the memory devices of form of ownership, for example comprises semiconductor memory devices (for example, EPROM, EEPROM and flash memory device); Disk (for example, internal hard drive or removable disk); Magneto-optic disk; And CD ROM and DVD-ROM dish.Processor and storer can or be incorporated in the dedicated logic circuit by supplemented.
For mutual with the user is provided, embodiment can be limited on the computing machine in fact, this computing machine for the display apparatus that shows information to the user (for example has, CRT (cathode-ray tube (CRT)) or LCD (liquid crystal display) monitor) and the user can be used for providing to computing machine keyboard and the pointing apparatus (for example, mouse or tracking ball) of input.The equipment of other kind also can be used to provide mutual with the user; For example, the feedback that provides to the user can be any type of sensory feedback (for example, visual feedback, audio feedback or tactile feedback); And can receive input from the user with any form that comprises sound, voice or sense of touch input.
Embodiment can be implemented in the computing system, this computing system comprises that back-end component (for example, as data server) or comprise that middleware component is (for example, application server) or comprise any combination of one or more parts in front end component (for example, having the user can be used for carrying out mutual graphic user interface or the client computer of Web browser with implementation) or such rear end, middleware or the front end component.The parts of system can be by any digital data communication form or medium (for example, communication network) interconnection.The example of communication network comprises LAN (Local Area Network) (" LAN ") and wide area network (" WAN "), for example, and the Internet.
Computing system can comprise client and server.Client and server general mutual away from and usually mutual by communication network.The computer program that concerns of client and server occurs, and these computer programs are in the corresponding computer operation and mutually have the client-server relation.
Although this instructions comprises many details, these should not be construed as the restriction to the scope of scope of the disclosure or content that can be claimed, and should be as the description that specific implementation is realized distinctive feature.Some feature that also can in single this instructions of embodiment combination enforcement, in the context of independent embodiment, describe.Otherwise, also can be in a plurality of embodiments separately or in any suitable sub-portfolio, be implemented in the various features of describing in the context of single embodiment.In addition; although above can describe feature as in some embodiments effect and even originally claimed like this; but can from claimed combination, remove in some cases one or more feature from this combination, and claimed combination can relate to the variant of sub-portfolio or sub-portfolio.
Similarly, although describe operation with particular order in the accompanying drawings, this should not be construed as require with shown in particular order or carry out such operation or carry out the result of operation shown in all to realize wishing with sequence order.In some circumstances, multitask and parallel processing can be favourable.In addition, the various system units of separation should not be construed as and require such separation in all embodiments in above-described embodiment, and should be appreciated that the program element of description and system generally can be integrated in together in the single software product or be encapsulated in a plurality of software products.
Mention therein in each example of html file, can replace with other file type or form.For example, html file can replace with the file of XML, JSON, plaintext or other type.In addition, when mentioning table or hash table, can use other data structure (such as spreadsheet, relational database or structured document).
Therefore, particular implementation has been described.Other embodiment within the scope of the appended claims.For example, the result of hope can carry out and still obtain by different order to the action of putting down in writing in the claims.
Claims (24)
1. system comprises:
One or more treatment facilities; And
One or more memory devices, it stores instruction, when described instruction is carried out by described one or more treatment facilities, so that described one or more treatment facility:
The sound signal that reception is generated based on the input from user's audio frequency by equipment, described sound signal comprise at least corresponding to the audio user part by one or more user spoken utterances of described equipment records;
The user speech model that access is associated with described user;
Determine that the background audio in the described sound signal is defining below the threshold value;
In response to determining described background audio in the described sound signal below described definition threshold value, based on the user speech model of the adaptive access of described sound signal to generate the adaptive user speech model to described user's characteristics of speech sounds modeling; And
Use described adaptive user speech model that the sound signal that receives is carried out noise compensation to generate the filtering audio signals of comparing the background audio with minimizing with the sound signal of described reception.
2. system according to claim 1, wherein said sound signal only comprises corresponding to the environment audio-frequency unit around described user's background audio, and in order to determine that the described background audio in the described sound signal is defining under the threshold value, described instruction comprises as giving an order, when it is performed, so that described one or more treatment facility:
Determine the amount of the energy in the described environment audio-frequency unit; And
Determine that the amount of the described energy in the described environment audio-frequency unit is under threshold energy.
3. system according to claim 2 is defining under the threshold value in order to determine the described background audio in the described sound signal, and described instruction comprises as giving an order, when it is performed, so that described one or more treatment facility:
Determine the signal to noise ratio (S/N ratio) of described sound signal; And
Determine that described signal to noise ratio (S/N ratio) is under threshold signal-to-noise ratio.
4. system according to claim 4, wherein said sound signal only comprises corresponding to the environment audio-frequency unit around described user's background audio, and in order to determine the described signal to noise ratio (S/N ratio) of described sound signal, described instruction comprises as giving an order, when it is performed, so that described one or more treatment facility:
Determine the amount of the energy in the described audio user part of described sound signal;
Determine the amount of the energy in the described environment audio-frequency unit of described sound signal; And
Determine described signal to noise ratio (S/N ratio) by the ratio between the amount of determining the energy in described audio user part and the described environment audio-frequency unit.
5. system according to claim 1, wherein the user speech model of access comprises the alternate user speech model that is not adapted to be described user's described characteristics of speech sounds modeling.
6. system according to claim 5, wherein said instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Select described alternate user speech model; And
Carry out related with described user described alternative speech model.
7. system according to claim 6, wherein in order to select described alternate user speech model, described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Determine described user's sex; And
Described sex based on described user is selected described alternate user speech model among a plurality of alternate user speech models.
8. system according to claim 6, wherein in order to select described alternate user speech model, described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Determine the position of described user when the described one or more language of record; And
Described alternate user speech model is selected in described position based on described user when recording described one or more language among a plurality of alternate user speech models.
9. system according to claim 6, in order to select described alternate user speech model, described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Determine described user's language or accent; And
Among a plurality of alternate user speech models, select described alternate user speech model based on described language or accent.
10. system according to claim 6, wherein in order to select described alternate user speech model, described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Receive the initial sound signal comprise at least corresponding to by the initial user audio-frequency unit of one or more user spoken utterances of described equipment records;
Similarity measurement between the described user's who determines a plurality of alternate user speech models and determine based on described initial sound signal the desired user speech model; And
Among described a plurality of alternate user speech models, select described alternate user speech model based on described similarity measurement.
11. system according to claim 1, wherein said instruction comprises as giving an order, when it is performed, so that described one or more treatment facility:
The noise model that access is associated with described user; And
Wherein in order to carry out noise compensation, described instruction comprises that further it is so that described one or more treatment facility uses described adaptive user speech model and access noise model that the sound signal that receives is carried out noise compensation as giving an order.
12. system according to claim 11, wherein in order to carry out noise compensation, described instruction comprises that further it is so that described one or more treatment facility as giving an order:
Based on the adaptive access noise model of sound signal that receives to generate the adaptive noise model around the characteristic modeling of described user's background audio; And
Come the sound signal that receives is carried out noise compensation with described adaptive user speech model and described adaptive noise model.
13. system according to claim 11, wherein said instruction comprises as giving an order, when it is performed, so that described one or more treatment facility:
Receive the second sound signal comprise at least corresponding to by the second audio user part of one or more user spoken utterances of described equipment records;
Determine that the background audio in described the second sound signal is defining on the threshold value; And
In response to determining described background audio in described the second sound signal on described definition threshold value, the described noise model that is associated based on adaptive and the described user of described the second sound signal is to generate the adaptive noise model to the characteristic modeling of the background audio that centers on described user.
14. system according to claim 11, wherein said access noise model comprises the alternative noise model that not yet is adapted to be around the characteristic modeling of described user's background audio.
15. system according to claim 14, wherein said instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Select described alternative noise model; And
Carry out related with described user described alternative noise model.
16. system according to claim 15, wherein in order to select described alternative noise model, described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Receive the initial sound signal comprise at least corresponding to by the initial user audio-frequency unit of one or more user spoken utterances of described equipment records;
Determine the position record described user during corresponding to described one or more language of described initial user audio-frequency unit; And
Based on described user's described position among a plurality of alternative noise models, selecting described alternative noise model at record during corresponding to described one or more language of described initial user audio-frequency unit.
17. system according to claim 15, wherein in order to select described alternative noise model, described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Receive the initial sound signal comprise at least corresponding to by the initial user audio-frequency unit of one or more user spoken utterances of described equipment records;
Similarity measurement between the described user's who determines a plurality of alternative noise models and determine based on described initial sound signal the expectation noise model; And
Among described a plurality of alternative noise models, select described alternative noise model based on described similarity measurement.
18. system according to claim 17, each the alternative noise model in wherein said a plurality of alternative noise models is to the characteristic modeling of the background audio in the ad-hoc location.
19. system according to claim 17, each the alternative noise model in wherein said a plurality of alternative noise models is to the characteristic modeling of the background audio in the environmental baseline of particular types.
20. system according to claim 11, wherein in order to access described noise model, described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Determine the position of described user when the described one or more language of record; And
Described noise model is selected in described position based on described user among a plurality of noise models.
21. system according to claim 1, wherein said sound signal be corresponding to voice search query, and described instruction comprises as giving an order, when it is carried out by described one or more treatment facilities, so that described one or more treatment facility:
Described filtering audio signals is carried out speech recognition transcribes with the one or more candidates that generate described one or more user spoken utterances;
Use described one or more candidate to transcribe and carry out search inquiry to generate Search Results; And
Send described Search Results to described equipment.
22. a system comprises:
Client device, it is configured to send the sound signal comprise at least corresponding to by the audio user part of one or more user spoken utterances of described equipment records to the automated voice recognition system;
The automated voice recognition system, it is configured to:
Receive described sound signal from described client device;
The user speech model that access is associated with described user;
Determine that the background audio in the described sound signal is defining under the threshold value;
In response to determining described background audio in the described sound signal under described definition threshold value, based on the user speech model of the adaptive access of described sound signal to generate the adaptive user speech model to described user's characteristics of speech sounds modeling; And
Use described adaptive user speech model that the sound signal that receives is carried out noise compensation to generate the filtering audio signals of comparing the background audio with minimizing with the sound signal of described reception.
23. system according to claim 22, wherein said automated voice recognition system is configured to that further described filtering audio signals is carried out speech recognition and transcribes with the one or more candidates that generate described one or more user spoken utterances, and described system further comprises:
Search engine system, it is configured to:
Use described one or more candidate to transcribe and carry out search inquiry to generate Search Results; And
Send described Search Results to described client device.
24. a method comprises:
The sound signal that reception is generated based on the input from user's audio frequency by equipment, described sound signal comprise at least corresponding to the audio user part by one or more user spoken utterances of described equipment records;
The user speech model that access is associated with described user;
Determine that the background audio in the described sound signal is defining below the threshold value;
In response to determining described background audio in the described sound signal below the definition threshold value, based on the user speech model of the adaptive access of described sound signal to generate the adaptive user speech model to described user's characteristics of speech sounds modeling; And
Use described adaptive user speech model that the sound signal of described reception is carried out noise compensation to generate the filtering audio signals of comparing the background audio with minimizing with the sound signal that receives.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/814,665 | 2010-06-14 | ||
US12/814,665 US8234111B2 (en) | 2010-06-14 | 2010-06-14 | Speech and noise models for speech recognition |
PCT/US2011/040225 WO2011159628A1 (en) | 2010-06-14 | 2011-06-13 | Speech and noise models for speech recognition |
Publications (2)
Publication Number | Publication Date |
---|---|
CN103069480A true CN103069480A (en) | 2013-04-24 |
CN103069480B CN103069480B (en) | 2014-12-24 |
Family
ID=44303537
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201180026390.4A Active CN103069480B (en) | 2010-06-14 | 2011-06-13 | Speech and noise models for speech recognition |
Country Status (5)
Country | Link |
---|---|
US (3) | US8234111B2 (en) |
EP (1) | EP2580751B1 (en) |
CN (1) | CN103069480B (en) |
AU (1) | AU2011267982B2 (en) |
WO (1) | WO2011159628A1 (en) |
Cited By (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105580071A (en) * | 2013-05-06 | 2016-05-11 | 谷歌技术控股有限责任公司 | Method and apparatus for training a voice recognition model database |
CN105719645A (en) * | 2014-12-17 | 2016-06-29 | 现代自动车株式会社 | Speech recognition apparatus, vehicle including the same, and method of controlling the same |
CN106663446A (en) * | 2014-07-02 | 2017-05-10 | 微软技术许可有限责任公司 | User environment aware acoustic noise reduction |
CN107004406A (en) * | 2014-12-02 | 2017-08-01 | 索尼公司 | Message processing device, information processing method and program |
CN108182270A (en) * | 2018-01-17 | 2018-06-19 | 广东小天才科技有限公司 | Search for content transmission and searching method, smart pen, search terminal and storage medium |
CN109087659A (en) * | 2018-08-03 | 2018-12-25 | 三星电子（中国）研发中心 | Audio optimization method and apparatus |
CN110648680A (en) * | 2019-09-23 | 2020-01-03 | 腾讯科技（深圳）有限公司 | Voice data processing method and device, electronic equipment and readable storage medium |
CN110956955A (en) * | 2019-12-10 | 2020-04-03 | 苏州思必驰信息科技有限公司 | Voice interaction method and device |
CN111415653A (en) * | 2018-12-18 | 2020-07-14 | 百度在线网络技术（北京）有限公司 | Method and apparatus for recognizing speech |
CN111684521A (en) * | 2018-02-02 | 2020-09-18 | 三星电子株式会社 | Method for processing speech signal for speaker recognition and electronic device implementing the same |
CN111710333A (en) * | 2016-06-08 | 2020-09-25 | 谷歌有限责任公司 | Extensible dynamic class language modeling method and system |
CN111742362A (en) * | 2018-01-23 | 2020-10-02 | 谷歌有限责任公司 | Selectively adapting and utilizing noise reduction techniques in call phrase detection |
CN112201247A (en) * | 2019-07-08 | 2021-01-08 | 北京地平线机器人技术研发有限公司 | Speech enhancement method and apparatus, electronic device, and storage medium |
CN112634932A (en) * | 2021-03-09 | 2021-04-09 | 南京涵书韵信息科技有限公司 | Audio signal processing method and device, server and related equipment |
CN112652304A (en) * | 2020-12-02 | 2021-04-13 | 北京百度网讯科技有限公司 | Voice interaction method and device of intelligent equipment and electronic equipment |
CN113056785A (en) * | 2018-12-18 | 2021-06-29 | 桑德托克斯公司 | Method for monitoring livestock facilities and/or livestock animals in livestock facilities using improved sound processing techniques |
CN113053382A (en) * | 2021-03-30 | 2021-06-29 | 联想(北京)有限公司 | Processing method and device |
CN114333881A (en) * | 2022-03-09 | 2022-04-12 | 深圳市迪斯声学有限公司 | Audio transmission noise reduction method, device, equipment and medium based on environment self-adaptation |
CN110648680B (en) * | 2019-09-23 | 2024-05-14 | 腾讯科技（深圳）有限公司 | Voice data processing method and device, electronic equipment and readable storage medium |
Families Citing this family (321)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU6630800A (en) * | 1999-08-13 | 2001-03-13 | Pixo, Inc. | Methods and apparatuses for display and traversing of links in page character array |
US8645137B2 (en) * | 2000-03-16 | 2014-02-04 | Apple Inc. | Fast, language-independent method for user authentication by voice |
ITFI20010199A1 (en) | 2001-10-22 | 2003-04-22 | Riccardo Vieri | SYSTEM AND METHOD TO TRANSFORM TEXTUAL COMMUNICATIONS INTO VOICE AND SEND THEM WITH AN INTERNET CONNECTION TO ANY TELEPHONE SYSTEM |
US7669134B1 (en) | 2003-05-02 | 2010-02-23 | Apple Inc. | Method and apparatus for displaying information during an instant messaging session |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US7633076B2 (en) | 2005-09-30 | 2009-12-15 | Apple Inc. | Automated response to and sensing of user activity in portable devices |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20080129520A1 (en) * | 2006-12-01 | 2008-06-05 | Apple Computer, Inc. | Electronic device with enhanced audio feedback |
US7912828B2 (en) * | 2007-02-23 | 2011-03-22 | Apple Inc. | Pattern searching methods and apparatuses |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
ITFI20070177A1 (en) | 2007-07-26 | 2009-01-27 | Riccardo Vieri | SYSTEM FOR THE CREATION AND SETTING OF AN ADVERTISING CAMPAIGN DERIVING FROM THE INSERTION OF ADVERTISING MESSAGES WITHIN AN EXCHANGE OF MESSAGES AND METHOD FOR ITS FUNCTIONING. |
US9053089B2 (en) * | 2007-10-02 | 2015-06-09 | Apple Inc. | Part-of-speech tagging using latent analogy |
US8595642B1 (en) | 2007-10-04 | 2013-11-26 | Great Northern Research, LLC | Multiple shell multi faceted graphical user interface |
US8165886B1 (en) | 2007-10-04 | 2012-04-24 | Great Northern Research LLC | Speech interface system and method for control and interaction with applications on a computing system |
US8364694B2 (en) * | 2007-10-26 | 2013-01-29 | Apple Inc. | Search assistant for digital media assets |
US8620662B2 (en) | 2007-11-20 | 2013-12-31 | Apple Inc. | Context-aware unit selection |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US8327272B2 (en) | 2008-01-06 | 2012-12-04 | Apple Inc. | Portable multifunction device, method, and graphical user interface for viewing and managing electronic calendars |
US8065143B2 (en) | 2008-02-22 | 2011-11-22 | Apple Inc. | Providing text input using speech data and non-speech data |
US8289283B2 (en) * | 2008-03-04 | 2012-10-16 | Apple Inc. | Language input interface on a device |
US8996376B2 (en) | 2008-04-05 | 2015-03-31 | Apple Inc. | Intelligent text-to-speech conversion |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US8464150B2 (en) | 2008-06-07 | 2013-06-11 | Apple Inc. | Automatic language identification for dynamic text processing |
US20100030549A1 (en) | 2008-07-31 | 2010-02-04 | Lee Michael M | Mobile device having human language translation capability with positional feedback |
US8768702B2 (en) | 2008-09-05 | 2014-07-01 | Apple Inc. | Multi-tiered voice feedback in an electronic device |
US8898568B2 (en) * | 2008-09-09 | 2014-11-25 | Apple Inc. | Audio user interface |
US8712776B2 (en) * | 2008-09-29 | 2014-04-29 | Apple Inc. | Systems and methods for selective text to speech synthesis |
US8396714B2 (en) * | 2008-09-29 | 2013-03-12 | Apple Inc. | Systems and methods for concatenation of words in text to speech synthesis |
US8583418B2 (en) | 2008-09-29 | 2013-11-12 | Apple Inc. | Systems and methods of detecting language and natural language strings for text to speech synthesis |
US8352272B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for text to speech synthesis |
US20100082328A1 (en) * | 2008-09-29 | 2010-04-01 | Apple Inc. | Systems and methods for speech preprocessing in text to speech synthesis |
US8352268B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for selective rate of speech and speech preferences for text to speech synthesis |
US8355919B2 (en) * | 2008-09-29 | 2013-01-15 | Apple Inc. | Systems and methods for text normalization for text to speech synthesis |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
WO2010067118A1 (en) | 2008-12-11 | 2010-06-17 | Novauris Technologies Limited | Speech recognition involving a mobile device |
US8862252B2 (en) | 2009-01-30 | 2014-10-14 | Apple Inc. | Audio user interface for displayless electronic device |
US8380507B2 (en) * | 2009-03-09 | 2013-02-19 | Apple Inc. | Systems and methods for determining the language to use for speech generated by a text to speech engine |
US10540976B2 (en) | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US9431006B2 (en) * | 2009-07-02 | 2016-08-30 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US20110010179A1 (en) * | 2009-07-13 | 2011-01-13 | Naik Devang K | Voice synthesis and processing |
US20110066438A1 (en) * | 2009-09-15 | 2011-03-17 | Apple Inc. | Contextual voiceover |
US8682649B2 (en) * | 2009-11-12 | 2014-03-25 | Apple Inc. | Sentiment prediction from textual data |
US20110167350A1 (en) * | 2010-01-06 | 2011-07-07 | Apple Inc. | Assist Features For Content Display Device |
US8600743B2 (en) * | 2010-01-06 | 2013-12-03 | Apple Inc. | Noise profile determination for voice-related feature |
US8381107B2 (en) | 2010-01-13 | 2013-02-19 | Apple Inc. | Adaptive audio feedback system and method |
US8311838B2 (en) | 2010-01-13 | 2012-11-13 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US8682667B2 (en) | 2010-02-25 | 2014-03-25 | Apple Inc. | User profiling for selecting user specific voice input processing information |
US9058732B2 (en) * | 2010-02-25 | 2015-06-16 | Qualcomm Incorporated | Method and apparatus for enhanced indoor position location with assisted user profiles |
US8639516B2 (en) * | 2010-06-04 | 2014-01-28 | Apple Inc. | User-specific noise suppression for voice quality improvements |
US8713021B2 (en) | 2010-07-07 | 2014-04-29 | Apple Inc. | Unsupervised document clustering using latent semantic density analysis |
US9104670B2 (en) | 2010-07-21 | 2015-08-11 | Apple Inc. | Customized search or acquisition of digital media assets |
US8521526B1 (en) * | 2010-07-28 | 2013-08-27 | Google Inc. | Disambiguation of a spoken query term |
KR20140061285A (en) * | 2010-08-11 | 2014-05-21 | 본 톤 커뮤니케이션즈 엘티디. | Background sound removal for privacy and personalization use |
US8719006B2 (en) | 2010-08-27 | 2014-05-06 | Apple Inc. | Combined statistical and rule-based part-of-speech tagging for text-to-speech synthesis |
US8719014B2 (en) | 2010-09-27 | 2014-05-06 | Apple Inc. | Electronic device with text error correction based on voice recognition data |
KR20120054845A (en) * | 2010-11-22 | 2012-05-31 | 삼성전자주식회사 | Speech recognition method for robot |
US10515147B2 (en) | 2010-12-22 | 2019-12-24 | Apple Inc. | Using statistical language models for contextual lookup |
US10762293B2 (en) | 2010-12-22 | 2020-09-01 | Apple Inc. | Using parts-of-speech tagging and named entity recognition for spelling correction |
WO2012107561A1 (en) * | 2011-02-10 | 2012-08-16 | Dolby International Ab | Spatial adaptation in multi-microphone sound capture |
US8781836B2 (en) | 2011-02-22 | 2014-07-15 | Apple Inc. | Hearing assistance system for providing consistent human speech |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
CA2831678A1 (en) * | 2011-03-28 | 2012-10-04 | Ambientz | Methods and systems for searching utilizing acoustical context |
US10672399B2 (en) | 2011-06-03 | 2020-06-02 | Apple Inc. | Switching between text data and audio data based on a mapping |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US8812294B2 (en) | 2011-06-21 | 2014-08-19 | Apple Inc. | Translating phrases from one language into another using an order-based set of declarative rules |
GB2493413B (en) | 2011-07-25 | 2013-12-25 | Ibm | Maintaining and supplying speech models |
TWI442384B (en) * | 2011-07-26 | 2014-06-21 | Ind Tech Res Inst | Microphone-array-based speech recognition system and method |
US8595015B2 (en) * | 2011-08-08 | 2013-11-26 | Verizon New Jersey Inc. | Audio communication assessment |
US8706472B2 (en) | 2011-08-11 | 2014-04-22 | Apple Inc. | Method for disambiguating multiple readings in language conversion |
US8994660B2 (en) | 2011-08-29 | 2015-03-31 | Apple Inc. | Text correction processing |
US8762156B2 (en) | 2011-09-28 | 2014-06-24 | Apple Inc. | Speech recognition repair using contextual information |
US8712184B1 (en) * | 2011-12-05 | 2014-04-29 | Hermes Microvision, Inc. | Method and system for filtering noises in an image scanned by charged particles |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US9280610B2 (en) | 2012-05-14 | 2016-03-08 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US8775442B2 (en) | 2012-05-15 | 2014-07-08 | Apple Inc. | Semantic search using a single-source semantic model |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11023520B1 (en) | 2012-06-01 | 2021-06-01 | Google Llc | Background audio identification for query disambiguation |
US9123338B1 (en) | 2012-06-01 | 2015-09-01 | Google Inc. | Background audio identification for speech disambiguation |
WO2013185109A2 (en) | 2012-06-08 | 2013-12-12 | Apple Inc. | Systems and methods for recognizing textual identifiers within a plurality of words |
US9721563B2 (en) | 2012-06-08 | 2017-08-01 | Apple Inc. | Name recognition system |
US9489940B2 (en) * | 2012-06-11 | 2016-11-08 | Nvoq Incorporated | Apparatus and methods to update a language model in a speech recognition system |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US9384737B2 (en) * | 2012-06-29 | 2016-07-05 | Microsoft Technology Licensing, Llc | Method and device for adjusting sound levels of sources based on sound source priority |
CN102841932A (en) * | 2012-08-06 | 2012-12-26 | 河海大学 | Content-based voice frequency semantic feature similarity comparative method |
WO2014025990A1 (en) | 2012-08-10 | 2014-02-13 | Nuance Communications, Inc. | Virtual agent communication for electronic devices |
US20140074466A1 (en) | 2012-09-10 | 2014-03-13 | Google Inc. | Answering questions using environmental context |
US9576574B2 (en) | 2012-09-10 | 2017-02-21 | Apple Inc. | Context-sensitive handling of interruptions by intelligent digital assistant |
US9547647B2 (en) | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
US8935167B2 (en) | 2012-09-25 | 2015-01-13 | Apple Inc. | Exemplar-based latent perceptual modeling for automatic speech recognition |
US9319816B1 (en) * | 2012-09-26 | 2016-04-19 | Amazon Technologies, Inc. | Characterizing environment using ultrasound pilot tones |
US9190057B2 (en) * | 2012-12-12 | 2015-11-17 | Amazon Technologies, Inc. | Speech model retrieval in distributed speech recognition systems |
US9653070B2 (en) | 2012-12-31 | 2017-05-16 | Intel Corporation | Flexible architecture for acoustic signal processing engine |
US8494853B1 (en) * | 2013-01-04 | 2013-07-23 | Google Inc. | Methods and systems for providing speech recognition systems based on speech recordings logs |
CN103065631B (en) * | 2013-01-24 | 2015-07-29 | 华为终端有限公司 | A kind of method of speech recognition, device |
CN103971680B (en) * | 2013-01-24 | 2018-06-05 | 华为终端（东莞）有限公司 | A kind of method, apparatus of speech recognition |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US9460715B2 (en) * | 2013-03-04 | 2016-10-04 | Amazon Technologies, Inc. | Identification using audio signatures and additional characteristics |
US20140278395A1 (en) * | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Method and Apparatus for Determining a Motion Environment Profile to Adapt Voice Recognition Processing |
US20140278392A1 (en) * | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Method and Apparatus for Pre-Processing Audio Signals |
US20140278415A1 (en) * | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Voice Recognition Configuration Selector and Method of Operation Therefor |
US9312826B2 (en) | 2013-03-13 | 2016-04-12 | Kopin Corporation | Apparatuses and methods for acoustic channel auto-balancing during multi-channel signal extraction |
US10306389B2 (en) | 2013-03-13 | 2019-05-28 | Kopin Corporation | Head wearable acoustic system with noise canceling microphone geometry apparatuses and methods |
US9977779B2 (en) | 2013-03-14 | 2018-05-22 | Apple Inc. | Automatic supplementation of word correction dictionaries |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US9368114B2 (en) | 2013-03-14 | 2016-06-14 | Apple Inc. | Context-sensitive handling of interruptions |
US10572476B2 (en) | 2013-03-14 | 2020-02-25 | Apple Inc. | Refining a search based on schedule items |
US9733821B2 (en) | 2013-03-14 | 2017-08-15 | Apple Inc. | Voice control to diagnose inadvertent activation of accessibility features |
US10642574B2 (en) | 2013-03-14 | 2020-05-05 | Apple Inc. | Device, method, and graphical user interface for outputting captions |
US10424292B1 (en) * | 2013-03-14 | 2019-09-24 | Amazon Technologies, Inc. | System for recognizing and responding to environmental noises |
KR101759009B1 (en) | 2013-03-15 | 2017-07-17 | 애플 인크. | Training an at least partial voice command system |
EP2973002B1 (en) | 2013-03-15 | 2019-06-26 | Apple Inc. | User training by intelligent digital assistant |
KR102057795B1 (en) | 2013-03-15 | 2019-12-19 | 애플 인크. | Context-sensitive handling of interruptions |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
WO2014144579A1 (en) | 2013-03-15 | 2014-09-18 | Apple Inc. | System and method for updating an adaptive speech recognition model |
US9208781B2 (en) | 2013-04-05 | 2015-12-08 | International Business Machines Corporation | Adapting speech recognition acoustic models with environmental and social cues |
US9953630B1 (en) * | 2013-05-31 | 2018-04-24 | Amazon Technologies, Inc. | Language recognition for device settings |
WO2014197336A1 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
WO2014197334A2 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
WO2014197335A1 (en) | 2013-06-08 | 2014-12-11 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
KR101959188B1 (en) | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
WO2014200731A1 (en) | 2013-06-13 | 2014-12-18 | Apple Inc. | System and method for emergency calls initiated by voice command |
KR101749009B1 (en) | 2013-08-06 | 2017-06-19 | 애플 인크. | Auto-activating smart responses based on activities from remote devices |
US10296160B2 (en) | 2013-12-06 | 2019-05-21 | Apple Inc. | Method for extracting salient dialog usage from live data |
US10534623B2 (en) * | 2013-12-16 | 2020-01-14 | Nuance Communications, Inc. | Systems and methods for providing a virtual assistant |
US9953634B1 (en) * | 2013-12-17 | 2018-04-24 | Knowles Electronics, Llc | Passive training for automatic speech recognition |
GB2524222B (en) * | 2013-12-18 | 2018-07-18 | Cirrus Logic Int Semiconductor Ltd | Activating speech processing |
US9589560B1 (en) * | 2013-12-19 | 2017-03-07 | Amazon Technologies, Inc. | Estimating false rejection rate in a detection system |
US9466310B2 (en) * | 2013-12-20 | 2016-10-11 | Lenovo Enterprise Solutions (Singapore) Pte. Ltd. | Compensating for identifiable background content in a speech recognition device |
JP6375521B2 (en) * | 2014-03-28 | 2018-08-22 | パナソニックＩｐマネジメント株式会社 | Voice search device, voice search method, and display device |
US10446168B2 (en) * | 2014-04-02 | 2019-10-15 | Plantronics, Inc. | Noise level measurement with mobile devices, location services, and environmental response |
KR102257910B1 (en) * | 2014-05-02 | 2021-05-27 | 삼성전자주식회사 | Apparatus and method for speech recognition, apparatus and method for generating noise-speech recognition model |
US9620105B2 (en) | 2014-05-15 | 2017-04-11 | Apple Inc. | Analyzing audio input for efficient speech and music recognition |
US10592095B2 (en) | 2014-05-23 | 2020-03-17 | Apple Inc. | Instantaneous speaking of content on touch devices |
US9502031B2 (en) | 2014-05-27 | 2016-11-22 | Apple Inc. | Method for supporting dynamic grammars in WFST-based ASR |
US10289433B2 (en) | 2014-05-30 | 2019-05-14 | Apple Inc. | Domain specific language for encoding assistant dialog |
US9760559B2 (en) | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9734193B2 (en) | 2014-05-30 | 2017-08-15 | Apple Inc. | Determining domain salience ranking from ambiguous words in natural speech |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US9904851B2 (en) | 2014-06-11 | 2018-02-27 | At&T Intellectual Property I, L.P. | Exploiting visual information for enhancing audio signals via source separation and beamforming |
US9858922B2 (en) | 2014-06-23 | 2018-01-02 | Google Inc. | Caching speech recognition scores |
US9639854B2 (en) | 2014-06-26 | 2017-05-02 | Nuance Communications, Inc. | Voice-controlled information exchange platform, such as for providing information to supplement advertising |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US9953646B2 (en) | 2014-09-02 | 2018-04-24 | Belleau Technologies | Method and system for dynamic speech recognition and tracking of prewritten script |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US9299347B1 (en) * | 2014-10-22 | 2016-03-29 | Google Inc. | Speech recognition using associative mapping |
US10999636B1 (en) * | 2014-10-27 | 2021-05-04 | Amazon Technologies, Inc. | Voice-based content searching on a television based on receiving candidate search strings from a remote server |
US10218630B2 (en) | 2014-10-30 | 2019-02-26 | Pearson Education, Inc. | System and method for increasing data transmission rates through a content distribution network |
US10116563B1 (en) | 2014-10-30 | 2018-10-30 | Pearson Education, Inc. | System and method for automatically updating data packet metadata |
US10318499B2 (en) | 2014-10-30 | 2019-06-11 | Pearson Education, Inc. | Content database generation |
US9667321B2 (en) * | 2014-10-31 | 2017-05-30 | Pearson Education, Inc. | Predictive recommendation engine |
WO2016070124A1 (en) | 2014-10-30 | 2016-05-06 | Pearson Education, Inc. | Content database generation |
US10110486B1 (en) | 2014-10-30 | 2018-10-23 | Pearson Education, Inc. | Automatic determination of initial content difficulty |
US10735402B1 (en) | 2014-10-30 | 2020-08-04 | Pearson Education, Inc. | Systems and method for automated data packet selection and delivery |
US10333857B1 (en) | 2014-10-30 | 2019-06-25 | Pearson Education, Inc. | Systems and methods for data packet metadata stabilization |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US10152299B2 (en) | 2015-03-06 | 2018-12-11 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US10460227B2 (en) | 2015-05-15 | 2019-10-29 | Apple Inc. | Virtual assistant in a communication session |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US10504509B2 (en) | 2015-05-27 | 2019-12-10 | Google Llc | Providing suggested voice-based action queries |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US9578173B2 (en) | 2015-06-05 | 2017-02-21 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US10255907B2 (en) | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
US9786270B2 (en) | 2015-07-09 | 2017-10-10 | Google Inc. | Generating acoustic models |
US10008199B2 (en) * | 2015-08-22 | 2018-06-26 | Toyota Motor Engineering & Manufacturing North America, Inc. | Speech recognition system with abbreviated training |
US10614368B2 (en) | 2015-08-28 | 2020-04-07 | Pearson Education, Inc. | System and method for content provisioning with dual recommendation engines |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US11631421B2 (en) | 2015-10-18 | 2023-04-18 | Solos Technology Limited | Apparatuses and methods for enhanced speech recognition in variable environments |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10468016B2 (en) | 2015-11-24 | 2019-11-05 | International Business Machines Corporation | System and method for supporting automatic speech recognition of regional accents based on statistical information and user corrections |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10229672B1 (en) | 2015-12-31 | 2019-03-12 | Google Llc | Training acoustic models using connectionist temporal classification |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US11138987B2 (en) * | 2016-04-04 | 2021-10-05 | Honeywell International Inc. | System and method to distinguish sources in a multiple audio source environment |
US10325215B2 (en) | 2016-04-08 | 2019-06-18 | Pearson Education, Inc. | System and method for automatic content aggregation generation |
US11188841B2 (en) | 2016-04-08 | 2021-11-30 | Pearson Education, Inc. | Personalized content distribution |
US10789316B2 (en) | 2016-04-08 | 2020-09-29 | Pearson Education, Inc. | Personalized automatic content aggregation generation |
US10642848B2 (en) | 2016-04-08 | 2020-05-05 | Pearson Education, Inc. | Personalized automatic content aggregation generation |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
DK179309B1 (en) | 2016-06-09 | 2018-04-23 | Apple Inc | Intelligent automated assistant in a home environment |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
DK179049B1 (en) | 2016-06-11 | 2017-09-18 | Apple Inc | Data driven natural language event detection and classification |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
DK179343B1 (en) | 2016-06-11 | 2018-05-14 | Apple Inc | Intelligent task discovery |
US20180018973A1 (en) | 2016-07-15 | 2018-01-18 | Google Inc. | Speaker verification |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10951720B2 (en) | 2016-10-24 | 2021-03-16 | Bank Of America Corporation | Multi-channel cognitive resource platform |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
DK201770439A1 (en) | 2017-05-11 | 2018-12-13 | Apple Inc. | Offline personal assistant |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
DK201770432A1 (en) | 2017-05-15 | 2018-12-21 | Apple Inc. | Hierarchical belief states for digital assistants |
DK201770431A1 (en) | 2017-05-15 | 2018-12-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
DK179560B1 (en) | 2017-05-16 | 2019-02-18 | Apple Inc. | Far-field extension for digital assistant services |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10706840B2 (en) | 2017-08-18 | 2020-07-07 | Google Llc | Encoder-decoder models for sequence to sequence mapping |
US10096311B1 (en) | 2017-09-12 | 2018-10-09 | Plantronics, Inc. | Intelligent soundscape adaptation utilizing mobile devices |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
CN107908742A (en) * | 2017-11-15 | 2018-04-13 | 百度在线网络技术（北京）有限公司 | Method and apparatus for output information |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
KR102446637B1 (en) * | 2017-12-28 | 2022-09-23 | 삼성전자주식회사 | Sound output system and speech processing method |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10923139B2 (en) | 2018-05-02 | 2021-02-16 | Melo Inc. | Systems and methods for processing meeting information obtained from multiple sources |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
DK201870355A1 (en) | 2018-06-01 | 2019-12-16 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
CN109841227B (en) * | 2019-03-11 | 2020-10-02 | 南京邮电大学 | Background noise removing method based on learning compensation |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11848023B2 (en) * | 2019-06-10 | 2023-12-19 | Google Llc | Audio noise reduction |
KR102260216B1 (en) * | 2019-07-29 | 2021-06-03 | 엘지전자 주식회사 | Intelligent voice recognizing method, voice recognizing apparatus, intelligent computing device and server |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11489794B2 (en) | 2019-11-04 | 2022-11-01 | Bank Of America Corporation | System for configuration and intelligent transmission of electronic communications and integrated resource processing |
CN112820307B (en) * | 2020-02-19 | 2023-12-15 | 腾讯科技（深圳）有限公司 | Voice message processing method, device, equipment and medium |
CN111461438B (en) * | 2020-04-01 | 2024-01-05 | 中国人民解放军空军93114部队 | Signal detection method and device, electronic equipment and storage medium |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
US11580959B2 (en) * | 2020-09-28 | 2023-02-14 | International Business Machines Corporation | Improving speech recognition transcriptions |
CN112669867B (en) | 2020-12-15 | 2023-04-11 | 阿波罗智联(北京)科技有限公司 | Debugging method and device of noise elimination algorithm and electronic equipment |
US11875798B2 (en) | 2021-05-03 | 2024-01-16 | International Business Machines Corporation | Profiles for enhanced speech recognition training |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1453767A (en) * | 2002-04-26 | 2003-11-05 | 日本先锋公司 | Speech recognition apparatus and speech recognition method |
US6718302B1 (en) * | 1997-10-20 | 2004-04-06 | Sony Corporation | Method for utilizing validity constraints in a speech endpoint detector |
US20050091274A1 (en) * | 2003-10-28 | 2005-04-28 | International Business Machines Corporation | System and method for transcribing audio files of various languages |
WO2005098827A1 (en) * | 2004-04-08 | 2005-10-20 | Siemens Aktiengesellschaft | Method for noise reduction in a speech input signal |
US20100145687A1 (en) * | 2008-12-04 | 2010-06-10 | Microsoft Corporation | Removing noise from speech |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5970446A (en) * | 1997-11-25 | 1999-10-19 | At&T Corp | Selective noise/channel/coding models and recognizers for automatic speech recognition |
US7209880B1 (en) * | 2001-03-20 | 2007-04-24 | At&T Corp. | Systems and methods for dynamic re-configurable speech recognition |
JP3826032B2 (en) * | 2001-12-28 | 2006-09-27 | 株式会社東芝 | Speech recognition apparatus, speech recognition method, and speech recognition program |
JP4357867B2 (en) * | 2003-04-25 | 2009-11-04 | パイオニア株式会社 | Voice recognition apparatus, voice recognition method, voice recognition program, and recording medium recording the same |
JP4340686B2 (en) * | 2004-03-31 | 2009-10-07 | パイオニア株式会社 | Speech recognition apparatus and speech recognition method |
EP2048656B1 (en) * | 2007-10-10 | 2010-02-10 | Harman/Becker Automotive Systems GmbH | Speaker recognition |
-
2010
- 2010-06-14 US US12/814,665 patent/US8234111B2/en active Active
-
2011
- 2011-06-13 WO PCT/US2011/040225 patent/WO2011159628A1/en active Application Filing
- 2011-06-13 EP EP11731192.8A patent/EP2580751B1/en active Active
- 2011-06-13 CN CN201180026390.4A patent/CN103069480B/en active Active
- 2011-06-13 AU AU2011267982A patent/AU2011267982B2/en active Active
- 2011-09-30 US US13/250,777 patent/US8249868B2/en active Active
-
2012
- 2012-06-22 US US13/530,614 patent/US8666740B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6718302B1 (en) * | 1997-10-20 | 2004-04-06 | Sony Corporation | Method for utilizing validity constraints in a speech endpoint detector |
CN1453767A (en) * | 2002-04-26 | 2003-11-05 | 日本先锋公司 | Speech recognition apparatus and speech recognition method |
US20050091274A1 (en) * | 2003-10-28 | 2005-04-28 | International Business Machines Corporation | System and method for transcribing audio files of various languages |
WO2005098827A1 (en) * | 2004-04-08 | 2005-10-20 | Siemens Aktiengesellschaft | Method for noise reduction in a speech input signal |
US20100145687A1 (en) * | 2008-12-04 | 2010-06-10 | Microsoft Corporation | Removing noise from speech |
Cited By (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105580071A (en) * | 2013-05-06 | 2016-05-11 | 谷歌技术控股有限责任公司 | Method and apparatus for training a voice recognition model database |
CN105580071B (en) * | 2013-05-06 | 2020-08-21 | 谷歌技术控股有限责任公司 | Method and apparatus for training a voice recognition model database |
CN106663446A (en) * | 2014-07-02 | 2017-05-10 | 微软技术许可有限责任公司 | User environment aware acoustic noise reduction |
CN106663446B (en) * | 2014-07-02 | 2021-03-12 | 微软技术许可有限责任公司 | User environment aware acoustic noise reduction |
CN107004406A (en) * | 2014-12-02 | 2017-08-01 | 索尼公司 | Message processing device, information processing method and program |
CN105719645A (en) * | 2014-12-17 | 2016-06-29 | 现代自动车株式会社 | Speech recognition apparatus, vehicle including the same, and method of controlling the same |
CN105719645B (en) * | 2014-12-17 | 2020-09-18 | 现代自动车株式会社 | Voice recognition apparatus, vehicle including the same, and method of controlling voice recognition apparatus |
CN111710333A (en) * | 2016-06-08 | 2020-09-25 | 谷歌有限责任公司 | Extensible dynamic class language modeling method and system |
CN111710333B (en) * | 2016-06-08 | 2024-01-09 | 谷歌有限责任公司 | Method and system for generating speech transcription |
CN108182270A (en) * | 2018-01-17 | 2018-06-19 | 广东小天才科技有限公司 | Search for content transmission and searching method, smart pen, search terminal and storage medium |
CN111742362B (en) * | 2018-01-23 | 2024-04-09 | 谷歌有限责任公司 | Selectively adapting and utilizing noise reduction techniques in call phrase detection |
CN111742362A (en) * | 2018-01-23 | 2020-10-02 | 谷歌有限责任公司 | Selectively adapting and utilizing noise reduction techniques in call phrase detection |
CN111684521A (en) * | 2018-02-02 | 2020-09-18 | 三星电子株式会社 | Method for processing speech signal for speaker recognition and electronic device implementing the same |
CN109087659A (en) * | 2018-08-03 | 2018-12-25 | 三星电子（中国）研发中心 | Audio optimization method and apparatus |
CN111415653A (en) * | 2018-12-18 | 2020-07-14 | 百度在线网络技术（北京）有限公司 | Method and apparatus for recognizing speech |
CN113056785A (en) * | 2018-12-18 | 2021-06-29 | 桑德托克斯公司 | Method for monitoring livestock facilities and/or livestock animals in livestock facilities using improved sound processing techniques |
CN112201247A (en) * | 2019-07-08 | 2021-01-08 | 北京地平线机器人技术研发有限公司 | Speech enhancement method and apparatus, electronic device, and storage medium |
CN112201247B (en) * | 2019-07-08 | 2024-05-03 | 北京地平线机器人技术研发有限公司 | Speech enhancement method and device, electronic equipment and storage medium |
CN110648680A (en) * | 2019-09-23 | 2020-01-03 | 腾讯科技（深圳）有限公司 | Voice data processing method and device, electronic equipment and readable storage medium |
CN110648680B (en) * | 2019-09-23 | 2024-05-14 | 腾讯科技（深圳）有限公司 | Voice data processing method and device, electronic equipment and readable storage medium |
CN110956955A (en) * | 2019-12-10 | 2020-04-03 | 苏州思必驰信息科技有限公司 | Voice interaction method and device |
CN110956955B (en) * | 2019-12-10 | 2022-08-05 | 思必驰科技股份有限公司 | Voice interaction method and device |
CN112652304B (en) * | 2020-12-02 | 2022-02-01 | 北京百度网讯科技有限公司 | Voice interaction method and device of intelligent equipment and electronic equipment |
CN112652304A (en) * | 2020-12-02 | 2021-04-13 | 北京百度网讯科技有限公司 | Voice interaction method and device of intelligent equipment and electronic equipment |
CN112634932B (en) * | 2021-03-09 | 2021-06-22 | 赣州柏朗科技有限公司 | Audio signal processing method and device, server and related equipment |
CN112634932A (en) * | 2021-03-09 | 2021-04-09 | 南京涵书韵信息科技有限公司 | Audio signal processing method and device, server and related equipment |
CN113053382A (en) * | 2021-03-30 | 2021-06-29 | 联想(北京)有限公司 | Processing method and device |
CN114333881A (en) * | 2022-03-09 | 2022-04-12 | 深圳市迪斯声学有限公司 | Audio transmission noise reduction method, device, equipment and medium based on environment self-adaptation |
Also Published As
Publication number | Publication date |
---|---|
US8234111B2 (en) | 2012-07-31 |
US20110307253A1 (en) | 2011-12-15 |
US20120022860A1 (en) | 2012-01-26 |
AU2011267982A1 (en) | 2012-11-01 |
AU2011267982B2 (en) | 2015-02-05 |
US8249868B2 (en) | 2012-08-21 |
US20120259631A1 (en) | 2012-10-11 |
WO2011159628A1 (en) | 2011-12-22 |
CN103069480B (en) | 2014-12-24 |
US8666740B2 (en) | 2014-03-04 |
EP2580751B1 (en) | 2014-08-13 |
EP2580751A1 (en) | 2013-04-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN103069480B (en) | Speech and noise models for speech recognition | |
CN104575493B (en) | Use the acoustic model adaptation of geography information | |
US9858917B1 (en) | Adapting enhanced acoustic models | |
US8428940B2 (en) | Metadata-based weighting of geotagged environmental audio for enhanced speech recognition accuracy | |
EP3308379B1 (en) | Motion adaptive speech processing | |
AU2014200999B2 (en) | Geotagged environmental audio for enhanced speech recognition accuracy |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
C14 | Grant of patent or utility model | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |