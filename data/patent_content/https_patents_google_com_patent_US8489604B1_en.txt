US8489604B1 - Automated resource selection process evaluation - Google Patents
Automated resource selection process evaluation Download PDFInfo
- Publication number
- US8489604B1 US8489604B1 US12/912,229 US91222910A US8489604B1 US 8489604 B1 US8489604 B1 US 8489604B1 US 91222910 A US91222910 A US 91222910A US 8489604 B1 US8489604 B1 US 8489604B1
- Authority
- US
- United States
- Prior art keywords
- resources
- resource
- query
- indexed
- group
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
- 238000000034 method Methods 0.000 title claims abstract description 239
- 230000008569 process Effects 0.000 title claims abstract description 185
- 238000011156 evaluation Methods 0.000 title description 20
- 238000012360 testing method Methods 0.000 claims abstract description 84
- 238000004590 computer program Methods 0.000 claims abstract description 14
- 230000004044 response Effects 0.000 claims description 12
- 238000012545 processing Methods 0.000 claims description 9
- 230000004931 aggregating effect Effects 0.000 claims description 4
- 230000006399 behavior Effects 0.000 description 7
- 238000004422 calculation algorithm Methods 0.000 description 7
- 238000004891 communication Methods 0.000 description 5
- 230000009193 crawling Effects 0.000 description 4
- 238000013515 script Methods 0.000 description 3
- 238000010586 diagram Methods 0.000 description 2
- 238000002474 experimental method Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000005070 sampling Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
Definitions
- This specification relates to evaluating automated resource selection processes for use by search engines.
- Search engines e.g., Internet search engines, provide information about resources (e.g., Web pages, images, text documents, multimedia content) that are responsive to a user's search query. Search engines return a set of search results (e.g., as a ranked list of results) in response to a user-submitted query.
- a search result includes, for example, a link (e.g., a URL) to, and a snippet of information from, a corresponding resource.
- search engines build indexes that map words and phrases to resources determined to be relevant to the words and phrases. To build this index, search engines crawl available resources, e.g., by crawling the Internet. Index space is finite; therefore, search engines determine whether to include each resource that is crawled in the index. In some search engines, the determination of whether to include a particular resource in the search engine index is made according to an automated resource selection process. Automated resource selection processes analyze the values of one or more index selection signals for the resource to determine whether the resource should be included in the index. Each index signal is a metric of a quality of the resource derived by combining one or more attributes of a resource.
- Each index selection signal value is a quantity (generally scalar) derived from one or more attributes of the resource.
- Resource attributes can be internal to a resource, e.g., a number of words in a given resource or a length of the title of the given resource.
- Resource attributes can also be external to the resource, e.g., attributes derived from resources that link to a given resource or attributes derived from user behavior toward the resource.
- a system can build separate indexes and consider the indexes side by side, e.g., by comparing the resources identified by each index in response to various queries.
- this requires the overhead of building and maintaining two separate indexes, which can be costly.
- a system can build a single index, where some resources are selected according to a first resource selection process and other resources are selected according to a different second resource selection process. User behavior toward the resources selected by the first resource selection process and user behavior toward the resources selected by the second resource process can then be observed.
- the user behavior data is incomplete, because it fails to account for how users would interact with the resources if only resources selected according to one of the resource selection processes were presented to users.
- a system can observe user behavior regarding resources selected according to a single resource selection process.
- only observing behavior with regard to one index can give an incomplete picture of the quality of the index selection algorithm.
- User behavior data for resources not selected by the resource selection process being evaluated is not available, and therefore one cannot determine whether the selection algorithm could have done better.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a plurality of test queries; generating, for each test query, a first group of resources corresponding to a first automated resource selection process and a second group of resources corresponding to a second automated resource selection process, the generating comprising, for each test query: identifying a plurality of resources responsive to the test query; determining, for each resource in the plurality of resources, whether the first automated resource selection process would classify the resource as to-be-indexed or not-to-be-indexed, and then selecting all resources classified as to-be-indexed as the first group of resources; determining, for each resource in the plurality of resources, whether the second automated resource selection process would classify the resource as to-be-indexed or not-to-be-indexed, and then selecting all resources classified as to-be-indexed as the second group of resources.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer programs recorded on computer storage devices, each configured to perform the operations of the methods.
- the determination whether the first automated resource selection process would classify a resource as to-be-indexed or not-to-be-indexed is made independently of the determination made for any other identified resource.
- the determination whether the second automated resource selection process would classify a resource as to-be-indexed or not-to-be-indexed is made independently of the determination made for any other identified resource.
- the actions further comprise determining, for at least one test query, that the first automated resource selection process would classify at least one resource as to-be-indexed and would classify at least one resource as not-to-be-indexed.
- the actions further comprise determining, for at least one test query, that the second automated resource selection process would classify at least one resource as to-be-indexed and would classify at least one resource as not-to-be-indexed.
- the first group of resources is different from the second group of resources.
- Determining whether the first automated resource selection process would classify the resource as to-be-indexed or not-to-be-indexed comprises deriving an index selection score for the resource, determining that the first automated resource selection process would classify the resource as to-be-indexed if the index selection score satisfies a threshold, and otherwise determining that the first automated resource selection process would classify the resource as not-to-be-indexed.
- Identifying a plurality of resources responsive to the test query comprises identifying the plurality of resources from a query results table, wherein the query results table maps each of a plurality of queries to a respective one or more resources responsive to the query and includes a query-independent quality score for each resource.
- the actions further comprise: receiving respective feedback for each test query, the respective feedback for each test query selecting either the first group of resources or the second group of resources; and selecting either the first automated resource selection process or the second automated resource selection process as a result of the respective feedback for each test query.
- the actions further comprise presenting first search results corresponding to the first group of resources and second search results corresponding to the second group of resources to each evaluator in a plurality of evaluators, wherein: receiving feedback selecting either the first group of resources or the second group of resources comprises receiving feedback from each evaluator indicating that the evaluator prefers the first search results or the second search results, and aggregating the received feedback.
- the actions further comprise obtaining a query-specific score for each resource in the first group of resources and obtaining a query-specific score for each resource in the second group of resources, wherein: the first search results corresponding to the first group of resources have a first order derived from the query-specific score for each resource and presenting the first search results comprises presenting the first search results according to the first order; and the second search results for the second group of resources have a second order derived from the query-specific score for each resource and presenting the second search results comprises presenting the second search results according to the second order.
- the actions further comprise for one or more of the test queries: receiving the test query through a search engine user interface from each of a plurality of users; presenting search results corresponding to the first group of resources to one or more first users in the plurality of users in response to receiving the test query from each first user, and presenting search results corresponding to the second group of resources to one or more second users in the plurality of users in response to receiving the test query from each second user; and comparing user assessment of the search results corresponding to the first group of resources and user assessment of search results corresponding to the second group of resources, resulting in a comparison; and analyzing the comparison for each of the one or more test queries to select either the first automated resource selection process or the second automated resource selection process.
- the operations further comprise obtaining a query-specific score for each resource in the first group of resources and obtaining a query-specific score for each resource in the second group of resources, wherein: the search results for the first subset of identified resources have a first order derived from the query-specific scores for each resource and presenting the search results comprises presenting the search results according to the first order; and the search results for the second subset of identified resources have a second order derived from the query-specific scores for each resource and presenting the search results comprises presenting the search results according to the second order.
- index selection signals comprise a totality of signals used by each of a plurality of automated resource selection processes, wherein each automated resource selection process uses one or more of the plurality of types index selection signals to determine whether to include or exclude a resource from an index; selecting a plurality of queries for inclusion in a query results table; for each of a plurality of resources: matching one or more terms of the resource to each of one or more of the plurality of queries; determining a respective ranking score for the resource for each matched query; and obtaining one or more index selection signal values for each matched resource, wherein each obtained index selection signal value is for one of the index selection signals in the plurality of index selection signals; and generating the query results table, wherein the query results table maps each of the plurality of queries to each resource matched to the query, and maps each resource to a respective score for the resource and the query, and one or
- the actions further comprise determining a ranking score threshold for each query; for each of the plurality of resources: matching one or more terms of the resource to each of one or more of the plurality of queries; for each matched query for the resource: determining an approximate ranking score for the resource for the matched query; comparing the approximate ranking score for the resource for the matched query to the ranking score threshold for the matched query; determining a final ranking score for the resource for the matched query if the approximate ranking score satisfies the ranking score threshold, and otherwise not determining a final ranking score for the resource and the matched query; and wherein the query results table maps each of the plurality of queries to any resource that matched the query and has a final ranking score that satisfies a threshold.
- Determining a ranking score threshold for each query comprises selecting a proper subset of the plurality of resources, and, for each resource in the proper subset matching one or more terms of the resource to each of one or more of the plurality of queries; and determining an approximate ranking score for the resource for each matched query; and determining a ranking score threshold for each query from the approximate ranking score for each resource matched to the query.
- Each type of index selection signal is derived from one or more resource attributes.
- the actions further include crawling a plurality of candidate resources to generate a search engine index and to identify the plurality of resources, wherein the plurality of resources includes at least one resource that is not in the search engine index.
- FIG. 1 is a block diagram of an example index selection evaluation system.
- FIG. 2 is a flow chart of an example method for selecting groups of resources for use in comparing automated resource selection processes.
- FIG. 3 is a flow chart of an example method for comparing automated resource selection processes by comparing search results for groups of resources associated with the automated resource selection processes.
- FIG. 4 illustrates an example graphical user interface for presenting two sets of search results in response to the same query.
- FIG. 5 is a flow chart of an example method for comparing automated resource selection processes by comparing user selections of search results corresponding to different groups of resources associated with the automated resource selection processes.
- FIG. 6 is a flow chart of an example method for generating a resource source that associates queries with resources responsive to the queries.
- FIG. 7 is a flow chart of an example method for determining whether to obtain a query specific score for a matched resource and query.
- FIG. 1 is a block diagram of an example index selection evaluation system 100 .
- the index selection evaluation system 100 includes an evaluation engine 102 , a resource source 104 , and one or more index selection engines 106 a and 106 b . While two index selection engines 106 a and 106 b are shown in FIG. 1 , other numbers of index selection engines can alternatively be used.
- the index selection evaluation system 100 is implemented on one or more computers.
- the evaluation engine 102 evaluates different automated resource selection processes. Each automated resource selection process determines whether individual resources should be included in an index of a search engine or should not be included in the index of the search engine.
- a resource is any data that can be provided by a website or other source, e.g., over a network or on a file system, and that is associated with a resource address or identifier, e.g., a Uniform Resource Locator (URL), a Uniform Resource Identifier (URI), or a file name. Examples of resources are HTML pages, word processing documents, portable document format (PDF) documents, presentation documents, images, videos, and feed sources.
- Each resource can include content, such as words, phrases, and pictures, and can include embedded information, e.g., meta information and hyperlinks, or embedded instructions, e.g., JavaScript scripts.
- the evaluation engine 102 evaluates resource selection processes by comparing a group of resources for each process, for each of various queries.
- Each group of resources for a process and a query is the resources that the process would have included in an index and that are responsive to the query.
- the groups of resources for a query are the group of resources A 108 a for automated resource selection process A and the group of resources B 108 b for automated resource selection process B.
- Different methods for evaluating resource selection processes by comparing groups of resources are described in more detail below with reference to FIGS. 3-5 .
- the evaluation engine 102 obtains the groups of resources 108 a and 108 b for each automated resource selection process from index selection engines 106 a and 106 b . While a separate index selection engine for each automated selection process is shown in FIG. 1 , in alternative implementations, multiple automated selection processes share the same index selection engine.
- the index selection engines 106 a and 106 b generate the groups of resources as follows. Each index selection engine 106 a and 106 b receives a query from the evaluation engine, sends the query to the resource source 104 , and receives resources responsive to the query, or data identifying the resources, from the resource source 104 .
- the resource source 104 stores data relating to resources, e.g., resources crawled to build a search engine index.
- the resource source 104 is configured to receive a query and return resources, or data identifying resources, responsive to the query.
- the resource source 102 is a query results table that maps queries to resources responsive to the queries and information about the resources. Example methods for building the resource source 104 are described in more detail below with reference to FIGS. 6-7 .
- the index selection engines 106 a and 106 b classify each resource as one that would be included in a search engine index by their respective automated resource selection processes or as one that would not be included in the index by their respective automated resource selection processes. All resources that are classified as ones that would be included in the index by the automated resource selection process are sent to the evaluation engine 102 as a group of resources.
- FIG. 2 is a flow chart of an example method 200 for selecting groups of resources for use in comparing automated resource selection processes.
- the example method 200 is described in reference to a system of one or more computers that performs the method 200 .
- the system can be, for example, the index selection evaluation system 100 described above with reference to FIG. 1 .
- the system identifies resources responsive to a test query ( 202 ), for example, by sending the test query to a resource source and receiving resources from the resource source.
- receives data identifying and characterizing the resources can include, for example, an identifier of each resource and one or more indexing signals describing the resource.
- Example indexing signals are the length of the resource, the words in the title of the resource, an identifier of the resource, the length of the body text of the resource, a query-independent quality score for the resource, and user selection information for the resource, for example, a click-through-rate for the resource.
- the indexing signals are signals describing the resource itself, independent of any specific query that the resource might be responsive to.
- the indexing signals can be stored in a resource representation of each resource, can be extracted from the resources as needed, or can be retrieved from a lookup table that stores index signals for resources.
- Each indexing signal for a resource can be accessed through an application programming interface (API) that specifies a naming convention for the indexing signals.
- API application programming interface
- the system determines, for each resource, whether a first automated resource selection process would classify the resource as to-be-indexed or as not-to-be-indexed, and selects all resources classified as to-be-indexed as a first group of resources ( 204 ).
- the determination of whether a given resource should be classified as to-be-indexed or not-to-be-indexed is made independently of the determinations made for any other resources. For example, the system can score each resource and compare the score to a threshold to determine the appropriate classification for the resource.
- the system determines whether a first automated resource selection process would classify the resource as to-be-indexed or not-to-be-indexed by applying a heuristic associated with the first automated resource selection process.
- the heuristic corresponds to the formula the first automated resource selection process uses to generate scores for resources from signals describing the resources. This results in a single query-independent index selection score that is a summary of all of the signals used by the heuristic.
- Each heuristic is specified by code that can be executed by the system. Each heuristic identifies the indexing signals it needs by invoking specific commands provided by an application programming interface (API).
- API application programming interface
- each heuristic is represented by a configuration file that identifies any parameters and formulas needed to generate the query-independent index selection score. A user can update the heuristics by editing the configuration file or editing any files that reference the configuration file and generate query-independent index selection scores.
- the system compares the query-independent index selection score for each resource to a threshold. If the index selection score for the resource satisfies, e.g., exceeds, the threshold, the resource is classified as to-be-indexed. Otherwise, the resource is classified as not-to-be-indexed.
- the threshold is a specified value that is associated with the first automated resource selection process. This value can be a fixed value.
- the threshold can be determined based on the range of index selection scores calculated using the heuristic for the index selection process and the capacity of the index. For example, before the system selects resources for any particular query, the system, or another system, for example, a score generator made up of one or more computers that generate query-independent index selection scores, generates scores for a fixed percentage of the resources in the subset. For example, the system or the score generator can select a subset of the resources and generate a query-independent index selection score for each resource in the subset. From this, the system can estimate a score cutoff that will result in an index of the desired size. The size of the index can be measured in various ways.
- the size can be the number of resources in the index or a resource cost of the resources in the index.
- An example resource cost is the physical storage space used to store data for the resources in the index. This resource cost can be estimated, for example, by a number of documents in the index or a number of tokens in the document.
- the system can sort the subset of resources according to the query-independent index selection scores. The system can then start from the best-scoring resource and work down, adding each resource's size to a running total until the total is approximately equal to the desired size of the index times the fixed percentage. The system can then select the score for the last considered resource as the threshold value.
- the system uses different thresholds for different types of resources.
- the system can have different thresholds for resources in different languages.
- the system can compare a query-independent quality score included in information received about each resource to the threshold, and classify resources whose query-independent quality score exceeds the threshold as to-be-indexed and resources whose query-independent quality score is less than or equal to the threshold as not-to-be-indexed.
- the query-independent index selection score can be calculated according to a more complicated heuristic involving one or more signals and rules for adjusting the final index selection score based on signal values, for example, rules based on the number of characters in the resource, the number of links to the resource.
- the system can start with a query-independent quality score for the resource, and then modify the score according to the following rules to obtain the final index selection score. If the identifier for the resource is longer than a pre-determined number of characters, the system multiplies the score by a first scoring factor. If the number of links to the resource from other resources is greater than a pre-defined number of links, the system multiplies the resulting score by a second scoring factor.
- the system determines, for each resource, whether a second automated resource selection process would classify the resource as to-be-indexed or as not-to-be-indexed, and selects all resources classified as to-be-indexed as a second group of resources ( 206 ).
- the system makes the determination for the second automated resource selection process much as the system makes the determination for the first automated resource selection process.
- the system caches the determination for one or more of the resources for the first automated resource selection process, the second automated resource selection process, or both automated resource selection processes. In these implementations, the system first checks the cache to see if a decision for a given resource is already stored before making the determination. If the decision is stored in the cache, the system uses the stored decision. Otherwise, the system makes the decision as described above.
- the system compares the first and second automated resource selection processes by comparing the first and second groups of resources ( 208 ).
- Example methods for comparing the processes by comparing the groups of resources are described in more detail below with reference to FIGS. 3-5 .
- FIG. 3 is a flow chart of an example method 300 for comparing automated resource selection processes by comparing groups of resources associated with the automated resource selection processes.
- the example method 300 is described with reference to a system of one or more computers that performs the method.
- the system can be, for example, the index selection evaluation system 100 described above with reference to FIG. 1 .
- the system presents, to one or more evaluators, search results for a first group of resources and a second group of resources for each of one or more test queries ( 302 ).
- Each evaluator is a human being that views the two groups of resources and provides feedback on which group of resources the evaluator thinks is better.
- Each evaluator can make his or her decision based on specified factors or can make his or her decision based on personal, subjective feelings about the relative quality of the resources.
- all of the evaluators evaluate search results corresponding to each of the one or more test queries. In other implementations, at least some evaluators evaluate search results for less than all of the one or more test queries.
- the system automatically evaluates the resources according to one or more predefined heuristics. For example, for each query, the system can identify the top ten resources according to a quality score for the resources. The system then evaluates each group of resources for the query by calculating the percentage of the top ten resources that are included in the group of resources selected for each resource selection process.
- the first group of resources for each test query corresponds to a first automated resource selection process
- the second group of resources for each test query corresponds to a second automated resource selection process.
- the first groups of resources and the second groups of resources can be selected, for example, as described above with reference to FIG. 2 .
- the system presents the resources by presenting search results corresponding to the resources, e.g., in a user interface presented to each evaluator.
- Each search result presents information about a resource.
- each search result can include a title of the resource, a URL that is an address of the resource, and an excerpt extracted from the resource.
- the system determines an order for the resources and presents the search results according to the order.
- the order can be the order that a search engine would use when presenting the search results to a user.
- the system can determine the order by instructing the search engine to determine a query-specific score for each resource before the resources are presented for a given query, and then ranking the search results according to the query-specific scores for the resources.
- the query-specific scores for the resources and each of a group of queries can be pre-computed by the search engine, e.g., at the time a resource source, such as a query results table, is constructed, and stored along with the resources identified for each query in the resource source.
- the system presents search results corresponding to all of the resources in first group of resources and corresponding to all of the resources in second group of resources. In other implementations, the system presents search results corresponding to a proper subset of those resources. For example, the system can present search results corresponding to the top ten resources (or a different number of resources) in the first group of resources and search results corresponding to the top ten resources (or a different number of resources) in the second group of resources, according to the order for the first group of resources and the order for the second group of resources.
- the system presents the search results in a manner that distinguishes the search results corresponding to the first group of resources from the search results corresponding to the second group of resources.
- the search results can be presented side-by-side, where the search results corresponding to the first group of resources are on one side of a display and the search results corresponding to the second group of resources are on the other side of the display.
- An example of this type of presentation is described in more detail below, with reference to FIG. 4 .
- the system can label the presentation of each search result, and use labels of one type for search results corresponding to resources in the first group of resources and labels of a different, second type for search results corresponding to resources in the second group of resources.
- the system receives feedback from the one or more evaluators ( 304 ).
- the feedback from an evaluator indicates whether the evaluator prefers the search results corresponding to the first set of resources or the search results corresponding to the second set of resources for each of one or more test queries.
- the feedback can optionally include an indication of how much the evaluator prefers the first set of search results or the second set of search results.
- each evaluator can provide a rating for the preferred set of search results.
- Each evaluator provides his or her feedback, for example, through an evaluation user interface. An example evaluation user interface is described in more detail below with reference to FIG. 4 .
- the system aggregates the feedback for each of the one or more test queries ( 306 ).
- the system aggregates the feedback to combine feedback received from multiple evaluators. For example, if the evaluation only indicates which set of search results was preferred, and six evaluators selected the first set of search results and two evaluators selected the second set of search results for a given test query, the system could count the number of selections of each set of search results, e.g., six and two, respectively. As another example, if the feedback includes a rating indicating how much a given set of search results was preferred, the system can sum the ratings for each set of search results. In other implementations, conventional statistical techniques are used to aggregate the ratings for the sets of search results.
- the system selects the first automated resource selection process or the second automated resource selection process according to the aggregated feedback ( 308 ).
- the system can make this selection according to various heuristics.
- the system aggregates the feedback across all test queries, and compares the aggregated feedback for the automated resource selection processes that are being tested. The system then selects the automated resource selection process having the highest aggregated feedback across all queries.
- the system determines a first number of test queries for which the number of evaluators that preferred the first set of search results exceeds the number of evaluators that preferred the second set of search results.
- the system also determines a second number of test queries for which the number of evaluators that selected the second set of search results exceeds the number of evaluators that selected the first set of search results. The system then compares the first number to the second number. If the first number exceeds the second number, the system selects the first automated index selection algorithm. If the second number exceeds the first number, the system selects the second automated index selection algorithm.
- the system further considers other factors when selecting one of the resource selection processes over the other. For example, the system can consider the cost of evaluating each resource according to the heuristic associated with the resource selection process.
- FIG. 4 illustrates an example graphical user interface for presenting two sets of search results 406 and 408 in response to the same query.
- the search results 406 and 408 correspond to resources that are selected according to two different automated resource selection processes.
- the user interface shown in FIG. 4 can be used, for example, to present sets of search results corresponding to resources selected according to different automated resource selection processes to evaluators and to receive feedback from the evaluators.
- the first set 406 includes search results corresponding to resources selected according to a first automated resource selection process.
- the second set 408 includes search results corresponding to resources selected according to a second automated resource selection process.
- the search results in both sets 406 and 408 are ordered according to an order that is the order a search engine would assign to the resources.
- An evaluator can select one set of search results over the other by dragging (e.g., with a mouse or other input device) the slider bar 410 between the left side of the display and the right side of the display.
- the evaluator indicates how much better one set of search results is as compared to the other set of search results by how far to the left or right he or she drags the slider bar.
- FIG. 5 is a flow chart of an example method 500 for comparing automated resource selection processes by comparing user selections of search results corresponding to different groups of resources associated with the automated resource selection processes.
- the example method 500 is described with reference to a system of one or more computers that performs the method.
- the system can be, for example, the index selection evaluation system 100 described above with reference to FIG. 1 .
- the system performs the following steps for each of one or more test queries to collect data comparing user assessment of resources selected according to a first automated resource selection process with user assessment of resources selected according to a second automated resource selection process.
- the system receives a test query, from each of a group of users, through a search engine user interface ( 502 ).
- the system presents first search results corresponding to a first group of resources to one or more first users in the group of users ( 504 ).
- the first group of resources is associated with a first automated resource selection process.
- the first group of resources can be identified, for example, as described above with reference to FIG. 2 .
- the system presents second search results corresponding to a second group of resources to one or more second users in the group of users ( 506 ).
- the second group of resources can be identified, for example, as described above with reference to FIG. 2 .
- the system presents the first search results and the second search results in an order corresponding to an order they would be assigned by a search engine.
- the system can determine the order, for example, as described above with reference to FIG. 4 .
- the first users and the second users are different.
- the system can decide whether a given user is in the first group or users or the second group of users according to conventional experiment techniques. For example, the system can use one or more heuristics to make this determination.
- the system randomly selects users as being in the first group or the second group.
- the system selects users in one physical location as the first group of users and users in a second different physical location as the second group of users.
- each user that issues one of the test queries is classified as either a first user or a second user.
- fewer than all of the users that issue one of the test queries are classified as either a first user or a second user.
- the system can classify a first percentage of the users as first users and can classify a second percentage of the users as second users. The rest of the users can be shown default search results.
- the system compares user assessment of the first search results and the second search results ( 508 ).
- the user assessment can take different forms.
- the system measures the user assessment by an aggregate click-through-rate for the search results.
- the click-through-rate for each individual search result can be calculated, for example, as follows:
- the system determines the aggregate click-through-rate for search results corresponding to a group of resources by summing the click-through-rates for each individual search result.
- Other techniques for determining an aggregate click-through-rate for example, averaging, can also be used.
- the system measures the user assessment by a length of time a user indicates interest in search results for the resources, e.g., by hovering a mouse or other input cursor over a search result, or by viewing the resource itself.
- the system analyzes the comparisons for each of the one or more test queries to select either the first automated resource selection process or the second automated resource selection process.
- the system can use conventional statistical techniques to determine which resource selection process was preferred by the users. For example, the system can aggregate the click-through-rates for the search result corresponding to each group of resources selected using the first resource selection process to obtain an overall click-through-rate for the first resource selection process and can aggregate the click-through-rates for the search results corresponding to each group of resources selected using the second resource selection process to obtain an overall click-through-rate for the second resource selection process, and then select the resource selection process with the higher overall click-through-rate.
- Similar techniques of aggregating and comparing can be used for other types of user assessment. For example, if user assessment is measured by the length of time a user views a resource, the system can aggregate or average the length of time for resources selected using the first resource selection process and can aggregate or average the length of time for resources selected using the second resource selection process.
- the system considers other factors in addition to the comparison of the user assessment, for example, as described in more detail above with reference to FIG. 4 .
- FIG. 6 is a flow chart of an example method 600 for generating a resource source that associates queries with resources responsive to the queries.
- the example method 600 will be described in reference to a system of one or more computers that performs the process.
- the system can be, for example, the index selection evaluation system 100 described above with reference to FIG. 1 , or a different system.
- the system selects queries for inclusion in a query results table ( 602 ).
- the system uses a heuristic to select queries from a group of candidate queries.
- the candidate queries can be all queries submitted by users to a search engine during a given time period, e.g., over the last three months.
- the system can then select queries from the group of candidate queries according to a selection heuristic.
- the system can randomly select queries, or can focus on rare queries by selecting queries that are submitted less than a threshold number of times by users.
- the system selects the queries so that a pre-determined number of queries from each of one or more locales are selected.
- a locale is, for example, a country, a language, or a country and a language pair.
- the system receives the sampled queries from another system that samples query logs.
- the system directly samples query logs.
- the query log data is maintained in anonymized form to protect user privacy. This does not affect the operations of the system.
- the system preferably takes actions to anonymize the query log data and protect user privacy.
- the system samples the query logs using programs implemented with a MapReduce framework and programming model. For example, the system can use a map step that processes the logs and outputs queries keyed by locale. The reduce step can then sample a pre-determined number N of queries from each locale using conventional statistical sampling techniques.
- the system uses people to select, or help select, queries. For example, a person or group of people can select the set of queries and instruct the system to use the selected queries.
- queries are selected that are expected to have a small number of matching resources, e.g., a number of resources that can be accommodated by the storage space allocated to the resource source.
- the system uses a combination of heuristic query selection and human intervention to select the queries.
- the system identifies resources that match one or more of the queries ( 604 ).
- the resources can be, for example, resources crawled by a search engine as part of the process of building a search engine index that have terms that match the terms of one or more of the queries.
- the system can identify the resources as they are being crawled, or can alternatively process data collected for the resources during the crawling process after the crawling process has been completed.
- the system considers all resources crawled by the search engine. In other implementations, the system considers all resources crawled by the search engine up to a predetermined depth. The depth can be selected to be deeper than the depth usually used when the search engine is building an index. When the selected depth is deeper than the depth used when the search engine is building an index, the system identifies resources that the search engine does not include in its index.
- the system determines if a given resource matches a given query by determining if one or more terms, e.g., words or phrases, in the query appear in the resource.
- the system modifies each query, for example, through stemming, normalization, adding synonyms, or other techniques, and tries to match the resource to the modified query rather than the original query.
- the system performs the same query modifications that the search engine for which the automated index selection algorithms are being tested would perform.
- the system also obtains a query-specific score for each resource for each query matched to the resource, for example, by requesting a query-specific score for the resource and the matched query from the search engine for which the automated resource selection processes are being tested.
- the query-specific score can later be used to rank resources responsive to a query, for example, as described above with reference to FIG. 3 .
- the system rather than obtaining a query-specific score for each resource for each query matched to the resource, the system only obtains a query-specific score for a resource for a given query when the resource is estimated to have a good query-specific score for the given query, e.g., estimated to have a query-specific score that satisfies a score threshold.
- a score threshold An example method for determining which matches are estimated to have a good query-specific score is described in more detail below with reference to FIG. 7 .
- the system stores data associating each query with resources matched to the query ( 606 ). In implementations where the system determined a query-specific score for each resource for each matched query, the system associates each query with each matched resource and the query-specific score for the resource. In implementations where the system only determined a query-specific score for some of the matched resources and queries, the system only associates the queries with resources that were scored for the queries.
- the system assigns an order to the resources matching each query according to the associated query-specific scores for the resources and the query.
- the system identifies index selection signals for inclusion in the resource source.
- Each index selection signal is derived from one or more resource attributes.
- the identified index selection signals include all signals used by any of the resource selection processes that may be tested.
- the system can obtain index selection signal values for each resource and store the obtained index signal values along with the data associating each query with all matching resources.
- the index selection values can be obtained, for example, by accessing code referenced through the application programming interface (API) for index selection signals.
- API application programming interface
- the resource source can be structured in various ways.
- the resource source can be multiple tables that are stored on multiple machines. Each table can correspond to a portion of the data.
- the index selection signals can be stored separately from the query-specific ranking scores for each document.
- FIG. 7 is a flow chart of an example method 700 for determining whether to obtain a query-specific score for a matched resource and query, and then obtaining the query-specific score when appropriate.
- the example method 700 will be described in reference to a system of one or more computers that performs the process.
- the system can be, for example, the index selection evaluation system 100 described above with reference to FIG. 1 , or a different system.
- the system determines a score threshold from selected queries and resources ( 702 ).
- the selected queries can be selected, for example, as described above with reference to FIG. 6 .
- the resources are the resources being considered for inclusion in the resource source.
- the system obtains the score threshold as follows. First, the system matches a proper subset of the resources, e.g., 1% of the resources, against all of the selected queries. The system selects this subset, for example, using random sampling. The system then obtains a query-specific score for each resource for each query matched to the resource, for example, as described above with reference to FIG. 6 . The system then uses the obtained query-specific scores to calculate the score threshold.
- the system selects the score threshold according to the obtained query-specific scores and an amount of storage space allocated to the resource source.
- the system uses the subset of the resources as a sample of the entire population of resources, and selects the threshold accordingly. For example, if the system is trying to select N resources for each query, and the matched and scored subset of the resources is x percent of the total resources that match the query, the system identifies a threshold that would result in keeping matching resources having a total size of
- the system ranks the matches of resources and queries according to their query-specific scores for each query, identifies the resource and matched query that would result in a size of approximately
- N ⁇ x 100 uses the query-specific score of the identified resource for the identified query as the score threshold.
- the system determines an approximate score for each matched resource and query ( 704 ).
- the approximate score is calculated according to a heuristic designed to approximate the actual query-dependent score for the resource, but with less computational overhead.
- the algorithms used to generate the approximate score can be optimized for the comparison of one query to many documents. In some implementations, the algorithms are selected to give a conservatively high estimate of the score that the full scoring function will assign.
- the system obtains a score for each matched resource and query having an approximate score that satisfies the score threshold ( 706 ).
- the system can obtain the score, for example, as described above with reference to FIG. 6 .
- the system selects the threshold and performs the matching and scoring of the resources using programs implemented with a MapReduce framework and programming model.
- the system determines the threshold as follows.
- the system performs a map step that loads the queries into memory and processes the subset of resources one at a time. For each resource, the map step finds all matching queries and estimates a score for each matched resource and query, as described above.
- the output of the map step is the query mapped to the estimated scores for each resource matched to the query.
- the system then performs a reduce step that sorts the resources for each query by the estimated scores and identifies the appropriate threshold as described above.
- the system performs the matching and scoring to build the full resource source as follows.
- the system first performs a map step that loads all queries into memory and processes one resource at a time. For each resource, the system identifies all matching queries, calculates the score estimate for each query, and if the score estimate is above a threshold, calculates the full score for the query and the resource.
- the map step outputs queries mapped to resources and full scores, along with any data needed to return search results to users.
- the reduce step then sorts the resources for each query by score.
- the system then performs a second map reduce that associates any needed index selection signal values with each resource.
- the map reduce for generating the full resource source can be split into multiple map reduces for different resources.
- the second map reduce can also merge the results from the multiple map reduces.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a computer storage medium for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on a propagated signal that is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending resources to and receiving resources from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
The system ranks the matches of resources and queries according to their query-specific scores for each query, identifies the resource and matched query that would result in a size of approximately
, and uses the query-specific score of the identified resource for the identified query as the score threshold.
Claims (31)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/912,229 US8489604B1 (en) | 2010-10-26 | 2010-10-26 | Automated resource selection process evaluation |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/912,229 US8489604B1 (en) | 2010-10-26 | 2010-10-26 | Automated resource selection process evaluation |
Publications (1)
Publication Number | Publication Date |
---|---|
US8489604B1 true US8489604B1 (en) | 2013-07-16 |
Family
ID=48749164
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/912,229 Expired - Fee Related US8489604B1 (en) | 2010-10-26 | 2010-10-26 | Automated resource selection process evaluation |
Country Status (1)
Country | Link |
---|---|
US (1) | US8489604B1 (en) |
Cited By (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120259831A1 (en) * | 2011-04-05 | 2012-10-11 | Microsoft Corporation | User Information Needs Based Data Selection |
US20140059062A1 (en) * | 2012-08-24 | 2014-02-27 | Google Inc. | Incremental updating of query-to-resource mapping |
US20150017987A1 (en) * | 2011-11-15 | 2015-01-15 | Samsung Electronics Co., Ltd. | Method and device for distributing idle user equipment in multi-carrier based mobile communication system |
US9189526B1 (en) * | 2012-03-21 | 2015-11-17 | Google Inc. | Freshness based ranking |
US20180101554A1 (en) * | 2012-12-31 | 2018-04-12 | Ebay Inc. | Next generation near real-time indexing |
US10380124B2 (en) * | 2016-10-06 | 2019-08-13 | Oracle International Corporation | Searching data sets |
US10592749B2 (en) | 2016-11-14 | 2020-03-17 | General Electric Company | Systems and methods for analyzing turns at an airport |
US10635726B2 (en) * | 2016-06-20 | 2020-04-28 | Silicon Motion, Inc. | Data processing circuit and data processing method |
US10834336B2 (en) | 2018-01-29 | 2020-11-10 | Ge Aviation Systems Llc | Thermal imaging of aircraft |
US11106722B2 (en) * | 2018-05-07 | 2021-08-31 | Apple Inc. | Lyric search service |
US11468098B2 (en) * | 2013-04-11 | 2022-10-11 | Oracle International Corporation | Knowledge-intensive data processing system |
US11528690B2 (en) * | 2017-02-11 | 2022-12-13 | Huawei Technologies Co., Ltd. | Data communication method, apparatus, and system |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5920854A (en) | 1996-08-14 | 1999-07-06 | Infoseek Corporation | Real-time document collection search engine with phrase indexing |
US20040117367A1 (en) | 2002-12-13 | 2004-06-17 | International Business Machines Corporation | Method and apparatus for content representation and retrieval in concept model space |
US20040186827A1 (en) | 2003-03-21 | 2004-09-23 | Anick Peter G. | Systems and methods for interactive search query refinement |
US20050071465A1 (en) | 2003-09-30 | 2005-03-31 | Microsoft Corporation | Implicit links search enhancement system and method for search engines using implicit links generated by mining user access patterns |
US20060117002A1 (en) * | 2004-11-26 | 2006-06-01 | Bing Swen | Method for search result clustering |
US20060287993A1 (en) | 2005-06-21 | 2006-12-21 | Microsoft Corporation | High scale adaptive search systems and methods |
US20100257164A1 (en) * | 2009-04-07 | 2010-10-07 | Microsoft Corporation | Search queries with shifting intent |
US20110022590A1 (en) * | 2009-07-23 | 2011-01-27 | Hwanjo Yu | Method of performing database search using relevance feedback and storage medium having program recorded thereon for executing the same |
US8255386B1 (en) | 2008-01-30 | 2012-08-28 | Google Inc. | Selection of documents to place in search index |
-
2010
- 2010-10-26 US US12/912,229 patent/US8489604B1/en not_active Expired - Fee Related
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5920854A (en) | 1996-08-14 | 1999-07-06 | Infoseek Corporation | Real-time document collection search engine with phrase indexing |
US20040117367A1 (en) | 2002-12-13 | 2004-06-17 | International Business Machines Corporation | Method and apparatus for content representation and retrieval in concept model space |
US20040186827A1 (en) | 2003-03-21 | 2004-09-23 | Anick Peter G. | Systems and methods for interactive search query refinement |
US20050071465A1 (en) | 2003-09-30 | 2005-03-31 | Microsoft Corporation | Implicit links search enhancement system and method for search engines using implicit links generated by mining user access patterns |
US20060117002A1 (en) * | 2004-11-26 | 2006-06-01 | Bing Swen | Method for search result clustering |
US20060287993A1 (en) | 2005-06-21 | 2006-12-21 | Microsoft Corporation | High scale adaptive search systems and methods |
US8255386B1 (en) | 2008-01-30 | 2012-08-28 | Google Inc. | Selection of documents to place in search index |
US20100257164A1 (en) * | 2009-04-07 | 2010-10-07 | Microsoft Corporation | Search queries with shifting intent |
US20110022590A1 (en) * | 2009-07-23 | 2011-01-27 | Hwanjo Yu | Method of performing database search using relevance feedback and storage medium having program recorded thereon for executing the same |
Non-Patent Citations (1)
Title |
---|
U.S. Appl. No. 13/617,198, filed Sep. 14, 2012, Automated Resource Selection Process Evaluation, Adam Sadovsky et al. |
Cited By (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9589056B2 (en) * | 2011-04-05 | 2017-03-07 | Microsoft Technology Licensing Llc | User information needs based data selection |
US20120259831A1 (en) * | 2011-04-05 | 2012-10-11 | Microsoft Corporation | User Information Needs Based Data Selection |
US10721667B2 (en) | 2011-11-15 | 2020-07-21 | Samsung Electronics Co., Ltd. | Method and device for distributing idle user equipment in multi-carrier based mobile communication system |
US20150017987A1 (en) * | 2011-11-15 | 2015-01-15 | Samsung Electronics Co., Ltd. | Method and device for distributing idle user equipment in multi-carrier based mobile communication system |
US9332470B2 (en) * | 2011-11-15 | 2016-05-03 | Samsung Electronics Co., Ltd | Method and device for distributing idle user equipment in multi-carrier based mobile communication system |
US11950158B2 (en) | 2011-11-15 | 2024-04-02 | Samsung Electronics Co., Ltd. | Method and device for distributing idle user equipment in multi-carrier based mobile communication system |
US10237800B2 (en) | 2011-11-15 | 2019-03-19 | Samsung Electronics Co., Ltd. | Method and device for distributing idle user equipment in multi-carrier based mobile communication system |
US11102692B2 (en) | 2011-11-15 | 2021-08-24 | Samsung Electronics Co., Ltd. | Method and device for distributing idle user equipment in multi-carrier based mobile communication system |
US9189526B1 (en) * | 2012-03-21 | 2015-11-17 | Google Inc. | Freshness based ranking |
US20140059062A1 (en) * | 2012-08-24 | 2014-02-27 | Google Inc. | Incremental updating of query-to-resource mapping |
US11216430B2 (en) * | 2012-12-31 | 2022-01-04 | Ebay Inc. | Next generation near real-time indexing |
US20180101554A1 (en) * | 2012-12-31 | 2018-04-12 | Ebay Inc. | Next generation near real-time indexing |
US11468098B2 (en) * | 2013-04-11 | 2022-10-11 | Oracle International Corporation | Knowledge-intensive data processing system |
US10635726B2 (en) * | 2016-06-20 | 2020-04-28 | Silicon Motion, Inc. | Data processing circuit and data processing method |
US10380124B2 (en) * | 2016-10-06 | 2019-08-13 | Oracle International Corporation | Searching data sets |
US10592749B2 (en) | 2016-11-14 | 2020-03-17 | General Electric Company | Systems and methods for analyzing turns at an airport |
US11528690B2 (en) * | 2017-02-11 | 2022-12-13 | Huawei Technologies Co., Ltd. | Data communication method, apparatus, and system |
US10834336B2 (en) | 2018-01-29 | 2020-11-10 | Ge Aviation Systems Llc | Thermal imaging of aircraft |
US11106722B2 (en) * | 2018-05-07 | 2021-08-31 | Apple Inc. | Lyric search service |
US11573998B2 (en) | 2018-05-07 | 2023-02-07 | Apple Inc. | Lyric search service |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8489604B1 (en) | Automated resource selection process evaluation | |
US20230205828A1 (en) | Related entities | |
US9811566B1 (en) | Modifying search result ranking based on implicit user feedback | |
CA2781321C (en) | Cross-language search options | |
US9229989B1 (en) | Using resource load times in ranking search results | |
US10726083B2 (en) | Search query transformations | |
US9183499B1 (en) | Evaluating quality based on neighbor features | |
US9679027B1 (en) | Generating related questions for search queries | |
US8819006B1 (en) | Rich content for query answers | |
US9135307B1 (en) | Selectively generating alternative queries | |
US9116992B2 (en) | Providing time series information with search results | |
US9275113B1 (en) | Language-specific search results | |
US20090287645A1 (en) | Search results with most clicked next objects | |
US9521189B2 (en) | Providing contextual data for selected link units | |
RU2012138707A (en) | CUSTOMIZABLE SEMANTIC SEARCH BASED ON USER ROLE | |
CA2718621A1 (en) | Social network powered query refinement and recommendations | |
US20150169576A1 (en) | Dynamic Search Results | |
US9916384B2 (en) | Related entities | |
US8949229B1 (en) | Measuring video content of web domains | |
US20140059062A1 (en) | Incremental updating of query-to-resource mapping | |
US10169711B1 (en) | Generalized engine for predicting actions | |
US9424338B2 (en) | Clustering queries for image search | |
US9110943B2 (en) | Identifying an image for an entity | |
US9760641B1 (en) | Site quality score | |
Zhitomirsky-Geffet et al. | Testing the stability of “wisdom of crowds” judgments of search results over time and their similarity with the search engine rankings |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SADOVSKY, ADAM;HAAHR, PAUL;STROHMAN, TREVOR;AND OTHERS;REEL/FRAME:025487/0001Effective date: 20101021 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
CC | Certificate of correction | ||
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20210716 |