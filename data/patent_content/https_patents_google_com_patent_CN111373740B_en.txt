CN111373740B - Method for converting horizontal video into vertical movement layout by using selection interface - Google Patents
Method for converting horizontal video into vertical movement layout by using selection interface Download PDFInfo
- Publication number
- CN111373740B CN111373740B CN201880075932.9A CN201880075932A CN111373740B CN 111373740 B CN111373740 B CN 111373740B CN 201880075932 A CN201880075932 A CN 201880075932A CN 111373740 B CN111373740 B CN 111373740B
- Authority
- CN
- China
- Prior art keywords
- feature
- frame
- region
- score
- frames
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims abstract description 82
- 230000033001 locomotion Effects 0.000 title claims description 50
- 239000013598 vector Substances 0.000 claims description 18
- 230000004044 response Effects 0.000 claims description 15
- 230000002123 temporal effect Effects 0.000 claims description 5
- 230000008859 change Effects 0.000 description 41
- 230000003068 static effect Effects 0.000 description 24
- 238000001514 detection method Methods 0.000 description 23
- 238000010586 diagram Methods 0.000 description 21
- 238000004458 analytical method Methods 0.000 description 16
- 238000012015 optical character recognition Methods 0.000 description 16
- 238000013500 data storage Methods 0.000 description 15
- 230000001815 facial effect Effects 0.000 description 13
- 230000008569 process Effects 0.000 description 13
- 238000004422 calculation algorithm Methods 0.000 description 11
- 238000012545 processing Methods 0.000 description 11
- 238000004590 computer program Methods 0.000 description 9
- 230000003287 optical effect Effects 0.000 description 9
- 230000000007 visual effect Effects 0.000 description 9
- 241001465754 Metazoa Species 0.000 description 7
- 230000004927 fusion Effects 0.000 description 7
- 238000013135 deep learning Methods 0.000 description 6
- 238000007781 pre-processing Methods 0.000 description 6
- 238000012549 training Methods 0.000 description 6
- 230000005540 biological transmission Effects 0.000 description 5
- 238000006243 chemical reaction Methods 0.000 description 5
- 238000010191 image analysis Methods 0.000 description 5
- 230000000717 retained effect Effects 0.000 description 5
- 230000003993 interaction Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 239000003550 marker Substances 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000012731 temporal analysis Methods 0.000 description 3
- 238000013528 artificial neural network Methods 0.000 description 2
- 230000001427 coherent effect Effects 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/20—Image preprocessing
- G06V10/25—Determination of region of interest [ROI] or a volume of interest [VOI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04845—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range for image manipulation, e.g. dragging, rotation, expansion or change of colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/20—Drawing from basic elements, e.g. lines or circles
-
- G06T3/10—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/11—Region-based segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/05—Underwater scenes
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G5/00—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators
- G09G5/36—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators characterised by the display of a graphic pattern, e.g. using an all-points-addressable [APA] memory
- G09G5/37—Details of the operation on graphic patterns
- G09G5/373—Details of the operation on graphic patterns for modifying the size of the graphic pattern
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20112—Image segmentation details
- G06T2207/20132—Image cropping
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2210/00—Indexing scheme for image generation or computer graphics
- G06T2210/12—Bounding box
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/04—Changes in size, position or resolution of an image
- G09G2340/0442—Handling or displaying different aspect ratios, or changing the aspect ratio
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/04—Changes in size, position or resolution of an image
- G09G2340/045—Zooming at least part of an image, i.e. enlarging it or shrinking it
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N7/00—Television systems
- H04N7/01—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level
- H04N7/0117—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level involving conversion of the spatial resolution of the incoming video signal
- H04N7/0122—Conversion of standards, e.g. involving analogue television standards or digital television standards processed at pixel level involving conversion of the spatial resolution of the incoming video signal the input and the output signals having different aspect ratios
Abstract
Systems and methods of converting media dimensions are described herein. A device may identify a set of frames in a video in a first orientation as belonging to a scene. The device may receive selected coordinates on frames of a set of frames for a scene. The device may identify a first region within the frame that includes a first feature corresponding to the selected coordinates and a second region within the frame that includes a second feature. The device may generate a first score for the first feature and a second score for the second feature. The first score may be greater than the second score based on the first feature corresponding to the selected coordinate. The device may crop a frame to include a first area and a second area within a predetermined display area, the predetermined display area including a subset of the areas of the frame in a second orientation.
Description
Cross Reference to Related Applications
This application claims the benefit and priority of PCT/US2017/064719 entitled "method of converting landscape video to portrait shifted layout" filed on year 2017, month 12 and 05 and the benefit and priority of PCT/US2016/065025 entitled "method of converting landscape video to portrait shifted layout" filed on year 2016, month 12 and 05, the entire contents of which are incorporated herein by reference.
Background
In a networked environment, such as the internet or other network, a first party content provider may provide information for public presentations on resources, such as web pages, documents, applications, and/or other resources. The first party content may include text, video and/or audio information provided by the first party content provider via the resource server for presentation on the client device over the internet. Videos and similar media recorded with a wide aspect ratio (aspect ratio) may be designed to be viewed on a desktop or in a landscape (landscaped) orientation, they may not be displayed directly full-screen on a mobile device held in a vertical or portrait (portal) orientation, and may often be cropped to the center, lose detail of the left and right edges of the video, or be surrounded by black bars at the top and bottom, thereby reducing the display size of the video. In many applications, vertically oriented media is a popular format for viewing and displaying media. Since many videos and other media are only recorded in wide aspect ratio layouts, there is a large inventory of such layouts and there is an increasing demand from publishers for vertical layout requirements.
Disclosure of Invention
Most media may be in the lateral dimension rather than the longitudinal dimension. In contrast, the display on most mobile phones may be taller rather than wider and therefore may be more suitable for content in the portrait dimension. Thus, when media in the landscape dimension is presented on the display in the portrait dimension, there may be a large amount of negative space (negative space) around the media. Techniques to eliminate negative space may include cropping media in the lateral dimension to accommodate the longitudinal dimension. Each frame in the media may be cropped based on features identified within the frame. The detected features may include face tracking, object detection and/or recognition, text detection, dominant color detection, motion analysis, scene change detection, and image saliency. However, the identification of multiple salient features in a frame may cause the cropping of media to include other features that are not relevant to the viewer. Sub-optimal cropping may result in the need to regenerate the cropped media.
To address sub-optimal clipping, the present technology can clip media with human-assisted marking of salient features. In one embodiment, the system may apply a scene boundary detection algorithm to identify a scene of the media. The graphical user interface may allow a user to mark coordinates corresponding to salient features in the frames of the media for each identified scene. The computing device may identify salient features from the coordinates marked by the user.
The system may then traverse the media frames of the scene to identify other features using various image object recognition algorithms. For each of the other identified features, the system may calculate a weight. However, features marked by the user may be assigned the highest weight. The system may crop features of the media to a size of a longitudinal dimension of a display of the mobile computing device based on weights of the features marked by the user and weights of other features identified by the object recognition algorithm.
This process may be repeated multiple times to obtain media in the vertical dimension. While the prior art may have resulted in sub-optimal cropping and subsequent need to regenerate the cropped media, the present technology provides an improved user interface that allows users to generate videos more efficiently. By obtaining user input in this process, the present technology may allow for more relevant media, thereby obtaining more accurate media more quickly and reducing the number of times media needs to be regenerated. By giving higher weight to features marked by the user, this can help ensure that the weight applied by the system to other identified features does not negatively impact the cropped media. Furthermore, although described primarily with respect to cropping or conversion from portrait video to landscape video, the same techniques may be used for cropping or conversion from landscape video to portrait video, such as for vertical displays, conversion of traditional content for mobile phone consumption, and so forth.
Drawings
The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the disclosure will become apparent from the description, the drawings, and the claims, wherein:
FIG. 1 is a block diagram depicting an embodiment of an environment for automatic transition of media from one orientation to another.
Fig. 2A is a diagram depicting the cropping of a single media frame in one embodiment of a use case.
Figure 2B is a diagram depicting the cropping of a single media frame based on the selection of a bounding box in one use case embodiment.
Fig. 2C and 2D are block diagrams depicting a graphical user interface of an application for cropping media frames using selection data.
Fig. 3 is a block diagram depicting an embodiment of software and/or hardware modules configured for media pre-processing, media analysis, and cropping of received media.
FIG. 4 is a flow diagram depicting an embodiment of a method of cropping a media frame.
FIG. 5 is a flow diagram depicting an embodiment of a method of cropping a media frame by determining a score for each of a plurality of regions.
FIG. 6 is a flow diagram depicting an embodiment of a method of generating or updating a score based on movement of a feature.
FIG. 7 is a flow diagram depicting an embodiment of a method for cropping media frames using received metadata.
FIG. 8 is a flow diagram depicting an embodiment of a method of adjusting cropping based on orientation change.
FIG. 9 is a flow diagram depicting an embodiment of a method for cropping a media frame using iterative (iterative) of selection data.
FIG. 10 is a block diagram depicting a general architecture of a computing system that may be used to implement various elements of the systems and methods described and illustrated herein.
It will be appreciated that some or all of the figures are schematic representations for purposes of illustration. The drawings are provided for the purpose of illustrating one or more embodiments and are to be clearly understood as not being used to limit the scope or meaning of the claims.
Detailed Description
Following are more detailed descriptions of various concepts related to methods, apparatus, and systems for providing information over a computer network and embodiments thereof. The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways, as the concepts described are not limited to any particular implementation. Various embodiments and applications are provided primarily for illustrative purposes.
"dumb" cropping to the center of a picture or video is based on an assumption that the most important content of the video or picture appears in the center of the picture. This may be incorrect because important content may be off center, such as text at the bottom of the screen or a person in the horizontal "third" position in the frame. Displaying a video of a person entering a room may include the person entering from one side of a frame while the rest of the frame is still, or a video of two people talking may have each person placed on both sides of the frame with empty space between them. Applying center cropping in this case may result in an empty room or blank space, which may be confusing and frustrating to the viewer.
Thus, disclosed herein are systems and methods for intelligent or "smart" cropping to automatically convert video or images in landscape mode to accommodate portrait mode, and vice versa, while remaining full screen, rather than just applying "non-intelligent" or center cropping or adding padding to the top/bottom. The conversion may include detecting important portions (e.g., features) of the image or video for each frame. Based on the identified important regions, the image or video may be intelligently cropped or padded to preserve important features while discarding unimportant areas, static boundaries, and the like. The detected features may include face tracking, object detection and/or recognition, text detection, dominant color detection, motion analysis, scene change detection, and image saliency. Detection and identification may use deep learning based methods and algorithms. Text detection may use Optical Character Recognition (OCR). Feature detection allows for optimal clipping paths. Other aspects of the invention may include filling the image to match the background color, and removing and/or reformatting any borders to accommodate the new display mode. Although discussed primarily with respect to video, in many embodiments, the system may be applied to individual images or frames.
FIG. 1 is a block diagram of an embodiment of an environment 100 for automatically transitioning a video from one orientation to another orientation over a network 106. Network 106 may include a Local Area Network (LAN), a Wide Area Network (WAN), a telephone network, such as the Public Switched Telephone Network (PSTN), a wireless link, an intranet, the internet, or a combination thereof. The environment 100 also includes a mobile device 102. In some implementations, the mobile device 102 includes a processor 122, a data storage 124, a network interface 126, a display 128, an input/output module 130, a sensor module 132, and a media module 134. Sensor module 132 may include sensors (e.g., accelerometers and/or magnetometers) for detecting orientation of a computing device, as well as other similar sensors included in many mobile devices. The processor 122 may include a microprocessor, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), the like, or combinations thereof. The data storage 124 may include, but is not limited to, electronic, optical, magnetic storage or transmission devices or any other storage or transmission devices capable of providing a processor with program instructions. The memory may include a floppy disk, a compact disk read only memory (CD-ROM), a Digital Versatile Disk (DVD), a magnetic disk, a memory chip, a Read Only Memory (ROM), a Random Access Memory (RAM), an Electrically Erasable Programmable Read Only Memory (EEPROM), an Erasable Programmable Read Only Memory (EPROM), a flash memory, an optical medium, or any other suitable memory from which processor 122 may read instructions. The instructions may include code from any suitable computer programming language, such as, but not limited to C, C + +, C #, and,
The mobile device 102 may run a software application (e.g., a web browser or other application) to retrieve content from other computing devices over the network 106. Such an application may retrieve first-party content from the media server system 104. In some cases, the application running on the mobile device 102 may itself be first-party content (e.g., games, media players, etc.). In one embodiment, mobile device 102 may run a web browser application that provides a browser window on a display of a client device. A web browser application providing a browser window may operate by receiving an input of a Uniform Resource Locator (URL), such as a web address, from an input device, such as a pointing device, a keyboard, a touch screen, or another form of input device. In response, the one or more processors of the client device running the instructions from the web browser application may request data from another device (e.g., media server system 104) connected to network 106, where the other device is referred to by a URL address. The other device may then provide the web page data and/or other data to the mobile device 102, which causes a display of the mobile device 102 to display visual indicia (indicia). Accordingly, the browser window displays the retrieved first-party content, such as web pages from various websites, to facilitate user interaction with the first-party content.
In some implementations, the media module 134 of the mobile device 102 can receive a plurality of media frames and associated metadata. The media may include video (e.g., video frames), images, photographs, rendered content, panoramic content, three-dimensional content, or any other type and form of visual media content, and may be accompanied by audio content. Media may be received and stored in the data storage 124 through the network interface 126. In some implementations, the media frame is received as part of streaming media data. Streaming media may be received through network interface 146. In some implementations, the media module 134 can identify regions in the frame based on the received metadata. In some implementations, the media module 134 may crop the media frame based on the region. In some implementations, the cropped area is also based on one or more of the media frames preceding and/or following the media frame.
In some implementations, the media module 134 of the mobile device 102 can receive an indication of an orientation change from one or more sensor modules 132. In some implementations, the media module 134 can dynamically adjust the cropping of the playing media based on the orientation change.
The media server system 104 may include a processor 142, a data storage 144, a network interface 146, a content selection module 148, a media cropping module 150, a metadata module 152, and a media content database 154. In some implementations, the content selection module 148 of the media server system 104 can select media from the media content database 154. In some implementations, the media cropping module 150 may pre-process the media, analyze features and/or objects of the media, and crop the media based on the analysis of the features and/or objects. In some implementations, the metadata module 152 may extract data based on pre-processing the media, analyzing features and/or objects of the media, and determining a clipping path for a target aspect ratio or resolution. Although shown on the media server system 104, in many implementations, the media cropping module 150 may run on one or more mobile devices 102.
The media server system 104 may include a media cropping module 150. In some implementations, the media cropping module 150 may pre-process the media, analyze features and/or objects of the media, and crop the media based on the analysis of the features and/or objects. In some implementations, the media cropping module 150 may determine whether cropping is needed based on whether one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame being analyzed. In some implementations, the media cropping module 150 may crop a media frame only if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media cropping module 150 may crop the media to match a target aspect ratio or to match a target resolution. The media cropping module 150 may add additional padding to one or more edges of the cropped media frame to match the target aspect ratio or to match the target resolution. In some implementations, the media cropping module 150 may also base the cropping area on one or more of the media frames before and/or after the current media frame being cropped. In some implementations, the media clipping module 150 may include one or more regions that exceed a threshold. In some implementations, when the media cropping module 150 is determining the regions to include when cropping the media frame, the media cropping module 150 may include at least one or more of the plurality of regions having a score that exceeds a threshold.
In some implementations, the metadata module 152 may extract data based on pre-processing the media, analyzing features and/or objects of the media, and determining a clipping path for a target aspect ratio or resolution. In some implementations, the metadata module 152 may receive the metadata as part of a media file containing a plurality of media frames. In some implementations, the metadata module 152 may receive the metadata independently, along with an identifier or other data that associates the metadata with the received plurality of media frames. In some implementations, the metadata module 152 can analyze the metadata to determine portions of the data related to regions associated with one or more of the media frames associated with the media. In some implementations, the metadata module 152 can extract bounding information included in the metadata for one or more regions of each of the plurality of media frames. In some implementations, the metadata module 152 may extract the location of one or more features within each of the plurality of media frames. Features may include objects (such as cars, buildings, people, animals, street signs, etc.), text, boundaries of media frames, uniform color fill of one or more edges of media frames, and so forth. In some implementations, the metadata module 152 may identify a plurality of features and/or regions of one or more of the plurality of media frames. In some implementations, the metadata module 152 may associate the received metadata with a target aspect ratio or a target resolution.
Fig. 2A depicts cropping a single media frame in an embodiment of a use case. The input image 202 may be in a horizontal or lateral orientation as shown. Once the input image 202 is processed to detect important objects and/or features of the input image 202, regions containing the important objects and/or features may be retained until the output image 204 is generated. In fig. 2A, the output image 204 is shown in a vertical or portrait orientation displayed on the mobile device 102. In this description of an embodiment of the use case, facial features are identified in the input image 202 and the region including the facial features is retained in the output image 204 displayed on the mobile device 102.
Fig. 2B depicts cropping a single media frame in an embodiment of a use case. The input image 206 may be in a horizontal or lateral orientation as shown. The input image 206 may be determined to have a number of features: a character 208A on a bicycle and two characters 208B. The bounding box 210 may be used to select a rectangular area containing the character 208A on the bicycle. Once the input image 206 with the bounding box 210 is processed, the area containing the selected feature of the person 208A on the bicycle may be retained until the output image 212 is generated. As shown in fig. 2B, the output image 212 is shown in a vertical or portrait orientation displayed on the mobile device 102. In the description of the embodiment of this use case, the person 208A on the bicycle selected using the bounding box 210 is retained in the output image 204 displayed on the mobile device 102.
Fig. 2C and 2D illustrate graphical user interfaces of an application 220 for cropping media frames using selection data. In the context of fig. 1, the application 220 may be part of the media clipping module 150 and may be provided by the media clipping module 150. The application 220 may include executable instructions to be processed by one or more processors. The administrator may use the graphical user interface of the application 220 to automatically crop the frames of each scene in the media using the selection data. Beginning with FIG. 2C, the application 220 may include an original video preview 225, a cropped video preview 230, a scene list interface 235, a process button 240, and a finish button 245. The original video preview 225 may include a video player that plays back an unedited version of the media. Cropping video preview 230 may include a video player playing back a cropped version of the media. The scene list interface 235 may include one or more scene cuts 250A-250N, a time range 260A-260N for each scene cut 250A-250N, and a feature list 265A-265N for each scene cut 250A-250N. The media cropping module 150 may identify one or more scenes. Based on this identification, representative frames (e.g., intermediate frames) may be selected for display of the respective scene cuts 250A-250N. In addition, the time range 260A-260N for each scene may be identified and displayed within the scene list interface 235. Further, one or more features may be detected from the frames of each scene using image analysis techniques described in detail herein, and may be used to populate the feature lists 265A-265N.
Turning to FIG. 2D, for each scene identified, a graphical user interface may be used to make selections 280A-280N. The selections 280A-280N may be specific to frames of the scene 275A, and may specify coordinates or coordinate ranges within one frame of the scene 275A (e.g., the bounding box 210). The selections 280A-280N may correspond to features identified on frames of the scene 275A. Selection 280A-N may be used to crop frames of scenes 275A-275N upon interaction with process button 240. The cropped frames of scenes 275A-275N may be used to generate a cropped video for playback in cropped video preview 230. Additional or different selections 280A-280N may be made after the initial generation of cropped video iteratively updates or changes the cropping of the frames of each scene 275A-275N. By interacting with the done button 245, cropping of the media frame may be achieved.
FIG. 3 is a block diagram of an embodiment of software and/or hardware modules for media pre-processing, media analysis, and cropping received media. In some implementations, the pre-processing module 310 may pre-process the media and downconvert the media using the downconverter 312, the downconverter 314, the downconverter 316, the downconverter 318, and so on. As shown, the downconverter 312 and 318 may have different output settings. In some implementations, the pre-processing module 310 can send the resulting output to one or more of the time analyzer 320, the image analyzer 330, and the focus area analyzer 350. The time analyzer 320 may include a scene change detector 322 and a static boundary detector 324. The image analyzer 330 may include an OCR 332 module, an object detector 334, a face tracking 336 module, a motion analysis 338 module, and an entropy 345 module. The time analyzer 320 and the image analyzer 330 may send the data results to the signal fusion calculator 360 and the cropping calculator 362. In some embodiments, the data results from the time analyzer 320 and the image analyzer 330 may be sent to a focus area analyzer 350. The focus area analyzer 350 may send selection data to the image analyzer 330, the signal fusion calculator 360, and the cropping calculator 362.
Although shown separately, in many embodiments, the temporal analysis module and the image analysis module may be part of the same analyzer system or module. Similarly, the components shown within the temporal analysis module and the image analysis module may be separate from the temporal analysis module or the image analysis module, or may be provided by other modules. In some implementations, the image analyzer 330 can incorporate a deep learning inference model that can be trained using input data. In some embodiments, the input data may be input based on the marked area or the selected area.
In some implementations, the time analyzer 320 may include an application, applet, service, server, daemon (daemon), routine, or other executable logic for performing analysis on a sequence of images, such as images of a video. The time analyzer 320 may include a scene change detector 322, and the scene change detector 322 may analyze a plurality of media frames to determine a scene change. Scene change detector 322 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying differences between successive images that indicate a scene change or significant interruption in a video. In some implementations, the scene change detector 322 can determine a scene change by using keypoint detection to analyze when there is a large change in keypoints indicating a scene break or scene change. In some implementations, scene-change detector 322 may compare all pixels in one frame to pixels in consecutive frames, and if more than a certain threshold of pixels are different when considered as part of the optical flow, this may be an indication of a scene change. Based on the comparison of the pixels, the scene change detector 322 may identify one or more sets of frames in the video, each set belonging to a particular scene.
In some implementations, scene change detector 322 may calculate motion vectors between multiple media frames, and the absence of coherent motion vectors between successive frames indicates a scene change. Features may then be identified within a particular scene and regions containing the particular features tracked among a plurality of media frames within the particular scene. In some implementations, the scene change detector 322 can track information of the location of particular features within multiple media frames, and such information is also used to determine where to crop media frames based on the region.
In some implementations, the time analyzer 320 can include a static boundary detector 324, and the static boundary detector 324 can analyze a plurality of media frames to determine if and where a static boundary exists. The static boundary detector 324 may include an application, applet, service, server, daemon, routine or other executable logic for identifying a static boundary that remains substantially unchanged between successive images indicating a boundary on at least one edge of a frame. In some implementations, the static boundary detector 324 may receive a plurality of media frames and be configured to analyze the plurality of media frames to find a static boundary along edges of the plurality of frames. In some implementations, the static boundary detector 324 can locate the boundary by selecting one or more random pixels and comparing the pixel rows vertically and/or horizontally to the random pixels to determine whether there is an uninterrupted pixel row that is a color similar to the randomly selected pixels. In some implementations, such rows of pixels can extend across the entire image, or a portion of the image (e.g., a quarter of the image). In some implementations, the static boundary detector 324 may locate a boundary that is static from one frame to the next and contains pixels that are relatively uniform in color. In some implementations, the static boundary detector 324 may locate a boundary that is static from one frame to the next and contains pixels that are relatively uniform in color, but also contains some additional static information (such as different color text embedded in the boundary). Once the boundary is located, whether or not it contains embedded text, it can be treated as an image during the cropping process.
In some implementations, the image analyzer 330 can include an Optical Character Recognition (OCR) analyzer 332, and the OCR analyzer 332 can detect text embedded in the image data. The image data may be one or more frames of media, such as video. OCR analyzer 332 may comprise an application, applet, service, server, daemon, routine or other executable logic for identifying text embedded in image data of one or more media frames. In some implementations, the OCR analyzer 332 can compare a predetermined vector or bitmap image corresponding to a letter to a portion of the image, such as via a sliding window. In some implementations, OCR analyzer 332 can select a reference image (e.g., a letter) based on previous letters (e.g., according to a text prediction system), which can improve efficiency.
In some implementations, the image analyzer 330 may include an object detector 334, and the object detector 334 may use a neural network trained on different objects, such as via tens, hundreds, or thousands of reference images of the object. Object detector 334 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying visual objects (i.e., data that, when displayed, creates a visual representation of an object) in one or more media frames. The object detector 334 may detect objects (such as cars, buildings, people, animals, street signs, etc.), text, boundaries of media frames, uniform color fill of one or more edges of media frames, and so forth. The neural network may identify similar elements in an image of the object and create a classification of the elements representing the object, which may then be used to identify the object in the new image. In some embodiments, object detector 334 may use the semantic knowledge graph to identify one or more keywords associated with each object identified in one or more frames of the video. The semantic knowledge graph may include a mapping between visual objects and one or more keywords. The image analyzer 330 may generate a bounding box around the identified object such that the bounding box may be tracked from one image to another.
In some implementations, the image analyzer 330 module may include a face tracking 336 module, which may receive and analyze a plurality of media frames to detect facial features, e.g., via an intrinsic face (eigenface) or similar structure. The face tracking 336 module may include an application, applet, service, server, daemon, routine, or other executable logic for identifying similarities between one or more successive media frames that, when displayed, create a visual representation of one or more faces and relative motion of the one or more faces. Face tracking may then be performed by tracking the facial features to match the facial features in each of the plurality of media frames.
In some implementations, the image analyzer 330 may include a motion analysis 338 module, and the motion analysis 338 module may analyze motion of objects detected in the plurality of media frames and calculate motion vectors between the plurality of media frames. The motion analysis 338 module may include an application, applet, service, server, daemon, routine, or other executable logic for identifying similarities between one or more successive media frames that, when displayed, create a visual representation of one or more objects and relative motion of the one or more objects. In some implementations, the motion analysis 338 module is configured to calculate the global motion vector from differences of pixels in the region of the first media frame to pixels of the second media frame.
In some implementations, the analyzer 330 may include an entropy 340 module, and the entropy 340 module may analyze the entropy of each of the plurality of media frames and calculate an entropy difference (a measure of the amount of change or difference that has occurred from one frame to another) to determine key frames. The entropy 340 module may include an application, applet, service, server, daemon, routine, or other executable logic to analyze the entropy of one or more media frames. In some implementations, the entropy 340 module may analyze entropy between the identified regions of the media frame to calculate an entropy difference to determine key regions. In some implementations, the entropy 340 module can extract values from multiple media frames that characterize the randomness of motion vectors associated with regions in the frames, allowing the multiple media frames to be segmented into different events (e.g., scene changes in a video).
In some implementations, the focus area analyzer 350 can include a selection interface 352 to select one or more coordinates in one or more frames of each scene identified by the scene change detector 322. Selection interface 352 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying selection data (e.g., coordinates or keywords) via a graphical user interface, such as the interface of application 220 shown in fig. 2C and 2D. Selection interface 352 may receive one or more selected coordinates via a graphical user interface. In some implementations, the selection interface 352 can receive one or more bounding regions (e.g., bounding box 210 as shown in fig. 2B) over one or more frames of each scene. The bounding region may have any shape for dividing (demarrate) the area of the frame, such as a triangle, a rectangular box, or a nonagon. In some implementations, the selection interface 352 can receive the feature identifier via a graphical user interface. The feature identifier may include one or more keywords associated with a particular feature on one or more frames of the video. In some implementations, the selection interface 352 can select one or more coordinates in one or more frames of each scene in the video after the video is cropped at least once. The selected coordinates may be outside of the initial cropping of the video.
In some implementations, the focus area analyzer 350 can include feature tags 354 to identify one or more features within each frame that correspond to the one or more coordinates selected from the selection interface 352. Feature indicia 354 may include an application, applet, service, server, daemon, routine, or other executable logic for identifying one or more features within each frame using selection data from selection interface 352. In some implementations, the functions of the feature labels 354 can be performed by the image analyzer 330 and data from the selection interface 352 can be fed into the image analyzer 330. In some implementations, the feature tag 352 can receive one or more features identified for each frame of the video from the image analyzer 330. Feature 354 may identify one or more features within the frame corresponding to the selection data. In some embodiments, feature indicia 354 may identify the feature at or closest to the selected coordinates. In some embodiments, the feature indicia 354 may identify one or more features within the bounding region. In the example depicted in fig. 2B, image analyzer 330 may have detected the presence of person 208A and person 208B on the bicycle. Based on the bounding box 210 around the person 208A on the bicycle, the feature 354 may identify the person 208A on the bicycle as being within the bounding box 210. In some implementations, feature tags 354 may use a semantic knowledge graph to identify one or more features within an entire frame that correspond to feature identifiers received via selection interface 350. Based on the features identified as being within the frame, feature tags 354 may identify one or more keywords associated with each feature based on a semantic knowledge graph. Feature tags 354 may also use a semantic knowledge graph to determine whether any of the one or more keywords of each feature semantically match a feature identifier.
In some implementations, feature tags 354 may identify one or more features corresponding to one or more selected coordinates, one or more bounding regions, or feature identifiers in other frames of the same scene. Selection interface 352 may have been used to select coordinates, one or more bounding regions, or feature identifiers for a particular frame. In this way, the same features corresponding to the selection may be located at different positions on other frames. Feature marker 354 may traverse the set of frames identified by scene change detector 322 as belonging to the same scene. Feature marker 354 may identify one or more features corresponding to one or more features of the frame used to select the coordinates, bounding region, or feature identifier while traversing each of the other frames of the same scene. In some implementations, feature tags 354 may use motion analysis to determine motion vectors for features between frames of the same scene. Feature marker 354 may identify coordinates and bounding regions in a frame of the same scene for one or more features corresponding to the selection.
In some implementations, the signal fusion calculator 360 can combine the data from the time analyzer 320, the image analyzer 330, and the focus area analyzer 350 and determine important objects and/or features of the entire scene including the plurality of media frames. The merged data may then be used by the clipping calculator 362 module to clip multiple media frames to regenerate media. To determine the subset regions of each frame to crop the media, the cropping calculator 362 may generate a score for each feature identified in the frame. For features corresponding to selections received via the selection interface 352, the cropping calculator 362 may assign a higher score than the score of one or more features identified without the selection interface 352. In some implementations, the crop calculator 362 can determine the distance between the feature and the feature corresponding to the selection. Based on the distance and the characteristics of the features, the clipping calculator 362 may generate a score for the identified features. In some embodiments, the closer the distance, the higher the score may be; and the farther away the distance, the lower the score may be. In other implementations, the scores may be reversed (e.g., a lower score indicates higher significance or relevance). In some embodiments, the score versus distance relationship may be linear, while in other embodiments it may be non-linear (e.g., hierarchical, piecewise linear, geometric, exponential, etc.). The score of the selected feature may be set to a predetermined margin above the highest score among other features identified in the frame.
Based on the scores, the clipping calculator 362 may also identify the feature subset with the highest N subset scores per scene. Once identified, cropping calculator 362 may crop the frames of each scene to include the identified features plus the features corresponding to the selection. Cropping calculator 362 may traverse the frames of each scene to crop each frame to include the identified features and the features corresponding to the selection. A clipping calculator 362. In some implementations, the cropping calculator 362 may crop each frame using motion vectors determined using motion analysis of the features. In some implementations, the media is regenerated to the target aspect ratio. In some embodiments, the signal fusion calculator 360 may assign weights to the different outputs of the analyzer. The signal fusion calculator 360 may normalize the different outputs within a specified range to values that have been determined by a deep learning method.
In some embodiments, the functions of the signal fusion calculator 360 and the clipping calculator 362 may be repeated iteratively. After one or more crop of the media, a signal fusion calculator 360 and a crop calculator 362. In some implementations, the cropping calculator 362 may determine that the initial subset area of each cropped frame does not include the one or more selected features. Based on this determination, the cropping calculator 362 may change or modify the initial subset area of the cropped frame to include the one or more selected features to generate a new subset area. In some implementations, the cropping calculator 362 may generate or recalculate a score for each feature identified in the frame in the manner described above. For features corresponding to selections received via the selection interface 352, the cropping calculator 362 may assign a higher score than the score of one or more features identified without the selection interface 352. The clipping calculator 362 may also identify the feature subset with the highest subset score for each scene. Once identified, the cropping calculator 362 may re-crop the frames of each scene to include the identified features plus the features corresponding to the selection. This process may be repeated iteratively. The selection of features may be different in each run, resulting in different cropping of the media frame.
Fig. 4 is a flow diagram of an embodiment of a method 400 of cropping a media frame. In some implementations, the method 400 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data storage 144 and may use media extracted from the media content database 154. Briefly, the method 400 may include receiving a media frame at 402, identifying a selected region of the frame corresponding to a selected feature at 404, and identifying a region of the frame that includes a feature at 406. If additional regions are identified at 408, the method returns to identifying regions of the frame that include features at 406. If the identification of the regions fails at 408, the method continues to crop the media frame based on the one or more regions identified at 410.
Still referring to fig. 4 and in more detail, the method 400 may begin when a media frame is received 402. In some implementations, the media may include a media file (e.g., a video file) that includes a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the media frame is received as part of streaming media data. Streaming media may be received through network interface 146. In some implementations, the media frames are part of a list of stored media, and each media is processed in turn. In some implementations, it is first determined whether the media needs to be cropped and/or processed. This determination may be accomplished by comparing the storage dimensions, aspect ratio, resolution, etc. of the stored media to target values.
At 404, a selected region of the frame corresponding to the selected feature may be identified. Using various image recognition algorithms, the selected features may be identified based on coordinates, bounding regions, or feature identifiers received via the selection interface. In some implementations, the selected features can be identified by analyzing the frame for any objects or features near or on the selected coordinates. In some implementations, the selected features can be identified by analyzing the frames within the bounding region for any objects or features therein. In some implementations, the selected features can be identified by determining one or more keywords associated with features detected within the entire frame that correspond to the feature identifier. Once a selected feature corresponding to the selection data received via the interface is identified, a selected region associated with the selected feature may be identified.
At 406, regions of the frame that include features may be identified. In some implementations, the features are identified by analyzing the frames using facial recognition. In some implementations, the features are identified by analyzing frames for text using optical character recognition. In some implementations, the features are identified by analyzing the frames for objects (e.g., cars, buildings, people, animals, street signs, etc.) using object recognition. In some implementations, the features are identified by analyzing the frame for boundaries, frames, and/or padding (e.g., boundaries at which the color at one or more edges of the frame is uniform or near uniform). In some implementations, the frames are analyzed to identify a plurality of features. These features may be of different types (e.g., face, text, object, etc.). If the identification of no additional regions fails at 408, the method returns to identifying additional regions in the frame that include features at 406.
If the identification of additional regions fails at 408, the media frame may be cropped at 410 based on the one or more regions identified from 406 and the selected region identified from 404. In some implementations, a media frame may be cropped if one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame. In some implementations, a media frame may be cropped if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media frame is cropped to match a target aspect ratio or to match a target resolution. Additional padding may be added to one or more edges of the cropped media frame to match a target aspect ratio or to match a target resolution. In some implementations, the cropped area is also based on one or more media frames before and/or after the media frame.
In some implementations, during cropping, some padding may be added to meet the target aspect ratio. In some implementations, if static boundaries exist on one or more edges of a media frame, they may be moved or reformatted to form and/or be part of the padding. In some implementations, additional overlay images may be added to the cropped frame. Additional overlay images may be received via the graphical user interface.
In some implementations, multiple media frames may be received and analyzed to determine a scene change. Keypoint detection may be used to analyze when there is a large change in keypoints indicating a scene break or scene change. In some embodiments, the comparison of all pixels in one frame is to pixels in consecutive frames, and if more than a certain threshold of pixels are different when considered as part of the optical flow, this is an indication of a scene change. In some implementations, motion vectors are computed between multiple media frames, and the absence of coherent motion vectors between successive frames indicates a scene change. Features may then be identified within a particular scene and regions containing the particular features tracked among a plurality of media frames within the particular scene. In an embodiment, information of the location within the plurality of media frames where a particular feature is tracked is also used to determine where to crop a media frame based on the region.
In some implementations, a plurality of media frames are received and analyzed to identify facial features. Face tracking may then be performed by tracking the facial features to match the facial features in each of the plurality of media frames.
In some implementations, multiple media frames can be received and analyzed to find static boundaries along edges of the multiple frames. In some embodiments, to locate the boundary, a random pixel is selected and the row of pixels is compared to the random pixel vertically and/or horizontally to determine if there is an uninterrupted row of pixels that is similar in color to the randomly selected pixel. In some implementations, a boundary is located that is static from one frame to the next and contains pixels that are relatively uniform in color. In some implementations, a boundary may be located that is static from one frame to the next and contains pixels that are relatively uniform in color, but also contains some additional static information (such as different color text embedded in the boundary). Once the boundary is located, whether or not it contains embedded text, it can be treated as an image during the cropping process.
Fig. 5 is a flow diagram of an embodiment of a method 500 of cropping a media frame by determining a score for each of a plurality of regions. In some implementations, the method 400 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data storage 144 and may use media extracted from the media content database 154. Briefly, the method 500 may include receiving a media frame at 502 and identifying a region in the frame that includes a feature at 504. If additional regions are identified at 506, the method determines a score for the identified regions based on the respective characteristics at 508, and returns to identifying regions that include features in the frame at 504. If the identification of regions fails at 506, the method continues by determining that a score for one or more of the identified regions exceeds a threshold at 510, and cropping the media frame to include the one or more regions that exceed the threshold at 512.
Still referring to fig. 5 and in more detail, the method 500 may begin when a media frame is received 502. In some implementations, the media may include a media file (e.g., a video file) that includes a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the media frame is received as part of streaming media data. Streaming media may be received through network interface 146.
At 504, a selected region of the frame corresponding to the selected feature may be identified. Using various image recognition algorithms, the selected features may be identified based on coordinates, bounding regions, or feature identifiers received via the selection interface. In some implementations, the selected features can be identified by analyzing the frame for any objects or features near or on the selected coordinates. In some implementations, the selected features can be identified by analyzing the frames within the bounding region for any objects or features therein. In some implementations, the selected features can be identified by determining one or more keywords associated with features detected within the entire frame that correspond to the feature identifier. Once a selected feature corresponding to the selection data received via the interface is identified, a selected region associated with the selected feature may be identified.
At 506, regions of the frame that include features are identified. In some implementations, one or more of the features in the region are identified by using facial recognition analysis frames. In some implementations, one or more of the features in the region are identified by analyzing the frames for text using optical character recognition. In some implementations, one or more of the features in the area are identified by analyzing the frames for objects (e.g., cars, buildings, people, animals, street signs, etc.) using object recognition. In some implementations, one or more of the features in the region are identified by analyzing the frame for boundaries, frames, and/or fills (e.g., boundaries at one or more edges of the frame that are uniform or nearly uniform in color). In some implementations, each region is further analyzed to potentially identify a plurality of features in one or more of the regions. The features in each of the plurality of regions may be of a different type (e.g., face, text, object, etc.). If the identification of no additional regions fails at 508, the method determines a score for the identified regions based on the corresponding characteristics at 510 and returns to identifying additional regions including features in the frame at 506.
At 510, a score for the identified region is determined based on the corresponding characteristic. In some embodiments, the score is based on the type of feature located in the region or at least partially located in the region. In some implementations, the score is weighted based on the type of features located in the region. The weights may be determined by using training data. In some implementations, the training data can be used as input to a deep learning inference model. In some implementations, the training data is data that is input based on a selection of an important region of the media. Some characteristics on which the score may be based may include the size of features in the region, the type of features in the region, the motion of features in the region, the relative motion of features in the region, the amount of blur associated with features in the region, and so forth. In some implementations, the score is assigned to a feature, rather than a region containing the feature. In some implementations, determining the score for each of the plurality of regions may include determining a ranking of the plurality of regions, wherein at least a highest ranked region of the plurality of regions is determined. In some implementations, determining the score for each of the plurality of regions may include ranking each of the plurality of regions from highest to lowest, where higher ranked regions are more likely to be included in any clipping of the media frame. In some implementations, the score can be based on a distance between the feature identified in 506 and the selected feature identified in 504. In some implementations, the score can be based on a distance between a previously identified feature and the selected feature identified in 904. For features corresponding to a selection received via the selection interface, a higher score may be assigned than a score of one or more features identified without the selection interface. In some implementations, a distance between the feature and the feature corresponding to the selection may be determined for each previously identified feature. Based on the distance and the characteristics of the features, a score may be generated for the identified features. In some embodiments, the closer the distance, the higher the score may be; and the farther away the distance, the lower the score may be. In other implementations, the scores may be reversed (e.g., a lower score indicates higher significance or relevance). In some embodiments, the score versus distance relationship may be linear, while in other embodiments it may be non-linear (e.g., hierarchical, piecewise linear, geometric, exponential, etc.). The score of the selected feature may be set to a predetermined margin above the highest score among other features identified in the frame.
If the identification of additional regions fails at 508, the method determines that the score for one or more regions exceeds a threshold at 512. In some embodiments, the score for each of the plurality of regions comprises a value for comparison. In some implementations, when cropping a media frame, the score of a region must exceed a threshold before the region is considered. In some embodiments, only the region with the highest score is prioritized for inclusion when cropping the media frame. In some implementations, the priority of the plurality of regions is assigned based on respective scores of the plurality of regions to be included in cropping the media frame. In some implementations, it is determined which combination of regions results in a maximized score, where all regions are able to fit within the region of the media frame being cropped.
If, at 508, the identification of additional regions fails, then, at 514, the media frame is cropped to include the selected region identified at 504 and the one or more regions identified at 506 having associated scores that exceed the threshold. In some implementations, only the determination of regions of the plurality of regions whose scores exceed the threshold is considered when determining the regions to include in cropping the media frame. In some implementations, a media frame may be cropped if one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame. In some implementations, a media frame may be cropped if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media frame is cropped to match a target aspect ratio or to match a target resolution. Additional padding may be added to one or more edges of the cropped media frame to match a target aspect ratio or to match a target resolution. In some implementations, the cropped regions are also based on one or more of the media frames before and/or after the media frame.
FIG. 6 is a flow diagram of an embodiment of a method 600 of generating or updating a score based on movement of a feature. In some implementations, the method 600 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data storage 144 and may use media extracted from the media content database 154. Briefly, the method 600 may include, at 602, receiving a plurality of media frames and, at 606, identifying regions comprising identical features in each of the plurality of frames. If, at 608, the additional region is not identified as failed, the method continues by: an amount of movement of the feature from the region is determined at 610, and a score for the region is generated or updated based on the movement of the feature at 612, and then back to 606 to identify additional regions in each of the plurality of frames that include the same feature. If the identification of the additional region fails at 608, the method stops.
Still referring to fig. 6 and in more detail, the method 600 may begin when multiple media frames are received 602. In some implementations, the media is a media file (e.g., a video file) that contains a plurality of media frames. The media may be stored in the media content database 154 or retrieved through the network interface 146. In some implementations, the plurality of media frames may be received as part of streaming media data. Streaming media may be received through network interface 146.
At 604, a selected region of the frame corresponding to the selected feature may be identified. Using various image recognition algorithms, the selected features may be identified based on coordinates, bounding regions, or feature identifiers received via the selection interface. In some implementations, the selected features can be identified by analyzing the frame for any objects or features near or on the selected coordinates. In some implementations, the selected features can be identified by analyzing the frames within the bounding region for any objects or features therein. In some implementations, the selected features can be identified by determining one or more keywords associated with features detected within the entire frame that correspond to the feature identifier. In some implementations, the selected features can be identified by analyzing the frames using facial recognition. In some implementations, the selected features can be identified by analyzing frames for text using optical character recognition. In some implementations, the selected features can be identified by analyzing the frames for objects (e.g., cars, buildings, people, animals, street signs, etc.) using object recognition. In some implementations, the selected features can be identified by analyzing the frame for boundaries, frames, and/or fills (e.g., boundaries at one or more edges of the frame that are uniform or nearly uniform in color). In some implementations, the selected features can be analyzed to identify a plurality of features. The features may be of different types (e.g., face, text, object, etc.). Once a selected feature corresponding to the selection data received via the interface is identified, a selected region associated with the selected feature may be identified.
At 606, regions that include the same features in each of the plurality of frames are identified. In some implementations, identifying features as being the same may include comparing characteristics of the features. Characteristics of a feature may include object properties, color values, dimensions, and the like. In some implementations, identifying a feature as the same feature is also based on a proximity of a region bounding the feature between frames of the plurality of frames that precede and follow the frame.
If at 608, it fails to identify additional regions, then at 610, an amount of movement of features from the regions is determined. The determination may include the features identified in 604 and 606. In some embodiments, the amount of movement of the feature from the region is determined by the absolute position of the feature within each of the plurality of frames. In some embodiments, the amount of movement of the feature from the region when compared to one or more of the preceding or following frames is determined by the relative position of the feature in each of the plurality of frames. In some embodiments, the amount of movement is determined by one of the plurality of frames and an increase or decrease in feature size between the plurality of frames. Combinations of different ways of determining the amount of movement of a feature may be used to determine the amount of movement between two or more of the plurality of frames.
At 612, a score for the region is generated or updated based on the movement of the feature. In some embodiments, the score is adjusted based on, or based on, an amount of movement of the feature between two or more of the received plurality of frames. In some embodiments, the adjustment of the score is accomplished by weighting an existing score for a region containing one or more features of one or more of the frames based on a determined amount of movement between the frames. In some implementations, the score is assigned to a feature, rather than a region containing the feature. In some implementations, determining the score for each of the plurality of regions may include determining a ranking of the plurality of regions, wherein at least a highest ranked region of the plurality of regions is determined. In some implementations, determining the score for each of the plurality of regions may include ranking each of the plurality of regions from highest to lowest, where higher ranked regions are more likely to be included in any clipping of the media frame. In some implementations, the score corresponding to the feature identified in 604 may be set to a higher value than the score corresponding to the feature identified in 606.
The conversion of the media may be performed on different computing systems, including detecting important portions (e.g., features) in the image or video for each frame and intelligently cropping or filling in to preserve important features while discarding unimportant areas, static boundaries, and the like. In some implementations, the detection of portions of images, videos, or other media may be done on a server system and used to create metadata that associates regions or bounds containing features with media frames. Based on the identified important regions, the image or video may be intelligently cropped or padded to preserve important features while dropping unimportant regions, static boundaries, etc. on another device (such as a mobile device). The detected features may include face tracking, object detection and/or recognition, text detection, dominant color detection, motion analysis, scene change detection, and image saliency. Detection and identification may use deep learning based methods and algorithms. Text detection may use Optical Character Recognition (OCR). The detection of features to be placed in the metadata allows an optimal clipping path to be run on the mobile device. Other aspects of the invention may include filling the image to match the background color, and removing and/or reformatting any borders to accommodate the new display mode. Although the media is discussed primarily with respect to video, in many embodiments, the system may be applied to individual images or frames.
Fig. 7 is a flow diagram of an embodiment of a method 700 for cropping media frames using received metadata. In some implementations, the method 700 is implemented by the processor 122 of the mobile device 102 executing instructions stored on the data storage 124. Briefly, method 700 may include receiving a media frame at 702, receiving metadata associated with the media at 706, identifying a region in the frame based on the received metadata at 708, cropping the media frame based on the region at 710, and receiving a next media frame at 712.
Still referring to fig. 7 and in more detail, the method 700 may begin when a media frame is received at 702. In some implementations, the media is a media file (e.g., a video file) that contains a plurality of media frames. Media may be received and stored in the data storage 124 through the network interface 126. In some implementations, the media frame is received as part of streaming media data. Streaming media may be received through network interface 146.
At 704, selected regions of the frame corresponding to the selected features can be identified. Using various image recognition algorithms, the selected features may be identified based on coordinates, bounding regions, or feature identifiers received via the selection interface. In some implementations, the selected features can be identified by analyzing the frame for any objects or features near or on the selected coordinates. In some implementations, the selected features can be identified by analyzing the frames within the bounding region for any objects or features therein. In some implementations, the selected features can be identified by determining one or more keywords associated with features detected within the entire frame that correspond to the feature identifier. In some implementations, the selected features can be identified by finding metadata corresponding to the feature identifier and features associated with the metadata throughout the frame. Once a selected feature corresponding to the selection data received via the interface is identified, a selected region associated with the selected feature may be identified.
At 706, metadata associated with the media is received. In some implementations, the metadata is received as part of a media file containing a plurality of media frames. In some implementations, the metadata is received independently, along with an identifier or other data that associates the metadata with the received plurality of media frames. In some implementations, the metadata can include data about an area associated with one or more of the plurality of media frames. In some implementations, the metadata includes delimitation information for one or more regions of each of the plurality of media frames. In some implementations, the location of the one or more features within each of the plurality of media frames may be included in the metadata. Features may include objects (such as cars, buildings, people, animals, street signs, etc.), text, boundaries of media frames, uniform color fill of one or more edges of media frames, and so forth. In some implementations, the metadata can identify a plurality of features and/or regions of one or more of the plurality of media frames. In some implementations, the metadata is associated with a target aspect ratio or a target resolution. In some implementations, the metadata can identify one or more regions of the media frame. Each of the one or more identified regions may be a region determined to have a score that exceeds a threshold. The score may be determined by a clipping calculator as described above.
At 708, regions in the frame are identified based on the received metadata. In some implementations, a region in a frame can be retrieved from the metadata and can include features identified using facial recognition. In some implementations, a region in a frame can be retrieved from the metadata and can include features identified by analyzing the frame for text using optical character recognition. In some implementations, a region in the frame is retrieved from the metadata, and the region can include features identified by analyzing the frame for objects (e.g., cars, buildings, people, animals, street signs, etc.) using object recognition. In some implementations, a region in a frame is retrieved from the metadata, and the region can include features identified by analyzing the frame for boundaries, frames, and/or padding (e.g., a uniformly colored or nearly uniform boundary at one or more edges of the frame). In some implementations, a region in a frame can be retrieved from metadata, and the region can include a plurality of features. These features may be of different types (e.g., face, text, object, etc.). In some implementations, the plurality of regions may be retrieved from metadata of the media frame. In some implementations, multiple media frames may be received and metadata associated with the multiple media frames.
At 710, the media frame is cropped based on the area identified in 708 and the selected area identified in 704. In some implementations, a media frame may be cropped if one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame. In some implementations, a media frame may be cropped if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media frame is cropped to match a target aspect ratio or to match a target resolution. The target aspect ratio or target resolution may vary depending on the orientation of the mobile device 102 displaying the media frame. Additional padding may be added to one or more edges of the cropped media frame to match a target aspect ratio or to match a target resolution. In some implementations, the cropped regions are also based on one or more of the media frames before and/or after the media frame.
The next media frame is received at 712 until no more frames are available. The next media frame may be received through the network interface 126 and stored in the data storage 124. In some implementations, the next media frame is received as part of the streaming media data. Streaming media may be received through network interface 146. As long as more frames are available, the method may continue by again identifying regions in the next frame based on the received metadata.
FIG. 8 is a flow diagram of an embodiment of a method 800 of adjusting cropping based on orientation change. In some implementations, the method 800 is implemented by the processor 122 of the mobile device 102 executing instructions stored on the data storage 124 and receiving data from one or more sensor modules 132. Briefly, the method 800 may include receiving an indication of an orientation change at 802, identifying a resolution of a new orientation at 804, and dynamically adjusting a cropping of the played media based on the new orientation at 806.
Still referring to fig. 8 and in more detail, the method 800 may begin when an indication of an orientation change is received 802. In some implementations, an indication of the orientation change is received from the sensor module 132 (e.g., an accelerometer and/or magnetometer). In some implementations, the orientation change or detection of the orientation occurs before the media display. In some implementations, the change in orientation occurs during the media display, and the change in the media display occurs in real-time after the change in orientation is detected.
At 804, the resolution and/or aspect ratio of the new orientation is identified. In some implementations, the resolution and/or aspect ratio is predetermined by the application displaying the media. The resolution and/or aspect ratio may have predetermined values for both the lateral and longitudinal orientations. In some implementations, the orientation-dependent resolution and/or aspect ratio is determined to minimize the amount of unused display space. In some implementations, the orientation-dependent resolution and/or aspect ratio is determined to minimize the amount of fill required to place the displayed media in the available display space.
At 806, the cropping of the playing media is dynamically adjusted based on the new orientation. In some implementations, the change in orientation occurs during the media display, and the change in the media display occurs in real-time after the change in orientation is detected. In some implementations, the media frame or frames remain the same, but the cropping is altered based on the received metadata to accommodate the new resolution and/or aspect ratio.
FIG. 9 is a flow diagram of an embodiment of a method 900 of iteratively cropping a media frame using selection data. In some implementations, the method 900 is implemented by the processor 142 of the media server system 104 executing instructions stored on the data storage 144 and may use media extracted from the media content database 154. Briefly, the method 900 may include: a media frame that is cropped based on one or more regions previously identified as including a feature is identified at 902, a selected region of the frame corresponding to the selected feature is identified at 904, and a determination is made at 906 as to whether the selected region is within the cropped frame. If the selected area is determined to be within the cropped frame at 906, the method 800 may include maintaining the cropped media frame. Otherwise, if it is determined at 906 that the selected region is outside of the cropped frame, the method may include recalculating 910 the score for each previously identified feature in the frame, and cropping 912 the media frame based on the selected region and the one or more regions having the recalculated scores.
In more detail, the method 900 may begin at 902 with identifying a media frame that is cropped based on one or more regions previously identified as including a feature. In some implementations, each frame belonging to a scene may have been cropped based on one or more features previously identified therein. Various image analysis techniques, such as facial recognition, optical character recognition, object recognition, and other algorithms may have been used to identify one or more features. In some implementations, the previous crop may have been completed without any selection data from the selection interface.
At 904, a selected region of the frame corresponding to the selected feature can be identified. Using various image recognition algorithms, the selected features may be identified based on coordinates, bounding regions, or feature identifiers received via the selection interface. In some implementations, the selected features can be identified by analyzing the frame for any objects or features near or on the selected coordinates. In some implementations, the selected features can be identified by analyzing the frames within the bounding region for any objects or features therein. In some implementations, the selected features can be identified by determining one or more keywords associated with features detected within the entire frame that correspond to the feature identifier. Once a selected feature corresponding to the selection data received via the interface is identified, a selected region associated with the selected feature may be identified.
At 906, it may be determined whether the selected area is within a cropped frame. It may be determined whether the selected area identified in terms of the selected coordinates, bounding area, or feature identifier overlaps with or is otherwise included within the cropped frame. In some implementations, the selected region may be determined to be within the cropped frame if a threshold percentage (e.g., 50-100%) of the selected region overlaps the cropped frame. In some implementations, the selected area may be determined to be within the cropped frame if the selected coordinates for identifying the selected area are within the cropped frame. The selected coordinates may be transformed (translate) with respect to the ratio between the full-size frame and the cropped frame. In some implementations, the select area may be determined to be outside of the cropped frame if a threshold percentage (e.g., 50-100%) of the select area does not overlap with the cropped frame. In some implementations, the selected area may be determined to be outside the cropped frame if the selected coordinates used to identify the selected area are outside the cropped frame.
If the selected area is within the cropped frame as determined at 906, the cropped media frame may be retained at 908. On the other hand, if the selected region is determined to be outside of the cropped frame at 906, the score for each previously identified feature in the frame may be recalculated at 910. In some implementations, the score can be based on the type of feature located in the region or at least partially located in the region. In some implementations, the score may be weighted based on the type of features located in the region. The weights may be determined by using training data. In some implementations, the training data can be used as input to a deep learning inference model. In some implementations, the training data is data that is input based on a selection of an important region of the media. Some characteristics on which the score may be based may include: the size of the feature in the region, the type of feature in the region, the motion of the feature in the region, the relative motion of the feature in the region, the amount of blur associated with the feature in the region, and the like. In some implementations, the score is assigned to a feature, rather than a region containing the feature. In some implementations, determining the score for each of the plurality of regions may include determining a ranking of the plurality of regions, wherein at least a highest ranked region of the plurality of regions is determined. In some implementations, determining the score for each of the plurality of regions may include ranking each of the plurality of regions from highest to lowest, where higher ranked regions are more likely to be included in any clipping of the media frame.
In some implementations, the score can be based on a distance between a previously identified feature and the selected feature identified in 904. For features corresponding to a selection received via the selection interface, a higher score may be assigned than a score of one or more features identified without the selection interface. In some implementations, a distance between the feature and the feature corresponding to the selection may be determined for each previously identified feature. Based on the distance and the characteristics of the features, a score may be generated for the identified features. In some embodiments, the closer the distance, the higher the score may be; and the farther away the distance, the lower the score may be. In other implementations, the scores may be reversed (e.g., a lower score indicates higher significance or relevance). In some embodiments, the score versus distance relationship may be linear, while in other embodiments it may be non-linear (e.g., hierarchical, piecewise linear, geometric, exponential, etc.). The score of the selected feature may be set to a predetermined margin above the highest score among other features identified in the frame.
Based on the scores recalculated at 910, the media frame may be cropped to include the one or more regions previously identified and the selected region identified from 904. In some implementations, a media frame may be cropped if one or more values of the target aspect ratio are less than a current value of the aspect ratio of the media frame. In some implementations, a media frame may be cropped if one or more values of the target resolution are less than a current value of the resolution of the media frame. In some implementations, the media frame is cropped to match a target aspect ratio or to match a target resolution. Additional padding may be added to one or more edges of the cropped media frame to match a target aspect ratio or to match a target resolution. In some implementations, the cropped regions are also based on one or more of the media frames before and/or after the media frame.
Fig. 10 is a block diagram of a general architecture of a computing system 900 that may be used to implement the mobile device 102, the media server system 104, and the like. Computing system 900 includes a bus 1005 or other communication component for communicating information, and a processor 1010 coupled to bus 1005 for processing information. Computing system 1000 may also include one or more processors 1010 coupled to the bus for processing information. Computing system 1000 also includes a main memory 1015, such as a RAM or other dynamic storage device, coupled to bus 1005 for storing information and instructions to be executed by processor 1010. Main memory 1015 also may be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 1010. Computing system 1000 may also include a ROM 1020 or other static storage device coupled to bus 1005 for storing static information and instructions for processor 1010. A storage device 1025, such as a solid state device, magnetic disk or optical disk, is coupled to bus 1005 for persistently storing information and instructions. Computing system 1000 can include, but is not limited to, a digital computer such as a laptop computer, desktop computer, workstation, personal digital assistant, server, blade server, mainframe, cellular telephone, smart phone, mobile computing device (e.g., notepad, e-reader, etc.), and the like.
According to various embodiments, the processes and/or methods described herein may be implemented by computing system 1000 in response to processor 1010 executing an arrangement of instructions contained in main memory 1015. Such instructions may be read into main memory 1015 from another computer-readable medium, such as storage device 1025. Execution of the arrangement of instructions contained in main memory 1015 causes computing system 1000 to perform the illustrative processes and/or method steps described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 1015. In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions to implement the illustrative embodiments. Thus, implementations are not limited to any specific combination of hardware circuitry and software.
Although an embodiment of a computing system 1000 has been described in fig. 10, embodiments of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
At least one aspect of the present disclosure is directed to a method of converting media dimensions. A time analyzer running on a computing device may identify a set of frames in a video in a first orientation as belonging to a scene. A focus area analyzer running on a computing device may receive, via a user interface, selected coordinates on frames in a set of frames identified as belonging to a scene. An image analyzer running on the computing device may identify a first region within the frame that includes a first feature corresponding to the selected coordinates. The image analyzer may identify a second region within the frame that includes a second feature. A cropping calculator running on the computing device may generate a first score for the first feature, a second score for the second feature. The first score may be greater than the second score based on the first feature corresponding to the selected coordinate. The clipping calculator may determine that the second score exceeds a threshold. An image processor running on the computing device may crop frames of the set of frames identified as belonging to the scene to include the first region and the second region within a predetermined display area including a subset region of the frames in the second orientation in response to the determination.
In some implementations, the image processor can identify an initial subset of regions of the frame in the second orientation, the initial subset of regions including a second region and a third region, the second region including the second feature and the third region including the third feature. In some implementations, the image processor may determine that the initial subset region does not include a first region, where the first region includes a first feature corresponding to the selected coordinate. In some implementations, the image processor may modify the initial subset region of the frame to include a first region and a second region in response to determining that the initial subset region does not include the first region, wherein the first region includes the first feature and the second region includes the second feature to generate the subset region of the frame in the second orientation.
In some implementations, the image analyzer may identify, for each frame belonging to the scene, a first region and a second region within the frame that include the first feature and the second feature. In some implementations, cropping may further include cropping the frame to include the first region and the second region within a predetermined display area including a subset region of the frames in the second orientation in response to identifying the first region and the second region in each frame of the set of frames.
In some implementations, wherein receiving the selected coordinates on the frames can further include receiving, via the user interface, a bounding box on a frame of the set of frames identified as belonging to the scene. In some implementations, identifying the first region can further include identifying the first region including the first feature based on a bounding box selected using the user interface.
In some implementations, receiving the selected coordinate on the frame can further include receiving a second selected coordinate on the frame via the user interface. In some implementations, the image analyzer can identify a fourth region within the frame that includes a fourth feature corresponding to the second selected coordinate. In some implementations, the temporal analyzer may determine a motion vector of the fourth feature between the frame and a second frame of the set of frames identified as belonging to the scene. In some implementations, the image processor may crop a second frame of the set of frames identified as belonging to the scene based on the motion vector of the fourth feature.
In some implementations, the focal region analyzer can receive, via the user interface, a second selected coordinate on a frame of the subset of frames after cropping the frame. In some implementations, the image analyzer can adjust the first region to generate a fourth region that includes the first feature corresponding to the second selected coordinate. In some implementations, the cropping calculator may generate a fourth score for the first feature, a fifth score for the second feature, and a sixth score for the third feature, the sixth score being greater than the fifth score based on a first distance between the second selected coordinate and the second feature and a second distance between the second selected coordinate and the third feature. In some embodiments, the image processor may crop the frame to include a third region and a fourth region within a second predetermined display area, the third region corresponding to the third feature, the second predetermined display area including a second subset of regions of the frame in a second orientation.
In some implementations, the focus area analyzer can receive, via the user interface, a feature identifier identified as belonging to a second frame in the set of frames of the scene. In some implementations, the image analyzer can identify a fourth region within the second frame that includes a fourth feature corresponding to the feature analyzer using the semantic knowledge map. In some implementations, the image analyzer can identify a fifth region including a fifth feature within the second frame. In some implementations, the cropping calculator may generate a fourth score for the fourth feature, a fifth score for the fifth feature, the fourth score being greater than the fifth score based on the fourth feature corresponding to the feature identifier received via the user interface. In some embodiments, the image processor may crop a second frame of the set of frames identified as belonging to the scene to include a fourth region and a fifth region within a second predetermined display area, the second predetermined display area including a subset region of the second frame in a second orientation.
In some implementations, the focus area analyzer can receive the overlay image via a user interface to add it to the frame at the specified coordinates. In some implementations, after cropping the frame, the image processor can add the overlay image to the subset region of the frame at the specified coordinates.
In some implementations, the second feature can include text. In some implementations, generating the second score for the second feature may further include generating the second score for the second feature based on at least one of a characteristic of the second feature proportional to a size of the text and a distance of the text from a center of the frame. In some implementations, the second feature includes a face. In some implementations, generating the second score for the second feature based on the characteristic of the second feature may further include generating the second score based on a size of the face relative to the frame.
At least one aspect of the present disclosure is directed to a system for converting media dimensions. The system may include a time analyzer running on the computing device. The temporal analyzer may identify a set of frames in the video in the first orientation as belonging to a scene. The system may include a focal region analyzer running on a computing device. The focus area analyzer may receive, via the user interface, selected coordinates on frames in the set of frames identified as belonging to the scene. The system may include an image analyzer running on a computing device. The image analyzer may identify a first region within the frame that includes a first feature corresponding to the selected coordinates. The image analyzer may identify a second region within the frame that includes a second feature. The system may include a clipping calculator running on the computing device. The cropping calculator may generate a first score for the first feature and a second score for the second feature. The first score may be greater than the second score based on the first feature corresponding to the selected coordinate. The clipping calculator may determine that the second score exceeds a threshold. The system may include an image processor running on a computing device. The image processor may crop frames of the set of frames identified as belonging to the scene in response to the determination to include the first region and the second region within a predetermined display area, the predetermined display area including a subset region of the frames in the second orientation.
In some implementations, the image processor can identify an initial subset of regions of the frame in the second orientation, the initial subset of regions including a second region and a third region, wherein the second region includes the second feature and the third region includes the third feature. In some implementations, the image processor may determine that the initial subset region does not include a first region, where the first region includes a first feature corresponding to the selected coordinate. In some implementations, the image processor may modify the initial subset region of the frame to include the first region and the second region to generate a subset region of the frame in the second orientation in response to determining that the initial subset region does not include the first region, wherein the first region includes the first feature and the second region includes the second feature.
In some implementations, the image analyzer may identify, for each frame belonging to the scene, a first region and a second region within the frame that include the first feature and the second feature. In some embodiments, the image processor may crop the frame to include the first and second regions within a predetermined display area including a subset of regions of the frame in the second orientation in response to identifying the first and second regions in each frame of the set of frames.
In some implementations, the focus area analyzer can also receive, via the user interface, a bounding box on a frame of the set of frames identified as belonging to the scene. In some implementations, the image analyzer can identify a first region including a first feature based on a bounding box selected using the user interface.
In some implementations, the focal region analyzer can receive a second selected coordinate on the frame via the user interface. In some implementations, the image analyzer can identify a fourth region within the frame that includes a fourth feature corresponding to the second selected coordinate. In some implementations, the temporal analyzer may determine a motion vector of the fourth feature between the frame and a second frame of the set of frames identified as belonging to the scene. In some implementations, the image processor may crop a second frame of the set of frames identified as belonging to the scene based on the motion vector of the fourth feature.
In some implementations, the focal region analyzer can receive, via the user interface, a second selected coordinate on a frame of the subset of frames after cropping the frame. In some implementations, the image analyzer can adjust the first region to generate a fourth region that includes the first feature corresponding to the second selected coordinate. In some implementations, the cropping calculator may generate a fourth score for the first feature, a fifth score for the second feature, and a sixth score for the third feature, the sixth score being greater than the fifth score based on a first distance between the second selected coordinate and the second feature and a second distance between the second selected coordinate and the third feature. In some embodiments, the image processor may crop the frame to include a third region and a fourth region within a second predetermined display area, the third region corresponding to the third feature, the second predetermined display area including a second subset of regions of the frame in a second orientation.
In some implementations, the focus area analyzer can receive, via the user interface, a feature identifier identified as belonging to a second frame in the set of frames of the scene. In some implementations, the image analyzer can identify a fourth region within the second frame that includes a fourth feature corresponding to the feature analyzer using the semantic knowledge map. In some implementations, the image analyzer can identify a fifth region including a fifth feature within the second frame. In some implementations, the cropping calculator may generate a fourth score for the fourth feature, a fifth score for the fifth feature, the fourth score being greater than the fifth score based on the fourth feature corresponding to the feature identifier received via the user interface. In some embodiments, the image processor may crop a second frame of the set of frames identified as belonging to the scene to include a fourth region and a fifth region within a second predetermined display area, the second predetermined display area including a subset region of the second frame in a second orientation.
In some implementations, the focus area analyzer can receive the overlay image via a user interface to add it to the frame at the specified coordinates. In some implementations, after cropping the frame, the image processor can add the overlay image to the subset region of the frame at the specified coordinates.
In some implementations, the second feature can include text. In some implementations, generating the second score for the second feature may further include generating the second score for the second feature based on at least one of a characteristic of the second feature proportional to a size of the text and a distance of the text from a center of the frame. In some implementations, the second feature includes a face. In some implementations, generating the second score for the second feature based on the characteristic of the second feature may further include generating the second score based on a size of the face relative to the frame.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware embodied in tangible media, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage media may be or be embodied in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). Thus, computer storage media is tangible and non-transitory.
The operations described in this specification may be performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The terms "data processing apparatus," "computing device," or "processing circuitry" include all kinds of apparatus, devices, and machines for processing data, including in some implementations programmable processor(s), computer(s), system on chip(s), a portion of a programmed processor, or a combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA or an ASIC. The apparatus can include, in addition to hardware, code that creates a runtime environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and operating environment may implement a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
Processors suitable for the execution of a computer program include, in some embodiments, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer does not require such a device. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game player, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including in some embodiments: semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with the user; in some embodiments, the feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of what may be claimed, but rather as descriptions of features specific to particular implementations. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products embodied on tangible media.
References to "or" may be construed as inclusive such that any term described using "or" may mean any of a single, more than one, and all of the described terms.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
The claims should not be read as limited to the described order or elements unless stated to that effect. It will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the appended claims. All embodiments that come within the spirit and scope of the following claims and equivalents thereto are claimed.
Claims (18)
1. A method of converting media dimensions, comprising:
identifying, by a time analyzer running on a computing device, a set of frames in a video in a first orientation as belonging to a scene;
receiving, by a focus area analyzer running on a computing device, via a user interface, first and second selected coordinates on frames of the set of frames identified as belonging to a scene;
identifying, by an image analyzer running on a computing device, a first region within the frame that includes a first feature corresponding to a first selected coordinate;
identifying, by an image analyzer running on a computing device, a second region within the frame that includes a second feature;
generating, by a cropping calculator running on a computing device, a first score for the first feature and a second score for the second feature, the first score being greater than the second score based on the first feature corresponding to the first selected coordinate;
determining, by the clipping calculator, that the second score exceeds a threshold;
cropping, by an image processor running on a computing device, the frame of the set of frames identified as belonging to a scene to include a first region and a second region within a predetermined display area including a subset region of the frame in a second orientation in response to the determination;
identifying, by the image analyzer, a third region within the frame that includes a third feature corresponding to the second selected coordinate;
determining, by a temporal analyzer, a motion vector of a third feature between the frame and a second frame of the set of frames identified as belonging to a scene; and
cropping, by the image processor, a second frame of the set of frames identified as belonging to the scene based on the motion vector of the third feature.
2. The method of claim 1, further comprising:
identifying, by the image processor, that the initial subset of regions of the frame in the second orientation comprise second regions comprising a second feature;
determining, by the image processor, that the initial subset of regions does not include a first region, the first region including a first feature corresponding to a first selected coordinate; and
modifying, by the image processor, the initial subset region of the frame to include a first region and a second region to generate a subset region of the frame in a second orientation, the first region including the first feature and the second region including the second feature, in response to determining that the initial subset region does not include the first region.
3. The method of claim 1, further comprising:
identifying, by an image analyzer, for each frame belonging to the scene, a first region and a second region within the frame comprising a first feature and a second feature; and is
Wherein cropping further comprises cropping the frame to include the first and second regions within a predetermined display area that includes a subset of the regions of the frame in the second orientation in response to identifying the first and second regions in each frame of the set of frames.
4. The method of claim 1, wherein receiving a first selected coordinate on the frame further comprises receiving, via a user interface, a bounding box on the frame of the set of frames identified as belonging to a scene; and is
Wherein identifying the first region further comprises identifying the first region comprising the first feature based on a bounding box selected using the user interface.
5. The method of claim 1, further comprising
Receiving, by a focus area analyzer, a second selected coordinate on the frame of the subset of frames via a user interface after cropping the frame;
adjusting, by the image analyzer, the first region to generate a fourth region comprising the first feature corresponding to the second selected coordinate;
generating, by a cropping calculator running on the computing device, a fourth score for the first feature, a fifth score for the second feature, and a sixth score for the third feature, the sixth score being greater than the fifth score based on a first distance between the second selected coordinate and the second feature and a second distance between the second selected coordinate and the third feature; and
cropping, by the image processor, the frame to include a third region and a fourth region within a second predetermined display area, the third region corresponding to the third feature, the second predetermined display area including a second subset of regions of the frame in a second orientation.
6. The method of claim 1, further comprising:
receiving, by a focus area analyzer via a user interface, a feature identifier identified as belonging to a second frame of the set of frames of a scene;
identifying, by the image analyzer, a fourth region within the second frame that includes a fourth feature corresponding to the feature analyzer using the semantic knowledge map;
identifying, by the image analyzer, a fifth region within the second frame that includes a fifth feature;
generating, by the cropping calculator, a fourth score for a fourth feature, a fifth score for a fifth feature, the fourth score being greater than the fifth score based on the fourth feature corresponding to the feature identifier received via the user interface; and
cropping, by the image processor, a second frame of the set of frames identified as belonging to the scene to include a fourth region and a fifth region within a second predetermined display area, the second predetermined display area including a subset region of the second frame in a second orientation.
7. The method of claim 1, further comprising:
receiving, by a focal region analyzer via a user interface, an overlay image to add to the frame at specified coordinates; and
adding, by an image processor, the overlay image to the subset region of the frame at the specified coordinates after cropping the frame.
8. The method of claim 1, wherein the second feature comprises text, and wherein generating the second score for the second feature further comprises generating the second score for the second feature based on at least one of a characteristic of the second feature proportional to a size of the text and a distance of the text from a center of the frame.
9. The method of claim 1, wherein the second feature comprises a face, and wherein generating a second score for the second feature based on characteristics of the second feature further comprises generating a second score based on a size of the face relative to the frame.
10. A system for converting media dimensions, comprising:
a time analyzer, executable on a computing device, configured to identify a set of frames in a video in a first orientation as belonging to a scene;
a focus area analyzer executable on a computing device configured to receive, via a user interface, first and second selected coordinates on frames of the set of frames identified as belonging to a scene;
an image analyzer, executable on a computing device, configured to:
identifying, within the frame, a first region comprising a first feature corresponding to a first selected coordinate; and
identifying a second region within the frame that includes a second feature;
a clipping calculator executable on a computing device configured to:
generating a first score for the first feature and a second score for the second feature, the first score being greater than the second score based on the first feature corresponding to the first selected coordinate; and
determining that the second score exceeds a threshold; and
an image processor, executable on a computing device, configured to crop the frames of the set of frames identified as belonging to a scene to include a first region and a second region within a predetermined display area, the predetermined display area including a subset of regions of the frames in a second orientation, in response to the determination,
wherein the image analyzer is further configured to identify a third region within the frame that includes a third feature corresponding to the second selected coordinate;
wherein the temporal analyzer is further configured to determine a motion vector of a third feature between the frame and a second frame of the set of frames identified as belonging to the scene; and is
Wherein the image processor is further configured to crop a second frame of the set of frames identified as belonging to the scene based on the motion vector of the third feature.
11. The system of claim 10, wherein the image processor is further configured to:
identifying that the initial subset of regions of the frame in the second orientation comprises a second region comprising a second feature;
determining that the initial subset area does not include a first area that includes a first feature corresponding to a first selected coordinate; and
in response to determining that the initial subset area does not include a first area, modifying the initial subset area of the frame to include the first area and a second area to generate a subset area of the frame in a second orientation, the first area including the first feature and the second area including the second feature.
12. The system of claim 10, wherein the image analyzer is further configured to, for each frame belonging to the scene, identify within the frame a first region and a second region comprising a first feature and a second feature; and is
Wherein the image processor is further configured to crop the frame to include the first and second regions within a predetermined display area including a subset of regions of the frame in a second orientation in response to identifying the first and second regions in each frame of the set of frames.
13. The system of claim 10, wherein the focus area analyzer is further configured to receive, via a user interface, a bounding box on the frame of the set of frames identified as belonging to a scene; and is
Wherein the image analyzer is further configured to identify a first region comprising the first feature based on the bounding box selected using the user interface.
14. The system of claim 10, wherein the focus area analyzer is further configured to receive, via a user interface, a second selected coordinate on the frame in the subset of frames after cropping the frame;
wherein the image analyzer is further configured to adjust the first region to generate a fourth region comprising the first feature corresponding to the second selected coordinate;
wherein the cropping calculator is further configured to generate a fourth score for the first feature, a fifth score for the second feature, and a sixth score for the third feature, the sixth score being greater than the fifth score based on a first distance between the second selected coordinate and the second feature and a second distance between the second selected coordinate and the third feature; and is
Wherein the image processor is further configured to crop the frame to include a third region and a fourth region within a second predetermined display area, the third region corresponding to the third feature, the second predetermined display area including a second subset of regions of the frame in a second orientation.
15. The system of claim 10, wherein the focus area analyzer is further configured to receive, via a user interface, a feature identifier identified as belonging to a second frame of the set of frames of a scene;
wherein the image analyzer is further configured to identify a fourth region within the second frame that includes a fourth feature corresponding to the feature analyzer using the semantic knowledge map, and identify a fifth region within the second frame that includes a fifth feature;
wherein the cropping calculator is further configured to generate a fourth score for a fourth feature, a fifth score for a fifth feature, the fourth score being greater than the fifth score based on the fourth feature corresponding to the feature identifier received via the user interface; and is
Wherein the image processor is further configured to crop a second frame of the set of frames identified as belonging to the scene to include a fourth region and a fifth region within a second predetermined display area, the second predetermined display area including a subset region of the second frame in a second orientation.
16. The system of claim 10, wherein the image analyzer is further configured to identify a fourth region within the frame that includes a fourth feature;
wherein the cropping calculator is further configured to generate a fourth score for a fourth region and determine that the fourth score is less than the threshold; and is
Wherein the image processor is further configured to crop a fourth region from the frame in response to the determination.
17. The system of claim 10, wherein the second feature comprises text, and wherein the cropping calculator is further configured to generate a second score for the second feature based on at least one of a characteristic of the second feature proportional to a size of the text and a distance of the text from a center of the frame.
18. The system of claim 10, wherein the second feature comprises a face, and wherein the cropping calculator is further configured to generate a second score based on a size of the face relative to the frame.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2017/064719 WO2018106692A1 (en) | 2016-12-05 | 2017-12-05 | Method for converting landscape video to portrait mobile layout using a selection interface |
USPCT/US2017/064719 | 2017-12-05 | ||
PCT/US2018/034390 WO2019112642A1 (en) | 2017-12-05 | 2018-05-24 | Method for converting landscape video to portrait mobile layout using a selection interface |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111373740A CN111373740A (en) | 2020-07-03 |
CN111373740B true CN111373740B (en) | 2021-10-01 |
Family
ID=62492710
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880075932.9A Active CN111373740B (en) | 2017-12-05 | 2018-05-24 | Method for converting horizontal video into vertical movement layout by using selection interface |
Country Status (4)
Country | Link |
---|---|
US (3) | US11282163B2 (en) |
EP (1) | EP3698540A1 (en) |
CN (1) | CN111373740B (en) |
WO (1) | WO2019112642A1 (en) |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019112642A1 (en) * | 2017-12-05 | 2019-06-13 | Google Llc | Method for converting landscape video to portrait mobile layout using a selection interface |
US11226731B1 (en) * | 2018-01-24 | 2022-01-18 | Snap Inc. | Simulated interactive panoramas |
CN114402355A (en) | 2019-12-13 | 2022-04-26 | 谷歌有限责任公司 | Personalized automatic video cropping |
US11574200B2 (en) * | 2019-12-18 | 2023-02-07 | W.S.C. Sports Technologies Ltd. | System and method of determining a region of interest in media |
FR3109686B1 (en) * | 2020-04-22 | 2023-05-05 | Wildmoka | Process for transposing an audiovisual stream |
WO2021252697A1 (en) * | 2020-06-11 | 2021-12-16 | Dolby Laboratories Licensing Corporation | Producing and adapting video images for presentation on displays with different aspect ratios |
CN112188283B (en) | 2020-09-30 | 2022-11-15 | 北京字节跳动网络技术有限公司 | Method, device and equipment for cutting video and storage medium |
CN112218160A (en) * | 2020-10-12 | 2021-01-12 | 北京达佳互联信息技术有限公司 | Video conversion method and device, video conversion equipment and storage medium |
CN113840159A (en) * | 2021-09-26 | 2021-12-24 | 北京沃东天骏信息技术有限公司 | Video processing method, device, computer system and readable storage medium |
CN114584832B (en) * | 2022-03-16 | 2024-03-08 | 中信建投证券股份有限公司 | Video self-adaptive multi-size dynamic playing method and device |
CN117909001A (en) * | 2022-10-19 | 2024-04-19 | 华为技术有限公司 | Full screen display method, electronic device and computer readable storage medium |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105247850A (en) * | 2013-03-15 | 2016-01-13 | 谷歌公司 | Automatic adjustment of video orientation |
CN105611357A (en) * | 2015-12-25 | 2016-05-25 | 百度在线网络技术（北京）有限公司 | Image processing method and device |
CN106454407A (en) * | 2016-10-25 | 2017-02-22 | 广州华多网络科技有限公司 | Video live broadcast method and device |
Family Cites Families (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2000036032A (en) | 1998-07-17 | 2000-02-02 | Fujitsu Ltd | Foreground picture extracting method, picture processor, automatic trimming device, recording medium and portrait picture device |
GB0116113D0 (en) * | 2001-06-30 | 2001-08-22 | Hewlett Packard Co | Tilt correction of electronic images |
US7315630B2 (en) | 2003-06-26 | 2008-01-01 | Fotonation Vision Limited | Perfecting of digital image rendering parameters within rendering devices using face detection |
CN100490505C (en) * | 2004-02-13 | 2009-05-20 | 索尼株式会社 | Image processing device and image processing method |
US7528846B2 (en) * | 2005-02-23 | 2009-05-05 | Microsoft Corporation | Systems and methods to adjust a source image aspect ratio to match a different target display aspect ratio |
US7864978B2 (en) | 2006-02-06 | 2011-01-04 | Microsoft Corporation | Smart arrangement and cropping for photo views |
US20080019661A1 (en) * | 2006-07-18 | 2008-01-24 | Pere Obrador | Producing output video from multiple media sources including multiple video sources |
US9240056B2 (en) | 2008-04-02 | 2016-01-19 | Microsoft Technology Licensing, Llc | Video retargeting |
RU2462757C2 (en) | 2008-09-08 | 2012-09-27 | Сони Корпорейшн | Device and method to process images, device to input images and software |
US9020298B2 (en) | 2009-04-15 | 2015-04-28 | Microsoft Technology Licensing, Llc | Automated image cropping to include particular subjects |
US8819172B2 (en) * | 2010-11-04 | 2014-08-26 | Digimarc Corporation | Smartphone-based methods and systems |
US8938100B2 (en) | 2011-10-28 | 2015-01-20 | Intellectual Ventures Fund 83 Llc | Image recomposition from face detection and facial features |
US9008436B2 (en) | 2011-10-28 | 2015-04-14 | Intellectual Ventures Fund 83 Llc | Image recomposition from face detection and facial features |
US9781409B2 (en) * | 2012-08-17 | 2017-10-03 | Nec Corporation | Portable terminal device and program |
US9741150B2 (en) * | 2013-07-25 | 2017-08-22 | Duelight Llc | Systems and methods for displaying representative images |
TWI490746B (en) * | 2013-03-29 | 2015-07-01 | Chunghwa Picture Tubes Ltd | Control system applied to touch and capable of switching two-dimensional/three-dimensional mode |
TWI631506B (en) * | 2013-04-29 | 2018-08-01 | 群邁通訊股份有限公司 | Method and system for whirling view on screen |
US10141022B2 (en) * | 2013-07-10 | 2018-11-27 | Htc Corporation | Method and electronic device for generating multiple point of view video |
US20160225410A1 (en) * | 2015-02-03 | 2016-08-04 | Garmin Switzerland Gmbh | Action camera content management system |
EP3274986A4 (en) * | 2015-03-21 | 2019-04-17 | Mine One GmbH | Virtual 3d methods, systems and software |
EP3289430B1 (en) * | 2015-04-27 | 2019-10-23 | Snap-Aid Patents Ltd. | Estimating and using relative head pose and camera field-of-view |
WO2016200692A1 (en) * | 2015-06-11 | 2016-12-15 | Vieu Labs, Inc. | Editing, sharing, and viewing video |
US20160365119A1 (en) * | 2015-06-11 | 2016-12-15 | Eran Steinberg | Video editing system with multi-stakeholder, multi-stage control |
US10078917B1 (en) * | 2015-06-26 | 2018-09-18 | Lucasfilm Entertainment Company Ltd. | Augmented reality simulation |
US20180132006A1 (en) * | 2015-11-02 | 2018-05-10 | Yaron Galant | Highlight-based movie navigation, editing and sharing |
US20180014049A1 (en) * | 2016-07-08 | 2018-01-11 | Tastemade, Inc. | Orientation Based, Aspect Ratio Switching Video Playback System |
US20200066305A1 (en) * | 2016-11-02 | 2020-02-27 | Tomtom International B.V. | Creating a Digital Media File with Highlights of Multiple Media Files Relating to a Same Period of Time |
WO2018106213A1 (en) * | 2016-12-05 | 2018-06-14 | Google Llc | Method for converting landscape video to portrait mobile layout |
US20190034734A1 (en) * | 2017-07-28 | 2019-01-31 | Qualcomm Incorporated | Object classification using machine learning and object tracking |
US11394898B2 (en) * | 2017-09-08 | 2022-07-19 | Apple Inc. | Augmented reality self-portraits |
WO2019112642A1 (en) * | 2017-12-05 | 2019-06-13 | Google Llc | Method for converting landscape video to portrait mobile layout using a selection interface |
WO2019151798A1 (en) * | 2018-01-31 | 2019-08-08 | 엘지전자 주식회사 | Method and device for transmitting/receiving metadata of image in wireless communication system |
US10235998B1 (en) * | 2018-02-28 | 2019-03-19 | Karen Elaine Khaleghi | Health monitoring system and appliance |
US11689686B2 (en) * | 2018-10-29 | 2023-06-27 | Henry M. Pena | Fast and/or slowmotion compensating timer display |
CA3066383A1 (en) * | 2019-01-03 | 2020-07-03 | James Harvey Elder | System and method for automated video processing of an input video signal using tracking of a single movable bilaterally-targeted game-object |
US10559307B1 (en) * | 2019-02-13 | 2020-02-11 | Karen Elaine Khaleghi | Impaired operator detection and interlock apparatus |
US11636438B1 (en) * | 2019-10-18 | 2023-04-25 | Meta Platforms Technologies, Llc | Generating smart reminders by assistant systems |
US11297260B1 (en) * | 2020-11-20 | 2022-04-05 | Donald Siu | Techniques for capturing video in landscape mode by a handheld device |
-
2018
- 2018-05-24 WO PCT/US2018/034390 patent/WO2019112642A1/en unknown
- 2018-05-24 US US16/769,758 patent/US11282163B2/en active Active
- 2018-05-24 CN CN201880075932.9A patent/CN111373740B/en active Active
- 2018-05-24 EP EP18728785.9A patent/EP3698540A1/en active Pending
-
2022
- 2022-02-28 US US17/682,791 patent/US11605150B2/en active Active
-
2023
- 2023-03-13 US US18/182,710 patent/US11978238B2/en active Active
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN105247850A (en) * | 2013-03-15 | 2016-01-13 | 谷歌公司 | Automatic adjustment of video orientation |
CN105611357A (en) * | 2015-12-25 | 2016-05-25 | 百度在线网络技术（北京）有限公司 | Image processing method and device |
CN106454407A (en) * | 2016-10-25 | 2017-02-22 | 广州华多网络科技有限公司 | Video live broadcast method and device |
Also Published As
Publication number | Publication date |
---|---|
US11282163B2 (en) | 2022-03-22 |
US20230237760A1 (en) | 2023-07-27 |
US11605150B2 (en) | 2023-03-14 |
CN111373740A (en) | 2020-07-03 |
WO2019112642A1 (en) | 2019-06-13 |
US11978238B2 (en) | 2024-05-07 |
US20210012502A1 (en) | 2021-01-14 |
US20220245756A1 (en) | 2022-08-04 |
EP3698540A1 (en) | 2020-08-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN111373740B (en) | Method for converting horizontal video into vertical movement layout by using selection interface | |
CN109791600B (en) | Method for converting horizontal screen video into vertical screen mobile layout | |
US11322117B2 (en) | Media rendering with orientation metadata | |
US11687707B2 (en) | Arbitrary size content item generation | |
RU2662632C2 (en) | Presenting fixed format documents in reflowed format | |
US10878024B2 (en) | Dynamic thumbnails | |
CN112215171B (en) | Target detection method, device, equipment and computer readable storage medium | |
EP3910496A1 (en) | Search method and device | |
KR102408256B1 (en) | Method for Searching and Device Thereof | |
US11663398B2 (en) | Mapping annotations to ranges of text across documents | |
KR20220097945A (en) | Non-Closed Video Overlays | |
US11526652B1 (en) | Automated optimization of displayed electronic content imagery | |
US20230215466A1 (en) | Digital Video Generation depicting Edit Operations to Digital Content | |
Yin et al. | User guided semantic image adaptation for mobile display devices |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |