JP2024511625A - Conformer-based speech conversion model - Google Patents
Conformer-based speech conversion model Download PDFInfo
- Publication number
- JP2024511625A JP2024511625A JP2023558802A JP2023558802A JP2024511625A JP 2024511625 A JP2024511625 A JP 2024511625A JP 2023558802 A JP2023558802 A JP 2023558802A JP 2023558802 A JP2023558802 A JP 2023558802A JP 2024511625 A JP2024511625 A JP 2024511625A
- Authority
- JP
- Japan
- Prior art keywords
- speech
- spectrogram
- conformer
- blocks
- output
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000006243 chemical reaction Methods 0.000 title claims abstract description 94
- 238000000034 method Methods 0.000 claims abstract description 62
- 230000009466 transformation Effects 0.000 claims abstract description 23
- 238000012549 training Methods 0.000 claims description 52
- 230000008569 process Effects 0.000 claims description 29
- 238000013518 transcription Methods 0.000 claims description 21
- 230000035897 transcription Effects 0.000 claims description 21
- 238000013527 convolutional neural network Methods 0.000 claims description 18
- 230000007246 mechanism Effects 0.000 claims description 17
- 238000012545 processing Methods 0.000 claims description 17
- 238000005070 sampling Methods 0.000 claims description 12
- 239000012634 fragment Substances 0.000 claims description 10
- 238000011176 pooling Methods 0.000 claims description 5
- 230000002123 temporal effect Effects 0.000 claims description 5
- 230000015654 memory Effects 0.000 description 41
- 238000013519 translation Methods 0.000 description 12
- 238000010586 diagram Methods 0.000 description 10
- 206010002026 amyotrophic lateral sclerosis Diseases 0.000 description 9
- 238000004590 computer program Methods 0.000 description 9
- 208000037265 diseases, disorders, signs and symptoms Diseases 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 239000011159 matrix material Substances 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 208000032041 Hearing impaired Diseases 0.000 description 3
- 230000006872 improvement Effects 0.000 description 3
- 230000006403 short-term memory Effects 0.000 description 3
- 238000013528 artificial neural network Methods 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 238000013135 deep learning Methods 0.000 description 2
- 230000006735 deficit Effects 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 201000010099 disease Diseases 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 230000037433 frameshift Effects 0.000 description 2
- 230000001771 impaired effect Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000000926 neurological effect Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 206010013887 Dysarthria Diseases 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000001994 activation Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001186 cumulative effect Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- -1 etc.) Substances 0.000 description 1
- 238000002474 experimental method Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 238000012887 quadratic function Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 230000033764 rhythmic process Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000004984 smart glass Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 208000027765 speech disease Diseases 0.000 description 1
- 238000003786 synthesis reaction Methods 0.000 description 1
- 230000002194 synthesizing effect Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/027—Concept to speech synthesisers; Generation of natural phrases from machine-based concepts
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/003—Changing voice quality, e.g. pitch or formants
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
- G10L13/047—Architecture of speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/03—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters
- G10L25/18—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters the extracted parameters being spectral information of each sub-band
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0316—Speech enhancement, e.g. noise reduction or echo cancellation by changing the amplitude
- G10L21/0364—Speech enhancement, e.g. noise reduction or echo cancellation by changing the amplitude for improving intelligibility
Abstract
音声変換の方法（６００）は、音声変換モデル（２００）のエンコーダ（２１０）への入力として、発話（１０８）に対応する入力スペクトログラム（１０２）を受信する工程を含み、エンコーダは、自己注意ブロック（４００）のスタックを含む。方法は、エンコーダからの出力として、エンコード済みスペクトログラム（２１２）を生成する工程と、音声変換モデルのスペクトログラムデコーダ（２２０）への入力として、エンコーダからの出力として生成されたエンコード済みスペクトログラムを受信する工程とをさらに含む。方法は、スペクトログラムデコーダからの出力として、発話の合成音声表現に対応する出力スペクトログラム（２２２）を生成することをさらに含む。A method (600) of speech transformation includes receiving an input spectrogram (102) corresponding to an utterance (108) as an input to an encoder (210) of a speech transformation model (200), the encoder comprising a self-attention block. (400) stacks. The method includes the steps of: generating an encoded spectrogram (212) as an output from the encoder; and receiving the generated encoded spectrogram as an output from the encoder as an input to a spectrogram decoder (220) of an audio conversion model. further including. The method further includes producing an output spectrogram (222) corresponding to a synthetic speech representation of the utterance as an output from the spectrogram decoder.
Description
本開示は、コンフォーマベースの音声変換モデルに関する。 The present disclosure relates to conformer-based speech conversion models.
音声変換モデルは、音声の言語情報を変更することなくソース話者の音声を別の形式に修正するために使用可能である。例えば、音声変換モデルは、ユーザの音声のトランスクリプトを生成することができる。或いは、音声変換モデルは、ユーザの音声を別の言語の音声の音声波形に変換することができる。機械学習方法は、音声を別の形式に正確かつ効率的に変換する音声変換モデルをトレーニングするために使用可能である。 A speech conversion model can be used to modify a source speaker's speech into another format without changing the linguistic information of the speech. For example, the speech translation model can generate a transcript of the user's speech. Alternatively, the speech conversion model can convert the user's speech into a speech waveform of speech in another language. Machine learning methods can be used to train speech conversion models that accurately and efficiently convert speech to another format.
コンフォーマベースの音声変換モデルを提供する。 Provides a conformer-based speech conversion model.
本開示の一態様は、発話に対応する入力スペクトログラムをエンコードするよう構成された自己注意ブロックのスタックを含むエンコーダを含む音声変換モデルを提供する。音声変換モデルは、エンコーダからエンコード済みスペクトログラムを入力として受信するよう構成されたスペクトログラムデコーダをさらに含む。スペクトログラムデコーダは、発話の合成音声表現に対応する出力スペクトログラムを出力として生成するようさらに構成される。 One aspect of the present disclosure provides a speech conversion model that includes an encoder that includes a stack of self-attention blocks configured to encode an input spectrogram corresponding to an utterance. The audio conversion model further includes a spectrogram decoder configured to receive the encoded spectrogram as input from the encoder. The spectrogram decoder is further configured to produce as output an output spectrogram corresponding to a synthetic speech representation of the utterance.
本開示の実装は、以下の選択的特徴のうちの１つまたは複数を含んでよい。いくつかの実装では、発話に対応する入力スペクトログラムは、非典型的な音声に関連付けられた話者によって話された入力音声から抽出される。これらの実装では、発話の合成音声表現は、発話の合成された標準的で流暢な音声表現を含む。さらに、音声変換モデルは、エンコーダからエンコード済みスペクトログラムを入力として受信するとともに、発話のトランスクリプションに対応するテキスト表現を出力として生成するよう構成された単語片デコーダを含んでよい。 Implementations of this disclosure may include one or more of the following optional features. In some implementations, an input spectrogram corresponding to an utterance is extracted from input speech spoken by a speaker associated with the atypical speech. In these implementations, the synthesized speech representation of the utterance includes a synthesized standard fluent speech representation of the utterance. Furthermore, the speech conversion model may include a word fragment decoder configured to receive the encoded spectrogram from the encoder as input and to generate as output a textual representation corresponding to a transcription of the utterance.
さらに、音声変換モデルは、エンコーダからエンコード済みスペクトログラムを入力として受信するとともに、発話の音素表現を出力として生成するよう構成された音素デコーダを含んでよい。 Additionally, the speech conversion model may include a phoneme decoder configured to receive the encoded spectrogram from the encoder as input and to generate a phoneme representation of the utterance as output.
いくつかの実装では、自己注意ブロックのスタックは、コンフォーマブロックのスタックを含み、各コンフォーマブロックは、マルチヘッド自己注意機構を有する。これらの実装では、エンコーダは、コンフォーマブロックのスタックの前に配置されるとともに、入力スペクトログラムを受信するよう構成された第１のサブサンプリング層をさらに含んでよく、第１のサブサンプリング層は、ＣＮＮ（畳み込みニューラルネットワーク）層を含み、その後、時間方向にプーリングが行われることで、コンフォーマブロックのスタック内の最初のコンフォーマブロックによって処理されるフレームの数を低減する。さらに、これらの実装では、エンコーダは、コンフォーマブロックのスタック内のコンフォーマブロックの最初のセットとコンフォーマブロックのスタック内のコンフォーマブロックの最後のセットとの間に配置された第２のサブサンプリング層を含んでよく、第２のサブサンプリング層は、コンフォーマブロックの最初のセット内の最後のコンフォーマブロックによって出力された隠れ表現をサブサンプリングすることで、コンフォーマブロックの最後のセットによって処理されるフレームの数を低減するよう構成される。これらの実装では、エンコーダは、コンフォーマブロックのスタックの後に配置されたアップサンプリング層をさらに含んでよく、アップサンプリング層は、コンフォーマブロックのスタック内の最後のコンフォーマブロックによって出力された隠れ表現をアップサンプリングするよう構成された単一の転置ＣＮＮ層を含むことで、エンコーダとスペクトログラムデコーダとの間に配置された相互注意機構によって処理されるフレームの数を増加させる。 In some implementations, the stack of self-attention blocks includes a stack of conformer blocks, each conformer block having a multi-head self-attention mechanism. In these implementations, the encoder may further include a first subsampling layer disposed in front of the stack of conformer blocks and configured to receive the input spectrogram, the first subsampling layer comprising: It includes a CNN (Convolutional Neural Network) layer, followed by temporal pooling to reduce the number of frames processed by the first conformer block in the stack of conformer blocks. Additionally, in these implementations, the encoder encodes a second sub-conformer block located between the first set of conformer blocks in the stack of conformer blocks and the last set of conformer blocks in the stack of conformer blocks. a second subsampling layer, the second subsampling layer subsamples the hidden representation output by the last conformer block in the first set of conformer blocks. configured to reduce the number of frames processed. In these implementations, the encoder may further include an upsampling layer placed after the stack of conformer blocks, where the upsampling layer captures the hidden representation output by the last conformer block in the stack of conformer blocks. Including a single transposed CNN layer configured to upsample the spectrogram increases the number of frames processed by the mutual attention mechanism placed between the encoder and the spectrogram decoder.
さらに、音声変換モデルは、標準的で流暢な音声に関連付けられた典型的な話者による複数の話された発話について音声変換モデルを事前トレーニングする第１のトレーニングステップを含む２ステップトレーニングプロセスを使用してトレーニングされてよい。ここで、各話された発話は、発話に対応したグラウンドトゥルースであって合成された標準的で流暢な音声表現とペアにされる。２ステップトレーニングプロセスは、非典型的な音声に関連付けられた話者によって話された複数の非典型的な音声サンプルに基づいて、事前トレーニングされた音声変換モデルのパラメータを微調整する第２のトレーニングステップをさらに含む。 Additionally, the speech translation model uses a two-step training process that includes a first training step to pre-train the speech translation model on multiple spoken utterances by typical speakers associated with standard, fluent speech. may be trained. Here, each spoken utterance is paired with a ground truth, synthesized standard fluent speech representation corresponding to the utterance. The two-step training process includes a second training step that fine-tunes the parameters of the pre-trained speech conversion model based on multiple atypical speech samples spoken by speakers associated with the atypical speech. further comprising steps.
いくつかの実装では、スペクトログラムデコーダは、発話のトランスクリプションに対応するテキスト表現に対して中間のテキスト－音声変換を実行することなく、エンコード済みスペクトログラムから出力スペクトログラムを直接的に生成する。 In some implementations, the spectrogram decoder generates the output spectrogram directly from the encoded spectrogram without performing intermediate text-to-speech conversion on the textual representation corresponding to the transcription of the utterance.
本開示の別の態様は、音声変換モデルのためにコンピュータが実施する方法を提供する。コンピュータが実施する方法は、データ処理ハードウェア上で実行されると、データ処理ハードウェアに動作を実行させる。動作は、音声変換モデルのエンコーダへの入力として、発話に対応する入力スペクトログラムを受信する工程を含み、エンコーダは、自己注意ブロックのスタックを含む。動作は、エンコーダからの出力として、エンコード済みスペクトログラムを生成する工程をさらに含む。動作は、音声変換モデルのスペクトログラムデコーダへの入力として、エンコーダからの出力として生成されたエンコード済みスペクトログラムを受信する工程を含む。動作は、スペクトログラムデコーダからの出力として、発話の合成音声表現に対応する出力スペクトログラムを生成する、出力スペクトログラム生成工程をさらに含む。 Another aspect of the disclosure provides a computer-implemented method for a speech conversion model. The computer-implemented method, when executed on data processing hardware, causes the data processing hardware to perform an operation. The operations include receiving an input spectrogram corresponding to an utterance as an input to an encoder of a speech transformation model, the encoder including a stack of self-attention blocks. The operations further include generating an encoded spectrogram as an output from the encoder. The operations include receiving an encoded spectrogram produced as an output from the encoder as an input to a spectrogram decoder of the speech transformation model. The operations further include generating an output spectrogram that corresponds to a synthesized speech representation of the utterance as an output from the spectrogram decoder.
この態様は、以下の選択的特徴のうちの１つまたは複数を含んでよい。いくつかの実装では、発話に対応する入力スペクトログラムは、非典型的な音声に関連付けられた話者によって話された入力音声から抽出される。これらの実装では、発話の合成音声表現は、発話の合成された標準的で流暢な音声表現を含む。 This aspect may include one or more of the following optional features. In some implementations, an input spectrogram corresponding to an utterance is extracted from input speech spoken by a speaker associated with the atypical speech. In these implementations, the synthesized speech representation of the utterance includes a synthesized standard fluent speech representation of the utterance.
いくつかの実装では、動作は、音声変換モデルの単語片デコーダへの入力として、エンコーダからの出力として生成されたエンコード済みスペクトログラムを受信する工程を含む。これらの実装は、単語片デコーダからの出力として、発話のトランスクリプションに対応するテキスト表現を生成する工程をさらに含む。動作は、音声変換モデルの音素デコーダへの入力として、エンコーダからの出力として生成されるエンコード済みスペクトログラムを受信する工程と、音素デコーダからの出力として、発話の音素表現を生成する工程とをさらに含んでよい。 In some implementations, the operations include receiving the encoded spectrogram generated as an output from the encoder as an input to a word piece decoder of the speech transformation model. These implementations further include generating a textual representation corresponding to the transcription of the utterance as an output from the word fragment decoder. The operations further include receiving the encoded spectrogram produced as an output from the encoder as an input to a phoneme decoder of the speech conversion model, and producing a phoneme representation of the utterance as an output from the phoneme decoder. That's fine.
いくつかの実装では、自己注意ブロックのスタックは、コンフォーマブロックのスタックを含み、各コンフォーマブロックは、マルチヘッド自己注意機構を有する。これらの実装では、エンコーダは、コンフォーマブロックのスタックの前に配置されるとともに、入力スペクトログラムを受信するよう構成された第１のサブサンプリング層をさらに含んでよく、第１のサブサンプリング層は、ＣＮＮ（畳み込みニューラルネットワーク）層を含み、その後、時間方向にプーリングが行われることで、コンフォーマブロックのスタック内の最初のコンフォーマブロックによって処理されるフレームの数を低減する。さらに、これらの実装では、エンコーダは、コンフォーマブロックのスタック内のコンフォーマブロックの最初のセットとコンフォーマブロックのスタック内のコンフォーマブロックの最後のセットとの間に配置された第２のサブサンプリング層を含んでよく、第２のサブサンプリング層は、コンフォーマブロックの最初のセット内の最後のコンフォーマブロックによって出力された隠れ表現をサブサンプリングすることで、コンフォーマブロックの最後のセットによって処理されるフレームの数を低減するよう構成される。これらの実装では、エンコーダは、コンフォーマブロックのスタックの後に配置されたアップサンプリング層をさらに含んでよく、アップサンプリング層は、コンフォーマブロックのスタック内の最後のコンフォーマブロックによって出力された隠れ表現をアップサンプリングするよう構成された単一の転置ＣＮＮ層を含むことで、エンコーダとスペクトログラムデコーダとの間に配置された相互注意機構によって処理されるフレームの数を増加させる。 In some implementations, the stack of self-attention blocks includes a stack of conformer blocks, each conformer block having a multi-head self-attention mechanism. In these implementations, the encoder may further include a first subsampling layer disposed in front of the stack of conformer blocks and configured to receive the input spectrogram, the first subsampling layer comprising: It includes a CNN (Convolutional Neural Network) layer, followed by temporal pooling to reduce the number of frames processed by the first conformer block in the stack of conformer blocks. Additionally, in these implementations, the encoder encodes a second sub-conformer block located between the first set of conformer blocks in the stack of conformer blocks and the last set of conformer blocks in the stack of conformer blocks. a second subsampling layer, the second subsampling layer subsamples the hidden representation output by the last conformer block in the first set of conformer blocks. configured to reduce the number of frames processed. In these implementations, the encoder may further include an upsampling layer placed after the stack of conformer blocks, where the upsampling layer captures the hidden representation output by the last conformer block in the stack of conformer blocks. Including a single transposed CNN layer configured to upsample the spectrogram increases the number of frames processed by the mutual attention mechanism placed between the encoder and the spectrogram decoder.
さらに、音声変換モデルは、標準的で流暢な音声に関連付けられた典型的な話者による複数の話された発話について音声変換モデルを事前トレーニングする第１のトレーニングステップを含む２ステップトレーニングプロセスを使用してトレーニングされてよい。ここで、各話された発話は、発話に対応したグラウンドトゥルースであって合成された標準的で流暢な音声表現とペアにされる。２ステップトレーニングプロセスは、非典型的な音声に関連付けられた話者によって話された複数の非典型的な音声サンプルに基づいて、事前トレーニングされた音声変換モデルのパラメータを微調整する第２のトレーニングステップをさらに含む。 Additionally, the speech translation model uses a two-step training process that includes a first training step to pre-train the speech translation model on multiple spoken utterances by typical speakers associated with standard, fluent speech. may be trained. Here, each spoken utterance is paired with a ground truth, synthesized standard fluent speech representation corresponding to the utterance. The two-step training process includes a second training step that fine-tunes the parameters of the pre-trained speech conversion model based on multiple atypical speech samples spoken by speakers associated with the atypical speech. further comprising steps.
いくつかの実装では、スペクトログラムデコーダは、発話のトランスクリプションに対応するテキスト表現に対して中間のテキスト－音声変換を実行することなく、エンコード済みスペクトログラムから出力スペクトログラムを直接的に生成する。 In some implementations, the spectrogram decoder generates the output spectrogram directly from the encoded spectrogram without performing intermediate text-to-speech conversion on the textual representation corresponding to the transcription of the utterance.
本開示の１つまたは複数の実装の詳細は、添付の図面および以下の説明に記載される。他の態様、特徴、および利点は、説明および図面から、ならびに特許請求の範囲から明らかになるであろう。 The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
様々な図面における同様の参照符号は、同様の要素を示す。
より包括的な音声技術、特に発話障害を有する人々を助けることができる音声技術を開発することへの関心が高まっている。ＡＳＲ（自動音声認識）は、正確なトランスクリプションへの変換のために、構音障害または非典型的な音声パターンを有する話者からの音声を認識するＥ２Ｅ（エンドツーエンド）深層学習ベースのモデルの導入によって、途方もなく進歩してきた。例えば、非典型的な音声パターンは、身体的または神経学的状態（例えば、ＡＬＳ（筋萎縮性側索硬化症）疾患を有する話者）に起因する障害のある音声、重度のアクセントのある音声、および聴覚障害のある音声を含み得るが、これに限定されない。音声変換システムは、同様の深層学習ベースのモデルを適用することで、非典型的な音声パターンを有する音声を標準的で流暢な出力音声に変換することができる。
Like reference numbers in the various drawings indicate similar elements.
There is growing interest in developing more comprehensive speech technologies, particularly those that can help people with speech disorders. ASR (Automatic Speech Recognition) is an E2E (end-to-end) deep learning-based model that recognizes speech from speakers with dysarthria or atypical speech patterns for conversion into accurate transcriptions. Since its introduction, tremendous progress has been made. For example, atypical speech patterns include impaired speech due to physical or neurological conditions (e.g., speakers with ALS (amyotrophic lateral sclerosis) disease), severely accented speech, , and may include, but are not limited to, hearing impaired speech. Speech conversion systems can apply similar deep learning-based models to convert speech with atypical speech patterns into standard, fluent output speech.
マッチングされたトレーニングデータ分布およびテストデータ分布は、音声変換モデルをトレーニングするための最適な性能をもたらすことが知られている。しかしながら、発話障害を有する話者からのトレーニングデータが不十分なので、現在の方法を使用してモデルをトレーニングすることは困難になり得る。さらに、発話障害を有するユーザは、モデルを十分にトレーニングするために必要なだけのデータを記録することに多大な労力を要すると感じ得るので、そのようなトレーニングデータを得ることは困難である。本開示は、エンコーダ／デコーダアーキテクチャを有する音声変換モデルに対する改善を提供する。この改善は、必要なトレーニングデータを少なくし、音声変換モデルのトレーニングを促進し、モデルが大きなユーザの集団に拡大することを可能にし、さらに広範囲の非典型的な音声に対して対応できる。本開示は、エンコーダアクティベーションのサブサンプルと、典型的なエンコーダ出力の対応するアップサンプリングとを使用して、音声変換モデルに対するアーキテクチャ変更を通じてこの改善を提供する。本開示は、タスクのための共有エンコーダアーキテクチャを使用して推論中に音声とテキストの両方を共同でデコードする統合モデルにおける多対１のＶＣ（音声変換）とＡＳＲとの組合せをさらに提供する。 Matched training data distributions and test data distributions are known to yield optimal performance for training speech conversion models. However, training models using current methods can be difficult due to insufficient training data from speakers with speech impairments. Furthermore, it is difficult to obtain such training data, as users with speech impairments may find it a great effort to record as much data as is necessary to adequately train the model. The present disclosure provides improvements to speech conversion models with encoder/decoder architectures. This improvement requires less training data, speeds up the training of the speech conversion model, allows the model to scale to larger user populations, and accommodates a wider range of atypical speech. The present disclosure provides this improvement through architectural changes to the speech conversion model using subsampling of encoder activations and corresponding upsampling of typical encoder outputs. The present disclosure further provides a many-to-one VC (speech conversion) and ASR combination in an integrated model that jointly decodes both speech and text during inference using a shared encoder architecture for the task.
本明細書で使用される場合、別段の指定がない限り、「音声変換システム」および「音声変換モデル」という用語は、入力された非典型的な音声が認識されるとともに、対応するテキスト（例えば、トランスクリプション）および／または非典型的な音声を表す音素のセットに変換されるＡＳＲシステム／モデル、または入力された非典型的な音声が音声認識を実行せずに標準的で流暢な合成音声に直接的に変換される音声－音声変換システム／モデルの任意の組合せを指し得る。別の言い方をすれば、音声変換システム／モデルは、入力音声波形を中間表現（例えば、テキストまたは音素）に変換することなく、非典型的な音声に対応する入力音声波形またはスペクトログラムを、標準的で流暢な音声に対応する出力音声波形またはスペクトログラムに直接的に変換するよう構成される。明らかになるように、音声変換モデル、および音声変換モデルをトレーニングするための技法は、ユーザの意図した音声の認識および／または再生を可能にすることによって、非典型的な音声を有するユーザが、他の人間および音声インターフェース（例えば、デジタルアシスタント）の両方と話すこと、およびそれらによって理解されることを可能にする。本明細書の例は、標準的で流暢な音声に対応する出力音声波形またはスペクトログラムへの変換のための、非典型的な音声に対応する入力音声波形またはスペクトログラムを受信する音声変換モデルを示すが、音声変換モデルは、本開示の範囲から逸脱することなく、他の種類の音声変換タスクを実行するように同様に適合されてよい。例えば、音声変換モデルは、第１言語での発話に対応する入力音声波形またはスペクトログラムを、異なる第２言語での発話の翻訳に対応する出力音声波形またはスペクトログラムに変換してよい。音声変換モデルは、同様に、ユーザによる発話入力を受信し、発話入力と同じ言語内容を含むが、ターゲット話者の異なる音声特性を有する合成音声を出力してもよい。 As used herein, unless otherwise specified, the terms "speech conversion system" and "speech conversion model" mean that input atypical speech is recognized and the corresponding text (e.g. , transcription) and/or ASR systems/models where the input atypical speech is converted into a set of phonemes representing atypical speech, or where the input atypical speech is synthesized into a standard, fluent synthesis without performing speech recognition. Can refer to any combination of speech-to-speech conversion systems/models that convert directly to speech. In other words, the speech conversion system/model converts the input speech waveform or spectrogram corresponding to atypical speech into a standard one without converting the input speech waveform into an intermediate representation (e.g., text or phonemes). and is configured to convert directly into an output speech waveform or spectrogram corresponding to fluent speech. As will be apparent, speech translation models, and techniques for training speech translation models, enable users with atypical speech to recognize and/or reproduce the user's intended speech. Allows to speak to and be understood by both other humans and voice interfaces (e.g. digital assistants). Examples herein illustrate a speech conversion model that receives an input speech waveform or spectrogram corresponding to atypical speech for conversion to an output speech waveform or spectrogram corresponding to standard, fluent speech. , the speech conversion model may be similarly adapted to perform other types of speech conversion tasks without departing from the scope of this disclosure. For example, the speech translation model may convert an input speech waveform or spectrogram corresponding to an utterance in a first language to an output speech waveform or spectrogram corresponding to a translation of the utterance in a different second language. A speech conversion model may similarly receive speech input by a user and output a synthesized speech containing the same linguistic content as the speech input, but having different speech characteristics of the target speaker.
図１は、音声変換モデル２００およびボコーダ３７４を含む音声変換システム１００を示す。音声変換モデル２００は、非典型的な音声に関連付けられたソース話者１０４によって話された発話１０８に対応する入力音声データ１０２を、ターゲット話者１０４によって話された同じ発話１１４の合成された標準的で流暢な音声表現に対応する出力音声データ１０６に変換するよう構成される。本明細書で使用される場合、入力音声データ１０２は、発話１０８に対応する入力スペクトログラムを含み得る。本明細書で使用される場合、出力音声データ１０６は、同じ発話１１４の合成された標準的で流暢な音声表現に対応する出力スペクトログラム２２２、またはボコーダ３７５によって出力スペクトログラム２２２から変換された時間領域音声波形３７６を含み得る。図示されていないが、ユーザデバイス１１０上に常駐する音響フロントエンドは、ユーザデバイス１１０のマイクロフォンを介してキャプチャされた発話１０８の時間領域音声波形を入力スペクトログラム１０２または他の種類の音声データ１０２に変換してよい。いくつかの実装では、音声変換モデル２００は、発話１０８に対応する入力音声データ１０２を、発話１０８のトランスクリプション２０１または音素表現２０２に対応するテキスト表現（例えば、書記素、単単語片、または単語）に変換する。いくつかの追加的な実装では、音声変換システム１００の音声変換モデル２００は、音声認識を実行することなく、または入力音声データ１０２から中間の離散的表現（例えば、テキストまたは音素）の生成を必要とすることなく、入力音声データ１０２（例えば、入力スペクトログラム）を出力音声データ１０６（例えば、出力スペクトログラム２２２）に直接的に変換するよう構成される。 FIG. 1 shows a speech conversion system 100 that includes a speech conversion model 200 and a vocoder 374. Speech transformation model 200 converts input speech data 102 corresponding to an utterance 108 spoken by a source speaker 104 associated with an atypical speech into a synthesized standard of the same utterance 114 spoken by a target speaker 104. The output audio data 106 is configured to convert into output audio data 106 that corresponds to an accurate and fluent audio expression. As used herein, input audio data 102 may include an input spectrogram corresponding to utterance 108. As used herein, output audio data 106 is output spectrogram 222 corresponding to a synthesized canonical fluent speech representation of the same utterance 114, or time-domain audio converted from output spectrogram 222 by vocoder 375. Waveform 376 may be included. Although not shown, an acoustic front end residing on the user device 110 converts the time-domain audio waveform of the speech 108 captured via the user device 110 microphone into an input spectrogram 102 or other type of audio data 102. You may do so. In some implementations, the speech conversion model 200 converts the input speech data 102 corresponding to the utterance 108 into a transcription 201 of the utterance 108 or a textual representation (e.g., a grapheme, single word piece, or word). In some additional implementations, the speech conversion model 200 of the speech conversion system 100 does not perform speech recognition or requires the generation of intermediate discrete representations (e.g., text or phonemes) from the input speech data 102. The input audio data 102 (e.g., input spectrogram) is configured to directly convert input audio data 102 (e.g., input spectrogram) to output audio data 106 (e.g., output spectrogram 222) without having to do so.
音声変換モデル２００は、入力スペクトログラム１０２をエンコード済みスペクトログラム２１２（例えば、一連のベクトルを含む隠れ特徴表現）にエンコードするよう構成されたスペクトログラムエンコーダ２１０と、エンコード済みスペクトログラム２１２を、合成された標準的で流暢な音声表現、トランスクリプション２０１、および／または音素表現２０２に対応する出力スペクトログラム２２２にデコードするよう構成された１つまたは複数のデコーダ２２０、２２０ａ～ｃとを含む。トランスクリプト２０１は、人間の読み手によって理解され得る、および／または下流のアプリケーション（例えば、デジタルアシスタント）によって理解され得る、発話１０８の標準的で流暢なトランスクリプションを含んでよい。 The speech conversion model 200 includes a spectrogram encoder 210 configured to encode the input spectrogram 102 into an encoded spectrogram 212 (e.g., a hidden feature representation including a sequence of vectors) and a spectrogram encoder 210 configured to encode the input spectrogram 102 into an encoded spectrogram 212 (e.g., a hidden feature representation that includes a sequence of vectors), and a spectrogram encoder 210 configured to encode the encoded spectrogram 212 into a synthesized standard one or more decoders 220, 220a-c configured to decode into an output spectrogram 222 corresponding to a fluent phonetic representation, a transcription 201, and/or a phoneme representation 202. Transcript 201 may include a standard, fluent transcription of utterance 108 that can be understood by a human reader and/or by a downstream application (eg, a digital assistant).
エンコーダ２１０は、コンフォーマまたはトランスフォーマを含み得るマルチヘッド注意ブロック４００（本明細書ではコンフォーマブロック４００と呼ばれる）のスタックを含み得る。各マルチヘッド注意ブロック４００は、マルチヘッド注意機構４２０（図４）を含んでよい。コンフォーマブロック４００は、エンコーダ２１０によって実装されて、入力される非典型的な音声の高分解能なスペクトルパターン（ｐａｔｔｅｒｓ）をキャプチャしてよい。例えば、スペクトログラムエンコーダ２１０が発話１０８の入力音声データ１０２を受信すると、スペクトログラムエンコーダ２１０は、コンフォーマブロック４００を使用して入力スペクトログラム１０２の１０ミリ秒（ｍｓ）の音声サンプルを処理することで、アップサンプリングされた４０ｍｓのエンコード済みスペクトログラム２１２を生成してよい。エンコーダ２１０のコンフォーマブロック４００によるアップサンプリングのプロセスは、以下の図３および図４を用いてより詳細に説明される。次に、スペクトログラムデコーダ２２０ａは、スペクトログラムエンコーダ２１０から出力されたアップサンプリングされたエンコード済みスペクトログラム２１２に基づいて、合成された標準的で流暢な音声表現に対応する出力スペクトログラム２２２を生成してよい。例えば、スペクトログラムデコーダ２２０ａは、入力スペクトログラム１０２の１０ｍｓの音声サンプルを表す、アップサンプリングされた４０ｍｓのエンコード済みスペクトログラム２１２を、スペクトログラムエンコーダ２１０から受信してよい。ここで、相互注意機構２３１、２３１ａ（図２および図３）を通じて、スペクトログラムデコーダ２２０ａは、１０ｍｓの入力音声データ１０２として意図された単語または単語の一部を含むが非典型的な音声の非流暢性を含まない、発話１１４の合成された標準的で流暢な音声表現に対応する１２．５ｍｓの出力スペクトログラム２２２を生成してよい。 Encoder 210 may include a stack of multi-head attention blocks 400 (referred to herein as conformer blocks 400) that may include conformers or transformers. Each multi-head attention block 400 may include a multi-head attention mechanism 420 (FIG. 4). Conformer block 400 may be implemented by encoder 210 to capture high-resolution spectral patterns of input atypical speech. For example, when spectrogram encoder 210 receives input audio data 102 for utterance 108 , spectrogram encoder 210 uses conformer block 400 to process 10 millisecond (ms) audio samples of input spectrogram 102 . A sampled 40 ms encoded spectrogram 212 may be generated. The process of upsampling by conformer block 400 of encoder 210 is explained in more detail with reference to FIGS. 3 and 4 below. Spectrogram decoder 220a may then generate an output spectrogram 222 corresponding to a synthesized standard fluent speech representation based on the upsampled encoded spectrogram 212 output from spectrogram encoder 210. For example, spectrogram decoder 220 a may receive an upsampled 40 ms encoded spectrogram 212 from spectrogram encoder 210 that represents a 10 ms audio sample of input spectrogram 102 . Here, through the mutual attention mechanism 231, 231a (FIGS. 2 and 3), the spectrogram decoder 220a detects non-fluent speech that contains the intended word or part of a word but is atypical as the 10 ms input speech data 102. A 12.5 ms output spectrogram 222 may be generated that corresponds to a synthesized standard fluent speech representation of the utterance 114 without gender.
いくつかの例では、音声変換モデル２００はまた、エンコード済みスペクトログラム２１２をテキスト表現、例えばトランスクリプション２０１にデコードする単語片デコーダ２２０ｂを含む。例えば、単語片デコーダ２２０ｂは、エンコード済みスペクトログラム２１２を、トランスクリプション２０１を形成し得る対応する単語片にデコードするようトレーニングされてよい。図示の例では、モデル２００によって単語片デコーダ２２０ｂが使用されているが、モデル２００は、代わりに、エンコード済みスペクトログラムを書記素または単語にデコードするよう構成された書記素デコーダ２２０ｂまたは単語デコーダ２２０ｂを使用してよい。追加的または代替的には、音声変換モデル２００は、エンコード済みスペクトログラム２１２を、発話１１４の合成された標準的で流暢な音声表現を示す音素を含む音素表現２０２にデコードする音素デコーダ２２０ｃを含んでもよい。したがって、スペクトログラム、単語片、および音素デコーダ２２０ａ～ｃは、音声変換モデル２００の並列のデコード分岐に対応してよい。デコード分岐のそれぞれは、スペクトログラムエンコーダ２１０によってエンコードされた、アップサンプリングされたエンコード済みスペクトログラム２１２を受信するとともに、出力スペクトログラム２２２、トランスクリプション２０１、および音素表現２０２のうちの対応するものを並列に出力する。音声変換システム１００のボコーダ３７５（シンセサイザ３７５とも呼ばれる）は、スペクトログラムデコーダ２２０ａによって出力された出力スペクトログラム２２２を、別のコンピューティングデバイス１１６からの可聴出力のために、同じ発話１１４の合成された標準的で流暢な音声の時間領域波形３７６に変換するよう構成される。時間領域音声波形は、経時的な音声信号の振幅を定める音声波形を含む。ボコーダ３７５は、出力スペクトログラム２２２を標準的で流暢な音声の時間領域波形に合成するためのユニット選択モジュールまたはＷａｖｅＮｅｔモジュールを含んでよい。いくつかの実装では、シンセサイザ３７５は、ボコーダネットワーク、すなわち、時間領域音声波形への変換のためにメル周波数スペクトログラム上で別個にトレーニングされ調整されるニューラルボコーダを含む。いくつかの追加的な例では、ボコーダ３７５は、ストリーミングＧｒｉｆｆｉｎ－Ｌｉｍボコーダなどのストリーミングボコーダ３７５を含む。例示的ストリーミングボコーダは、２０２２年２月２１日に出願された米国仮出願第６３／３１２１９５号に説明されており、その全体の内容は、参照により組み込まれる。 In some examples, speech conversion model 200 also includes a word fragment decoder 220b that decodes encoded spectrogram 212 into a textual representation, such as transcription 201. For example, word piece decoder 220b may be trained to decode encoded spectrogram 212 into corresponding word pieces that may form transcription 201. Although in the illustrated example, word piece decoder 220b is used by model 200, model 200 may instead use grapheme decoder 220b or word decoder 220b configured to decode encoded spectrograms into graphemes or words. May be used. Additionally or alternatively, the speech conversion model 200 may include a phoneme decoder 220c that decodes the encoded spectrogram 212 into a phoneme representation 202 that includes phonemes indicative of a synthesized canonical fluent phonetic representation of the utterance 114. good. Accordingly, spectrogram, word fragment, and phoneme decoders 220a-c may correspond to parallel decoding branches of speech conversion model 200. Each of the decoding branches receives an upsampled encoded spectrogram 212 encoded by a spectrogram encoder 210 and outputs a corresponding one of an output spectrogram 222, a transcription 201, and a phoneme representation 202 in parallel. do. Vocoder 375 (also referred to as synthesizer 375) of speech conversion system 100 converts the output spectrogram 222 output by spectrogram decoder 220a into a synthesized standard version of the same utterance 114 for audible output from another computing device 116. 376 into a fluent speech time-domain waveform 376. Time-domain audio waveforms include audio waveforms that define the amplitude of an audio signal over time. Vocoder 375 may include a unit selection module or a WaveNet module for synthesizing output spectrogram 222 into a standard, fluent speech time-domain waveform. In some implementations, synthesizer 375 includes a vocoder network, ie, a neural vocoder that is separately trained and tuned on the mel frequency spectrogram for conversion to a time domain speech waveform. In some additional examples, vocoder 375 includes a streaming vocoder 375, such as a streaming Griffin-Lim vocoder. An exemplary streaming vocoder is described in US Provisional Application No. 63/312,195, filed February 21, 2022, the entire contents of which are incorporated by reference.
示された例では、ソース話者１０４は、理解が困難であり得る非典型的な音声パターンでソース話者１０４が話すような、非典型的な音声に関連付けられている。非典型的な音声パターンは、身体的または神経学的状態（例えば、ＡＬＳ（筋萎縮性側索硬化症）疾患を有する話者）に起因する障害のある発話、重度のアクセントのある発話、および聴覚障害のある発話を含み得るが、これに限定されない。例として、ソース話者１０４は、ＡＬＳ疾患を有するとともに、ＡＬＳ疾患による非典型的な音声に関連付けられている。したがって、音声変換モデル２００は、ＡＬＳ音声に関連付けられたソース話者１０４によって話された発話１０８に対応する入力スペクトログラム１０２を、同じ発話１０８の合成された標準的で流暢な音声表現に対応する出力スペクトログラム２２２に直接的に変換するようトレーニングされる。したがって、出力スペクトログラム２２２によって提供された合成された標準的で流暢な音声表現は、ソース話者１０４によって話されたＡＬＳ音声の了解度を改善する。本開示の範囲から逸脱することなく、音声変換モデル２００は、第１言語の発話１０８に対応する入力スペクトログラム１０２を、ソース話者と同じ声であるが異なる第２言語の発話１０８の合成音声表現に対応する出力スペクトログラム２２２に直接的に変換する多言語音声変換モデルとしてトレーニングされてもよい。さらに、モデル２０は、第１の音声特性を有するソース話者によって話された発話１０８に対応する入力スペクトログラム１０２を、ターゲット話者に対応した異なる音声特性を有する同じ発話１０８の合成音声表現に対応する出力スペクトログラム２２２に直接的に変換するようトレーニングされてもよい。 In the example shown, source speaker 104 is associated with atypical speech, such as source speaker 104 speaking in an atypical speech pattern that may be difficult to understand. Atypical speech patterns include impaired speech due to physical or neurological conditions (e.g., speakers with ALS (amyotrophic lateral sclerosis) disease), heavily accented speech, and May include, but is not limited to, hearing impaired speech. As an example, source speaker 104 has ALS disease and is associated with atypical speech due to ALS disease. Thus, the speech transformation model 200 converts an input spectrogram 102 corresponding to an utterance 108 spoken by a source speaker 104 associated with an ALS speech into an output corresponding to a synthesized canonical, fluent speech representation of the same utterance 108. It is trained to convert directly to a spectrogram 222. Therefore, the synthesized standard fluent speech representation provided by output spectrogram 222 improves the intelligibility of the ALS speech spoken by source speaker 104. Without departing from the scope of this disclosure, the speech transformation model 200 converts the input spectrogram 102 corresponding to the first language utterance 108 into a synthesized speech representation of the second language utterance 108 in the same voice as the source speaker but different. may be trained as a multilingual speech translation model that directly translates into an output spectrogram 222 corresponding to . Additionally, the model 20 maps the input spectrogram 102 corresponding to an utterance 108 spoken by a source speaker with a first phonetic characteristic to a synthesized speech representation of the same utterance 108 with different phonetic characteristics corresponding to a target speaker. may be trained to convert directly to an output spectrogram 222.
ソース話者１０４に関連付けられたコンピューティングデバイス１１０は、ソース話者１０４によって話された発話１０８をキャプチャするとともに、出力スペクトログラム２２２、トランスクリプション２０１、または音素表現２０２のうちのいずれかへの変換のために、対応する入力音声データ１０２を音声－音声変換システム１００に提供してよい。コンピューティングデバイス１１０は、スマートフォン、タブレット、デスクトップ／ラップトップコンピュータ、スマートスピーカ、スマートディスプレイ、スマート家電、アシスタント対応ウェアラブルデバイス（例えば、スマートウォッチ、スマートヘッドフォン、スマートグラスなど）、または車両インフォテインメント（ｉｎｆｏｔａｉｎｍｅｎｔ）システムを含み得るが、これに限定されない。その後、音声変換システム１００は、ボコーダ３７５を使用することで、出力スペクトログラム２２２を時間領域音声波形３７６に変換してよい。時間領域音声波形３７６は、合成された標準的で流暢な音声の発話１１４としてコンピューティングデバイス１１０または別のコンピューティングデバイス１１６から可聴的に出力されてよい。本開示の範囲から逸脱することなく、音声変換システム１００はまた、ソース話者１０４によって話された同じ発話１１４の合成された標準的で流暢な音声表現に対応するトランスクリプション２０１および／または音素表現２０２を、ユーザ１１８に関連付けられた別のコンピューティングデバイス１１６に提供してよい。これによって、別のコンピューティングデバイス１１６は、標準的なトランスクリプション２０１をソース話者１０４によって話された発話１０８の理解可能な表現として表示してよく、および／またはトランスクリプション２０１または音素表現２０２を合成された標準的で流暢な音声に変換するＴＴＳ（テキスト－音声）システムを使用してよい。この例では、ソース話者１０４およびユーザ１１８は、電話または他の種類の音声通信プロトコル、例えば、ボイスオーバインターネットプロトコルを介して、それぞれのコンピューティングデバイス１１０、１１６を通じて会話をしている。ソース話者１０４および他のユーザ１１８は、同一の言語を話し得るが、ソース話者１０４は、医学的状態（例えば、非典型的な音声）、強いアクセント、または異なる母国語による非典型的な音声を有するので、他のユーザ１１８がソース話者１０４を理解することは困難であり得る。したがって、ソース話者１０４は、理解が困難であり得る非典型的な音声（例えば、ＡＬＳ音声）で話すが、合成された標準的で流暢な音声表現を聞いている他のユーザ１１８は、ソース話者１０４によって意図された発話１０８の理解がより容易になる。別の言い方をすれば、合成された標準的で流暢な音声表現は、非典型的な音声でターゲット話者によって話された元の発話１０８よりも別のユーザにとって理解が容易であり得る、より一貫性のあるリズムを提供する。特に、合成された標準的で流暢な音声表現は、ソース話者１０４の声で話される。しかしながら、用途に応じて、音声変換システム１００は、合成された標準的で流暢な音声を、ソース話者とは異なる音声特性を有するターゲット話者の声で生成してもよい。 A computing device 110 associated with a source speaker 104 captures the utterance 108 spoken by the source speaker 104 and converts it into either an output spectrogram 222, a transcription 201, or a phoneme representation 202. Corresponding input audio data 102 may be provided to the speech-to-speech conversion system 100 for the purpose of processing. Computing device 110 may be a smartphone, tablet, desktop/laptop computer, smart speaker, smart display, smart home appliance, assistant-enabled wearable device (e.g., smart watch, smart headphones, smart glasses, etc.), or a vehicle infotainment device. ) systems may include, but are not limited to. Audio conversion system 100 may then convert output spectrogram 222 to a time-domain audio waveform 376 using vocoder 375 . The time-domain speech waveform 376 may be audibly output from the computing device 110 or another computing device 116 as a synthesized standard fluent speech utterance 114. Without departing from the scope of this disclosure, speech conversion system 100 also includes transcriptions 201 and/or phonemes corresponding to synthesized canonical and fluent speech representations of the same utterance 114 spoken by source speaker 104. Representation 202 may be provided to another computing device 116 associated with user 118. Thereby, another computing device 116 may display standard transcription 201 as an understandable representation of utterance 108 spoken by source speaker 104, and/or transcription 201 or a phonemic representation. A TTS (text-to-speech) system may be used to convert the 202 into synthesized standard fluent speech. In this example, source speaker 104 and user 118 are conversing through their respective computing devices 110, 116 via telephone or other type of voice communication protocol, such as Voice over Internet Protocol. Source speaker 104 and other users 118 may speak the same language, but source speaker 104 may have an atypical voice due to a medical condition (e.g., atypical voice), a strong accent, or a different native language. Because of the voice, it may be difficult for other users 118 to understand the source speaker 104. Thus, although the source speaker 104 speaks in an atypical voice (e.g., ALS voice) that may be difficult to understand, other users 118 listening to the synthesized standard, fluent speech expression may The utterance 108 intended by the speaker 104 is easier to understand. In other words, the synthesized standard and fluent speech representation may be easier to understand for another user than the original utterance 108 spoken by the target speaker in an atypical voice. Provide a consistent rhythm. In particular, the synthesized standard fluent speech expression is spoken in the voice of the source speaker 104. However, depending on the application, speech conversion system 100 may generate synthesized, standard, fluent speech with a target speaker's voice having different speech characteristics than the source speaker.
いくつかの追加的な例では、音声変換システム１００は、ソース話者１０４によって話された発話の合成された標準的で流暢な音声表現に対応する出力音声データ１０６を、合成された標準的で流暢な音声表現をソース話者１０４の声で可聴的に聞き手に出力するための出力音声デバイスに送る。例えば、ソース話者１０４は、クラスの学生に講義を行う心理学教授であってもよく、ソース話者１０４によって話される発話は、特定のドメイン、例えば心理学に属する医学用語を含む。明らかになるように、音声－音声変換モデル２００は、特定のドメインに関連付けられた言語的多様性を学習するとともに、ソース話者１０４に関連付けられた特定の種類の非典型的な音声に関連付けられた音響的多様性を学習するようトレーニングされる。 In some additional examples, speech conversion system 100 converts output speech data 106 corresponding to a synthesized canonical fluent speech representation of the utterance spoken by source speaker 104 into a synthesized canonical fluent speech representation. A fluent speech representation is sent to an output audio device for audible output to a listener in the voice of the source speaker 104. For example, source speaker 104 may be a psychology professor giving a lecture to students in a class, and the utterances spoken by source speaker 104 include medical terms belonging to a particular domain, eg, psychology. As will be apparent, the speech-to-speech conversion model 200 learns the linguistic diversity associated with a particular domain and learns the linguistic diversity associated with a particular type of atypical speech associated with the source speaker 104. trained to learn acoustic diversity.
或いは、他のコンピューティングデバイス１１６は、音声変換システム１００が、合成された標準的で流暢な音声表現に対応する出力音声データ１０６を、認識されたテキストへの変換のためのＡＳＲ（自動音声認識）システムへの入力として提供するフロントエンドとして機能する、下流のＡＳＲシステムに関連付けられてよい。認識されたテキストは、他のユーザ１１８に提示されてよく、および／またはさらなる処理のためにＮＬＵ（自然言語理解）システムに提供されてよい。音声変換システム１００の機能は、リモートサーバ１１２、コンピューティングデバイス１１０、１１６のいずれかもしくは両方、またはリモートサーバおよびコンピューティングデバイス１１０、１１６の任意の組合せ上に常駐してよい。音声変換システム１００は、音声変換モデル２００がコンピューティングデバイス１１０またはリモートサーバ１１２のうちの１つに常駐し、ボコーダ３７５がリモートサーバ１１２または他のコンピューティングデバイス１１６のうちの１つに常駐するように、複数のデバイス間で分散されてよい。いくつかの実装では、音声変換モデル２００は、ソース話者１０４が発話に対応する部分を非典型的な音声として話すときに、発話の合成された標準的で流暢な音声表現に対応する出力スペクトログラム２２２を連続的に生成する。ソース話者１０４によって話された発話１０８の部分の合成された標準的で流暢な音声表現に対応する出力スペクトログラム２２２を連続的に生成することによって、ソース話者１０４とユーザ１１８（または聞き手）との間の会話は、より自然にペース調整され得る。いくつかの追加的な実装では、音声変換モデル２００は、非典型的な音声を有する発話１０８に対応する入力音声データ１０２を、同じ発話１１４の合成された標準的で流暢な音声表現に対応する出力スペクトログラム２２２に変換する前に、ボイスアクティビティ検出、終点、クエリ検出の終了などの技法を使用することで、ソース話者１０４が発話を停止するときを判定／検出するよう待機する。 Alternatively, the other computing device 116 may perform automatic speech recognition (ASR) for converting the output audio data 106, which corresponds to the synthesized standard fluent speech expression, into recognized text. ) may be associated with a downstream ASR system, acting as a front end to provide input to the system. The recognized text may be presented to other users 118 and/or provided to an NLU (natural language understanding) system for further processing. The functionality of speech conversion system 100 may reside on a remote server 112, one or both of computing devices 110, 116, or any combination of remote servers and computing devices 110, 116. Voice conversion system 100 is configured such that voice conversion model 200 resides on one of computing device 110 or remote server 112 and vocoder 375 resides on one of remote server 112 or other computing device 116. may be distributed across multiple devices. In some implementations, the speech transformation model 200 generates an output spectrogram corresponding to a synthesized canonical and fluent speech representation of the utterance when the source speaker 104 speaks the corresponding portion of the utterance as an atypical speech. 222 are generated continuously. The source speaker 104 and the user 118 (or listener) are Conversations between conversations can be more naturally paced. In some additional implementations, the speech transformation model 200 converts the input speech data 102 corresponding to an utterance 108 having an atypical voice to a synthesized canonical and fluent speech representation of the same utterance 114. Before converting to the output spectrogram 222, we wait to determine/detect when the source speaker 104 stops speaking by using techniques such as voice activity detection, endpoint, end of query detection, etc.
図２は、図１の音声変換システム１００によって使用される例示的な音声変換モデル２００の概略図を示す。音声変換モデル２００は、エンコーダ２１０および１つまたは複数のデコーダ２２０、２２０ａ～ｃを含む。エンコーダ２１０は、入力音声データ１０２をエンコード済みスペクトログラム２１２にエンコードするよう構成される。ここで、入力音声データ１０２は、ソース話者１０４によって話された発話１０８に対応する入力スペクトログラムのシーケンスを含む。いくつかの実施形態において、エンコーダ２１０は、コンフォーマブロック４００のスタックを含む。これらの実装では、エンコーダは、畳み込み層を使用して入力音声データ１０２をサブサンプリングし、次いで、コンフォーマブロック４００のスタックを用いて入力音声データ１０２を処理する。各コンフォーマブロック４００は、フィードフォワード層、自己注意層、畳み込み層、および第２のフィードフォワード層を含んでよい。いくつかの例では、コンフォーマブロック４００のスタックは、それぞれが５１２個の状態、８個の注意ヘッド、および３２×１の畳み込みカーネルサイズを有する１７層のコンフォーマブロックを含む。図４は、例示的なコンフォーマブロックの概略図である。エンコーダ２１０は、代わりに、コンフォーマブロックの代わりに、トランスフォーマブロックまたは軽量畳み込みブロックのスタックを使用してもよい。 FIG. 2 shows a schematic diagram of an exemplary speech conversion model 200 used by the speech conversion system 100 of FIG. 1. Audio conversion model 200 includes an encoder 210 and one or more decoders 220, 220a-c. Encoder 210 is configured to encode input audio data 102 into an encoded spectrogram 212. Here, input audio data 102 includes a sequence of input spectrograms corresponding to an utterance 108 spoken by a source speaker 104. In some embodiments, encoder 210 includes a stack of conformer blocks 400. In these implementations, the encoder subsamples the input audio data 102 using convolutional layers and then processes the input audio data 102 using a stack of conformer blocks 400. Each conformer block 400 may include a feedforward layer, a self-attention layer, a convolution layer, and a second feedforward layer. In some examples, the stack of conformer blocks 400 includes 17 layers of conformer blocks each having 512 states, 8 attention heads, and a convolution kernel size of 32×1. FIG. 4 is a schematic diagram of an example conformer block. Encoder 210 may alternatively use a stack of transformer blocks or lightweight convolutional blocks in place of conformer blocks.
スペクトグラム、音素、および単語片デコーダ２２０、２２０ａ～ｃはそれぞれ、エンコーダ２１０によって出力された共有のエンコード済みスペクトログラム２１２をそれぞれ受信する回帰型ニューラルネットワークベースのアーキテクチャを含んでよい。スペクトログラムデコーダ２２０ａは、エンコーダ２１０から共有のエンコード済みスペクトログラム２１２を受信するよう構成された相互注意機構２３１、２３１ａ（図３にも示す）を含んでよい。スペクトログラムデコーダ２２０ａはさらに、複数のＬＳＴＭ（長短期記憶）層２３３、２３３ａおよび複数の畳み込み層２３５を使用することで、共有のエンコード済みスペクトログラム２１２を処理してよい。例えば、スペクトログラムデコーダ２２０ａは、５つのＬＳＴＭ層２３３ａと５つの変換層２３５とを含んでよい。スペクトログラムデコーダ２２０ａは、出力スペクトログラム２２２を生成してよい。いくつかの実装では、スペクトログラムデコーダ２２０ａは、発話のトランスクリプションに対応するテキスト表現に対して中間のテキスト－音声変換を実行することなく、エンコード済みスペクトログラム２１２から出力スペクトログラム２２２を直接的に生成してよい。 Spectogram, phoneme, and word fragment decoders 220, 220a-c may each include a recurrent neural network-based architecture that each receives a shared encoded spectrogram 212 output by encoder 210. The spectrogram decoder 220a may include a mutual attention mechanism 231, 231a (also shown in FIG. 3) configured to receive the shared encoded spectrogram 212 from the encoder 210. Spectrogram decoder 220a may further process the shared encoded spectrogram 212 using multiple LSTM (long short-term memory) layers 233, 233a and multiple convolutional layers 235. For example, spectrogram decoder 220a may include five LSTM layers 233a and five transform layers 235. Spectrogram decoder 220a may generate an output spectrogram 222. In some implementations, spectrogram decoder 220a generates output spectrogram 222 directly from encoded spectrogram 212 without performing intermediate text-to-speech conversion on the textual representation corresponding to the transcription of the utterance. It's fine.
図示の例では、単語片デコーダ２２０ｂは、エンコーダから共有のエンコード済みスペクトログラムを受信するよう構成された対応する相互注意機構２３１、２３１ｂと、その後に続く、２つのＬＳＴＭ（長短期記憶）層２３３、２３３ｂと、発話のトランスクリプションに対応するテキスト表現２０１を出力するＳｏｆｔｍａｘ層２４５、２４５ａとを含む。 In the illustrated example, the word fragment decoder 220b includes a corresponding mutual attention mechanism 231, 231b configured to receive a shared encoded spectrogram from the encoder, followed by two LSTM (long short-term memory) layers 233; 233b, and a Softmax layer 245, 245a that outputs a textual representation 201 corresponding to the transcription of the utterance.
単語片デコーダ２２０ｂと同様に、音素デコーダ２２０ｃも、エンコーダ２１０から共有のエンコード済みスペクトログラム２１２を受信するよう構成された相互注意機構２３１、２３１ｃと、その後に続く、２つのＬＳＴＭ（長短期記憶）層２３３、２３３ｃと、発話２０２の音素表現を出力するＳｏｆｔｍａｘ層２４５、２４５ｂとを含んでよい。 Similar to the word fragment decoder 220b, the phoneme decoder 220c also includes a mutual attention mechanism 231, 231c configured to receive a shared encoded spectrogram 212 from the encoder 210, followed by two LSTM (long short-term memory) layers. 233 and 233c, and Softmax layers 245 and 245b that output the phoneme representation of the utterance 202.
図３は、図１の音声変換モデル２００のトレーニング時間および推論時間を改善するための例示的な混合フレームレート処理方式の概略図３００を示す。混合フレームレート処理方式は、音声－音声処理（すなわち、スペクトログラムデコーダ２２０ａを介して出力スペクトログラム２２２を生成すること）におけるエンコーダ２２０のメモリ消費およびトレーニング速度を改善し得る。予測されたターゲットまたは入力シーケンスがテキストである、ＡＳＲ（自動音声認識）またはＴＴＳ（テキスト－音声）などの他のモデルとは異なり、音声－音声変換モデルは、入力シーケンスとして音響フレームを使用する一方で、音響フレームのシーケンスも出力する。音響フレームの出力数はテキストシーケンスの出力数よりもはるかに大きいので、音声－音声の変換は、ＡＳＲまたはＴＴＳモデルと比較して多くの計算を必要とする。場合によっては、モデル複雑度は、エンコーダ２１０の自己注意機構に起因して、入力フレームの数に基づく二次関数になる。さらに、メモリ使用は、音響シーケンスの長さに直接的に比例し得るので、結果的に、バッチサイズが小さくなり、トレーニング速度が遅くなりかねない。図３に示す混合フレームレート処理方式は、計算の数を大幅に削減し、その後、トレーニングを改善し得る。 FIG. 3 shows a schematic diagram 300 of an example mixed frame rate processing scheme to improve training and inference times of the speech conversion model 200 of FIG. 1. The mixed frame rate processing scheme may improve memory consumption and training speed of encoder 220 in audio-to-speech processing (ie, generating output spectrogram 222 via spectrogram decoder 220a). Unlike other models such as ASR (Automatic Speech Recognition) or TTS (Text-to-Speech), where the predicted target or input sequence is text, speech-to-speech conversion models use acoustic frames as the input sequence while It also outputs a sequence of acoustic frames. Since the number of audio frame outputs is much larger than the number of text sequence outputs, speech-to-speech conversion requires more computation compared to ASR or TTS models. In some cases, model complexity is a quadratic function based on the number of input frames due to the self-attention mechanism of encoder 210. Additionally, memory usage can be directly proportional to the length of the acoustic sequence, which can result in smaller batch sizes and slower training speeds. The mixed frame rate processing scheme shown in FIG. 3 can significantly reduce the number of calculations and subsequently improve training.
いくつかの実装では、混合フレームレート処理方式は、３×３カーネルサイズおよび２×２ストライドを有する畳み込みサブサンプリングを使用するので、結果的に、サブサンプリング係数が４になる。これらの実装では、転置畳み込みネットワークは、５１２のチャネル、４のフィルタサイズ、時間方向に２のストライドを有する１つのＣＮＮ（畳み込みニューラルネットワーク）層を含む。さらに、混合フレームレート方式は、３０ｍｓウィンドウおよび１０ｍｓフレームシフトを使用することで、入力音声から１２８次元のログメルスペクトログラム特徴を抽出することを含んでよい。その特徴は、エンコーダ２１０に提供されてよい。例示的な実装では、スペクトログラムデコーダ２２０ａのターゲットは、５０ｍｓのフレーム長、１２．５ｍｓのシフト、および２０４８点のＦＦＴを用いて計算された１０２５次元のＳＴＦＴ（短時間フーリエ変換）振幅を含む。 In some implementations, the mixed frame rate processing scheme uses convolutional subsampling with a 3x3 kernel size and a 2x2 stride, resulting in a subsampling factor of 4. In these implementations, the transposed convolutional network includes one CNN (convolutional neural network) layer with 512 channels, a filter size of 4, and a stride of 2 in time. Additionally, the mixed frame rate scheme may include extracting 128-dimensional logmel spectrogram features from the input audio using a 30ms window and a 10ms frame shift. That feature may be provided to encoder 210. In an exemplary implementation, the spectrogram decoder 220a target includes a 50 ms frame length, a 12.5 ms shift, and a 1025-dimensional STFT (short-time Fourier transform) amplitude computed using a 2048-point FFT.
処理方式は、スペクトログラムエンコーダ２１０が入力スペクトログラム１０２の１０ｍｓ（ミリ秒）の音声サンプルを受信することによって開始してよい。エンコーダ２１０は、最初に、複数のＣＮＮ層を含む第１のサブサンプリング層３０５を使用して１０ｍｓの音声サンプルを処理してよい。第１のサブサンプリング層３０５によるサブサンプリングの実施の際には、ＣＮＮ層が使用され、その後、時間方向にプーリングが行われることで、コンフォーマブロック４００、４００ａ～ｂのスタック内の最初のコンフォーマブロックによって処理されるフレームの数を低減する。ＣＮＮは、１０ｍｓの音声を４０ｍｓの表現にサブサンプリングしてもよく、４０ｍｓの表現は次いで、コンフォーマブロック４００ａの最初のセットに提供される。コンフォーマブロック４００ａの最初のセットは、４０ｍｓの表現を処理した後、それを第２のサブサンプリング層３１５に提供してよい。第２のサブサンプリング層３１５は、コンフォーマブロック４００ａの最初のセットとコンフォーマブロック４００ｂの最後のセットとの間に配置されてよい。いくつかの例では、エンコーダ２１０のコンフォーマブロックの総数が１７であるように、コンフォーマブロック４００ａの最初のセットは４つのコンフォーマブロックを含み、コンフォーマブロック４００ｂの最後のセットは１３個のコンフォーマブロックを含む。ここで、第２のサブサンプリング層３１５は、コンフォーマブロック４００ａの最初のセット内の最後のコンフォーマブロックによって出力された隠れ表現３０８をサブサンプリングすることで、コンフォーマブロック４００ｂの最後のセットによって処理されるフレームの数を低減するよう構成されてよい。例えば、第２のサブサンプリング層３１５は、コンフォーマブロック４００ａの最初のセットによって出力された４０ｍｓの隠れ表現３０８を、対応する８０ｍｓの表現３１８にサブサンプリングするよう構成されてよい。コンフォーマブロック４００ｂの最後のセットの最終コンフォーマブロックの終わりに、エンコーダ２１０は、アップサンプリング層３２５を使用して８０ｍｓの隠れ表現３２２をアップサンプリングする。アップサンプリング層３２５は、コンフォーマブロック４００ｂの最後のセットの最後のコンフォーマブロックによって出力された８０ｍｓの隠れ表現３２２を、エンコード済みスペクトログラム２１２の対応する４０ｍｓの表現にアップサンプリングすることで、エンコード済みスペクトログラム２１２のフレーム数を増加させるよう構成された単一の転置ＣＮＮ層を含んでよい。 The processing scheme may begin with spectrogram encoder 210 receiving 10 ms (millisecond) audio samples of input spectrogram 102 . Encoder 210 may first process the 10 ms audio samples using a first subsampling layer 305 that includes multiple CNN layers. In performing the subsampling by the first subsampling layer 305, a CNN layer is used, followed by temporal pooling to obtain the first conformer in the stack of conformer blocks 400, 400a-b. Reduce the number of frames processed by the former block. The CNN may subsample the 10 ms audio into a 40 ms representation, which is then provided to a first set of conformer blocks 400a. The first set of conformer blocks 400a may process the 40 ms representation and then provide it to the second sub-sampling layer 315. A second sub-sampling layer 315 may be placed between the first set of conformer blocks 400a and the last set of conformer blocks 400b. In some examples, the first set of conformer blocks 400a includes four conformer blocks and the last set of conformer blocks 400b includes thirteen conformer blocks such that the total number of conformer blocks in encoder 210 is seventeen. Contains conformer blocks. Here, the second subsampling layer 315 subsamples the hidden representation 308 output by the last conformer block in the first set of conformer blocks 400a to It may be configured to reduce the number of frames processed. For example, the second subsampling layer 315 may be configured to subsample the 40ms hidden representation 308 output by the first set of conformer blocks 400a into a corresponding 80ms representation 318. At the end of the final conformer block of the last set of conformer blocks 400b, encoder 210 upsamples the 80 ms hidden representation 322 using upsampling layer 325. The upsampling layer 325 upsamples the 80 ms hidden representation 322 output by the last conformer block of the last set of conformer blocks 400b to the corresponding 40 ms representation of the encoded spectrogram 212. A single transposed CNN layer configured to increase the number of frames in spectrogram 212 may be included.
エンコード済みスペクトログラム２１２は、エンコーダ２１０とスペクトログラムデコーダ２２０ａとの間に配置された相互注意機構２３１ａによって受信されてよい。いくつかの実装では、相互注意機構２３１ａは、スペクトログラムデコーダ２２０ａに含まれる。スペクトログラムデコーダ２２０ａは、相互注意機構２３１ａを使用して、エンコード済みスペクトログラム２１２の４０ｍｓの表現を２５ｍｓの表現に低減してよい。２５ｍｓの表現は次いで、ＬＳＴＭ２３３ａに提供されてよい。ＬＳＴＭ２３３ａの出力は、低減係数３３５によって低減されてよい。スペクトログラムデコーダ２２０ａは、１２．５ｍｓの最終サイズで、結果として生じる出力スペクトログラム２２２を出力してよい。出力スペクトログラム２２２は、合成音声の対応する時間領域音声波形への変換のためにボコーダ３７５（図１）に提供されてよい。 Encoded spectrogram 212 may be received by mutual attention mechanism 231a located between encoder 210 and spectrogram decoder 220a. In some implementations, mutual attention mechanism 231a is included in spectrogram decoder 220a. Spectrogram decoder 220a may reduce the 40ms representation of encoded spectrogram 212 to a 25ms representation using mutual attention mechanism 231a. The 25ms representation may then be provided to LSTM 233a. The output of LSTM 233a may be reduced by a reduction factor 335. Spectrogram decoder 220a may output a resulting output spectrogram 222 with a final size of 12.5 ms. Output spectrogram 222 may be provided to vocoder 375 (FIG. 1) for conversion of the synthesized speech to a corresponding time-domain speech waveform.
上記の例は、限定を意図するものではない。エンコーダ２１０は、処理のために任意の適切な長さの音声サンプルを受信してよい。エンコーダ２１０は、次いで、音声サンプルを処理、サブサンプリング、またはアップサンプリングすることで、任意の適切な長さを有し得るエンコード済みスペクトログラム２１２を生成してよい。同様に、デコーダ２２０ａは、エンコード済みスペクトログラム２１２を処理することで、適切な長さの出力スペクトログラム２２２を生成してよい。 The above examples are not intended to be limiting. Encoder 210 may receive audio samples of any suitable length for processing. Encoder 210 may then process, subsample, or upsample the audio samples to produce encoded spectrogram 212, which may have any suitable length. Similarly, decoder 220a may process encoded spectrogram 212 to produce an output spectrogram 222 of appropriate length.
実験では、同一のエンコーダフレームシフトが与えられた場合、混合フレームレート方式は、異なるサブサンプリングおよびアップサンプリング設定による異なる実現を可能にする。例えば、サブサンプリングの増加は、一般に、トレーニングの改善をもたらすが、アップサンプリングを通じて回復することがより困難なスペクトログラムＷＥＲの回帰を引き起こす。情報損失は、エンコーダ２１０のコンフォーマブロック４００ｂの最後のセットの最後のコンフォーマブロックにおけるフィードフォワードニューラルネットワーク重み行列のスパース性に基づいて評価されてもよい。ＣＰＶ（累積分散割合）は、以下の式によって計算されてよい。 In our experiments, given the same encoder frame shift, the mixed frame rate scheme allows different realizations with different subsampling and upsampling settings. For example, increasing subsampling generally results in improved training, but causes regression in the spectrogram WER that is more difficult to recover through upsampling. Information loss may be evaluated based on the sparsity of the feedforward neural network weight matrix in the last conformer block of the final set of conformer blocks 400b of encoder 210. CPV (cumulative variance percentage) may be calculated by the following formula.
ここで、ｓｉは行列のｉ番目の特異値であり、ｋは考慮する特異値の数であり、Ｄはフィードフォワード行列のサイズである（Ｄ＝５１２）。任意の所与のｋについて、より大きなＣＰＶは、ネットワークがｋのスパース性指数を有するデータの構造を学習することができることを示す。小さい値のｋは、疎な行列構造を示す。 Here, s i is the i-th singular value of the matrix, k is the number of singular values to consider, and D is the size of the feedforward matrix (D=512). For any given k, a larger CPV indicates that the network is able to learn the structure of data with a sparsity index of k. A small value of k indicates a sparse matrix structure.
図４は、エンコーダ２１０のコンフォーマ層のスタックにおけるコンフォーマブロック４００の例を示す。コンフォーマブロック４００は、前半のフィードフォワード層４１０と、後半のフィードフォワード層４４０と、前半のフィードフォワード層４１０と後半のフィードフォワード層４４０との間に配置されたマルチヘッド自己注意ブロック４２０および畳み込み層４３０と、連結演算子４０５とを含む。前半のフィードフォワード層４１０は、入力メルスペクトログラムシーケンスを含む入力音声データ１０２を処理する。続いて、マルチヘッド自己注意ブロック４２０は、前半のフィードフォワード層４１０の出力と連結された入力音声データ１０２を受信する。端的に言えば、マルチヘッド自己注意ブロック４２０の役割は、強調される各入力フレームについてノイズのコンテキストを個別に要約することである。畳み込み層４３０は、前半のフィードフォワード層４１０の出力と連結されたマルチヘッド自己注意ブロック４２０の出力をサブサンプリングする。その後、後半のフィードフォワード層４４０は、畳み込み層４３０出力とマルチヘッド自己注意ブロック４２０との連結を受信する。ｌａｙｅｒｎｏｒｍモジュール４５０は、後半のフィードフォワード層４４０からの出力を処理する。数学的には、コンフォーマブロック４００は、変調特徴ｍを使用して入力特徴ｘを変換することで、以下のように出力特徴ｙを生成する。 FIG. 4 shows an example of a conformer block 400 in a stack of conformer layers of encoder 210. The conformer block 400 includes a first half feedforward layer 410, a second half feedforward layer 440, a multi-head self-attention block 420 disposed between the first half feedforward layer 410 and the second half feedforward layer 440, and a convolution It includes a layer 430 and a concatenation operator 405. The first half feedforward layer 410 processes input audio data 102 including an input mel spectrogram sequence. Subsequently, the multi-head self-attention block 420 receives the input audio data 102 concatenated with the output of the first half feedforward layer 410 . Briefly, the role of the multi-head self-attention block 420 is to individually summarize the noise context for each input frame that is enhanced. The convolution layer 430 subsamples the output of the multi-head self-attention block 420, which is concatenated with the output of the first half of the feedforward layer 410. A later feedforward layer 440 then receives the concatenation of the convolutional layer 430 output and the multi-head self-attention block 420. A layernorm module 450 processes the output from the late feedforward layer 440. Mathematically, conformer block 400 transforms input feature x using modulation feature m to produce output feature y as follows.
図５は、音声変換モデル２００のためのトレーニングプロセス５００を示す。いくつかの実装では、プロセス５００は、２ステップトレーニング技法を採用する。はじめに、音声変換モデル２００は、話者の大きなプールからの典型的な音声に対して事前トレーニングされることで多対１の音声変換モデル２００を得て、結果的に、話者から独立したＡＳＲ／変換ベースモデルになる。トレーニングに使用されるターゲット音声は、典型的な音声を反映する所定の音声による基準トランスクリプトから合成された音声であってもよい。個別化のために、基本モデルの任意のパラメータが、単一入力話者（例えば、聴覚障害のある話者）からの音声に微調整されることで、非典型的な音声から典型的な音声への１対１音声変換モデル（および話者依存のＡＳＲ）モデルを取得してよい。 FIG. 5 shows a training process 500 for speech conversion model 200. In some implementations, process 500 employs a two-step training technique. First, the speech translation model 200 is pre-trained on typical speech from a large pool of speakers to obtain a many-to-one speech translation model 200, resulting in a speaker-independent ASR. / Becomes a conversion-based model. The target speech used for training may be speech synthesized from a reference transcript of predetermined speech that reflects typical speech. For personalization, any parameter of the base model is fine-tuned to speech from a single input speaker (e.g., a hearing-impaired speaker), thereby converting atypical speech to typical speech. A one-to-one speech conversion model (and speaker-dependent ASR) model may be obtained.
図５を参照すると、プロセス５００は、はじめに、事前トレーニングデータ５０５を使用して音声変換モデル２００を事前トレーニングする。モデルの事前トレーニングは、モデルを初期化するために使用される技法である。モデルは、次いで、追加的なトレーニングデータ５１０に基づいてさらに微調整されてよい。音声変換モデル２００に関して、事前トレーニングは、標準的で流暢な音声に関連付けられた典型的な話者による複数の話された発話を含む事前トレーニングデータ５０５を用いて音声変換モデル２００を開始することを含んでよい。事前トレーニングデータ５０５は、話された発話に対応したグラウンドトゥルースであって合成された標準的で流暢な音声表現とペアにされた話された発話をさらに含んでよい。 Referring to FIG. 5, process 500 first pre-trains speech conversion model 200 using pre-training data 505. Model pre-training is a technique used to initialize a model. The model may then be further fine-tuned based on additional training data 510. With respect to speech conversion model 200, pre-training includes starting speech conversion model 200 with pre-training data 505 that includes a plurality of spoken utterances by a typical speaker associated with standard, fluent speech. may be included. Pre-training data 505 may further include spoken utterances paired with synthesized standard fluent speech representations that are ground truth corresponding to the spoken utterances.
次いで、プロセス５００は、非典型的な音声の事前トレーニングされた音声変換モデル２００のパラメータを微調整してよい。トレーニングプロセスは、エンコーダ２１０またはデコーダ２２０、２２０ａ～ｃのいずれかを別々にまたは任意の適切な組合せで共同でトレーニングすることを含んでよい。プロセス５００は、トレーニング入力５１０を音声変換モデル２００に供給することを含む。いくつかの実装では、トレーニング入力５１０は、非典型的な音声に関連付けられた１人または複数の話者によって話された複数の非典型的な音声サンプルを含む。さらに、トレーニング入力５１０は、トレーニング入力５１０に関連付けられたターゲット出力を示すラベル５２０を使用してラベル付けされてよい。トレーニング入力５１０を受信すると、音声変換モデル２００は、出力５１５（例えば、トランスクリプト２０１、音素表現２０２、出力スペクトログラム２２２）を生成してよい。音声変換モデル２００は、図２乃至図４のいずれかに関して説明した方法で、または音声変換のために任意の他の好適な方法で、トレーニング入力５１０を処理してよい。 The process 500 may then fine-tune the parameters of the pre-trained speech transformation model 200 for atypical speech. The training process may include training either encoder 210 or decoders 220, 220a-c separately or jointly in any suitable combination. Process 500 includes providing training input 510 to speech transformation model 200. In some implementations, training input 510 includes a plurality of atypical speech samples spoken by one or more speakers associated with the atypical speech. Additionally, training input 510 may be labeled using a label 520 that indicates a target output associated with training input 510. Upon receiving training input 510, speech transformation model 200 may generate output 515 (eg, transcript 201, phoneme representation 202, output spectrogram 222). Speech conversion model 200 may process training input 510 in the manner described with respect to any of FIGS. 2-4 or in any other suitable manner for speech conversion.
いくつかの実装では、出力５１５は損失関数５３０によって使用されて損失５４０が生成される。すなわち、損失関数５３０は、出力５１５とラベル５２０とを比較することで損失５４０を生成する。損失５４０は、ラベル５２０（すなわち、ターゲット出力）と出力５１５との間の不一致を示す。損失関数３５０は、回帰損失、平均二乗誤差、平均二乗対数誤差、平均絶対誤差、バイナリ分類、バイナリクロスエントロピー、ヒンジ損失、マルチクラス損失などの損失を判定するための任意の適切な技法を実装してよい。次いで、損失５４０は、音声変換モデル２００に直接的に供給されてよい。ここで、音声変換モデル２００は、損失５４０を処理するとともに、損失５４０を補償するよう音声変換モデル２００の１つまたは複数のパラメータを調整する。 In some implementations, output 515 is used by loss function 530 to generate loss 540. That is, loss function 530 generates loss 540 by comparing output 515 and label 520. Loss 540 indicates a mismatch between label 520 (ie, target output) and output 515. Loss function 350 implements any suitable technique for determining loss, such as regression loss, mean square error, mean square log error, mean absolute error, binary classification, binary cross entropy, hinge loss, multiclass loss, etc. It's fine. Loss 540 may then be fed directly to speech conversion model 200. Here, speech conversion model 200 processes loss 540 and adjusts one or more parameters of speech conversion model 200 to compensate for loss 540.
図６は、音声変換を実行するためにコンピュータが実施する方法６００の動作の例示的な構成のフローチャートである。方法６００は、例えば、図１の例示的な音声変換システム１００の様々な要素によって実行されてよい。動作６１０において、方法は、音声変換モデル２００のエンコーダ２１０への入力として、発話１０８に対応する入力スペクトログラム１０２を受信する工程を含む。エンコーダ２１０は、自己注意ブロック４００のスタックを含む。動作６２０において、方法６００は、エンコーダ２１０からの出力として、エンコード済みスペクトログラム２１２を生成する工程を含む。動作６３０において、方法６００は、音声変換モデル２２０のスペクトログラムデコーダ２００ａへの入力として、エンコーダ２１０からの出力として生成されたエンコード済みスペクトログラム２１２を受信する工程を含む。動作６４０において、方法６００は、スペクトログラムデコーダ２２０ａからの出力として、同じ発話１１４の合成された標準的で流暢な音声表現に対応する出力スペクトログラム２２２を生成する工程を含む。 FIG. 6 is a flowchart of an example arrangement of operations in a computer-implemented method 600 for performing speech conversion. Method 600 may be performed by various elements of example speech conversion system 100 of FIG. 1, for example. At act 610, the method includes receiving input spectrogram 102 corresponding to utterance 108 as input to encoder 210 of speech transformation model 200. Encoder 210 includes a stack of self-attention blocks 400. At act 620, method 600 includes generating encoded spectrogram 212 as an output from encoder 210. At act 630, method 600 includes receiving encoded spectrogram 212 generated as an output from encoder 210 as an input to spectrogram decoder 200a of audio conversion model 220. At act 640, method 600 includes producing an output spectrogram 222 that corresponds to a synthesized canonical fluent speech representation of the same utterance 114 as an output from spectrogram decoder 220a.
ソフトウェアアプリケーション（すなわち、ソフトウェアリソース）は、コンピューティングデバイスにタスクを実行させるコンピュータソフトウェアを指し得る。いくつかの例では、ソフトウェアアプリケーションは、「アプリケーション」、「アプリ」、または「プログラム」と呼ばれることがある。例示的なアプリケーションは、システム診断アプリケーション、システム管理アプリケーション、システム保守アプリケーション、ワードプロセッシングアプリケーション、スプレッドシートアプリケーション、メッセージングアプリケーション、メディアストリーミングアプリケーション、ソーシャルネットワーキングアプリケーション、およびゲームアプリケーションを含むが、これらに限定されない。 A software application (i.e., software resource) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an "application," "app," or "program." Exemplary applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
非一時的メモリは、コンピューティングデバイスによる使用のために、プログラム（例えば、命令のシーケンス）またはデータ（例えば、プログラム状態情報）を一時的または永続的に記憶するために使用される物理デバイスであってよい。非一時的メモリは、揮発性および／または不揮発性のアドレス指定可能な半導体メモリであってもよい。不揮発性メモリの例は、フラッシュメモリおよびＲＯＭ（リードオンリーメモリ）／ＰＲＯＭ（プログラマブルリードオンリーメモリ）／ＥＰＲＯＭ（消去可能プログラマブルリードオンリーメモリ）／ＥＥＰＲＯＭ（電子的消去可能プログラマブルリードオンリーメモリ）（例えば、ブートプログラムなどのファームウェアに典型的に使用される）を含むが、これらに限定されない。揮発性メモリの例は、ＲＡＭ（ランダムアクセスメモリ）、ＤＲＡＭ（ダイナミックランダムアクセスメモリ）、ＳＲＡＭ（スタティックランダムアクセスメモリ）、ＰＣＭ（相変化メモリ）、およびにディスクまたはテープを含むが、これらに限定されない。 Non-transitory memory is a physical device used to temporarily or permanently store programs (e.g., sequences of instructions) or data (e.g., program state information) for use by a computing device. It's fine. Non-transitory memory may be volatile and/or non-volatile addressable semiconductor memory. Examples of non-volatile memories are flash memory and ROM (read only memory)/PROM (programmable read only memory)/EPROM (erasable programmable read only memory)/EEPROM (electronically erasable programmable read only memory) (e.g. boot (typically used for firmware such as programs), but is not limited to. Examples of volatile memory include, but are not limited to, RAM (Random Access Memory), DRAM (Dynamic Random Access Memory), SRAM (Static Random Access Memory), PCM (Phase Change Memory), and disk or tape. .
図７は、本明細書で説明されたシステムおよび方法を実装するために使用され得る例示的なコンピューティングデバイス７００の概略図である。コンピューティングデバイス７００は、ラップトップ、デスクトップ、ワークステーション、携帯情報端末、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピュータ等、様々な形態のデジタルコンピュータを表すよう意図されている。本明細書で示される構成要素、それらの接続および関係、ならびにそれらの機能は、例示的なものに過ぎず、本明細書で説明および／または特許請求される本発明の実装を限定するものではない。
FIG. 7 is a schematic diagram of an
コンピューティングデバイス７００は、プロセッサ７１０、メモリ７２０、記憶デバイス７３０、メモリ７２０および高速拡張ポート７５０に接続する高速インターフェース／コントローラ７４０、ならびに低速バス７７０および記憶デバイス７３０に接続する低速インターフェース／コントローラ７６０を含む。構成要素７１０、７２０、７３０、７４０、７５０、および７６０の各々は、様々なバスを使用して相互接続され、共通のマザーボード上に、または必要に応じて他の方法で実装され得る。プロセッサ７１０は、高速インターフェース７４０に接続されたディスプレイ７８０などの外部入力／出力デバイス上にＧＵＩ（グラフィカルユーザインターフェース）のためのグラフィカル情報を表示するようメモリ７２０または記憶デバイス７３０に記憶された命令を含む、コンピューティングデバイス７００内で実行するための命令を処理することができる。他の実装では、複数のメモリおよびメモリの種類とともに、必要に応じて複数のプロセッサおよび／または複数のバスが使用されてよい。また、複数のコンピューティングデバイス７００が接続されてもよく、各デバイスは、例えば、サーババンク、ブレードサーバのグループ、またはマルチプロセッサシステムとして、必要な動作の一部を提供する。プロセッサ７１０は、リモートサーバ１１２、コンピューティングデバイス１１０、１１６のいずれかもしくは両方、またはリモートサーバおよびコンピューティングデバイス１１０、１１６の任意の組合せ上に常駐するデータ処理ハードウェア７１０と呼ばれることがある。メモリ７１０は、リモートサーバ１１２、コンピューティングデバイス１１０、１１６のいずれかもしくは両方、またはリモートサーバおよびコンピューティングデバイス１１０、１１６の任意の組合せ上に常駐するメモリハードウェア７２０と呼ばれることがある。
メモリ７２０は、コンピューティングデバイス７００内に情報を非一時的に記憶する。メモリ７２０は、コンピュータ可読媒体、揮発性メモリユニット（複数可）、または不揮発性メモリユニット（複数可）であってもよい。非一時的メモリ７２０は、コンピューティングデバイス７００による使用のために一時的または永続的にプログラム（例えば、命令のシーケンス）またはデータ（例えば、プログラム状態情報）を記憶するために使用される物理的なデバイスであってもよい。不揮発性メモリの例は、フラッシュメモリおよびＲＯＭ（リードオンリーメモリ）／ＰＲＯＭ（プログラマブルリードオンリーメモリ）／ＥＰＲＯＭ（消去可能プログラマブルリードオンリーメモリ）／ＥＥＰＲＯＭ（電子的消去可能プログラマブルリードオンリーメモリ）（例えば、ブートプログラムなどのファームウェアに典型的に使用される）を含むが、これらに限定されない。揮発性メモリの例は、ＲＡＭ（ランダムアクセスメモリ）、ＤＲＡＭ（ダイナミックランダムアクセスメモリ）、ＳＲＡＭ（スタティックランダムアクセスメモリ）、ＰＣＭ（相変化メモリ）、およびにディスクまたはテープを含むが、これらに限定されない。
記憶デバイス７３０は、コンピューティングデバイス７００に大容量ストレージを提供することが可能である。いくつかの実装では、記憶デバイス７３０はコンピュータ可読媒体である。様々な異なる実装では、記憶デバイス７３０は、フロッピー（登録商標）ディスクデバイス、ハードディスクデバイス、光学ディスクデバイス、テープデバイス、フラッシュメモリもしくは他の同様のソリッドステートメモリデバイス、またはストレージエリアネットワークもしくは他の構成におけるデバイスを含むデバイスのアレイであってよい。追加的な実装では、コンピュータプログラム製品は、情報担体として有形に具現化される。コンピュータプログラム製品は、実行されると、上記のような１つまたは複数の方法を実行する命令を含む。情報担体は、メモリ７２０、記憶デバイス７３０、またはプロセッサ７１０上のメモリなどのコンピュータ可読媒体または機械可読媒体である。
高速コントローラ７４０は、コンピューティングデバイス７００のための帯域幅集中型の演算を管理し、低速コントローラ７６０は、より低い帯域幅集中型の演算を管理する。このようなデューティの割り当ては例示にすぎない。いくつかの実装では、高速コントローラ７４０は、メモリ７２０、ディスプレイ７８０（例えば、グラフィックスプロセッサまたはアクセラレータを介して）、および様々な拡張カード（図示せず）を受け入れ得る高速拡張ポート７５０に接続される。いくつかの実装では、低速コントローラ７６０は、記憶デバイス７３０および低速拡張ポート７９０に接続される。種々の通信ポート（例えば、ＵＳＢ、Ｂｌｕｅｔｏｏｔｈ（登録商標）、イーサネット（登録商標）、無線イーサネット）を含み得る低速拡張ポート７９０は、例えばネットワークアダプタを通じて、キーボード、ポインティングデバイス、スキャナー、または、スイッチまたはルータ等のネットワークデバイス等の、１つまたは複数の入力／出力デバイスに接続されてよい。
A high-
コンピューティングデバイス７００は、図７に示されるように、複数の異なる形態で実装されてよい。例えば、コンピューティングデバイス７００は、ラップトップコンピュータ７００ｂとして、ラックサーバシステム７００ｃの一部として、または標準的なサーバ７００ａとしてもしくはそのようなサーバ７００ａのグループにおいて複数回実装されてもよい。
本明細書で説明されたシステムおよび技法の様々な実装は、デジタル電子回路および／または光回路、集積回路、特別に設計されたＡＳＩＣ（特定用途向け集積回路）、コンピュータハードウェア、ファームウェア、ソフトウェア、および／またはそれらの組合せで実現され得る。これらの様々な実装は、記憶デバイス、１つ以上の入力デバイス、および１つ以上の出力デバイスに対してデータおよび命令を送信すると共にこれらからデータおよび命令を受信するよう接続された、特定目的または汎用目的の１つ以上のプログラマブルプロセッサを備えたプログラマブルシステム上で実行可能および／または翻訳可能な１つまたは複数のコンピュータプログラムでの実装を含んでよい。 Various implementations of the systems and techniques described herein may include digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (Application Specific Integrated Circuits), computer hardware, firmware, software, and/or a combination thereof. These various implementations may include specific purpose or It may include implementation in one or more computer programs executable and/or translatable on a programmable system with one or more general purpose programmable processors.
これらのコンピュータプログラム（プログラム、ソフトウェア、ソフトウェアアプリケーションまたはコードとしても知られる）は、プログラマブルプロセッサのための機械命令を含み、高レベル手続き型および／またはオブジェクト指向プログラミング言語、および／またはアセンブリ／機械言語で実装され得る。本明細書で使用されるように、「機械可読媒体」および「コンピュータ可読媒体」という用語は、機械可読信号として機械命令を受信する機械可読媒体を含む、機械命令および／またはデータをプログラマブルプロセッサに提供するために使用される任意のコンピュータプログラム製品、非一時的コンピュータ可読媒体、機器および／または装置（例えば、磁気ディスク、光学ディスク、メモリ、ＰＬＤ（プログラマブル論理デバイス））を指す。「機械可読信号」という用語は、機械命令および／またはデータをプログラマブルプロセッサに提供するために使用される任意の信号を指す。 These computer programs (also known as programs, software, software applications or code) contain machine instructions for a programmable processor and are written in high-level procedural and/or object-oriented programming languages, and/or assembly/machine language. Can be implemented. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to a machine-readable medium that receives machine instructions as a machine-readable signal and that transmits machine instructions and/or data to a programmable processor. Refers to any computer program product, non-transitory computer readable medium, equipment and/or device (eg, magnetic disk, optical disk, memory, PLD (programmable logic device)) used to provide the computer program product. The term "machine readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
本明細書で説明されたプロセスおよび論理フローは、入力データに対して動作しかつ出力を生成することによって機能を実行するために１つまたは複数のコンピュータプログラムを実行する、データ処理ハードウェアとも呼ばれる１つまたは複数のプログラマブルプロセッサによって実行され得る。プロセスおよび論理フローは、特殊目的論理回路、例えば、ＦＰＧＡ（フィールドプログラマブルゲートアレイ）またはＡＳＩＣ（特定用途向け集積回路）によって実行することもできる。コンピュータプログラムの実行に適したプロセッサは、例として、汎用マイクロプロセッサおよび専用マイクロプロセッサの両方、ならびに任意の種類のデジタルコンピュータの任意の１つまたは複数のプロセッサを含む。一般に、プロセッサは、読み出し専用メモリもしくはランダムアクセスメモリまたはその両方から命令およびデータを受信する。コンピュータの必須要素は、命令を実行するプロセッサ、ならびに命令およびデータを記憶するための１つまたは複数のメモリデバイスである。一般的に、コンピュータはまた、データを記憶するための１つまたは複数の大容量記憶デバイス、例えば、磁気、光磁気ディスク、もしくは光学ディスクを含むか、またはそれらからデータを受信するか、それらにデータを転送するか、もしくはその両方を行うように動作可能に接続される。しかしながら、コンピュータはそのようなデバイスを有する必要はない。コンピュータプログラム命令およびデータを記憶するのに適したコンピュータ可読媒体は、例として、半導体メモリデバイス、例えば、ＥＰＲＯＭ、ＥＥＰＲＯＭ、およびフラッシュメモリデバイス、磁気ディスク、例えば、内蔵ハードディスクまたはリムーバブルディスク、光磁気ディスク、ならびにＣＤＲＯＭおよびＤＶＤ－ＲＯＭディスクを含む、すべての形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。プロセッサおよびメモリは、特殊目的論理回路によって補完され得るか、または特殊目的論理回路に組み込まれ得る。 The processes and logic flows described herein are also referred to as data processing hardware that executes one or more computer programs to perform functions by operating on input data and producing output. It may be executed by one or more programmable processors. The processes and logic flows may also be performed by special purpose logic circuits, such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any type of digital computer. Generally, a processor receives instructions and data from read-only memory and/or random access memory. The essential elements of a computer are a processor for executing instructions, and one or more memory devices for storing instructions and data. Typically, a computer also includes one or more mass storage devices for storing data, such as magnetic, magneto-optical, or optical disks, or receives data from or uses them. operably connected to transfer data or both; However, a computer does not need to have such a device. Computer readable media suitable for storing computer program instructions and data include, by way of example, semiconductor memory devices such as EPROM, EEPROM and flash memory devices, magnetic disks such as internal hard disks or removable disks, magneto-optical disks, and all forms of non-volatile memory, media, and memory devices, including CD-ROM and DVD-ROM discs. The processor and memory may be supplemented by or incorporated into special purpose logic circuits.
ユーザとの対話を提供するために、本開示の１つまたは複数の態様は、ユーザに情報を表示するためのディスプレイデバイス、例えば、ＣＲＴ（陰極線管）、ＬＣＤ（液晶ディスプレイ）モニタ、またはタッチスクリーンと、任意選択で、ユーザがコンピュータに入力を提供することができるキーボードおよびポインティングデバイス、例えば、マウスまたはトラックボールとを有するコンピュータ上で実装されてよい。他の種類のデバイスを使用して、ユーザとの対話を提供してもよい。例えば、ユーザに提供されるフィードバックは、任意の形式の感覚的なフィードバック、例えば、視覚的フィードバック、聴覚的フィードバック、または触覚的フィードバックであってよく、ユーザからの入力は、音響的入力、音声的入力、または触覚的入力を含む任意の形式で取り込まれてよい。さらに、コンピュータは、ユーザによって使用されるデバイスに文書を送信し、デバイスから文書を受信することによって、例えば、ウェブブラウザから受信された要求に応答して、ユーザのクライアントデバイス上のウェブブラウザにウェブページを送信することによって、ユーザと対話することができる。 To provide user interaction, one or more aspects of the present disclosure utilize a display device, such as a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen, for displaying information to the user. and, optionally, a keyboard and pointing device, such as a mouse or trackball, that allow a user to provide input to the computer. Other types of devices may be used to provide user interaction. For example, the feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback, and the input from the user may be acoustic input, audio The input may be captured in any form, including input or tactile input. Further, the computer sends a web browser to a web browser on a user's client device, e.g., in response to a request received from a web browser, by sending the document to the device used by the user and receiving the document from the device. You can interact with users by submitting pages.
複数の実装について説明してきたが、本開示の主旨および範囲から逸脱することなく、様々な変更が行われ得ることが理解されるであろう。したがって、他の実施形態は、以下の特許請求の範囲内にある。 Although multiple implementations have been described, it will be appreciated that various changes may be made without departing from the spirit and scope of this disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (20)
自己注意ブロック（４００）のスタックを含むエンコーダ（２１０）であって、発話（１０８）に対応する入力スペクトログラム（１０２）をエンコードするよう構成されたエンコーダ（２１０）と、
スペクトログラムデコーダ（２２０ａ）であって、
前記エンコーダ（２１０）からエンコード済みスペクトログラム（２１２）を入力として受信するとともに、
前記発話（１０８）の合成音声表現に対応する出力スペクトログラム（２２２）を出力として生成するよう構成されたスペクトログラムデコーダ（２２０ａ）と、を備える音声変換モデル（２００）。 A voice conversion model (200),
an encoder (210) comprising a stack of self-attention blocks (400) configured to encode an input spectrogram (102) corresponding to an utterance (108);
A spectrogram decoder (220a), comprising:
receiving as input an encoded spectrogram (212) from said encoder (210);
a spectrogram decoder (220a) configured to generate as output an output spectrogram (222) corresponding to a synthesized speech representation of the utterance (108).
前記発話（１０８）の前記合成音声表現は、前記発話の合成された標準的で流暢な音声表現を含む、請求項１に記載の音声変換モデル（２００）。 the input spectrogram (102) corresponding to the utterance (108) is extracted from input speech spoken by a speaker (104) associated with atypical speech;
The speech conversion model (200) of claim 1, wherein the synthesized speech representation of the utterance (108) comprises a synthesized standard fluent speech representation of the utterance.
前記エンコーダ（２１０）から前記エンコード済みスペクトログラム（２１２）を入力として受信するとともに、
前記発話（１０８）のトランスクリプション（２０１）に対応するテキスト表現を出力として生成するよう構成された単語片デコーダ（２２０ｂ）をさらに備える、請求項１または２に記載の音声変換モデル（２００）。 a word piece decoder (220b),
receiving as input the encoded spectrogram (212) from the encoder (210);
Speech conversion model (200) according to claim 1 or 2, further comprising a word fragment decoder (220b) configured to generate as output a textual representation corresponding to a transcription (201) of the utterance (108). .
前記エンコーダ（２１０）から前記エンコード済みスペクトログラム（２１２）を入力として受信するとともに、
前記発話（１０８）の音素表現（２０２）を出力として生成するよう構成された音素デコーダ（２２０ｃ）をさらに備える、請求項１乃至３のいずれか一項に記載の音声変換モデル（２００）。 A phoneme decoder (220c),
receiving as input the encoded spectrogram (212) from the encoder (210);
A speech conversion model (200) according to any preceding claim, further comprising a phoneme decoder (220c) configured to generate as output a phoneme representation (202) of the utterance (108).
標準的で流暢な音声に関連付けられた典型的な話者による複数の話された発話について前記音声変換モデル（２００）を事前トレーニングする第１のトレーニングステップであって、各話された発話は、前記発話に対応したグラウンドトゥルースであって合成された標準的で流暢な音声表現とペアにされる、第１のトレーニングステップと、
非典型的な音声に関連付けられた話者によって話された複数の非典型的な音声サンプルに基づいて、事前トレーニングされた前記音声変換モデル（２００）のパラメータを微調整する、第２のトレーニングステップと、を含む請求項１乃至８のいずれか一項に記載の音声変換モデル（２００）。 The speech conversion model (200) is trained using a two-step training process (500), the two-step training process comprising:
a first training step of pre-training said speech transformation model (200) on a plurality of spoken utterances by a typical speaker associated with standard and fluent speech, each spoken utterance comprising: a first training step, in which the ground truth corresponding to the utterance is paired with a synthesized standard fluent speech representation;
a second training step of fine-tuning the parameters of the pre-trained speech transformation model (200) based on a plurality of atypical speech samples spoken by a speaker associated with the atypical speech; A speech conversion model (200) according to any one of claims 1 to 8, comprising:
音声変換モデル（２００）のエンコーダ（２１０）への入力として、発話（１０８）に対応する入力スペクトログラム（１０２）を受信する工程であって、前記エンコーダ（２１０）は、自己注意ブロック（４００）のスタックを含む、工程と、
前記エンコーダ（２１０）からの出力として、エンコード済みスペクトログラム（２１２）を生成する工程と、
前記音声変換モデル（２００）のスペクトログラムデコーダ（２００ａ）への入力として、前記エンコーダ（２１０）からの出力として生成された前記エンコード済みスペクトログラム（２１２）を受信する工程と、
前記スペクトログラムデコーダ（２２０ａ）からの出力として、前記発話（１０８）の合成音声表現に対応する出力スペクトログラム（２２２）を生成する、出力スペクトログラム生成工程と、を含むコンピュータが実施する方法（６００）。 A computer-implemented method (600), when executed on data processing hardware (710), causes the data processing hardware (710) to perform operations, the operations comprising: a speech conversion model (200); receiving as input to an encoder (210) an input spectrogram (102) corresponding to an utterance (108), said encoder (210) comprising a stack of self-attention blocks (400);
producing an encoded spectrogram (212) as an output from said encoder (210);
receiving the encoded spectrogram (212) produced as an output from the encoder (210) as input to a spectrogram decoder (200a) of the speech conversion model (200);
a computer-implemented method (600) comprising: generating an output spectrogram (222) corresponding to a synthesized speech representation of the utterance (108) as an output from the spectrogram decoder (220a).
前記発話の前記合成音声表現は、前記発話の合成された標準的で流暢な音声表現を含む、請求項１１に記載の方法（６００）。 the input spectrogram (102) corresponding to the utterance (108) is extracted from input speech spoken by a speaker (104) associated with atypical speech;
12. The method (600) of claim 11, wherein the synthesized speech representation of the utterance comprises a synthesized standard fluent speech representation of the utterance.
前記音声変換モデル（２００）の単語片デコーダ（２２０ｂ）への入力として、前記エンコーダ（２１０）からの出力として生成された前記エンコード済みスペクトログラム（２１２）を受信する工程と、
前記単語片デコーダ（２２０ｂ）からの出力として、前記発話（１０８）のトランスクリプション（２０１）に対応するテキスト表現を生成する工程と、をさらに含む請求項１１または１２に記載の方法（６００）。 The said operation is
receiving the encoded spectrogram (212) produced as an output from the encoder (210) as input to a word fragment decoder (220b) of the speech conversion model (200);
The method (600) of claim 11 or 12, further comprising: generating as output from the word fragment decoder (220b) a textual representation corresponding to a transcription (201) of the utterance (108). .
前記音声変換モデル（２００）の音素デコーダ（２２０ｃ）への入力として、前記エンコーダ（２１０）からの出力として生成された前記エンコード済みスペクトログラム（２１２）を受信する工程と、
前記音素デコーダ（２２０ｃ）からの出力として、前記発話（１０８）の音素表現（２０２０）を生成する工程と、をさらに含む請求項１１乃至１３のいずれか一項に記載の方法（６００）。 The said operation is
receiving the encoded spectrogram (212) produced as an output from the encoder (210) as input to a phoneme decoder (220c) of the speech transformation model (200);
14. The method (600) of any one of claims 11 to 13, further comprising: generating a phoneme representation (2020) of the utterance (108) as output from the phoneme decoder (220c).
標準的で流暢な音声に関連付けられた典型的な話者による複数の話された発話について前記音声変換モデル（２００）を事前トレーニングする第１のトレーニングステップであって、各話された発話は、前記発話に対応したグラウンドトゥルースであって合成された標準的で流暢な音声表現とペアにされる、第１のトレーニングステップと、
非典型的な音声に関連付けられた話者によって話された複数の非典型的な音声サンプルに基づいて、事前トレーニングされた前記音声変換モデルのパラメータを微調整する、第２のトレーニングステップと、を含む請求項１１乃至１８のいずれか一項に記載の方法（６００）。 The speech conversion model (200) is trained using a two-step training process (500), the two-step training process comprising:
a first training step of pre-training said speech transformation model (200) on a plurality of spoken utterances by a typical speaker associated with standard and fluent speech, each spoken utterance comprising: a first training step, in which the ground truth corresponding to the utterance is paired with a synthesized standard fluent speech representation;
a second training step of fine-tuning parameters of the pre-trained speech transformation model based on a plurality of atypical speech samples spoken by a speaker associated with the atypical speech; 19. A method (600) according to any one of claims 11 to 18.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163166954P | 2021-03-26 | 2021-03-26 | |
US63/166,954 | 2021-03-26 | ||
US202263312195P | 2022-02-21 | 2022-02-21 | |
US63/312,195 | 2022-02-21 | ||
PCT/US2022/020606 WO2022203922A1 (en) | 2021-03-26 | 2022-03-16 | Conformer-based speech conversion model |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2024511625A true JP2024511625A (en) | 2024-03-14 |
Family
ID=81327723
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023558802A Pending JP2024511625A (en) | 2021-03-26 | 2022-03-16 | Conformer-based speech conversion model |
Country Status (5)
Country | Link |
---|---|
US (1) | US20220310056A1 (en) |
EP (1) | EP4298631A1 (en) |
JP (1) | JP2024511625A (en) |
KR (1) | KR20230158614A (en) |
WO (1) | WO2022203922A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11595517B2 (en) * | 2021-04-13 | 2023-02-28 | Apple Inc. | Digital assistant integration with telephony |
CN116386609A (en) * | 2023-04-14 | 2023-07-04 | 南通大学 | Chinese-English mixed speech recognition method |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11017761B2 (en) * | 2017-10-19 | 2021-05-25 | Baidu Usa Llc | Parallel neural text-to-speech |
US11238843B2 (en) * | 2018-02-09 | 2022-02-01 | Baidu Usa Llc | Systems and methods for neural voice cloning with a few samples |
US10923141B2 (en) * | 2018-08-06 | 2021-02-16 | Spotify Ab | Singing voice separation with deep u-net convolutional networks |
US10971170B2 (en) * | 2018-08-08 | 2021-04-06 | Google Llc | Synthesizing speech from text using neural networks |
CN113678200A (en) * | 2019-02-21 | 2021-11-19 | 谷歌有限责任公司 | End-to-end voice conversion |
-
2022
- 2022-03-16 EP EP22714708.9A patent/EP4298631A1/en active Pending
- 2022-03-16 WO PCT/US2022/020606 patent/WO2022203922A1/en active Application Filing
- 2022-03-16 JP JP2023558802A patent/JP2024511625A/en active Pending
- 2022-03-16 US US17/655,030 patent/US20220310056A1/en active Pending
- 2022-03-16 KR KR1020237036325A patent/KR20230158614A/en unknown
Also Published As
Publication number | Publication date |
---|---|
US20220310056A1 (en) | 2022-09-29 |
EP4298631A1 (en) | 2024-01-03 |
KR20230158614A (en) | 2023-11-20 |
WO2022203922A1 (en) | 2022-09-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7436709B2 (en) | Speech recognition using unspoken text and speech synthesis | |
US11908448B2 (en) | Parallel tacotron non-autoregressive and controllable TTS | |
JP2024511625A (en) | Conformer-based speech conversion model | |
US11475874B2 (en) | Generating diverse and natural text-to-speech samples | |
US20220310059A1 (en) | Phonemes And Graphemes for Neural Text-to-Speech | |
JP7393585B2 (en) | WaveNet self-training for text-to-speech | |
Purohit et al. | Intelligibility improvement of dysarthric speech using mmse discogan | |
Li et al. | End-to-end mongolian text-to-speech system | |
Săracu et al. | An analysis of the data efficiency in Tacotron2 speech synthesis system | |
US20230013777A1 (en) | Robust Direct Speech-to-Speech Translation | |
CN117396958A (en) | Speech conversion model based on convolution enhanced transformation neural network | |
Ai et al. | A new approach to accent recognition and conversion for mandarin chinese | |
US20230360632A1 (en) | Speaker Embeddings for Improved Automatic Speech Recognition | |
US20240135915A1 (en) | Residual adapters for few-shot text-to-speech speaker adaptation | |
WO2024091526A1 (en) | Residual adapters for few-shot text-to-speech speaker adaptation | |
WO2023288169A1 (en) | Two-level text-to-speech systems using synthetic training data |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20231106 |