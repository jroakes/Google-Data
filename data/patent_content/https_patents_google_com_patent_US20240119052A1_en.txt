US20240119052A1 - Tuning Approximate Nearest Neighbor Search Engines for Speed-Recall Tradeoffs Via Lagrange Multiplier Methods - Google Patents
Tuning Approximate Nearest Neighbor Search Engines for Speed-Recall Tradeoffs Via Lagrange Multiplier Methods Download PDFInfo
- Publication number
- US20240119052A1 US20240119052A1 US18/474,907 US202318474907A US2024119052A1 US 20240119052 A1 US20240119052 A1 US 20240119052A1 US 202318474907 A US202318474907 A US 202318474907A US 2024119052 A1 US2024119052 A1 US 2024119052A1
- Authority
- US
- United States
- Prior art keywords
- search
- vector
- tuning
- generating
- computing device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 title claims abstract description 66
- 238000013139 quantization Methods 0.000 claims abstract description 107
- 238000005457 optimization Methods 0.000 claims abstract description 15
- 239000013598 vector Substances 0.000 claims description 165
- 238000010200 validation analysis Methods 0.000 claims 4
- 230000006870 function Effects 0.000 description 40
- 238000013459 approach Methods 0.000 description 11
- 238000010586 diagram Methods 0.000 description 8
- 239000010410 layer Substances 0.000 description 8
- 230000008901 benefit Effects 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 5
- 238000012549 training Methods 0.000 description 3
- 230000004931 aggregating effect Effects 0.000 description 2
- 238000000354 decomposition reaction Methods 0.000 description 2
- 238000013136 deep learning model Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 238000010845 search algorithm Methods 0.000 description 2
- 230000009466 transformation Effects 0.000 description 2
- 230000004888 barrier function Effects 0.000 description 1
- 235000000332 black box Nutrition 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 238000007781 pre-processing Methods 0.000 description 1
- 238000013138 pruning Methods 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2453—Query optimisation
- G06F16/24534—Query rewriting; Transformation
- G06F16/24542—Plan optimisation
- G06F16/24545—Selectivity estimation or determination
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2453—Query optimisation
- G06F16/24534—Query rewriting; Transformation
- G06F16/24549—Run-time optimisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
Definitions
- the present disclosure relates generally to search engines, and more particularly to tuning approximate nearest neighbor (ANN) search engines for speed-recall tradeoffs via Lagrange multiplier methods.
- ANN approximate nearest neighbor
- Efficient nearest neighbor search is an integral part of approaches to numerous tasks in machine learning and information retrieval; it has been leveraged to effectively solve a number of challenges in recommender systems, coding theory, multimodal search, and language modeling.
- Vector search over the dense, high-dimensional embedding vectors generated from deep learning models has become especially important following the rapid rise in capabilities and performance of such models.
- Nearest neighbor search is also increasingly being used for assisting training tasks in machine learning.
- finding the “exact” nearest neighbor to a query becomes computationally inefficient.
- ANN approximate nearest neighbor
- the ANN search problem is fundamental to efficiently serving many real-world machine learning applications.
- a number of techniques have been developed for ANN search that allow queries over large datasets to be answered with low latency and high accuracy.
- such techniques often require setting numerous hyperparameters that affect the speed-recall tradeoff, and suboptimal settings may degrade performance significantly from the maximum achievable.
- the embodiments are directed towards automatically tuning quantization-based approximate nearest neighbor (ANN) search methods and systems (e.g., search engines) to perform at the speed-recall pareto frontier.
- ANN quantization-based approximate nearest neighbor
- the embodiments employ Lagrangian-based methods to perform constrained optimization on theoretically-grounded search cost and recall models.
- the resulting tunings when paired with a set of tuning parameters (e.g., a vector of tuning parameters), exhibit excellent performance on standard benchmarks while requiring minimal tuning or configuration complexity.
- the method may include generating, at a computing device, a recall loss function.
- the recall loss function may indicate an error metric for the search engine.
- the recall loss function may be based on a tuning vector and a set of approximate nearest neighbor (ANN) search results.
- the set of ANN search results may be based on a multi-level set of quantizations of a set of vector embeddings for a set of data objects.
- a tradeoff objective function may be generated at the computing device.
- the tradeoff objective function may indicate a tradeoff between the error metric and a search cost for the search engine.
- the tradeoff objective function may be based on the recall loss function, a search cost constraint, the tuning vector, and a Lagrange multiplier.
- a value for the Lagrange multiplier may be determined at the computing device. Determining the value of the Lagrange multiplier may be based on a value for the tradeoff between the error metric and the search cost for the search engine.
- Values for components of the tuning vector may be determined at the computing device. Determining the values for the components may be based on the value for the Lagrange multiplier and the tradeoff objective function.
- the search engine may be configured at the computing device. Configuring the search engine may be based on (or in accordance with) the values for the components of the tuning vector.
- FIG. 1 depicts an example search environment that various embodiments may be practiced in
- FIG. 2 depicts an example multi-level hierarchical quantization index according to various embodiments
- FIG. 3 shows a non-limiting example of pseudo-code to implement a method that employs the quantization index of FIG. 2 for ANN search, according to various embodiments;
- FIG. 4 depicts a flowchart diagram of an example method for providing search services to a client device according to example embodiments of the present disclosure
- FIG. 5 depicts a flowchart diagram of an example method for optimizing a hyperparameter vector for a search engine according to example embodiments of the present disclosure
- FIG. 6 depicts a flowchart diagram of an example method for tuning a tunable search engine according to example embodiments of the present disclosure.
- FIG. 7 depicts a flowchart diagram of an example method for configuring a search engine according to example embodiments of the present disclosure.
- Efficient nearest neighbor search is an integral part of approaches to numerous tasks in machine learning and information retrieval. For example, nearest neighbor search has been leveraged to effectively solve a number of challenges in recommender systems, coding theory, multimodal search, and language modeling. Vector search over the dense, high-dimensional embedding vectors generated from deep learning models has become especially important following the rapid rise in capabilities and performance of such models. Nearest neighbor search is also increasingly being used for assisting training tasks in machine learning. However, as the number of elements to search over scales to large numbers, finding the “exact” nearest neighbor to a query becomes computationally inefficient. Thus, when the number of search elements is sufficiently large, a transformation to an approximate nearest neighbor (ANN) search paradigm may be applied.
- ANN approximate nearest neighbor
- the embodiments are directed towards automatically tuning quantization-based ANN search methods and systems (e.g., search engines) to perform at the speed-recall pareto frontier.
- quantization-based ANN search methods and systems e.g., search engines
- the embodiments employ Lagrangian-based methods to perform constrained optimization on theoretically-grounded search cost and recall models.
- the resulting tunings when paired with the efficient quantization-based ANN implementation of the embodiments, exhibit excellent performance on standard benchmarks while requiring minimal tuning or configuration complexity.
- the nearest neighbor search problem is as follows: an n-item dataset ⁇ n ⁇ d composed of d-dimensional vectors may be given or inputted.
- a function for computing the distance e.g., a distance function and/or metric
- D d ⁇ : d ⁇ : may also be given, inputted, and/or defined.
- the embodiments find the indices of the k-nearest neighbors in the dataset to q:
- D e.g., a distance metric
- D(q,x) ⁇ (q,x) for maximum inner product search (MIPS)
- D(q,x) ⁇ q ⁇ x ⁇ 2 2 for Euclidean distance search.
- MIPS maximum inner product search
- D(q,x) ⁇ q ⁇ x ⁇ 2 2 for Euclidean distance search
- the embodiments employ theoretically-grounded models for recall and search cost for quantization-based ANN algorithms.
- the embodiments further employ efficient Lagrange multipliers-based techniques for optimizing either of these metrics with respect to the other (i.e., recall and search cost).
- the embodiments include a constrained optimization approach that is very general that may be extended to distance measures, quantization algorithms, and search paradigms beyond those that are explicitly discussed herein.
- the embodiments include methods directed towards tuning ANN algorithms in online (e.g., real time) and offline modes.
- the search algorithm In an online mode, the search algorithm must respond to a sufficiently voluminous stream of latency-sensitive queries arriving at roughly constant frequency.
- Such online modes may be employed in recommender and semantic systems where ANN latency directly contributes to smoothness of the end-user experience. Due to the latency-sensitive nature of these applications, query batching may not be employed.
- a sample set of queries, representative of the overall query distribution, may be employed to tune a data structure used by the embodiments.
- Exact brute-force search methods typically fail to perform well in an online mode, where the lack of batching makes search highly memory-bandwidth constrained.
- the embodiments directed towards approximate search algorithms are therefore generally preferred.
- the performance of the embodiments may be evaluated along two axes: (1) accuracy and (2) search cost.
- Accuracy may be quantified (e.g., benchmarked) via recall@ k, where k is the desired number of neighbors.
- the c-approximation ratio e.g., the ratio of the approximate and the true nearest-neighbor distance
- the search cost axis may be quantified (e.g., benchmarked) via by the average number of queries per second (QPS) a given server can handle.
- aspects of the present disclosure provide a number of technical effects and benefits.
- the technical effects and benefits may be measured via various accuracy and search cost (e.g., computational complexity or efficiency) benchmarks.
- accuracy and search cost e.g., computational complexity or efficiency
- the embodiments when compared to brute force (or exhaustive) conventional grid search techniques applied to millions-scale datasets, the embodiments (which are significantly more computationally efficient than these conventional brute-force methods) provide similar or even enhanced performance.
- the embodiments achieve superior performance to tunings generated by a black-box optimizer on the same ANN index, and over conventional methods to various benchmark (e.g., computation efficiency and recall) for hyperparameter tuning on billions-scale datasets.
- FIG. 1 depicts an example search environment 100 that various embodiments may be practiced in.
- Search environment 100 may include a include a client device 102 , a server device 104 , and a set of data objects 106 that are communicatively coupled via a communication network 108 .
- the servicer device 104 may implement search services 110 for the client device 102 . That is, client device 102 may provide a search query to the search services 110 .
- the search query may be a query to search over objects included in the set of data objects 106 .
- the search services 110 may return a subset of the set of data objects 106 to the client device 102 .
- the subset of the set of data objects 106 may at least approximately match the search query.
- search services 110 may include a trained vector embedder 112 , a set of vector embeddings 114 , and a query vector embedding 116 .
- Search services 110 may also include a quantization index generator 118 , a set of vector embedding quantizations 120 , a quantization index 122 , and a tunable search engine 124 .
- search services 110 may include a search engine tuner 126 , a set of tuning queries 128 , and an optimized hyperparameter vector 130 .
- the trained vector embedder 112 is generally responsible for generating a vector embedding for each data object of the set of data objects 106 . That is, the trained vector embedder 112 is responsible for generating the set of vector embeddings 114 .
- the set of vector embeddings may include one or more vector embeddings for each data object of the set of data objects 106 . Accordingly, each vector embedding of the set of vector embeddings 114 may be a data object vector embedding for a corresponding data object of the set of data objects 106 . In some embodiments, there may be a one-to-one correspondence between the data objects of the set of data objects 106 and the vector embeddings of the set of vector embeddings.
- the trained vector embedder 112 may generate the query vector embedding 116 for a search query received from the client device 102 .
- the vector embeddings of the set of vector embeddings 114 e.g., data object vector embeddings
- the query vector embedding 116 may be vectors within the same vector space.
- the tunable search engine 124 may perform approximate nearest neighbor (AAN) searches over the set of data objects 106 , based on one or more queries.
- quantization generator 118 is generally responsible for generating the set of vector embedding quantizations 120 .
- the set of vector embedding quantizations 120 may include a set of quantizations for each vector embedding of the set of vector embeddings.
- the set of quantizations for a vector embedding may include a set of vector quantizations (VO) for the vector embedding and a set of product quantizations (PQ) for the vector embedding.
- the quantization generator 118 may additionally generate the quantization index 122 .
- the quantization index 122 and the set of vector embedding quantizations 120 are discussed at least in conjunction with FIG. 2 .
- the quantization index 122 may be a multi-level quantization hierarchy composed of multiple VQ layers and multiple PQ layers corresponding to a hierarchical structure (e.g., an index) of the set of vector embedding quantizations 120 .
- the tunable search engine 124 may employ the set of vector embedding quantizations 120 and the quantization index 122 to efficiently search (e.g., ANN search) over the set of data objects 106 , and find search results matching a search query (e.g., corresponding to the query vector embedding 116 ).
- ANN search method employed by the tunable search engine 124
- FIG. 3 discusses the embodiments of a ANN search method
- the tunable search engine 124 may be tuned via the optimized hyperparameter vector 130 .
- Each component of the optimized hyperparameter vector 130 may correspond to a hyperparameter for searching the set of vector embedding quantizations (and the quantization index 122 ).
- the search engine tuner 126 may find (or generate) the optimized hyperparameter vector 130 within a vector space of hyperparameters.
- the search engine tuner 126 may employ the set of tuning queries 128 to generate the optimized hyperparameter vector 130 .
- the set of tuning queries 128 may be a set of training queries.
- FIG. 2 depicts an example multi-level hierarchical quantization index 200 according to various embodiments.
- the quantization index 200 may be equivalent or similar to the quantization index 122 of FIG. 1 .
- the quantization index 200 may be a multi-level quantization hierarchy composed of multiple vector quantization (VQ) layers and multiple product quantization (PQ) layers. That is, the embodiments employ hierarchical quantization index 200 , which includes VQ layers and PQ layers.
- VQ vector quantization
- PQ product quantization
- VQ(X) Vector-quantizing an input set of vectors C ⁇ n ⁇ d , which is denoted as VQ(X), produces a codebook C ⁇ c ⁇ d and codewords w ⁇ 1,2, . . . , c ⁇ n .
- the set referred to as X may be equivalent or similar to the set of vector embeddings 114 of FIG. 1 .
- Each element of X is quantized to the closest codebook element in C, and the quantization assignments are stored in w.
- VQ can be interpreted as a pruning tree whose root stores C and has c children; the ith child contains the points ⁇ j
- w j i ⁇ ; equivalently, this tree is an inverted index which maps each centroid to the datapoints belonging to the centroid.
- the approximate representation of i under PQ can be recovered as the concatenation of
- VQ is generally performed with a large codebook whose size scales with n and whose size is significant relative to the size of the codewords.
- PQ is generally performed with a constant, small c k that allows for fast in-register single instruction/multiple data (SIMD) lookups for each codebook element, and its storage cost is dominated by the codeword size.
- SIMD single instruction/multiple data
- VQ and PQ both produce fixed-bitrate encodings of the original dataset.
- such matching of bitrates may be achieved by using multiple quantization levels and using the lower-bitrate levels to select which portions of the higher-bitrate levels to evaluate.
- VQ may be recursively applied to C for arbitrarily many levels. and all C are product-quantized as well.
- This procedure of generating quantization index 200 results in a set of quantizations ⁇ tilde over ( ⁇ ) ⁇ 1 , . . . , ⁇ tilde over ( ⁇ ) ⁇ m of progressively higher bitrate.
- the resulting set of quantizations (e.g., both VQ and PQ) may comprise the set of vector embedding quantizations 120 of FIG. 1 .
- FIG. 3 shows a non-limiting example of pseudo-code 300 to implement a method that employs the quantization index 200 of FIG. 2 for ANN search, according to various embodiments.
- the tunable search engine 124 of FIG. 1 may implement a method equivalent or similar to pseudo-code 300 .
- the search engine tuner 126 is optimizing the hyperparameter vector (e.g., optimized hyperparameter vector 130 of FIG. 1 )
- the method of pseudo-code 300 may be employed.
- the method of pseudo-code 300 performs a ANN search using the quantization index 200 of FIG.
- the optimized hyperparameter vector 130 may be equivalent to or similar to the length-m vector of search hyperparameters t.
- the search engine tuner 126 of FIG. 1 is responsible for optimizing the components of the length-m vector of search hyperparameters t.
- the search engine tuner 126 finds values for the components of t that give excellent tradeoffs between search speed and recall.
- the following illustrates various proxy metrics for ANN recall and search latency as a function of the tuning t, and then describe a Lagrange multipliers-based approach to efficiently computing t to optimize for a given speed-recall tradeoff.
- the operations of the search engine tuner are discussed for its optimization of the optimized hyperparameter vector 130 .
- the recall may be computed by simply performing approximate search over and computing the recall empirically.
- the set of tuning queries 128 may be equivalent or similar to the query set .
- this empirical recall is approximated in a manner amenable to the constrained optimization approach.
- the quantizations ⁇ tilde over ( ⁇ ) ⁇ (i) may be similar to set of vector embedding quantizations 120 of FIG. 1 , which as discussed above, may be generated by quantization generator 118 of FIG. 1 .
- the quantization index 122 of FIG. 1 may encode the hierarchy of the quantizations. Define functions 0 (q,t), . . . , m (q,t) to denote the various computed by pseudo-code 200 for query q and tuning t, and let (q) be the set of ground-truth nearest neighbors for q. Note that this recall equals
- This recall may be decomposed into a telescoping product and multiply it among all queries in to derive the following expression for geometric-mean recall:
- the telescoping decomposition takes advantage of the fact that
- Some embodiments may employ the geometric mean for aggregating recall over a query set.
- Other embodiments may employ the arithmetic mean for aggregating recall over a query set.
- the geometric mean discussion focusses on the geometric mean embodiments. Note that geometric embodiments enable the decomposition in log-space. Note that the arithmetic mean is bounded from below by the geometric mean.
- the discussion focusses on the inner quantity inside the logarithm and how to compute it efficiently.
- the chief problem is that i (q,t) has an implicit dependency on i ⁇ 1 (q,t) because i ⁇ 1 is the candidate set from which the embodiments compute quantized distances using ⁇ tilde over ( ⁇ ) ⁇ (1) in pseudo-code 200 . This results in i (q,t) depending on all t 1 , . . . , t i and not just t i .
- the single-layer candidate set is defined as:
- search cost may be directly measured empirically, but a simple yet effective search cost proxy compatible with the Lagrange optimization method of the embodiments is presented.
- J gives the ratio of memory accesses performed per-query when performing approximate search with tuning t to the number of memory accesses performed by exact brute-force search. This gives a good approximation to real-world search cost because memory bandwidth is the bottleneck for quantization-based ANN in the non-batched case. It is emphasized that this cost model is effective for comparing amongst tunings for a quantization-based ANN index, which is sufficient for the present purposes, but likely lacks the power to compare performance among completely different ANN approaches, like graph-based solutions. Differences in memory read size, memory request queue depth, amenability to vectorization, and numerous other characteristics have a large impact on overall performance but are not captured in this model.
- each per-quantization loss i may be taken before passing it into the constrained optimization procedure. This results in a better-behaved optimization result but is also justified from an ANN method perspective.
- any quantization level i consider some two choices of t i that lead to loss and cost contributions of (l 1 ,j 1 ) and (l 2 ,j 2 ). Any (loss, cost) tuple on the line segment between these two points may be achieved via a randomized algorithm that picks between the two choices of t i with the appropriate weighting, which implies the entire convex hull is achievable. Empirically, it may be found that i is close to convex already, so this is more of a theoretical safeguard than a practical concern.
- the tuning problem of maximizing recall with a search cost limit J max may be phrased as:
- the objective function is a sum of convex functions and therefore convex itself, while the constraints are linear and strictly feasible, so strong duality holds for this optimization problem. Therefore, the Lagrangian may be utilized, as:
- the pareto frontier itself will be piecewise, composed of at most nm points. It follows then that there are at most nm relevant ⁇ that result in different optimization results, namely those obtained by taking the consecutive differences among each i and dividing by
- FIGS. 4 - 7 depict flowcharts for various methods implemented by the embodiments. Although the flowcharts of FIGS. 4 - 7 depict steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. Various steps of the methods of FIGS. 4 - 7 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- a computing device e.g., client device 102 and/or server device 104 of FIG. 1
- Various software components and/or modules implemented by the computing devices e.g., search services 110 of FIG. 1
- FIG. 4 depicts a flowchart diagram of an example method 400 for providing search services to a client device according to example embodiments of the present disclosure.
- Method 400 begins at block 402 , where a set of vector embeddings (e.g., set of vector embeddings 114 of FIG. 1 ) for a set of data objects (e.g., set of data objects 106 of FIG. 1 ) is generated at a server device (e.g., server device 104 of FIG. 1 ).
- a set of vector embedding quantizations e.g., vector embedding quantizations 120 of FIG.
- a hyperparameter vector e.g., t
- a tunable search engine e.g., tunable search engine 124 of FIG. 1
- the tunable search engine is configured in accordance with the optimized hyperparameter vector.
- the configuration of the tunable search engine may be performed at the server device.
- a search query is received at the server device.
- the search query may have been transmitted from a client device (e.g., client device 102 of FIG. 1 ).
- a set of search results is determined at the server device.
- determining a set of search results are discussed at least in conjunction with pseudo-code 300 of FIG. 3 . However, briefly here, determining the set of search results may be based on the received search query, the set of vector embedding quantizations, the quantitation index, the configured search engine, and the optimized hyperparameter vector.
- indication of the set of search results is transmitted from the server device to the client device.
- FIG. 5 depicts a flowchart diagram of an example method 500 for optimizing a hyperparameter vector for a search engine according to example embodiments of the present disclosure.
- Method 500 begins at block 502 , where a set of ground-truth search results (e.g., ) is generated at the server device. Generating the set of ground truth search results may be based on a set of tuning queries (e.g., ) and a set of vector embeddings (e.g., set of vector embeddings 114 of FIG. 1 ) for a set of data objects (e.g., set of data objects 106 of FIG. 1 ).
- a set of tuning queries e.g.,
- vector embeddings e.g., set of vector embeddings 114 of FIG. 1
- data objects e.g., set of data objects 106 of FIG. 1 .
- a set of approximate nearest neighbor (ANN) search results (e.g., S 1 ) for each quantization level of the quantization index (e.g., quantization index 200 of FIG. 2 where the levels are addressed by the ⁇ i ) is generated at the server device.
- Generating the set of ANN search results (e.g., S i ) may be based on the set of tuning queries and a set of vector embedding quantity quantizations (e.g., ⁇ i ) for the set of data objects.
- a recall loss function (e.g., see equations (1)-(5)) is generated at the server device.
- the recall loss function may be a function of the hyperparameter vector (e.g., or tuning vector t) and the set of tuning queries. The generation of the recall loss function may be based on the set of ground truth search results and the set of ANN search results.
- a search cost constraint (e.g., see equation (6)) is generated at the server device.
- the search cost constraint maybe a function of the hyperparameter vector. Generating the search cost constraint may be based on a set of vector embeddings, the set of vector embedding quantizations, and the quantization index.
- a tradeoff objective function is generated at the server device.
- the tradeoff objective function may be a function of a Lagrange multiplier (e.g., ⁇ ) and the hyperparameter vector. Generating the tradeoff objective function may be based on the recall loss function and the search cost constraint.
- a value of the Lagrange multiplier is determined at the server device. Determining the value of those around multiplier maybe based on a value of a search cost and recall tradeoff.
- values of components of the optimized hyperparameter vector may be determined at the server device. Determining the values of the components of the optimized hyperparameter vector maybe based on the tradeoff objective function and the determined value of the Lagrange multiplier.
- FIG. 6 depicts a flowchart diagram of an example method 600 for tuning a tunable search engine according to example embodiments of the present disclosure.
- Method 600 begins at block 602 , where a recall loss function (e.g., see equations ( 1 )-( 5 )) is generated at a computing device (e.g., server device 104 of FIG. 1 ).
- the recall loss function may indicate an error metric (e.g., false negatives or Type II errors) for a search engine (e.g., tunable search engine 124 of FIG. 1 ).
- the recall loss function may be based on a tuning vector (e.g., a hyperparameter vector) and a set of approximate nearest neighbor (ANN) search results.
- ANN approximate nearest neighbor
- the set of ANN search results may be based on a multi-level set of quantizations (e.g., set of vector embedding quantizations 120 of FIG. 1 ) of a set of vector embeddings (e.g., set of vector embeddings 114 of FIG. 1 ) for a set of data objects (e.g., set of data objects 106 of FIG. 1 ).
- a tradeoff objective function may be generated at the computing device.
- the tradeoff objective function may indicate a tradeoff between the error metric and a search cost (e.g., J(t)) for the search engine.
- the tradeoff objective function is based on the recall loss function (e.g., ( , t)) , a search cost constraint (e.g., see equation (6)), the tuning vector, and a Lagrange multiplier (e.g., ⁇ ).
- a value for the Lagrange multiplier is determined at the computing device. Determining the value for the Lagrange multiplier may be based on a value for the tradeoff between the error metric and the search cost for the search engine.
- values for components of the tuning vector are determined at the computing device. Determining the values for the components of the tuning vector may be based on the value for the Lagrange multiplier and the tradeoff objective function.
- the search engine is configured at the computing device. Configuring the search engine may be based on the values for the components of the tuning vector.
- method 600 includes determining, at the computing device, a set of ground-truth search results. Determining the set of ground-truth search results may be based on a set of tuning queries and the set of vector embeddings for the set of data objects. The recall loss function may be generated at the computing device further based on the set of ground-truth search results. In some embodiments, the set of ANN search results are determined further based on a set of tuning queries.
- method 600 further includes receiving, at the computing device, a search query.
- the search query may be received from a client of the configured search engine.
- a set of search results may be determined, at the computing device, based on the search query and the configured search engine.
- the set of search results may be provided to the client of the configured search engine.
- method 600 may further include generating, at the computing device, a set of vector embeddings for the set of data objects.
- the multi-level set of quantizations for the set of vector embeddings may be generated based on the set of vector embeddings for the set of data objects.
- the multi-level set of quantizations for the set of vector embeddings may include a set of vector quantization (VQ) layers and a set of product quantization (PQ) layers.
- method 600 may further include generating, at the computing device, the set of vector embeddings for the set of objects.
- a set of ground-truth search results may be generated, at the computing device, based on a set of tuning queries and the set of vector embeddings for the set of objects.
- the set of ANN search results may be generated further based on the set of tuning queries and the multi-level set of quantization of the set of vector embeddings.
- the recall loss function may be generated, at the computing device, based on the set of ground-truth search results and the set of ANN search results.
- method 600 includes generating, at the computing device, the search cost constraint based on a set of vector embeddings and a quantization index corresponding to the multi-level set of quantizations of the set of vector embeddings for the set of data objects.
- the tradeoff objective function may be indicative of a speed-recall Pareto frontier for the tradeoff between the error metric and the search cost for the search engine.
- the search cost for the search engine may be based on the multi-level set of quantizations of a set of vector embeddings for a set of data objects.
- FIG. 7 depicts a flowchart diagram of an example method 700 for configuring a search engine according to example embodiments of the present disclosure.
- Method 700 begins at block 702 , where a set of optimized tuning vectors is determined. Each optimized tuning vector of the set of optimized tuning vectors is optimized for a search cost and recall tradeoff of a set of search cost and recall tradeoffs.
- a look up table LUT is generated. The LUT indicates (or encodes) corresponding optimized tuning vectors and search cost and recall tradeoffs.
- an indication of a selected value of search cost and recall tradeoff is received.
- an optimized tuning vector corresponding to the selected value of the search cost and recall tradeoff is accessed the the LUT.
- the tunable search engine is configured in accordance to the optimized tuning vector corresponding to the selected value of search cost and recall trade off.
Abstract
The disclosure is directed towards automatically tuning quantization-based approximate nearest neighbors (ANN) search methods and systems (e.g., search engines) to perform at the speed-recall pareto frontier. With a desired search cost or recall as input, the embodiments employ Lagrangian-based methods to perform constrained optimization on theoretically-grounded search cost and recall models. The resulting tunings, when paired with the efficient quantization-based ANN implementation of the embodiments, exhibit excellent performance on standard benchmarks while requiring minimal tuning or configuration complexity.
Description
- The present application claims the benefit of priority of U.S. Provisional Application Ser. No. 63/410,536, filed on Sep. 27, 2022, titled TUNING APPROXIMATE NEAREST NEIGHBOR SEARCH ENGINES FOR SPEED-RECALL TRADEOFFS VIA LAGRANGE MULTIPLIER METHODS, which is incorporated herein by reference.
- The present disclosure relates generally to search engines, and more particularly to tuning approximate nearest neighbor (ANN) search engines for speed-recall tradeoffs via Lagrange multiplier methods.
- Efficient nearest neighbor search is an integral part of approaches to numerous tasks in machine learning and information retrieval; it has been leveraged to effectively solve a number of challenges in recommender systems, coding theory, multimodal search, and language modeling. Vector search over the dense, high-dimensional embedding vectors generated from deep learning models has become especially important following the rapid rise in capabilities and performance of such models. Nearest neighbor search is also increasingly being used for assisting training tasks in machine learning. However, as the number of elements to search over scales to large numbers, finding the “exact” nearest neighbor to a query becomes computationally inefficient. Thus, when the number of search elements is sufficiently large, a transformation to an approximate nearest neighbor (ANN) search paradigm may be applied.
- The ANN search problem is fundamental to efficiently serving many real-world machine learning applications. A number of techniques have been developed for ANN search that allow queries over large datasets to be answered with low latency and high accuracy. However, such techniques often require setting numerous hyperparameters that affect the speed-recall tradeoff, and suboptimal settings may degrade performance significantly from the maximum achievable.
- Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.
- The embodiments are directed towards automatically tuning quantization-based approximate nearest neighbor (ANN) search methods and systems (e.g., search engines) to perform at the speed-recall pareto frontier. With a desired search cost or recall as input, the embodiments employ Lagrangian-based methods to perform constrained optimization on theoretically-grounded search cost and recall models. The resulting tunings, when paired with a set of tuning parameters (e.g., a vector of tuning parameters), exhibit excellent performance on standard benchmarks while requiring minimal tuning or configuration complexity.
- One example aspect of the present disclosure is directed to a computer-implemented method for operating a search engine. The method may include generating, at a computing device, a recall loss function. The recall loss function may indicate an error metric for the search engine. The recall loss function may be based on a tuning vector and a set of approximate nearest neighbor (ANN) search results. The set of ANN search results may be based on a multi-level set of quantizations of a set of vector embeddings for a set of data objects. A tradeoff objective function may be generated at the computing device. The tradeoff objective function may indicate a tradeoff between the error metric and a search cost for the search engine. The tradeoff objective function may be based on the recall loss function, a search cost constraint, the tuning vector, and a Lagrange multiplier. A value for the Lagrange multiplier may be determined at the computing device. Determining the value of the Lagrange multiplier may be based on a value for the tradeoff between the error metric and the search cost for the search engine. Values for components of the tuning vector may be determined at the computing device. Determining the values for the components may be based on the value for the Lagrange multiplier and the tradeoff objective function. The search engine may be configured at the computing device. Configuring the search engine may be based on (or in accordance with) the values for the components of the tuning vector.
- Other aspects of the present disclosure are directed to various systems, methods, apparatuses, non-transitory computer-readable media, computer-readable instructions, and computing devices.
- These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, explain the related principles.
- Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which refers to the appended figures, in which:
-
FIG. 1 depicts an example search environment that various embodiments may be practiced in; -
FIG. 2 depicts an example multi-level hierarchical quantization index according to various embodiments; -
FIG. 3 shows a non-limiting example of pseudo-code to implement a method that employs the quantization index ofFIG. 2 for ANN search, according to various embodiments; -
FIG. 4 depicts a flowchart diagram of an example method for providing search services to a client device according to example embodiments of the present disclosure; -
FIG. 5 depicts a flowchart diagram of an example method for optimizing a hyperparameter vector for a search engine according to example embodiments of the present disclosure; -
FIG. 6 depicts a flowchart diagram of an example method for tuning a tunable search engine according to example embodiments of the present disclosure; and -
FIG. 7 depicts a flowchart diagram of an example method for configuring a search engine according to example embodiments of the present disclosure. - Efficient nearest neighbor search is an integral part of approaches to numerous tasks in machine learning and information retrieval. For example, nearest neighbor search has been leveraged to effectively solve a number of challenges in recommender systems, coding theory, multimodal search, and language modeling. Vector search over the dense, high-dimensional embedding vectors generated from deep learning models has become especially important following the rapid rise in capabilities and performance of such models. Nearest neighbor search is also increasingly being used for assisting training tasks in machine learning. However, as the number of elements to search over scales to large numbers, finding the “exact” nearest neighbor to a query becomes computationally inefficient. Thus, when the number of search elements is sufficiently large, a transformation to an approximate nearest neighbor (ANN) search paradigm may be applied.
- The embodiments are directed towards automatically tuning quantization-based ANN search methods and systems (e.g., search engines) to perform at the speed-recall pareto frontier. With a desired search cost or recall as input, the embodiments employ Lagrangian-based methods to perform constrained optimization on theoretically-grounded search cost and recall models. The resulting tunings, when paired with the efficient quantization-based ANN implementation of the embodiments, exhibit excellent performance on standard benchmarks while requiring minimal tuning or configuration complexity.
- Formally, the nearest neighbor search problem is as follows: an n-item dataset ∈n×d composed of d-dimensional vectors may be given or inputted. A function for computing the distance (e.g., a distance function and/or metric) between two vectors D:
-
- Common choices of D (e.g., a distance metric) include D(q,x)=−(q,x) for maximum inner product search (MIPS) and D(q,x)=∥q−x∥2 2 for Euclidean distance search. A linear-time scan over χ solves the “exact” nearest neighbor search problem but doesn't scale to the large dataset sizes often found in modern-day applications. In order to remain practically computable, this dramatic increase in complexity with scaling the dataset size necessitates the development of approximate nearest neighbor (ANN) algorithms.
- A number of conventional approaches to the ANN problem have been attempted to control the trade off a small search accuracy loss, measured in result recall, for a correspondingly large increase in search speed. However, these conventional approaches rely on tuning a number of hyperparameters that adjust the tradeoff between speed and recall, and poor hyperparameter choices may result in performance far below what is achievable at the same recall with ideal hyperparameter tuning. One conventional approach to tuning is performing a brute force grid search that empirically measures recall and speed using sample queries and finds tunings on the pareto frontier. However, empirically measuring recall is an expensive process; capturing samples for even a few thousand tunings might take days. Combined with the observation that the number of hyperparameters necessary for efficient ANN grows with respect to dataset size, it is found that the algorithmic tuning space is too large to practically grid-search for billions-scale datasets.
- As a result, efficient ANN search at the billions-scale is conventionally achievable only with domain expertise in ANN algorithms and an understanding of how various algorithmic hyperparameters interact with each other and affect overall performance. This barrier to entry is becoming increasingly problematic with the growth in dataset sizes and in the popularity of the ANN-based retrieval paradigm. To counter these issues with conventional approaches, the embodiments are directed towards highly performant ANN indices may be created and tuned with minimal configuration complexity to the end user.
- More specifically, the embodiments employ theoretically-grounded models for recall and search cost for quantization-based ANN algorithms. The embodiments further employ efficient Lagrange multipliers-based techniques for optimizing either of these metrics with respect to the other (i.e., recall and search cost). The embodiments include a constrained optimization approach that is very general that may be extended to distance measures, quantization algorithms, and search paradigms beyond those that are explicitly discussed herein.
- The embodiments include methods directed towards tuning ANN algorithms in online (e.g., real time) and offline modes. In an online mode, the search algorithm must respond to a sufficiently voluminous stream of latency-sensitive queries arriving at roughly constant frequency. Such online modes may be employed in recommender and semantic systems where ANN latency directly contributes to smoothness of the end-user experience. Due to the latency-sensitive nature of these applications, query batching may not be employed. A sample set of queries, representative of the overall query distribution, may be employed to tune a data structure used by the embodiments.
- Exact brute-force search methods typically fail to perform well in an online mode, where the lack of batching makes search highly memory-bandwidth constrained. The embodiments directed towards approximate search algorithms are therefore generally preferred. The performance of the embodiments may be evaluated along two axes: (1) accuracy and (2) search cost. Accuracy may be quantified (e.g., benchmarked) via recall@ k, where k is the desired number of neighbors. In other embodiments, the c-approximation ratio (e.g., the ratio of the approximate and the true nearest-neighbor distance) may be employed to quantify the accuracy axis. The search cost axis may be quantified (e.g., benchmarked) via by the average number of queries per second (QPS) a given server can handle.
- Aspects of the present disclosure provide a number of technical effects and benefits. The technical effects and benefits may be measured via various accuracy and search cost (e.g., computational complexity or efficiency) benchmarks. For instance, when compared to brute force (or exhaustive) conventional grid search techniques applied to millions-scale datasets, the embodiments (which are significantly more computationally efficient than these conventional brute-force methods) provide similar or even enhanced performance. Furthermore, the embodiments achieve superior performance to tunings generated by a black-box optimizer on the same ANN index, and over conventional methods to various benchmark (e.g., computation efficiency and recall) for hyperparameter tuning on billions-scale datasets.
-
FIG. 1 depicts anexample search environment 100 that various embodiments may be practiced in.Search environment 100 may include a include aclient device 102, aserver device 104, and a set ofdata objects 106 that are communicatively coupled via acommunication network 108. Theservicer device 104 may implementsearch services 110 for theclient device 102. That is,client device 102 may provide a search query to the search services 110. The search query may be a query to search over objects included in the set of data objects 106. Thesearch services 110 may return a subset of the set ofdata objects 106 to theclient device 102. The subset of the set ofdata objects 106 may at least approximately match the search query. - To perform its search functions and capabilities,
search services 110 may include a trainedvector embedder 112, a set ofvector embeddings 114, and a query vector embedding 116.Search services 110 may also include aquantization index generator 118, a set ofvector embedding quantizations 120, aquantization index 122, and atunable search engine 124. Furthermore,search services 110 may include asearch engine tuner 126, a set of tuningqueries 128, and an optimizedhyperparameter vector 130. - The trained
vector embedder 112 is generally responsible for generating a vector embedding for each data object of the set of data objects 106. That is, the trainedvector embedder 112 is responsible for generating the set ofvector embeddings 114. The set of vector embeddings may include one or more vector embeddings for each data object of the set of data objects 106. Accordingly, each vector embedding of the set ofvector embeddings 114 may be a data object vector embedding for a corresponding data object of the set of data objects 106. In some embodiments, there may be a one-to-one correspondence between the data objects of the set ofdata objects 106 and the vector embeddings of the set of vector embeddings. In addition to generating the set ofvector embeddings 114 for the set of data objects, the trainedvector embedder 112 may generate the query vector embedding 116 for a search query received from theclient device 102. The vector embeddings of the set of vector embeddings 114 (e.g., data object vector embeddings) and the query vector embedding 116 may be vectors within the same vector space. - The
tunable search engine 124 may perform approximate nearest neighbor (AAN) searches over the set of data objects 106, based on one or more queries. As such,quantization generator 118 is generally responsible for generating the set ofvector embedding quantizations 120. The set ofvector embedding quantizations 120 may include a set of quantizations for each vector embedding of the set of vector embeddings. The set of quantizations for a vector embedding may include a set of vector quantizations (VO) for the vector embedding and a set of product quantizations (PQ) for the vector embedding. Thequantization generator 118 may additionally generate thequantization index 122. Thequantization index 122 and the set ofvector embedding quantizations 120 are discussed at least in conjunction withFIG. 2 . However, briefly here, thequantization index 122 may be a multi-level quantization hierarchy composed of multiple VQ layers and multiple PQ layers corresponding to a hierarchical structure (e.g., an index) of the set ofvector embedding quantizations 120. Thetunable search engine 124 may employ the set ofvector embedding quantizations 120 and thequantization index 122 to efficiently search (e.g., ANN search) over the set of data objects 106, and find search results matching a search query (e.g., corresponding to the query vector embedding 116). Various embodiments of a ANN search method (employed by the tunable search engine 124) are discussed in conjunction with at leastFIG. 3 . - The
tunable search engine 124 may be tuned via the optimizedhyperparameter vector 130. Each component of the optimizedhyperparameter vector 130 may correspond to a hyperparameter for searching the set of vector embedding quantizations (and the quantization index 122). Thesearch engine tuner 126 may find (or generate) the optimizedhyperparameter vector 130 within a vector space of hyperparameters. Thesearch engine tuner 126 may employ the set of tuningqueries 128 to generate the optimizedhyperparameter vector 130. The set of tuningqueries 128 may be a set of training queries. -
FIG. 2 depicts an example multi-levelhierarchical quantization index 200 according to various embodiments. Thequantization index 200 may be equivalent or similar to thequantization index 122 ofFIG. 1 . As such, thequantization index 200 may be a multi-level quantization hierarchy composed of multiple vector quantization (VQ) layers and multiple product quantization (PQ) layers. That is, the embodiments employhierarchical quantization index 200, which includes VQ layers and PQ layers. The below discussion first provides a brief review of VQ and PQ before describing how VQ and PQ are composed to produce a performant ANN search index. - Vector-quantizing an input set of vectors C∈n×d, which is denoted as VQ(X), produces a codebook C∈c×d and codewords w∈{1,2, . . . , c}n. The set referred to as X may be equivalent or similar to the set of
vector embeddings 114 ofFIG. 1 . Each element of X is quantized to the closest codebook element in C, and the quantization assignments are stored in w. The quantized form of the ith element of χ can therefore be computed as χi=Cwi . - The embodiments may employ VQ by computing the closest codebook elements to the query via:
-
- and returning indices of datapoints belonging to those codebook elements, {j|wj∈
- In contrast to VQ, PQ divides the full d-dimensional vector space into K subspaces and quantizes each space separately. If it is assumed that the subspaces are all equal in dimensionality, each covering l=[d/K] dimensions, then PQ gives K codebooks C(1), . . . , C(K) and K codeword vectors w(1), . . . , w(K), with C(k)∈
k ×l and w(k)∈{1, . . . , ci}n where ck is the number of centroids in subspace k. The approximate representation of i under PQ can be recovered as the concatenation of -
- In the embodiments directed towards ANN search, VQ is generally performed with a large codebook whose size scales with n and whose size is significant relative to the size of the codewords. In contrast, PQ is generally performed with a constant, small ck that allows for fast in-register single instruction/multiple data (SIMD) lookups for each codebook element, and its storage cost is dominated by the codeword size.
- VQ and PQ both produce fixed-bitrate encodings of the original dataset. However, if it is desired to allocate more bitrate to the vectors closer to the query, such matching of bitrates may be achieved by using multiple quantization levels and using the lower-bitrate levels to select which portions of the higher-bitrate levels to evaluate.
- To generate the multiple levels of
hierarchical quantization index 200, one may start with the original dataset and vector-quantize it, resulting in a smaller d-dimensional dataset of codewords C. VQ may be recursively applied to C for arbitrarily many levels. and all C are product-quantized as well. This procedure of generatingquantization index 200 results in a set of quantizations {tilde over (χ)}1, . . . , {tilde over (χ)}m of progressively higher bitrate. The resulting set of quantizations (e.g., both VQ and PQ) may comprise the set ofvector embedding quantizations 120 ofFIG. 1 . -
FIG. 3 shows a non-limiting example ofpseudo-code 300 to implement a method that employs thequantization index 200 ofFIG. 2 for ANN search, according to various embodiments. When searching, thetunable search engine 124 ofFIG. 1 may implement a method equivalent or similar topseudo-code 300. Furthermore, when thesearch engine tuner 126 is optimizing the hyperparameter vector (e.g., optimizedhyperparameter vector 130 ofFIG. 1 ), the method ofpseudo-code 300 may be employed. The method ofpseudo-code 300 performs a ANN search using thequantization index 200 ofFIG. 2 and the set ofvector embedding quantizations 120 and a length-m vector of search hyperparameters t, which controls how quickly the candidate set of neighbors is narrowed down while iterating through the quantization levels. The optimizedhyperparameter vector 130 may be equivalent to or similar to the length-m vector of search hyperparameters t. Thesearch engine tuner 126 ofFIG. 1 is responsible for optimizing the components of the length-m vector of search hyperparameters t. - The
search engine tuner 126 finds values for the components of t that give excellent tradeoffs between search speed and recall. The following illustrates various proxy metrics for ANN recall and search latency as a function of the tuning t, and then describe a Lagrange multipliers-based approach to efficiently computing t to optimize for a given speed-recall tradeoff. - The following discussion details various operations of the components of search services 101 of
FIG. 1 . Notably, the operations of the search engine tuner are discussed for its optimization of the optimizedhyperparameter vector 130. For a given query setqueries 128 may be equivalent or similar to the query set - First fix the dataset χ and all quantizations {tilde over (χ)}(i). The quantizations {tilde over (χ)}(i) may be similar to set of
vector embedding quantizations 120 ofFIG. 1 , which as discussed above, may be generated byquantization generator 118 ofFIG. 1 . Thequantization index 122 ofFIG. 1 may encode the hierarchy of the quantizations. Define functionspseudo-code 200 for query q and tuning t, and let -
-
- where the telescoping decomposition takes advantage of the fact that |
- Maximizing
Equation 1 is equivalent to minimizing its negative logarithm: -
- Next, the discussion focusses on the inner quantity inside the logarithm and how to compute it efficiently. The chief problem is that
pseudo-code 200. This results in -
- which computes the closest ti neighbors to q according to only {tilde over (χ)}(i) irrespective of other quantizations or their tuning settings. This definition may be leveraged by rewriting the cardinality ratio as:
-
-
-
-
-
- which allows us to compute the loss for any tuning t by summing m elements from L.
- Similar to ANN recall, search cost may be directly measured empirically, but a simple yet effective search cost proxy compatible with the Lagrange optimization method of the embodiments is presented.
- Let |{tilde over (χ)}(i)| denote the storage footprint of {tilde over (χ)}(i). At quantization level i, for i<m, selecting the top top ti candidates necessarily implies that a ti/n proportion of {tilde over (χ)}(i+1) will need to be accessed in the next level. Meanwhile, {tilde over (χ)}(1) is always fully searched because it's encountered at the beginning of the search process, where the method has no prior on what points are closest to q. From these observations, a model of the cost of quantization-based ANN search with a tuning t may be constructed as:
-
- J gives the ratio of memory accesses performed per-query when performing approximate search with tuning t to the number of memory accesses performed by exact brute-force search. This gives a good approximation to real-world search cost because memory bandwidth is the bottleneck for quantization-based ANN in the non-batched case. It is emphasized that this cost model is effective for comparing amongst tunings for a quantization-based ANN index, which is sufficient for the present purposes, but likely lacks the power to compare performance among completely different ANN approaches, like graph-based solutions. Differences in memory read size, memory request queue depth, amenability to vectorization, and numerous other characteristics have a large impact on overall performance but are not captured in this model.
- The convex hull of each per-quantization loss
- Formally, the tuning problem of maximizing recall with a search cost limit Jmax may be phrased as:
-
- which may be subject to the constraint that
-
J(t)≤J max -
t 1 ≥ . . . ≥t m. - The objective function is a sum of convex functions and therefore convex itself, while the constraints are linear and strictly feasible, so strong duality holds for this optimization problem. Therefore, the Lagrangian may be utilized, as:
-
- subject to the constraint
-
t 1 ≥ . . . ≥t m. - Furthermore, because the objective function is a sum of m functions, each a convex hull defined by n points, the pareto frontier itself will be piecewise, composed of at most nm points. It follows then that there are at most nm relevant λ that result in different optimization results, namely those obtained by taking the consecutive differences among each
-
FIGS. 4-7 depict flowcharts for various methods implemented by the embodiments. Although the flowcharts ofFIGS. 4-7 depict steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. Various steps of the methods ofFIGS. 4-7 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. A computing device (e.g.,client device 102 and/orserver device 104 ofFIG. 1 ) or a combination of computing devices may perform at least a portion of the steps included in the flowcharts ofFIGS. 4-7 . Various software components and/or modules implemented by the computing devices (e.g.,search services 110 ofFIG. 1 ) may implement at least a portion of the steps included in the flowcharts ofFIGS. 4-7 . -
FIG. 4 depicts a flowchart diagram of anexample method 400 for providing search services to a client device according to example embodiments of the present disclosure.Method 400 begins atblock 402, where a set of vector embeddings (e.g., set ofvector embeddings 114 ofFIG. 1 ) for a set of data objects (e.g., set ofdata objects 106 ofFIG. 1 ) is generated at a server device (e.g.,server device 104 ofFIG. 1 ). Atblock 404, a set of vector embedding quantizations (e.g.,vector embedding quantizations 120 ofFIG. 1 ) and a quantization index (e.g., quantization index 122) for the set of vector embeddings quantizations is generated at the server advice. Various embodiments of a set of vector embedding quantizations and a quantization index are discussed, as least in conjunction withFIG. 2 . Atblock 406, a hyperparameter vector (e.g., t) for a tunable search engine (e.g.,tunable search engine 124 ofFIG. 1 ) is optimized at the server device. Various embodiments of optimizing a hyperparameter vector are discussed at least in conjunction withmethod 500 ofFIG. 5 . Atblock 408, the tunable search engine is configured in accordance with the optimized hyperparameter vector. The configuration of the tunable search engine may be performed at the server device. Atblock 410, a search query is received at the server device. The search query may have been transmitted from a client device (e.g.,client device 102 ofFIG. 1 ). Atblock 412, a set of search results is determined at the server device. Various embodiments of determining a set of search results are discussed at least in conjunction withpseudo-code 300 ofFIG. 3 . However, briefly here, determining the set of search results may be based on the received search query, the set of vector embedding quantizations, the quantitation index, the configured search engine, and the optimized hyperparameter vector. Atblock 414, and indication of the set of search results is transmitted from the server device to the client device. -
FIG. 5 depicts a flowchart diagram of an example method 500 for optimizing a hyperparameter vector for a search engine according to example embodiments of the present disclosure. Method 500 begins at block 502, where a set of ground-truth search results (e.g.,vector embeddings 114 ofFIG. 1 ) for a set of data objects (e.g., set ofdata objects 106 ofFIG. 1 ). Atblock 504, a set of approximate nearest neighbor (ANN) search results (e.g., S1) for each quantization level of the quantization index (e.g.,quantization index 200 ofFIG. 2 where the levels are addressed by the χi) is generated at the server device. Generating the set of ANN search results (e.g., Si) may be based on the set of tuning queries and a set of vector embedding quantity quantizations (e.g., χi) for the set of data objects. Atblock 506, a recall loss function (e.g., see equations (1)-(5)) is generated at the server device. The recall loss function may be a function of the hyperparameter vector (e.g., or tuning vector t) and the set of tuning queries. The generation of the recall loss function may be based on the set of ground truth search results and the set of ANN search results. Atblock 508, a search cost constraint (e.g., see equation (6)) is generated at the server device. The search cost constraint maybe a function of the hyperparameter vector. Generating the search cost constraint may be based on a set of vector embeddings, the set of vector embedding quantizations, and the quantization index. Atblock 510, a tradeoff objective function is generated at the server device. The tradeoff objective function may be a function of a Lagrange multiplier (e.g., λ) and the hyperparameter vector. Generating the tradeoff objective function may be based on the recall loss function and the search cost constraint. Atblock 512, a value of the Lagrange multiplier is determined at the server device. Determining the value of those around multiplier maybe based on a value of a search cost and recall tradeoff. Atblock 514, values of components of the optimized hyperparameter vector may be determined at the server device. Determining the values of the components of the optimized hyperparameter vector maybe based on the tradeoff objective function and the determined value of the Lagrange multiplier. -
FIG. 6 depicts a flowchart diagram of an example method 600 for tuning a tunable search engine according to example embodiments of the present disclosure. Method 600 begins at block 602, where a recall loss function (e.g., see equations (1)-(5)) is generated at a computing device (e.g., server device 104 ofFIG. 1 ). The recall loss function may indicate an error metric (e.g., false negatives or Type II errors) for a search engine (e.g., tunable search engine 124 ofFIG. 1 ). The recall loss function may be based on a tuning vector (e.g., a hyperparameter vector) and a set of approximate nearest neighbor (ANN) search results. The set of ANN search results may be based on a multi-level set of quantizations (e.g., set of vector embedding quantizations 120 ofFIG. 1 ) of a set of vector embeddings (e.g., set of vector embeddings 114 ofFIG. 1 ) for a set of data objects (e.g., set of data objects 106 ofFIG. 1 ). Ay block 604, a tradeoff objective function may be generated at the computing device. The tradeoff objective function may indicate a tradeoff between the error metric and a search cost (e.g., J(t)) for the search engine. The tradeoff objective function is based on the recall loss function (e.g.,block 606, a value for the Lagrange multiplier is determined at the computing device. Determining the value for the Lagrange multiplier may be based on a value for the tradeoff between the error metric and the search cost for the search engine. Atblock 608, values for components of the tuning vector are determined at the computing device. Determining the values for the components of the tuning vector may be based on the value for the Lagrange multiplier and the tradeoff objective function. Atblock 610, the search engine is configured at the computing device. Configuring the search engine may be based on the values for the components of the tuning vector. - In various embodiments,
method 600 includes determining, at the computing device, a set of ground-truth search results. Determining the set of ground-truth search results may be based on a set of tuning queries and the set of vector embeddings for the set of data objects. The recall loss function may be generated at the computing device further based on the set of ground-truth search results. In some embodiments, the set of ANN search results are determined further based on a set of tuning queries. - In some embodiments,
method 600 further includes receiving, at the computing device, a search query. The search query may be received from a client of the configured search engine. A set of search results may be determined, at the computing device, based on the search query and the configured search engine. The set of search results may be provided to the client of the configured search engine. - In various embodiments,
method 600 may further include generating, at the computing device, a set of vector embeddings for the set of data objects. The multi-level set of quantizations for the set of vector embeddings may be generated based on the set of vector embeddings for the set of data objects. The multi-level set of quantizations for the set of vector embeddings may include a set of vector quantization (VQ) layers and a set of product quantization (PQ) layers. - In various embodiments,
method 600 may further include generating, at the computing device, the set of vector embeddings for the set of objects. A set of ground-truth search results may be generated, at the computing device, based on a set of tuning queries and the set of vector embeddings for the set of objects. The set of ANN search results may be generated further based on the set of tuning queries and the multi-level set of quantization of the set of vector embeddings. The recall loss function may be generated, at the computing device, based on the set of ground-truth search results and the set of ANN search results. - In at least one embodiment,
method 600 includes generating, at the computing device, the search cost constraint based on a set of vector embeddings and a quantization index corresponding to the multi-level set of quantizations of the set of vector embeddings for the set of data objects. The tradeoff objective function may be indicative of a speed-recall Pareto frontier for the tradeoff between the error metric and the search cost for the search engine. The search cost for the search engine may be based on the multi-level set of quantizations of a set of vector embeddings for a set of data objects. -
FIG. 7 depicts a flowchart diagram of anexample method 700 for configuring a search engine according to example embodiments of the present disclosure.Method 700 begins atblock 702, where a set of optimized tuning vectors is determined. Each optimized tuning vector of the set of optimized tuning vectors is optimized for a search cost and recall tradeoff of a set of search cost and recall tradeoffs. Atblock 704, a look up table (LUT) is generated. The LUT indicates (or encodes) corresponding optimized tuning vectors and search cost and recall tradeoffs. Atblock 706, an indication of a selected value of search cost and recall tradeoff is received. Atblock 708, an optimized tuning vector corresponding to the selected value of the search cost and recall tradeoff is accessed the the LUT. Atblock 710, the tunable search engine is configured in accordance to the optimized tuning vector corresponding to the selected value of search cost and recall trade off.
Claims (20)
1. A computer-implemented method for operating a search engine, the method comprising:
generating, at a computing device, a recall loss function indicating an error metric for the search engine based on a tuning vector and a set of approximate nearest neighbor (ANN) search results that is based on a multi-level set of quantizations of a set of vector embeddings for a set of data objects;
generating, at the computing device, a tradeoff objective function indicating a tradeoff between the error metric and a search cost for the search engine, wherein the tradeoff objective function is based on the recall loss function, a search cost constraint, the tuning vector, and a Lagrange multiplier;
determining, at the computing device, a value for the Lagrange multiplier based on a value for the tradeoff between the error metric and the search cost for the search engine;
determining, at the computing device, values for components of the tuning vector based on the value for the Lagrange multiplier and the tradeoff objective function; and
configuring, at the computing device, the search engine based on the values for the components of the tuning vector.
2. The method of claim 1 , further comprising:
determining, at the computing device, a set of ground-truth search results based on a set of tuning queries and the set of vector embeddings for the set of data objects;
and generating, at the computing device, the recall loss function further based on the set of ground-truth search results.
3. The method of claim 1 , further comprising:
determining, at the computing device, the set of ANN search results are further based on a set of tuning queries.
4. The method of claim 1 , further comprising:
receiving, at the computing device, a search query, wherein the search query is received from a client of the configured search engine;
determining, at the computing device, a set of search results based on the search query and the configured search engine; and
providing the set of search results to the client of the configured search engine.
5. The method of claim 1 , further comprising:
generating, at the computing device, a set of vector embeddings for the set of data objects; and
generating, at the computing device, the multi-level set of quantizations for the set of vector embeddings based on the set of vector embeddings for the set of data objects.
6. The method of claim 1 , wherein the multi-level set of quantizations for the set of vector embeddings includes a set of vector quantization (VQ) layers and a set of product quantization (PQ) layers.
7. The method of claim 1 , further comprising:
generating, at the computing device, the set of vector embeddings for the set of objects;
generating, at the computing device, a set of ground-truth search results based on a set of tuning queries and the set of vector embeddings for the set of objects;
generating, at the computing device, the set of ANN search results further based on the set of tuning queries and the multi-level set of quantization of the set of vector embeddings; and
generating, at the computing device, the recall loss function based on the set of ground-truth search results and the set of ANN search results.
8. The method of claim 1 , further comprising:
generating, at the computing device, the search cost constraint based on a set of vector embeddings and a quantization index corresponding to the multi-level set of quantizations of the set of vector embeddings for the set of data objects.
9. The method of claim 1 , wherein the tradeoff objective function is indicative of a speed-recall Pareto frontier for the tradeoff between the error metric and the search cost for the search engine.
10. The method of claim 1 , wherein the search cost for the search engine is based on the multi-level set of quantizations of a set of vector embeddings for a set of data objects.
11. A computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media that store instructions that when executed by the one or more processors, cause the computing system to perform operations comprising:
generating a recall loss function indicating an error metric for the search engine based on a tuning vector and a set of approximate nearest neighbor (ANN) search results that is based on a multi-level set of quantizations of a set of vector embeddings for a set of data objects;
generating a tradeoff objective function indicating a tradeoff between the error metric and a search cost for the search engine, wherein the tradeoff objective function is based on the recall loss function, a search cost constraint, the tuning vector, and a Lagrange multiplier;
determining a value for the Lagrange multiplier based on a value for the tradeoff between the error metric and the search cost for the search engine;
determining values for components of the tuning vector based on the value for the Lagrange multiplier and the tradeoff objective function; and
configuring the search engine based on the values for the components of the tuning vector.
12. The system of claim 11 , the operations further comprising:
determining a set of ground-truth search results based on a set of tuning queries and the set of vector embeddings for the set of data objects; and
generating the recall loss function further based on the set of ground-truth search results.
13. The system of claim 11 , the operations further comprising:
determining the set of ANN search results that are further based on a set of tuning queries.
14. The system of claim 11 , the operations further comprising:
receiving a search query, wherein the search query is received from a client of the configured search engine;
determining a set of search results based on the search query and the configured search engine; and
providing the set of search results to the client of the configured search engine.
15. The system of claim 11 , the operations further comprising:
generating a set of vector embeddings for the set of data objects; and
generating the multi-level set of quantizations for the set of vector embeddings based on the set of vector embeddings for the set of data objects.
16. The system of claim 11 , wherein the multi-level set of quantizations for the set of vector embeddings includes a set of vector quantization (VQ) layers and a set of product quantization (PQ) layers.
17. The system of claim 11 , the operations further comprising:
generating the set of vector embeddings for the set of objects;
generating a set of ground-truth search results based on a set of tuning queries and the set of vector embeddings for the set of objects;
generating the set of ANN search results further based on the set of tuning queries and the multi-level set of quantization of the set of vector embeddings; and
generating, at the computing device, the recall loss function based on the set of ground-truth search results and the set of ANN search results.
18. The system of claim 11 , the operations further comprising:
generating the search cost constraint based on a set of vector embeddings and a quantization index corresponding to the multi-level set of quantizations of the set of vector embeddings for the set of data objects.
19. The system of claim 11 , further comprising:
generating, at the computing device, the search cost constraint based on a set of vector embeddings and a quantization index corresponding to the multi-level set of quantizations of the set of vector embeddings for the set of data objects.
20. One or more non-transitory computer-readable media that store instructions, wherein when the instructions are executed by a processor device, cause the processor device to perform operations comprising:
determining, by the computing system, a plurality of feature vectors respectively associated with a plurality of machine-learned models, wherein the feature vector for each machine-learned model describes a respective performance of the machine-learned model on each of a plurality of loss function components;
determining, by the computing system, a plurality of validation errors respectively for the plurality of machine-learned models, wherein the validation error for each machine-learned model describes a performance of the machine-learned model relative to a validation metric; and
for each of one or more optimization iterations respectively associated with one or more of the machine-learned models:
attempting to optimize, by the computing system, a cost function to learn the vector of variable hyperparameter values subject to a constraint, wherein the cost function evaluates a sum, for all of the machine-learned models, of an absolute or squared error between a respective loss function for each machine-learned model and the validation error for such machine-learned model, wherein the respective loss function for each machine-learned model comprises the feature vector for the machine-learned model respectively multiplied by the vector of variable hyperparameter values, and wherein the constraint requires that the vector of variable hyperparameter values be such that minimization of the respective loss for each machine-learned model returns a current machine-learned model associated with a current optimization iteration; and
if the cost function is successfully optimized subject to the constraint, providing the vector of variable hyperparameter values as an output.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/474,907 US20240119052A1 (en) | 2022-09-27 | 2023-09-26 | Tuning Approximate Nearest Neighbor Search Engines for Speed-Recall Tradeoffs Via Lagrange Multiplier Methods |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202263410536P | 2022-09-27 | 2022-09-27 | |
US18/474,907 US20240119052A1 (en) | 2022-09-27 | 2023-09-26 | Tuning Approximate Nearest Neighbor Search Engines for Speed-Recall Tradeoffs Via Lagrange Multiplier Methods |
Publications (1)
Publication Number | Publication Date |
---|---|
US20240119052A1 true US20240119052A1 (en) | 2024-04-11 |
Family
ID=90574220
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US18/474,907 Pending US20240119052A1 (en) | 2022-09-27 | 2023-09-26 | Tuning Approximate Nearest Neighbor Search Engines for Speed-Recall Tradeoffs Via Lagrange Multiplier Methods |
Country Status (1)
Country | Link |
---|---|
US (1) | US20240119052A1 (en) |
-
2023
- 2023-09-26 US US18/474,907 patent/US20240119052A1/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Guo et al. | Accelerating large-scale inference with anisotropic vector quantization | |
Wang et al. | Rank-one matrix pursuit for matrix completion | |
US7945576B2 (en) | Location recognition using informative feature vocabulary trees | |
Cheng et al. | SPALS: Fast alternating least squares via implicit leverage scores sampling | |
US9311403B1 (en) | Hashing techniques for data set similarity determination | |
US20130238681A1 (en) | Signature representation of data with aliasing across synonyms | |
US20240061889A1 (en) | Systems and Methods for Weighted Quantization | |
JP2020515986A (en) | Coding method based on mixture of vector quantization and nearest neighbor search (NNS) method using the same | |
Hamoudi et al. | Quantum and classical algorithms for approximate submodular function minimization | |
Park et al. | VeST: Very sparse tucker factorization of large-scale tensors | |
Kratsios et al. | Small transformers compute universal metric embeddings | |
Sun et al. | Automating nearest neighbor search configuration with constrained optimization | |
US11531695B2 (en) | Multiscale quantization for fast similarity search | |
Egiazarian et al. | Extreme Compression of Large Language Models via Additive Quantization | |
US20240119052A1 (en) | Tuning Approximate Nearest Neighbor Search Engines for Speed-Recall Tradeoffs Via Lagrange Multiplier Methods | |
Li et al. | Online variable coding length product quantization for fast nearest neighbor search in mobile retrieval | |
Zhao et al. | On entropy-constrained vector quantization using Gaussian mixture models | |
Qiu et al. | Efficient document retrieval by end-to-end refining and quantizing BERT embedding with contrastive product quantization | |
Zhang et al. | Query-aware quantization for maximum inner product search | |
US11763136B2 (en) | Neural hashing for similarity search | |
Zhou et al. | Compressed sensing in the presence of speckle noise | |
Sandhawalia et al. | Searching with expectations | |
Chen et al. | Hierarchical quantization for billion-scale similarity retrieval on gpus | |
CN112580805A (en) | Method and device for quantizing neural network model | |
Wu et al. | Local orthogonal decomposition for maximum inner product search |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SUN, PHILIP WENJIE;GUO, RUIQI;KUMAR, SANJIV;REEL/FRAME:065046/0128Effective date: 20220926 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |