JP6929644B2 - Systems and methods for gaze media selection and editing - Google Patents
Systems and methods for gaze media selection and editing Download PDFInfo
- Publication number
- JP6929644B2 JP6929644B2 JP2016544612A JP2016544612A JP6929644B2 JP 6929644 B2 JP6929644 B2 JP 6929644B2 JP 2016544612 A JP2016544612 A JP 2016544612A JP 2016544612 A JP2016544612 A JP 2016544612A JP 6929644 B2 JP6929644 B2 JP 6929644B2
- Authority
- JP
- Japan
- Prior art keywords
- user
- image
- eye
- camera
- display
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims description 84
- 238000011093 media selection Methods 0.000 title 1
- 230000004424 eye movement Effects 0.000 claims description 16
- 208000003443 Unconsciousness Diseases 0.000 claims description 5
- 230000003287 optical effect Effects 0.000 claims description 4
- 230000003213 activating effect Effects 0.000 claims 1
- 210000001747 pupil Anatomy 0.000 description 50
- 230000008569 process Effects 0.000 description 40
- 239000000835 fiber Substances 0.000 description 38
- 230000033001 locomotion Effects 0.000 description 38
- 238000012544 monitoring process Methods 0.000 description 24
- 210000000744 eyelid Anatomy 0.000 description 23
- 210000003128 head Anatomy 0.000 description 18
- 238000012545 processing Methods 0.000 description 18
- 238000003384 imaging method Methods 0.000 description 17
- 238000012360 testing method Methods 0.000 description 13
- 238000005516 engineering process Methods 0.000 description 10
- 230000000007 visual effect Effects 0.000 description 10
- 230000000694 effects Effects 0.000 description 9
- 125000001475 halogen functional group Chemical group 0.000 description 9
- 230000004397 blinking Effects 0.000 description 8
- 238000001514 detection method Methods 0.000 description 8
- 230000006870 function Effects 0.000 description 8
- 238000005259 measurement Methods 0.000 description 8
- 230000004044 response Effects 0.000 description 7
- 230000009471 action Effects 0.000 description 6
- 238000004458 analytical method Methods 0.000 description 6
- 230000006399 behavior Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 6
- 230000007246 mechanism Effects 0.000 description 6
- 230000004439 pupillary reactions Effects 0.000 description 6
- 230000001953 sensory effect Effects 0.000 description 6
- 208000032140 Sleepiness Diseases 0.000 description 5
- 206010041349 Somnolence Diseases 0.000 description 5
- 230000008859 change Effects 0.000 description 5
- 238000000537 electroencephalography Methods 0.000 description 5
- 230000004438 eyesight Effects 0.000 description 5
- 230000003190 augmentative effect Effects 0.000 description 4
- 238000006243 chemical reaction Methods 0.000 description 4
- 230000006872 improvement Effects 0.000 description 4
- 238000004091 panning Methods 0.000 description 4
- 230000035484 reaction time Effects 0.000 description 4
- 230000006641 stabilisation Effects 0.000 description 4
- 238000011105 stabilization Methods 0.000 description 4
- 230000001360 synchronised effect Effects 0.000 description 4
- 241000699666 Mus <mouse, genus> Species 0.000 description 3
- 230000036626 alertness Effects 0.000 description 3
- 230000015556 catabolic process Effects 0.000 description 3
- 238000006731 degradation reaction Methods 0.000 description 3
- 230000002996 emotional effect Effects 0.000 description 3
- 230000010354 integration Effects 0.000 description 3
- 230000001012 protector Effects 0.000 description 3
- 230000010344 pupil dilation Effects 0.000 description 3
- 210000001525 retina Anatomy 0.000 description 3
- 230000002123 temporal effect Effects 0.000 description 3
- 238000012549 training Methods 0.000 description 3
- LFQSCWFLJHTTHZ-UHFFFAOYSA-N Ethanol Chemical compound CCO LFQSCWFLJHTTHZ-UHFFFAOYSA-N 0.000 description 2
- 230000004913 activation Effects 0.000 description 2
- 238000003491 array Methods 0.000 description 2
- QVGXLLKOCUKJST-UHFFFAOYSA-N atomic oxygen Chemical compound [O] QVGXLLKOCUKJST-UHFFFAOYSA-N 0.000 description 2
- 230000036765 blood level Effects 0.000 description 2
- 239000002131 composite material Substances 0.000 description 2
- 230000001010 compromised effect Effects 0.000 description 2
- 230000008602 contraction Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- IWEDIXLBFLAXBO-UHFFFAOYSA-N dicamba Chemical compound COC1=C(Cl)C=CC(Cl)=C1C(O)=O IWEDIXLBFLAXBO-UHFFFAOYSA-N 0.000 description 2
- 230000010339 dilation Effects 0.000 description 2
- 239000003814 drug Substances 0.000 description 2
- 229940079593 drug Drugs 0.000 description 2
- 230000008451 emotion Effects 0.000 description 2
- 238000005265 energy consumption Methods 0.000 description 2
- 238000004880 explosion Methods 0.000 description 2
- 208000013057 hereditary mucoepithelial dysplasia Diseases 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012806 monitoring device Methods 0.000 description 2
- 230000004297 night vision Effects 0.000 description 2
- 239000013307 optical fiber Substances 0.000 description 2
- 239000001301 oxygen Substances 0.000 description 2
- 229910052760 oxygen Inorganic materials 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000001681 protective effect Effects 0.000 description 2
- 230000035945 sensitivity Effects 0.000 description 2
- 238000004088 simulation Methods 0.000 description 2
- 235000014214 soft drink Nutrition 0.000 description 2
- 230000000638 stimulation Effects 0.000 description 2
- 230000000153 supplemental effect Effects 0.000 description 2
- 238000001356 surgical procedure Methods 0.000 description 2
- 230000009182 swimming Effects 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 208000007204 Brain death Diseases 0.000 description 1
- 206010012335 Dependence Diseases 0.000 description 1
- UFHFLCQGNIYNRP-UHFFFAOYSA-N Hydrogen Chemical compound [H][H] UFHFLCQGNIYNRP-UHFFFAOYSA-N 0.000 description 1
- 208000013016 Hypoglycemia Diseases 0.000 description 1
- 206010021143 Hypoxia Diseases 0.000 description 1
- 206010061216 Infarction Diseases 0.000 description 1
- WHXSMMKQMYFTQS-UHFFFAOYSA-N Lithium Chemical compound [Li] WHXSMMKQMYFTQS-UHFFFAOYSA-N 0.000 description 1
- 241000699670 Mus sp. Species 0.000 description 1
- 241000112598 Pseudoblennius percoides Species 0.000 description 1
- 241001362551 Samba Species 0.000 description 1
- 229910000831 Steel Inorganic materials 0.000 description 1
- 208000006011 Stroke Diseases 0.000 description 1
- 241000385223 Villosa iris Species 0.000 description 1
- 238000009825 accumulation Methods 0.000 description 1
- 239000008186 active pharmaceutical agent Substances 0.000 description 1
- 230000001154 acute effect Effects 0.000 description 1
- 239000000853 adhesive Substances 0.000 description 1
- 230000001070 adhesive effect Effects 0.000 description 1
- 238000004378 air conditioning Methods 0.000 description 1
- 230000037007 arousal Effects 0.000 description 1
- 230000003542 behavioural effect Effects 0.000 description 1
- 238000005452 bending Methods 0.000 description 1
- 230000000903 blocking effect Effects 0.000 description 1
- 230000006931 brain damage Effects 0.000 description 1
- 231100000874 brain damage Toxicity 0.000 description 1
- 208000029028 brain injury Diseases 0.000 description 1
- 229910052793 cadmium Inorganic materials 0.000 description 1
- BDOSMKKIYDKNTQ-UHFFFAOYSA-N cadmium atom Chemical compound [Cd] BDOSMKKIYDKNTQ-UHFFFAOYSA-N 0.000 description 1
- 230000009194 climbing Effects 0.000 description 1
- 235000021443 coca cola Nutrition 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 238000001816 cooling Methods 0.000 description 1
- 238000013480 data collection Methods 0.000 description 1
- 230000000916 dilatatory effect Effects 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 230000004399 eye closure Effects 0.000 description 1
- 210000000720 eyelash Anatomy 0.000 description 1
- 210000000887 face Anatomy 0.000 description 1
- 239000013305 flexible fiber Substances 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000003306 harvesting Methods 0.000 description 1
- 230000036541 health Effects 0.000 description 1
- 238000010438 heat treatment Methods 0.000 description 1
- 239000001257 hydrogen Substances 0.000 description 1
- 229910052739 hydrogen Inorganic materials 0.000 description 1
- 230000002218 hypoglycaemic effect Effects 0.000 description 1
- 208000018875 hypoxemia Diseases 0.000 description 1
- 238000007654 immersion Methods 0.000 description 1
- 230000007574 infarction Effects 0.000 description 1
- 238000011900 installation process Methods 0.000 description 1
- 229910052744 lithium Inorganic materials 0.000 description 1
- 230000013011 mating Effects 0.000 description 1
- 230000006996 mental state Effects 0.000 description 1
- 210000003205 muscle Anatomy 0.000 description 1
- 208000010125 myocardial infarction Diseases 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 238000012634 optical imaging Methods 0.000 description 1
- 235000015205 orange juice Nutrition 0.000 description 1
- 239000002420 orchard Substances 0.000 description 1
- 239000002245 particle Substances 0.000 description 1
- 230000000149 penetrating effect Effects 0.000 description 1
- 230000008447 perception Effects 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 238000004321 preservation Methods 0.000 description 1
- 238000003825 pressing Methods 0.000 description 1
- 230000001179 pupillary effect Effects 0.000 description 1
- 230000002787 reinforcement Effects 0.000 description 1
- 238000011160 research Methods 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 230000001711 saccadic effect Effects 0.000 description 1
- 230000035807 sensation Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000012732 spatial analysis Methods 0.000 description 1
- 230000000087 stabilizing effect Effects 0.000 description 1
- 239000010959 steel Substances 0.000 description 1
- 230000004936 stimulating effect Effects 0.000 description 1
- 230000002459 sustained effect Effects 0.000 description 1
- 231100000167 toxic agent Toxicity 0.000 description 1
- 239000003440 toxic substance Substances 0.000 description 1
- 239000003053 toxin Substances 0.000 description 1
- 231100000765 toxin Toxicity 0.000 description 1
- 108700012359 toxins Proteins 0.000 description 1
- 238000001429 visible spectrum Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/0093—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00 with means for monitoring data relating to the user, e.g. head-tracking, eye-tracking
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1686—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being an integrated camera
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1694—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being a single or a set of motion sensors for pointer control or gesture input obtained by sensing movements of the portable computer
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/013—Eye tracking input arrangements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04845—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range for image manipulation, e.g. dragging, rotation, expansion or change of colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/0485—Scrolling or panning
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2203/00—Indexing scheme relating to G06F3/00 - G06F3/048
- G06F2203/048—Indexing scheme relating to G06F3/048
- G06F2203/04806—Zoom, i.e. interaction techniques or interactors for controlling the zooming operation
Description
本発明は、例えば、眼、眼瞼、及び／又は人の片眼若しくは両眼のその他の構成要素の動きに基づく疲労のモニタリング、目的を持ったコミュニケーション、及び／又はデバイスの制御のために、人の眼をモニタリングするための装置、システム、及び方法全般に関する。さらに、本発明は、より具体的には、ウェアラブルデバイスで取り込まれたメディアイメージをユーザーが編集出来るようにするシステム及び方法に関する。このシステムは、取り込み時の前、取り込み時の間、又は取り込み時の後であっても、ユーザーの少なくとも片方の眼の視線追跡データを取り込んで様々な編集機能を制御するために、眼にレファレンスフレームを投射し、投射されたレファレンスフレームをディスプレイの第二レファレンスフレームと関連付ける視線追跡サブシステムを用いている。 The present invention is used, for example, for monitoring fatigue based on the movement of the eyes, eyelids, and / or other components of one or both eyes of a person, purposeful communication, and / or control of a device. General equipment, systems, and methods for monitoring the eye of the eye. Furthermore, the present invention more specifically relates to a system and a method that allows a user to edit a media image captured by a wearable device. The system provides a reference frame to the eye to capture eye tracking data from at least one eye of the user and control various editing functions, whether before, during, or after capture. It uses a line-of-sight tracking subsystem that projects and associates the projected reference frame with the second reference frame on the display.
携帯電子デバイスが急激に広まり、次第に高性能及び高機能となるに従って、それが一般的に用いられる機能は移り変わってきた。ポケットサイズのデバイスが、純粋な通信デバイスからコンテンツ消費デバイスとなる方向へ、コンテンツ作成デバイスとなる方向へ移行するに従って、ユーザーも、巨大なコンテンツクリエーターとなる方向へ移行してきた。これまでに撮影されたすべての写真のうちの１０パーセントが、２０１２年に撮影されたものであると推計されている。類似の作成率は、ビデオシーンにも当てはまる。Ｇｏ−Ｐｒｏカメラ及びＧｏｏｇｌｅ Ｇｌａｓｓなどのヘッドマウントビデオキャプチャデバイスの登場が、ユーザーの通常の視野で取り込まれるビデオを促進している。残念なことに、この供給過剰なイメージ撮影は、作り出されたコンテンツの質を上げては来なかった。特にビデオシーンの場合、目的のクリップの検査、加工、編集、及び／又は取り出しに要する時間は、記録されたビデオシーンの量に比例する。従って、取り込まれたビデオシーンの量が増加する場合、価値のあるコンテンツを抽出するのに要する時間の量は、おおよそ直線的に増加する。 With the rapid spread of portable electronic devices and their increasing performance and functionality, the functions in which they are commonly used have changed. As pocket-sized devices move from pure communication devices to content-consuming devices to content-creating devices, so do users. It is estimated that 10 percent of all photographs taken so far were taken in 2012. Similar creation rates also apply to video scenes. The advent of head-mounted video capture devices such as Go-Pro cameras and Google Glass is driving video captured in the user's normal field of view. Unfortunately, this oversupplied imaging has not improved the quality of the content produced. Especially in the case of video scenes, the time required to inspect, process, edit, and / or retrieve the desired clip is proportional to the amount of recorded video scenes. Therefore, as the amount of captured video scene increases, the amount of time required to extract valuable content increases approximately linearly.
本出願中のすべての開示事項及び請求項において、「メディアイメージ」は、ビデオイメージ及び静止イメージのうちの少なくとも１つとして定義される。 In all disclosures and claims in the present application, "media image" is defined as at least one of a video image and a still image.
いずれの種類のメディアイメージであっても、コンテンツクリエーターにとっての典型的な目標は、特定の視聴者にとって望ましいコンテンツを作成することである。「望ましい」の定義は、視聴者に応じて変化し得る。特にビデオイメージに関しては、ビデオイメージを選択し、編集するための１つの方法又は一式の基準は、ある視聴者には適切であり得るが、別の視聴者にはそうでない可能性がある。さらに、他のイメージと時間的に近接して取り込まれるイメージが、様々な理由から望ましい場合もある。望ましさ及び適切性のこのような様々な具体化は、単に「顕著性（saliency）」と称される場合がある。 For any type of media image, a typical goal for content creators is to create content that is desirable for a particular audience. The definition of "desirable" can vary depending on the viewer. Especially with respect to video images, one method or set of criteria for selecting and editing a video image may be appropriate for one viewer, but not for another. In addition, an image that is captured in close temporal proximity to other images may be desirable for a variety of reasons. Such various reifications of desirability and suitability are sometimes referred to simply as "saliency."
メディアイメージは、注目すべきイベントを含んでいる可能性がある、特定の友人若しくは親族を含んでいる可能性がある、ソーシャルメディアにおいて他者が興味深いと考える出来事を含んでいる可能性がある、特定の場所で取り込まれた可能性がある、及び／又はユーザーが取り込みたいと考える感情を含んでいる可能性がある、といったいくつもの理由から顕著と見なされ得る。視線追跡を他のセンサーに追加することにより、このプロセスの過程にて、視線追跡の出現なしでは利用できなかったであろうレベルの分析及び制御がユーザーにとって可能となるものと想定される。 The media image may contain notable events, may contain certain friends or relatives, may contain events that others find interesting on social media, It can be considered prominent for a number of reasons, such as it may have been captured in a particular location and / or it may contain emotions that the user wants to capture. By adding gaze tracking to other sensors, it is expected that in the course of this process, users will be able to analyze and control levels that would not have been available without the advent of gaze tracking.
「編集」の語によって意図する範囲を考察する際には、注意深い考慮が必要とされる。典型的なフォト及びビデオアプリケーションの場合、「編集」は、典型的には、イメージの操作を意味し、又はビデオの場合は、トリミングしたイメージをより望ましい順番に再配列するプロセスも含む。さらなる工程が実施されることになるイメージの選択若しくはタグ付けの工程は、このような工程は、正式には編集プロセスの一部と見なされるべきではあるが、「編集」に含まれない場合が多い。しかし、本出願内の開示事項及び請求項の目的のために、「編集」は、選択及びタグ付けの工程を含むものとする。さらに、デジタルメディア作成以前の時代には、すべての編集（選択及びタグ付けを含む）は、取り込みの時点よりもかなり後に行われる必要があった。しかし、現在、ビデオ及びスチールカメラには、取り込み時点の直後に、又は「カメラ内で（in-camera）」編集プロセスを行うことができる機能が含まれている。本明細書での開示事項では、編集プロセスを、取り込みの最中、又はさらには取り込みの前の時点を含むようにシフトさせることができる方法について述べる。しかし、本明細書で述べるシステム及び方法が実行されるまで、それを行うことは実用的に実現可能ではなかった。 Careful consideration is required when considering the intended range by the word "edit". For typical photo and video applications, "editing" typically means manipulating the image, or for video, also includes the process of rearranging the cropped images in a more desirable order. The process of image selection or tagging for which additional steps will be performed, such steps should be formally considered part of the editing process, but may not be included in "editing". many. However, for the purposes of the disclosures and claims within this application, "editing" shall include the steps of selection and tagging. Moreover, in the pre-digital media era, all editing (including selection and tagging) had to be done well after the point of capture. However, video and still cameras now include the ability to perform editing processes "in-camera" either immediately after capture. Disclosures herein describe how the editing process can be shifted to include during or even prior to capture. However, it has not been practically feasible to do so until the systems and methods described herein have been implemented.
残念なことに、多くのユーザーにとって、取り込んだままの状態のビデオイメージを消費可能に仕上げられたビデオに変換するのに要する時間的な拘束は、このプロセスに対する最終的な障害である。この障害に直面した後の一般的な結果は、２通り存在する。第一は、プロセス全体が放棄され、ビデオイメージが視聴者と共有されることがなくなってしまうことである。第二の一般的な結果は、すべての編集が避けられ、質及び妥当性が極めて低いイメージが視聴者と共有されることである。これらの結果はいずれも、クリエーター及び視聴者の両方にとって望ましいものではない。クリエーターにとっては、このことにより、ビデオを紹介可能な形態に編集することが難しすぎることが分かることで、ビデオを記録しようとする自身の意欲が減退し得る。消費者にとっては、粗悪なビデオイメージを視聴することで、負の強化（negative reinforcement）が与えられ、将来的にビデオイメージを見たいと思わなくなり得る。 Unfortunately, for many users, the time constraint required to convert an as-captured video image into a consumable finished video is the ultimate obstacle to this process. There are two general consequences after facing this obstacle. The first is that the entire process is abandoned and the video image is no longer shared with the viewer. The second general result is that all editing is avoided and images of very low quality and relevance are shared with the viewer. None of these results are desirable for both creators and viewers. For creators, this can discourage them from recording video by finding it too difficult to edit the video into an introductory form. For consumers, watching a bad video image gives them negative reinforcement and may make them unwilling to watch the video image in the future.
技術が進歩するに従って、コンテンツを作成するためのユーザーが携帯することのできるデバイスのフォームファクタも、移り変わってきた。かつては、コンテンツ作成デバイスには、他の技術は無かった。やがてスマートフォン及びタブレットでビデオの取り込みが可能となり、以前には想像もできなかった小型軽量化の時代を迎えた。現在、ヘッドマウントディスプレイが、消費者向けデバイスとして実現可能となり始めており、センサーなどからデータを単にログ記録するのではなく、コンテンツの作成を可能とするウェアラブル技術における移り変わりを特徴付けている。さらに、コンタクトレンズ及び人工網膜は、人の視覚系にとっての実行可能な向上である。本明細書のシステム及び方法は、ビデオの取り込み、視線追跡、及び顕著性を持つビデオの編集のこのようなモードにも適用可能であり、本発明の一部と見なされる。視線追跡を通してユーザーの注視を特定するための必要不可欠な技術を、ここでウェアラブルデバイス及びインプラトデバイスへ組込むことができることにより、眼は、デバイス入力及び編集のための実現可能なツールとなる。 As technology has advanced, so has the form factor of the devices that users can carry to create content. In the past, content creation devices had no other technology. Eventually, it became possible to capture video on smartphones and tablets, and we entered an era of smaller size and lighter weight that was unimaginable before. Currently, head-mounted displays are beginning to become feasible as consumer devices, and characterize the transition in wearable technology that enables the creation of content rather than simply logging data from sensors and the like. In addition, contact lenses and artificial retinas are feasible improvements to the human visual system. The systems and methods herein are also applicable to such modes of video capture, eye tracking, and video editing with prominence and are considered part of the present invention. The ability to incorporate essential techniques for identifying a user's gaze through gaze tracking into wearable and implantable devices here makes the eye a viable tool for device input and editing.
本発明の最良の理解は、以下に提示される明細書及び請求項を充分に読むことから得られるが、この概要は、本出願で述べるシステム及び方法の新しく有用な特徴の一部を読者に知らせる目的で提供される。当然、この概要は、本発明のシステム及び方法の特徴のすべてを完全に列挙することを意図するものではなく、また、いかなる形であっても、本出願の詳細な記述の最後に提示される請求項の範囲を限定することを意図するものでもない。 The best understanding of the invention comes from reading the specification and claims presented below, but this overview will give the reader some of the new and useful features of the systems and methods described in this application. Provided for the purpose of informing. Of course, this summary is not intended to fully list all the features of the systems and methods of the invention, and in any form is presented at the end of the detailed description of the application. It is not intended to limit the scope of the claims.
本発明は、ウェアラブルデバイスに実装可能であるシステム及び方法を提供する。このシステムは、ウェアラブルデバイスを用いて取り込まれたメディアイメージをユーザーが編集可能となるように設計される。このシステムは、取り込み時の前、取り込み時の最中、又は取り込み時の後であっても、視線追跡データを用いて様々な編集機能を制御することができる。また、メディアイメージのどのセクション又は領域が、ユーザー又は視聴者にとってより興味のあるものであり得るかを決定するための方法も提示される。この方法は、視線追跡データを用いて、取り込まれたメディアに顕著性を割り当てることができる。これらのシステム及び方法のいずれにおいても、操作を向上する目的で、視線追跡データが追加のセンサーからのデータと組み合わされてよい。 The present invention provides systems and methods that can be implemented in wearable devices. The system is designed to allow users to edit media images captured using wearable devices. The system can use the line-of-sight tracking data to control various editing functions before, during, or after capture. Also presented is a method for determining which section or area of the media image may be of more interest to the user or viewer. This method can use eye tracking data to assign saliency to captured media. In any of these systems and methods, gaze tracking data may be combined with data from additional sensors for the purpose of improving operation.
上記を考慮して、本出願は、ウェアラブルデバイス、ユーザーの周囲のメディアイメージを取り込むようにそのデバイスに搭載されたシーンカメラ、ユーザーの少なくとも片方の眼の視線追跡データを取り込むための、眼にレファレンスフレームを投射し、投射されたレファレンスフレームをディスプレイの第二レファレンスフレームと関連付ける視線追跡サブシステム、及び視線追跡データに少なくとも部分的に基づいてシーカメラによって取り込まれたメディアイメージにタグ付けするための、シーンカメラ及び視線追跡サブシステムと通信する１つ以上のプロセッサーを含む、メディアイメージを編集するための装置、システム、並びに方法について述べる。 In view of the above, this application applies to a wearable device, a scene camera mounted on the device to capture a media image of the user's surroundings, and an eye reference to capture the line-of-sight tracking data of at least one eye of the user. A line-of-sight tracking subsystem that projects a frame and associates the projected reference frame with a second reference frame on the display, and for tagging media images captured by the sea camera based at least in part on the line-of-sight tracking data. A device, system, and method for editing a media image, including one or more processors that communicate with a scene camera and a line-of-sight tracking subsystem, are described.
別の実施形態では、この装置、システム、及び方法は、異なる観点から妥当なイベントを記録する目的で、近位に位置するウェアラブルデバイスによって決定されるビデオイメージ中の相対的顕著性を定量的に評価することができるものであり、個々のユーザーによって装着されるように構成され、各ウェアラブルデバイスが、個々のユーザーの周囲のメディアイメージを取り込むようにその上に搭載されたシーンカメラ、１つ以上のセンサー、及び通信インターフェイスを含む複数のウェアラブルデバイス；各ウェアラブルデバイスの通信インターフェイスを介してウェアラブルデバイスと通信するためのサーバーを含む。 In another embodiment, the device, system, and method quantitatively determines the relative prominence in a video image determined by a wearable device located proximally for the purpose of recording reasonable events from different perspectives. One or more scene cameras that can be evaluated, configured to be worn by individual users, and each wearable device mounted on top of it to capture a media image of the individual user's surroundings. Sensors, and multiple wearable devices including a communication interface; including a server for communicating with the wearable device through the communication interface of each wearable device.
さらに別の実施形態では、ウェアラブルデバイス上のシーンカメラを用いてユーザーの周囲のメディアイメージを取り込むこと；ウェアラブルデバイス上の視線追跡サブシステムを用いてユーザーの少なくとも片方の眼の視線追跡データを取り込むこと；並びに視線追跡データから識別された少なくとも１つの眼の事象の動作に少なくとも部分的に基づいてメディアイメージを選択すること及び編集することのうちの少なくとも一方を含む、ユーザーが装着したウェアラブルデバイスからのメディアイメージを選択又は編集するための方法が提供される。 In yet another embodiment, a scene camera on the wearable device is used to capture a media image of the user's surroundings; a line-of-sight tracking subsystem on the wearable device is used to capture the line-of-sight tracking data of at least one eye of the user. And from a wearable device worn by the user, including at least one of selecting and editing a media image based at least in part on the behavior of at least one eye event identified from the gaze tracking data. A method for selecting or editing a media image is provided.
ここで提示される本発明の態様及び適用を、図面及び本発明の詳細な記述において以下で述べる。特に断りのない限り、明細書及び請求項における語及び句には、適用可能な技術分野の当業者にとって一般的かつ通常の慣習的な意味が与えられることを意図している。本発明者らは、所望される場合、本発明者ら自身の辞書編集者であり得ることは充分に認識している。本発明者らは、明確にそれ以外が述べられていない場合、自身の辞書編集者として、明細書及び請求項の用語の一般的かつ通常の意味のみを用いることを明白に選択し、そしてさらに、その用語の「特別の」定義を明白に示し、それがその一般的かつ通常の意味からいかに異なるかを説明する。「特別の」定義を適用する意図のそのような明確な記載が存在しない場合、その用語に対する単純な一般的かつ通常の意味が明細書及び請求項の解釈に適用されるということが、発明者らの意図であり、所望するところである。 The embodiments and applications of the invention presented herein will be described below in the drawings and in the detailed description of the invention. Unless otherwise stated, the terms and phrases in the specification and claims are intended to give common and ordinary customary meaning to those skilled in the art of applicable technology. We are fully aware that we can be our own lexicographer if desired. As their lexicographers, we explicitly choose to use only the general and ordinary meanings of the terms in the specification and claims, unless otherwise explicitly stated, and further. , Clarify the "special" definition of the term and explain how it differs from its general and ordinary meaning. In the absence of such a clear statement of intent to apply the "special" definition, it is the inventor that a simple general and ordinary meaning to the term applies to the interpretation of the specification and claims. These are the intentions and desires.
本発明のより完全な理解は、以下の説明のための図面と合わせて考慮される場合、詳細な記述を参照することで得られるであろう。図面において、同じ符号は、図面全体を通して同じ要素又は作用を意味する。本発明の本代表的実施形態が、添付の図面で説明される。 A more complete understanding of the invention will be obtained by reference to the detailed description when considered in conjunction with the drawings for the following description. In a drawing, the same reference numerals mean the same element or action throughout the drawing. Representative embodiments of the present invention will be described in the accompanying drawings.
図面を参照すると、図１は、ベッド１２で患者１０の眼及び／又は眼瞼の動きを検出するための検出デバイス３０を装着している患者１０を示す。検出デバイス３０は、本明細書で述べるいずれのバイオセンサーデバイスを含んでもよく、それは、例えば目的を持ったコミュニケーションのための意識的な眼の動きのモニタリング、例えば傾眠若しくはその他の状態における無意識の眼の動きのモニタリング、及び／又は１つ以上の電子デバイス（図示せず）の制御のために用いられてよい。検出デバイス３０は、検出された眼及び／又は眼瞼の動きを、例えばビデオディスプレイ５０を用いて医療提供者４０と通信されてよいデータストリーム、理解可能なメッセージ、及び／又はその他の情報へ変換する処理ボックス１３０と連結されていてよい。
With reference to the drawings, FIG. 1 shows a patient 10 wearing a
図２を参照すると、装置又はシステム１４の代表的実施形態が示され、それは、従来の眼鏡２０に取り付け可能であり、目標に向けることができ、焦点を合わせることができる検出デバイス３０を含む。眼鏡２０は、レンズ２１の間に延びるブリッジ２４を含むフレーム２２に取り付けられた１対のレンズ２１、及びモダン２６を有する側枠部（side members）又はテンプル２５を含み、これらはすべて従来のものである。別の選択肢として、レンズ２１が必要ではない場合もあることから、フレーム２２は、レンズ２１無しで提供されてもよい。
With reference to FIG. 2, a representative embodiment of the device or system 14 is shown, which includes a
検出デバイス３０は、側枠部２５のうちの一方に取り付けるためのクランプ又はその他の機構２７、並びに１つ以上の発光器３２及びセンサー３３（１つを図示）が搭載される調節可能アーム３１を含む。発光器３２及びセンサー３３は、発光器３２が眼鏡２０を装着した人の眼３００に向かってシグナルを発することができるように所定の関係で搭載され、センサー３３は、眼３００及び眼瞼３０２の表面から反射されたシグナルを検出することができる。別の選択肢として、発光器３２及びセンサー３３は、互いに近接して搭載されてもよい。
The
１つの実施形態では、発光器３２及びセンサー３３は、例えば装着者の通常の視覚に対する混乱又は妨害を最小限に抑えるために赤外範囲内である連続光又はパルス光をそれぞれ発生及び検出する。発光器３２は、所定の周波数のパルスとして光を発してよく、センサー３３は、その所定の周波数での光パルスを検出するように構成される。このパルスによる作動は、発光器３２によるエネルギー消費を削減することができ、及び／又はその他の光源への干渉を最小限に抑えることができる。 In one embodiment, the light emitter 32 and the sensor 33 generate and detect, respectively, continuous or pulsed light in the infrared range to minimize confusion or interference with the wearer's normal vision, for example. The light emitter 32 may emit light as a pulse of a predetermined frequency, and the sensor 33 is configured to detect the light pulse at that predetermined frequency. This pulsed operation can reduce the energy consumption of the light emitter 32 and / or minimize interference with other light sources.
別の選択肢として、紫外光などの可視スペクトルを超えた、又は可視スペクトル内である光のその他の所定の周波数範囲、又は電波、音波などのエネルギーのその他の形態が用いられてもよい。 Alternatively, other predetermined frequency ranges of light beyond or within the visible spectrum, such as ultraviolet light, or other forms of energy, such as radio waves, sound waves, may be used.
処理ボックス１３０は、中に１つ以上のワイヤ（図示せず）が含まれるケーブル３４によって検出デバイス３０と連結される。処理ボックス１３０は、中央演算処理装置（ＣＰＵ）及び／又はその他の回路を含んでよい。処理ボックス１３０は、発光器３２及び／又はセンサー３３を制御するための制御回路を含んでもよく、又はＣＰＵが、内部制御回路を含んでよい。
The
例えば、１つの実施形態では、制御回路は、発光器３２を制御して、１秒間あたり少なくとも約５〜２０パルスを例とする１秒間あたり何千パルスという高さから１秒間あたり約４〜５パルスという低さまでの所定の周波数でパルス化された明滅する赤外シグナルを発生させてよく、それによって、１回あたり約２００ミリ秒という短さの目的を持たない又は目的を持った瞬きの検出が容易となる。センサー３３は、発光器３２の明滅周波数に固有の所定の周波数の光パルスのみを検出するように制御されてよい。このようにして、発光器３２及びセンサー３３を所定の周波数に同期させることにより、システム１０は、例えば、明るい太陽光、完全な暗闇、周囲赤外光のバックグラウンド、又は異なる明滅周波数で作動しているその他の発光器によって出力シグナルが実質的に影響を受けることなく、様々な周囲条件下で用いることができる。明滅周波数を調節することで、電力消費を最小限に抑えると同時に、単位時間あたりの瞬き回数（例：１分間あたり約１０から約２０回の瞬き）、各瞬きの継続時間（例：約２００ミリ秒から約３００ミリ秒）、及び／若しくはＰＥＲＣＬＯＳ（すなわち、眼瞼が完全に若しくは部分的に閉じられる時間のパーセント）の測定効率を最大化するか、又はシステムの効率を最大化することができる。
For example, in one embodiment, the control circuit controls the light emitter 32 from a height of at least about 5-20 pulses per second, for example thousands of pulses per second, to about 4-5 per second. It may generate a flickering infrared signal pulsed at a given frequency down to the low pulse, thereby detecting unintended or purposeful blinks as short as about 200 milliseconds each time. Becomes easier. The sensor 33 may be controlled to detect only light pulses of a predetermined frequency specific to the blinking frequency of the light emitter 32. By synchronizing the light emitter 32 and the sensor 33 to a predetermined frequency in this way, the
制御回路及び／又は処理ボックス１３０は、当業者であれば理解されるように、発光器３２によって発光される光の周波数、焦点、若しくは強度を調節するための、発光器３２のオン及びオフを切り替えるための、センサー３３の閾値感度を調節するための、及び／又は閉じられた眼瞼からの最大の赤外反射を得る自己集束（self-focusing）を可能とするための、マニュアル及び／又はソフトウェアによる制御部（図示せず）を含んでよい。
The control circuit and / or
加えて、処理ボックス１３０はまた、発光器３２、センサー３３、ＣＰＵ、及び／又は処理ボックス１３０内のその他のコンポーネントに電力を供給するための電源も含んでよい。処理ボックス１３０は、従来のＤＣ電池、例えば９ボルト電池、又は充電式のリチウム、カドミウム、若しくは水素電池によって、並びに／又はシステム１４に取り付けられた、若しくは中に組み込まれた太陽電池によって電源が供給されてよい。別の選択肢として、従来のＡＣアダプター又は１２ボルト自動車ライターア用ダプターなどのアダプター（図示せず）が、処理ボックス１３０に接続されてもよい。
In addition, the
別の選択肢として、人とデバイスとの間の直接のインターフェイスを提供するために、受信器１５６が、ラジオ若しくはテレビの操作部（radio or television controls）、ランプ、ファン、ヒーター、モーター、振動触覚シート（vibro-tactile seats）、リモートコントロール輸送機、輸送機モニタリング若しくは制御デバイス、コンピューター、プリンター、電話、ライフラインユニット（lifeline unit）、電子玩具、又は補助コミュニケーションシステムなどの様々なデバイス（図示せず）と直接連結されてもよい。 Alternatively, the receiver 156 can be a radio or television controls, a lamp, a fan, a heater, a motor, a vibrating tactile sheet to provide a direct interface between a person and a device. (Vibro-tactile seats), remote control transport aircraft, transport aircraft monitoring or control devices, computers, printers, telephones, lifeline units, electronic toys, or various devices such as auxiliary communication systems (not shown) May be directly linked with.
さらなる別の選択肢として、バイオセンサーデバイス、個々の発光器、及び／又は検出器によって発光及び／又は検出される光を制御するために、１つ以上のレンズ又はフィルターが備えられてよい。例えば、発光された光の角度が、プリズム若しくはその他のレンズを用いて変化されてよく、又は光は、眼に向けられる光を所定の形態のビームとするために、若しくはセンサーによって反射光を受光するために、スリットを通して平行化（columnated）又は集束されてよい。発光された光のビームの形態、例えば幅を制御するように、又はセンサーの感度を調節するように調節可能であるレンズアレイが備えられてよい。レンズは、当業者であれば理解されるように、プラスチックなどのケースに発光器と共に入れられるか、又は別個のアタッチメントとして提供されてよい。 As yet another option, one or more lenses or filters may be provided to control the light emitted and / or detected by the biosensor device, individual light emitters, and / or detectors. For example, the angle of the emitted light may be varied using a prism or other lens, or the light receives reflected light in order to make the light directed at the eye a beam of a given form or by a sensor. It may be columnated or focused through a slit. A lens array may be provided that is adjustable to control the morphology of the emitted light beam, eg, the width, or the sensitivity of the sensor. The lens may be placed with a photophore in a case such as plastic or provided as a separate attachment, as will be appreciated by those skilled in the art.
図３を参照すると、眼の動きをモニタリングするためのシステム８１０のなお別の実施形態が示される。一般的に、システム８１０は、ブリッジ８１４及び１対の耳保持部（ear support）、１つ以上の発光器８２０、１つ以上のセンサー８２２、及び／又は１つ以上のカメラ８３０、８４０を含んでよいフレーム８１２を含む。フレーム８１２は、処方箋レンズ、色付き（shaded）レンズ、又は保護レンズなどの１対のレンズ（図示せず）を含んでよいが、含んでいなくてもよい。別の選択肢として、システムは、パイロットの酸素マスク、眼の保護具、患者の人工呼吸装置、スキューバ若しくは水泳用マスク、ヘルメット、ハット、ヘッドバンド、ヘッドバイザー、頭部保護具などのユーザーの頭部に装着され得るその他のデバイス上に、又は頭部及び／若しくは顔面を保護する密閉型スーツ（enclosed suits）などの中に備えられてもよい（図示せず）。システムのコンポーネントは、ユーザーの視覚及び／又はデバイスの通常の使用への干渉をおおよそ最小限に抑えるデバイス上の様々な位置に備えられてよい。
With reference to FIG. 3, yet another embodiment of the
示されるように、発光器８２０のアレイが、フレーム８１２上に備えられており、例えば、縦方向アレイ８２０ａ及び横方向アレイ８２０ｂである。加えて、又は別の選択肢として、発光器８２０は、円形状アレイ（図示せず）などのその他の構成で備えられていてよく、光フィルター及び／又は光拡散器を含んでいても、若しくは含んでなくてもよい。代表的実施形態では、発光器８２０は、本明細書の他所で述べるその他の実施形態と同様に、所定の周波数でのパルスを発光するように構成された赤外発光器である。発光器８２０は、ユーザーの眼の片方を含むユーザーの顔面の領域上へレファレンスフレーム８５０を投射するように、フレーム上に配置されてよい。示されるように、レファレンスフレームは、その領域を４つの象限に分割する１対の交差したバンド８５０ａ、８５０ｂを含む。代表的実施形態では、交差したバンドの交点は、正面視（primary gaze）時、すなわち、ユーザーがおおよそまっすぐ前を見ている場合の眼の瞳孔に実質的に対応する位置に配置されてよい。別の選択肢として、その他のレファレンスフレームが提供されてもよく、例えば、縦横の構成要素、角度のついた放射状構成要素、又はその他の直交構成要素が挙げられる。以下でさらに説明するように、所望に応じて、実質的に静止して維持される１若しくは２つのレファレンス点であっても、眼の相対的動きを判定するための充分なレファレンスフレームを提供し得る。
As shown, an array of light emitters 820 is provided on the
ユーザーの眼瞼から反射される発光器８２０からの光を検出するために、センサー８２２のアレイが、フレーム８１２上に備えられてもよい。センサー８２２は、本明細書の他所で述べるその他の実施形態と同様に、眼瞼の開閉を識別する強度を有する出力シグナルを発生させることができる。センサー８２２は、眼瞼の対応する部分から反射された光を検出するために、対応する発光器８２０に近接して配置されてよい。別の選択肢として、センサー８２２は、本明細書の他所で述べる実施形態と同様に、眼瞼が閉じた度合いをモニタリングするために、例えばブリッジ８１４に沿って、縦方向アレイとしてのみ提供されてもよい。さらなる別の選択肢として、発光器８２０及びセンサー８２２は、単一デバイスに発光及び検出の両機能を備えたソリッドステートバイオセンサー（図示せず）であってよい。以下でさらに説明されるように、所望に応じて、発光器８２０及び／又はセンサー８２２は、例えばカメラ８３０、８４０が充分な情報を提供する場合、取り除かれてもよい。
An array of sensors 822 may be provided on the
センサーアレイによって発生されたシグナルを用いてＰＥＲＣＬＯＳ又はその他のパラメーターを測定するために、回路及び／又はソフトウェアが備えられてよい。例えば、図７は、例えばＰＥＲＣＬＯＳ測定値又はその他の注意パラメーター（alertness parameter）を得るために、５素子アレイからのシグナルを処理するために用いられてよい代表的な概略図を示す。 Circuits and / or software may be provided to measure PERCLOS or other parameters using the signals generated by the sensor array. For example, FIG. 7 shows a representative schematic that may be used to process a signal from a five-element array, eg, to obtain a PERCLOS measurement or other alertness parameter.
図３に戻ると、システム８１０は、ユーザーの片眼又は両眼に対して概略的に向けられた１つ以上のカメラ８３０も含む。各カメラ８３０は、ブリッジ８１４（又はフレーム８１２上のその他の位置、例えば、ユーザーの視覚の妨害を最小限に抑える位置）に取り付けられるか又はそれに近接する第一の端部、及びイメージをデジタルビデオシグナルに変換することができるＣＣＤ又はＣＭＯＳを例とする検出器８３８と連結される第二の端部８３７を含む光ファイバーバンドル８３２を含んでよい。図４に示されるように、例えばイメージの焦点を光ファイバーバンドル８３２に合わせるために、対物レンズ８３４が光ファイバーバンドル８３２の第一の端部に備えられてよい。やはり図４に示されるように、所望に応じて、発光器８３６を備えるために、光ファイバーバンドル８３２は、端部がレンズ８３４に近接してよい１つ以上の発光ファイバーを含んでよい。（１若しくは複数の）発光ファイバーは、例えば図９に示される実施形態に類似し、以下でさらに記載される光源（図示せず）に接続されてよい。図３には、１つのカメラ８３０しか示されていないが（例：ユーザーの左眼をモニタリングするため）、ユーザーの他方の眼（例：右眼）をモニタリングするために、光ファイバーバンドル、レンズ、（１若しくは複数の）発光器、及び／又は検出器（もっとも、以下でさらに説明されるように、所望に応じて、これらのカメラは、共通の検出器を共有してもよい）を例とする類似のコンポーネントを含む別のカメラ（図示せず）が対称の構成で備えられてよいことは理解される。
Returning to FIG. 3, the
所望に応じて、異なる角度から片眼又は両眼に面していることを例とする、各々の眼に向けられた複数のカメラ（図示せず）を有することが望ましい場合もある。所望に応じて、これらの（１若しくは複数の）カメラは、延長光ファイバー、プリズムレンズ、及び／又は反射ミラー（例：赤外光を反射するもの）、眼に面しているレンズの側面の非貫通性若しくは遮断性鏡面などを含んでよい。そのようなアクセサリーは、（１若しくは複数の）カメラに送信される眼のイメージの所望される方法での曲げ、回転、反射、又は反転のために備えられてよい。 If desired, it may be desirable to have multiple cameras (not shown) pointed at each eye, for example facing one or both eyes from different angles. If desired, these (s) cameras include extended fiber optics, prism lenses, and / or reflective mirrors (eg, those that reflect infrared light), non-side facing lenses. It may include a penetrating or blocking mirror surface and the like. Such accessories may be provided for bending, rotating, reflecting, or flipping the image of the eye transmitted to the camera (s) in the desired manner.
（１若しくは複数の）カメラ８３０は、発光器８２０及び／又は８３６によって発光される光、例えば赤外光又は可視光範囲を超えるその他の光の周波数を検出するように構成されてよい。所望に応じて、（１若しくは複数の）光ファイバーバンドル８３２が、発光器８３６のための１つ以上の発光ファイバーを含む場合、フレーム８１２上の発光器８２０は取り除かれてよい。この実施形態では、例えば以下でさらに説明されるように、センサー８２２を取り除き、（１若しくは複数の）カメラ８３０を用いて、ユーザーの片眼又は両眼の動きをモニタリングすることも可能であり得る。所望に応じて、システム８１０は、例えば、ユーザーの顔のすぐ前の領域など、ユーザーの周囲をモニタリングするために、ユーザーの頭部から外側に向けられた第二のカメラ８４０を含んでよい。カメラ８４０は、カメラ８３０に類似のコンポーネントを含んでよく、例えば、光ファイバーバンドル８４１、レンズ（図示せず）、及び／又は（１若しくは複数の）発光器（これも図示せず）である。所望に応じて、カメラ８３０は、周囲光の条件下でイメージを生成するのに充分な感度であってよく、発光器は、取り除かれてよい。カメラ８４０は、図３に示されるように、別の検出器８３９に連結されてよく、又は以下でさらに説明されるように、（１若しくは複数の）カメラ８３０と検出器８３８を共有してもよい。
The camera 830 (s) may be configured to detect frequencies of light emitted by the light emitters 820 and / or 836, such as infrared light or other light beyond the visible light range. If desired, if the fiber optic bundle 832 (s) includes one or more light emitting fibers for the light emitter 836, the light emitter 820 on the
耳支持部８１６の一方又は両方は、代表的なプロセッサー８４２などのコントローラー若しくはプロセッサー、送信器８４４、アンテナ８４５、（１若しくは複数の）検出器８３８、８３９、及び／又は電池８４６を例とする１つ以上のコンポーネントを搭載するためのパネル８１８を含んでよい。プロセッサー８４０は、発光器８２０、センサー８２２、及び／又はカメラ８３０、８４０（例：（１若しくは複数の）検出器８３８、８３９）と、それらの作動を制御するために連結されてよい。送信器８４４は、センサー８２２及び／又はカメラ８３０、８４０からの出力シグナルを受けて、例えば、以下で述べるように、そのシグナルをリモート位置へ送信するために、プロセッサー８４２及び／又は（１若しくは複数の）検出器８３８、８３９と連結されてよい。別の選択肢として、送信器８４４は、センサー８２２及び／又はカメラ８３０、８４０からの出力リードと直接連結されてもよい。フレーム８１２は、例えば、電源のオンオフの切り替え、又は発光器８２０、センサー８２２、及び／若しくはカメラ８３０、８４０の強度及び／若しくは閾値の調節のために、例えば耳支持部８１６上に、マニュアル制御部（図示せず）も含んでよい。
One or both of the ear supports 816 exemplify a controller or processor such as a typical processor 842, a
所望される場合、システム８１０は、例えば、ユーザーの認知、感情、及び／又は行動の状態に関連する追加の生物又は神経生理学的データの統合及び相互相関の目的で、生理学的センサーを例とする１つ以上の追加のセンサーをフレーム８１２上に含んでもよい。センサーは、センサーからのシグナルのモニタリング、記録、及び／又はリモート位置への送信が可能となるように、プロセッサー８４２及び／又は送信器８４４に連結されてよい。例えば、１つ以上の位置センサー８５２ａ、８５２ｂが、例えばフレーム８１２の、従ってユーザーの頭部の空間的方向を特定するために備えられてよい。例えば、頭部の傾き又は動きを測定するために、例えばユーザーの頭部が前方に垂れているか又は側方に傾いているかどうかをモニタリングするために、アクチグラフセンサーが備えられてよい。音響センサー、例えばマイク８５４が、環境ノイズ又はユーザーによって発せられる音を検出するために備えられてよい。
If desired, the
加えて、システム８１０は、フレーム８１２上に１つ以上のフィードバックデバイスを含んでよい。このようなデバイスは、例えば、傾眠又は意識欠如の状態を例とする所定の状態が検出された場合に、ユーザーに警告する、及び／又はユーザーを起こすために、ユーザーにフィードバックを提供することができる。フィードバックデバイスは、その起動を制御することができるプロセッサー８４２と連結されてよい。例えば、触覚振動刺激を皮膚接触を通して提供することができる機械的バイブレーターデバイス８６０が、耳支持部８１６を例とするユーザーと接触することができる位置に備えられてよい。比較的低パワーの電気刺激を発生させることができる電極（図示せず）が備えられてよい。１つ以上のＬＥＤなどの可視白色又は色付き光発光器が、ブリッジ８１４の上部を例とする所望される位置に備えられてよい。別の選択肢として、本明細書の他所でその他の実施形態と同様に、ブザー又はその他のアラームなどの音響デバイス８６２が備えられてよい。さらなる別の選択肢として、芳香器（aroma-emitters）が、フレーム８１０上、例えば、ブリッジ８１４の上又はそれに近接して備えられてよい。
In addition, the
加えて、又は別の選択肢として、１つ以上のフィードバックデバイスが、フレーム８１２とは別であるが、ユーザーにフィードバック応答を提供することができる形で位置するように備えられてよい。例えば、音響的、視覚的、触覚的（例：振動シート）、又は嗅覚的発生器がユーザーの近傍に備えられてよく、本明細書の他所で述べるいずれかのデバイスなどである。さらなる別の選択肢として、遠隔操作されるファン若しくはエアコンディショニングユニットを例とするユーザーに対する熱刺激を発生させることができる発熱又は冷却デバイスが備えられてよい。
In addition, or as an alternative, one or more feedback devices may be provided, apart from
システム８１０はまた、本明細書の他所で述べるその他の実施形態と同様に、フレーム８１２からリモートにあるコンポーネントを含んでもよい。例えば、システム８１０は、受信器、プロセッサー、及び／又はディスプレイ（図示せず）を、フレーム８１２からのリモート位置に含んでよく、例えば、同じ室内、近くにあるモニタリングステーション、又はより遠くにある位置である。受信器は、センサー８２２、カメラ８３０、８４０、又はフレーム８１２上に備えられるその他のセンサーのいずれかからの出力シグナルを含む、送信器８４２によって送信されるシグナルを受けることができる。
プロセッサーは、フレーム８１２上のコンポーネントからのシグナルを分析して、例えばグラフィック表示用にそのシグナルを処理するために、受信器に連結されてよい。例えば、プロセッサーは、センサー８２２及び／又はカメラ８３０、８４０からのシグナルを、モニター上に表示するために処理してよく、それによって、ユーザーを他者がモニタリングすることが可能となる。同時に、単一又は別々のディスプレイ上に、その他のパラメーターが表示されてよい。例えば、図５Ａ〜５Ｉは、フレーム８１２上に存在してよい種々のセンサーの出力を示すシグナルを示しており、それらは、共通の時間軸で、又はそうでなければ、例えば、ユーザーの眼の動き及び／又は傾眠のレベルに対して補正されて表示されてよい。プロセッサーは、ビデオシグナルを、その他の感知されたパラメーターと合わせて重ね合わせるか、又はそうでなければ同時に表示して、医師又はその他の個人が、これらのパラメーターをモニタリングし、ユーザーの行動と個人的に相関させることを可能とすることができる。
The processor may be coupled to a receiver to analyze the signal from the component on
カメラ８３０からのビデオシグナルが処理されて、瞳孔サイズ、例えば交差バンド８５０で定められる４つの象限内を例とする位置、追跡眼球運動、視線移動距離（eye gaze distance）などの様々な眼のパラメーターがモニタリングされてよい。例えば、（１若しくは複数の）カメラ８３０は、発光器８２２によって発光される光を検出する能力を有し得ることから、（１若しくは複数の）カメラ８３０は、発光器によってユーザーの眼の領域に投射されるレファレンスフレームを検出し得る。図６は、縦方向配列で配置される２０個の発光器を有するシステムに含まれるカメラからの代表的ビデオ出力を示す。
Video signals from
カメラは、縦方向バンドとして配列される光の２０個の別々の領域を検出することができる。カメラはまた、「グリント（glint）」点Ｇ、及び／又は移動する明瞳孔Ｐも検出することができる。従って、瞳孔の動きは、グリント点Ｇに対して、及び／又は縦方向バンド１〜２０に対してモニタリングすることができる。 The camera can detect 20 separate regions of light arranged as vertical bands. The camera can also detect the "glint" point G and / or the moving bright pupil P. Therefore, pupillary motion can be monitored with respect to glint points G and / or with respect to longitudinal bands 1-20.
発光器８２２がフレーム８１２に固定されていることから、レファレンスフレーム８５０は、ユーザーに対して実質的に静止状態に維持することができる。従って、プロセッサーは、レファレンスフレーム８５０に対して、直交座標（例：ｘ−ｙ又は角度−半径）での瞳孔の位置を特定することができる。別の選択肢として、レファレンスフレームが除去される場合、瞳孔の位置は、ユーザーの眼のいずれかの静止「グリント」点、又はその他の所定のレファレンス点に対して特定されてよい。例えば、カメラ８３０自体が、反射してカメラによって検出されてよい光の点を眼に投射してよい。この「グリント」点は、カメラ８３０がフレーム８１２に固定されていることから、実質的に静止状態に維持することができ、それによって、それに続く眼の相対的な動きを特定可能とする基になる所望されるレファレンス点が提供される。
Since the light emitter 822 is fixed to the
図３に戻ると、別の選択肢としての実施形態では、カメラ８３２、８４０は、図９に示される構成と同様に、単一の検出器（図示せず）と連結されてよい。光ファイバーバンドル８３２、８４１は、カメラ８３０、８４０からのイメージを、検出器の対応する領域に送達し、及び／又は焦点を合わせるために、１つ以上のレンズと連結されてよい。検出器は、例えば断面が約５から１０ミリメートル（５〜１０ｍｍ）であるアクティブイメージング領域（active imaging area）を有するＣＣＤ又はＣＭＯＳチップであってよい。代表的実施形態では、検出器のアクティブイメージング領域は、カメラ８３０及びカメラ８４０の両方からのイメージを同時に受けるのに充分な領域が存在する限りにおいて、正方形、長方形、円形、又は楕円形であってよい。カメラ８３０、８４０からの同時のビデオイメージを表示する代表的な出力を、図１０Ａ及び１０Ｂに示し、以下でさらに述べる。この別の選択肢では、充分な解像度及び処理を伴う場合、システム８１０から発光器８２０及び／又はセンサー８２２を除去することが可能であり得る。
Returning to FIG. 3, in another alternative embodiment, the
図８Ａ及び８Ｂを参照すると、装置９１０を装着した個人の眼瞼の動きをモニタリングするための装置９１０の別の実施形態が示される。本明細書の他所で述べるように、装置９１０は、バイオセンサー、コミュニケーター、及び／若しくはコントローラーとして用いられてよく、並びに／又は、例えば意識的−目的を持った及び／若しくは無意識−目的を持たないユーザーの片眼又は両眼の動きのモニタリングためのシステムに含まれてよい。
With reference to FIGS. 8A and 8B, another embodiment of the
示されるように、装置９１０は、ユーザーの頭部に装着されてよいヘルメット９１２、及びバイオセンサーアセンブリー９２０を含む。ヘルメット９１２は、標準的な飛行士のヘルメットであってよく、ヘリコプター又はジェット飛行機のパイロットが用いるものなどであり、例えば、それに搭載された暗視スコープ（night vision tubes）又はその他のゴーグル９１４を含む。所望に応じて、ヘルメット９１２は、片眼若しくは両眼の前又は近傍に搭載された小型フラットパネルＬＣＤを例とする１つ以上のヘッドアップディスプレイを含んでよい（図示せず）。
As shown, the
別の選択肢として、ヘルメット９１２は、ユーザーの頭部に装着されるように構成されたフレーム又はその他のデバイスに置き換えられてもよい。例えば、本明細書の他所でさらに述べるように、図１５は、フレーム１０１２及びバイオセンサーアセンブリー１０２０を含む装置１０１０の代表的実施形態を示す。本明細書で述べる他の実施形態と同様に、一般的に、フレーム１０１２は、ブリッジ１０１２ａ、各眼の上若しくは周りに延び、開口部１０１２ｃを定めるリム１０１２ｂ、及び／又は１対の耳支持部１０１２ｄを含む。フレーム１０１２は、処方箋レンズ、色付きレンズ、及び／又は保護レンズなど、開口部１０１２ｃの中又は全体にわたって搭載される１対のレンズ（これも図示せず）を含んでよいが、それらは、装置１０１０の作動にとって必要ではない。例えば、レンズは、青若しくはグレーフィルター、偏光レンズなどを含んでよい。代表的実施形態では、レンズは、例えば、カメラ１０２４によって取り込まれたイメージにおける過飽和、グリントなどの発生を抑制するために、カメラ１０２４によって検出される帯域幅に相当する光の所定の帯域幅を透過させるように選択されてよい。
Alternatively, the
別の選択肢として、片方又は両方のレンズは、比較的小型のフラットパネルＬＣＤを例とするディスプレイに置き換えられてよく、又は以下でさらに説明されるように、シミュレーター及び／若しくは娯楽用デバイスとして用いられてよい、ヘッドアップディスプレイに類似のものを例とするイメージの投射が可能である領域（図示せず）を含んでよい。さらなる別の選択肢として、本明細書の装置は、ハット、キャップ、ヘッドバンド、ヘッドバイザー、眼及び頭部の保護具、フェイスマスク、酸素マスク、人工呼吸マスク、スキューバ若しくは水泳用マスクなど（図示せず）、ユーザーの頭部に装着されてよいその他のデバイスを含んでよい。 Alternatively, one or both lenses may be replaced with a display such as a relatively small flat panel LCD, or used as a simulator and / or entertainment device as further described below. It may include a region (not shown) where an image can be projected, such as an example of a head-up display. As yet another option, the devices herein include hats, caps, headbands, head visors, eye and head protectors, face masks, oxygen masks, ventilator masks, scuba or swimming masks (shown). It may include other devices that may be worn on the user's head.
本明細書の他所でさらに述べるように、装置９１０又は１０１０のコンポーネントは、例えば、装置９１０又は１０１０の装着中におけるユーザーの視覚及び／又は通常の活動への干渉をおおよそ最小限とするために、ヘルメット９１２又はフレーム１０１２（又はその他の頭部装着デバイス）の様々な位置に備えられてよい。
As further described elsewhere herein, the components of
図８Ａ及び８Ｂに示されるように、バイオセンサーアセンブリー９２０は、例えばベルクロ、ストラップ、及び／又はその他の一時的若しくは取り外し可能なコネクタ（図示せず）を用いて、ヘルメット９１２の上部に搭載されたカメラ９２２を含む。このことにより、非使用時にカメラ９２２を取り外すことが可能となる。別の選択肢として、本明細書で述べるその他の実施形態と同様に、カメラ９２２は、実質的に恒久的にヘルメット９１２と接続されてよく、ヘルメット９１２（又はその他のフレーム）中に直接組み込まれる、ヘッドマウントテレビ、ＬＣＤモニター、若しくはその他のデジタルディスプレイに接続される、などである。
As shown in FIGS. 8A and 8B, the
バイオセンサーアセンブリー９２０はまた、ユーザーの片眼又は両眼のイメージングのための１つ以上の「エンド−カメラ（endo-cameras）」を備えるために、カメラ９２２からヘルメット９１２の前部へ延びる１つ以上の光ファイバーバンドル９２４も含む。カメラ９２２から対応するゴーグル９１４のスコープ部へ延びる１対の光ファイバーバンドル９２４が示される。代表的な実施形態では、光ファイバーバンドル９２４は、カメラ９２２からゴーグル９１４まで延びるのに充分な長さであってよく、例えば、約１２から１８インチの長さであるが、別の選択肢として、光ファイバーバンドル９２４は、約２から４フィートの長さを例としてそれより長くてもよく、又は短くてもよく、ヘルメット９１０上のカメラ９２２の位置に応じて異なる（又は、カメラ９２２がヘルメット９１０とは別に備えられる場合）。
The
光ファイバーバンドル９２４の端部９２６は、恒久的に又は取り外し可能にゴーグル９１４に取り付けられてよく、例えば、ゴーグル９１４に接続されているか、又はそうでなければゴーグル９１４から延びるブラケット９１６に取り付けられてよい。別の選択肢として、光ファイバーバンドル９２４は、クリップ、留め具、接着剤など（図示せず）を用いて、一時的又は実質的に恒久的にゴーグル９１４上に保持されてよい。示されるように、光ファイバーバンドル９２４の端部９２６は、ゴーグル９１４の下部に搭載され、ユーザーの眼に向かって上向きの角度が付けられる。端部９２６の角度は、例えば、約４５度のベース角から約１５度上下に調節可能であってよい。別の選択肢として、光ファイバーバンドル９２４の端部９２６は、ヘルメット９１２及び／又はゴーグル９１４のその他の位置に備えられてよいが、それでも、ユーザーの眼の方向に向けられている。
The
さらに図９を参照すると、各光ファイバーバンドル９２４は、光ファイバーイメージガイド９２８、すなわち、光イメージングファイバーバンドル及び発光ファイバーバンドル９３０を含んでよく、例えば、収縮チューブ（図示せず）に包まれて、カメラ９２２から光ファイバーバンドル９２４の端部９２６まで延びている。各発光ファイバーバンドル９３０は、例えばカメラ９２２の内部にある光源と連結された１つ以上の光ファイバーを含んでよい。例えば、カメラ９２２は、１つ以上のＬＥＤ９３４を含む（簡略化のための１つを図示）発光ダイオード（ＬＥＤ）筐体９３２を含んでよく、（１若しくは複数の）発光ファイバーバンドル９３０は、ＬＥＤ筐体９３２と連結されて、（１若しくは複数の）端部９２６に光が送達されてよい。
Further referring to FIG. 9, each
光源９３４によって発光される光は、通常の人の視覚範囲外、例えば赤外範囲であってよく、例えば、定格出力波長は、約８４０から８８０ナノメートル（８４０〜８８０ｎｍ）であり、それによって、発光される光が、ユーザーの通常の視覚を大きく妨げることはない。本明細書の他所で述べる実施形態と同様に、光源は、実質的に連続的に光を発してよく、又は所望される周波数の光パルスを発してもよい。例えば、所望される場合、発光されるパルスの周波数、長さ、及び／又は振幅のうちの１つ以上を調節するために、コントローラー（図示せず）が（１若しくは複数の）光源９３４に連結されてよい。 The light emitted by the light source 934 may be outside the normal human visual range, eg, the infrared range, eg, the rated output wavelength is about 840 to 880 nanometers (840-880 nm), thereby. The emitted light does not significantly interfere with the user's normal vision. Similar to the embodiments described elsewhere herein, the light source may emit light substantially continuously, or may emit light pulses of a desired frequency. For example, if desired, a controller (not shown) is coupled to (s) light sources 934 to adjust one or more of the frequencies, lengths, and / or amplitudes of the emitted pulses. May be done.
別の選択肢として、発光ファイバーバンドル９３０の代わりに、ユーザーの顔面及び／又は片眼若しくは両眼を照らすためのその他の光源が備えられてもよい。例えば、本明細書の他所で述べる実施形態と同様に、１つ以上の発光器（図示せず）が備えられてよく、例えば、ヘルメット９１２及び／又はゴーグル９１４の１つ以上の領域に沿って配置される発光器アレイである。
Alternatively, instead of the luminescent
各光ファイバーバンドル９２４の端部９２６は、イメージガイド９２８の焦点を、所望される形で、例えばユーザーの眼に向かって合わせることができる対物レンズ９３６（図８Ａに示す）を例とする１つ以上のレンズを含んでよい。各イメージガイド９２８は、前方視線（forward line of sight）（ゼロ度（０°）視野）を有してよく、対物レンズ９３６は、約４５度（４５°）を例とするより広い視野を提供してよい。所望に応じて、視線は、対物レンズ９３６の調節によって調節可能であってよく、例えば、約３０から６０度（３０〜６０°）である。さらに、対物レンズ９３６は、視距離を、例えば約２インチ（２ｉｎ．）に最適化することができ、それによって、ユーザーの片眼又は両眼への焦点合わせが改善される。従って、（１若しくは複数の）イメージガイド９２８は、ユーザーの片眼又は両眼のイメージを、（１若しくは複数の）光ファイバーバンドル９２４を通してカメラ９２２に伝達することができる。
The
図９に示されるように、カメラ９２２は、イメージングデバイス９４０のアクティブ領域９４２上にある（１若しくは複数の）イメージガイド９２８（及び／又はカメラ９４４）からのイメージを送達し及び／又はイメージの焦点を合わせるために、拡大セクション９３８を例とする１つ以上のレンズを含んでよい。イメージングデバイス９４０は、イメージを受けるための二次元アクティブ領域を備える様々な公知のデバイスであってよく、例えば、ＣＭＯＳ又はＣＣＤ検出器である。代表的実施形態では、イメージングデバイス９４０は、ＣＭＯＳデバイスであってよく、センソベーション（Sensovation）製Ｍｏｄｅｌ ｃｍｏｓ ＳａｍＢａ ＨＲ−１３０、又はマイクロンイメージング（Micron Imaging）製Ｆａｓｔ Ｃａｍｅｒａ １３ Ｍｏｄｅｌ ＭＩ−ＭＶ１３などである。拡大セクション９３８は、Ｃマウント又はその他の接続法（図示せず）を介して、カメラ９２２と機械的に接続されてよい。
As shown in FIG. 9, the
代表的実施形態では、各イメージガイド９２８は、例えば本明細書の他所で述べる光ファイバーバンドルと同様に、１万から５万（１０，０００から５０，０００）ピクセルものイメージデータを提供する能力を有してよく、それらは、イメージングデバイス９４０のアクティブ領域９４２上に投射されてよい。図８Ａ及び８Ｂに示される装置９１０の場合、両光ファイバーバンドル９２４からのイメージは、図９に示されるように、単一のイメージングデバイス９４０に投射され、すなわち、それによって、ユーザーの眼の各々からのイメージが占めるのは、アクティブ領域９４２の半分未満である。
In a exemplary embodiment, each
本明細書の他所で述べる実施形態と同様に、所望に応じて、装置９１０は、例えばユーザーの周囲をモニタリングするために、ユーザーの頭部から外側に向けられた「エキソ−カメラ（exo-camera）」９４４を含んでよい。
As in other embodiments described herein, if desired, the
例えば、図８Ａに示されるように、カメラ９２２から延びる別の光ファイバーバンドル９４５が備えられてよい。示されるように、光ファイバーバンドル９４５は、「前方向」、すなわち、ユーザーがまっすぐ前を見る際とおおよそ同じ方向に向けられ、末端にはマイクロレンズ９４６を有する。この光ファイバーバンドル９４５は、その視野がヘルメット９１２に対して実質的に固定されるように、比較的短くてよく、及び／又は実質的に剛性であってよい。別の選択肢として、エキソ−カメラ９４４は、上記で述べるエキソ−カメラ８４０同様に、例えばフレキシブル光ファイバーバンドルを含む、ヘルメット９１２及び／又はゴーグル９１４のその他の場所に備えられてもよい。従って、エキソ−カメラ９４４は、ユーザーから外側のイメージ、例えばユーザーの顔面のまっすぐ前方向のイメージを提供することができる。
For example, as shown in FIG. 8A, another fiber optic bundle 945 extending from the
エキソ−カメラ９４４は、１つ以上の発光ファイバーを含んでも又は含んでいなくてもよいが、イメージングデバイス９４０と、例えば拡大セクション９３８を介して、又は別々に連結されてよいイメージガイドを含んでよい。従って、本明細書で述べるその他の実施形態と同様に、エキソ−カメラ９４４からのイメージは、イメージガイド９２８から受けるユーザーの眼の各々のイメージと同じアクティブ領域９４２上に送達されてよい。この構成により、時間的及び／又は空間的同期を可能とするか、又は容易とすることができ、それによって、（１若しくは複数の）エンド−カメライメージをエキソ−カメライメージ上に重ねる、若しくは重ね合わせることが可能となるか、又は「三角測量」若しくは視線追跡目的のその他のアルゴリズムを通して、ユーザーの頭部の方向位置（head directional position）に対してユーザーの眼が「どこを」、「何を」、及び／又は「どれだけの時間」（注視の継続時間）見ているかを識別することが可能となる。
The exo-
従って、カメラ９２２は、１つ以上の「エンド−カメラ」から、すなわち、光ファイバーバンドル９２４から、及びエキソ−カメラ９４４からのイメージを同時に取り込むことができる。このことにより、各デバイスによって取り込まれたイメージを、確実に互いに同期させることができ、すなわち、特定の時間に取得された一方の眼のイメージが、実質的に同じ時間に取得された他方の眼のイメージに対応するように、互いに時間でリンクさせることができる。さらに、これらのイメージは、１つ以上の生理学的センサーを例とするその他のセンサーからのデータと実質的に同期させることもでき、このことにより、ユーザーのモニタリング及び／若しくは診断、並びに／又はユーザーの行動の予測を行う能力を向上させることができる。この同期のために、イメージデータは、比較的高い速度で取り込まれてよく、例えば、１秒あたり約５００から７５０フレーム又はヘルツ（５００〜７５０Ｈｚ）である。別の選択肢として、例えばデータを受けるプロセッサーによって同期されてよいイメージデータを取り込む別の検出器が備えられてもよい。この別の選択肢では、取り込みに続くプロセッサー又はその他のデバイスによる同期を容易とするために、約３０から６０ヘルツ（３０〜６０Ｈｚ）を例とするより遅い取り込み速度が用いられてよい。所望に応じて、カメラ９２２及び／又は付随するプロセッサーは、例えば１秒間あたり約１５から６０（１５〜６０）フレームの速度での比較的遅い眼部計測（oculometrics）を取り込む能力を有していてよい。
Thus, the
図１０Ａ及び１０Ｂは、２つのエンド−カメラ２０１０及びエキソ−カメラ２０２０からの同時イメージシグナルを受けるカメラからの（又は、別々のカメラ及び／若しくは検出器からのイメージをコンパイルするデバイスからの）代表的な出力を示す。示されるように、エンド−カメラは、ユーザーの眼の各々に向かって向けられており、エキソ−カメラは、外側に向かってユーザーの周囲に向けられている（すなわち、ユーザーの顔面のおおよそまっすぐ前方向）。図１０Ａでは、ユーザーの眼の両方２０１０Ｌ、２０１０Ｒは、開いており、エキソ−カメライメージ２０２０は、ユーザーの前にある部屋の水平の視界を示している。反対に、図１０Ｂでは、ユーザーの眼の一方２０１０Ｌは、完全に閉じられており、他方の眼２０１０Ｒは、眼瞼が瞳孔のほとんどを覆うように部分的に閉じられている。エキソ−カメライメージ２０２０は、ユーザーの頭部が、左に傾き、前に垂れ始めたことを示している。
Figures 10A and 10B are representative from cameras that receive simultaneous image signals from two end-
図８Ａ、８Ｂ、及び９に戻ると、カメラ９２２（及び／又はカメラ９４４）からのイメージは、装置９１０からケーブル９４８を介して転送される（図８Ａが最もよく分かる）。例えば、本明細書の他所で述べるその他の実施形態と同様に、イメージングデバイス９４０は、アクティブ領域９４２からの光学イメージを、ケーブル９４８を介して１つ以上のプロセッサー及び／又はコントローラー（図示せず）に送ることができる電気シグナルに変換することができる。別の選択肢として、本明細書で述べるその他の実施形態と同様に、光ファイバーバンドル９２４及び／又はエキソ−カメラ９４４からのイメージは、装置９１０から１つ以上のリモートデバイス、例えば、カメラ、検出器、及び／又はプロセッサー（図示せず）に送られてよい。この別の選択肢では、バンドル９２４は、約２から６フィートの長さであってよく、例えば、ユーザーが通常通りに移動可能であるが、それでも、（１若しくは複数の）リモートデバイスに連結された状態を維持可能であるのに充分な長さが提供される。
Returning to FIGS. 8A, 8B, and 9, the image from camera 922 (and / or camera 944) is transferred from
別の選択肢として、又は加えて、装置９１０は、無線送信器（図示せず）を含んでよく、例えばブルートゥース若しくはその他のプロトコルを用いた、カメラ９２２と連結することができる短距離又は長距離無線周波数（ＲＦ）送信器などである。送信器は、カメラ９２２の中、又はヘルメット９１２の他所に位置されてよい。本明細書の他所で述べるその他の実施形態と同様に、送信器は、イメージデータを表すイメージシグナルを、リモート位置の受信器に送信することができる。なお別の選択肢として、装置９１０は、送信器及び／若しくはケーブル９４８に代えて、又は加えて、イメージデータを保存するためのメモリ（これも図示せず）を含んでよい。例えば、データは、例えば航空機で用いられる「ブラックボックス」レコーダーに類似のレコーダーデバイスに保存されてよく、それによって、レコーダーは、例えば輸送機事故、医療事象などの後の分析のために、後から回収することができる。
Alternatively, or in addition,
所望に応じて、装置９１０は、例えば、カメラ９２２の内部に、及び／又はヘルメット９１２の上若しくは中に、装置９１０の様々なコンポーネントを制御するために、１つ以上のコントローラー（図示せず）を含んでよい。例えば、コントローラーは、１つ以上のＬＥＤ９３４と連結されてよく、それによって、ＬＥＤ９３４は、所定のパルス、又は例えばパルスの周波数、長さ、及び／若しくは振幅のうちの１つ以上を変化させる可変パルスで光を発光し、例えば、装置９１０のエネルギー消費が低減される。加えて、装置９１０は、装置９１０の１つ以上のコンポーネントに電力を供給するための、電池及び／又はケーブルを例とする１つ以上の電源も含んでよい。例えば、１つ以上の電池（図示せず）が、イメージングデバイス９４０及び／又は（１若しくは複数の）ＬＥＤ９３４に電力を供給するために、カメラ９２２の中に備えられてよい。
If desired, the
図１５を参照すると、別の選択肢としてのバイオセンサーアセンブリー１０２０が示され、これは、本明細書で述べるその他の実施形態のいずれに備えられてもよく、及び／又は所望に応じて、本明細書で述べるその他の実施形態のコンポーネントのいずれを含んでいてもよい。アセンブリー９２０とは異なり、複数の光源１０３０が、フレーム１０１２上のいくつかの場所に備えられている。例えば、各光源１０３０は、例えば、約６４０〜７００ナノメートルの１つ以上の波長の赤外光、白色光を例とする広帯域可視光などの比較的狭い又は広い帯域の光を発光するように構成された発光ダイオードを含んでよい。本明細書のその他の実施形態と同様に、光源１０３０は、例えば、ユーザーの眼及び／又は顔面を照らすために、レンズ、拡散器、又はその他の機能部（図示せず）を含んでよい。光源１０３０は、例えば、１つ以上の縦方向アレイ、又はフレーム１０１２の対応する開口部１０１２ｃの周囲に位置するその他のアレイとして、互いに間隔を空けて配置されてよい。
With reference to FIG. 15, a biosensor assembly 1020 is shown as an alternative, which may be provided with any of the other embodiments described herein and / or, optionally, the present. It may include any of the components of other embodiments described herein. Unlike the
加えて、ユーザーの片眼又は両眼のモニタリングのために、及び所望に応じて、ユーザーの周囲のモニタリングのために、個別のマイクロカメラ１０２４、１０４６が備えられてよい。例えば、示されるように、ＣＭＯＳ、ＣＣＤ、又はその他の検出器１０２４が、各開口部１０１２ｃの下側を例とするフレーム１０１２上に、装置１０１０を装着しているユーザーの対応する眼の方向に検出器１０２４が向けられるように備えられてよい。図１６に示されるように、各検出器１０２４は、例えば、フレームを装着している人の全体的な視野から外して検出器１０２４を配置するために、フレーム１０１２の対応する開口部１０１２ｃからずらされてよい。例えば、示されるように、フレームは、例えばフレーム１０１２によっておおよそ定められる平面に対して直交する、開口部１０１２ｃを通って延びる注視軸線（eye-gaze axis）１０１３をおおよそ定めてよい。注視軸線１０１３は、開口部１０１２ｃを通してまっすぐ前方を見ている場合のフレームを装着している人が見る方向に対応し得る。検出器１０２４は、例えば検出器１０２４のアクティブ領域の視野の中心を識別する検出器１０２４のセンターラインイメージング軸（centerline imaging axis）１０２５が、注視軸線１０１３からずれるようにフレーム１０１２に搭載されてよい。１つの実施形態では、注視軸線１０１３及びセンターラインイメージング軸線１０２５は、例えば検出器１０２４の方向を調節する前又は後に、軸線間に鋭角が定められるように互いに交差していてよい。
In addition, separate microcameras 1024, 1046 may be provided for monitoring one or both eyes of the user and, if desired, for monitoring the user's surroundings. For example, as shown, a CMOS, CCD, or
例えば、各検出器１０２４には、検出器１０２４の方向の調節を可能とし得る回転式マウント（swivel mount）１０２６が備えられてよい。１つ以上のレンズ、フィルターなども（図示せず）、検出器１０２４を覆うように回転式マウント１０２６に固定されるか、又はカメラ９２２と同様に、例えばそのアクティブ領域を覆うように、検出器１０２４に直接固定されてよい。
For example, each
（１若しくは複数の）回転式マウント１０２６は、１つ以上の軸線を中心に調節可能であってよく、例えば、センターラインイメージング軸線１０２５など、ユーザーの眼又は顔面の方向を向いたピボット軸線を中心に、例えば斜め上方向及びフレーム１０１２から離れるように回転可能であってよい。回転式マウント１０２６は、例えば、個々のユーザーの眼を、検出器１０２４のアクティブ領域内の中心に合わせるために、検出器１０２４の配向を調節可能とするものであってよい。回転式マウント１０２６は、所望される方向で回転式マウント１０２６（及び結果として検出器１０２４）を選択的にロックするが、必要に応じて回転式マウント１０２６の解放及び調節も可能とするための止めネジ、対応するネジ山（mating threads）、カラー（collar）、及び／又はその他の機能部（図示せず）を含んでよい。
The rotary mount 1026 (s) may be adjustable around one or more axes, centered on a pivot axis oriented towards the user's eyes or face, such as the
検出器１０２４は、イメージの焦点を検出器１０２４のアクティブ領域上に合わせるためのレンズ（図示せず）を含んでよい。所望に応じて、例えば検出器１０２４によって得られたイメージから所望されない波長の光を除去するために、フィルター（図示せず）が検出器１０２４上に備えられてよい。例えば、フィルターは、除去されなければ検出器１０２４のアクティブ領域に受光され、イメージ上におけるグリント若しくはその他の所望されないアーチファクトの発生、検出器１０２４の飽和などを起こし得る可視光及び／若しくは紫外光の強度を低下させるか、又はそれらを完全に除去してよい。
The
加えて、又は別の選択肢として、本明細書の他所で述べるように、除去されなければ眼のイメージ上にグリント又はその他のアーチファクトを発生させ得る外部光の所望されない帯域幅を除去するために、フレーム１０１２上に着色レンズが備えられてよい。例えば、レンズは、除去されなければモニタリングされている眼の瞳孔、眼瞼、又はその他の構造による所望されない反応を引き起こし得る所望される波長光の強度の低下又は除去を、使用中に静的又は動的に行ってよい。さらなる別の選択肢として、検出器１０２４は、約６４０〜７００ナノメートルの波長の赤外光を例とする所望される帯域幅内の光のイメージのみを取り込むように構成されてもよい。
In addition, or as an alternative, to eliminate the undesired bandwidth of external light that, as described elsewhere herein, can cause glint or other artifacts on the image of the eye if not removed. A colored lens may be provided on the
加えて、装置１０１０を装着している人の周囲のイメージを取り込むために、１つ以上のエキソ−カメラ１０４６、１０４７がフレーム１０１２上に備えられてよい。示されるように、第一の検出器１０４６は、これまでの実施形態と同様に、フレーム１０１２のブリッジ１０１２ａに搭載され、ユーザーから外側に向けられる。検出器１０４６は、本明細書のその他の検出器又はカメラと同様に、フレーム１０１２に固定されて搭載されてよく、又は、例えば回転式マウント、折り曲げ式チップ（bendable tip）などを含んで調節可能とされてもよい。
In addition, one or more exo-
所望に応じて、例えば検出器１０４６に加えて、又はその代わりに、１対の検出器１０４７が、例えばリム１０１２ｂの左側及び右側に備えられてよい。例えば、左右の検出器１０４７は、検出器１０４７によって三次元で取り込まれたイメージ中の物体の三角測量又はその他の識別を容易とすることができる。
If desired, in addition to, for example, the detector 1046, or instead, a pair of
エキソ−カメラ１０４６、１０４７は、所望される場合、類似の若しくは異なる視野、解像度、焦点距離、又はその他の特徴を有していてよい。例えば、異なるエキソ−カメラは、相対的に広い若しくは狭い視野を有してよく、フレームを装着している人の周囲の異なる領域のイメージを取り込むために異なる軸線に沿って延びていてよく、並びに／又はより高い及び低い相対的解像度、焦点距離などを有していてよい。例えば、相対的に低い解像度、広い角度の第一の検出器が、相対的に高い解像度、狭い角度の第二の検出器と合わせて用いられてよく、例えば、それによって、第一の検出器からのイメージは、全般的分析のために用いられてよく、一方第二の検出器からのイメージは、より高い粒度が所望される場合に用いられてよい。加えて、又は別の選択肢として、第一及び第二の検出器は、例えば、一方が、他方よりも近い物体又はシーンのイメージを取得するように、異なる焦点距離を有していてよい。
Exo-
各検出器１０２４、１０４６は、それぞれの検出器１０２４、１０４６によって発生される、すなわち、検出器１０２４、１０４６のアクティブ領域で受けるイメージに相当するシグナルが、検出器１０２４、１０４６から他所へ通信されるように、個々のケーブル、ワイヤセットなどと連結されていてよい。例えば、図１５に示されるように、検出器１０２４、１０４６と連結された個々のケーブル又はワイヤセットを含むケーブル１０４８は、フレーム１０１２から延びていてよい。ケーブル１０４８はまた、対応する光源１０３０と連結された個々のケーブル又はワイヤセットも含んでよい。個々のケーブル又はワイヤセットは、所望される場合、例えば、装置１０１０の全体プロファイルを低減するために、例えば、対応する検出器１０２４、１０４６又は光源１０３０からリム１０１２ｂに沿ってケーブル１０４８内部に取り込まれるまで、フレーム１０１２の中に埋め込まれていてよい。
Each
ケーブル１０４８は、例えば、本明細書のその他の実施形態と同様に、フレーム１０１２とは別のプロセッサーボックス１０５０まで延びていてよい。例えば、プロセッサーボックス１０５０は、光源１０３０を制御するための１つ以上のコントローラー又はプロセッサー、検出器１０２４、１０４６からのイメージシグナルを保存するためのメモリなどを含んでよい。加えて、プロセッサーボックス１０５０は、例えば装置１０１０のコンポーネントを作動させるために、１つ以上の電源を含んでよい。別の選択肢として、本明細書のその他の実施形態と同様に、１つ以上のプロセッサー、電源などは、フレーム１０１２上に備えられてよい。所望に応じて、フレーム１０１２はまた、本明細書のその他の実施形態と同様に、データの送信、指示の受信などのための１つ以上の送信機及び／若しくは受信器（図示せず）、１つ以上のセンサー（これも図示せず）、並びに／又はその他のコンポーネントも含んでよい。
The
装置９１０（又は１０１０）を含むシステムは、本明細書の他所で述べるその他の実施形態と同様に、装置９１０からリモートにあるコンポーネントを含んでよい。例えば、図８Ａ及び８Ｂの装置９１０を参照すると（装置１０１０にも同等に適用されるが）、システムは、１つ以上の受信器、プロセッサー、及び／又はディスプレイ（図示せず）を、装置９１０からのリモート位置に含んでよく、例えば、同じ室内、近くにあるモニタリングステーション、又はより遠くにある位置である。受信器は、カメラ９２２からのイメージシグナル及び／又は装置９１０上のその他のセンサーからのシグナルを含む装置９１０上の送信器によって送信されるシグナルを受けることができる。
A system that includes device 910 (or 1010) may include components that are remote from
プロセッサーは、装置９１０からのシグナルを分析して、例えばグラフィック表示用にそのシグナルを処理するために、受信器に連結されてよい。例えば、図１０Ａ及び１０Ｂに示されるイメージと同様に、プロセッサーは、カメラ９２２からのビデオシグナルを、モニター上に表示するために処理してよく、それによって、例えば、医療専門家、管理者、又はその他の共同作業者などの第三者が、ユーザーをモニタリングすることが可能となる。本明細書の他所で述べるその他の実施形態と同様に、単一モニター又は別々のディスプレイ上に、同時にその他のパラメーターが表示されてよい。プロセッサーは、ユーザーの眼のビデオシグナル及び／又はエキソ−カメライメージを、単独で、又はその他の感知されたパラメーターと合わせて重ね合わせるか、又はそうでなければ同時に表示して、医師又はその他の個人が、これらのパラメーターをモニタリングし、ユーザーの行動と個人的に相関させることを可能とすることができる。
The processor may be coupled to a receiver to analyze the signal from
加えて、図１１Ａ〜１１Ｃを参照すると、プロセッサーは、ディスプレイ上に、例えばビデオイメージ上にグラフィックを重ね合わせて、眼３００の瞳孔３０１の識別及び／又はモニタリングを容易とすることができる。示されるように、瞳孔３０１のエッジと周囲の虹彩３０４との間のコントラストのために、プロセッサーは、この境界を近似し、片眼又は両眼（図１１Ａ〜１１Ｃでは、単純化のために片眼３００しか示していない）のイメージデータ上に重ね合わせることができるグラフィックのハロー（halo）、楕円、又はその他のグラフィック３０６を作り出すことができる。観察者は、このグラフィック３０６を用いて、装置９１０のユーザーのモニタリングを容易とすることができる。
In addition, with reference to FIGS. 11A-11C, the processor can superimpose graphics on the display, eg, on a video image, to facilitate identification and / or monitoring of
加えて、又は別の選択肢として、プロセッサーは、ハローのサイズ及び／又は形状を仮想的に特定して、ディスプレイ上に実際にハローを表示することなく、ユーザーのモニタリングを容易とすることができる。例えば、プロセッサーは、瞳孔３０１のエッジを識別して、実際にハローを表示することなく、識別されたエッジに基づいて瞳孔３０１のサイズ及び形状を特定することができる。従って、プロセッサーは、ハロー３０６及び／若しくは瞳孔３０１の断面積又は直径を、リアルタイムで特定することができる。加えて、又は別の選択肢として、プロセッサーは、サイズ及び／又は形状を用いて、ハロー３０６又は瞳孔３０１の中心を識別し、それによって、例えばｘ−ｙ座標系において、ハロー３０６又は瞳孔３０１の中心の座標を特定することができる。
In addition, or as an alternative, the processor can virtually identify the size and / or shape of the halo, facilitating user monitoring without actually displaying the halo on the display. For example, the processor can identify the edge of the
加えて、又は別の選択肢として、プロセッサーは、瞳孔３０１（又はグラフィック３０６）のサイズ及び／又は形状に関する情報を自動的に分析し、それによって、ビデオシグナルを相関させて、人の傾眠レベル、又はその他の身体的及び／若しくは精神的状態を特定することができる。この分析は、瞳孔の相対的位置、瞳孔のサイズ、及び／又は瞳孔の離心率を、例えば経時でモニタリングすることを含んでよい。例えば、プロセッサーは、瞳孔３００の直径を経時でモニタリングすることができ、それは、例えばリアルタイムで、チャートの形態で表示されるか、時間の関数としてメモリに保存されるか、及び／又は眼のイメージ上に重ね合わされてよい。
In addition, or as an alternative, the processor automatically analyzes information about the size and / or shape of pupil 301 (or graphic 306), thereby correlating the video signal to the level of somnolence in the person, or Other physical and / or mental states can be identified. This analysis may include monitoring the relative position of the pupil, the size of the pupil, and / or the eccentricity of the pupil, eg, over time. For example, the processor can monitor the diameter of the
例えば、図１１Ａは、例えば直径「ｄ１」を有するグラフィック３０６に相当する、周囲条件下でのリラックス状態の瞳孔３０１を示し得る。図１１Ｂに示されるように、ユーザーが眼３００を瞬きするか、又は閉じている場合、瞳孔３０１は、直径「ｄ２」を有するグラフィック３０６で表されるように拡張していてよく、それによって、眼３００が再度開かれた際に、瞳孔３０１は最初は拡張されている。プロセッサーは、グラフィック３０６又は瞳孔３０１自体の直径の変化を比較して、瞬き又はその他の眼を閉じた後に瞳孔３０１が直径「ｄ１」に戻るための遅延を特定することができる。この遅延、又は可視若しくは不可視光のフラッシュに対する反応性の喪失は、少なくとも部分的に、傾眠のレベル、中毒を例とする障害のレベル、及び／又は低酸素血症、低血糖症、脳卒中、心筋梗塞、毒素、毒物などに起因する脳損傷若しくは脳死などの致死的又は末期的イベントを含む医学的イベントの発生を示し得る。
For example, FIG. 11A may show a
加えて、又は別の選択肢として、プロセッサーは、例えば、瞳孔が眼瞼３０２によって部分的に覆われている場合に、瞳孔のおおよその離心率を特定することができる。例えば、図１１Ｃに示されるように、眼瞼３０２が部分的に閉じられている場合、イメージ上に重ね合わされたハロー３０６（又は、そうでなければ、実際に表示することなく、プロセッサーによって仮想的に特定される）は、瞳孔３０１の露出した部分の幅「ｗ」及び高さ「ｈ」に相当する楕円形状を採り入れてよい。高さ「ｈ」は、直径「ｄ１」と関連付けることができ、すなわち、高さ「ｈ」の直径「ｄ１」に対する比率は、１以下であり得る（ｈ／ｄ１≧１）。例えば、この比率は、瞳孔３０１が眼瞼３０２によって完全に覆われると、１からゼロに減少し得る。
In addition, or as an alternative, the processor can identify the approximate eccentricity of the pupil, for example, when the pupil is partially covered by the
同様に、幅「ｗ」も、例えば、眼瞼３０２が瞳孔３０１の半分を超えて覆い始める際の眼瞼３０２が瞳孔３０１を覆う度合いの指標として、直径「ｄ１」と関連付けることができる（ｗ／／ｄ１≧１）。加えて、又は別の選択肢として、高さ及び幅の比率（ｈ／ｗ≧１）は、例えば、眼瞼３０２による覆い度合いに基づいて、瞳孔３０１の離心率に関する情報と関連付けることができる。同様に、瞳孔３０１の面積（例：ハロー３０６内）を、算出し、モニタリングすることができる。そのようなパラメーターは、個別に、集合的に、並びに／又はその他の眼部計測及び／若しくは生理学的パラメーターと共に分析して、ユーザーの今後の行動をモニタリング、分析、及び／又は予測することができる。
Similarly, the width "w" can also be associated with the diameter "d 1 " as an indicator of the degree to which the
図１２Ａを参照すると、本明細書で述べる装置及びシステムのいずれかのユーザーの覚醒度を試験するための代表的方法が示される。例えば、ユーザーは、上記でさらに述べたように、ユーザーの眼の片方又は両方をモニタリングする図８に示される装置８１０を装着してよい。工程２２１０において、関連する状態下でのユーザーの片眼若しくは両眼のベース又はパラメーターが特定されてよい。例えば、弛緩時の瞳孔径が、周囲条件下で測定されるか、又はそうでなければモニタリングされてよい。
With reference to FIG. 12A, a representative method for testing the alertness of a user of any of the devices and systems described herein is shown. For example, the user may wear the
工程２２２０において、光の１つ以上のパルスが、片眼又は両眼に向かって発光されてよく、それは、例えばパルス化光のフラッシュの周波数と実質的に同じ周波数で、片眼又は両眼の弛緩状態からの拡張及び／又は収縮を引き起こし得る。例えば、装置８１０上の１つ以上の発光器が、所定の順序で作動されて、片眼又は両眼の拡張を引き起こしてよい。その後、工程２２３０において、ユーザーの片眼又は両眼が、カメラ８３０又はセンサー８２２により、例えば潜在意識下又は無意識下でモニタリングされて、眼が弛緩状態に戻るための反応時間が特定されてよい。反応時間は、実験データベース又はその他のデータと比較されて、ユーザーの意識がある、覚醒している、及び／又は生存していることが確認されてよい。所望される場合、工程２２２０及び２２３０は、所望される場合、例えば偽陰性特定を回避するために、１回以上繰り返されて、反応時間が確認され、及び／又は平均反応時間が取得されてよい。
In
例えば、閾値試験時では、光の単一のフラッシュを発生させて、瞳孔の応答をモニタリングすることで充分であり得る。別の選択肢として、例えば、傾向の研究のために、又は単一フラッシュの場合に起こり得る誤データを排除するために、一連のフラッシュが用いられて瞳孔応答が経時でモニタリングされてもよい。一連のフラッシュの場合、パルスレートは、光のフラッシュに応答して拡張した後、瞳孔が自然にその弛緩状態へ戻るのに要する時間よりも長くするべきであり、例えば、少なくとも約５０から１００ミリ秒である（５０〜１００ｍｓ）。別の選択肢として、近赤外光（約６４０〜７００ナノメートルの波長を有する）を例とする光のパルスが、ユーザーの片眼又は両眼に向けられてよい。システムは、瞳孔の応答における周期的な変動を検出し得る。そのような応答は、例えば暗闇で「見ている」又は暗闇で赤外光源を感知している暗視状態に恐らくは関連する原始的な眼部計測応答（oculometric response）に起因するものであり得る。 For example, during a threshold test, it may be sufficient to generate a single flash of light and monitor the response of the pupil. Alternatively, a series of flashes may be used to monitor the pupillary response over time, for example, for trend studies or to eliminate possible false data in the case of a single flash. For a series of flashes, the pulse rate should be longer than the time it takes for the pupil to spontaneously return to its relaxed state after dilating in response to the flash of light, eg, at least about 50-100 mm. Seconds (50-100 ms). Alternatively, a pulse of light, such as near-infrared light (having a wavelength of about 640-700 nanometers), may be directed to one or both eyes of the user. The system can detect periodic fluctuations in the pupillary response. Such a response may result from, for example, a primitive ocular measurement response that is probably associated with a night-vision condition that is "seeing" in the dark or sensing an infrared light source in the dark. ..
そのような瞳孔応答試験はまた、例えばユーザーが死亡している場合でも、眼が閉じていること及び／又は眼の動きをシステムがまったく検出することができないといった偽陽性の識別に用いられてもよい。同様に、瞳孔応答試験はまた、ユーザーが睡眠状態又は意識が無い状態であるかどうかを判定することも可能であり得る。加えて、瞳孔応答試験は、ユーザーがアルコール、薬物などの影響下にあるかどうかの判定に用いられてもよく、それは、光のフラッシュに応答しての拡張後、瞳孔がその弛緩状態に収縮して戻る速さに影響を与え得る。加えて、又は別の選択肢として、瞳孔応答試験はまた、眼部計測測定値と対応する化学的に特定された血中レベルとの間の相関に依存して、ユーザーの体内の薬物若しくはアルコールの血中濃度又は量を特定するために用いられてもよい。 Such pupillary response tests may also be used to identify false positives, for example when the user is dead, but the eye is closed and / or the system cannot detect any eye movements. good. Similarly, the pupillary response test may also be able to determine if the user is asleep or unconscious. In addition, a pupillary response test may be used to determine if the user is under the influence of alcohol, drugs, etc., which causes the pupil to contract to its relaxed state after dilation in response to a flash of light. And can affect the speed of return. In addition, or as an alternative, the pupillary response test also depends on the correlation between the eye measurements and the corresponding chemically identified blood levels of the drug or alcohol in the user's body. It may be used to identify blood levels or amounts.
図ｑ２Ｂを参照すると、閾値覚醒度を試験するための別の方法が示される。この方法は、概略的に、工程２２４０において、所望される方法で片眼又は両眼を意識的に動かすようにユーザーに指示する刺激を与えること、及び工程２２５０において、例えば、ユーザーがその指示に従って片眼又は両眼を所望される方法で動かしたかどうかを確認する意識的な動きについて、眼のモニタリングを行うことを含む。本明細書で述べる装置のいずれも、スピーカー、ライト、バイブレーター、又はその他の触覚デバイスを例とする１つ以上の刺激デバイスを含んでよい。別の選択肢として、そのようなデバイスは、例えば、輸送機のダッシュボード上、ビデオディスプレイ上など、ユーザーから離れて備えられてもよい。
Reference to FIG. q2B shows another method for testing threshold alertness. This method generally gives a stimulus in
例えば、ユーザーは、装置上の視認可能なライトが作動された場合に、片眼又は両眼を所定の時間閉じるように指示されてよい。ライトが作動すると、システムは、片眼又は両眼をモニタリングして、ユーザーが所定の時間枠内で、及び／又は所定の方法（例：所定の順番での１回以上の瞬き）で応答することを確認してよい。別の選択肢として、光のフラッシュの代わりにその他の刺激が提供されてもよく、ディスプレイ（装置上又は装置とは別の）上の視認可能な指示、聴取可能なシグナル（例：デバイス上又はデバイス近傍のスピーカーからの声による指令）、触覚シグナルなどである。これらの実施形態では、ユーザーは、一連の動作を行うように指示されてよく、例えば、上又は下、左又は右を見る、所望される順序で瞬きを行う、指示されるまで眼を閉じている、ディスプレイ上のポインターを追うなどである。そのような試験は、例えば、試験対象が、一連の試験の過程で、又は種々の動作を行っている間、覚醒している、認識している、及び／又は警戒しているかどうかを確認するために有用であり得る。 For example, the user may be instructed to close one or both eyes for a predetermined time when a visible light on the device is activated. When the light is activated, the system monitors one or both eyes and the user responds within a predetermined time frame and / or in a predetermined manner (eg, one or more blinks in a predetermined order). You may confirm that. Alternatively, other stimuli may be provided in place of the flash of light, visible instructions on the display (on or separate from the device), audible signals (eg, on or on the device). Command by voice from a nearby speaker), tactile signal, etc. In these embodiments, the user may be instructed to perform a series of actions, eg, looking up or down, left or right, blinking in the desired order, closing eyes until instructed. Yes, follow the pointer on the display, etc. Such tests determine, for example, whether the test subject is awake, aware, and / or alert during the course of a series of tests or during various movements. Can be useful for.
別の実施形態では、本明細書の他所で述べるものなどの装置及びシステムは、例えばコンピューターマウス、ジョイスティックなどと同様に、コンピューターシステムの制御に用いられてよい。例えば、図３に示され、それを参照して記載される装置８１０を参照すると、（１若しくは複数の）カメラ８３０は、コンピュータースクリーン又はその他のディスプレイ上のマウスポインタ―の指向及び／又は作動のために、ユーザーの（片方又は両方の）瞳孔の位置をモニタリングするのに用いられてよい。カメラ９２２からのイメージデータを受けるプロセッサーは、イメージデータを分析して、検出器９４０のアクティブ領域９４２内の（片方又は両方の）瞳孔の相対位置を特定してよい。所望に応じて、１つ以上のディスプレイが、ユーザーの眼の片方又は両方の前又は視野内に配置されるフレーム８１２に対して固定されてよい。例えば、フラットパネルＬＣＤ又はその他のディスプレイ（図示せず）が、レンズの代わりにフレーム８１２に搭載されてよい。そのような装置は、例えば医療又はその他の研究施設内でのシミュレーション用、例えばビデオゲームコンソールとしての娯楽用途などに用いられてよい。
In another embodiment, devices and systems, such as those described elsewhere herein, may be used to control computer systems, such as computer mice, joysticks, and the like. For example, referring to
図１３を参照すると、本明細書で述べる装置又はシステムのいずれかを用いる検出された眼の動きに基づくコンピュータデバイスを制御するための代表的方法が示される。例えば、ユーザーの片方又は両方の眼をイメージングするための光ファイバーバンドル９２４を含む図８Ａに示される装置９１０が用いられてよい。所望に応じて、以下でさらに説明されるように、装置はまた、１つ以上のエキソ−カメラも有していてよく、例えば、ユーザーの片方又は両方の眼の近傍に配置され、ユーザー前方の視界に沿って外側に向けられてよい。まず、工程２３１０において、そのような装置を含むシステムの初期化、すなわち、ベース又はレファレンス位置、直交成分を持つレファレンスフレームなどのレファレンスフレームの確立を行うことが望ましいものであり得る。例えば、ユーザーは、ディスプレイ上のポインター又はその他の所定の位置を見るように指示されてよく、それによって、ユーザーの片眼又は両眼、及び従ってユーザーの片方又は両方の瞳孔が、実質的に静止状態に維持される。プロセッサーは、ユーザーの片眼又は両眼が実質的に静止状態にある間に、カメラ８３０からのイメージデータを分析し、例えば、レファレンスポイント又は「ベース位置」に相当するイメージ上の瞳孔の位置を特定してよい。例えば、ポインター又はベース位置は、ユーザーの瞳孔の実質的にまっすぐ前に位置してよい。所望に応じて、ユーザーは、ディスプレイ上の２つ以上の識別された位置を順に見るように指示されてよく、それによって、ユーザーの眼の相対的動きのスケールが得られる。この別の選択肢では、例えば、ディスプレイに対するおおよその眼の動きの限界を識別するために、ユーザーに、ディスプレイの対向するコーナー部を見させることが望ましいものであり得る。
With reference to FIG. 13, representative methods for controlling computer devices based on detected eye movements using any of the devices or systems described herein are shown. For example, the
初期化が完了すると、ユーザーは、例えばポインター及び／又はディスプレイの残りの部分に対して、例えば左及び右、上及び下に、片眼又は両眼を自由に動かしてよい。工程２３２０において、システムは、眼のそのような動きをモニタリングしてよく、すなわち、プロセッサーが、イメージデータを分析して、（１若しくは複数の）ベース位置からのユーザーの（片方又は両方の）瞳孔の相対位置を特定してよい。例えば、ユーザーが自身の（片方又は両方の）眼を、ベース位置から上及び右に、すなわち、コンピュータースクリーン上ポインターに対して上及び右に動かす場合に、プロセッサーが、この動きを特定してよい。それに応じて、工程２３３０において、プロセッサーは、ポインターを上及び右に動かしてよく、すなわち、それによって、ユーザーの注視が追跡される。ユーザーが、自身の（片方又は両方の）眼の動きを止めると、プロセッサーは、ユーザーが現在見ているディスプレイ上の位置に到達したところで、ポインターを止めてよい。
Once the initialization is complete, the user is free to move one or both eyes, eg, left and right, up and down, with respect to, for example, the pointer and / or the rest of the display. In
所望に応じて、工程２３４０において、ユーザーは、ポインターがディスプレイ上の所望される位置まで移動したところで、例えば、マウスのボタンを押すことに類似した命令を実行することが可能であってよい。例えば、プロセッサーは、所定の順序での１回以上の目的を持った瞬きを例とするユーザーからのシグナルについてのイメージデータをモニタリングしてよい。これは、数秒間の長さを例とする所定の長さの１回の瞬きといった単純なものから、例えばユーザーの片方又は両方の眼を含む一連の瞬きといったより複雑なものであってもよい。別の選択肢として、シグナルは、例えば３秒、５秒、又はそれ以上の秒数の長さの瞬きを行わない所定の時間であってもよい。プロセッサーがシグナルを識別すると、プロセッサーは、命令を実行してよい。例えば、ユーザーは、ディスプレイ上のアイコン、文字命令などに到達した時点で、（片方又は両方の）眼の動きを止めてよく、プロセッサーは、ポイントを、それがアイコン又は命令の上に重なるか、又はそうでなければその位置に来るまで移動させてよい。次に、ユーザーは、上記で説明されるように、コンピューターマウス上のボタンの「ダブルクリック」と同様に、瞬き又は動作を行ってよく、それによって、プロセッサーは、選択された命令を完了するか、又は選択された命令を所望される行先へ通信するよう指示される。例えば、選択された命令は、コンピュータープログラムが実行されるか、又は装置若しくはその他のデバイスが作動、作動停止、又はそうでなければ所望される方法で制御される結果となってよい。従って、システムは、プロセッサー及び／又はディスプレイと連結されたコンピュータデバイスの制御から、ライトのスイッチ若しくは輸送機のオン又はオフまで、様々な作業を完了するために用いられてよい。そのような装置及び／又はシステムは、それによって、ハンズフリーで、すなわち、ユーザーの（片方又は両方の）眼の動きだけでコンピューターを使用するための方法を提供することができる。
If desired, in
例えば、１つの用途において、システムは、ヘリコプター、ジェット機、又はその他の飛行機などの輸送機の操作に、例えば武器、ナビゲーションシステム、又はその他の輸送機搭載システムの作動又はそうでなければ制御のために用いることができる。別の用途では、システムは、ビデオゲーム又はその他のシミュレーションにおいて、例えばバーチャルリアリティーへの没頭を高めるために、用いることができる。例えば、システムは、複数のメニュー、シーン、又はその他のアクティビティを通して、ユーザーが素早く操作することを可能とすることができ、同時に、その他の機能、例えば、眼で制御される機能に加えて、又はそれと同時にその他の活動を行うようにユーザーの両手を自由にして、それによって、より多くの及び／又はより複雑な作業を同時に行うことを可能とすることができる。 For example, in one application, the system is used to operate transport aircraft such as helicopters, jets, or other airplanes, for example for the operation or otherwise control of weapons, navigation systems, or other transport-mounted systems. Can be used. In another application, the system can be used in video games or other simulations, for example to increase immersion in virtual reality. For example, the system can allow the user to interact quickly through multiple menus, scenes, or other activities, while at the same time in addition to or other functions, such as those that are controlled by the eyes. At the same time, the user's hands can be freed to perform other activities, thereby allowing more and / or more complex tasks to be performed simultaneously.
加えて、ディスプレイ上のポインターに対する眼の動きの追跡を向上するために、及び／又はそうでなければ容易とするために、１つ以上のエキソ−カメラが用いられてもよい。例えば、ディスプレイの方向に向けられたエキソ−カメラが、少なくとも片方の眼の近傍に、例えば、眼からの所定の距離に、又はその他の関係で備えられてよい。従って、エキソ−カメラは、ディスプレイのイメージを提供することができ、例えば、エンド−カメラでモニタリングされる眼の動きと同期し得るポインターの動きをリアルタイムで示す。プロセッサーは、三角測量又はその他のアルゴリズムを用いてこのデータを関連付け、眼の動きによるポインターの追跡の精度を高めることができる。このことにより、命令上にポインターを置いて瞬きを行うことによって命令を実行することをユーザーが意図する場合、例えばディスプレイが複数の利用可能な命令を示している場合に、意図する命令が実際に選択されるという精度を確保することができる。 In addition, one or more exo cameras may be used to improve tracking of eye movements with respect to the pointer on the display and / or to facilitate otherwise. For example, an exo-camera pointed in the direction of the display may be provided in the vicinity of at least one eye, eg, at a predetermined distance from the eye, or in some other relationship. Thus, the exo-camera can provide an image of the display, eg, showing in real time the movement of the pointer that can be synchronized with the movement of the eye monitored by the end-camera. The processor can correlate this data using triangulation or other algorithms to improve the accuracy of pointer tracking by eye movements. This allows the intended instruction to actually be executed when the user intends to execute the instruction by placing a pointer over the instruction and blinking, for example when the display shows multiple available instructions. The accuracy of being selected can be ensured.
別の選択肢としての実施形態では、プロセッサーは、エンド−カメラからのビデオシグナルを受けて処理することで、エンド−カメライメージに基づいて眼の瞳孔のエッジを識別し、識別されたエッジを用いて、エンド−カメライメージのレファレンスフレームに対する瞳孔の座標を近似してよい。例えば、プロセッサーは、エンド−カメライメージに対してｘ−ｙ座標系を割り当て、瞳孔のエッジを用いて、この座標系中での瞳孔の中心を識別してよい。プロセッサーはまた、エキソ−カメラからのイメージを受けて処理し、エンド−カメライメージから得られた座標を、その人の周囲のエキソ−カメライメージと相関させて、そのデバイスを装着している人が見ている位置を、その人の周囲に対して近似してもよい。 In another alternative embodiment, the processor receives and processes a video signal from the end-camera to identify the edge of the pupil of the eye based on the end-camera image and use the identified edge. , The coordinates of the pupil with respect to the reference frame of the end-camera image may be approximated. For example, the processor may assign an xy coordinate system to the end-camera image and use the edges of the pupil to identify the center of the pupil in this coordinate system. The processor also receives and processes the image from the exo-camera, correlates the coordinates obtained from the end-camera image with the exo-camera image around the person, and the person wearing the device wears it. The viewing position may be approximated to the person's surroundings.
例えば、エキソ−カメライメージを表示するために、ディスプレイ（図示せず）がプロセッサーに連結されてよく、プロセッサーは、例えば一式の十字線、カーソルなどのグラフィックをディスプレイ上に示されたエキソ−カメライメージ上に重ね合わせて、その人が見ている近似的な位置を識別してよい。ディスプレイが、装置に外付けされた、又はフレーム中に組み込まれたものを例とする、装置を装着している人に示されるコンピューターディスプレイである場合、その人は、そのグラフィックを用いて、ディスプレイ上でナビゲーションを行ってよい。例えば、その人は、ディスプレイ上に示される物体へとカーソルを移動させ、その物体上で、例えば瞬きするか、又は所定の時間にわたって注視することによって「クリック」して、物体に関する識別又は追加情報を要求するなどの操作の要求を行ってよい。 For example, a display (not shown) may be attached to a processor to display an exo-camera image, which may display a graphic such as a set of crosshairs, cursors, etc. on the display. It may be overlaid on top to identify the approximate position the person is looking at. If the display is a computer display that is shown to the person wearing the device, for example, external to the device or embedded in a frame, that person will use that graphic to display. You may navigate above. For example, the person moves the cursor to an object shown on the display and "clicks" on the object, for example by blinking or by gazing for a predetermined time, to identify or provide additional information about the object. You may request an operation such as requesting.
図１４を参照すると、別の実施形態では、装置２４１０を装着しているユーザーの眼３００に経皮的に光を当てるための装置２４１０が提供されてよい。装置２４１０は、図３に示されるフレーム８１２など、本明細書で述べる実施形態のいずれとおおよそ類似していてもよい。
Referring to FIG. 14, in another embodiment, a device 2410 for percutaneously illuminating the
図１７を参照すると、全体としてのシステムアーキテクチャの代表的実施形態が示され、これは、本明細書で述べる機能のいずれを行うために用いられてもよい。示されるように、このシステムアーキテクチャは、一般的に、プロセッサー１０３５及びメモリ１０４０、ハードウェアアブストラクション層（ＨＡＬ）１０３０及び外部ハードウェアとの物理接続１２３５、オペレーティングシステム１０２５、頭部装着デバイスのためのミドルウェアサービスを扱う、以降ＥＦ−ＥｙｅＰと称するコントロール１０００を含む。ミドルウェアサービスの上には、ＥＦ−ＥｙｅＰオブジェクトのサードパーティアプリケーションとのソフトウェア統合を促進するソフトウェアを含むソフトウェア層１０１５がある。ミドルウェアサービスの上にはさらに、サードパーティーハードウェア統合、並びにＩＥＥＥ １１４９．１スタンダードテストアクセスポート及びバウンダリー−スキャンアーキテクチャによってサポートされるジョイントテストアクショングループ（ＪＴＡＧ）によるシングルステップ及びブレイクポイントのようなオペレーションを含むデバグのための一式のソフトウェアツール１０２０が存在する。ツール及び統合層の上にはＡＰＩ １０１０が、続いてアプリケーション１００５が存在する。
FIG. 17 shows a representative embodiment of the system architecture as a whole, which may be used to perform any of the functions described herein. As shown, this system architecture typically includes
図１８は、オブジェクト１０００の細分を示し、最上部のアプリケーション１１００、１１０５、１１１０、１１１５、及び１１１６は、ソフトウェアオブジェクトブロック１０００内で実行される様々なユーティリティを示す。ＥｙｅＣと表示されるブロック１１１０は、ＥｙｅＣｌｉｐｓ又はＥＣと称されるビデオイメージのアスペクトを表し、これは、例えばメディアイメージの取得、編集、操作、及び／又は処理のために、本明細書で述べるシステムを用いて行うことができる方法を意味するために一般的に用いられる。 FIG. 18 shows a subdivision of object 1000, with top-level applications 1100, 1105, 1110, 1115, and 1116 showing various utilities running within software object block 1000. Block 1110, labeled EyeC, represents an aspect of the video image, referred to as EyeClips or EC, which is the system described herein, for example for the acquisition, editing, manipulation, and / or processing of media images. Is commonly used to mean a method that can be performed using.
本出願内のすべての開示事項及び請求項において、「メディアイメージ」とは、ビデオイメージ及び静止イメージの少なくとも１つとして定義される。いずれの種類のメディアイメージであっても、コンテンツクリエーターにとっての典型的な目標は、特定の視聴者にとって望ましいコンテンツを作成することである。「望ましい」の定義は、視聴者に応じて変化し得る。特にビデオイメージに関しては、ビデオイメージを選択し、編集するための１つの方法又は一式の基準は、ある視聴者には適切であり得るが、別の視聴者にはそうでない可能性がある。さらに、他のイメージと時間的に近接して取り込まれるイメージが、様々な理由から望ましい場合もある。望ましさ及び適切性のこのような様々な具体化は、単に「顕著性」と称される場合がある。 In all disclosures and claims within this application, "media image" is defined as at least one of a video image and a still image. For any type of media image, a typical goal for content creators is to create content that is desirable for a particular audience. The definition of "desirable" can vary depending on the viewer. Especially with respect to video images, one method or set of criteria for selecting and editing a video image may be appropriate for one viewer, but not for another. In addition, an image that is captured in close temporal proximity to other images may be desirable for a variety of reasons. Such various reifications of desirability and suitability are sometimes referred to simply as "significance."
メディアイメージは、注目すべきイベントを含んでいる可能性がある、特定の友人若しくは親族を含んでいる可能性がある、ソーシャルメディアにおいて他者が興味深いと考える出来事を含んでいる可能性がある、特定の場所で取り込まれた可能性がある、及び／又はユーザーが取り込みたいと考える感情を含んでいる可能性がある、といったいくつもの理由から顕著と見なされ得る。視線追跡を他のセンサーに追加することにより、このプロセスの過程にて、視線追跡の出現なしでは利用できなかったであろうレベルの分析及び制御がユーザーにとって可能となるものと想定される。 The media image may contain notable events, may contain certain friends or relatives, may contain events that others find interesting on social media, It can be considered prominent for a number of reasons, such as it may have been captured in a particular location and / or it may contain emotions that the user wants to capture. By adding gaze tracking to other sensors, it is expected that in the course of this process, users will be able to analyze and control levels that would not have been available without the advent of gaze tracking.
「編集」の語によって意図する範囲を考察する際には、注意深い考慮が必要とされる。典型的なフォト及びビデオアプリケーションの場合、「編集」は、典型的には、イメージの操作を意味し、又はビデオの場合は、トリミングしたイメージをより望ましい順番に再配列するプロセスも含む。さらなる工程が実施されることになるイメージの選択若しくはタグ付けの工程は、このような工程は、正式には編集プロセスの一部と見なされるべきではあるが、「編集」の行為には含まれない場合が多い。しかし、本出願内のこの開示事項及び請求項の目的のために、「編集」は、選択及びタグ付けの工程を含むものとする。さらに、デジタルメディア作成以前の時代には、すべての編集（選択及びタグ付けを含む）は、取り込みの時点よりもかなり後に行われる必要があった。しかし、現在、ビデオ及びスチールカメラには、取り込み時点の直後に、又は「カメラ内で」編集プロセスを行うことができる機能が含まれている。本明細書での開示事項では、編集プロセスを、取り込みの最中、又はさらには取り込みの前の時点を含むようにシフトさせることができる方法について述べる。しかし、本明細書で述べるシステム及び方法が実行されるまで、それを行うことは実用的に実現可能ではなかった。 Careful consideration is required when considering the intended range by the word "edit". For typical photo and video applications, "editing" typically means manipulating the image, or for video, also includes the process of rearranging the cropped images in a more desirable order. The process of image selection or tagging, for which further steps will be performed, is included in the act of "editing", although such steps should be formally considered part of the editing process. Often not. However, for the purposes of this disclosure and claim within this application, "editing" shall include the steps of selection and tagging. Moreover, in the pre-digital media era, all editing (including selection and tagging) had to be done well after the point of capture. However, video and still cameras now include the ability to perform the editing process "in-camera" or immediately after capture. Disclosures herein describe how the editing process can be shifted to include during or even prior to capture. However, it has not been practically feasible to do so until the systems and methods described herein have been implemented.
残念なことに、多くのユーザーにとって、取り込んだままの状態のビデオイメージを消費可能に仕上げられたビデオに変換するのに要する時間的な拘束は、このプロセスに対する最終的な障害である。この障害に直面した後の一般的な結果は、２通り存在する。第一は、プロセス全体が放棄され、ビデオイメージが視聴者と共有されることがなくなってしまうことである。第二の一般的な結果は、すべての編集が避けられ、質及び妥当性が極めて低いイメージが視聴者と共有されることである。これらの結果はいずれも、クリエーター及び視聴者の両方にとって望ましいものではない。クリエーターにとっては、このことにより、ビデオを紹介可能な形態に編集することが難しすぎることが分かることで、ビデオを記録しようとする自身の意欲が減退し得る。消費者にとっては、粗悪なビデオイメージを視聴することで、負の強化が与えられ、将来的にビデオイメージを見たいと思わなくなり得る。 Unfortunately, for many users, the time constraint required to convert an as-captured video image into a consumable finished video is the ultimate obstacle to this process. There are two general consequences after facing this obstacle. The first is that the entire process is abandoned and the video image is no longer shared with the viewer. The second general result is that all editing is avoided and images of very low quality and relevance are shared with the viewer. None of these results are desirable for both creators and viewers. For creators, this can discourage them from recording video by finding it too difficult to edit the video into an introductory form. For consumers, watching a bad video image gives them a negative boost and may make them unwilling to watch the video image in the future.
技術が進歩するに従って、コンテンツを作成するためのユーザーが携帯することのできるデバイスのフォームファクタも、移り変わってきた。かつては、コンテンツ作成デバイスには、他の技術は無かった。やがてスマートフォン及びタブレットでビデオの取り込みが可能となり、以前には想像もできなかった小型軽量化の時代を迎えた。現在、ヘッドマウントディスプレイが、消費者向けデバイスとして実現可能となり始めており、センサーなどからデータを単にログ記録するのではなく、コンテンツの作成を可能とするウェアラブル技術における移り変わりを特徴付けている。さらに、コンタクトレンズ及び人工網膜は、人の視覚系にとっての実行可能な向上である。本明細書のシステム及び方法は、ビデオの取り込み、視線追跡、及び顕著性を持つビデオの編集のこのようなモードにも適用可能であり、本発明の一部と見なされる。視線追跡を通してユーザーの注視を特定するための必要不可欠な技術を、ここでウェアラブルデバイス及びインプラトデバイスへ組込むことができることにより、眼は、デバイス入力及び編集のための実現可能なツールとなる。 As technology has advanced, so has the form factor of the devices that users can carry to create content. In the past, content creation devices had no other technology. Eventually, it became possible to capture video on smartphones and tablets, and we entered an era of smaller size and lighter weight that was unimaginable before. Currently, head-mounted displays are beginning to become feasible as consumer devices, and characterize the transition in wearable technology that enables the creation of content rather than simply logging data from sensors and the like. In addition, contact lenses and artificial retinas are feasible improvements to the human visual system. The systems and methods herein are also applicable to such modes of video capture, eye tracking, and video editing with prominence and are considered part of the present invention. The ability to incorporate essential techniques for identifying a user's gaze through gaze tracking into wearable and implantable devices here makes the eye a viable tool for device input and editing.
より良いビデオクリップを視聴者へ最終的に確実に送り届けるために克服すべき第一の障害は、興味のあるイメージの選択又は「タグ付け」である。典型的には、これは、最初の編集工程で達成される。しかし、近年、イメージのオンザフライレーティング（on-the-fly rating）を可能とする能力がスチールカメラに付与されてきた。この能力をウェアラブルデバイスで可能とするように思考を変えることは、タグ付けの異なる方法を必要するが、同時に、１つ以上のセンサーからの入力を用いてタグ付けする可能性を開くものでもある。 The first obstacle to overcome to ensure that a better video clip is ultimately delivered to the viewer is the selection or "tagging" of the image of interest. Typically, this is achieved in the first editing step. However, in recent years, steel cameras have been given the ability to enable on-the-fly ratings of images. Changing thinking to make this ability possible on wearable devices requires different ways of tagging, but at the same time opens up the possibility of tagging with input from one or more sensors. ..
タグ付け入力のための１つのそのようなセンサーは、視線追跡システムである。ビデオは、眼で見ることのできるものを取り込む目的で記録されるため、記録プロセスの過程におけるユーザーの注視に関する情報がタグ付けプロセスにとって有益であるとの推定は自然なことである。ユーザーの注視方向、注視継続時間（「ドゥエル（dwell）」）、瞳孔径、及び衝動性運動（saccadic activity）などのメトリックスは、記録イベント中におけるユーザーの意図に関する価値のある情報のいくつかの例に過ぎない。 One such sensor for tagging input is a line-of-sight tracking system. Since video is recorded for the purpose of capturing what is visible to the eye, it is natural to presume that information about the user's gaze during the recording process is useful for the tagging process. Metrics such as the user's gaze direction, gaze duration (“dwell”), pupil diameter, and saccadic activity are some examples of valuable information about the user's intentions during a recording event. It's just that.
本明細書のシステム及び方法のある実施形態は、 単眼（一眼）視線追跡技術の周辺技術として設計される視線追跡サブシステムを用いてよく、システムの他の実施形態は、双眼（両眼）視線追跡技術の周辺技術として設計される視線追跡サブシステムを用いてよい。これらの実施形態は、ウェアラブルデバイスの設計及び意図する用途に対して特有のものである。ウェアラブルデバイスのある実施形態は、ユーザーに提示される単一ディスプレイを用いてよく、一方他の実施形態は、ユーザーに提示される２つのディスプレイを用いてよい。視線追跡サブシステムの実施形態は、必ずしもこのディスプレイ構成の実施形態に適合していなくてもよい。 Some embodiments of the systems and methods herein may use gaze tracking subsystems designed as peripheral techniques of monocular (single eye) gaze tracking techniques, while other embodiments of the system are binocular (binocular) gaze. A line-of-sight tracking subsystem designed as a peripheral technology of tracking technology may be used. These embodiments are specific to the design and intended use of wearable devices. Some embodiments of the wearable device may use a single display presented to the user, while other embodiments may use two displays presented to the user. Embodiments of the line-of-sight tracking subsystem may not necessarily conform to the embodiments of this display configuration.
本明細書の視線追跡サブシステムのある実施形態は、サブシステムのための主入力としてイメージセンサーを用いてよく、一方さらなる実施形態は、ユーザーの注視方向を特定するのに適するデータを発生させることができる別の選択肢としてのセンサーを用いてよい。 Some embodiments of the line-of-sight tracking subsystem herein may use an image sensor as the main input for the subsystem, while further embodiments generate data suitable for identifying the user's gaze direction. A sensor may be used as an alternative option.
従って、その他のセンサーが、視線追跡データと連携して用いられてよいということになる。例えば、ユーザーは、後からの検索が容易となるように、「セーブ」の語を口に出すような単純な方法で言葉によってタグを付与してよく、又は友人の名前（若しくは、イメージ中の個人のその他の識別名）などのより複雑な言葉による合図を付与してもよい。複数のセンサーからの入力を組み合わせるより高度な方法は、加速度計データ、位置及び／又は方向データ、カメラセンサーからのウィンドウデータ（windowing data）、マイクデータ、タッチセンサーデータなどを含んでよく、これらの方法は、これらのセンサーの１つ以上からのデータの解釈を含んでよい。 Therefore, other sensors may be used in conjunction with the line-of-sight tracking data. For example, the user may tag by words in a simple way, such as saying the word "save", or in a friend's name (or in the image) to facilitate later searches. More complex verbal cues such as (another identification name of the individual) may be given. More advanced methods of combining inputs from multiple sensors may include accelerometer data, position and / or orientation data, windowing data from camera sensors, microphone data, touch sensor data, etc. The method may include the interpretation of data from one or more of these sensors.
例えば、シーンに向いているカメラセンサーの、ユーザーが注視している位置に対応する領域において飽和度が急速に変化することは、平均よりも高い保存される可能性を有し得るビデオのセクションの明らかな例である。加えて、デバイスのマイクにおける急速で持続的な変化は、群衆の大声での歓声に相当し得るものであり、これは、ビデオイメージが望ましいものである可能性がより高いことに相当し得る。 For example, a rapid change in saturation in the area of a camera sensor that is oriented towards the scene, corresponding to the position the user is looking at, may have a higher than average preservation of the section of the video. This is a clear example. In addition, the rapid and sustained changes in the device's microphone can correspond to the loud cheers of the crowd, which can correspond to the more likely video images being desirable.
明白で単純な使用のケースは、ユーザーが、記録したばかりのビデオを特に重要であると意識的に決定することを含む。ユーザーは、特定の出来事のどの程度をタグ付けすべきかどうかに特に関して、システムのオペレーションを補助するための追加情報を提供することが可能であってよく、例えば、ａ）ビデオの最後の１０秒間、ｂ）メディアイメージに特定の人物が出現した長さ、及び／又はｃ）ユーザーの注視が、ユーザーがクリップの終点（又は中間点）をタグ付けする直前に、同じ人物又は位置に向けられた長さである。より複雑な使用のケースは、ユーザーが重要であると考え得るイベントのそれ自身の分析を行うためのより高度なヒューリスティックスを有するシステムを含む。例えば、ユーザーは、特定の顔が出現するいずれのビデオにもタグ付けするように、又は加速度計が＞０．５Ｇを記録する間のいずれのビデオにもタグ付けするように、又は複数のセンサーデータに対するヒューリスティックスの組み合わせさえも、システムに予め指定してよい。 Cases of obvious and simple use involve the user consciously deciding that the video just recorded is of particular importance. The user may be able to provide additional information to assist in the operation of the system, especially regarding how much of a particular event should be tagged, eg a) the last 10 seconds of the video. , B) The length of appearance of a particular person in the media image, and / or c) the user's gaze was directed to the same person or position just before the user tags the end point (or midpoint) of the clip. The length. Cases of more complex use include systems with higher heuristics for performing their own analysis of events that the user may consider important. For example, the user may tag any video in which a particular face appears, or any video while the accelerometer is recording> 0.5G, or multiple sensors. Even the combination of heuristics for the data may be pre-specified in the system.
このような記載したシステムの特徴のすべては、タグ付けのプロセスを補助するために設計されており、これは、編集プロセスにおける第一工程でしかない。編集プロセスをさらに改善するための方法を以下で述べ、並びに／又は図１９及び２０に示す。 All of these described system features are designed to assist in the tagging process, which is only the first step in the editing process. Methods for further improving the editing process are described below and / or shown in FIGS. 19 and 20.
広視野のカメラがウェアラブルデバイスに普及したことで、ユーザーが、自身の仕上がったビデオシーンをより狭いウィンドウにクロッピングすることを所望し得るということが非常に起こり得る。どのような場合であっても、ユーザーが見ているシーン中の位置（「注視点」）が、ユーザーが編集中にクロッピングしたいと考え得る要素を含んでいる可能性がより高い。従って、基本ビデオの記録の過程、又は単純若しくは複雑なタグ付け手順の過程で注視点情報を取り込むことは、編集プロセスに価値を付加するものである。 With the widespread use of wide-field cameras in wearable devices, it is very likely that users may wish to crop their finished video scene into a narrower window. In any case, the position in the scene that the user is looking at (the "gaze point") is more likely to contain elements that the user may want to crop during editing. Therefore, capturing gaze information in the process of recording a basic video or in the process of a simple or complex tagging procedure adds value to the editing process.
双眼視線追跡の場合、視野内の平面（ｘ−ｙ）位置を特定することに加えて、奥行き（ｚ）位置も、注視に対して算出されてよい。このことにより、ビデオの場合であれ、又は静止フレームショットの場合であれ、カメラの焦点を、標的とする距離に合わせること、又はより専門的な記録の場合に被写界深度を調節することが可能となる。 In the case of binocular tracking, in addition to specifying the plane (xy) position in the field of view, the depth (z) position may also be calculated for gaze. This allows the camera to focus on the target distance, whether for video or still frame shots, or to adjust the depth of field for more professional recordings. It will be possible.
さらに、記録されたビデオの場合における衝動性運動は、ユーザーの側の興味の異なるレベルを示し得る。注視標的の急速な変化は、異なる興味の標的領域を示すことができるだけでなく、ユーザーの考え方の、又は記録されているシーンの要素の属性を示すメタデータとしても作用することができる。加えて、ユーザーの衝動性運動の度合いが、編集プロセスの過程でクロッピングする領域の境界を示し得ることも見出され得る。 Moreover, impulsive movements in the case of recorded video can indicate different levels of interest on the part of the user. Rapid changes in gaze targets can not only indicate target areas of different interests, but can also serve as metadata that indicates the attributes of the user's mindset or the elements of the recorded scene. In addition, it can be found that the degree of user impulsivity can indicate the boundaries of the area to be cropped during the editing process.
その他の一般的ビデオ編集機能としては、スローモーション又はファストモーションクリップの作成、フレームの異なる領域間のスムーズなパンの作成、及び揺らいでいるビデオシーンの安定化が挙げられる。これらの機能はすべて、視線追跡単独によって、又は上記で考察したように外側に向いたカメラと組み合わされた場合には、視線追跡データを１つ以上のその他のセンサーからのデータと組み合わせることによって、向上させることができる。 Other common video editing features include creating slow motion or fast motion clips, creating smooth pans between different areas of a frame, and stabilizing a fluctuating video scene. All of these features, either by eye tracking alone or, when combined with an outward-facing camera as discussed above, by combining eye tracking data with data from one or more other sensors. Can be improved.
例えば、揺らいでいるビデオの安定化は、多くの場合、フレーム間の類似の特徴を識別すること、及び隣接するフレームを標準化してこれらの特徴を静止状態に維持することによって達成される。残念なことに、これは、標準化プロセスがフレームのエッジからピクセルを除去することから、ビデオのクロッピングも必要とする。視線追跡データを組み合わせることによって、安定化の許容量の判定を補助することができ、例えば、ユーザーがフレーム全体に興味を有する場合、可能な限り多くのピクセルを保存するために、最小限のレベルの安定化が適用されてよい。しかし、ユーザーがフレームの特定の領域にのみ興味を有する場合、考え得る最良のイメージを得るために、最大の安定化が適用されてよい。 For example, stabilization of fluctuating video is often achieved by identifying similar features between frames and by standardizing adjacent frames to keep these features stationary. Unfortunately, this also requires video cropping as the standardization process removes pixels from the edges of the frame. Combining eye tracking data can help determine stabilization tolerances, for example, if the user is interested in the entire frame, the minimum level to store as many pixels as possible. Stabilization may be applied. However, if the user is only interested in a particular area of the frame, maximum stabilization may be applied to get the best possible image.
所望に応じて、瞳孔の拡張及び収縮が、例えば、特定のクリップをスローモーションクリップに引き伸ばすべきであるかどうかを判断するために、ユーザーの興奮の尺度として用いられてよい。加えて、さらなるセンサーデータを拡張データと組み合わせて、ユーザーの感情状態を判断してよく、その度合い（extension）は、興味のある領域を分析する際にビデオクリップに適用される異なる一式の基準であり得る。 If desired, pupil dilation and contraction may be used, for example, as a measure of user excitement to determine if a particular clip should be stretched into a slow motion clip. In addition, additional sensor data may be combined with extended data to determine the user's emotional state, the extension of which is a different set of criteria applied to the video clip when analyzing the area of interest. could be.
システムは、虹、日の出、日没、及び月の出など、典型的には興味のあるものである出来事を認識するように構成されてよい。これらは、デバイス中のコンパス（又はその他の方向センサー）からの入力も、ビデオイメージにタグ付けする決定の一部であり得る場合に相当する。 The system may be configured to recognize events that are typically of interest, such as rainbows, sunrises, sunsets, and moonrises. These correspond to cases where the input from the compass (or other directional sensor) in the device can also be part of the decision to tag the video image.
群衆の中の複数のウェアラブルデバイスが法執行組織にとって有用であり得る場合が存在する。爆弾の爆発及びそれに続くパニックという状況を考慮されたい。群衆の中の複数の人が、このシステムを装備したデバイスを装着している場合、すべてのデバイスは、そのマイクを通して同時に爆発音を認識し、並びに／又は周囲の光及び／若しくはピクセル飽和の大規模で急速な変化を同時に認識し得る。この広範囲に及ぶセンサーの入力及び分析は、すべてのデバイスにその事件に繋がるビデオイメージを保存するように促してよく（「クラッシュカメラ（crash camera）」）、及び／又はその事件の後にも、ビデオイメージの保存を続けるように促してよい。すべてのデバイスからのこれらのビデオイメージの集積は、証拠及び被疑者について群衆を調べるための多くの視点を法執行側に与え得る。さらに、事件の瞬間のマイク及びカメラデータの空間的分析により、事件の前、最中、及び後の瞬間を、法執行側が再構築することが可能となり得る。 There are cases where multiple wearable devices in the crowd can be useful to law enforcement organizations. Consider the situation of a bomb explosion followed by a panic. If multiple people in the crowd are wearing devices equipped with this system, all devices will simultaneously recognize the explosion through their microphones and / or have a large amount of ambient light and / or pixel saturation. You can recognize rapid changes on a scale at the same time. This extensive sensor input and analysis may prompt all devices to store video images that lead to the incident (“crash camera”) and / or video after the incident. You may be encouraged to continue saving the image. The accumulation of these video images from all devices can give law enforcement a number of perspectives for examining the crowd for evidence and suspects. In addition, spatial analysis of microphone and camera data at the moment of the incident may allow law enforcement to reconstruct the moments before, during, and after the incident.
視線追跡を装備したカメラを搭載したウェアラブルデバイスを複数の人が装着している場合において、共通の注視データが、取り込まれたビデオと組み合わされてよい。複数のウェアラブルカメラによって取り込まれたビデオを処理することによるカメラが共通の標的を見ていることの判断には、大量のイメージ処理が関与する。視線追跡データを含めることにより、共通の要素を見るという視聴者の傾向を考えると、整列のプロセスを劇的に低減し、処理パワーを削減することができる。さらに、共通の標的を見た群衆を抽出することが意図される場合、視線追跡データを用いることにより、ある既知パーセントの群衆が共通の標的を見ているということを明らかとするために、より大きな視野イメージを重ねることに伴う不確かな確率的処理が除去される。 Common gaze data may be combined with the captured video when multiple people are wearing a wearable device equipped with a camera equipped with gaze tracking. A large amount of image processing is involved in determining that a camera is looking at a common target by processing video captured by multiple wearable cameras. By including eye tracking data, given the viewer's tendency to see common elements, the alignment process can be dramatically reduced and processing power reduced. In addition, if it is intended to extract a crowd that has seen a common target, eye tracking data can be used to make it clear that a known percentage of the crowd is looking at a common target. Uncertain stochastic processing associated with overlaying large visual field images is eliminated.
眼に向けられている（１若しくは複数の）カメラに対して特定の領域にユーザーの注視が向けられている場合、視線追跡サブシステムがより高い注視精度を発生させることができ、注視が他所に動く場合、精度が低くなるという場合が存在し得る。例えば、視線を追跡するカメラが、眼の前に直接配置される場合、ユーザーが下を向くと、まつ毛又は眼瞼が、視線追跡に用いられる眼の重要な特徴を部分的に遮蔽してしまうことがあり、従って、算出される注視の精度が低下してしまう。眼が特定の注視位置に動く場合におけるこのような精度の低下は、本明細書にて「優雅な劣化（graceful degradation）」と称され、視線追跡システムの性能が劣化しているが、完全には失われていないことから、劣化した情報は、そのようにマークされてよく、編集システムに提供される場合は、精度の低下した損傷データ（compromised data）として処理されてよいことを示している。 If the user's gaze is directed to a particular area with respect to the camera (s) that are aimed at the eye, the gaze tracking subsystem can generate higher gaze accuracy and the gaze is elsewhere. If it moves, there may be cases where the accuracy is low. For example, if the line-of-sight tracking camera is placed directly in front of the eye, the eyelashes or eyelids may partially block the important features of the eye used for line-of-sight tracking when the user is facing down. Therefore, the accuracy of the calculated gaze is reduced. Such a decrease in accuracy when the eye moves to a specific gaze position is referred to herein as "graceful degradation", which degrades the performance of the line-of-sight tracking system, but completely. Is not lost, indicating that degraded information may be marked as such and, if provided to the editing system, may be treated as compromised data. ..
そのような場合、視線追跡データは、複数のセンサーから顕著性を演算する際に、重要性又は妥当性の重み付けを変えて処理されてよい。視線追跡データが、顕著性のために考慮される唯一のセンサーデータである場合、精度の低下した（劣化した）データがシステムからの所望されない性能を引き起こすことのないように、マージンが定められてよい。精度が低下している場合、ユーザーの経験は、予測可能で変化のないものであるはずであり、性能のこの優雅な劣化からフラストレーションが起こることはないはずである。 In such cases, the line-of-sight tracking data may be processed with varying weights of importance or validity when calculating saliency from multiple sensors. If the line-of-sight tracking data is the only sensor data considered for saliency, margins are set so that inaccurate (degraded) data does not cause undesired performance from the system. good. If accuracy is compromised, the user's experience should be predictable and unchanging, and this graceful degradation of performance should not cause frustration.
視線追跡サブシステムの精度が最も高い領域は、眼に向いているセンサー及びディスプレイの形状に応じて、システムのディスプレイが占めるユーザーの視野の領域に相当し得る。しかし、ディスプレイのサイズ及び／又は位置が、システムの異なる実施形態において変化することから、その領域中での視線追跡サブシステムの正確性が変化し得る。さらに、視線追跡サブシステムは、ディスプレイが占めるよりも広いユーザーの視野の領域にわたる正確性が高まるように設計されてよいが、それでも、視線追跡サブシステムの正確性が劣化する領域が存在することになると仮定することが自然である。この場合も、その劣化は、優雅であり、ユーザーにとって目立たないものであるべきである。 The most accurate area of the line-of-sight tracking subsystem may correspond to the area of the user's field of view occupied by the system's display, depending on the shape of the sensor and display facing the eye. However, as the size and / or position of the display varies in different embodiments of the system, the accuracy of the line-of-sight tracking subsystem within that region can vary. In addition, the line-of-sight tracking subsystem may be designed to be more accurate over a wider area of the user's field of view than the display occupies, but there are still areas where the line-of-sight tracking subsystem is less accurate. It is natural to assume that. Again, the degradation should be graceful and unobtrusive to the user.
システムは、細まっている眼を、音響センサーからの笑い声と組み合わせて（又は組み合わせずに）、幸福又は笑顔を表すものとして解釈するように構成されてよい。これは、システムがビデオイメージに顕著性を割り当てる基となるメトリックとして構成されてよい。さらに、ビデオイメージは、細目の開始及び終了によって区切られた時間、又は笑いの開始及び終了によって区切られた時間に従ってトリミングされてよい。ＥＥＧセンサーによって測定される脳波活動も用いられてよい。 The system may be configured to interpret the narrowed eyes in combination with (or without) laughter from acoustic sensors as representing happiness or a smile. It may be configured as a metric on which the system assigns saliency to the video image. In addition, the video image may be cropped according to the time separated by the start and end of the item, or the time separated by the start and end of laughter. EEG activity measured by an EEG sensor may also be used.
多くのウェアラブル電子デバイスは、電池寿命を保護するために、積極的電源管理スキーム（aggressive power-management schemes）に依存している。このようなデバイスにおける消費電力の大きい部分の１つは、ディスプレイであり、このようなスキームは、多くの場合、ディスプレイの作動状態の積極的管理を含む。このため、デバイスのディスプレイは、メディアイメージを取り込むプロセスの間、電源が落とされてよい。システム設計のいくつかの側面は、所望される制御を確保するためにデバイスのディスプレイとの相互作用を含む必要があり得るが、適切に機能するために、すべてのシステム機能がデバイスのディスプレイの電源をオンとすることに依存する必要はない。デバイスのディスプレイの電源をオンとする必要のないシナリオの１つのそのような例は、音響センサーからの入力に基づいてビデオイメージに顕著性を有するとしてタグ付けすることであり得る。それでも、システムが設計通りに作動していることのフィードバックをユーザーが受けることが望ましい場合があり、従って、デバイスのスピーカー（装備されている場合）に対して、特徴的なトーン又はユーザーが認識可能であるその他の音声を出力するようにシステムから指示が出されてよい。 Many wearable electronic devices rely on aggressive power-management schemes to protect battery life. One of the most power-consuming parts of such devices is the display, and such schemes often include active management of the operating state of the display. For this reason, the device display may be powered off during the process of capturing the media image. Some aspects of system design may need to include interaction with the device's display to ensure the desired control, but for proper functioning, all system features power the device's display. You don't have to rely on turning on. One such example of a scenario where the device's display does not need to be turned on could be tagging the video image as prominent based on the input from the acoustic sensor. Nevertheless, it may be desirable for the user to receive feedback that the system is working as designed, so that the device's speakers (if equipped) are distinctive tones or user recognizable. The system may instruct you to output other audio that is.
別の実施形態では、システムは、「魔法の瞬間」又はユーザーが期待している可能性のあるレベルを超える顕著性を有するメディアイメージを自動的に認識するように構成されてよい。これらの魔法の瞬間は、ビデオイメージ中の決定的時点に自社の製品が出現し得る企業などの第三者にとって興味深いものであり得る。この例は、山岳地帯の遠方の山頂へ向かって軽登山を行っている友人のグループであってよい。これらの友人の１人以上がこのシステムを装備したデバイスを装着していると、グループが山頂に居る間、ビデオイメージが取り込まれている。グループの１人が冗談を言って他の全員を笑わせ；偶然にも同時に、グループの別の１人が、銘柄の付いたソフトドリンクを自分のバッグから取り出し、蓋を開ける。このソフトドリンクの企業は、これでなければ、広告代理店を通じてこの瞬間を作り出すのに、何十万ドルという出費を行うことになった可能性があり、従って、この企業としては、その代わりに、このユーザーに「魔法の瞬間」に対する報酬を支払うことに興味を持つ可能性がある。 In another embodiment, the system may be configured to automatically recognize media images that have a "magical moment" or a level of saliency that exceeds the level that the user may have expected. These magical moments can be of interest to third parties, such as companies, where their products may appear at critical points in the video image. An example of this could be a group of friends climbing lightly towards a distant mountain peak in a mountainous area. When one or more of these friends are wearing devices equipped with this system, video images are captured while the group is at the top of the mountain. One member of the group jokes and makes everyone else laugh; by chance, another member of the group removes the branded soft drink from his bag and opens the lid. The soft drink company could otherwise have spent hundreds of thousands of dollars to create this moment through an advertising agency, so for the company, instead. , May be interested in paying this user for a "magical moment".
なお別の実施形態では、このシステムは、メディアイメージをクロッピングするために、ユーザーのハンドジェスチャを認識するように構成されてよい。ユーザーは、自身の手及び指を用いてピクチャフレームを作り出すオプションを有してよく、システムはそれを、メディアイメージがクロッピングされるべき長方形領域として解釈することになる。この仮想ピクチャフレームは、ユーザーによっていずれの方向に移動されてもよく、システムは、そのフレームのサイズ及び／又は位置の両方を追跡して、出力ビデオイメージを連続的に調節してよい。 In yet another embodiment, the system may be configured to recognize the user's hand gestures in order to crop the media image. The user may have the option of creating a picture frame with his or her hands and fingers, which the system will interpret as a rectangular area where the media image should be cropped. The virtual picture frame may be moved in any direction by the user and the system may continuously adjust the output video image by tracking both the size and / or position of the frame.
なお別の実施形態では、システムは、デバイスに物理的に位置していないか、又は接続されていないセンサーからの入力を受けるように構成されてよい。このようなセンサーの例としては、心拍数モニター、ＥＥＧ、パルスオキシメーター、又は自転車電力計（bicycle power meters）などのフィットネスデバイスが挙げられ得る。例えば、システムは、サイクリストの出力が所定のレベルを超えるいずれの時点でも、サイクリストに対応するいずれのビデオクリップにも自動的にタグ付けするようにプログラムされてよく、それは、このような瞬間が、レースのエキサイティングな時間帯に対応し得るからである。 In yet another embodiment, the system may be configured to receive input from sensors that are not physically located or connected to the device. Examples of such sensors may include fitness devices such as heart rate monitors, EEGs, pulse oximeters, or bicycle power meters. For example, the system may be programmed to automatically tag any video clip that corresponds to a cyclist at any time when the cyclist's output exceeds a given level. This is because it can handle the exciting times of the race.
別の実施形態では、システムは、個別指導用又は説明用ビデオを簡単に作成したいと考えるユーザーに応えるよう構成されてよい。これは、視線に基づくクロッピングを、「次の工程では」、又は「ここで分かることは」、又は「これに細心の注意を払う」などの予め構成された語句の認識によって行われるタグ付けと組み合わせることによって達成され得る。これらの例では、ユーザーの視聴者は、そのシーンで見られたものをその通りに知ることに興味を持ち得るものであることから、ユーザーの注視点は、メディアイメージ上に自動的に重ねられてよく、又は別に保存されて、後の時点の編集プロセスで重ねられてもよい。 In another embodiment, the system may be configured to cater to users who want to easily create tutoring or instructional videos. This is the tagging of gaze-based cropping by recognizing pre-configured phrases such as "in the next step", or "what you see here", or "pay close attention to this". It can be achieved by combining. In these examples, the user's gaze is automatically overlaid on the media image, as the user's viewer may be interested in knowing exactly what was seen in the scene. It may be stored separately, or it may be overlaid in a later editing process.
なお別の実施形態では、システムは、ソーシャルメディアアプリケーションとインターフェイス接続するように構成されてよく、それによって、ユーザーにとってトランスペアレントであり得る動的な顕著性が可能となる。例えば、注目すべき報道価値のあるイベントに起因して、特定の場所がツィッター上で流行っている場合、システムは、ユーザーの近くであり得るそのようないずれの場所にも基づいて、顕著性に対するメトリックス及びヒューリスティックスを周期的に更新してよい。次に、ユーザーがそのようないずれかの場所に入った場合、そこで取り込まれるメディアイメージに対して、そこ以外で取り込まれることになるメディアイメージよりも高い顕著性が割り当てられてよい。このプロセスは、ユーザーによって拒否又は促進されるように構成されてよく、この機能の作動は、ユーザーにとって明らかであっても、又はトランスペアレントであってもよい。 In yet another embodiment, the system may be configured to interface with a social media application, allowing for dynamic prominence that can be transparent to the user. For example, if a particular location is prevalent on the tweeter due to a noteworthy news event, the system will be against saliency based on any such location that can be close to the user. Metrics and heuristics may be updated periodically. Second, if the user enters any such location, the media image captured there may be assigned a higher saliency than the media image that would otherwise be captured. This process may be configured to be rejected or facilitated by the user, and the activation of this feature may be obvious to the user or transparent.
別の実施形態では、メディアイメージを選択し、編集するための方法は、ユーザーの特定のニーズに合うようにカスタマイズされてよい。しかし、多くの場合、ユーザーは、システム内に組み込まれているか、又はそれからリモートに位置する１若しくは複数のセンサーからの入力に基づく複数のメトリックス及びヒューリスティックスを包含する予め定められたテンプレートを単純に選択することを希望し得る。 In another embodiment, the method for selecting and editing a media image may be customized to suit the specific needs of the user. However, in many cases, the user simply selects a pre-determined template that includes multiple metrics and heuristics based on inputs from one or more sensors embedded in the system or located remotely from it. You may wish to do so.
別の実施形態では、装着者、例えばメアリーが、視線追跡を装備したディスプレイ付きのデバイスをやはり装着しているリモートの個人、例えばジョンに向けてビデオを送信していてよい。ジョンがディスプレイ上でそのビデオを見る際、彼の注視情報は、ネットワーク（例：電気通信ネットワーク、インターネットなど）を介してメアリーに送信されて、メアリーに注意を向けさせ、ビデオの取り込みが指示されてよい。さらに、ジョンは、メアリーのビデオ取り込みを指定の方向に移動させたいというジョンの希望を示す付随する意味を持つ彼のディスプレイ上にあるアイコンを注視することによって、メアリーに、彼女の頭の向きを変えるように指示してよく、又はメアリーを前後又は上下に移動させるように指示してよい。例えば、メアリーは、ジョンのための疑似的な映像を提供するために、アドベンチャービデオを記録していてよく、又はニュース若しくは娯楽ビデオを記録していてよい。メアリーに与えられる指示は、複数のリモートビューアー／コントローラーによって集約されてよく、その集約された所望される指示が、メアリーに与えられてよい。 In another embodiment, the wearer, eg Mary, may send the video to a remote individual, eg John, who is also wearing a device with a display equipped with eye tracking. When John watches the video on the display, his gaze information is sent to Mary over a network (eg, telecommunications network, internet, etc.) to draw attention to Mary and direct her to capture the video. You can. In addition, John turned her head to Mary by gazing at the icon on his display with the accompanying implications of John's desire to move Mary's video capture in a given direction. You may instruct them to change, or you may instruct Mary to move back and forth or up and down. For example, Mary may be recording an adventure video, or a news or entertainment video, to provide a fake footage for John. The instructions given to Mary may be aggregated by multiple remote viewers / controllers, and the aggregated desired instructions may be given to Mary.
別の代表的実施形態では、本明細書のシステムは、視線制御によるヘッドマウント又はボディマウントカメラのズーム、焦点合わせ、回転、及び／又はパンのために用いられてよい。例えば、多くのＨＭＤは、外側に向いたカメラを有する。高解像度のカメラ及びレンズを有するものもある。別の実施形態によると、何らかの形でユーザーの注視で追跡するか、それによって制御されるか、又はそれに対して応答してよい電子制御された機械式又は電子／光学式のズーム、焦点合わせ、又はパンの機能を有する一体化された、又は取り付け可能のアクセサリーと連結するためのシステム及び方法が備えられてよい。 In another representative embodiment, the systems herein may be used for eye-controlled head-mounted or body-mounted camera zooming, focusing, rotating, and / or panning. For example, many HMDs have an outward facing camera. Some have high resolution cameras and lenses. According to another embodiment, electronically controlled mechanical or electronic / optical zooming, focusing, which may somehow be tracked by the user's gaze, controlled by it, or responded to it. Alternatively, a system and method for connecting with an integrated or attachable accessory having the function of a pan may be provided.
制御により、光学コンポーネントの物理的動き、及び／又はカメラチップ内の用いられるべきピクセルの電子選択、及び／又は標的とする若しくは所望される光を取り込むためのその他の手段が起こる結果となり得る。そのようなシステムは、ユーザーの周囲のメディアイメージを取り込むためにシーンカメラが搭載されたウェアラブルデバイス；カメラのレンズ若しくはカメラのシステムの焦点合わせ、ズーム、パン、又はそれ以外の制御を行うためのデジタル制御可能である一体化又は取り付け可能機構；ユーザーの少なくとも片方の眼の視線追跡データを取り込む視線追跡システム；並びにデジタル（電子）制御可能機構、シーンカメラ、及び視線追跡サブシステムと通信し、視線追跡データに少なくとも部分的に基づいてシーンへのズーム、焦点合わせ、及びパンを行うように制御機構に指示するための１つ以上のプロセッサーを含んでよい。 Control can result in physical movement of the optics and / or electronic selection of pixels to be used in the camera chip and / or other means for capturing the targeted or desired light. Such systems are wearable devices equipped with a scene camera to capture media images around the user; digital for focusing, zooming, panning, or otherwise controlling the camera lens or camera system. Controllable integrated or attachable mechanism; line-of-sight tracking system that captures line-of-sight tracking data for at least one eye of the user; and line-of-sight tracking by communicating with digital (electronic) controllable mechanisms, scene cameras, and line-of-sight subsystems. It may include one or more processors for instructing the control mechanism to zoom, focus, and pan the scene based on the data, at least in part.
システムは、視線追跡データの解釈を行って、ユーザーの眼による注視動作に基づいて、イメージ取り込み（静止又はビデオ）の様々なモードを行ってよい。カメラの制御は、システムによって取り込まれた個々のデータ又は複合データによって決定されてよく、それらとしては、２次元若しくは３次元空間での１つ以上のユーザーの注視点（片眼又は両眼で特定される）、注視継続時間、衝動性運動の一般的パターン、見ている要素若しくは状況に関する衝動性運動のパターン（テキスト、顔、シーン、コンテンツを表示しているディスプレイスクリーン、ムービー、店の陳列、手術、軍隊訓練、保安員による保安活動、スポーツ活動の全体にわたる眼の動き）、衝動性運動の頻度、体の部分（眼、互いの眼での会話を含む）、服などに対する注視を含む人に特有の注視、瞬きの率及び継続時間、瞳孔拡張、細目、ウィンク（意図的な瞬き）、上記要素の予測可能な予め定められた組み合わせ、集められたユーザーの眼のデータを用いて特定された顕著性の動作に適合するように決定された動作の組み合わせを含むその他の眼部計測が挙げられる。 The system may interpret the gaze tracking data and perform various modes of image capture (still or video) based on the gaze movement by the user's eyes. Control of the camera may be determined by individual or composite data captured by the system, such as one or more user's gaze points (identified with one or both eyes) in a two-dimensional or three-dimensional space. ), Gaze duration, general patterns of impulsive movement, patterns of impulsive movement regarding the element or situation being viewed (display screens, movies, store displays displaying text, faces, scenes, content, etc.) Persons who include surgery, military training, security activities by security personnel, eye movements throughout sports activities), frequency of impulsive movements, body parts (including eyes, conversations with each other's eyes), gaze on clothes, etc. Specific gaze, blink rate and duration, pupil dilation, fineness, wink (intentional blink), predictable pre-determined combination of the above factors, identified using collected user eye data Other ocular measurements include combinations of movements determined to fit the movements of saliency.
システムは、複数のユーザー全体にわたってデータを収集してよく、意図的動作（例：ズーム、焦点合わせ、又はパンを制御するための装着者によって明白に意図される眼のジェスチャ）、又は意図しない動作（例：そのようなパターンの解釈、並びに／又は、顕著性を有するシーンイメージの取り込み、並びにカメラのズーム、パン、及び焦点合わせ機構の望ましい制御との関連付け）のパターンを識別し、抽出するための大人数のユーザー（ビッグデータ）を含む。 The system may collect data across multiple users, either intentional movements (eg, eye gestures explicitly intended by the wearer to control zooming, focusing, or panning) or unintended movements. To identify and extract patterns (eg, interpretation of such patterns, and / or capture of prominent scene images, and association with desirable controls of camera zoom, pan, and focusing mechanisms). Including a large number of users (big data).
取り込まれたイメージデータと眼の動作に関するメタデータとの出力に連結された編集システムが考慮される。編集システムを用いてユーザーが行う動作は、登録及び／又は保存されてよい。所望に応じて、カメラの制御、並びに上記システムの性能、分析、及び向上に寄与するデータを取り込むために、その他のセンサーが含まれてよい。これらとしては、本明細書の他所で述べるように、１つ以上の慣性、音響、バイオメトリック、及び／又はその他のセンサーが挙げられ得る。 An editing system linked to the output of captured image data and metadata about eye movements is considered. Actions performed by the user using the editing system may be registered and / or saved. If desired, other sensors may be included to capture data that contributes to camera control and performance, analysis, and improvement of the system. These may include one or more inertial, acoustic, biometric, and / or other sensors, as described elsewhere herein.
レンズ制御機構は、ヘッドマウントデバイス（ＨＭＤ）への取り付け可能アクセサリーとして提供されてよく、又はＨＭＤ内に一体化されたハードウェア及びソフトウェアソリューションとして提供されてもよい。アクセサリーとして提供される場合、ハードウェアは、可変屈折力を有するレンズなどの追加のさらなる交換可能コンポーネントを有していてよい。システムは、インストールプロセスの過程で、ユーザーの注視によって較正されてよい。この較正には、シーンカメラ及び／又はディスプレイが組込まれてよく、それによって、シーンカメラに対するアクセサリーカメラの相対的位置が決定される。較正は、継続的に行われる自動更新を含んでよい。 The lens control mechanism may be provided as an accessory that can be attached to a head-mounted device (HMD), or may be provided as a hardware and software solution integrated within the HMD. When provided as an accessory, the hardware may have additional additional interchangeable components such as lenses with variable power. The system may be calibrated by the user's gaze during the installation process. A scene camera and / or display may be incorporated into this calibration, which determines the relative position of the accessory camera with respect to the scene camera. Calibration may include continuous automatic updates.
視聴者の眼、シーン、音、及び／又はその他の入力のこれまでの知識を用いてクリップの開始及び／又は終了点を指定することによって予測可能なビデオクリップの場面を最適に抽出するために、データテンプレートが作成されてよい。そのようなこれまでの知識は、推測的に決定されてよく、続いて、様々なビデオ記録の状況において、ユーザーの中で収集され、改良されてよい。これは、ユーザーが、充分な電池の残量がある場合、野球の試合を例とするイベント全体にわたってカメラをオンにしておいてよいことを意味する。重要なヒット、得点、アウト、及びその他のプレーがクリップされ、魅力的なビデオに構成されてよい。そのようなビデオは、手動若しくは自動で、個人的に、ソーシャルメディアを介してシェアされてよく、又はさらにはバイヤーに販売されるか、若しくはライフブログライブラリ（life-blog library）の要素として維持されてもよい。その他のテンプレート化可能なビデオとしては：
誕生日パーティ：ハッピーバースデーを歌いながらロウソクを吹き消す；
トラック／クロスカントリーレース：娘／息子が親（ＨＭＤを装着）の近くを走る、又はゴールラインを切る
フットボールの試合：クォーターバックがボールを投げ、レシーバーがそれをキャッチしてタッチダウンへと走る；
野球の試合：ピッチャーが投球し、バッターがホームランを打つ−及びその他のイベント、それによって、ユーザーは、試合全体を記録してよく、ユーザーの物理的観点から顕著性を持つイベントが抽出されてよい。さらに、本明細書の他所で述べるように、クリップは、適切に制御されたカメラ設備による自動ズーム、焦点合わせ、被写界深度、及びパンによって向上されてよい；
ディナーパーティでのディナーテーブルで人々をスキャニング：装着者は、招待客を見回す。ビデオは、ユーザーのスキャン終了時点から開始時点まで戻る方向で、又はその逆で眼によってクリップされる（eye-clipped）。乾杯の間を例として、さらに細かく選択されてもよい；
眺望を前にして休暇中のファミリー／グループビデオ：最も興味深いグループの集まりを抽出したビデオ又はイメージ；
休日；
具体的作業機能；
授与イベント；
祝典、
が挙げられ得る。
To optimally extract predictable video clip scenes by specifying the start and / or end points of the clip using the viewer's eyes, scenes, sounds, and / or other input prior knowledge. , A data template may be created. Such prior knowledge may be speculatively determined and subsequently collected and refined within the user in various video recording situations. This means that the user may keep the camera on throughout the event, such as a baseball game, if sufficient battery power is available. Significant hits, scores, outs, and other plays may be clipped into a compelling video. Such videos may be shared manually or automatically, personally, via social media, or even sold to buyers, or maintained as an element of the life-blog library. You may. Other templates that can be templated include:
Birthday party: Blow out the candles while singing Happy Birthday;
Track / Cross Country Race: Daughter / Son runs near parent (with HMD) or cuts goal line Football Match: Quarterback throws ball, receiver catches it and runs to touchdown;
Baseball games: pitchers throw and batters hit home runs-and other events, which allow the user to record the entire game and extract events that are prominent from the user's physical point of view. .. In addition, as described elsewhere herein, clips may be enhanced by automatic zooming, focusing, depth of field, and panning with well-controlled camera equipment;
Scanning people at the dinner table at a dinner party: The wearer looks around the invited guests. The video is eye-clipped in the direction back from the end of the user's scan to the start, or vice versa. A finer selection may be made, for example during a toast;
Family / group video on vacation in front of the view: a video or image of the most interesting group gathering;
holiday;
Specific work function;
Award event;
celebration,
Can be mentioned.
追加のデータと関連付けられる場合、これらのビデオテンプレートは、時間、場所、人物、イベント、サブイベント、パフォーマンス情報などに関する補足テキストによって自動的に向上されてよい。 When associated with additional data, these video templates may be automatically enhanced with supplemental text about time, place, person, event, sub-event, performance information, and so on.
別の実施形態では、複数ユーザーのシーンが、共通の場所で複数のユーザーのために記録され、上述のようにテンプレートによって情報が付与され、続いてリアルタイム又は非リアルタイムで、シーンショットの複合セットへと構築されて魅力的なビデオ作品を形成するクリップを含んでよい。これらは、テンプレート、注視データ、及びその他の感覚入力によって駆動される高品質電子制御カメラを備えたＨＭＤを装着しているユーザーを多く有する可能性が高い。ビデオフィード及びクリップは、注視同期、地上同期（geo-synchronized）、及び／又は時間同期されて、高品質のリアルタイム又は非リアルタイムでの「クラウドソーシングによる（crowd-sourced）」ビデオが作成されてよい。さらに、ビデオキャプチャと組み合わされた著名人、プロフェッショナル、又は専門家の注視データからの許可された注視拡張ビデオ（licensed Gaze-Augmented Video）が、リアルタイム又は非リアルタイムで、取り込まれ、販売されてよい。別のシーンは、プレイヤーの観点から送信される注視データ付きのシーンカメラによる球技の試合である。ビデオは、プレイヤーの注視方向のインジケーター（レチクル）が重ねられてよく、又はビデオは、プレイヤーの注視（上記で概説したように）及び／又はその他のセンサーによって駆動されるズームなどで向上され得る。レチクルが重ねられたビデオ又は注視で向上された（ズームされた＋）ビデオは、試合の視聴者に、トレーニングのレビューのためにプレイヤーのコーチに、意欲的な就学年代のスポーツ選手に、又はビデオゲーム作成者に対して、リアルタイムで提供／販売されてよい。そのような向上されたビデオの価値は、スポーツ及び娯楽から、軍隊トレーニング、手術などまで、膨大な範囲の活動に及び得る。 In another embodiment, scenes of multiple users are recorded for multiple users in a common location, informed by templates as described above, and then into a composite set of scene shots in real time or non-real time. May include clips that are constructed with and form attractive video works. These are likely to have many users wearing HMDs with high quality electronically controlled cameras driven by templates, gaze data, and other sensory inputs. Video feeds and clips may be gaze-synchronized, geo-synchronized, and / or time-synchronized to create high-quality, real-time or non-real-time "crowd-sourced" video. .. In addition, licensed Gaze-Augmented Video from celebrity, professional, or professional gaze data combined with video capture may be captured and sold in real-time or non-real-time. Another scene is a ball game with a scene camera with gaze data transmitted from the player's point of view. The video may be overlaid with an indicator (reticle) of the player's gaze direction, or the video may be enhanced by the player's gaze (as outlined above) and / or zoom driven by other sensors. Reticle-layered video or gaze-enhanced (zoomed +) video to match viewers, player coaches for training reviews, aspiring school age athletes, or videos It may be offered / sold in real time to game creators. The value of such enhanced video can range from sports and entertainment to military training, surgery, and much more.
アイクリップ（EyeClips）のための別の実施形態は、広告主のためのビデオのクリップである。広告主は、大金を費やして、製品ショットを含み得るか、又は製品ショットに関連し得る現実の魅力的で情緒的なシナリオを作成している。ＨＭＤ装着者は、広告主にとって価値のある高品質のビデオ；例えばコカ・コーラの缶が写っているクリップを記録する意図で、特定の活動（急流下りラフティング、マウンテンバイク、スキー、スカイダイビング）のために自身のカメラを（充分な電池残量がある場合）作動させるように動機付けされ得る。選択されたクリップを広告主に送信するための、及び価値のあるクリップに対して広告主が代金を支払うためのシステムが提供されてよい。 Another embodiment for EyeClips is a video clip for an advertiser. Advertisers are spending a fortune to create fascinating and emotional scenarios of reality that can include or relate to product shots. HMD wearers are of high quality video of value to advertisers; for example, for certain activities (torrent rafting, mountain biking, skiing, skydiving) with the intention of recording a clip showing a can of Coca-Cola. It can be motivated to activate its own camera (if there is enough battery power). A system may be provided for sending selected clips to advertisers and for advertisers to pay for clips of value.
別の実施形態は、集合意識（Collective Consciousness）と称されるものを含み、これは、個々の株式、株式市場の方向性、国際的イベント、政治的な成果、映画の成果、劇場の成果、リアルタイムでのスポーツイベントの予測、健康上の脅威、並びに人の観察、感覚入力、及び関連する測定可能な人の反応に基づいてある程度予測可能であるその他の出来事を予測するためのシステム及び方法である。 Another embodiment includes what is referred to as the Collective Consciousness, which includes individual stocks, stock market directions, international events, political achievements, film achievements, theater achievements, etc. With systems and methods for predicting sporting events in real time, health threats, and other events that are somewhat predictable based on human observation, sensory input, and related measurable human reactions. be.
本明細書で述べる感覚取り込みデータを有するウェアラブルコンピューターは、今までは存在していなかった予測データの作成を可能とする。コンピューターの装着者は、特定のデータを取り込むこと、並びに場合によっては、イメージ、音、人の反応、及び／又は感覚入力が所望される特定の場所を探し出すことが求められてよい。又は、装着者は、自身が関与することなく、データが個人的に取り込まれるか、又は集約されて匿名とされることを可能とするように許可するよう求められてよい。関連する視線追跡データ、眼部計測、バイオメトリック、及びその他のセンサーを有するビデオキャプチャの普及に伴って、将来のイベント及び成果を予測するための集合意識又は集合潜在意識（collective sub-consciousness）を利用するためのシステムが提供されてよい。例えば、フロリダのオレンジ果樹園の労働者は、カメラ及びディスプレイを有する視線追跡可能ウェアラブルコンピューターを装備していてよい。労働者の収穫動作及び今回のオレンジジュースの見込みを予測する観察に関するデータが収集されてよい。折に触れて、個々の労働者は、ディスプレイ情報を介して、オレンジをより詳細に調べる、又は収穫物若しくはその他の情報に関する意見を求める質問に応答するなどのより有用なデータを取り込む特定の動作を行うように指示されてよい。カメラ及びディスプレイを有する視線追跡可能ウェアラブルコンピューターを装備しているホワイトカラー労働者は、自身のテレビ視聴、ラジオ聴取、又はウェブブラウジングの習慣について、並びに／又はテレビ放送（ポータブル感覚キャプチャ装備ニールソンモニタリングデバイス（portable, sensory-capture-equipped, Neilson monitoring device））、ラジオショー、及び／若しくはウェブコンテンツに対する自身の反応についてモニタリングされてよい。そのようなデータから誘導可能である集合意識も分析されてよく、リアルタイム若しくは非リアルタイムで装着者にフィードバックされてよい質問、指示、及び／又は動作がそこから得られてよい。 A wearable computer having the sensory capture data described in the present specification enables the creation of predictive data that did not exist until now. Computer wearers may be required to capture specific data and, in some cases, seek out specific locations where image, sound, human response, and / or sensory input is desired. Alternatively, the wearer may be required to allow the data to be personally captured or aggregated and anonymized without his involvement. With the widespread use of video captures with relevant eye tracking data, eye measurements, biometrics, and other sensors, collective sub-consciousness to predict future events and outcomes. A system for use may be provided. For example, Florida orange orchard workers may be equipped with a line-of-sight trackable wearable computer with a camera and display. Data on workers' harvesting behavior and observations predicting the prospects for this orange juice may be collected. Occasionally, individual workers take in more useful data through display information, such as examining oranges in more detail or answering questions asking for opinions on crops or other information. You may be instructed to do. White-collar workers equipped with eye-tracking wearable computers with cameras and displays can learn about their television watching, radio listening, or web browsing habits, and / or television broadcasts (Neilson monitoring devices with portable sensation capture). Portable, sensory-capture-equipped, Neilson monitoring device)), radio shows, and / or their reaction to web content may be monitored. Collective consciousness that can be derived from such data may also be analyzed, from which questions, instructions, and / or actions that may be fed back to the wearer in real-time or non-real-time may be obtained.
別の実施形態は、単眼ディスプレイ上での双眼視線追跡を用いて、ＵＩ、ＵＸ、及び／又はその他の拡張現実双眼／単眼視線追跡（Other Augmented Reality Binocular/monocular tracking）のための３Ｄ拡張を提供することを含む。２つのカメラを用いた追跡によって３次元（３Ｄ）注視データ（単一眼から得られる平面ｘ−ｙ軸データに加えてｚ軸を有する）を取り込むことにより、３次元の注視点が得られる。システムは、様々な方法で双眼視線追跡データを用いるために、ディスプレイを装備していないか、又は単一眼用の１つのディスプレイ、両眼用の１つのディスプレイ、若しくは各眼用の１つのディスプレイを装備していてよい。 Another embodiment provides a 3D extension for UI, UX, and / or other Augmented Reality Binocular / monocular tracking using binocular tracking on a monocular display. Including doing. A three-dimensional gaze point can be obtained by capturing three-dimensional (3D) gaze data (having a z-axis in addition to the plane xy-axis data obtained from a single eye) by tracking with two cameras. The system is not equipped with a display or has one display for a single eye, one display for both eyes, or one display for each eye in order to use the binocular eye tracking data in various ways. You may equip it.
２つのディスプレイが備えられる場合、ディスプレイ用のオプションは、部分的視野用、ディスプレイを通しての透明性及び情報を重ねる拡張現実を有する完全視野用、又は外部世界のすべてのイメージが遮蔽され、そうでなければ不透明であるディスプレイ上にいずれの拡張現実も重ねて表示される完全視野用であっても、各眼に対して１つを含んでよい。 If two displays are provided, the options for the display are for partial field of view, for full field of view with augmented reality that is transparent through the display and overlays information, or all images of the outside world are obscured or otherwise. One for each eye may be included, even for a complete field of view in which any augmented reality is superimposed on an opaque display.
双眼注視情報を得るために両眼に向けられる２つのカメラを用いる場合、注視情報を算出するプロセッサーは、ディスプレイデータを制御するプロセッサーに、ＵＩ情報又は「拡張現実」（若しくは「ＡＲ」）情報／イメージの３次元重なりイメージを提示するための情報を送って、片方の眼で見ることのできる単一のモノディスプレイ、両方の眼で見ることのできる単一のモノディスプレイ、又は両方の眼で別々に見ることのできる２つのモノディスプレイ上にある程度の量のシミュレーション立体イメージを作成してよい。この方法により、双眼視線追跡は、単一のディスプレイスクリーン上で、効果的な３次元ディスプレイ又は３次元ダイアログを可能とし得る。ユーザーが視線を集中させる、又は視線を逸らすに従って、ディスプレイは変化して、次元性の感覚が作り出され得る。ユーザーの両眼の焦点が、距離Ｚにある物品に合わせられる場合、両眼は、ユーザーが提示されたシーンを見るのに片方の眼、恐らくは利き眼だけを用いている場合であっても、ともむき筋を介して並行して動く。次元性をエミュレートする情報を提示するためのシステムは、眼の優位性に対して敏感であり得る。 When using two cameras aimed at both eyes to obtain binocular gaze information, the processor that calculates the gaze information is the processor that controls the display data, UI information or "augmented reality" (or "AR") information / Three-dimensional overlap of images Sending information to present an image, a single monodisplay that can be seen by one eye, a single monodisplay that can be seen by both eyes, or separate by both eyes A certain amount of simulated 3D images may be created on the two monodisplays that can be seen in. By this method, binocular tracking may enable an effective 3D display or 3D dialog on a single display screen. As the user focuses or diverts his or her gaze, the display can change to create a sense of dimensionality. If the user's eyes are focused on an object at distance Z, then both eyes are using one eye, perhaps only the dominant eye, to see the scene presented by the user, even if they are using only one eye. It moves in parallel through the tomo-peeling muscle. Systems for presenting information that emulates dimensionality can be sensitive to eye dominance.
ＵＩは、ディスプレイ上に焦点の合っていない要素のイメージを提示し、視線を逸らす又は合わせることによって焦点を合わせる動機をユーザーに与える。例えば、そのイメージが表面上はより近くにあり、ユーザーがそのイメージに焦点を合わせることを選択し、ユーザーが視線を合わせるか（双眼視線追跡カメラによって測定可能なように、より詳細に見ようとして）又は視線を逸らせる（さらに遠くを見ようとして）場合の適切なフィードバックがある場合、イメージは、互いに対してシフトされて、焦点が合わさせるか、又は焦点が外されてよい。ユーザーが、人工的に不明瞭なイメージで提示され得る何かをより詳細に見たいと考える際に、ユーザーが視線を合わせる／逸らせる場合、イメージは、片方又は両方の眼で見られる通りに調整されてよい。 The UI presents an image of the out-of-focus element on the display and motivates the user to focus by diversion or focusing. For example, the image is superficially closer, the user chooses to focus on the image, and the user looks at it (trying to see it in more detail so that it can be measured by a binocular eye tracking camera). Alternatively, the images may be shifted and focused or out of focus with respect to each other if there is appropriate feedback in the case of diversion (trying to look further). When the user looks in / out of sight when he / she wants to see in more detail what can be presented in an artificially obscured image, the image is as seen by one or both eyes. May be adjusted.
加えて、又は別の選択肢として、瞳孔拡張も追跡されて、シーンビュー内の知覚される次元性を変化させる時点が判断されてよい。例えば、単一の利き眼で遠くの物体を見ている場合、手前に示されるべき物体は、焦点が外されて示されてよい。ユーザーがこの利き眼を、少しの程度であっても（例えば０．５度）シフトさせて焦点の外れた手前のイメージを注視する場合、イメージは、焦点が合った状態とされてよく、それによって、ユーザーの瞳孔の反応が引き起こされ、さらには、この新たな標的がより近いという感覚が作り出される。同時に、遠くのイメージは、距離のより強い知覚を作り出すように調整されてよい。 In addition, or as an alternative, pupil dilation may also be tracked to determine when to change the perceived dimensionality within the scene view. For example, when looking at a distant object with a single dominant eye, the object to be shown in the foreground may be shown out of focus. If the user shifts this dominant eye even slightly (eg 0.5 degrees) to gaze at the out-of-focus foreground image, the image may be in focus. Causes a reaction in the user's pupils, and also creates a feeling that this new target is closer. At the same time, the distant image may be adjusted to create a stronger perception of distance.
意図的な／無意識のタグ付け又は編集により、様々な方法でのディスプレイを用いた対話によって、価値のある可能性のあるクリップの開始点及び／又は終点のタグ付けをユーザーが意図的にリアルタイムで行うことが可能となる。このタグ付けは、ディスプレイを用いた、又はディスプレイを用いない対話を介して行われてよい。ディスプレイを用いた対話の場合、ユーザーとディスプレイとの間にダイアログが存在してよく、ユーザーが注視対象とするビジュアルメニューが組み込まれ、凝視、ドゥエル、衝動性運動、オンスクリーン、オフスクリーン、オン／オフスクリーン操作によってディスプレイが制御される。ディスプレイは、不透明、半透明、又は透明であってよい。それは、部分視野ディスプレイ又は完全視野ディスプレイであってよい。ダイアログは、タグ付け作業の迅速化、タグ付けを行うかどうかの判断、何に対してタグ付けを行うかの判断、クリップをどの程度の長さにするべきかに関する情報の入手、視野内における顕著性を有する領域への焦点合わせ、テキストによるクリップの向上、音声による注釈、特殊効果、メタデータとして視認可能又は利用可能であってよいその他の感覚情報の組み込みをユーザーが行う補助となり得る。メディアイメージにタグ付けを行うための追加のオプションとしては、以下の１つ以上が挙げられ得る：
タグ付けは、クリップが記録される前又は後に、非リアルタイムで行われてよい；
タグ付けは、ディスプレイを用いることなく、その他の入力又は制御によって行われてよい；
タグ付けは、視線追跡及び／又はディスプレイ機能が組み込まれてよいコンタクトレンズのユーザーがシステムに組み込まれた場合に行われてよい。
By intentional / unknowing tagging or editing, through dialogue with the display in various ways, the user intentionally tags the start and / or end points of potentially valuable clips in real time. It becomes possible to do. This tagging may be done through display-based or non-display dialogue. For display-based interactions, there may be a dialog between the user and the display, which incorporates a visual menu for the user to gaze at, gaze, duel, impulsive movement, on-screen, off-screen, on / The display is controlled by off-screen operation. The display may be opaque, translucent, or transparent. It may be a partial field display or a full field display. Dialogs speed up tagging, decide if you want to tag, decide what to tag, get information about how long a clip should be, and keep it in sight. It can help the user focus on areas of prominence, improve text clips, voice annotations, special effects, and incorporate other sensory information that may be visible or available as metadata. Additional options for tagging media images may include one or more of the following:
Tagging may occur in non-real time before or after the clip is recorded;
Tagging may be done by other inputs or controls without the use of a display;
Tagging may be done when a contact lens user who may incorporate eye tracking and / or display functionality is incorporated into the system.
タグ付け及び編集を制御するためのユーザーとプロセッサーとの間のダイアログには、ＥＥＧ又はその他のバイオメトリックス源からのデータ収集が組み込まれてよく、脳波刺激を介してのフィードバック又は感覚フィードバックがユーザーに提供されてよい。クリップ編集、重ね合わせ、効果、及び向上を算出するための処理は、ローカルで実施され、割り当てられたユーザーのプロセッサー間でシェアされてよく、又はリモートサーバーで実施されてもよい。 Dialogs between the user and the processor to control tagging and editing may incorporate data collection from EEG or other biometric sources, and feedback or sensory feedback via EEG stimulation to the user. May be provided. The process for calculating clip editing, overlaying, effects, and improvements may be performed locally, shared among the assigned user's processors, or performed on a remote server.
本発明は、ユーザーの部分視野、複数の部分視野、又はユーザーの完全視野までの中でディスプレイが観察可能である場合の操作を支援するものである。ディスプレイは、完全な透明であるものから、ユーザーから見える世界及びディスプレイを拡張するためにディスプレイを通して見える実世界のイメージ上に重ねられたいずれかのイメージのいずれかの明度レベルのものまで様々であってよい。さらに、イメージが完全に電子的に作成される不透明ディスプレイ（典型的には、バーチャルリアリティーディスプレイと称される）が組み込まれる構成が、補足イメージ又は視覚制御、及び上述したように重ねられるフィードバックと共に用いられてもよく、それは、ユーザーの明確な動作によって終点がタグ付けされるビデオクリップの開始点のヒューリスティック選択を明らかに実証し、支援するものである。 The present invention assists in operations when the display is observable within the user's partial visual field, multiple partial visual fields, or the user's complete visual field. The display can range from being completely transparent to one of the brightness levels of any of the images superimposed on the world visible to the user and the real world image seen through the display to extend the display. You can. In addition, configurations incorporating an opaque display (typically referred to as a virtual reality display) in which the image is created entirely electronically are used with supplemental images or visual controls, and feedback superimposed as described above. It may be, and it clearly demonstrates and assists in the heuristic selection of the start point of the video clip whose end points are tagged by the user's explicit action.
簡便さのために、本明細書で示され、記載される操作は、様々な相互接続された機能ブロック又は別個のソフトウェアモジュールとして記載される。しかし、これは必須ではなく、これらの機能ブロック又はモジュールが同等に集約されて、境界が明確ではない単一のロジックデバイス、プログラム、又は操作となる場合もあり得る。いずれの場合であっても、機能ブロック及びソフトウェアモジュール、又は記載される特徴は、単独で、又はハードウェア又はソフトウェアのいずれであってもよいその他の操作と組み合わされて実行されてよい。このような機能ブロックは、明らかに、ウェアラブルディスプレイ技術の使用を支援しており、網膜、一体化された、投射された、又はその他のディスプレイを有するウェアラブルコンタクトレンズ、及び一体化された視線追跡機能を有するウェアラブルコンタクトレンズ上へのイメージの投射を含む。 For convenience, the operations shown and described herein are described as various interconnected functional blocks or separate software modules. However, this is not essential, and these functional blocks or modules may be equally aggregated into a single logic device, program, or operation with unclear boundaries. In any case, functional blocks and software modules, or features described, may be performed alone or in combination with other operations, which may be hardware or software. Such functional blocks clearly support the use of wearable display technology, including retinas, wearable contact lenses with integrated, projected, or other displays, and integrated eye tracking capabilities. Includes projection of an image onto a wearable contact lens with.
本明細書のいずれの実施形態と共に示される要素又はコンポーネントも、具体的実施形態のための代表例であり、本明細書で開示されるその他の実施形態で、又はそれと組み合わせて用いられてよいことは理解される。本発明には、様々な改変及び別の選択肢としての形態が可能であるが、その具体例を、図面で示し、本明細書で詳細に記載してきた。しかし、本発明は、開示される特定の形態又は方法に限定されるものではなく、逆に、本発明は、添付の請求項の範囲内に含まれるすべての改変、均等物、及び別の選択肢のすべてを含むものであることは理解されるべきである。 The elements or components shown with any of the embodiments herein are representative of a particular embodiment and may be used in or in combination with other embodiments disclosed herein. Is understood. Various modifications and alternative forms are possible in the present invention, the specific examples of which have been shown in the drawings and described in detail herein. However, the present invention is not limited to the particular form or method disclosed, and conversely, the present invention is all modifications, equivalents, and alternatives within the scope of the appended claims. It should be understood that it includes all of.
Claims (9)
ユーザーの周囲のメディアイメージを取り込むように前記ウェアラブルデバイス上に搭載されたシーンカメラと、
前記ウェアラブルデバイスが前記ユーザーに装着されたときに前記ユーザーに視認可能であるように前記ウェアラブルデバイス上に搭載されるディスプレイと、
１つ以上のプロセッサーとを備え、
前記ユーザーの少なくとも片方の眼にレファレンスフレームを投射し、前記ユーザーの前記少なくとも片方の眼の視線追跡データを出力する視線追跡サブシステムとを備え、
前記１つ以上のプロセッサーは、前記ディスプレイに２つ以上の識別された位置を表示して前記ユーザーに前記２つ以上の識別された位置を順に見るように指示し、
前記ディスプレイは、前記シーンカメラによって取り込まれたメディアイメージを表示し、
前記１つ以上のプロセッサーは、前記視線追跡データを分析して、タグを前記メディアイメージと関連付けるタグ付けの直前に前記ユーザーが前記メディアイメージ内の同じ人物または位置に向けられた時間の長さを前記メディアイメージに追加するように構成されており、
前記１つ以上のプロセッサーは、前記タグ付けの手順の過程に前記ユーザーが見ているメディアイメージの位置を表わす注視点情報を取得するように構成されている、メディアイメージを編集するためのシステム。 Wearable devices and
A scene camera mounted on the wearable device to capture media images around the user,
A display mounted on the wearable device so that it is visible to the user when the wearable device is worn by the user.
With one or more processors
A line-of-sight tracking subsystem that projects a reference frame onto at least one eye of the user and outputs line-of-sight tracking data of the user's at least one eye.
The one or more processors display the two or more identified positions on the display and instruct the user to look at the two or more identified positions in sequence.
The display displays a media image captured by the scene camera.
The one or more processors analyzes the eye-tracking data, the length of time is directed to the same person or location tagging of pre SL user immediately prior to associating the tag with the media image in the media image is configured to add to the media image the is,
A system for editing a media image, wherein the one or more processors are configured to acquire gaze information representing the position of the media image viewed by the user in the course of the tagging procedure.
Applications Claiming Priority (9)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361922724P | 2013-12-31 | 2013-12-31 | |
US61/922,724 | 2013-12-31 | ||
US201461991435P | 2014-05-09 | 2014-05-09 | |
US61/991,435 | 2014-05-09 | ||
US201462074920P | 2014-11-04 | 2014-11-04 | |
US201462074927P | 2014-11-04 | 2014-11-04 | |
US62/074,920 | 2014-11-04 | ||
US62/074,927 | 2014-11-04 | ||
PCT/US2014/073094 WO2015103444A1 (en) | 2013-12-31 | 2014-12-31 | Systems and methods for gaze-based media selection and editing |
Publications (3)
Publication Number | Publication Date |
---|---|
JP2017507400A JP2017507400A (en) | 2017-03-16 |
JP2017507400A5 JP2017507400A5 (en) | 2018-02-15 |
JP6929644B2 true JP6929644B2 (en) | 2021-09-01 |
Family
ID=53494037
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2016544612A Active JP6929644B2 (en) | 2013-12-31 | 2014-12-31 | Systems and methods for gaze media selection and editing |
Country Status (6)
Country | Link |
---|---|
US (3) | US9870060B2 (en) |
EP (1) | EP3090322A4 (en) |
JP (1) | JP6929644B2 (en) |
KR (2) | KR102182605B1 (en) |
CN (1) | CN106030458B (en) |
WO (1) | WO2015103444A1 (en) |
Families Citing this family (99)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2014089542A1 (en) | 2012-12-06 | 2014-06-12 | Eyefluence, Inc. | Eye tracking wearable devices and methods for use |
KR101926942B1 (en) * | 2013-09-03 | 2019-03-07 | 토비 에이비 | Portable eye tracking device |
CN104681048A (en) * | 2013-11-28 | 2015-06-03 | 索尼公司 | Multimedia read control device, curve acquiring device, electronic equipment and curve providing device and method |
JP6929644B2 (en) * | 2013-12-31 | 2021-09-01 | グーグル エルエルシーＧｏｏｇｌｅ ＬＬＣ | Systems and methods for gaze media selection and editing |
KR20230142657A (en) * | 2014-03-19 | 2023-10-11 | 인튜어티브 서지컬 오퍼레이션즈 인코포레이티드 | Medical devices, systems, and methods using eye gaze tracking |
JP6326901B2 (en) * | 2014-03-26 | 2018-05-23 | 富士ゼロックス株式会社 | Image processing apparatus and program |
US10564714B2 (en) | 2014-05-09 | 2020-02-18 | Google Llc | Systems and methods for biomechanically-based eye signals for interacting with real and virtual objects |
AU2015297035B2 (en) | 2014-05-09 | 2018-06-28 | Google Llc | Systems and methods for biomechanically-based eye signals for interacting with real and virtual objects |
KR102240632B1 (en) * | 2014-06-10 | 2021-04-16 | 삼성디스플레이 주식회사 | Method of operating an electronic device providing a bioeffect image |
US10852838B2 (en) * | 2014-06-14 | 2020-12-01 | Magic Leap, Inc. | Methods and systems for creating virtual and augmented reality |
KR102253444B1 (en) | 2014-07-08 | 2021-05-20 | 삼성디스플레이 주식회사 | Apparatus, method, computer-readable recording medium of displaying inducement image for inducing eye blinking |
US10282057B1 (en) * | 2014-07-29 | 2019-05-07 | Google Llc | Image editing on a wearable device |
US9720259B2 (en) * | 2014-10-13 | 2017-08-01 | William Hart | Eyewear pupilometer |
US10567641B1 (en) * | 2015-01-19 | 2020-02-18 | Devon Rueckner | Gaze-directed photography |
US10921896B2 (en) * | 2015-03-16 | 2021-02-16 | Facebook Technologies, Llc | Device interaction in augmented reality |
CN104834446B (en) * | 2015-05-04 | 2018-10-26 | 惠州Tcl移动通信有限公司 | A kind of display screen multi-screen control method and system based on eyeball tracking technology |
US9939644B2 (en) * | 2015-06-25 | 2018-04-10 | Intel Corporation | Technologies for controlling vision correction of a wearable computing device |
US10222619B2 (en) * | 2015-07-12 | 2019-03-05 | Steven Sounyoung Yu | Head-worn image display apparatus for stereoscopic microsurgery |
US9852329B2 (en) * | 2015-07-24 | 2017-12-26 | International Business Machines Corporation | Calculation of a characteristic of a hotspot in an event |
CN105635561A (en) * | 2015-07-31 | 2016-06-01 | 宇龙计算机通信科技(深圳)有限公司 | Image acquisition system and method |
US20170102697A1 (en) * | 2015-10-08 | 2017-04-13 | General Motors Llc | Selecting a vehicle function to control using a wearable electronic device |
US10338677B2 (en) * | 2015-10-28 | 2019-07-02 | Microsoft Technology Licensing, Llc | Adjusting image frames based on tracking motion of eyes |
US10137777B2 (en) | 2015-11-03 | 2018-11-27 | GM Global Technology Operations LLC | Systems and methods for vehicle system control based on physiological traits |
CN105245660A (en) * | 2015-11-05 | 2016-01-13 | 天津大学 | Mobile phone antitheft tracking method based on bracelet |
US10444972B2 (en) | 2015-11-28 | 2019-10-15 | International Business Machines Corporation | Assisting a user with efficient navigation between a selection of entries with elements of interest to the user within a stream of entries |
JP6705206B2 (en) * | 2016-02-25 | 2020-06-03 | セイコーエプソン株式会社 | Electronic device control device and electronic device control program |
US9864431B2 (en) * | 2016-05-11 | 2018-01-09 | Microsoft Technology Licensing, Llc | Changing an application state using neurological data |
US10395428B2 (en) * | 2016-06-13 | 2019-08-27 | Sony Interactive Entertainment Inc. | HMD transitions for focusing on specific content in virtual-reality environments |
CN106200901B (en) * | 2016-06-24 | 2019-03-29 | 联想(北京)有限公司 | A kind of bearing calibration of wear-type ocular pursuit device and wear-type ocular pursuit device |
US10223067B2 (en) | 2016-07-15 | 2019-03-05 | Microsoft Technology Licensing, Llc | Leveraging environmental context for enhanced communication throughput |
EP3488282A4 (en) * | 2016-07-19 | 2019-08-07 | Supereye, Inc. | Systems and methods for predictive visual rendering |
US11086473B2 (en) * | 2016-07-28 | 2021-08-10 | Tata Consultancy Services Limited | System and method for aiding communication |
KR102548199B1 (en) | 2016-08-03 | 2023-06-28 | 삼성전자주식회사 | Electronic device and method for tracking gaze in the electronic device |
CN106339668A (en) * | 2016-08-16 | 2017-01-18 | 信利光电股份有限公司 | Iris recognition method and iris recognition system |
CN107957774B (en) * | 2016-10-18 | 2021-08-31 | 阿里巴巴集团控股有限公司 | Interaction method and device in virtual reality space environment |
US10540491B1 (en) | 2016-10-25 | 2020-01-21 | Wells Fargo Bank, N.A. | Virtual and augmented reality signatures |
WO2018085426A1 (en) | 2016-11-01 | 2018-05-11 | Snap Inc. | Systems and methods for fast video capture and sensor adjustment |
CN109863502A (en) * | 2016-11-10 | 2019-06-07 | 纽诺创科技术公司 | By the image picking-up apparatus method and system to analyze cognitive ability associated with human user |
US10591731B2 (en) | 2016-12-06 | 2020-03-17 | Google Llc | Ocular video stabilization |
US10484623B2 (en) * | 2016-12-20 | 2019-11-19 | Microsoft Technology Licensing, Llc | Sensor with alternating visible and infrared sensitive pixels |
US10088911B2 (en) | 2016-12-30 | 2018-10-02 | Manuel Saez | Programmable electronic helmet |
FR3061583B1 (en) * | 2017-01-04 | 2020-12-04 | Oreal | DEVICE FOR ACQUIRING A SKIN ALTERATIONS VIDEO, ASSOCIATED CHARACTERIZATION SYSTEM AND CHARACTERIZATION METHOD |
EP3381351A1 (en) * | 2017-03-29 | 2018-10-03 | Vivior AG | System for assessing a health condition of a user |
US10609025B2 (en) * | 2017-04-06 | 2020-03-31 | Htc Corporation | System and method for providing simulated environment |
US10401954B2 (en) * | 2017-04-17 | 2019-09-03 | Intel Corporation | Sensory enhanced augmented reality and virtual reality device |
DE102017108371B4 (en) * | 2017-04-20 | 2020-08-27 | Carl Zeiss Meditec Ag | Medical optical display system and method for operating the same |
CA3065131A1 (en) * | 2017-05-31 | 2018-12-06 | Magic Leap, Inc. | Eye tracking calibration techniques |
US20180357670A1 (en) * | 2017-06-07 | 2018-12-13 | International Business Machines Corporation | Dynamically capturing, transmitting and displaying images based on real-time visual identification of object |
US10460527B2 (en) * | 2017-06-30 | 2019-10-29 | Tobii Ab | Systems and methods for displaying images in a virtual world environment |
CN107169338A (en) * | 2017-07-25 | 2017-09-15 | 上海闻泰电子科技有限公司 | Unlocking method and device |
KR102481884B1 (en) | 2017-09-22 | 2022-12-28 | 삼성전자주식회사 | Method and apparatus for displaying a virtual image |
US10254832B1 (en) | 2017-09-28 | 2019-04-09 | Microsoft Technology Licensing, Llc | Multi-item selection using eye gaze |
WO2019069812A1 (en) * | 2017-10-05 | 2019-04-11 | 株式会社ソニー・インタラクティブエンタテインメント | Sight-line detection system for head-mounted display, head-mounted display, and sight-line detection method for head-mounted display |
US11282133B2 (en) | 2017-11-21 | 2022-03-22 | International Business Machines Corporation | Augmented reality product comparison |
US10586360B2 (en) | 2017-11-21 | 2020-03-10 | International Business Machines Corporation | Changing view order of augmented reality objects based on user gaze |
KR102483781B1 (en) * | 2017-12-18 | 2023-01-03 | 광주과학기술원 | Operating method of augmented reality glass for providing docent service |
US20190235621A1 (en) * | 2018-01-26 | 2019-08-01 | Snail Innovation Institute | Method and apparatus for showing an expression of how an object has been stared at in a displayed video |
US20190235246A1 (en) * | 2018-01-26 | 2019-08-01 | Snail Innovation Institute | Method and apparatus for showing emoji on display glasses |
CN110095866B (en) * | 2018-01-29 | 2020-07-28 | 京东方科技集团股份有限公司 | Augmented reality device, augmented reality system and information prompting method thereof |
CN110115842B (en) * | 2018-02-06 | 2023-01-13 | 日本聚逸株式会社 | Application processing system, application processing method, and application processing program |
US10481403B2 (en) * | 2018-02-15 | 2019-11-19 | Tectus Corporation | Contact lens with retinal camera |
KR102495796B1 (en) | 2018-02-23 | 2023-02-06 | 삼성전자주식회사 | A method for biometric authenticating using a plurality of camera with different field of view and an electronic apparatus thereof |
US10521013B2 (en) * | 2018-03-01 | 2019-12-31 | Samsung Electronics Co., Ltd. | High-speed staggered binocular eye tracking systems |
US10949969B1 (en) * | 2018-03-20 | 2021-03-16 | Welch Allyn, Inc. | Pupil edge region removal in digital imaging |
US10747500B2 (en) * | 2018-04-03 | 2020-08-18 | International Business Machines Corporation | Aural delivery of environmental visual information |
US10853014B2 (en) * | 2018-04-17 | 2020-12-01 | Rockwell Collins, Inc. | Head wearable device, system, and method |
US11568166B1 (en) * | 2018-07-27 | 2023-01-31 | Verily Life Sciences Llc | Suggesting behavioral adjustments based on physiological responses to stimuli on electronic devices |
CN109189222B (en) * | 2018-08-28 | 2022-01-11 | 广东工业大学 | Man-machine interaction method and device based on pupil diameter change detection |
US20200082176A1 (en) * | 2018-09-12 | 2020-03-12 | Toyota Motor Engineering & Manufacturing North America, Inc. | Systems and methods for extending detachable automobile sensor capabilities for identification of selected object types |
US10833945B2 (en) * | 2018-11-13 | 2020-11-10 | International Business Machines Corporation | Managing downloading of content |
CN109451236A (en) * | 2018-11-13 | 2019-03-08 | 深圳龙图腾创新设计有限公司 | A kind of camera focus method based on the tracking of pupil position |
US11282259B2 (en) | 2018-11-26 | 2022-03-22 | International Business Machines Corporation | Non-visual environment mapping |
CN109711286B (en) * | 2018-12-11 | 2022-11-11 | 中国科学院深圳先进技术研究院 | Control method and device based on artificial retina space perception |
US10786729B1 (en) * | 2019-03-08 | 2020-09-29 | Sony Interactive Entertainment Inc. | Thermopile array fusion tracking |
WO2020185219A1 (en) * | 2019-03-13 | 2020-09-17 | Hewlett-Packard Development Company, L.P. | Detecting eye tracking calibration errors |
US10928647B2 (en) * | 2019-04-01 | 2021-02-23 | Heidi Kershaw | Therapeutic training device for autism |
US10832733B1 (en) | 2019-05-15 | 2020-11-10 | International Business Machines Corporation | Verbal command video editing |
US10812771B1 (en) * | 2019-06-12 | 2020-10-20 | At&T Intellectual Property I, L.P. | Methods, systems, and devices for adjusting image content for streaming panoramic video content |
US20210038329A1 (en) * | 2019-07-16 | 2021-02-11 | Transenterix Surgical, Inc. | Augmented reality using eye tracking in a robot assisted srugical system |
JP7375373B2 (en) * | 2019-08-28 | 2023-11-08 | セイコーエプソン株式会社 | Optical devices and wearable display devices |
US11297224B2 (en) * | 2019-09-30 | 2022-04-05 | Snap Inc. | Automated eyewear device sharing system |
DE112020006208T5 (en) * | 2019-12-18 | 2022-10-13 | Korrus, Inc. | System and method for gaze-based illumination of displays |
CN111158492B (en) * | 2019-12-31 | 2021-08-06 | 维沃移动通信有限公司 | Video editing method and head-mounted device |
CN113079311B (en) * | 2020-01-06 | 2023-06-27 | 北京小米移动软件有限公司 | Image acquisition method and device, electronic equipment and storage medium |
RU2744548C1 (en) * | 2020-04-29 | 2021-03-11 | федеральное государственное бюджетное образовательное учреждение высшего образования "Алтайский государственный университет" | Device for determining pupillary responses to visual stimulus |
CN111552076B (en) * | 2020-05-13 | 2022-05-06 | 歌尔科技有限公司 | Image display method, AR glasses and storage medium |
US20230336826A1 (en) * | 2020-05-22 | 2023-10-19 | Beijing Baidu Netcom Science And Technology Co., Ltd. | Method and apparatus for controlling video playing, electronic device and storage medium |
US11817201B2 (en) | 2020-09-08 | 2023-11-14 | Medtronic, Inc. | Imaging discovery utility for augmenting clinical image management |
WO2022075983A1 (en) * | 2020-10-07 | 2022-04-14 | Hewlett-Packard Development Company, L.P. | Operation of a configurable end-point device based on accessibility settings |
US11908208B2 (en) * | 2020-10-20 | 2024-02-20 | Toyota Motor Engineering & Manufacturing North America, Inc. | Interface sharpness distraction mitigation method and system |
CN115398828A (en) * | 2020-12-14 | 2022-11-25 | 船井电机株式会社 | Real-time immersion of multiple users |
US20220344057A1 (en) * | 2021-04-27 | 2022-10-27 | Oura Health Oy | Method and system for supplemental sleep detection |
CN113509141A (en) * | 2021-06-29 | 2021-10-19 | 昆明依利科特科技有限公司 | Glasses type virus-involved behavior pupil detector and detection method |
US20230119935A1 (en) * | 2021-10-18 | 2023-04-20 | Meta Platforms Technologies, Llc | Gaze-guided image capture |
US11592899B1 (en) | 2021-10-28 | 2023-02-28 | Tectus Corporation | Button activation within an eye-controlled user interface |
WO2023081297A1 (en) * | 2021-11-05 | 2023-05-11 | Zinn Labs, Inc. | Eye tracking system for determining user activity |
US11619994B1 (en) | 2022-01-14 | 2023-04-04 | Tectus Corporation | Control of an electronic contact lens using pitch-based eye gestures |
US11874961B2 (en) | 2022-05-09 | 2024-01-16 | Tectus Corporation | Managing display of an icon in an eye tracking augmented reality device |
WO2023244247A1 (en) * | 2022-06-17 | 2023-12-21 | Hewlett-Packard Development Company, L.P. | Image portion combination signals |
Family Cites Families (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
USRE39539E1 (en) * | 1996-08-19 | 2007-04-03 | Torch William C | System and method for monitoring eye movement |
US6542081B2 (en) * | 1996-08-19 | 2003-04-01 | William C. Torch | System and method for monitoring eye movement |
ATE454849T1 (en) * | 2002-10-15 | 2010-01-15 | Volvo Technology Corp | METHOD FOR EVALUATION OF A PERSON'S HEAD AND EYE ACTIVITY |
US7206022B2 (en) * | 2002-11-25 | 2007-04-17 | Eastman Kodak Company | Camera system with eye monitoring |
US7574016B2 (en) * | 2003-06-26 | 2009-08-11 | Fotonation Vision Limited | Digital image processing using face detection information |
JP2005252734A (en) | 2004-03-04 | 2005-09-15 | Olympus Corp | Head-mounted camera |
CA2967756C (en) * | 2004-04-01 | 2018-08-28 | Google Inc. | Biosensors, communicators, and controllers monitoring eye movement and methods for using them |
US9269322B2 (en) * | 2006-01-09 | 2016-02-23 | Ignis Innovation Inc. | Method and system for driving an active matrix display circuit |
US8726194B2 (en) * | 2007-07-27 | 2014-05-13 | Qualcomm Incorporated | Item selection using enhanced control |
WO2009127065A1 (en) * | 2008-04-18 | 2009-10-22 | Ignis Innovation Inc. | System and driving method for light emitting device display |
JP5318503B2 (en) * | 2008-09-02 | 2013-10-16 | ヤフー株式会社 | Image search device |
DK2389095T3 (en) * | 2009-01-26 | 2014-11-17 | Tobii Technology Ab | Viewpoint detection assisted by optical reference signals |
EP2237237B1 (en) * | 2009-03-30 | 2013-03-20 | Tobii Technology AB | Eye closure detection using structured illumination |
US20110044512A1 (en) * | 2009-03-31 | 2011-02-24 | Myspace Inc. | Automatic Image Tagging |
JP5613025B2 (en) * | 2009-11-18 | 2014-10-22 | パナソニック株式会社 | Gaze detection apparatus, gaze detection method, electrooculogram measurement apparatus, wearable camera, head mounted display, electronic glasses, and ophthalmologic diagnosis apparatus |
US9373123B2 (en) * | 2009-12-30 | 2016-06-21 | Iheartmedia Management Services, Inc. | Wearable advertising ratings methods and systems |
WO2011115783A1 (en) | 2010-03-15 | 2011-09-22 | Oregon Health & Science University | Methods for assessing the risk of cardiovascular disease |
WO2012021967A1 (en) * | 2010-08-16 | 2012-02-23 | Tandemlaunch Technologies Inc. | System and method for analyzing three-dimensional (3d) media content |
US9690099B2 (en) * | 2010-12-17 | 2017-06-27 | Microsoft Technology Licensing, Llc | Optimized focal area for augmented reality displays |
EP2499963A1 (en) * | 2011-03-18 | 2012-09-19 | SensoMotoric Instruments Gesellschaft für innovative Sensorik mbH | Method and apparatus for gaze point mapping |
US8510166B2 (en) * | 2011-05-11 | 2013-08-13 | Google Inc. | Gaze tracking system |
US20130021374A1 (en) * | 2011-07-20 | 2013-01-24 | Google Inc. | Manipulating And Displaying An Image On A Wearable Computing System |
US9342610B2 (en) | 2011-08-25 | 2016-05-17 | Microsoft Technology Licensing, Llc | Portals: registered objects as virtualized, personalized displays |
US9128520B2 (en) * | 2011-09-30 | 2015-09-08 | Microsoft Technology Licensing, Llc | Service provision using personal audio/visual system |
EP2774353A4 (en) * | 2011-11-03 | 2015-11-18 | Intel Corp | Eye gaze based image capture |
US10282055B2 (en) * | 2012-03-06 | 2019-05-07 | Apple Inc. | Ordered processing of edits for a media editing application |
US9096920B1 (en) * | 2012-03-22 | 2015-08-04 | Google Inc. | User interface method |
US9201512B1 (en) * | 2012-04-02 | 2015-12-01 | Google Inc. | Proximity sensing for input detection |
US20140328570A1 (en) * | 2013-01-09 | 2014-11-06 | Sri International | Identifying, describing, and sharing salient events in images and videos |
US20130328925A1 (en) * | 2012-06-12 | 2013-12-12 | Stephen G. Latta | Object focus in a mixed reality environment |
US20140176591A1 (en) * | 2012-12-26 | 2014-06-26 | Georg Klein | Low-latency fusing of color image data |
EP2972678A4 (en) * | 2013-03-15 | 2016-11-02 | Interaxon Inc | Wearable computing apparatus and method |
US9213403B1 (en) * | 2013-03-27 | 2015-12-15 | Google Inc. | Methods to pan, zoom, crop, and proportionally move on a head mountable display |
US20140354533A1 (en) * | 2013-06-03 | 2014-12-04 | Shivkumar Swaminathan | Tagging using eye gaze detection |
US10089786B2 (en) * | 2013-08-19 | 2018-10-02 | Qualcomm Incorporated | Automatic customization of graphical user interface for optical see-through head mounted display with user interaction tracking |
US10114532B2 (en) * | 2013-12-06 | 2018-10-30 | Google Llc | Editing options for image regions |
JP6929644B2 (en) * | 2013-12-31 | 2021-09-01 | グーグル エルエルシーＧｏｏｇｌｅ ＬＬＣ | Systems and methods for gaze media selection and editing |
AU2015297035B2 (en) * | 2014-05-09 | 2018-06-28 | Google Llc | Systems and methods for biomechanically-based eye signals for interacting with real and virtual objects |
-
2014
- 2014-12-31 JP JP2016544612A patent/JP6929644B2/en active Active
- 2014-12-31 EP EP14876031.7A patent/EP3090322A4/en not_active Ceased
- 2014-12-31 KR KR1020167020195A patent/KR102182605B1/en active IP Right Grant
- 2014-12-31 CN CN201480075406.4A patent/CN106030458B/en active Active
- 2014-12-31 WO PCT/US2014/073094 patent/WO2015103444A1/en active Application Filing
- 2014-12-31 KR KR1020207033096A patent/KR102246310B1/en active IP Right Grant
- 2014-12-31 US US14/588,371 patent/US9870060B2/en active Active
-
2017
- 2017-12-08 US US15/835,891 patent/US20180173319A1/en not_active Abandoned
-
2018
- 2018-07-06 US US16/028,505 patent/US10915180B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
JP2017507400A (en) | 2017-03-16 |
US20150220157A1 (en) | 2015-08-06 |
KR102246310B1 (en) | 2021-04-29 |
EP3090322A1 (en) | 2016-11-09 |
WO2015103444A1 (en) | 2015-07-09 |
CN106030458B (en) | 2020-05-08 |
US10915180B2 (en) | 2021-02-09 |
CN106030458A (en) | 2016-10-12 |
KR20200133392A (en) | 2020-11-27 |
KR102182605B1 (en) | 2020-11-24 |
KR20160105439A (en) | 2016-09-06 |
US20200387226A9 (en) | 2020-12-10 |
US20190179418A1 (en) | 2019-06-13 |
EP3090322A4 (en) | 2017-07-19 |
US20180173319A1 (en) | 2018-06-21 |
US9870060B2 (en) | 2018-01-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6929644B2 (en) | Systems and methods for gaze media selection and editing | |
CN112507799B (en) | Image recognition method based on eye movement fixation point guidance, MR glasses and medium | |
EP3392739B1 (en) | Eye-brain interface (ebi) system and method for controlling same | |
CN106471419B (en) | Management information is shown | |
US10831268B1 (en) | Systems and methods for using eye tracking to improve user interactions with objects in artificial reality | |
CN105378632B (en) | The oriented user input of user focus control | |
CN112034977B (en) | Method for MR intelligent glasses content interaction, information input and recommendation technology application | |
US20240042232A1 (en) | Head-worn therapy device | |
US20180190011A1 (en) | Content rendering systems for head-worn computers | |
CN112181152A (en) | Advertisement push management method, equipment and application based on MR glasses | |
KR20130130740A (en) | Comprehension and intent-based content for augmented reality displays | |
US11782508B2 (en) | Creation of optimal working, learning, and resting environments on electronic devices | |
WO2017143128A1 (en) | Haptic systems for head-worn computers | |
US20230282080A1 (en) | Sound-based attentive state assessment | |
US20230418372A1 (en) | Gaze behavior detection | |
US20230351676A1 (en) | Transitioning content in views of three-dimensional environments using alternative positional constraints | |
WO2023114079A1 (en) | User interactions and eye tracking with text embedded elements | |
WO2022212070A1 (en) | Attention detection | |
WO2024058986A1 (en) | User feedback based on retention prediction | |
WO2023049089A1 (en) | Interaction events based on physiological response to illumination | |
WO2023283161A1 (en) | Enhanced meditation experience based on bio-feedback |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A711 | Notification of change in applicant |
Free format text: JAPANESE INTERMEDIATE CODE: A711Effective date: 20171010 |
|
A521 | Written amendment |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20171225 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20171225 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20180822 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20180905 |
|
A521 | Written amendment |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20181205 |
|
A02 | Decision of refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A02Effective date: 20190514 |
|
A521 | Written amendment |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20190912 |
|
C60 | Trial request (containing other claim documents, opposition documents) |
Free format text: JAPANESE INTERMEDIATE CODE: C60Effective date: 20190912 |
|
A911 | Transfer to examiner for re-examination before appeal (zenchi) |
Free format text: JAPANESE INTERMEDIATE CODE: A911Effective date: 20190918 |
|
C21 | Notice of transfer of a case for reconsideration by examiners before appeal proceedings |
Free format text: JAPANESE INTERMEDIATE CODE: C21Effective date: 20190924 |
|
A912 | Re-examination (zenchi) completed and case transferred to appeal board |
Free format text: JAPANESE INTERMEDIATE CODE: A912Effective date: 20191011 |
|
C211 | Notice of termination of reconsideration by examiners before appeal proceedings |
Free format text: JAPANESE INTERMEDIATE CODE: C211Effective date: 20191023 |
|
C22 | Notice of designation (change) of administrative judge |
Free format text: JAPANESE INTERMEDIATE CODE: C22Effective date: 20200707 |
|
C22 | Notice of designation (change) of administrative judge |
Free format text: JAPANESE INTERMEDIATE CODE: C22Effective date: 20200915 |
|
C13 | Notice of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: C13Effective date: 20201027 |
|
A521 | Written amendment |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210127 |
|
C13 | Notice of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: C13Effective date: 20210216 |
|
A521 | Written amendment |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210507 |
|
C23 | Notice of termination of proceedings |
Free format text: JAPANESE INTERMEDIATE CODE: C23Effective date: 20210608 |
|
C03 | Trial/appeal decision taken |
Free format text: JAPANESE INTERMEDIATE CODE: C03Effective date: 20210713 |
|
C30A | Notification sent |
Free format text: JAPANESE INTERMEDIATE CODE: C3012Effective date: 20210713 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20210811 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6929644Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |