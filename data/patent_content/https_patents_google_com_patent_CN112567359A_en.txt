CN112567359A - System for optimizing storage replication in a distributed data analysis system using historical data access patterns - Google Patents
System for optimizing storage replication in a distributed data analysis system using historical data access patterns Download PDFInfo
- Publication number
- CN112567359A CN112567359A CN201980052412.0A CN201980052412A CN112567359A CN 112567359 A CN112567359 A CN 112567359A CN 201980052412 A CN201980052412 A CN 201980052412A CN 112567359 A CN112567359 A CN 112567359A
- Authority
- CN
- China
- Prior art keywords
- data
- storage
- processors
- items
- server
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000003860 storage Methods 0.000 title claims description 62
- 230000010076 replication Effects 0.000 title description 23
- 238000007405 data analysis Methods 0.000 title description 6
- 238000004458 analytical method Methods 0.000 claims abstract description 13
- 238000000034 method Methods 0.000 claims description 36
- 230000008520 organization Effects 0.000 claims description 30
- 238000012546 transfer Methods 0.000 claims description 23
- 230000015654 memory Effects 0.000 claims description 19
- 238000004891 communication Methods 0.000 claims description 8
- 238000013500 data storage Methods 0.000 claims description 7
- 230000000717 retained effect Effects 0.000 claims description 4
- 230000001186 cumulative effect Effects 0.000 claims description 3
- 230000003362 replicative effect Effects 0.000 abstract description 4
- 238000009826 distribution Methods 0.000 description 10
- 230000008569 process Effects 0.000 description 9
- 238000004422 calculation algorithm Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 5
- 238000012545 processing Methods 0.000 description 4
- 230000003068 static effect Effects 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 239000000835 fiber Substances 0.000 description 3
- 238000013515 script Methods 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000012432 intermediate storage Methods 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000004140 cleaning Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 235000019580 granularity Nutrition 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000003012 network analysis Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 238000012856 packing Methods 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 229920001690 polydopamine Polymers 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/27—Replication, distribution or synchronisation of data between databases or within a distributed database system; Distributed database system architectures therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/21—Design, administration or maintenance of databases
- G06F16/219—Managing data history or versioning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/23—Updating
- G06F16/2358—Change logging, detection, and notification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/28—Databases characterised by their database models, e.g. relational or object models
- G06F16/284—Relational databases
- G06F16/285—Clustering or classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
Abstract
Historical analysis of query patterns is used to discover relationships between data sets. These relationships are used to make the best decision as to where to place data in a globally distributed environment where the locality of the data is an important factor in providing good query performance. The mixed integer programming model is used to address a constraint-based system that strikes a balance between the need to keep data local to other data and the cost of replicating data across low bandwidth networks.
Description
Cross Reference to Related Applications
This application is a continuation of U.S. patent application No.16/107,452 filed on 21.8.2018, the disclosure of which is incorporated herein by reference.
Background
Cloud computing systems sometimes include distributed data analysis engines that run in multiple data centers distributed globally. Each data center contains one or more servers. Users of such cloud computing systems may create organizations and projects. In a project, a distributed data analysis engine allows users to create datasets and tables. Internally, the table is divided into data replication units, called storage sets. Each storage set corresponds to one or more files stored on a server. Although users typically query their own data set, it is also possible for one user to share the data set with another user, or to disclose it for use by many users. Multiple data sets may be merged together at query time, potentially requiring the system to read data from a large number of different data sets that may belong to any user.
When evaluating a query, the distributed data analysis engine performs a set of processes within a particular server. When the files to be read are stored on the same server that runs the analysis process, these processes read the above-mentioned storage set files and perform most efficiently. Reading data from a remote server is inherently more expensive and involves higher latency. The amount of available bandwidth for data transfer across servers is also limited and a scarce resource. Because of these limitations, in some systems, cross-server reads that exceed a very small limit will be prohibited, and all data processed by the analysis process must be present in the local server.
To provide fault tolerance, redundancy, and high availability, some systems replicate all storage sets to each server in which an analytics process may be run. However, copying data to each server in use is problematic because it is costly to increase the number of servers in use in terms of the amount of data transfer required. The data transmission amount of each additional server is linearly increased, so that the data transmission amount cannot be expanded. Furthermore, copying data to each server also has the problem that it limits the increase in system storage capacity, since the system will be limited by the available storage size of the smallest server used.
Disclosure of Invention
The present disclosure provides a method for optimizing replicated data storage. The method comprises the following steps: identifying, using one or more processors, a dataset that is commonly owned by an organization; automatically storing the co-owned data sets on a same computing device of a distributed computing system; analyzing, using the one or more processors, query logs of the distributed computing system; identifying, using the one or more processors, items having linked data sets in the query log, and storing or replicating, using the one or more processors, items having most frequently linked data sets on a same computing device of the distributed computing system. In alternative approaches, the data sets may have other relationships that result in a common grouping.
According to some examples, the method may further include generating, using the one or more processors, a graph, wherein each identified item is represented by a node in the graph, and wherein a number of times that the identified two items are linked in the query log is represented by a weighted edge between the nodes that present the two items. Clusters of items connected by edges having a weight greater than a predetermined threshold may be identified, wherein storing items having the most linked data sets includes storing the identified clusters.
According to other examples, the method may include identifying clusters of items based on linear relationships between the items using a mixed integer programming model. Constraints of the mixed integer programming model may include storage and computation capacity of each server, storage and computation requirements of each cluster, cumulative storage and computation requirements of each cluster must not exceed storage and computation capacity of the servers assigned to the cluster, each cluster has multiple copies that must be stored on separate servers where data should be retained. An allocation map may be generated based on the output of the mixed integer programming model, the allocation map indicating where each cluster is stored. Data transfer operations required to implement the allocation map may be determined to ensure that the amount of available storage in the server is not exceeded when copying data from a source server to a target server.
Another aspect of the present disclosure provides a system for optimizing replicated data storage. The system includes one or more memories that store a log of transactions between data sets in a distributed computing system over a period of time; one or more processors in communication with the one or more memories. The one or more processors are configured to identify a dataset that is commonly owned by an organization; automatically storing the co-owned data sets on a same computing device of a distributed computing system; analyzing logs of the distributed computing system; identifying an item having a linked data set based on an analysis of the log; storing the item having the most frequently linked data set on the same computing device of the distributed computing system.
Another aspect of the disclosure provides a computer-readable medium storing instructions executable by a processor to perform a method for optimizing replicated data storage. The method comprises the following steps: identifying a dataset commonly owned by an organization; automatically storing the co-owned data sets on a same computing device of a distributed computing system; analyzing a query log of the distributed computing system; identifying items in the query log having linked datasets; storing the item having the most frequently linked data set on the same computing device of the distributed computing system.
Drawings
Fig. 1 is a block diagram of an example distributed computing system, in accordance with aspects of the present disclosure.
FIG. 2 is a block diagram of an example data distribution system, in accordance with aspects of the present disclosure.
FIG. 3 is a block diagram of an example allocation planner according to aspects of the present disclosure.
4A-4B illustrate example groupings of items according to aspects of the present disclosure.
FIG. 5 is an example diagram illustrating the dependencies of items according to aspects of the present disclosure.
Fig. 6 is a flow diagram illustrating an example method in accordance with aspects of the present disclosure.
Detailed Description
Overview
The present disclosure describes a system for discovering relationships between data sets using historical analysis of query patterns. These relationships are used to make the best decision as to where to place data in a globally distributed environment where the locality of the data is an important factor in providing good query performance. The mixed integer programming model is used to address a constraint-based system that strikes a balance between the need to keep data local to other data and the cost of replicating data across a low-bandwidth Wide Area Network (WAN).
According to one example implementation, the system includes a distribution planner and a replication controller. These entities may be modules within a computing device that perform particular processes, as described below. The allocation planner and the replication controller work cooperatively to implement a replication strategy to replicate user data to a fixed number of servers out of a possible total number of available servers. By replicating to only a subset of the servers, rather than to each server, there is a fixed limit to the amount of replication required for each new storage set. The number of servers in the subset may be predetermined, for example, based on redundancy requirements. In addition, the total amount of memory required by the system is greatly reduced. By way of example only, in a system involving the use of 5 servers, previous replication techniques required 5 copies of each storage set. Instead, the system described herein may choose to replicate to only 2 of 5 servers. Thus, each server contains on average only 40% of the amount of data required by the previous system.
Copying data to only a subset of servers may result in the need to read data stored on a remote server. While remote reads are possible, they can severely impact performance and suffer from greater throughput limitations. To minimize these effects, the system minimizes the number of remote reads. In particular, data that might be merged by a user query will remain together while still being able to merge any first user data with the second user data.
To minimize the possibility of remote requests, the system automatically places all storage sets owned by the same organization in the same server. Query log analysis is performed to identify historical data access patterns that are used to predict future behavior. For example, the system will examine the query log to determine which data sets tend to appear together most frequently in the query, and attempt to replicate these data sets to the same server. Users may have one or more organizations or projects, and they may organize their data within the organization to keep their data available locally.
The distribution planner will periodically query the metadata catalog of the storage set to determine the amount of data stored for each item. For example, such analysis may be performed every few hours, every few days, etc. The distribution planner also queries the project/organizational relationships and groups all the projects that are part of the same organization together. It queries the job log for instances where data from one item (or organization) is linked with data from another item (or organization). The query sums the amount of data transfer completed by queries performed in the past, such as over a recent past period of time, e.g., recent days, to assign weights to the relationships between items.
The allocation planner may use a greedy solver, assuming that the data set should not be moved, if possible. In this example, the graph-connection component algorithm can be used to cluster items together based on these relationships. These entries represent nodes in the graph and the query history is used to construct weighted edges in the graph. The connected component algorithm finds clusters of items connected by edges with weights greater than a certain threshold. These clusters represent the relevant set of items and should be stored on the same server. In some examples, two different connected component passes are run. The first pass establishes relationships between items based on sharing a common reservation, where a reservation is a set of pre-allocated computing resources that a customer may purchase and reserve for his own use. The second pass uses historical analysis of the query log to associate and merge items together based on query history.
In another example, the allocation planner may use a mixed integer programming model to solve the problem of where to store each cluster based on constraints. In this example, the relationship between related items is represented as a linear constraint. Constraints of the mixed integer programming model may include: 1) each server has storage and computing capabilities; 2) each project cluster has storage requirements and computing requirements; 3) the sum of the storage and computation requirements of the project clusters assigned to the server cannot exceed the storage and computation capacity of the server; 4) each cluster of items has multiple copies of the data, which must be stored on separate servers; 5) when any one server is out of service or "exhausted," the remaining servers must bear the previous load on the exhausted server; 6) where feasible, the data should be retained on the server already in place to minimize data transfer costs. The model generates an allocation map that maps each cluster of items to a server. This is the output of the distribution planner.
The replication controller determines a replication plan based on the allocation map and the current state of the allocation. The replication plan identifies the data transfer operations required to implement the new allocation map. This includes some number of data copy operations, as well as data delete operations to reclaim unused space. The replication plan takes into account the intermediate storage capacity so that even during the transfer of data, the available storage is not exceeded and more copies of the data can be temporarily made than is necessary in the static state. For example, project data may be temporarily copied to the source server and the target server when it is transferred from the source server to the target server. The replication controller ensures that both the source server and the target server have sufficient storage capacity to accommodate the transmission.
Although the concepts described herein may be used to manage data replicated across multiple servers, it should be understood that such concepts may also be applied to other network architectures, such as those utilizing "cells. For example, a cell is a network of tightly connected computing devices that may, for example, run processes cooperatively, with shared storage and very low network latency and high throughput between the computing devices in the cell. Thus, although the examples herein are described primarily with respect to assigning items to particular servers, it should be understood that the examples are similarly applicable to assigning items to cells or other computing devices or groups of computing devices.
Example System
FIG. 1 illustrates an example system that includes a distributed computing environment. Multiple data centers 160, 170, 180 may be communicatively coupled, for example, over network 150. The data centers 160, 170, 180 may further communicate with one or more client devices (e.g., clients 110) over the network 150. Thus, for example, the client 110 may perform operations in the "cloud". In some examples, the data centers 160, 170, 180 may further be in communication with a controller 190.
The data centers 160 and 180 may be positioned a substantial distance from each other. For example, data centers may be located in various countries around the world. Each data center 160, 170, 180 may include one or more computing devices, such as processors, servers, shards, widgets, and the like. For example, as shown in fig. 1, data center 160 includes computing devices 162, 164, data center 170 includes computing device 172, and data center 180 includes computing device 181-186. The program may be executed across these computing devices, e.g., such that some operations are performed by one or more computing devices of a first data center and other operations are performed by one or more computing devices of a second data center. In some examples, computing devices in various data centers may have different capabilities. For example, different computing devices may have different processing speeds, workloads, etc. Although only a few of these computing devices are shown, it should be understood that each data center 160, 170, 180 may include any number of computing devices, and that the number of computing devices in a first data center may be different than the number of computing devices in a second data center. Moreover, it should be understood that the number of computing devices in each data center 160-180 may change over time, for example, because hardware is removed, replaced, upgraded, or expanded.
In some examples, each data center 160-180 may also include a plurality of storage devices (not shown), such as hard disk drives, random access memory, diskettes, disk arrays, tape drives, or any other type of storage device. The data centers 162, 172, 182 may implement any of a variety of architectures and technologies, including but not limited to Direct Attached Storage (DAS), Network Attached Storage (NAS), Storage Area Network (SAN), Fibre Channel (FC), fibre channel over ethernet (FCoE), hybrid architecture networks, and the like. In addition to storage devices, the data center may include many other devices, such as cables, routers, and the like. Further, in some examples, data center 160-180 may be a virtualized environment. Further, although only a few data centers 160-180 are shown, multiple data centers may be coupled by network 150 and/or additional networks.
In some examples, controller 190 may be in communication with a computing device in data center 160 and 180 and may facilitate execution of the program. For example, controller 190 may track the capabilities, status, workload, or other information of each computing device and use such information to assign tasks. Similar to client 110 described above, controller 190 may include a processor 198 and memory 192, with memory 192 including data 194 and instructions 196. In other examples, such operations may be performed by one or more computing devices in one of the data centers 160 and 180, and a separate controller may be omitted from the system.
Each client 110 may be, for example, a computer intended for use by an individual or entity. Client 110 may have all internal components typically found in a personal computer, such as a Central Processing Unit (CPU), CD-ROM, hard drive, and display device, such as a monitor with a screen, a projector, a touch screen, a small LCD screen, a television or other device (such as an electronic device operable to display information processed by processor 120), speakers, a modem and/or network interface device, user input (such as a mouse, keyboard, touch screen, or microphone), and all components for interconnecting these elements. Further, computers in accordance with the systems and methods described herein may include devices capable of processing instructions and transmitting data to and from people and other computers, including general purpose computers, PDAs, tablets, mobile phones, smart watches, network computers lacking local storage capability, set-top boxes for televisions, and other networked devices.
Client 110 may include a processor 120, memory 130, and other components typically found in a general purpose computer. Memory 130 may store information accessible to processor 120, including instructions 132 that may be executed by processor 120. The memory may also include data 134 that may be retrieved, manipulated or stored by the processor 120. Memory 130 may be a non-transitory computer readable medium of a type capable of storing information accessible by processor 120, such as a hard disk drive, a solid state drive, a tape drive, optical storage, memory cards, ROM, RAM, DVD, CD-ROM, memory with write capability and read-only. The processor 120 may be a well known processor or other less well known type of processor. Alternatively, the processor 120 may be a dedicated controller, such as an ASIC.
The instructions 132 may be a set of instructions, such as machine code, that are directly executed by the processor 120, or instructions, such as scripts, that are indirectly executed. In this regard, the terms "instructions," "steps," and "programs" may be used interchangeably herein. The instructions 132 may be stored in an object code format for direct processing by the processor 120, or in other types of computer languages, including scripts or collections of independent source code modules, which scripts or collections are interpreted or pre-compiled as needed.
Data 134 may be retrieved, stored, or modified by processor 120 according to instructions 132. For example, although the systems and methods are not limited by a particular data structure, the data 134 may be stored in computer registers, in a relational database as a table having a plurality of different fields and records or XML documents. Data 134 may also be formatted in a computer readable format, such as, but not limited to, binary values, ASCII, or Unicode. Further, the data 134 may include information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories, including other network locations, or information used by functions used to calculate the relevant data.
Application 136 may be used for any of a variety of operations. The application 136 may, for example, be downloaded, executable from the instructions 132, or remotely accessed. In some examples, the application may be executed remotely. For example, an application on a client device may execute in the cloud.
Although fig. 1 functionally shows the processor 120 and memory 130 as being within the same block, the processor 120 and memory 130 may actually comprise multiple processors and memories that may or may not be stored within the same physical housing. For example, some of the instructions 132 and data 134 may be stored on a removable CD-ROM, while other instructions may be stored within a read-only computer chip. Some or all of the instructions and data may be stored in a location physically remote from processor 120 but still accessible to processor 120. Similarly, processor 120 may actually comprise a collection of processors, which may or may not operate in parallel.
The client 110, data center 160, 180, and control 190 are capable of direct and indirect communication, for example, over the network 150. For example, using an internet socket, the client 110 may connect to a service operating on a remote server through an internet protocol suite. The server may set up a listening socket that can accept the initial connection for sending and receiving information. The network 150 and intermediate nodes may include various configurations and protocols, including the internet, world wide web, intranets, virtual private networks, wide area networks, local area networks, private networks using communication protocols proprietary to one or more companies, ethernet, WiFi (e.g., 702.71, 702.71b, g, n, or other such standards), and HTTP, as well as various combinations thereof. Such communication may be facilitated by devices, such as modems (e.g., dial-up, cable, or fiber optic) and wireless interfaces, capable of transmitting data to and from other computers.
FIG. 2 illustrates an example architecture of a system for optimizing replicated storage. The system may, for example, reside on a computing device in one of the data centers 160 and 180 of FIG. 1, on the controller 190 of FIG. 1, or some combination thereof. The system includes a load balancer 250, which may be responsible for allocating reservations to compute servers in a balanced manner. For example, load balancer 250 may balance the distribution of tasks within a data center or across a globally distributed system. Load balancer 250 may, for example, compute an explicit item-to-server allocation mapping to achieve appropriate load balancing. When all servers are available, it can handle both static query routes and dynamic query routes when the servers are exhausted. In addition, the load balancer 250 may provide a reliable way to publish these allocations to all components in the data analysis engine that uses this information.
The data set may be replicated two or more times, for example, in the primary and secondary replicas. The load balancer 250 and routing system run queries in a location with a primary replica. The secondary copy is a backup that will become the active location of the query route if the primary server is exhausted. This can have a greater impact on the system when the copy is bound to a reservation with a pre-allocated slot. Computing capacity may be reserved for the primary servers, and there is a further limitation that any single server may be down and the system may reroute queries to a secondary location without overloading any of the servers.
Load balancer 250 may include an allocation planner 252, a replication controller 254, and a reservation exchange manager 256. The allocation planner 252 may operate as a subcomponent in the load balancer 250. For example, the allocation planner 252 may be a module, such as a C + + or other type of module. In some cases, the allocation planner may replace an existing route calculation component. Although in some examples, a single allocation planner 252 may generate allocations for each region, in other examples, additional or fewer allocation planners may be implemented. For example, a distribution planner may generate a distribution for a global distributed system, multiple distribution planners may be implemented within a given area, and so on.
The allocation planner 252 is responsible for calculating and publishing allocations. In calculating the allocation, the allocation planner considers a number of factors, such as the hierarchy of organizations and items to be allocated (216), on-demand computing consumption, amount of storage (e.g., on an organization basis), server storage and computing power, and so forth. In some examples, the allocation planner may also consider computing reservations (214), such as resources guaranteed to a particular customer. To obtain the most up-to-date reservation information, for example, the distribution planner 252 may subscribe to periodic updates, receive push notifications when new reservations are made, and the like. The information considered by the allocation planner 252 in generating the new allocation plan may include historical query information, such as metadata snapshots 212, data sets, and the like.
The allocation planner 252 may periodically take data from sources and predict future events, taking such predictions into account when determining allocations. For example, the allocation planner 252 may predict a rejection, a large amount of data that the customer will send, potential reservation changes, etc. Such events may affect server size, project storage, or other constraints.
The allocation planner 252 may run continuously on schedule, on demand, or in response to environmental changes or configuration changes. It may operate in a dry run mode, where it generates allocations but does not apply them; or in an autonomous mode in which it automatically sends the newly generated assignment to copy controller 254 for immediate application.
Once the new allocation is calculated, it may be different from the previous allocation. For example, a set of assignments, such as for a particular organization, may be moved from a first copy to a second copy. The copy controller 254 is responsible for moving the data to conform to the new allocation and cleaning the data in the previous location. For example, the replication controller 254 may determine a replication plan for the data transfer operations necessary to achieve the new assignment. The plan may include a number of data transfer and copy operations, as well as data delete operations to reclaim unused space. The replication plan may take into account intermediate storage capacity so that even during data transfer the available storage is not exceeded and temporarily has more copies of data than needed in the static state.
According to some examples, replication controller 254 may manage state machines for each organization that needs to move to a new replica. In doing so, copy controller 254 may begin by adding a new copy to the organization's copy configuration so that all new data begins to enter the new copy. Copy controller 254 further commands movement to ensure that there is sufficient space available to copy the entire tissue. It also calls the copy worker 264 to perform the data copy. When an organization's primary reservation needs to be moved between servers, replication controller 254 submits a reservation move request to Reservation Switching Manager (RSM) 256. For example, the reservation may be moved when it is moved out of the destination server. Once the data movement is completed, the publishing mechanism of the load balancer 250 can push those changes to all servers involved in the routing decision.
The replication worker 264 may be a service that handles replicated storage at different metadata granularities. In some examples, it may be created by splitting the copy-specific functionality from the copy queue handler. It may be able to replicate a single memory set or all of the memory of a table or range of tables. It may also support deleting copies.
The RSM 256 handles reservation moves and may perform reservation exchanges when there is insufficient space to accommodate a reservation move between servers. For example, if there are two servers, each with 1000 slots available, and a reservation of 500 slots is to be moved from the first to the second of the two servers, it may be moved because resources are available. However, if the reservations of 1500 and 2000 slots are to be moved simultaneously, they are automatically swapped. For example, a reservation may issue its intention, and when both reservations are ready, an exchange will be performed and both reservations are moved at the same time.
According to some examples, each of allocation planner 252, replication controller 254, and reservation exchange manager 256 may export a dashboard of states, metrics, and the like.
FIG. 3 shows an example of the allocation planner 252 in more detail. In this example, allocation planner 252 includes a number of components, such as input collector/analyzer 302, input validator 304, one or more solvers 306, and allocation validator 308. It should be understood that these components are merely examples and that additional, fewer, or different components may be included in the allocation planner 252.
The input collector/analyzer 302 reads the item usage input from the metadata snapshot to determine the current size of the submitted data for each item. According to some examples, the data may be merged with project data from other sources to obtain organization information for each project. The input collector/analyzer 302 also obtains reservation information. For example, the reservation information may be read from a storage file designated for storing static reservation information or from another source.
The input analyzer/collector 302 may also determine groupings of items and organizations. According to one example, a hash policy can be used to distribute a small item or organization (e.g., a small item or organization with 512GB or less of data) across servers. For example, small items or organizations may be grouped into hash groups, which are provided to solver 306 for placement. Input analyzer/collector 302 can also determine hard groupings of data and create assignment groups that can be provided to solver 306 as atomic units that cannot be split by the solver. For example, an allocation group may include any individual item, all items in an organization, a hash bucket for a small organization or item, all items in a reservation, and so forth. All items in an allocation group may be placed on the same server.
If there are cases where a reservation includes items from multiple organizations, the reserved allocation group may be extended to include all items of any organization to which the reservation relates. For example, as shown in FIG. 4A, reservation R contains items P1 and P2 for organization A and items P3 and P4 for organization B, but does not include all of the items in organization B. The reservation R does not contain the entry P5. Thus, as shown in FIG. 4B, the allocation group of reservation R is extended to include all the items in organization B. In particular, an allocation group G' is created that covers all groups in the reservation R as well as any covered organisations. In this case, the allocation group will have the computational requirements of reservation R and the storage requirements of all included items P1-P5. If item P5 is located in a different reservation Q, the G' group will cover all items in reservations R and Q. In this case, G' combines the computational requirements of reservations R and Q.
The input analyzer/collector 302 may further determine the available capabilities of each server. For example, it may receive such an indication of availability through an Application Programming Interface (API) or any other direct or indirect communication with the server. Any input or output data of the system may have manual override. For example, manual override may be used if the server is intentionally underutilized or overutilized to allow for planned capacity changes, planned reservation changes, and expected increase in size or computational requirements of the project in the near future. Data for manual override may be read from, for example, a configuration file and incorporated into dynamically collected input from other parts of the system.
The system may further calculate the dependencies. For example, the system may obtain a list of groups and create a weighted graph based on the query history. For example, the input collector/analyzer 302 may review a history of queries over a period of time, such as the past few days, to determine which items are relevant. According to some examples, the system may generate a graph of related items.
FIG. 5 illustrates an example graph of dependencies. According to this example, a query running in item P reads 5GB from item Q and 10GB from item R. If a job requires access to information from project Q, R, each job executed by project P may generate one or more edges between node P and related node Q, R. The edges of all jobs are added to give a total weight (e.g., 5GB, 10 GB).
Some items may contain publicly visible or widely shared data that is available to a large number of users beyond the item that owns the data. These frequently accessed datasets may have a large number of edges and create a "super cluster" of linked nodes. These items can be excluded from graph-dependent analysis and automatically copied to all nodes. For example, the system may detect a threshold count of edges for a given item and automatically fully replicate the item.
The input validator 304 may ensure that each input is correctly entered into the system. For example, the input verifier 304 may confirm that all required elements exist (e.g., each group has a valid, non-zero size), that all IDs in the relationship graph exist in the input list, and so on. According to some examples, input validator 304 may also validate input related to the primary and secondary replicas. For example, the input validator 304 may confirm that the relationship of the primary and secondary replicas is valid (e.g., both have the same storage requirements and only the primary represents a computational requirement), that the primary and secondary replicas of the same set are weighted to infinity, meaning that they should not be placed in the same server, and so on.
As described above, solver 306 can execute one or more algorithms to generate the assignments. One example includes a greedy algorithm. For example, inputs to the process might include a list of n servers C1 … C n with storage capacity CS 1 … CS n and computing capacity CC 1 … CC n, and a list of 2m groups G1 … G2m with storage capacity GS 1 … GS 2m and computing capacity GC 1 … GC 2 m. For each group G, there may be an assignment a (G) → C, which maps the group to the server. The input collector 302 produces m sets of items, and then generates 2 inputs to the assignment solver for each set-a primary set and a secondary set, which are backup copies of the primary data. Further inputs to the solver might be a list of m inverted edges (where the weight is infinity) of G1 → G2, G3 → G4 … G2 m-1 → G2m, and a list of E edges E1 … E E from group G i to G j, where for each edge i/j en there is a non-zero positive weight w (en). The negative edge represents a constraint that the primary and secondary copies of a group should not be located on the same server. Edges represent data sharing relationships between items in a group based on empirical analysis of task history.
A greedy solver can be used with a graph-connected component (GCC) solver that takes a graph { N, E } of nodes N and edges E and returns a set L of clusters. Each cluster L n is a collection of groups from G. The storage requirement LS n is sigma GS of all groups in the cluster, and the calculation requirement LC n is sigma GC of all groups in the cluster. The allocation function a (l n) for the cluster returns L n the server in all groups that was allocated to its maximum storage. According to some examples, this may be implemented as a machine learning model.
The greedy solver can perform clustering on all groups whose weights are greater than some threshold. Initially, the threshold may be 0, so all weights are considered in the first iteration. The clusters are arranged in descending order of computation requirements and then in descending order of storage requirements. The solver may attempt to first bin pack the cluster using the existing assignments. If the existing allocation cannot accommodate, the cluster is placed in the server with the largest computing capacity (if the cluster has computing requirements), otherwise it is placed in the server with the largest available storage capacity. If there is no solution for the cluster, the solver will increase the edge weight threshold, regenerate the cluster, and then re-run the bin packing algorithm. An example pseudo-code for a greedy solver is as follows:
the greedy solver is biased towards the current allocation and will not move an existing cluster out of its currently allocated server as long as the server has the computing/storage capabilities of the cluster. If the clusters are merged together, the data may be moved. For example, if C i and C j are linked to form cij, then this new cluster may have a different assignment. Let C i be the larger of the two clusters, and A (Ci) ≠ A (C j). In this case, a (cij) would be the same as the previous allocation a (ci), and C j would be moved.
Another example algorithm performed by solver 306 is a mixed integer problem. One example of a mixed integer problem includes the following constants and variables:
constant quantity
Variables of
The model includes a plurality of constraints. For example, such constraints may include that each copy is assigned once, that some copies cannot be on the same server, that the server has maximum capacity, limits on movement variables or transfer cost variables, and so forth. Such constraints may be represented using the constants and variables described above. For example, for the constraint that can be expressed as once assigned for each copy, sum _ j x (i, j) is 1 for all copies i. For the constraint that it can be expressed that some copies cannot be on the same server, so that for all copies i and i ', TRANSFER _ COST (i, i ') -infinity, for all servers j, x (i, j) + x (i ', j) < ═ 1. For the server with the greatest capacity, for all server pairs, all primary replicas assigned to j and all secondary replicas assigned to j and whose primary replicas are assigned to j' cannot exceed the computing capacity of server j. This covers the case where server j 'is down and the secondary copy of the primary copy assigned to j' must take over the computational load. For all servers j and j ' where j ≠ j ', sum _ i x (i, j) · USAGE (i, component) · IS _ primary (i) + sum _ i ' (x (i, j ', i ', j) · USAGE (i ', component) · IS _ BACKUP (i ', i, component)) < ═ CAPACITY (j, component) for all resources r. In binding the move variable, m (i) > (i) 1-x (i, PREVIOUS _ server (i)) for all i in the copy. In the binding TRANSFER COST variable, j, t (i, j) > sum _ i ' sum _ j ' > j x (i ', j ')/TRANSFER _ COST (i, i ') -x (i, j) (sum _ i ' TRANSFER _ COST (i, i ')) in the server for all i in the replica. This constraint will ignore copy i ', for which TRANSFER _ COST (i, i ') is equal to-infinity, and assume that TRANSFER _ COST (i, i ') is always positive or null. The goal of the mixed integer solver is to minimize the cost of the move: minimize sum _ i m (i) × MOVE _ cost (i) + sum _ j t (j).
Example method
FIG. 6 illustrates an example method 600 for generating allocations of replicated data in a distributed system based on analysis of historical queries. The method may be performed, for example, at any device in a distributed system, such as a server, controller, load balancer, or other computing device. It should be understood that the following operations need not be performed in the exact order described below. Rather, the various steps may be processed in a different order or simultaneously. Steps may also be added or omitted unless otherwise noted.
In block 610, data sets commonly owned by the same organization are identified. For example, the online retailer may be a cloud computing/storage customer. An online retailer may have multiple projects, for example, where each project is owned by a different team within the company. For example, an online retailer may have a table associated with operations, such as sales orders or inventory in an item. The retailer may also have items related to marketing or network analysis in different items. The retailer may wish to merge the data together to find a relationship between marketing efforts and order history.
In block 620, the commonly owned data sets are automatically stored on the same device in the distributed system. For example, commonly owned data sets may be hard grouped, which are not separated in later group assignment processes. Referring to the online retailer example above, sales item information and marketing item information may be automatically stored together so that frequent association of items may be accomplished more efficiently. The common ownership may be determined, for example, by comparing identifiers in the metadata, each identifier corresponding to a particular organization.
In block 630, query logs for the distributed system are analyzed to identify relationships between the data sets, for example. For example, the query log may include metadata snapshots, computing reservations, information about the organization/project hierarchy, and the like. The log may provide an indication of how a given item has access to information for another item in a different organization. For example, an online retailer may have one or more affiliated entities, such as advertising agencies, marketing platforms, affiliated websites, and the like. The online retailer may need to access data from one or more affiliates, and vice versa, although one or more affiliates may store such data in different areas, such as different servers, different data centers, different regions, and so forth. Many other example implementations are possible where data of multiple linked organizations is stored in the cloud or across distributed computing systems.
In block 640, items having linked datasets are identified by analyzing the query log. For example, the query log may reveal a history of transactions between the online retailer and one or more affiliates over a period of time. For example, the time period may be hours, days, weeks, etc. immediately prior to the time of analysis. Accordingly, the data sets of the online retailer and one or more affiliates accessed in those transactions in the query log may be identified as linked.
In block 650, the items of the dataset with the most frequent links are stored on the same computing device in the distributed system. For example, a solver may be used to determine the most frequently accessed data sets. Such as by generating a weighted graph or solving a mixed integer problem. This may also take into account the cost of each access, such as latency, priority, etc. In some examples, the data sets may be weighted, scored, ranked, or otherwise evaluated to determine those most frequently accessed and/or those data sets having the highest access costs. For example, the online retailer may access the affiliate advertising agency data 3 times over a historical period of time, with a relatively low cost per access. The online retailer may also visit the affiliate mortgage analyst's website 50 times over a historical period of time, with a slightly higher cost per visit. If the server has only enough capacity to store data for two of the three organizations, they may store the data for the online retailer and affiliate mortgage analysts together in the same device.
An advantage of the foregoing techniques is that they provide increased data storage efficiency, which translates into increased computing and trading efficiency. For example, by identifying data sets linked by transaction history and storing the data sets together, the delay in performing future transactions involving both data sets may be reduced.
Unless otherwise specified, the foregoing alternative examples are not mutually exclusive and may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of the subject matter of the defined claims. Furthermore, the provision of examples described herein, as well as words expressed in terms of "such as," "including," and the like, should not be construed to limit claimed subject matter to particular examples. Rather, these examples are intended to illustrate only one of many possible embodiments. Further, the same reference numbers in different drawings may identify the same or similar elements.
Claims (20)
1. A method for optimizing replicated data storage, the method comprising:
identifying, using one or more processors, a dataset that is commonly owned by an organization;
automatically storing the co-owned data sets on a same computing device of a distributed computing system;
analyzing, using the one or more processors, query logs of the distributed computing system;
identifying, using the one or more processors, items in the query log having linked datasets;
storing, using the one or more processors, the item having the most frequently linked data set on the same computing device of the distributed computing system.
2. The method of claim 1, further comprising:
generating a graph using the one or more processors,
wherein each identified item is represented by a node in the graph, and
wherein a number of times two of the identified items are linked in the query log is represented by a weighted edge between nodes presenting the two items.
3. The method of claim 2, further comprising identifying, using the one or more processors, clusters of items connected by edges having weights greater than a predetermined threshold.
4. The method of claim 3, wherein storing the item having the most linked data sets comprises storing the identified clusters.
5. The method of any of the preceding claims, further comprising: determining, using the one or more processors, a data transfer operation required to enable storage of the item with the most frequently linked data set on the same computing device, wherein determining the data transfer operation comprises ensuring that an amount of available storage in a source server is not exceeded when copying data from the server to a target server.
6. The method of any of the preceding claims, further comprising identifying clusters of items based on linear relationships between items using a mixed integer programming model.
7. The method of claim 6, wherein the mixed integer programming model is configured to solve a problem of where to store the cluster based on constraints, and the constraints of the mixed integer programming model include one or more of: the storage and computing capacity of each server, the storage and computing requirements of each cluster, the cumulative storage and computing requirements of each cluster must not exceed the storage and computing capacity of the servers assigned to the cluster, each cluster has multiple copies that must be stored on separate servers where data should be retained.
8. The method of claim 6 or claim 7, further comprising generating, using the one or more processors, an allocation map based on an output of the mixed integer programming model, the allocation map indicating where to store each cluster.
9. The method of claim 8, further comprising determining, using the one or more processors, data transfer operations required to implement the allocation map.
10. The method of claim 9, wherein determining the data transfer operation comprises ensuring that an amount of available storage in a source server is not exceeded when copying data from the server to a target server.
11. A system for optimizing replicated data storage, the system comprising:
one or more memories storing a log of transactions between data sets in a distributed computing system over a period of time;
one or more processors in communication with the one or more memories, the one or more processors configured to:
identifying a dataset commonly owned by an organization;
automatically storing the co-owned data sets on a same computing device of the distributed computing system;
analyzing logs of the distributed computing system;
identifying an item having a linked data set based on an analysis of the log;
storing the item having the most frequently linked data set on the same computing device of the distributed computing system.
12. The system of claim 11, wherein the one or more processors are further configured to:
generating a drawing
Wherein each identified item is represented by a node in the graph, and
wherein a number of times two of the identified items are linked in the log is represented by a weighted edge between nodes presenting the two items.
13. The system of claim 12, wherein the one or more processors are further configured to identify clusters of items connected by edges having a weight greater than a predetermined threshold.
14. The system of claim 13, wherein storing the item having the most linked data sets comprises storing the identified clusters.
15. The system of any of claims 11 to 14, wherein the one or more processors are further configured to identify clusters of items based on linear relationships between items using a mixed integer programming model.
16. The system of claim 15, wherein the mixed integer programming model is configured to solve a problem of where to store the cluster based on constraints, and the constraints of the mixed integer programming model include one or more of: the storage and computing capacity of each server, the storage and computing requirements of each cluster, the cumulative storage and computing requirements of each cluster must not exceed the storage and computing capacity of the servers assigned to the cluster, each cluster has multiple copies that must be stored on separate servers where the data should be retained.
17. The system of claim 15 or claim 16, wherein the one or more processors are further configured to generate an allocation map based on an output of the mixed integer programming model, the allocation map indicating where to store each cluster.
18. The system of claim 17, wherein the one or more processors are further configured to determine data transfer operations required to implement the allocation map.
19. The system of claim 18, wherein determining the data transfer operation comprises ensuring that an amount of available storage in a source server is not exceeded when copying data from the server to a target server.
20. A computer readable medium storing instructions executable by a processor to perform a method for optimizing replicated data storage, the method comprising:
identifying a dataset commonly owned by an organization;
automatically storing the co-owned data sets on a same computing device of a distributed computing system;
analyzing a query log of the distributed computing system;
identifying items in the query log having linked datasets;
storing the item having the most frequently linked data set on the same computing device of the distributed computing system.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/107,452 US20200065415A1 (en) | 2018-08-21 | 2018-08-21 | System For Optimizing Storage Replication In A Distributed Data Analysis System Using Historical Data Access Patterns |
US16/107,452 | 2018-08-21 | ||
PCT/US2019/043389 WO2020040928A1 (en) | 2018-08-21 | 2019-07-25 | System for optimizing storage replication in a distributed data analysis system using historical data access patterns |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112567359A true CN112567359A (en) | 2021-03-26 |
Family
ID=67620528
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980052412.0A Pending CN112567359A (en) | 2018-08-21 | 2019-07-25 | System for optimizing storage replication in a distributed data analysis system using historical data access patterns |
Country Status (4)
Country | Link |
---|---|
US (1) | US20200065415A1 (en) |
EP (1) | EP3818453A1 (en) |
CN (1) | CN112567359A (en) |
WO (1) | WO2020040928A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116611664A (en) * | 2023-06-13 | 2023-08-18 | 杭州派迩信息技术有限公司 | Ground clothing label management system, device and method thereof |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11206305B1 (en) * | 2020-09-17 | 2021-12-21 | Sap Se | Latency minimization in datacenters |
US20230289077A1 (en) * | 2022-03-10 | 2023-09-14 | Google Llc | Soft Capacity Constraints For Storage Assignment In A Distributed Environment |
CN117035897B (en) * | 2023-05-26 | 2024-04-12 | 浙江大学 | Instant delivery platform rider unilateral method |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100299343A1 (en) * | 2009-05-22 | 2010-11-25 | Microsoft Corporation | Identifying Task Groups for Organizing Search Results |
US20130024443A1 (en) * | 2011-01-24 | 2013-01-24 | Aol Inc. | Systems and methods for analyzing and clustering search queries |
CN104699567A (en) * | 2013-10-21 | 2015-06-10 | 国际商业机器公司 | Method and system for recovering data objects in a distributed data storage system |
US20170032463A1 (en) * | 2015-07-30 | 2017-02-02 | Palantir Technologies Inc. | Systems and user interfaces for holistic, data-driven investigation of bad actor behavior based on clustering and scoring of related data |
CN107004024A (en) * | 2014-12-12 | 2017-08-01 | 微软技术许可有限责任公司 | The multi-user communication of context driving |
US20180218734A1 (en) * | 2017-01-31 | 2018-08-02 | Microsoft Technology Licensing, Llc | Associating meetings with projects using characteristic keywords |
US10055421B1 (en) * | 2017-04-14 | 2018-08-21 | Xactly Corporation | Pre-execution query optimization |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7460549B1 (en) * | 2004-06-18 | 2008-12-02 | Honeywell International Inc. | Resource management for ad hoc wireless networks with cluster organizations |
JP5305840B2 (en) * | 2008-11-04 | 2013-10-02 | キヤノン株式会社 | Printer, control method therefor, and program |
US9805108B2 (en) * | 2010-12-23 | 2017-10-31 | Mongodb, Inc. | Large distributed database clustering systems and methods |
US10073840B2 (en) * | 2013-12-20 | 2018-09-11 | Microsoft Technology Licensing, Llc | Unsupervised relation detection model training |
US9633073B1 (en) * | 2014-03-24 | 2017-04-25 | Amazon Technologies, Inc. | Distributed data store for hierarchical data |
US20170249325A1 (en) * | 2016-02-26 | 2017-08-31 | Microsoft Technology Licensing, Llc | Proactive favorite leisure interest identification for personalized experiences |
US10691700B1 (en) * | 2016-12-30 | 2020-06-23 | Uber Technologies, Inc. | Table replica allocation in a replicated storage system |
US20190286757A1 (en) * | 2018-03-15 | 2019-09-19 | Ca, Inc. | Reducing search space for graph similarity calculations |
US10684947B2 (en) * | 2018-04-20 | 2020-06-16 | International Business Machines Corporation | Increasing high performance data storage utilization by reducing write operations |
-
2018
- 2018-08-21 US US16/107,452 patent/US20200065415A1/en active Pending
-
2019
- 2019-07-25 CN CN201980052412.0A patent/CN112567359A/en active Pending
- 2019-07-25 EP EP19753223.7A patent/EP3818453A1/en active Pending
- 2019-07-25 WO PCT/US2019/043389 patent/WO2020040928A1/en unknown
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100299343A1 (en) * | 2009-05-22 | 2010-11-25 | Microsoft Corporation | Identifying Task Groups for Organizing Search Results |
US20130024443A1 (en) * | 2011-01-24 | 2013-01-24 | Aol Inc. | Systems and methods for analyzing and clustering search queries |
CN104699567A (en) * | 2013-10-21 | 2015-06-10 | 国际商业机器公司 | Method and system for recovering data objects in a distributed data storage system |
CN107004024A (en) * | 2014-12-12 | 2017-08-01 | 微软技术许可有限责任公司 | The multi-user communication of context driving |
US20170032463A1 (en) * | 2015-07-30 | 2017-02-02 | Palantir Technologies Inc. | Systems and user interfaces for holistic, data-driven investigation of bad actor behavior based on clustering and scoring of related data |
US20180218734A1 (en) * | 2017-01-31 | 2018-08-02 | Microsoft Technology Licensing, Llc | Associating meetings with projects using characteristic keywords |
US10055421B1 (en) * | 2017-04-14 | 2018-08-21 | Xactly Corporation | Pre-execution query optimization |
Non-Patent Citations (1)
Title |
---|
DUC A. TRAN 等: "s-clone: socially-aware data replication for social networks", 《COMPUTER NETWORKS》, vol. 56, no. 7, 3 May 2012 (2012-05-03), pages 2001 - 2013, XP055620949, DOI: 10.1016/j.comnet.2012.02.010 * |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116611664A (en) * | 2023-06-13 | 2023-08-18 | 杭州派迩信息技术有限公司 | Ground clothing label management system, device and method thereof |
CN116611664B (en) * | 2023-06-13 | 2024-02-13 | 杭州派迩信息技术有限公司 | Ground clothing label management system, device and method thereof |
Also Published As
Publication number | Publication date |
---|---|
US20200065415A1 (en) | 2020-02-27 |
WO2020040928A1 (en) | 2020-02-27 |
EP3818453A1 (en) | 2021-05-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CA2978889C (en) | Opportunistic resource migration to optimize resource placement | |
Hu et al. | Flutter: Scheduling tasks closer to data across geo-distributed datacenters | |
US9378067B1 (en) | Automated load balancing across the distributed system of hybrid storage and compute nodes | |
CN112567359A (en) | System for optimizing storage replication in a distributed data analysis system using historical data access patterns | |
US9110727B2 (en) | Automatic replication of virtual machines | |
Ruiz-Alvarez et al. | A model and decision procedure for data storage in cloud computing | |
Golab et al. | Distributed data placement to minimize communication costs via graph partitioning | |
US20190229992A1 (en) | System and Methods for Auto-Tuning Big Data Workloads on Cloud Platforms | |
You et al. | Scalable load balancing in cluster storage systems | |
Limam et al. | Data replication strategy with satisfaction of availability, performance and tenant budget requirements | |
Fang et al. | Integrating workload balancing and fault tolerance in distributed stream processing system | |
Bawankule et al. | Historical data based approach for straggler avoidance in a heterogeneous Hadoop cluster | |
Da Silva et al. | An evaluation of relational and NoSQL distributed databases on a low-power cluster | |
You et al. | Ursa: Scalable load and power management in cloud storage systems | |
US10594620B1 (en) | Bit vector analysis for resource placement in a distributed system | |
Eugster et al. | Big data analytics beyond the single datacenter | |
Tabet et al. | A data replication strategy for document-oriented NoSQL systems | |
Marzuni et al. | Cross-MapReduce: Data transfer reduction in geo-distributed MapReduce | |
Li et al. | Efficient multi-attribute precedence-based task scheduling for edge computing in geo-distributed cloud environment | |
US11860835B1 (en) | Efficient drop column requests in a non-relational data store | |
Shabeera et al. | A novel approach for improving data locality of mapreduce applications in cloud environment through intelligent data placement | |
US11336519B1 (en) | Evaluating placement configurations for distributed resource placement | |
Dong | Extending starfish to support the growing hadoop ecosystem | |
US20230289240A1 (en) | Compute Load Balancing In A Distributed Environment | |
Taft | Elastic database systems |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |