US9276771B1 - Lossless multipath table compression - Google Patents
Lossless multipath table compression Download PDFInfo
- Publication number
- US9276771B1 US9276771B1 US14/039,586 US201314039586A US9276771B1 US 9276771 B1 US9276771 B1 US 9276771B1 US 201314039586 A US201314039586 A US 201314039586A US 9276771 B1 US9276771 B1 US 9276771B1
- Authority
- US
- United States
- Prior art keywords
- rule
- entries
- routing rules
- multipath table
- routing
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/02—Topology update or discovery
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L12/00—Data switching networks
- H04L12/54—Store-and-forward switching systems
- H04L12/56—Packet switching systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/24—Multipath
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/74—Address processing for routing
- H04L45/745—Address table lookup; Address filtering
Definitions
- various routing techniques may be used to transport data packets through the network. There may be multiple paths to transport the data packets between two nodes of the network.
- the network may be configured to split the traffic among these multiple paths.
- a multipath routing technique may be used to determine how the traffic will be split among the multiple paths in the network.
- Exemplary multipath routing techniques may include Weighted Cost MultiPath (WCMP) routing and Equal Cost MultiPath (ECMP) routing.
- WCMP routing technique may distribute the traffic among available paths based on a set of pre-determined ratios. If the pre-determined ratios are equal, the WCMP routing may be a ECMP routing where the traffic is distributed evenly among the available paths.
- WCMP routing may be accomplished using a multipath table.
- the number of entries supported by a multipath table may be limited and may not be able accommodate the various multi-path forwarding rules to support hundreds to thousands of traffic flows. Therefore, a technique is needed to reduce the number of entries to be programmed into a multipath table for a given set of WCMP forwarding rules.
- Various embodiments provide a computer-implemented method comprising obtaining a set of routing rules including at least a first rule and a second rule.
- the first rule and the second rule each includes one or more building blocks.
- the method further comprises programming the first rule into a multipath table of a network supporting weighted cost multipath routing. It is determined whether there is an overlap between the second rule and the first rule. When it is determined that there is the overlap between the second rule and the first rule, one or more remaining building blocks of the second rule that remain outside the overlap to the one or more building blocks of the first rule are added to the multipath table.
- the method also comprises adding all of the one or more building blocks of the second rule to the multipath table when it is determined that there is no overlap between the second rule and the first rule.
- Some embodiments provide a method comprising obtaining a plurality of routing rules ordered in a first order.
- a compression algorithm is performed on the plurality of routing rules to compress the plurality of routing rules into a first set of building blocks by determining two or more of the plurality of routing rules that overlap.
- the method further comprises storing the first set of building blocks in a first multipath table of a network supporting weighted cost multipath routing.
- the plurality of routing rules are re-ordered in a second order.
- the compression algorithm is performed on the re-ordered plurality of routing rules.
- the performing comprises compressing the re-ordered plurality of routing rules to a second set of building blocks by determining two or more of the re-ordered plurality of routing rules that overlap.
- the performing further comprises storing the second set of building blocks in a second multipath table and determining if the second multipath table occupies less memory space than the first multipath table.
- Various embodiments provide a computer-implemented method comprising obtaining a plurality of routing rules.
- a first rule of the plurality of rules includes one or more entries.
- the method further comprises performing a compression algorithm to compress the plurality of routing rules into a first set of building blocks by determining two or more of the plurality of routing rules that overlap.
- the first set of building blocks are stored in a first multipath table of a network supporting weighted cost multipath routing.
- the method further includes scaling up the first rule by adding at least one additional entry to the one or more entries of the first rule.
- the compression algorithm is performed on the plurality of routing rules including the scaled up first rule to compress the plurality of routing rules into a second set of building blocks.
- the second set of building blocks is stored in a second multipath table.
- the method also comprises determining if the second multipath table occupies less memory space than the first multipath table.
- FIG. 1 depicts routing of a data packet in a network using conventional routing techniques
- FIG. 2A illustrates entering two routing rules into a multipath table where the first routing rule is a subset of the second routing rule
- FIG. 2B illustrates entering two routing rules into a multipath table where the first routing rule and the second routing rule partially overlap
- FIG. 3 is a flowchart describing how to reduce the amount of data to be stored in a multipath table
- FIG. 4 is a flowchart describing a compression algorithm to reduce the number of data to be stored in a multipath table
- FIG. 5 is a flowchart describing an optimization algorithm that may be applied following the compression algorithm illustrated in FIG. 4 ;
- FIG. 6A depicts an exemplary embodiment that enters three routing rules to a multipath table using a compression algorithm
- FIG. 6B depicts an exemplary embodiment that enters four routing rules using scaling to a multipath table using a compression algorithm
- FIG. 7 is a flowchart illustrating an exemplary compression algorithm that uses scaling for one or more of the rules
- FIG. 8 depicts an exemplary computing device suitable for use with exemplary embodiments described herein.
- FIG. 9 depicts an exemplary network implementation of processing performed according to an exemplary embodiment.
- a routing rule associated with a network switch may be represented using a building block that includes one or more entries.
- the one or more entries form an unordered enumeration of egress ports of the network switch.
- the weight of an egress port is represented by the number of times the egress port appears in the building block.
- the routing rule may indicate a traffic load balancing ratio among the ports of the switch based on the weights of the ports.
- compression of data may be accomplished by combining one or more of the routing rules into a same building block having one or more entries.
- the building block may be stored in the multipath table, hereby programming the one or more of the rules into the multipath table.
- the exemplary algorithms used for reducing the total space in the multipath table for a given set of routing rules may include the greedy algorithm and the non-deterministic polynomial-time (N-P) hard algorithm.
- FIG. 1 illustrates routing of a data packet in a network using conventional routing techniques.
- the routing technique identifies the egress port of the switch where the data packet will be forwarded.
- the switch 102 when a data packet 100 enters a switch 102 having multiple egress ports, the switch 102 must determine which egress port to use for forwarding the data packet 100 .
- each port of the switch 102 may be connected to a different network device. In some embodiments, more than one of the egress ports of the switch 102 may be connected to a same network device.
- the destination IP address 104 of the data packet 100 is matched against components 108 , 110 in a lookup table 106 .
- the components 108 , 110 of the lookup table 106 may each correspond to a routing rule of the network. As illustrated in FIG. 1 , each components 108 , 110 of the lookup table 106 may include a destination IP address 112 , a base value 114 and a size value 116 associated therewith.
- the destination IP address 104 of the data packet 100 may be compared to the destination IP addresses 112 of the components 108 , 110 .
- the destination IP address 104 of the data packet 100 is (1.1.2.5). This IP address best corresponds to destination IP address 112 of component 110 which has the value of (1.1.2.0/24).
- the egress port of the switch 102 is determined using the base value 114 and the size value 116 associated with the component 110 .
- the size value 116 of component 110 may be used as a modulo-base in a hash/mod function 118 to obtain a hash value using the packet header of the data packet 100 .
- the packet header may include, for example, the source and destination IP addresses, port numbers, etc.
- a modulo function of the hash/mod function 118 may be applied to the hash value.
- the modulo function may yield to a random offset value 120 .
- the random offset value 120 may be in the range of [0, size ⁇ 1]. In the example illustrated in FIG.
- the size value 116 associated with the component 110 is 4. Accordingly, the random offset value 120 may be in the range of [0, 3]. For example, the random offset value 120 may be 3.
- the random offset value 120 may be added to the previously obtained base value 114 using a sum function 122 to produce a final position value 124 .
- the final position value 124 is then used to lookup the egress port value in a multipath table 126 .
- the multipath table 126 may include item pairs 128 that each have an index 130 and a corresponding egress port 132 .
- the final position value 124 may be matched against the indices 130 in the multipath table 126 to determine the egress port 132 for the data packet 100 .
- index value (6) base value (3)+random offset value (3).
- the egress port 132 is determined using the index 130 equivalent to 6.
- the index 130 equivalent to 6 corresponds to egress port 132 equivalent to 3.
- the traffic with destination subset 1.1.2.0/24 i.e. entry 110 of the lookup table 106 , will be uniformly randomly distributed among position indices 130 having values of 3-6.
- the corresponding egress ports 132 indicate that two data elements in the same traffic will be forwarded on the egress port 1 , one data element in the traffic will be forwarded on each of the egress ports 2 and 3 . Therefore, the routing scheme illustrated in FIG. 1 has the traffic load balancing ratio of 2:1:1 among egress ports 1 , 2 and 3 . That is, twice the traffic flows destined to subnet 1.1.2.0/24 will be routed to egress port 1 than egress ports 2 or 3 .
- each traffic group will have to occupy a physical block consisting of a set of consecutive entries in the multipath table 126 , as the lookup is determined by two parameters, the “base” and the “size”, where the size determines the maximum offset from the base due to hash/modulo operations.
- the size of the multipath table 126 grows dramatically as the number of routing rules in the lookup table 106 increases with the scale and complexity of the network.
- the multipath table 126 though extremely efficient and fast for lookup, is relatively small in size and expensive in cost. Thus, the number of the routing rules supported by the switch is limited. If the number of routing rules is too great, the network may be forced to operate in reduced performance with simpler rules (e.g. instead of using multipath routing weights ratio 1:2:3 which require block of size 6, using multipath routing weights ratio 1:1:1 which needs block of size 3), in order to fit into the multipath table 126 .
- the present application provides algorithms to reduce the required total space in the multipath table for a given set of routing rules without compromising the number of routing rules that are desired to be programmed into the multipath table.
- P represents the set of egress ports for WCMP load balancing, the number of egress ports is denoted by
- a building block B is defined as an unordered enumeration of egress ports from P, where a port number in P can appear zero or more times in the building block.
- B 1 ⁇ 1, 1, 2, 3, 4 ⁇ is a building block
- B 3 ⁇ 1, 2, 3, 3 ⁇ is another building block where the egress port 4 is not included in the building block.
- Ni(B) denotes the number of times egress port i appears in building block B.
- R(B) is the set of rules which occupy building block B.
- a routing rule R is a building block where the weight of an egress port i is represented by the number of times the egress port i appears in the building block B.
- R represents the set of routing rules need to be programmed into the multipath table.
- r.
- N(R) denotes the number of entries needed to implement R in the multipath table.
- the first observation is illustrated in FIG. 2A .
- Two routing rules 202 and 204 are provided in FIG. 2A .
- the first routing rule 202 and the second routing rule 204 each represent a traffic load balancing ratio.
- the traffic load balancing ratio represented by the first routing rule 202 is 1:1:1. That is, port 1 , port 2 and port 3 each receive equal amount of network traffic.
- the traffic load balancing ratio represented by the second rule 204 is 2:2:1. That is, ports 1 and 2 receive double the amount of network traffic than port 3 .
- the first routing rule 202 is a complete subset of the second routing rule 204 .
- the building block E 206 is one possible solution that combines the first routing rule 202 and the second routing rule 204 and that other alternative building blocks are possible as long as both rules are properly programmed into the multipath table.
- the building block E 206 may be formed by two smaller building blocks, the building block (A) 208 and the building block (B-A) 210 .
- the second observation is illustrated in FIG. 2B .
- Two routing rules 212 and 214 are provided in FIG. 2B .
- the first routing rule 212 and the second routing rule 214 partially overlap.
- the building block E 216 is one possible solution that combines the first routing rule 212 and the second routing rule 214 and that other alternative building blocks are possible as long as both rules are properly programmed into the multipath table.
- the building block E 216 may be formed by shuffling the entries in a first rule and appending or pre-pending the entries of a second rule to the first rule.
- shuffling the entries in a rule does not affect the traffic load balancing ratio defined by that rule.
- the entries of the first rule 212 are shuffled to form the building block (A) 218 without modifying the traffic load balancing ratio of 2:0:1:1.
- the entries of the second rule 214 are also shuffled to form the building block (B) 219 without modifying the traffic load balancing ratio of 2:2:1:0.
- the building block (A) 218 is pre-pended to the building block (B) 219 to form the building block E 216 .
- the building block E 216 may be formed by three smaller building blocks, the building block (A-B) 220 , the building block (A ⁇ B) 216 , and the building block (B-A) 222 .
- the routing rules for a hardware device may be determined by the network manager or upper layer services of the network.
- the present application may apply a compression algorithm and, optionally, an optimization algorithm.
- the approach is illustrated in FIG. 3 .
- the set of routing rules are received for programming into a multipath table associated with a hardware element of a network.
- a compression algorithm may be applied to the received set of routing rules.
- the compression algorithm may reduce the amount of data associated with the routing rules to be programmed into the multipath table.
- the present application preserves all the routing rules and programs all of the received routing rules into the multipath table.
- an optimization algorithm may be applied to the compressed set of rules at step 310 .
- the compression algorithm and the optimization algorithm are discussed below in greater detail in connection with FIGS. 4 and 5 , respectively.
- FIG. 4 illustrates an exemplary compression algorithm 400 according to various embodiments.
- a set of routing rules are obtained.
- the set of routing rules includes a first rule and a second rule.
- the first rule is programmed into a multipath table associated with one or more hardware devices of a network.
- the network may support WCMP routing.
- Programming the routing rules into the multipath table may be accomplished by storing building blocks in the multipath table, as discussed above in connection with FIGS. 2A-2B .
- the entries of the first rule may be shuffled before programming the first rule into the multipath table. The shuffling of the entries in a rule does not modify the traffic load balancing ratio defined by that routing rule.
- step 406 it is determined whether the second rule overlaps with the first rule. If the second rule overlaps with the first rule (YES to step 406 ), it is determined whether the overlap is a complete overlap, i.e. the second rule is a subset of the first rule (step 408 ). If the second rule is a complete subset of the first rule (YES to step 408 ), the second rule is considered to be already programmed into the multipath table (step 414 ) because the first rule that encompasses the second rule has been previously programmed into the multipath table at step 404 .
- the second rule is programmed into the multipath table by adding to the building blocks of the first rule the building blocks of the second rule which remain outside the overlap (step 412 ).
- the second rule may be programmed into the multipath table by appending or pre-pending one or more building blocks to the one or more building blocks of the first rule that are already programmed into the multipath table.
- the first rule and the second rule may not be compressed.
- the second rule will be programmed into the multipath table as is, i.e. without compression (step 410 ).
- the exemplary compression algorithm discussed in connection with FIG. 4 may be implemented using a greedy algorithm as follows:
- the greedy algorithm provided above may not produce a globally optimal solution while the problem is N-P hard.
- An optimization algorithm may be applied to the set of routing rules following the greedy algorithm to increase the chance of finding the optimal solution.
- the optimization algorithm may change the ordering of the routing rules in the iterations.
- the routing rules may be ordered by the increasing and/or decreasing value of N(R).
- the compression algorithm may be run over a set of randomly ordered routing rules (variations of R), and the best, e.g. more compact, solution may be selected over the iterations. This approached may be deemed as equivalent to randomizing the greedy algorithm discussed above.
- FIG. 5 illustrates an exemplary optimization algorithm 500 that modifies the ordering of the routing rules in a given set of routing rules.
- a plurality of routing rules are obtained as arranged in a first order.
- a compression algorithm such as one discussed in connection with FIG. 4 , is performed on the plurality of routing rules ordered in the first order to compress the rules into a first reduced set of building blocks (step 504 ).
- the first set of building blocks is stored in a first multipath table (step 506 ).
- the plurality of rules that are obtained at step 502 are re-ordered in a second order (step 508 ).
- the compression algorithm is performed on the plurality of routing rules ordered in the second order to compress the rules into a second set of building blocks (step 510 ).
- the second set of building blocks is stored in a second multipath table (step 512 ).
- the second multipath table is compared to the first multipath table to determine whether the second multipath table occupies less memory space than the first multipath table (step 514 ). As a result, the multipath table that occupies the less space may be deemed to be the optimal solution.
- routing rules as provided or merely shuffling the entries within the routing rules may not necessarily produce the optimal solution.
- FIGS. 6A-6B One such example is illustrated in FIGS. 6A-6B .
- FIG. 6A illustrates three routing rules, rule A 602 , rule B 604 and rule C 606 that will be programmed into a multipath table.
- a compression algorithm such as the compression algorithm 400 discussed above in connection with FIG. 4 . It may be determined that rule A 602 and rule B 604 overlap, i.e. rule A 602 is a subset of rule B 604 . It may also be determined that rule B and rule C partially overlap.
- rule A 602 , rule B 604 and rule C 606 may be programmed to the multipath table using the building block 610 .
- an additional rule D 608 may be desired to be added to the same multipath table as rule A 602 , rule B 604 and rule C 606 . Reshuffling the elements of rule C 606 and rule D 608 within each respective rule may allow programming all four rules into the multipath table as building block 612 . Accordingly, if the rules A, B, C and D remain unchanged, the most optimal solution requires 14 multipath table entries. However, there may be a better solution, i.e. a building block with less entries, for programming all four rules A, B, C and D into the multipath table.
- rule D 608 may be scaled up. Scaling up a rule does not modify the traffic load balancing ratio defined by the rule. For example, pre-scaling, rule D 608 has the traffic load balancing ratio of 1:1:1:1. That is, port 1 , port 2 , port 3 and port 4 each receive equal amount of network traffic. The rule D 608 may be scaled up by a factor of 2 to produce scaled rule D′ 618 that has the traffic load balancing ratio of 2:2:2:2. Accordingly, after scaling, port 1 , port 2 , port 3 and port 4 still receive equal amount of network traffic.
- the four rules A, B, C and D may be entered into a multipath table using the building block 614 that includes 12 entries. Accordingly, an improvement to the above greedy algorithm may be to supply R, 2R, . . . , up to K*R for a small constant K to the Greedy() function call.
- FIG. 7 illustrates an exemplary compression algorithm 700 that uses scaling for one or more of the routing rules.
- a plurality of routing rules is obtained.
- the plurality of rules may include a first rule defining a first traffic load balancing ratio.
- a compression algorithm such as one discussed in connection with FIG. 4 , is performed on the plurality of routing rules to compress the plurality of routing routes into a first set of building blocks (step 704 ).
- the first set building blocks is stored in a first multipath table (step 706 ).
- the first rule is scaled up by increasing the first traffic load balancing ratio by a given factor to produce a scaled up first rule (step 708 ).
- the scaling may be performed on any given rule of the plurality of routing rules.
- the scaling may be performed more than once.
- a different rule among the plurality of routing rules may be scaled up.
- the compression algorithm is performed on the plurality of routing rules including the scaled up first rule to generate a second set of building blocks (step 710 ).
- the second set of building blocks is stored in a second multipath table (step 712 ).
- the second multipath table is compared to the first multipath table to determine whether the second multipath table occupies less memory space than the first multipath table (step 714 ). As a result, the multipath table that occupies the less space may be deemed to be the optimal solution.
- FIG. 8 depicts an example of an electronic device 800 that may be suitable for use with one or more acts disclosed herein.
- the electronic device 800 may take many forms, including but not limited to a computer, workstation, server, network computer, quantum computer, optical computer, Internet appliance, mobile device, a pager, a tablet computer, a smart sensor, application specific processing device, etc.
- the electronic device 800 is illustrative and may take other forms.
- an alternative implementation of the electronic device 800 may have fewer components, more components, or components that are in a configuration that differs from the configuration of FIG. 8 .
- the components of FIG. 8 and/or other figures described herein may be implemented using hardware based logic, software based logic and/or logic that is a combination of hardware and software based logic (e.g., hybrid logic); therefore, components illustrated in FIG. 8 and/or other figures are not limited to a specific type of logic.
- the processor 802 may include hardware based logic or a combination of hardware based logic and software to execute instructions on behalf of the electronic device 800 .
- the processor 802 may include logic that may interpret, execute, and/or otherwise process information contained in, for example, the memory 804 .
- the information may include computer-executable instructions and/or data that may implement one or more embodiments of the invention.
- the processor 802 may comprise a variety of homogeneous or heterogeneous hardware.
- the hardware may include, for example, some combination of one or more processors, microprocessors, field programmable gate arrays (FPGAs), application specific instruction set processors (ASIPs), application specific integrated circuits (ASICs), complex programmable logic devices (CPLDs), graphics processing units (GPUs), or other types of processing logic that may interpret, execute, manipulate, and/or otherwise process the information.
- the processor may include a single core or multiple cores 803 .
- the processor 802 may include a system-on-chip (SoC) or system-in-package (SiP).
- the electronic device 800 may include one or more tangible non-transitory computer-readable storage media for storing one or more computer-executable instructions or software that may implement one or more embodiments of the invention.
- the non-transitory computer-readable storage media may be, for example, the memory 804 or the storage 818 .
- the memory 804 may comprise a ternary content addressable memory (TCAM) and/or a RAM that may include RAM devices that may store the information.
- TCAM ternary content addressable memory
- the RAM devices may be volatile or non-volatile and may include, for example, one or more DRAM devices, flash memory devices, SRAM devices, zero-capacitor RAM (ZRAM) devices, twin transistor RAM (TTRAM) devices, read-only memory (ROM) devices, ferroelectric RAM (FeRAM) devices, magneto-resistive RAM (MRAM) devices, phase change memory RAM (PRAM) devices, or other types of RAM devices.
- DRAM dynamic random access memory
- SRAM zero-capacitor RAM
- ZRAM twin transistor RAM
- ROM read-only memory
- FeRAM ferroelectric RAM
- MRAM magneto-resistive RAM
- PRAM phase change memory RAM
- One or more computing devices 800 may include a virtual machine (VM) 805 for executing the instructions loaded in the memory 804 .
- a virtual machine 805 may be provided to handle a process running on multiple processors so that the process may appear to be using only one computing resource rather than multiple computing resources. Virtualization may be employed in the electronic device 800 so that infrastructure and resources in the electronic device may be shared dynamically. Multiple VMs 805 may be resident on a single computing device 800 .
- a hardware accelerator 806 may be implemented in an ASIC, FPGA, or some other device.
- the hardware accelerator 806 may be used to reduce the general processing time of the electronic device 800 .
- the electronic device 800 may include a network interface 808 to interface to a Local Area Network (LAN), Wide Area Network (WAN) or the Internet through a variety of connections including, but not limited to, standard telephone lines, LAN or WAN links (e.g., T1, T3, 76 kb, X.25), broadband connections (e.g., integrated services digital network (ISDN), Frame Relay, asynchronous transfer mode (ATM), wireless connections (e.g., 802.11), high-speed interconnects (e.g., InfiniBand, gigabit Ethernet, Myrinet) or some combination of any or all of the above.
- LAN Local Area Network
- WAN Wide Area Network
- the Internet may include a network interface 808 to interface to a Local Area Network (LAN), Wide Area Network (WAN) or the Internet through a variety of connections including, but not limited to, standard telephone lines, LAN or WAN links (e.g., T1, T3, 76 kb, X.25), broadband connections (e.g., integrated services
- the network interface 808 may include a built-in network adapter, network interface card, personal computer memory card international association (PCMCIA) network card, card bus network adapter, wireless network adapter, universal serial bus (USB) network adapter, modem or any other device suitable for interfacing the electronic device 800 to any type of network capable of communication and performing the operations described herein.
- PCMCIA personal computer memory card international association
- USB universal serial bus
- the electronic device 800 may include one or more input devices 810 , such as a keyboard, a multi-point touch interface, a pointing device (e.g., a mouse), a gyroscope, an accelerometer, a haptic device, a tactile device, a neural device, a microphone, or a camera that may be used to receive input from, for example, a user.
- input devices 810 such as a keyboard, a multi-point touch interface, a pointing device (e.g., a mouse), a gyroscope, an accelerometer, a haptic device, a tactile device, a neural device, a microphone, or a camera that may be used to receive input from, for example, a user.
- input devices 810 such as a keyboard, a multi-point touch interface, a pointing device (e.g., a mouse), a gyroscope, an accelerometer, a haptic device, a tactile device, a neural device, a
- the input devices 810 may allow a user to provide input that is registered on a visual display device 814 .
- a graphical user interface (GUI) 816 may be shown on the display device 814 .
- a storage device 818 may also be associated with the computer 800 .
- the storage device 818 may be accessible to the processor 802 via an I/O bus.
- the information may be executed, interpreted, manipulated, and/or otherwise processed by the processor 802 .
- the storage device 818 may include, for example, a storage device, such as a magnetic disk, optical disk (e.g., CD-ROM, DVD player), random-access memory (RAM) disk, tape unit, and/or flash drive.
- the information may be stored on one or more non-transient tangible computer-readable media contained in the storage device.
- This media may include, for example, magnetic discs, optical discs, magnetic tape, and/or memory devices (e.g., flash memory devices, static RAM (SRAM) devices, dynamic RAM (DRAM) devices, or other memory devices).
- memory devices e.g., flash memory devices, static RAM (SRAM) devices, dynamic RAM (DRAM) devices, or other memory devices.
- the information may include data and/or computer-executable instructions that may implement one or more embodiments of the invention
- the storage device 818 may further store applications 824 , and the electronic device 800 can be running an operating system (OS) 826 .
- OS 826 may include the Microsoft® Windows® operating systems, the Unix and Linux operating systems, the MacOS® for Macintosh computers, an embedded operating system, such as the Symbian OS, a real-time operating system, an open source operating system, a proprietary operating system, operating systems for mobile electronic devices, or other operating system capable of running on the electronic device and performing the operations described herein.
- the operating system may be running in native mode or emulated mode.
- One or more embodiments of the invention may be implemented using computer-executable instructions and/or data that may be embodied on one or more non-transitory tangible computer-readable mediums.
- the mediums may be, but are not limited to, a hard disk, a compact disc, a digital versatile disc, a flash memory card, a Programmable Read Only Memory (PROM), a Random Access Memory (RAM), a Read Only Memory (ROM), Magnetoresistive Random Access Memory (MRAM), a magnetic tape, or other computer-readable media.
- FIG. 9 depicts a network implementation that may implement one or more embodiments of the invention.
- a system 900 may include the computing device 800 , a network 912 , a service provider 913 , a target environment 914 , and a cluster 915 .
- the system 900 may be a distributed system including a plurality of computing nodes, i.e. units of execution 916 , that implement one or more embodiments of the invention.
- the embodiment of FIG. 9 is exemplary, and other embodiments can include more devices, fewer devices, or devices in arrangements that differ from the arrangement of FIG. 9 .
- the network 912 may transport data from a source to a destination.
- Embodiments of the network 912 may use network devices, such as routers, switches, firewalls, and/or servers (not shown) and connections (e.g., links) to transport data.
- Data may refer to any type of machine-readable information having substantially any format that may be adapted for use in one or more networks and/or with one or more devices (e.g., the computing device 800 , the service provider 913 , etc.).
- Data may include digital information or analog information.
- Data may further be packetized and/or non-packetized.
- the network 912 may be a hardwired network using wired conductors and/or optical fibers and/or may be a wireless network using free-space optical, radio frequency (RF), and/or acoustic transmission paths.
- the network 912 may be a substantially open public network, such as the Internet.
- the network 912 may be a more restricted network, such as a corporate virtual network.
- the network 912 may include Internet, intranet, Local Area Network (LAN), Wide Area Network (WAN), Metropolitan Area Network (MAN), wireless network (e.g., using IEEE 802.11), or other type of network
- the network 912 may use middleware, such as Common Object Request Broker Architecture (CORBA) or Distributed Component Object Model (DCOM). Implementations of networks and/or devices operating on networks described herein are not limited to, for example, any particular data type, protocol, and/or architecture/configuration.
- CORBA Common Object Request Broker Architecture
- DCOM Distributed Component Object Model
- the service provider 913 may include a device that makes a service available to another device.
- the service provider 913 may include an entity (e.g., an individual, a corporation, an educational institution, a government agency, etc.) that provides one or more services to a destination using a server and/or other devices.
- Services may include instructions that are executed by a destination to perform an operation (e.g., an optimization operation).
- a service may include instructions that are executed on behalf of a destination to perform an operation on the destination's behalf.
- the server 914 may include a device that receives information over the network 912 .
- the server 914 may be a device that receives user input from the computer 800 .
- the cluster 915 may include a number of units of execution (UEs) 916 and may perform processing on behalf of the computer 800 and/or another device, such as the service provider 913 or server 914 .
- the cluster 915 may perform parallel processing on an operation received from the computer 800 .
- the cluster 915 may include UEs 916 that reside on a single device or chip or that reside on a number of devices or chips.
- the units of execution (UEs) 916 may include processing devices that perform operations on behalf of a device, such as a requesting device.
- a UE may be a microprocessor, field programmable gate array (FPGA), and/or another type of processing device.
- UE 916 may include code, such as code for an operating environment. For example, a UE may run a portion of an operating environment that pertains to parallel processing activities.
- the service provider 913 may operate the cluster 915 and may provide interactive optimization capabilities to the computer 800 on a subscription basis (e.g., via a web service).
- a hardware unit of execution may include a device (e.g., a hardware resource) that may perform and/or participate in parallel programming activities.
- a hardware unit of execution may perform and/or participate in parallel programming activities in response to a request and/or a task it has received (e.g., received directly or via a proxy).
- a hardware unit of execution may perform and/or participate in substantially any type of parallel programming (e.g., task, data, stream processing, etc.) using one or more devices.
- a hardware unit of execution may include a single processing device that includes multiple cores or a number of processors.
- a hardware unit of execution may also be a programmable device, such as a field programmable gate array (FPGA), an application specific integrated circuit (ASIC), a digital signal processor (DSP), or other programmable device.
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- DSP digital signal processor
- Devices used in a hardware unit of execution may be arranged in many different configurations (or topologies), such as a grid, ring, star, or other configuration.
- a hardware unit of execution may support one or more threads (or processes) when performing processing operations.
- a software unit of execution may include a software resource (e.g., a technical computing environment) that may perform and/or participate in one or more parallel programming activities.
- a software unit of execution may perform and/or participate in one or more parallel programming activities in response to a receipt of a program and/or one or more portions of the program.
- a software unit of execution may perform and/or participate in different types of parallel programming using one or more hardware units of execution.
- a software unit of execution may support one or more threads and/or processes when performing processing operations.
- one or more implementations consistent with principles of the invention may be implemented using one or more devices and/or configurations other than those illustrated in the Figures and described in the Specification without departing from the spirit of the invention.
- One or more devices and/or components may be added and/or removed from the implementations of the figures depending on specific deployments and/or applications.
- one or more disclosed implementations may not be limited to a specific combination of hardware.
- logic may perform one or more functions.
- This logic may include hardware, such as hardwired logic, an application-specific integrated circuit, a field programmable gate array, a microprocessor, software, or a combination of hardware and software.
- the article “a” is intended to include one or more items. Where only one item is intended, the term “a single” or similar language is used. Further, the phrase “based on,” as used herein is intended to mean “based, at least in part, on” unless explicitly stated otherwise.
- the term “user”, as used herein, is intended to be broadly interpreted to include, for example, an electronic device (e.g., a workstation) or a user of an electronic device, unless stated otherwise. The conjunction “or” is meant to be inclusive, unless stated otherwise.
Abstract
Description
index value (6)=base value (3)+random offset value (3).
Thus, in the example illustrated in
N(R={A, B})=N(B).
N(R={A, B})=N(A∪B).
-
- while (R≠∅) {
- R=R.pop_front {};
- <L*, R*, I*, M*>=<−1, −1, −1, N(R)>;
- for (i=0; i<=|B|; ++i) {
- <L, R, I, M>=Greedy(B, i, R);
- if (M*>M) {
- <L*, R*, I*, M*>=<L, R, I, M>;
- }
- if (M*==0) break;
- }
- Update (R, B, <L*, R*, I*, M*>);
- }
- while (R≠∅) {
-
- // Grow the building blocks towards the right as long as they
- // can be covered by R.
- right=i+1;
- s=Ø;
- while (Exist(Bright) && {S,Bright}⊂R) {
- S.append(Bright)
- right++;
- }
- if (R⊂(BL, S, Bright)) {
- return<i, right, −1, 0>;
- } else {
- // Find a location where we can insert extra building blocks
- // without breaking the rules which are already programmed.
- for (k=i; k<right; k++) {
- if (R(BK)==Ø||R(BK-1)==Ø||R(BK)∩R(BK-1)==Ø) {
- M=N(R)-N(R∩(BL, S, Bright));
- return<i, right, k, M>;
- }
- if (R(BK)==Ø||R(BK-1)==Ø||R(BK)∩R(BK-1)==Ø) {
- }
- return<−1, −1, −1, N(R)>;
- }
-
- if (<L*, R*, I*, M*>==<−1, −1, −1, N(R)>) {
- B=new Block(R);
- R(B)=R;
- B.append(B);
- } else {
- s=∅;
- for (k=L*+1; k<R*; k++) {
- R(BK).add(R);
- S.append(Bk);
- }
- if (Exist(BL*) {
- BL=BL*∩(R-S);
- if (BL!=∅) {
- R(BL)=R(BL*)+R;
- BLL=BL*-BL;
- R(BLL)=R(BL*);
- {
- }
- if (I* !=−1) {
- BR=BR*∩(R-S-BL);
- if (BR!=∅) {
- R(BR)=R(BR*)=R;
- BRR=BR*-BR;
- R(BRR)=R(BR*);
- }
- }
- if (I* !=−1) {
- BI=R-S-BLBR;
- R(BI)=R;
- B.insert_at_pos(I*, BI);
- }
- if (BL!=∅) B.replace(BL*, {BLL, BL});
- if (BR!=∅) B.replace(BR*, {BRR, BL});
- }
- if (<L*, R*, I*, M*>==<−1, −1, −1, N(R)>) {
Claims (13)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/039,586 US9276771B1 (en) | 2013-09-27 | 2013-09-27 | Lossless multipath table compression |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/039,586 US9276771B1 (en) | 2013-09-27 | 2013-09-27 | Lossless multipath table compression |
Publications (1)
Publication Number | Publication Date |
---|---|
US9276771B1 true US9276771B1 (en) | 2016-03-01 |
Family
ID=55360106
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/039,586 Active 2034-04-22 US9276771B1 (en) | 2013-09-27 | 2013-09-27 | Lossless multipath table compression |
Country Status (1)
Country | Link |
---|---|
US (1) | US9276771B1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN106126315A (en) * | 2016-06-17 | 2016-11-16 | 广东工业大学 | A kind of virtual machine distribution method in the data center of minimization communication delay |
US10824737B1 (en) * | 2017-02-22 | 2020-11-03 | Assa Abloy Ab | Protecting data from brute force attack |
US20220210058A1 (en) * | 2019-05-23 | 2022-06-30 | Hewlett Packard Enterprise Development Lp | Fat tree adaptive routing |
USRE49334E1 (en) | 2005-10-04 | 2022-12-13 | Hoffberg Family Trust 2 | Multifactorial optimization system and method |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6779030B1 (en) * | 1997-10-06 | 2004-08-17 | Worldcom, Inc. | Intelligent network |
US20070087756A1 (en) * | 2005-10-04 | 2007-04-19 | Hoffberg Steven M | Multifactorial optimization system and method |
US20090252033A1 (en) * | 2008-04-08 | 2009-10-08 | At&T Knowledge Ventures, L.P. | System and method of distributing media content |
US7903666B1 (en) | 2008-03-31 | 2011-03-08 | Extreme Networks, Inc. | Method and system for compressing route entries in a route table based on equal-cost multi-paths (ECMPs) matches |
US7936764B1 (en) | 2008-04-09 | 2011-05-03 | Extreme Networks, Inc. | Method for optimizing IP route table size through IP route aggregation |
US20130182712A1 (en) * | 2012-01-13 | 2013-07-18 | Dan Aguayo | System and method for managing site-to-site vpns of a cloud managed network |
US20130279503A1 (en) | 2012-02-17 | 2013-10-24 | Rockstar Consortium Us Lp | Next Hop Computation Functions for Equal Cost Multi-Path Packet Switching Networks |
-
2013
- 2013-09-27 US US14/039,586 patent/US9276771B1/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6779030B1 (en) * | 1997-10-06 | 2004-08-17 | Worldcom, Inc. | Intelligent network |
US20070087756A1 (en) * | 2005-10-04 | 2007-04-19 | Hoffberg Steven M | Multifactorial optimization system and method |
US7903666B1 (en) | 2008-03-31 | 2011-03-08 | Extreme Networks, Inc. | Method and system for compressing route entries in a route table based on equal-cost multi-paths (ECMPs) matches |
US20090252033A1 (en) * | 2008-04-08 | 2009-10-08 | At&T Knowledge Ventures, L.P. | System and method of distributing media content |
US7936764B1 (en) | 2008-04-09 | 2011-05-03 | Extreme Networks, Inc. | Method for optimizing IP route table size through IP route aggregation |
US20130182712A1 (en) * | 2012-01-13 | 2013-07-18 | Dan Aguayo | System and method for managing site-to-site vpns of a cloud managed network |
US20130279503A1 (en) | 2012-02-17 | 2013-10-24 | Rockstar Consortium Us Lp | Next Hop Computation Functions for Equal Cost Multi-Path Packet Switching Networks |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
USRE49334E1 (en) | 2005-10-04 | 2022-12-13 | Hoffberg Family Trust 2 | Multifactorial optimization system and method |
CN106126315A (en) * | 2016-06-17 | 2016-11-16 | 广东工业大学 | A kind of virtual machine distribution method in the data center of minimization communication delay |
US10824737B1 (en) * | 2017-02-22 | 2020-11-03 | Assa Abloy Ab | Protecting data from brute force attack |
US11874935B2 (en) | 2017-02-22 | 2024-01-16 | Assa Abloy Ab | Protecting data from brute force attack |
US20220210058A1 (en) * | 2019-05-23 | 2022-06-30 | Hewlett Packard Enterprise Development Lp | Fat tree adaptive routing |
US11973685B2 (en) * | 2019-05-23 | 2024-04-30 | Hewlett Packard Enterprise Development Lp | Fat tree adaptive routing |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3593503B1 (en) | Packet processor in virtual filtering platform | |
US10171362B1 (en) | System and method for minimizing disruption from failed service nodes | |
JP6177890B2 (en) | System and method for routing traffic between separate InfiniBand subnets based on fat tree routing | |
ES2720759T3 (en) | Download packet processing for virtualization of network devices | |
US10749805B2 (en) | Statistical collection in a network switch natively configured as a load balancer | |
US9992153B2 (en) | Managing link aggregation traffic in edge nodes | |
US9124652B1 (en) | Per service egress link selection | |
US10243914B2 (en) | Managing link aggregation traffic in edge nodes | |
JP6574054B2 (en) | Packet forwarding | |
US9276771B1 (en) | Lossless multipath table compression | |
JP2016134876A (en) | Information processing system, information processor, and method for controlling information processing system | |
US11343190B2 (en) | TCAM-based load balancing on a switch | |
US20140241347A1 (en) | Static translation of network forwarding plane models into target implementation in the hardware abstraction layer | |
WO2014154124A1 (en) | Packet forwarding | |
JP6437692B2 (en) | Packet forwarding | |
US20140241346A1 (en) | Translating network forwarding plane models into target implementation using network primitives | |
US10110668B1 (en) | System and method for monitoring service nodes | |
EP2959379B1 (en) | Implementing specifications related to a network forwarding plane of an electronic device having forwarding functionality | |
US10447585B2 (en) | Programmable and low latency switch fabric for scale-out router | |
US9479437B1 (en) | Efficient updates of weighted cost multipath (WCMP) groups | |
US9716657B1 (en) | TCP connection resiliency in multipath networks | |
US10305816B1 (en) | Adjustable bit mask for high-speed native load balancing on a switch | |
US9491102B1 (en) | Traffic load balancing in a multi-connect topology | |
JP2024507143A (en) | Scaling IP addresses in overlay networks | |
US9363162B1 (en) | Determining WCMP link capacities in multi-stage networks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:JI, ZHENGRONG;ZHOU, JUNLAN;SIGNING DATES FROM 20130925 TO 20130930;REEL/FRAME:031676/0598 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |