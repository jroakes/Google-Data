US20180270388A1 - Semi-Transparent Embedded Watermarks - Google Patents
Semi-Transparent Embedded Watermarks Download PDFInfo
- Publication number
- US20180270388A1 US20180270388A1 US15/712,395 US201715712395A US2018270388A1 US 20180270388 A1 US20180270388 A1 US 20180270388A1 US 201715712395 A US201715712395 A US 201715712395A US 2018270388 A1 US2018270388 A1 US 2018270388A1
- Authority
- US
- United States
- Prior art keywords
- image
- pixels
- encoded
- watermark
- pixel
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000004044 response Effects 0.000 claims description 66
- 238000000034 method Methods 0.000 claims description 64
- 238000013507 mapping Methods 0.000 claims description 13
- 238000010191 image analysis Methods 0.000 claims description 10
- 238000009877 rendering Methods 0.000 claims description 7
- 239000011159 matrix material Substances 0.000 claims 1
- 230000008569 process Effects 0.000 description 27
- 238000011084 recovery Methods 0.000 description 14
- 230000015654 memory Effects 0.000 description 13
- 239000003086 colorant Substances 0.000 description 9
- 238000004891 communication Methods 0.000 description 7
- 239000002131 composite material Substances 0.000 description 7
- 238000002156 mixing Methods 0.000 description 7
- 238000012545 processing Methods 0.000 description 7
- 238000004590 computer program Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 230000000694 effects Effects 0.000 description 4
- 238000004458 analytical method Methods 0.000 description 3
- 238000012854 evaluation process Methods 0.000 description 3
- 230000014509 gene expression Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000001934 delay Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 230000003116 impacting effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32144—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title embedded in the image data, i.e. enclosed or integrated in the image, e.g. watermark, super-imposed logo or stamp
- H04N1/32149—Methods relating to embedding, encoding, decoding, detection or retrieval operations
- H04N1/32331—Fragile embedding or watermarking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/10—Protecting distributed programs or content, e.g. vending or licensing of copyrighted material ; Digital rights management [DRM]
- G06F21/16—Program or content traceability, e.g. by watermarking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/0021—Image watermarking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/0021—Image watermarking
- G06T1/0028—Adaptive watermarking, e.g. Human Visual System [HVS]-based watermarking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32144—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title embedded in the image data, i.e. enclosed or integrated in the image, e.g. watermark, super-imposed logo or stamp
- H04N1/32149—Methods relating to embedding, encoding, decoding, detection or retrieval operations
- H04N1/32203—Spatial or amplitude domain methods
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32144—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title embedded in the image data, i.e. enclosed or integrated in the image, e.g. watermark, super-imposed logo or stamp
- H04N1/32149—Methods relating to embedding, encoding, decoding, detection or retrieval operations
- H04N1/32203—Spatial or amplitude domain methods
- H04N1/32208—Spatial or amplitude domain methods involving changing the magnitude of selected pixels, e.g. overlay of information or super-imposition
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32144—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title embedded in the image data, i.e. enclosed or integrated in the image, e.g. watermark, super-imposed logo or stamp
- H04N1/32149—Methods relating to embedding, encoding, decoding, detection or retrieval operations
- H04N1/32203—Spatial or amplitude domain methods
- H04N1/32229—Spatial or amplitude domain methods with selective or adaptive application of the additional information, e.g. in selected regions of the image
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32144—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title embedded in the image data, i.e. enclosed or integrated in the image, e.g. watermark, super-imposed logo or stamp
- H04N1/32149—Methods relating to embedding, encoding, decoding, detection or retrieval operations
- H04N1/32203—Spatial or amplitude domain methods
- H04N1/32251—Spatial or amplitude domain methods in multilevel data, e.g. greyscale or continuous tone data
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32144—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title embedded in the image data, i.e. enclosed or integrated in the image, e.g. watermark, super-imposed logo or stamp
- H04N1/32149—Methods relating to embedding, encoding, decoding, detection or retrieval operations
- H04N1/32288—Multiple embedding, e.g. cocktail embedding, or redundant embedding, e.g. repeating the additional information at a plurality of locations in the image
- H04N1/32293—Repeating the additional information in a regular pattern
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32144—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title embedded in the image data, i.e. enclosed or integrated in the image, e.g. watermark, super-imposed logo or stamp
- H04N1/32149—Methods relating to embedding, encoding, decoding, detection or retrieval operations
- H04N1/32309—Methods relating to embedding, encoding, decoding, detection or retrieval operations in colour image data
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/42—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation
- H04N19/423—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation characterised by memory arrangements
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/426—Internal components of the client ; Characteristics thereof
- H04N21/42684—Client identification by a unique number or address, e.g. serial number, MAC address, socket ID
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/835—Generation of protective data, e.g. certificates
- H04N21/8358—Generation of protective data, e.g. certificates involving watermark
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2201/00—General purpose image data processing
- G06T2201/005—Image watermarking
- G06T2201/0051—Embedding of the watermark in the spatial domain
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2201/00—General purpose image data processing
- G06T2201/005—Image watermarking
- G06T2201/0083—Image watermarking whereby only watermarked image required at decoder, e.g. source-based, blind, oblivious
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N2201/3201—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N2201/3225—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of data relating to an image, a page or a document
- H04N2201/3233—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of data relating to an image, a page or a document of authentication information, e.g. digital signature, watermark
Definitions
- This specification describes systems, methods, devices, and other techniques for generating watermark images to supplement source images that are displayed, for example, as third-party content in an electronic document.
- this specification describes systems, methods, devices, and other techniques for recovering information encoded in an encoded source image that shows a semi-transparent watermark overlaid on a source image.
- a system uses information recovered from an encoded source image to identify a provider of the source image, other characteristics of the source image, or context about a specific impression of the source image.
- the watermark image generator may generate the watermark image by spacing the set of encoded pixels among a set of blank pixels such that each encoded pixel in the watermark image neighbors one or more blank pixels in the watermark image, an in particular at least two blank pixels in the watermark image.
- distributing the encoded pixels among the set of blank pixels may comprise (i) creating an initial watermark canvas that has m rows of pixels and n columns of pixels, wherein each pixel in the initial watermark canvas is a blank pixel; and (ii) for every k th row in the initial watermark canvas, mapping the pixels from a corresponding row of the encoding image to every l th pixel in the respective row of the initial watermark canvas.
- FIGS. 3A-3J illustrate example techniques for generating a watermark image and recovering the watermark image, or a related image, from an encoded source image.
- FIG. 8 is a schematic diagram of a computer system that can be used to carry out the operations described in association with the computer-implemented methods, systems, devices, and other techniques described herein.
- the server system 102 Based on the requests from the client computer 104 , the server system 102 generates responses (e.g., electronic documents) to return to the client computer 104 .
- a given response can include a selected source image 128 a that is configured to be displayed to a user of the client computer 104 , where the source image 128 a is provided by one of the source image providers 106 a - n .
- the server system 102 can augment the response served to the client computer 104 with a semi-transparent watermark image 126 that is arranged for display in a presentation of the response document at the client computer 104 over the source image 128 .
- the displayed content can include the selected source image 128 a and the watermark image 126 displayed over top of the selected source image 128 a in a substantially transparent manner.
- the client computer 104 is a notebook computer, a smartphone, a tablet computer, a desktop computer, or a smartwatch or other wearable device.
- each component 108 - 120 may be defined by instructions stored in memory on one or more computer-readable storage devices, and the operations may be carried out when the instructions are executed by one or more processors of the server system 102 .
- some operations are described herein as being performed by a specific one of the components 108 - 120 by way of example, in other implementations, some or all of the operations of two or more of the components 108 - 120 may be consolidated and performed instead by a single component. In yet other implementations, the operations of any one of the components 108 - 120 may be divided among two or more components.
- the server system 102 can be configured to carry out the techniques illustrated in FIGS. 3A-3I and the processes 400 , 500 , and 700 .
- the operations of processes 400 , 500 , and 700 , and other techniques, are described in further detail below with respect to FIGS. 3A-3I, 4, 5, and 7 .
- An overview of the server system 102 and its components 108 - 120 follows below, with further detail of the operations and other techniques performed by these components described subsequently with respect to FIGS. 3A-3I and FIGS. 4, 5, and 7 .
- the front-end subsystem 108 provides an interface for communicating over one or more networks.
- the front-end subsystem 108 receives requests from the client computer 104 and transmits responses to the requests, along with any content associated with the requests such as a watermark image and, optionally, a source image, to the client computer 104 .
- the front-end subsystem 108 may also communicate with the computing systems of source image providers 106 a - n , e.g., to obtain a source image 128 a to serve to the client computer 104 .
- the front-end subsystem 108 may also communicate with, and include a controller for coordinating activities among, each of the components 112 - 120 of the server system 102 .
- the front-end system can include a wired (e.g., metallic or optical), wireless, or combination of wired and wireless communications interfaces that enable the front-end subsystem to connect to an active communications network.
- the image generation subsystem 110 is configured to generate images from input data.
- the image generation subsystem 110 includes an encoding input generator 112 and a watermark image generator 114 .
- the encoding input generator 112 processes a plaintext data item 122 to generate an encoding image 124 that encodes the plaintext data item 122 .
- the plaintext data item 122 can be any data that is capable of being encoded within the constraints of the encoding input generator 112 .
- the plaintext data item 122 may be a text sample with a maximum length of n characters, since the size of the encoding image 124 may be capable of providing lossless encoding for text samples only up to the pre-defined maximum length of n characters.
- the plaintext data item 122 is a session identifier that uniquely identifies a network session between the client computer 104 and the server system 102 during which a response is served to a request from the client computer 104 .
- the plaintext data item 122 includes or references source image data that identifies the particular source image 128 a served to the client computer 104 or information associated with the source image 128 a (e.g., information that indicates which of the source image providers 106 a - n provided the particular source image 128 a served to the client computer 104 ).
- the response records database 120 stores data that associates detailed information about a response served for a particular request, in order to make the detailed information accessible via the session identifier represented by the plaintext data item 122 .
- the response records database 120 can also associate a session identifier with source image data, thereby making the source image data accessible by querying the database 120 using the session identifier represented by the plaintext data item 122 .
- a user can then identify, for example, which of the source images 128 a - n was served to the client computer 104 for a request using the session identifier from the plaintext data item 122 .
- the encoding image 124 is an image that encodes the plaintext data item 122 .
- the encoding image 124 is a matrix-type barcode that represents the plaintext data item 122 .
- a suitable matrix-type barcode is a Quick Response Code (QR code).
- QR code Quick Response Code
- the encoding image 124 can have a pre-defined size in terms of a number of rows and columns of pixels.
- Each pixel in the encoding image 124 can encode a binary bit of data, where the value of each bit is represented by a different color. For example, a pixel that encodes the binary value ‘1’ may be black while a pixel that encodes the binary value ‘0’ may be white.
- the image generator subsystem 110 further includes a watermark image generator 114 .
- the watermark image generator 114 is configured to process the encoding image 124 to generate a semi-transparent watermark image 126 .
- the semi-transparent watermark image 126 is derived from the encoding image 124 and also encodes the plaintext data item 122 .
- the transparencies, colors, arrangement of encoded pixels and/or other features of the watermark image 126 may be changed from the transparencies, colors, arrangement of encoded pixels and/or other features of the encoding image 124 .
- the encoding image 124 may be uniformly opaque and consist of encoded pixels that are closely packed adjacent to each other
- the watermark image 126 may include some fully transparent pixels and some partially transparent pixels.
- the response formatter 116 is configured to generate a response to return to the client computer 104 in reply to the client's request for an electronic document.
- the response can include one or more content items, including first-party content items and third-party content items, which collectively form an electronic document such as a web page, an application interface, a PDF, a presentation slide deck, or a spreadsheet.
- the response includes a primary document that specifies how various content items are to be arranged and displayed.
- the primary document such as a hypertext markup language (HTML) page, may refer to first-party content items and third-party content items that are to be displayed in the presentation of the document.
- HTML hypertext markup language
- the response formatter 116 is configured to add computer code to the primary document that instructs the client computer 104 , when executing the response, to display one or more instances of the watermark image 126 over the source image 128 a , e.g., to add a watermark to the source image 128 a that is substantially imperceptible to human users. Because the watermark image 126 has fully and partially-transparent pixels, the application at the client computer 104 that renders the electronic document can perform a blending technique to overlay the watermark image 126 on the source image 128 a according to the specified transparencies of the watermark image 126 .
- the response formatter 116 may add code that directs the client computer 104 to display the source image 128 a as a background image in a third-party content slot in an electronic document and to display one or more instances of the watermark image 126 as a foreground image over the source image 128 a.
- a user of the client computer 104 may receive an inappropriate or irrelevant source image 128 a from one of the source image providers 106 a - n in response to a request for an electronic document.
- the user may take a screenshot of the encoded source image 130 and transmit the screenshot to the server system 102 for analysis, e.g., to inquire about the origin of the source image 128 a .
- the image analysis and decoder module 118 can process the screenshot to recover an encoded representation of the plaintext data item 122 , which in turn can be decoded to recover the plaintext data item 122 itself.
- the system 102 can then use the recovered plaintext data item 122 for various purposes, e.g., to query the response records database 120 to lookup detailed information about the source image 128 a and its origins, or other information about the particular client session in which the source image 128 a was served to the client computer 104 .
- the encoded representation of the plaintext data item 122 that the image analysis and decoder module 118 generates can be, for example, a recovered watermark image 126 or a recovered encoding image 124 . Additional details about the operations performed by the image analysis and decoder module 118 to recover an encoded representation of the plaintext data item 122 are described below with respect to FIG. 7 .
- FIG. 2 is a block diagram of an example environment 200 in which third-party content is distributed for presentation with electronic documents.
- the example environment 200 includes a network 202 , such as a local area network (LAN), a wide area network (WAN), the Internet, or a combination thereof.
- the network 202 connects electronic document servers 204 , user devices 206 , third-party content servers 208 , and a third-party content distribution system 210 (also referred to as a content distribution system).
- the example environment 200 may include many different electronic document servers 204 , user devices 206 (e.g., client computers), and third-party content servers 208 .
- a user device 206 is an electronic device that is capable of requesting and receiving resources (e.g., electronic documents) over the network 202 .
- Example user devices 206 include personal computers, mobile communication devices, and other devices that can send and receive data over the network 202 .
- a user device 206 typically includes a user application, such as a web browser, to facilitate the sending and receiving of data over the network 202 , but native applications executed by the user device 206 can also facilitate the sending and receiving of data over the network 202 .
- An electronic document is data that presents a set of content at a user device 206 .
- Examples of electronic documents include webpages, word processing documents, portable document format (PDF) documents, images, videos, search results pages, and feed sources.
- Native applications e.g., “apps”
- Electronic documents can be provided to user devices 206 by electronic document servers 204 .
- the electronic document servers 204 can include servers that host publisher websites.
- the user device 206 can initiate a request for a given publisher webpage, and the electronic server 204 that hosts the given publisher webpage can respond to the request by sending machine executable instructions that initiate presentation of the given webpage at the user device 206 .
- the electronic document servers 204 can include app servers from which user devices 206 can download apps.
- the user device 206 can download files required to install an app at the user device 206 , and then execute the downloaded app locally.
- Electronic documents can include a variety of content.
- electronic document can include static content (e.g., text or other specified content) that is within the electronic document itself and/or does not change over time.
- Electronic documents can also include dynamic content that may change over time or on a per-request basis.
- a publisher of a given electronic document can maintain a data source that is used to populate portions of the electronic document.
- the given electronic document can include a tag or script that causes the user device 206 to request content from the data source when the given electronic document is processed (e.g., rendered or executed) by a user device 206 .
- the user device 206 integrates the content obtained from the data source into a presentation of the given electronic document to create a composite electronic document including the content obtained from the data source.
- a given electronic document can include a third-party tag or third-party script that references the third-party content distribution system 210 .
- the third-party tag or third-party script is executed by the user device 206 when the given electronic document is processed by the user device 206 . Execution of the third-party tag or third-party script configures the user device 206 to generate a request for third-party content 212 , which is transmitted over the network 202 to the third-party content distribution system 210 .
- the third-party tag or third-party script can enable the user device 206 to generate packetized data request including a header and payload data.
- the request 212 can include data such as a name (or network location) of a server from which the third-party content is being requested, a name (or network location) of the requesting device (e.g., the user device 206 ), and/or information that the third-party content distribution system 210 can use to select third-party content provided in response to the request.
- the request 212 is transmitted, by the user device 206 , over the network 202 (e.g., a telecommunications network) to a server of the third-party content distribution system 210 .
- the request 212 can include data specifying the electronic document and characteristics of locations at which third-party content can be presented. For example, data specifying a reference (e.g., URL) to an electronic document (e.g., webpage) in which the third-party content will be presented, available locations (e.g., third-party content slots) of the electronic documents that are available to present third-party content, sizes of the available locations, positions of the available locations within a presentation of the electronic document, and/or media types that are eligible for presentation in the locations can be provided to the content distribution system 210 .
- a reference e.g., URL
- available locations e.g., third-party content slots
- data specifying keywords associated with the electronic document (“document keywords”) or entities (e.g., people, places, or things) that are referenced by the electronic document can also be included in the request 212 (e.g., as payload data) and provided to the content distribution system 210 to facilitate identification of third-party content items that are eligible for presentation with the electronic document.
- the third-party content can be represented by one or more files (e.g., digitally stored files) that cause a perceptible (e.g., visual) instance of the third-party content to be generated by a computing device that processes the one or more files.
- the third-party content is stored in image files, text files, or other files that cause a visually perceptible instance of the third-party content to be presented by a computing device (e.g., mobile device, tablet device, wearable device).
- the third-party content can include any information, including images of scenery, images of text, and images that include advertising information.
- Requests 212 can also include data related to other information, such as information that the user has provided, geographic information indicating a state or region from which the request was submitted, or other information that provides context for the environment in which the third-party content will be displayed (e.g., a type of device at which the third-party content will be displayed, such as a mobile device or tablet device).
- Data specifying characteristics of the user device 206 can also be provided in the request 212 , such as information that identifies a model of the user device 206 , a configuration of the user device 206 , or a size (e.g., physical size or resolution) of an electronic display (e.g., touchscreen or desktop monitor) on which the electronic document is presented.
- Requests 212 can be transmitted, for example, over a packetized network, and the requests 212 themselves can be formatted as packetized data having a header and payload data.
- the header can specify a destination of the packet and the payload data can include any of the information discussed above.
- the third-party content distribution system 210 is implemented in a distributed computing system that includes, for example, a server and a set of multiple computing devices 214 that are interconnected and identify and distribute third-party content in response to requests 212 .
- the set of multiple computing devices 214 operate together to identify a set of third-party content that are eligible to be presented in the electronic document from among a corpus of millions of available third-party content (3PC 1-x ).
- the millions of available third-party content can be indexed, for example, in a third-party corpus database 216 .
- Each third-party content index entry can reference the corresponding third-party content and/or include distribution parameters (DP 1 -DP x ) (e.g. selection criteria) that condition the distribution of the corresponding third-party content.
- the distribution parameters (e.g., selection criteria) for a particular third-party content can include distribution keywords that must be matched (e.g., by electronic documents or terms specified in the request 212 ) in order for the third-party content to be eligible for presentation.
- the distribution parameters can also require that the request 212 include information specifying a particular geographic region (e.g., country or state) and/or information specifying that the request 212 originated at a particular type of user device (e.g., mobile device or tablet device) in order for the third-party content to be eligible for presentation.
- the distribution parameters can also specify a bid and/or budget for distributing the particular third-party content.
- the identification of the eligible third-party content can be segmented into multiple tasks 217 a - 217 c that are then assigned among computing devices within the set of multiple computing devices 214 .
- different computing devices in the set 214 can each analyze a different portion of the third-party corpus database 216 to identify various third-party content having distribution parameters that match information included in the request 212 .
- each given computing device in the set 214 can analyze a different data dimension (or set of dimensions) and pass results (Res 2 -Res 3 ) 218 a - 218 c of the analysis back to the third-party content distribution system 210 .
- the winning third-party content that is provided in response to the request 212 is a source image that will be augmented with a watermark.
- the system 210 may generate a watermark image and configure an electronic document to overlay the watermark image on the source image when the document is presented at a user device 206 .
- FIG. 4 is a flowchart of an example process 400 for using a semi-transparent watermark image to augment a source image presented in an electronic document served to a client computer.
- the process 400 can be carried out by a system of one or more computers, e.g., server system 102 .
- an encoding input generator e.g., encoding input generator 112 , of the system generates an encoding image from the plaintext data.
- the encoding image is an image that encodes the plaintext data.
- the encoding image is a matrix-type barcode that represents the plaintext data.
- a suitable matrix-type barcode is a Quick Response Code (QR code).
- QR code Quick Response Code
- the encoding image can have a pre-defined size in terms of a number of row and columns of pixels.
- Each pixel in the encoding image can encode a binary bit of data, where the value of each bit is represented by a different color. For instance, a pixel that encodes the binary value ‘1’ may be black while a pixel that encodes the binary value ‘0’ may be white.
- the blending technique can involve, for example, for each pixel in the watermark image that overlays a pixel in the background source image, determining a composite/blended color to display by weighting the color of the watermark image pixel relative to the color of the background source image pixel using a transparency value assigned to the watermark image pixel.
- the watermark image is smaller than the source image in that it has fewer rows and/or columns of pixels than the source image.
- the client computer may tile multiple instances of the watermark image over the source image.
- FIG. 3H shows an example source image 316 .
- FIG. 3I shows an example watermark image 318 based on the QR code 302 .
- FIG. 3J shows an encoded source image 320 in which the watermark image 318 has been tiled multiple times over the source image 316 .
- the decoder module receives an encoded source image.
- a user of the client computer may be presented an inappropriate or irrelevant source image in an electronic document.
- the user may take a screenshot of the encoded source image and transmit the screenshot to the server system for analysis, e.g., to inquire about the origin of the source image.
- the client computer may have tiled multiple copies of the watermark image over the source image to render the encoded source image.
- the decoder module may only require a portion of the encoded source image that corresponds to a single, overlaid watermark image. Accordingly, at stage 704 , the decoder module selects such a portion of the encoded source image that corresponds to a single, overlaid watermark image.
- the server system may know the size/dimensions of the watermark image and selects a portion of the encoded source image that matches the known size.
- the encoded pixel is decoded to represent the first binary value (e.g., a value that represents a ‘black’ pixel in the original encoding image), because the match indicates that the encoded pixel was also darkened by (1 ⁇ B a ) during the blending process.
- the colors of the encoded pixel and its neighboring pixels may be provided as inputs to a machine-learning model that has been trained to classify encoded pixels as either encoding the first or second binary value based on the color of the encoded pixel and its neighboring pixels.
- the decoder module may arbitrarily assign either the first or second binary decoded value to that pixel.
- the decoder module may employ various techniques to determine a decoded value for a given encoded pixel from the encoded source image.
- the decoder module uses a matching technique that involves determining a match with the color values of a majority of neighboring pixels. If the colors of a majority of the neighboring pixels of an encoding pixel match the color of the encoding pixel within a specified tolerance, then the encoded pixel is decoded to represent the second binary value (e.g., a value that represents a ‘white’ pixel in the original encoding image).
- the encoded pixel is decoded to represent the first binary value (e.g., a value that represents a ‘black’ pixel in the original encoding image). If no match is determined for either the first or second binary value, then an encoding pixel can be arbitrarily decoded to either the first or second binary value.
- the decoder module uses a matching technique that involves determining an average color value among the neighboring pixels and comparing the averaged color value of the neighboring pixels to the color value of the target encoding pixel.
- the specified tolerance may be, for example, 0, 1, 2, 3, 4, or 5 percent, for example. When the tolerance is 0 percent, the decoder module may require an exact match in color values between the encoded pixel and neighboring pixels.
- the decoder module repeats operations 708 - 712 to decode each of the encoded pixels in the selected portion of the encoded source image, and can further use an appropriate mapping template to generate a recovery image such as a watermark image or a reconstruction of the original encoding image based on the decoded values of the pixels.
- the decoder module repeats stages 704 and 706 for each of multiple selected portions of the encoded source image and combines the decoding results from each selected portion. Each selected portion corresponds to a different instance of the watermark image overlaid on the source image, thereby adding redundancy and, potentially, reliability to the decoding results.
- the server system uses the decoding information to recover plaintext data.
- the system uses the plaintext data for various purposes.
- the plaintext data represents a session ID or a response ID.
- the server system can query the response records database using the session ID or response ID to lookup source image data and determine, for example, a particular content provider that provided the source image.
- FIG. 8 is a schematic diagram of a computer system 800 .
- the system 800 can be used to carry out the operations described in association with any of the computer-implemented methods, systems, devices, and other techniques described previously, according to some implementations.
- the system 800 is intended to include various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the system 800 can also include mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices.
- the system can include portable storage media, such as, Universal Serial Bus (USB) flash drives.
- USB flash drives may store operating systems and other applications.
- the USB flash drives can include input/output components, such as a wireless transmitter or USB connector that may be inserted into a USB port of another computing device.
- the memory 820 stores information within the system 800 .
- the memory 820 is a computer-readable medium.
- the memory 820 is a volatile memory unit.
- the memory 820 is a non-volatile memory unit.
- the features described can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them.
- the apparatus can be implemented in a computer program product tangibly embodied in an information carrier, e.g., in a machine-readable storage device for execution by a programmable processor; and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output.
- the described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device.
- a computer program is a set of instructions that can be used, directly or indirectly, in a computer to perform a certain activity or bring about a certain result.
- a computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices such as EPROM, EEPROM, and flash memory devices
- magnetic disks such as internal hard disks and removable disks
- magneto-optical disks and CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).
- ASICs application-specific integrated circuits
- the features can be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer. Additionally, such activities can be implemented via touchscreen flat-panel displays and other appropriate mechanisms.
- a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.
- a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.
- activities can be implemented via touchscreen flat-panel displays and other appropriate mechanisms.
- the features can be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component, such as a client computer having a graphical user interface or an Internet browser, or any combination of them.
- the components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), peer-to-peer networks (having ad-hoc or static members), grid computing infrastructures, and the Internet.
- LAN local area network
- WAN wide area network
- peer-to-peer networks having ad-hoc or static members
- grid computing infrastructures and the Internet.
- the computer system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a network, such as the described one.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
- This application claims priority to Israel Application Serial No. 251149, filed on Mar. 14, 2017, the entire contents of which are hereby incorporated by reference.
- This specification generally relates to techniques for generating and recovering watermark images. The watermark images are used to modify unknown source images, e.g., source images that are presented as third-party content in electronic documents.
- In a networked environment, such as the Internet or other networks, first-party content providers can provide information for presentation in electronic documents, for example web pages or application interfaces. The documents can include first-party content provided by first-party content providers and third-party content provided by third-party content providers.
- Third-party content can be added to an electronic document using various techniques. Some documents include tags that instruct a client computer on which the document is presented to request third-party content items directly from third-party content providers. Other documents include tags that instruct the client computer to call an intermediary service that partners with multiple third-party content providers to return third-party content items selected from one or more of the third-party content providers. In some instances, third-party content items are dynamically selected for presentation in electronic documents, and the particular third-party content items selected for a given serving of a document may differ from third-party content items selected for other servings of the same document.
- This specification describes systems, methods, devices, and other techniques for generating watermark images to supplement source images that are displayed, for example, as third-party content in an electronic document. In other aspects, this specification describes systems, methods, devices, and other techniques for recovering information encoded in an encoded source image that shows a semi-transparent watermark overlaid on a source image. In some implementations, a system uses information recovered from an encoded source image to identify a provider of the source image, other characteristics of the source image, or context about a specific impression of the source image.
- The semi-transparent watermark image may be generated by a watermark image generator including one or more processors. The watermark image may include a first set of encoded pixels each of which is assigned a first transparency value and a second set of encoded pixels each of which is assigned a second transparency value, the second transparency level being different from the first transparency level. The encoded pixels may be distributed among a set of blank pixels such that each encoded pixel neighbors one or more blank pixels in the watermark image, and in particular at least two blank pixels in the watermark image. Herein, each blank pixel may be assigned the second transparency value. More specifically, assigning the first transparency value to first pixels in the watermark image may comprise setting a transparency level of the first pixels in the watermark image to be less than 100-percent transparent and assigning the second transparency value to second pixels in the watermark image may comprise setting a transparency level of the second pixels in the watermark image to a 100-percent transparency.
- The watermark image generator may generate the watermark image by spacing the set of encoded pixels among a set of blank pixels such that each encoded pixel in the watermark image neighbors one or more blank pixels in the watermark image, an in particular at least two blank pixels in the watermark image. According to one embodiment, distributing the encoded pixels among the set of blank pixels may comprise (i) creating an initial watermark canvas that has m rows of pixels and n columns of pixels, wherein each pixel in the initial watermark canvas is a blank pixel; and (ii) for every kth row in the initial watermark canvas, mapping the pixels from a corresponding row of the encoding image to every lth pixel in the respective row of the initial watermark canvas. For example, if the encoding image has i rows of pixels and j columns of pixels, distributing the encoded pixels among the set of blank pixels may comprise (i) creating an initial watermark canvas that has i rows of pixels and 2j columns of pixels, wherein each pixel in the initial watermark canvas is a blank pixel; (ii) for every second row in the initial watermark canvas, mapping the pixels from a corresponding row of the encoding image to every second pixel in the row of the initial watermark canvas; and (iii) for every other row in the initial watermark canvas, mapping the pixels from a corresponding row of the encoding image to every second pixel in the row of the initial watermark canvas and shifting the pixels by one column in the initial watermark canvas.
- In order to recover such watermark image from an encoded source image, a system of one or more computers may select a portion of the encoded source image having a size that matches a size of the watermark image and may then determine, for each encoded pixel in the watermark image, a binary value of the encoded pixel based on a comparison of a value of a corresponding pixel in the selected portion of the encoded source image with respect to values of one or more other pixels in the selected portion of the encoded source image, and in particular with respect to values of at least two other pixels in the selected portion of the encoded source image. The one or more other pixels may be neighboring pixels of the encoded pixel for which a binary value is to be determined. Here, a neighboring pixel is generally any pixel that is adjacent to the encoded pixel, e.g., directly above, below, or to the left or right of the encoded pixel. In general, the comparison may also take into account next-nearest neighbors of the encoded pixel or other pixels in a specified surrounding of the encoded pixel.
- For each pixel in the encoded source image which corresponds to an encoded pixel in the watermark image, determining a binary value of the encoded pixel may comprise determining whether a color of the corresponding pixel in the encoded source image matches a color of at least a threshold number of neighboring pixels for the corresponding pixel. For example, if a color of a first encoded pixel in the encoded source image matches the color of at least the threshold number of neighboring pixels for the first encoded pixel, then, in response, a first binary value may be assigned to the first pixel in the recovered image of the watermark, and if a color of a second encoded pixel in the encoded source image does not match the color of at least the threshold number of neighboring pixels for the second encoded pixel, then, in response, a second binary value may be assigned to the second pixel in the recovered image of the watermark.
- Some implementations of the subject matter described herein can, in certain instances, realize one or more of the following advantages. First, a system may generate a watermark image to augment an arbitrary source image, even if the source image is not known when the watermark image is created. Because the watermark image may be overlaid on the source image for presentation to a user at a client computer remote from the system, the system need not directly obtain or modify the source image before the watermark image is transmitted to a client computer. Second, the system may recover a watermark image, or may recover an encoding image from which the watermark image was derived, even without a priori knowledge of the source image, thereby enabling a system to decode arbitrary images without knowing where they originated from. Third, the watermark image can be substantially transparent so as to be imperceptible to users when the watermark image is rendered over a source image so that the watermark image does not degrade the perceived visual quality of the source image. Fourth, the amount of time and computational expense required to generate a watermark image can be reduced by separating the watermark image from the source image over which the watermark image will be displayed. For example, rather than directly modifying a complex source image to add a watermark, which can take a significant amount of time and processing resources, the systems and techniques disclosed herein may generate the watermark image independently of the source image, which can be performed with fewer processing resources and/or in less time than modifying the source image. By relying on a separate client computer to blend the watermark image over the source image, the system can reduce latency in serving the watermark image responsive to a request from the client computer, while also reducing the number of computational cycles involved in responding to the request because the server system is not required to blend the watermark image and the source image prior to delivering the source image. Fifth, the system may generate binary watermark images, or other watermark images with relatively few color or transparency options, to maintain a compact file size that allows the images to be efficiently transmitted to a client computer over a network, whereas other watermark images may be much larger to accommodate.
-
FIG. 1 depicts a block diagram of a networked environment that augments electronic documents served to a client computer with watermark images that are arranged for display over one or more source images presented in the documents. -
FIG. 2 is a block diagram of an example environment in which third-party content is distributed for presentation with electronic documents -
FIGS. 3A-3J illustrate example techniques for generating a watermark image and recovering the watermark image, or a related image, from an encoded source image. -
FIG. 4 is a flowchart of an example process for using semi-transparent watermark images to augment source images presented in documents served to a client computer. -
FIG. 5 is a flowchart of an example process for generating a watermark image from an encoding image. -
FIG. 6 is a flowchart of an example process for rendering an encoded source image using a source image and a watermark image. -
FIG. 7 is a flowchart of an example process for generating a recovery image from an encoded source image. -
FIG. 8 is a schematic diagram of a computer system that can be used to carry out the operations described in association with the computer-implemented methods, systems, devices, and other techniques described herein. -
FIG. 1 depicts a block diagram of a networkedenvironment 100 that generates a watermark image for display over a source image presented in an electronic document served to aclient computer 104. Theenvironment 100 includes aserver system 102, aclient computer 104, and computing systems for one or more source image providers 106 a-n. Theserver system 102,client computer 104, and source image providers 106 a-n are connected over one or more networks such as the Internet or a local area network (LAN). In general, theclient computer 104 is configured to generate and transmit requests for electronic documents to theserver system 102. Based on the requests from theclient computer 104, theserver system 102 generates responses (e.g., electronic documents) to return to theclient computer 104. A given response can include a selectedsource image 128 a that is configured to be displayed to a user of theclient computer 104, where thesource image 128 a is provided by one of the source image providers 106 a-n. Theserver system 102 can augment the response served to theclient computer 104 with asemi-transparent watermark image 126 that is arranged for display in a presentation of the response document at theclient computer 104 over the source image 128. - The
client computer 104 can be any type of computing device that presents images and other content to one or more human users. Theclient computer 104 may include an application, such as a web browser application, that makes requests to and receives responses from theserver system 102. The application may execute a response from theserver system 102, such as web page code or other types of document files, to present the response to the one or more users of theclient computer 104. In some implementations, theclient computer 104 includes an electronic display device (e.g., an LCD or LED screen, a CRT monitor, a head-mounted virtual reality display, a head-mounted mixed-reality display), or is coupled to an electronic display device, that displays content from the rendered response to the one or more users of theclient computer 104. The displayed content can include theselected source image 128 a and thewatermark image 126 displayed over top of theselected source image 128 a in a substantially transparent manner. In some implementations, theclient computer 104 is a notebook computer, a smartphone, a tablet computer, a desktop computer, or a smartwatch or other wearable device. - In some implementations, the source image 128 provided in the response to the
client computer 104 is a third-party content item that, for example, is not among content provided by a first-party content provider of the response. For example, if the response is a web page, the creator of the web page may include a slot that is configured to be populated by a source image from a third-party content provider that differs from the creator of the web page (e.g., a provider of an image repository). In another example, the first-party content provider may directly link to a third-party source image 128. Theclient computer 104 may request the source image 128 directly from a corresponding computing system for one of the source image providers 106 a-n or indirectly via an intermediary service, such as a service provided byserver system 102 or another server system. - The
server system 102 is configured to respond to a request from theclient computer 104 with an electronic document and asemi-transparent watermark image 126 that is to be displayed in the electronic document over asource image 128 a. Theserver system 102 can be implemented as one or more computers in one or more locations. Theserver system 102 can include one or more of a front-end subsystem 108, animage generation subsystem 110, aresponse formatter 116, an image analysis anddecoder module 118, and aresponse records database 120. Each of the components 108-120 is configured to perform the respective operations described herein. For example, the operations associated with each component 108-120 may be defined by instructions stored in memory on one or more computer-readable storage devices, and the operations may be carried out when the instructions are executed by one or more processors of theserver system 102. Although some operations are described herein as being performed by a specific one of the components 108-120 by way of example, in other implementations, some or all of the operations of two or more of the components 108-120 may be consolidated and performed instead by a single component. In yet other implementations, the operations of any one of the components 108-120 may be divided among two or more components. - The
server system 102 can be configured to carry out the techniques illustrated inFIGS. 3A-3I and theprocesses processes FIGS. 3A-3I, 4, 5, and 7 . An overview of theserver system 102 and its components 108-120 follows below, with further detail of the operations and other techniques performed by these components described subsequently with respect toFIGS. 3A-3I andFIGS. 4, 5, and 7 . - The front-
end subsystem 108 provides an interface for communicating over one or more networks. The front-end subsystem 108 receives requests from theclient computer 104 and transmits responses to the requests, along with any content associated with the requests such as a watermark image and, optionally, a source image, to theclient computer 104. The front-end subsystem 108 may also communicate with the computing systems of source image providers 106 a-n, e.g., to obtain asource image 128 a to serve to theclient computer 104. The front-end subsystem 108 may also communicate with, and include a controller for coordinating activities among, each of the components 112-120 of theserver system 102. To facilitate communication with devices over a network, the front-end system can include a wired (e.g., metallic or optical), wireless, or combination of wired and wireless communications interfaces that enable the front-end subsystem to connect to an active communications network. - The
image generation subsystem 110 is configured to generate images from input data. In particular, theimage generation subsystem 110 includes anencoding input generator 112 and awatermark image generator 114. - The
encoding input generator 112 processes aplaintext data item 122 to generate anencoding image 124 that encodes theplaintext data item 122. Theplaintext data item 122 can be any data that is capable of being encoded within the constraints of theencoding input generator 112. For example, theplaintext data item 122 may be a text sample with a maximum length of n characters, since the size of theencoding image 124 may be capable of providing lossless encoding for text samples only up to the pre-defined maximum length of n characters. In some implementations, theplaintext data item 122 is a session identifier that uniquely identifies a network session between theclient computer 104 and theserver system 102 during which a response is served to a request from theclient computer 104. - In some implementations, the
plaintext data item 122 includes or references source image data that identifies theparticular source image 128 a served to theclient computer 104 or information associated with thesource image 128 a (e.g., information that indicates which of the source image providers 106 a-n provided theparticular source image 128 a served to the client computer 104). In some implementations, theresponse records database 120 stores data that associates detailed information about a response served for a particular request, in order to make the detailed information accessible via the session identifier represented by theplaintext data item 122. The response recordsdatabase 120 can also associate a session identifier with source image data, thereby making the source image data accessible by querying thedatabase 120 using the session identifier represented by theplaintext data item 122. A user can then identify, for example, which of the source images 128 a-n was served to theclient computer 104 for a request using the session identifier from theplaintext data item 122. - The
encoding image 124 is an image that encodes theplaintext data item 122. In some implementations, theencoding image 124 is a matrix-type barcode that represents theplaintext data item 122. One example of a suitable matrix-type barcode is a Quick Response Code (QR code). Theencoding image 124 can have a pre-defined size in terms of a number of rows and columns of pixels. Each pixel in theencoding image 124 can encode a binary bit of data, where the value of each bit is represented by a different color. For example, a pixel that encodes the binary value ‘1’ may be black while a pixel that encodes the binary value ‘0’ may be white. In some implementations, the smallest encoding unit of anencoding image 124 may actually be larger than a single pixel. But for purposes of the examples described herein, the smallest encoding unit is assumed to be a single pixel. It should be appreciated, however, that the techniques described herein may be extended to implementations where the smallest encoding unit is a set of multiple pixels, e.g., a 2×2 or 3×3 grid of pixels. - The
image generator subsystem 110 further includes awatermark image generator 114. Thewatermark image generator 114 is configured to process theencoding image 124 to generate asemi-transparent watermark image 126. Thesemi-transparent watermark image 126 is derived from theencoding image 124 and also encodes theplaintext data item 122. However, the transparencies, colors, arrangement of encoded pixels and/or other features of thewatermark image 126 may be changed from the transparencies, colors, arrangement of encoded pixels and/or other features of theencoding image 124. For example, whereas theencoding image 124 may be uniformly opaque and consist of encoded pixels that are closely packed adjacent to each other, thewatermark image 126 may include some fully transparent pixels and some partially transparent pixels. Moreover, the encoded pixels in thewatermark image 126 may be spaced relative to each other so that each encoded pixel is surrounded by non-encoded pixels (i.e., “blank” pixels). The transformation of theencoding image 124 to thewatermark image 126 may be performed so that, after thewatermark image 126 is overlaid and merged on abackground source image 128 a, the encoded information may be recovered, e.g., by reconstructing theencoding image 124 or thewatermark image 126. The details of how thewatermark image generator 114 creates thewatermark image 126 from theencoding image 124 are discussed more fully with respect to process 500 (FIG. 5 ) andFIGS. 3A-3E . - The
response formatter 116 is configured to generate a response to return to theclient computer 104 in reply to the client's request for an electronic document. The response can include one or more content items, including first-party content items and third-party content items, which collectively form an electronic document such as a web page, an application interface, a PDF, a presentation slide deck, or a spreadsheet. In some implementations, the response includes a primary document that specifies how various content items are to be arranged and displayed. The primary document, such as a hypertext markup language (HTML) page, may refer to first-party content items and third-party content items that are to be displayed in the presentation of the document. In some implementations, theresponse formatter 116 is configured to add computer code to the primary document that instructs theclient computer 104, when executing the response, to display one or more instances of thewatermark image 126 over thesource image 128 a, e.g., to add a watermark to thesource image 128 a that is substantially imperceptible to human users. Because thewatermark image 126 has fully and partially-transparent pixels, the application at theclient computer 104 that renders the electronic document can perform a blending technique to overlay thewatermark image 126 on thesource image 128 a according to the specified transparencies of thewatermark image 126. For example, theresponse formatter 116 may add code that directs theclient computer 104 to display thesource image 128 a as a background image in a third-party content slot in an electronic document and to display one or more instances of thewatermark image 126 as a foreground image over thesource image 128 a. - The
server system 102 further includes an image analysis anddecoder module 118. The image analysis anddecoder module 118 is configured to recover an encoded representation of theplaintext data item 122 from an encodedsource image 130. The encoded source image is an image that results from theclient computer 104 rendering thewatermark image 126 over thesource image 128 a. Even though thewatermark image 126 is separate from thesource image 128 a, the encodedsource image 130 processed by the image analysis anddecoder module 118 may be a merged image showing thewatermark image 126 blended over thesource image 128 a. For example, a user of theclient computer 104 may receive an inappropriate orirrelevant source image 128 a from one of the source image providers 106 a-n in response to a request for an electronic document. The user may take a screenshot of the encodedsource image 130 and transmit the screenshot to theserver system 102 for analysis, e.g., to inquire about the origin of thesource image 128 a. Because the screenshot shows theoriginal source image 128 a overlaid by thewatermark image 126, the image analysis anddecoder module 118 can process the screenshot to recover an encoded representation of theplaintext data item 122, which in turn can be decoded to recover theplaintext data item 122 itself. Thesystem 102 can then use the recoveredplaintext data item 122 for various purposes, e.g., to query theresponse records database 120 to lookup detailed information about thesource image 128 a and its origins, or other information about the particular client session in which thesource image 128 a was served to theclient computer 104. The encoded representation of theplaintext data item 122 that the image analysis anddecoder module 118 generates can be, for example, a recoveredwatermark image 126 or a recoveredencoding image 124. Additional details about the operations performed by the image analysis anddecoder module 118 to recover an encoded representation of theplaintext data item 122 are described below with respect toFIG. 7 . -
FIG. 2 is a block diagram of an example environment 200 in which third-party content is distributed for presentation with electronic documents. The example environment 200 includes a network 202, such as a local area network (LAN), a wide area network (WAN), the Internet, or a combination thereof. The network 202 connectselectronic document servers 204,user devices 206, third-party content servers 208, and a third-party content distribution system 210 (also referred to as a content distribution system). The example environment 200 may include many differentelectronic document servers 204, user devices 206 (e.g., client computers), and third-party content servers 208. - A
user device 206 is an electronic device that is capable of requesting and receiving resources (e.g., electronic documents) over the network 202.Example user devices 206 include personal computers, mobile communication devices, and other devices that can send and receive data over the network 202. Auser device 206 typically includes a user application, such as a web browser, to facilitate the sending and receiving of data over the network 202, but native applications executed by theuser device 206 can also facilitate the sending and receiving of data over the network 202. - An electronic document is data that presents a set of content at a
user device 206. Examples of electronic documents include webpages, word processing documents, portable document format (PDF) documents, images, videos, search results pages, and feed sources. Native applications (e.g., “apps”), such as applications installed on mobile, tablet, or desktop computing devices are also examples of electronic documents. Electronic documents can be provided touser devices 206 byelectronic document servers 204. For example, theelectronic document servers 204 can include servers that host publisher websites. In this example, theuser device 206 can initiate a request for a given publisher webpage, and theelectronic server 204 that hosts the given publisher webpage can respond to the request by sending machine executable instructions that initiate presentation of the given webpage at theuser device 206. - In another example, the
electronic document servers 204 can include app servers from whichuser devices 206 can download apps. In this example, theuser device 206 can download files required to install an app at theuser device 206, and then execute the downloaded app locally. - Electronic documents can include a variety of content. For example, electronic document can include static content (e.g., text or other specified content) that is within the electronic document itself and/or does not change over time. Electronic documents can also include dynamic content that may change over time or on a per-request basis. For example, a publisher of a given electronic document can maintain a data source that is used to populate portions of the electronic document. In this example, the given electronic document can include a tag or script that causes the
user device 206 to request content from the data source when the given electronic document is processed (e.g., rendered or executed) by auser device 206. Theuser device 206 integrates the content obtained from the data source into a presentation of the given electronic document to create a composite electronic document including the content obtained from the data source. - In some situations, a given electronic document can include a third-party tag or third-party script that references the third-party
content distribution system 210. In these situations, the third-party tag or third-party script is executed by theuser device 206 when the given electronic document is processed by theuser device 206. Execution of the third-party tag or third-party script configures theuser device 206 to generate a request for third-party content 212, which is transmitted over the network 202 to the third-partycontent distribution system 210. For example, the third-party tag or third-party script can enable theuser device 206 to generate packetized data request including a header and payload data. Therequest 212 can include data such as a name (or network location) of a server from which the third-party content is being requested, a name (or network location) of the requesting device (e.g., the user device 206), and/or information that the third-partycontent distribution system 210 can use to select third-party content provided in response to the request. Therequest 212 is transmitted, by theuser device 206, over the network 202 (e.g., a telecommunications network) to a server of the third-partycontent distribution system 210. - The
request 212 can include data specifying the electronic document and characteristics of locations at which third-party content can be presented. For example, data specifying a reference (e.g., URL) to an electronic document (e.g., webpage) in which the third-party content will be presented, available locations (e.g., third-party content slots) of the electronic documents that are available to present third-party content, sizes of the available locations, positions of the available locations within a presentation of the electronic document, and/or media types that are eligible for presentation in the locations can be provided to thecontent distribution system 210. Similarly, data specifying keywords associated with the electronic document (“document keywords”) or entities (e.g., people, places, or things) that are referenced by the electronic document can also be included in the request 212 (e.g., as payload data) and provided to thecontent distribution system 210 to facilitate identification of third-party content items that are eligible for presentation with the electronic document. The third-party content can be represented by one or more files (e.g., digitally stored files) that cause a perceptible (e.g., visual) instance of the third-party content to be generated by a computing device that processes the one or more files. In some implementations, the third-party content is stored in image files, text files, or other files that cause a visually perceptible instance of the third-party content to be presented by a computing device (e.g., mobile device, tablet device, wearable device). The third-party content can include any information, including images of scenery, images of text, and images that include advertising information. -
Requests 212 can also include data related to other information, such as information that the user has provided, geographic information indicating a state or region from which the request was submitted, or other information that provides context for the environment in which the third-party content will be displayed (e.g., a type of device at which the third-party content will be displayed, such as a mobile device or tablet device). Data specifying characteristics of theuser device 206 can also be provided in therequest 212, such as information that identifies a model of theuser device 206, a configuration of theuser device 206, or a size (e.g., physical size or resolution) of an electronic display (e.g., touchscreen or desktop monitor) on which the electronic document is presented.Requests 212 can be transmitted, for example, over a packetized network, and therequests 212 themselves can be formatted as packetized data having a header and payload data. The header can specify a destination of the packet and the payload data can include any of the information discussed above. - The third-party
content distribution system 210 selects third-party content that will be presented with the given electronic document in response to receiving therequest 212 and/or using information included in therequest 212. In some implementations, the third-party content is selected in less than a second to avoid errors that could be caused by delayed selection of the third-party content. For example, delays in providing third-party content in response to arequest 212 can result in page load errors at theuser device 206 or cause portions of the electronic document to remain unpopulated even after other portions of the electronic document are presented at theuser device 206. Also, as the delay in providing third-party content to theuser device 206 increases, it is more likely that the electronic document will no longer be presented at theuser device 206 when the third-party content, thereby negatively impacting a user's experience with the electronic document. Further, delays in providing the third-party content can result in a failed delivery of the third-party content, for example, if the electronic document is no longer presented at theuser device 206 when the third-party content is provided. - In some implementations, the third-party
content distribution system 210 is implemented in a distributed computing system that includes, for example, a server and a set ofmultiple computing devices 214 that are interconnected and identify and distribute third-party content in response torequests 212. The set ofmultiple computing devices 214 operate together to identify a set of third-party content that are eligible to be presented in the electronic document from among a corpus of millions of available third-party content (3PC1-x). The millions of available third-party content can be indexed, for example, in a third-party corpus database 216. Each third-party content index entry can reference the corresponding third-party content and/or include distribution parameters (DP1-DPx) (e.g. selection criteria) that condition the distribution of the corresponding third-party content. - In some implementations, the distribution parameters (e.g., selection criteria) for a particular third-party content can include distribution keywords that must be matched (e.g., by electronic documents or terms specified in the request 212) in order for the third-party content to be eligible for presentation. The distribution parameters can also require that the
request 212 include information specifying a particular geographic region (e.g., country or state) and/or information specifying that therequest 212 originated at a particular type of user device (e.g., mobile device or tablet device) in order for the third-party content to be eligible for presentation. The distribution parameters can also specify a bid and/or budget for distributing the particular third-party content. - The identification of the eligible third-party content can be segmented into multiple tasks 217 a-217 c that are then assigned among computing devices within the set of
multiple computing devices 214. For example, different computing devices in theset 214 can each analyze a different portion of the third-party corpus database 216 to identify various third-party content having distribution parameters that match information included in therequest 212. In some implementations, each given computing device in theset 214 can analyze a different data dimension (or set of dimensions) and pass results (Res 2-Res 3) 218 a-218 c of the analysis back to the third-partycontent distribution system 210. For example, the results 218 a-218 c provided by each of the computing devices in the set may identify a subset of third-party content that are eligible for distribution in response to the request and/or a subset of the third-party content that have certain distribution parameters or attributes. - The third-party
content distribution system 210 aggregates the results 218 a-218 c received from the set ofmultiple computing devices 214 and uses information associated with the aggregated results to select one or more instances of third-party content that will be provided in response to therequest 212. For example, the third-partycontent distribution system 210 can select a set of winning third-party content based on the outcome of one or more content evaluation processes, as discussed in further detail below. In turn, the third-partycontent distribution system 210 can generate and transmit, over the network 202, reply data 220 (e.g., digital data representing a reply) that enable theuser device 206 to integrate the set of winning third-party content into the given electronic document, such that the set of winning third-party content and the content of the electronic document are presented together at a display of theuser device 206. - In some implementations, the winning third-party content that is provided in response to the
request 212 is a source image that will be augmented with a watermark. Thesystem 210 may generate a watermark image and configure an electronic document to overlay the watermark image on the source image when the document is presented at auser device 206. - In some implementations, the
user device 206 executes instructions included in thereply data 220, which configures and enables theuser device 206 to obtain the set of winning third-party content from one or more third-party content servers. For example, the instructions in thereply data 220 can include a network location (e.g., a Uniform Resource Locator (URL)) and a script that causes theuser device 206 to transmit a third-party request (3PR) 221 to the third-party content server 208 to obtain a given winning third-party content from the third-party content server 208. In response to the request, the third-party content server 208 will transmit, to theuser device 206, third-party data (TP Data) 222 that causes the given winning third-party content to be incorporated to the electronic document and presented at theuser device 206. - The
content distribution system 210 can utilize one or more evaluation processes to identify and select the set of winning third-party content for each given request (e.g., based on data corresponding to the request). In some implementations, the evaluation process is not only required to determine which third-party content to select for presentation with the electronic document, but also the type of formatting that will be dynamically (e.g., on a per-request basis) applied to the selected third-party content, and the price that will be paid for presentation of the selected third-party content when presented with the applied formatting. -
FIGS. 3A-3I illustrate example techniques for generating a watermark image that encodes plaintext data, and for recovering an encoded representation of the plaintext data. Each ofFIGS. 3A-3I is discussed in further detail below with respect to the description of processes 400-700 ofFIGS. 4-7 , respectively. -
FIG. 4 is a flowchart of anexample process 400 for using a semi-transparent watermark image to augment a source image presented in an electronic document served to a client computer. Theprocess 400 can be carried out by a system of one or more computers, e.g.,server system 102. - In some implementations, the
process 400 begins atstage 402, where the system receives plaintext data, e.g.,plaintext data item 122. Plaintext data is generally any data that is capable of being encoded in an encoding image such as a QR code. In some implementations, the plaintext data is a session identifier that uniquely identifies a logical session between a client computer and the system, or that identifies an electronic document served in response to a request from a client computer. In some implementations, the plaintext data includes or references source image data that identifies a particular source image served to the client computer or information associated with the source image. - At
stage 404, an encoding input generator, e.g., encodinginput generator 112, of the system generates an encoding image from the plaintext data. The encoding image is an image that encodes the plaintext data. In some implementations, the encoding image is a matrix-type barcode that represents the plaintext data. One example of a suitable matrix-type barcode is a Quick Response Code (QR code). The encoding image can have a pre-defined size in terms of a number of row and columns of pixels. Each pixel in the encoding image can encode a binary bit of data, where the value of each bit is represented by a different color. For instance, a pixel that encodes the binary value ‘1’ may be black while a pixel that encodes the binary value ‘0’ may be white. - One example of an
encoding image 302 is shown inFIG. 3A . Theencoding image 302 in this example is a matrix-type barcode, e.g., a QR code, that has three rows of pixels and three columns of pixels. Of course, other sizes of encoding images are also possible with more or fewer rows and columns of pixels. In theencoding image 302, some pixels have a first binary color, i.e., black, and other pixels have a second binary color, i.e., white. The color of each pixel is defined by a color value. The pixels located at (row, column) coordinates (1, 2), (3, 1), and (3, 3), for example, are white, while the other pixels in encodingimage 302 are black. - Additionally, each pixel may have a transparency value (also referred to as an “alpha” value) that indicates a transparency level of the pixel. The transparency value may be normalized to the range [0, 1] where a value of ‘0’ represents fully transparent, a value of ‘1’ represents fully non-transparent (opaque), and intermediate values between ‘0’ and ‘1’ represent partial transparencies. In some implementations, the
encoding image 302 is fully non-transparent such that each of the pixels of theencoding image 302 has a transparency value of ‘1’. Image rendering applications can then use transparency values to determine composite colors for pixels that result when a foreground image is overlaid on a background image. For example, in some blending procedures, if the color value of the background pixel is ‘bgRGB’ and the color value of the foreground pixel is ‘fgRGB’ with a transparency (alpha) value of ‘fgA’ normalized to the range [0, 1], then the composite pixel that is ultimately rendered for display to a user will have a color value of fgRGB*fgA+bgRGB*(1−fgA). If the value of ‘fgA’ is zero, then the formula condenses to bgRGB*(1−fgA).FIG. 3F , for example, shows an example background image 312 (e.g., a source image), where each pixel has a color value identified by the symbol CnRGB. -
FIG. 3G shows an encodedsource image 314 that has been generated using the blending technique described in the preceding paragraph to overlay watermark image 310 (foreround image) on source image 312 (background image), where the colors of certain encoded pixels in the encodedsource image 314 are a blend of partially transparent foreground pixels from thewatermark image 310 and the corresponding background pixel colors from thesource image 312. It should be noted that the color values and expressions shown inFIG. 3G assume that shaded pixels in the foreground watermark image are black and thus the foreground RGB value for the pixel is (0, 0, 0). Accordingly, the full expression for the composite pixel that results from blending the foreground watermark image and the background source image reduces from ‘fgRGB*fgA+bgRGB*(1−fgA)’ to simply ‘bgRGB*(1−fgA).’ Thus, if a foreground pixel from the watermark image has color (0, 0, 0) and transparency value Ba, and the color of the background pixel is CnRGB, then the composite color value for the pixel is defined by the expression CnRGB*(1−Ba). If the foreground pixel from the watermark image has color (0, 0, 0) andtransparency value 0, and the color of the background pixel is CnRGB, then the composite color value for the pixel is CnRGB. - At
stage 406, a watermark image generator, e.g.,watermark image generator 114, of the system generates a watermark image. The watermark image is a semi-transparent image that includes pixels having two or more different levels of transparency to encode the plaintext data item. For bi-level transparency, the watermark image may consist of a first set of pixels having a first transparency level, e.g., partially transparent, and a second set of pixels having a second transparency level, e.g., fully transparent. The first set of pixels may be encoded pixels that each represents a respective pixel from the encoding image having a first color value such as black. The second set of pixels may include two subsets of pixels, namely encoded pixels and blank pixels. Although the encoded and blank pixels in the second subset may share the same transparency level, only the encoded subset of pixels represents pixels from the encoding image having a second color value, such as white. The blank pixels do not represent any pixels from the encoding image, but are instead interspersed among the encoded pixels of the first and second sets of pixels to facilitate recovery of information from an encoded source image that may later result from the watermark image being overlaid on a background image. - In some implementations, the system carries out
stage 404 according to theexample process 500 depicted in the flowchart ofFIG. 5 . Theprocess 500 is a process for generating a watermark image from an encoding image. Theprocess 500 can be performed by a system of one or more computers, e.g.,server system 102, and specifically by a watermark image generator, e.g.,watermark image generator 114. - At
stage 502, the watermark image generator identifies an encoding image such asencoding image 302. - Optionally, at
stage 504, the watermark image generator normalizes the encoding image. Normalizing the encoding image can involve mapping the respective color values and transparency values of all or some of the pixels in the encoding image to pre-defined color and/or transparency values. For example, if in the original encoding image, binary values encoded in the image were distinguished by pixel color, then in the normalized encoding image, binary values encoded in the normalized image may be distinguished by pixel transparency. -
FIGS. 3B and 3C depict one example of normalizing an encoding image. In particular,FIG. 3B shows anexample representation 304 of theoriginal encoding image 302 in which the black pixels all have the color value BRGB and have the transparency value BA and the white pixels all have the color value WRGB and have the transparency value WA. In some implementations, WA and BA are identical.FIG. 3C shows a normalizedencoding image 306 where the colors and transparencies of the pixels have been transformed so that the black and white pixels are assigned the common color BRGB but are differentiated by their transparencies. The black pixels are assigned the transparency value BA, while the white pixels are assigned the transparency value of ‘0’, i.e., fully transparent. - Referring again to
FIG. 5 , atstage 506, the watermark image generator generates an initial watermark canvas. In some implementations, the initial watermark canvas is a preliminary image that has a pre-defined size and provides a starting point for creation of a watermark image. An example initial watermark canvas iscanvas 308, as shown inFIG. 3D . Thecanvas 308 has the same number of rows as the normalizedencoding image 306, but is made to have twice the number of columns as the normalizedencoding image 306. Additionally, all the pixels in theinitial watermark canvas 308 are blank pixels that have the same color and transparency level. Blank pixels are pixels that do not encode information from the encoding image and do not correspond to any pixels in the encoding image. In some implementations, blank pixels have the same transparency as the second set of pixels in the normalizedencoding image 306. For example, the blank pixels incanvas 308 are fully transparent as indicated by the latter value in the respective parentheticals (color value, transparency value) for each pixel. - At
stage 508, the watermark image generator uses the normalized encoding image to add encoded pixels to the initial watermark canvas to create a final watermark image. Encoded pixels are pixels in the watermark image which, in contrast to blank pixels, do encode information from the encoding image. Each encoded pixel in the watermark image corresponds to one of the pixels in the normalized encoding image. In some implementations, the final watermark image is generated by replacing blank pixels in the initial watermark canvas with encoded pixels from the normalized encoding image. For example, a given blank pixel in the initial watermark canvas may be fully transparent. If that pixel is made to be an encoded pixel, then that pixel is assigned the transparency of a corresponding pixel from the normalized encoding image. In some cases, the pixel may remain fully transparent if the corresponding pixel from the normalized encoding image is fully transparent. In other cases, the transparency of the watermark pixel may be adjusted to be partially transparent if the corresponding pixel from the normalized encoding image is partially transparent. -
FIG. 3E shows one example of awatermark image 310 that the watermark image generator generates from theinitial watermark canvas 308 based on the normalizedencoding image 306. Each of the pixels in thewatermark image 310 that have a patterned background represents an encoded pixel. Each of the pixels in thewatermark image 310 having a non-patterned white background represents a blank pixel. As thewatermark image 310 shows, the encoded pixels are distributed among the blank pixels such that each encoded pixel is neighbored at the top, bottom, left, and/or right by at least two blank pixels. In general, the watermark image generator distributes encoded pixels in the watermark image so that each encoded pixel is directly adjacent to at least one, two, three, four, or more blank pixels. This arrangement can allow encoded information to later be recovered from a source image that has been augmented by a watermark image without a priori knowledge of the source image, as described further below. In the example ofFIG. 3E , the watermark image generator has interspersed the encoded pixels in the watermark image by starting with the arrangement of pixels in the normalized encoding image, inserting a blank pixel directly to the right of each encoded pixel in odd-numbered rows, and inserting a blank pixel directly to the left of each encoded pixel in even-numbered rows so as to shift the encoded pixels in even-numbered rows by one pixel relative to the encoded pixels in odd-numbered rows. The effect is to stagger the encoded pixels and to surround them by blank pixels, as shown in thefinal watermark image 310. Thus, in thefinal watermark image 310, the only pixels that are not fully transparent are the encoded pixels that correspond to black pixels in theoriginal encoding image 302. More generally, encoded pixels in thewatermark image 310 that correspond to pixels of a first binary color in theoriginal encoding image 302 may be assigned a first transparency level, whereas a different, second transparency level may be assigned to both blank pixels and encoded pixels that correspond to pixels of a second binary color from theoriginal encoding image 302. - In some implementations, the watermark image generator can use other techniques for mapping encoded pixels from the normalized
encoding image 306 to thefinal watermark image 310. The watermark image generator may use a mapping template that correlates positions for encoded pixels in thewatermark image 310 to pixel positions in the normalizedencoding image 306. Every pixel in the normalizedencoding image 306 can be mapped to a pixel in thewatermark image 310. In some implementations, the system can also use the mapping template to perform a reverse mapping of encoded pixels from an encoded source image in order to reconstruct thewatermark image 310 or theencoding image 302 from the encoded source image. The mapping template can identify any arrangement of encoding pixels (e.g., in any order) in thewatermark image 310, so long as each of the encoding pixels directly neighbors one or more blank pixels. - Referring back to
FIG. 4 , atstage 408, the system generates an entry in a response records database, e.g.,response records database 120. The response records database contains a log that stores information about each response or other electronic document (e.g., a pushed electronic document that is not responsive to a specific request) transmitted to one or more client computers in communication with the server system over a period of time. Each response may be associated with a client session, which may entail one or more request-response interactions between a given client computer and the server system. In some implementations, a given entry in the response records database includes a session ID, a response ID, and source image data. The session ID uniquely identifies a server-client network session, the response ID uniquely identifies a response served to the client computer during a given session, and the source image data identifies information about a source image transmitted to the client computer for a particular session or response. The session ID, response ID, or both may form the plaintext data that is encoded in the encoding image and watermark image so that the system can later use a watermark overlaid on a source image to recover the session ID, response ID, or both, and to lookup the appropriate source image data associated with the session ID and/or response ID. - At
stage 410, the server system serves an electronic document to the client computer. The electronic document may include computer code that, when executed by the client computer, causes the client computer to request and obtain the source image and the watermark image. Further, the computer code may include instructions for the client computer to overlay the watermark image on the source image when the electronic document is rendered for presentation to a user. For example, anexample process 600 for rendering an encoded source image at a client computer is depicted inFIG. 6 . Theprocess 600 can be carried out by one or more computing devices, e.g.,client computer 104. In some implementations, theprocess 600 is carried out by an application installed on the client computer, such as a web browsing application. - At
stage 602, the client computer receives a source image from the server system or another computing system. Atstage 604, the client computer receives a watermark image from the server system, along with an electronic document that contains instructions for the watermark image to be displayed in the foreground over the source image. Atstage 606, the client computer renders the electronic document, including displaying the watermark image over the source image. The resulting image that is displayed when the watermark is overlaid on the source image is referred to as an encoded source image. In some implementations, the client computer uses a blending technique to shade pixels in the source image that are covered by partially transparent pixels in the watermark image. The blending technique can involve, for example, for each pixel in the watermark image that overlays a pixel in the background source image, determining a composite/blended color to display by weighting the color of the watermark image pixel relative to the color of the background source image pixel using a transparency value assigned to the watermark image pixel. In some implementations, the watermark image is smaller than the source image in that it has fewer rows and/or columns of pixels than the source image. In order to watermark the entire source image, the client computer may tile multiple instances of the watermark image over the source image. By way of example,FIG. 3H shows anexample source image 316.FIG. 3I shows an example watermark image 318 based on theQR code 302.FIG. 3J shows an encodedsource image 320 in which the watermark image 318 has been tiled multiple times over thesource image 316. - At
stage 412, after serving the electronic document and watermark image to the client computer, the server system later receives an encoded source image that was rendered at the client computer. Atstage 414, a decoder module of the server system, e.g., image analysis anddecoder module 118, processes the encoded source image to recover an encoded representation of the plaintext data. In some implementations, the decoding involves generating a recovery image from the encoded source image. The recovery image may be, for example, the watermark image, the encoding image, the normalized encoding image, or another image from which the plaintext data can be read. Anexample process 700 for generating a recovery image from an encoded source image is depicted inFIG. 7 . Theprocess 700 can be carried out by a system of one or more computers, e.g., image analysis anddecoder module 118 ofserver system 102. - At
stage 702, the decoder module receives an encoded source image. For example, a user of the client computer may be presented an inappropriate or irrelevant source image in an electronic document. The user may take a screenshot of the encoded source image and transmit the screenshot to the server system for analysis, e.g., to inquire about the origin of the source image. - In some instances, the client computer may have tiled multiple copies of the watermark image over the source image to render the encoded source image. The decoder module may only require a portion of the encoded source image that corresponds to a single, overlaid watermark image. Accordingly, at
stage 704, the decoder module selects such a portion of the encoded source image that corresponds to a single, overlaid watermark image. The server system may know the size/dimensions of the watermark image and selects a portion of the encoded source image that matches the known size. In some implementations, the server system selects a portion of the encoded source image that is known to correspond to one full instance of the watermark image, e.g., a portion that extends down and to the right from the top left corner of the encoded source image. In other implementations, the server system may “hunt” for an appropriate portion of the encoded source image that encompasses a single, but entire watermark image that has been overlaid in the encoded source image. For example, the decoder module may select a portion at an arbitrary location, generate a recovery image based on the selected portion, and then verify whether the recovery image encodes valid plaintext data. If the recovery image does not encode valid plaintext data, then the decoder module selects another portion at a different location of the encoded source image, generates a recovery image based on the newly selected portion, and verifies whether the recovery image encodes valid plaintext data. The decoder module may continue searching until a portion is selected that yields valid plaintext data when decoded. - At
stage 706, the decoder module generates a recovery image using the selected portion of the encoded source image. Generating a recovery image can involve performing the operations represented in stages 708-712 for each encoded pixel in the selected portion of the encoded source image. The encoded source image can include both encoded pixels and blank pixels. Encoded pixels carry encoded information and correspond to encoded pixels in the watermark image. Blank pixels do not carry encoded information and correspond to blank pixels in the watermark image. The decoder module uses a mapping template that identifies an arrangement of encoded pixels in a watermark image to distinguish encoded pixels from blank pixels. - Because the encoded source image is created by overlaying a partially transparent watermark image over the original source image, the color of each pixel in the selected portion of the encoded source image is based on the color of a background pixel from the original source image, the color of a foreground pixel from the watermark image, and the transparency level of the foreground pixel in the watermark image. In some implementations, the transparency levels of the blank pixels and a portion of the encoded pixels in the overlaid watermark image that correspond to a second binary value (e.g., pixels representing white pixels in the original encoding image) may be ‘0’, i.e., fully transparent. The transparency levels of the remaining encoded pixels in the watermark image that correspond to a first binary value (e.g., pixels representing black pixels in the original encoding image) may be slightly greater than zero so as to slightly darken (e.g., by a user-imperceptible amount) the color of the respective background source image pixels when the watermark image is overlaid on the source image. Under these or similar conditions, and by applying the assumption that the color of any given pixel in the source image matches the color of its neighboring pixels within a specified tolerance, the decoder module can generate a recovery image by determining the encoded value of each encoded pixel in the selected portion of the encoded source image according to operations 708-712.
- In particular, at
stage 708, the decoder module identifies a color value that indicates the color of a given encoded pixel in the selected portion of the encoded source image. Atstage 710, the decoder module identifies the respective color values of the blank pixels that neighbor the given encoded pixel. A neighboring pixel is generally any pixel that is adjacent to the encoded pixel, e.g., directly above, below, or to the left or right of the encoded pixel. - At
stage 712, the decoder module decodes the given encoded pixel in the selected portion of the encoded source image using the identified color values of the encoded pixel and its neighboring pixels. Because the color value of each pixel in the source image is assumed to match the color value of its neighboring pixels within a specified tolerance, a decoded value of the encoded pixel can be determined by comparing the color value of the encoded pixel to the color values of one or more of its neighboring pixels. If the color value of the encoded pixel matches the color values of the neighboring pixels within the specified tolerance, then the encoded pixel is assumed not to have been darkened by a semi-transparent pixel in the overlaid watermark image, and the encoded pixel is decoded to represent a second binary value (e.g., a value that represents a ‘white’ pixel in the original encoding image). If the color value of the encoded pixel does not match the color values of the neighboring pixels within the specified tolerance, then the encoded pixel is considered to have been darkened by a semi-transparent pixel in the overlaid watermark image, and the encoded pixel is decoded to represent a first binary value (e.g., a value that represents a ‘black’ pixel in the original encoding image). Alternatively the decoder module may multiply the color values of the neighboring pixels from the encoded source image by (1−Ba), where Ba is the normalized transparency value that was assigned to encoded pixels having the first binary value (e.g., ‘black’ pixels) in the watermark image and where BRGB is represented by the value (0, 0, 0). If the resulting color values of the neighboring pixels after multiplication by (1−Ba) matches the color value of the encoded pixel within the specified tolerance, then the encoded pixel is decoded to represent the first binary value (e.g., a value that represents a ‘black’ pixel in the original encoding image), because the match indicates that the encoded pixel was also darkened by (1−Ba) during the blending process. As another alternative, the colors of the encoded pixel and its neighboring pixels may be provided as inputs to a machine-learning model that has been trained to classify encoded pixels as either encoding the first or second binary value based on the color of the encoded pixel and its neighboring pixels. In some instances, if the encoded pixel does not meet the matching criteria to be decoded to either the first or second binary value, the decoder module may arbitrarily assign either the first or second binary decoded value to that pixel. - During
stage 712, the decoder module may employ various techniques to determine a decoded value for a given encoded pixel from the encoded source image. In some implementations, the decoder module uses a matching technique that involves determining a match with the color values of a majority of neighboring pixels. If the colors of a majority of the neighboring pixels of an encoding pixel match the color of the encoding pixel within a specified tolerance, then the encoded pixel is decoded to represent the second binary value (e.g., a value that represents a ‘white’ pixel in the original encoding image). If the colors of a majority of the neighboring pixels after multiplication by (1−Ba) match the color of the encoding pixel within the specified tolerance, then the encoded pixel is decoded to represent the first binary value (e.g., a value that represents a ‘black’ pixel in the original encoding image). If no match is determined for either the first or second binary value, then an encoding pixel can be arbitrarily decoded to either the first or second binary value. In other implementations, the decoder module uses a matching technique that involves determining an average color value among the neighboring pixels and comparing the averaged color value of the neighboring pixels to the color value of the target encoding pixel. The specified tolerance may be, for example, 0, 1, 2, 3, 4, or 5 percent, for example. When the tolerance is 0 percent, the decoder module may require an exact match in color values between the encoded pixel and neighboring pixels. - The decoder module repeats operations 708-712 to decode each of the encoded pixels in the selected portion of the encoded source image, and can further use an appropriate mapping template to generate a recovery image such as a watermark image or a reconstruction of the original encoding image based on the decoded values of the pixels. In some implementations, the decoder module repeats
stages - Finally, referring back to
FIG. 4 , atstage 416 the server system uses the decoding information to recover plaintext data. The system uses the plaintext data for various purposes. In some implementations, the plaintext data represents a session ID or a response ID. The server system can query the response records database using the session ID or response ID to lookup source image data and determine, for example, a particular content provider that provided the source image. -
FIG. 8 is a schematic diagram of acomputer system 800. Thesystem 800 can be used to carry out the operations described in association with any of the computer-implemented methods, systems, devices, and other techniques described previously, according to some implementations. Thesystem 800 is intended to include various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Thesystem 800 can also include mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. Additionally the system can include portable storage media, such as, Universal Serial Bus (USB) flash drives. For example, the USB flash drives may store operating systems and other applications. The USB flash drives can include input/output components, such as a wireless transmitter or USB connector that may be inserted into a USB port of another computing device. - The
system 800 includes aprocessor 810, amemory 820, astorage device 830, and an input/output device 840. Each of thecomponents system bus 850. Theprocessor 810 is capable of processing instructions for execution within thesystem 800. The processor may be designed using any of a number of architectures. For example, theprocessor 810 may be a CISC (Complex Instruction Set Computers) processor, a RISC (Reduced Instruction Set Computer) processor, or a MISC (Minimal Instruction Set Computer) processor. - In some implementations, the
processor 810 is a single-threaded processor. In another implementation, theprocessor 810 is a multi-threaded processor. Theprocessor 810 is capable of processing instructions stored in thememory 820 or on thestorage device 830 to display graphical information for a user interface on the input/output device 840. - The
memory 820 stores information within thesystem 800. In one implementation, thememory 820 is a computer-readable medium. In one implementation, thememory 820 is a volatile memory unit. In another implementation, thememory 820 is a non-volatile memory unit. - The
storage device 830 is capable of providing mass storage for thesystem 400. In one implementation, thestorage device 830 is a computer-readable medium. In various different implementations, thestorage device 830 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device. - The input/
output device 840 provides input/output operations for thesystem 400. In one implementation, the input/output device 840 includes a keyboard and/or pointing device. In another implementation, the input/output device 840 includes a display unit for displaying graphical user interfaces. - The features described can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them. The apparatus can be implemented in a computer program product tangibly embodied in an information carrier, e.g., in a machine-readable storage device for execution by a programmable processor; and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device. A computer program is a set of instructions that can be used, directly or indirectly, in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- Suitable processors for the execution of a program of instructions include, by way of example, both general and special purpose microprocessors, and the sole processor or one of multiple processors of any kind of computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally, a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).
- To provide for interaction with a user, the features can be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer. Additionally, such activities can be implemented via touchscreen flat-panel displays and other appropriate mechanisms.
- The features can be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component, such as a client computer having a graphical user interface or an Internet browser, or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), peer-to-peer networks (having ad-hoc or static members), grid computing infrastructures, and the Internet.
- The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network, such as the described one. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular implementations of particular inventions. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely, various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Thus, particular implementations of the subject matter have been described. Other implementations are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/840,909 US11343402B2 (en) | 2017-03-14 | 2020-04-06 | Semi-transparent embedded watermarks |
US17/727,257 US11968344B2 (en) | 2017-03-14 | 2022-04-22 | Semi-transparent embedded watermarks |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
IL251149 | 2017-03-14 | ||
IL251149A IL251149A0 (en) | 2017-03-14 | 2017-03-14 | Semi-transparent watermarks for served content |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/840,909 Continuation US11343402B2 (en) | 2017-03-14 | 2020-04-06 | Semi-transparent embedded watermarks |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180270388A1 true US20180270388A1 (en) | 2018-09-20 |
US10616439B2 US10616439B2 (en) | 2020-04-07 |
Family
ID=60081267
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/712,395 Active 2037-12-14 US10616439B2 (en) | 2017-03-14 | 2017-09-22 | Semi-transparent embedded watermarks |
US16/840,909 Active 2037-12-18 US11343402B2 (en) | 2017-03-14 | 2020-04-06 | Semi-transparent embedded watermarks |
US17/727,257 Active 2037-12-23 US11968344B2 (en) | 2017-03-14 | 2022-04-22 | Semi-transparent embedded watermarks |
Family Applications After (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/840,909 Active 2037-12-18 US11343402B2 (en) | 2017-03-14 | 2020-04-06 | Semi-transparent embedded watermarks |
US17/727,257 Active 2037-12-23 US11968344B2 (en) | 2017-03-14 | 2022-04-22 | Semi-transparent embedded watermarks |
Country Status (5)
Country | Link |
---|---|
US (3) | US10616439B2 (en) |
EP (1) | EP3482330A1 (en) |
CN (2) | CN116600060A (en) |
IL (1) | IL251149A0 (en) |
WO (1) | WO2018169562A1 (en) |
Cited By (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190065122A1 (en) * | 2017-08-29 | 2019-02-28 | Toshiba Memory Corporation | Memory device including non-volatile memory and method of managing data thereof |
CN109543152A (en) * | 2018-11-06 | 2019-03-29 | 北京指掌易科技有限公司 | A kind of dark watermark display methods in mobile terminal |
CN110084735A (en) * | 2019-04-26 | 2019-08-02 | 新华三云计算技术有限公司 | Watermark adding method, analytic method, device, electronic equipment and storage medium |
WO2021045781A1 (en) | 2019-09-06 | 2021-03-11 | Google Llc | Detecting semi-transparent image watermarks |
CN113362215A (en) * | 2021-06-07 | 2021-09-07 | 展讯通信（上海）有限公司 | Image processing method, apparatus, device, storage medium, and program product |
CN113691885A (en) * | 2021-09-09 | 2021-11-23 | 深圳万兴软件有限公司 | Video watermark removing method and device, computer equipment and storage medium |
CN114092307A (en) * | 2021-11-25 | 2022-02-25 | 合芯科技(苏州)有限公司 | Watermark generation method, watermark addition method, watermark tracing equipment and storage medium |
CN114418825A (en) * | 2022-03-10 | 2022-04-29 | 太平金融科技服务(上海)有限公司深圳分公司 | Image processing method, image processing device, computer equipment and storage medium |
US11343402B2 (en) * | 2017-03-14 | 2022-05-24 | Google Llc | Semi-transparent embedded watermarks |
EP4064716A4 (en) * | 2019-11-22 | 2023-12-27 | Inka Entworks, Inc. | Client-side forensic watermark device, system and method |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112040336B (en) * | 2019-06-03 | 2023-03-14 | 中兴通讯股份有限公司 | Method, device and equipment for adding and extracting video watermark |
CN116957893B (en) * | 2023-06-26 | 2024-04-16 | 海易科技(北京)有限公司 | Watermark generation method, watermark generation device, electronic device and computer readable medium |
Family Cites Families (41)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6614914B1 (en) | 1995-05-08 | 2003-09-02 | Digimarc Corporation | Watermark embedder and reader |
US5825892A (en) | 1996-10-28 | 1998-10-20 | International Business Machines Corporation | Protecting images with an image watermark |
US7130442B2 (en) * | 1996-10-28 | 2006-10-31 | International Business Machines Corporation | Protecting images with an image watermark |
US5974172A (en) * | 1997-02-14 | 1999-10-26 | At&T Corp | Method and apparatus for coding segmented regions which may be transparent in video sequences for content-based scalability |
US6704024B2 (en) | 2000-08-07 | 2004-03-09 | Zframe, Inc. | Visual content browsing using rasterized representations |
US20040117427A1 (en) * | 2001-03-16 | 2004-06-17 | Anystream, Inc. | System and method for distributing streaming media |
JP4035383B2 (en) * | 2001-10-22 | 2008-01-23 | 株式会社リコー | Digital watermark code generation apparatus and code generation method, digital watermark decoding apparatus and decoding method, digital watermark code generation and decoding program, and recording medium recording the same |
CN1209730C (en) * | 2001-11-30 | 2005-07-06 | 温天 | Digital anti-fake method |
US7187780B2 (en) | 2001-12-13 | 2007-03-06 | Digimarc Corporation | Image processing methods using reversible watermarking |
US7046820B2 (en) * | 2002-07-16 | 2006-05-16 | Matsushita Electric Industrial Co., Ltd. | Methods for digital watermarking of images and images produced thereby |
US20080129758A1 (en) * | 2002-10-02 | 2008-06-05 | Harry Fox | Method and system for utilizing a JPEG compatible image and icon |
US20040258274A1 (en) * | 2002-10-31 | 2004-12-23 | Brundage Trent J. | Camera, camera accessories for reading digital watermarks, digital watermarking method and systems, and embedding digital watermarks with metallic inks |
US7581171B2 (en) * | 2004-01-06 | 2009-08-25 | Microsoft Corporation | Positionally encoded document image analysis and labeling |
US20060004630A1 (en) | 2004-07-02 | 2006-01-05 | Microsoft Corporation | Advertising through digital watermarks |
US8526666B1 (en) * | 2005-12-15 | 2013-09-03 | Emc Corporation | Method and system for rendering watermarked content using a watermark window |
US8243982B2 (en) * | 2008-11-21 | 2012-08-14 | Xerox Corporation | Embedding information in document border space |
US8077907B2 (en) * | 2007-08-31 | 2011-12-13 | Xerox Corporation | System and method for the generation of correlation-based digital watermarks |
US8363884B2 (en) | 2008-11-05 | 2013-01-29 | International Business Machines Corporation | Watermark hiding in designated applications |
US8373895B2 (en) * | 2008-11-21 | 2013-02-12 | Xerox Corporation | Prevention of unauthorized copying or scanning |
KR101529082B1 (en) | 2008-12-01 | 2015-06-17 | 주식회사 케이티 | Apparatus for watermarking by dividing off tracking information and method therefor |
US9214004B2 (en) * | 2008-12-18 | 2015-12-15 | Vmware, Inc. | Watermarking and scalability techniques for a virtual desktop planning tool |
KR101522555B1 (en) * | 2009-02-20 | 2015-05-26 | 삼성전자주식회사 | Method and apparatus for video display with inserting watermark |
US8553260B2 (en) * | 2009-09-28 | 2013-10-08 | Csr Imaging Us, Lp | Preview and modification of printable components of a document at a printing device |
US20120063678A1 (en) * | 2010-09-14 | 2012-03-15 | Rovi Technologies Corporation | Geometric image compression |
US8600158B2 (en) * | 2010-11-16 | 2013-12-03 | Hand Held Products, Inc. | Method and system operative to process color image data |
CN103748900B (en) * | 2011-02-04 | 2018-03-09 | 斯诺弗雷克解决方案公司 | Method and system for the unique watermark of Digital Media |
US20130128120A1 (en) * | 2011-04-06 | 2013-05-23 | Rupen Chanda | Graphics Pipeline Power Consumption Reduction |
EP2780892B1 (en) * | 2011-11-15 | 2016-10-05 | Trimble Navigation Limited | Controlling rights to a drawing in a three-dimensional modeling environment |
WO2013085807A1 (en) * | 2011-12-06 | 2013-06-13 | Gregory Dorso | Systems and methods for fast authentication with a mobile device |
US8751800B1 (en) * | 2011-12-12 | 2014-06-10 | Google Inc. | DRM provider interoperability |
US9854280B2 (en) * | 2012-07-10 | 2017-12-26 | Time Warner Cable Enterprises Llc | Apparatus and methods for selective enforcement of secondary content viewing |
US9245310B2 (en) * | 2013-03-15 | 2016-01-26 | Qumu Corporation | Content watermarking |
US9009826B1 (en) * | 2013-08-22 | 2015-04-14 | Amazon Technologies, Inc. | Image size communication |
JP2016030424A (en) * | 2014-07-30 | 2016-03-07 | キヤノン株式会社 | Recording device and recording control method |
US10089508B2 (en) * | 2015-05-28 | 2018-10-02 | Graphiclead LLC | System and method of embedding a two dimensional code with concealed secure message |
US9818051B2 (en) * | 2016-01-29 | 2017-11-14 | Ricoh Company, Ltd. | Rotation and clipping mechanism |
US20180018024A1 (en) * | 2016-07-12 | 2018-01-18 | Qualcomm Incorporated | Techniques for determining proximity based on image blurriness |
CN106372467B (en) * | 2016-08-31 | 2019-11-12 | 哈尔滨工程大学 | A kind of sea chart water mark method for copyright protection based on color component |
IL251149A0 (en) * | 2017-03-14 | 2017-06-29 | Google Inc | Semi-transparent watermarks for served content |
CN111321251B (en) * | 2020-04-16 | 2020-08-14 | 圣湘生物科技股份有限公司 | Composition, kit, method and application for detecting and typing pathogens causing respiratory tract infection |
KR20220077136A (en) * | 2020-11-30 | 2022-06-08 | 구글 엘엘씨 | Incorporate a secure watermark into your content |
-
2017
- 2017-03-14 IL IL251149A patent/IL251149A0/en unknown
- 2017-09-22 CN CN202310599666.2A patent/CN116600060A/en active Pending
- 2017-09-22 WO PCT/US2017/052860 patent/WO2018169562A1/en unknown
- 2017-09-22 EP EP17783623.6A patent/EP3482330A1/en active Pending
- 2017-09-22 US US15/712,395 patent/US10616439B2/en active Active
- 2017-09-22 CN CN201780047786.4A patent/CN109791579B/en active Active
-
2020
- 2020-04-06 US US16/840,909 patent/US11343402B2/en active Active
-
2022
- 2022-04-22 US US17/727,257 patent/US11968344B2/en active Active
Cited By (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11968344B2 (en) | 2017-03-14 | 2024-04-23 | Google Llc | Semi-transparent embedded watermarks |
US11343402B2 (en) * | 2017-03-14 | 2022-05-24 | Google Llc | Semi-transparent embedded watermarks |
US10838663B2 (en) * | 2017-08-29 | 2020-11-17 | Toshiba Memory Corporation | Memory device including non-volatile memory and method of managing data thereof |
US20190065122A1 (en) * | 2017-08-29 | 2019-02-28 | Toshiba Memory Corporation | Memory device including non-volatile memory and method of managing data thereof |
CN109543152A (en) * | 2018-11-06 | 2019-03-29 | 北京指掌易科技有限公司 | A kind of dark watermark display methods in mobile terminal |
CN110084735A (en) * | 2019-04-26 | 2019-08-02 | 新华三云计算技术有限公司 | Watermark adding method, analytic method, device, electronic equipment and storage medium |
JP7142709B2 (en) | 2019-09-06 | 2022-09-27 | グーグル エルエルシー | Detecting translucent image watermarks |
WO2021045781A1 (en) | 2019-09-06 | 2021-03-11 | Google Llc | Detecting semi-transparent image watermarks |
JP2022503307A (en) * | 2019-09-06 | 2022-01-12 | グーグル エルエルシー | Translucent image Watermark detection |
EP4064716A4 (en) * | 2019-11-22 | 2023-12-27 | Inka Entworks, Inc. | Client-side forensic watermark device, system and method |
CN113362215A (en) * | 2021-06-07 | 2021-09-07 | 展讯通信（上海）有限公司 | Image processing method, apparatus, device, storage medium, and program product |
CN113691885A (en) * | 2021-09-09 | 2021-11-23 | 深圳万兴软件有限公司 | Video watermark removing method and device, computer equipment and storage medium |
CN114092307A (en) * | 2021-11-25 | 2022-02-25 | 合芯科技(苏州)有限公司 | Watermark generation method, watermark addition method, watermark tracing equipment and storage medium |
CN114418825A (en) * | 2022-03-10 | 2022-04-29 | 太平金融科技服务(上海)有限公司深圳分公司 | Image processing method, image processing device, computer equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
CN109791579A (en) | 2019-05-21 |
CN109791579B (en) | 2023-05-30 |
US20220247884A1 (en) | 2022-08-04 |
WO2018169562A1 (en) | 2018-09-20 |
CN116600060A (en) | 2023-08-15 |
US11343402B2 (en) | 2022-05-24 |
EP3482330A1 (en) | 2019-05-15 |
US11968344B2 (en) | 2024-04-23 |
IL251149A0 (en) | 2017-06-29 |
US10616439B2 (en) | 2020-04-07 |
US20200304678A1 (en) | 2020-09-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11968344B2 (en) | Semi-transparent embedded watermarks | |
US10216467B2 (en) | Systems and methods for automatic content verification | |
CN109863527B (en) | Method and system for server-side rendering of rendered local content | |
US9330077B2 (en) | Dynamic image generation for customizable user interfaces | |
US20140189476A1 (en) | Image manipulation for web content | |
US20140337753A1 (en) | System and method for editing the appearance of a user interface | |
AU2016389048B2 (en) | Reducing latency in map interfaces | |
US9454515B1 (en) | Content browser system using graphics commands and native text intelligence | |
US8924251B2 (en) | Systems and methods for providing one or more pages from an electronic document | |
US9710440B2 (en) | Presenting fixed format documents in reflowed format | |
CN112789650A (en) | Detecting semi-transparent image watermarks | |
US20140164915A1 (en) | Conversion of non-book documents for consistency in e-reader experience | |
US20220301118A1 (en) | Image replacement inpainting | |
SG190606A1 (en) | High-fidelity rendering of documents in viewer clients | |
JP2015130158A (en) | Cloud-based font service system | |
US20170168997A1 (en) | System and computer-implemented method for incorporating an image into a page of content for transmission from a web-site | |
US20140033049A1 (en) | Context recognition through screensharing | |
JP2022536009A (en) | Rendering video with dynamic components | |
US8903120B1 (en) | System and method for providing an image having an embedded matrix code | |
US11070867B2 (en) | Method, device, and computing apparatus for acquiring broadcasting content | |
US11669291B2 (en) | System and method for sharing altered content of a web page between computing devices | |
US20170315972A1 (en) | Transforming web-based digital content to enable native rendering |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GHARAIBEH, ABDULLAH HASSAN;DABROWSKI, MICHAL;HAGGARTY, RYAN MATTHEW;AND OTHERS;SIGNING DATES FROM 20170308 TO 20170311;REEL/FRAME:043938/0795Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044283/0560Effective date: 20170930 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
CC | Certificate of correction | ||
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |