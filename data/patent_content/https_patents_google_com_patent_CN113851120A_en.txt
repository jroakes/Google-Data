CN113851120A - Developer voice action system - Google Patents
Developer voice action system Download PDFInfo
- Publication number
- CN113851120A CN113851120A CN202111019888.XA CN202111019888A CN113851120A CN 113851120 A CN113851120 A CN 113851120A CN 202111019888 A CN202111019888 A CN 202111019888A CN 113851120 A CN113851120 A CN 113851120A
- Authority
- CN
- China
- Prior art keywords
- user
- application
- intent
- computing device
- applications
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
- G10L17/22—Interactive procedures; Man-machine interfaces
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
- G10L2015/0638—Interactive procedures
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
Abstract
Methods, systems, and apparatus for receiving data identifying an application and a voice command trigger term, validating the received data, directing the received data to produce a passive data structure specifying the application, the voice command trigger term, and one or more other voice command trigger terms determined based at least on the voice command trigger term, and storing the passive data structure in a contextual intent database, wherein the contextual intent database includes one or more other passive data structures.
Description
This application is a divisional application of an invention patent application having an application date of 2016, 12/04, application number of 201680019717.8 and a name of "developer voice action system".
Technical Field
This specification relates generally, but not exclusively, to voice commands, and one particular implementation relates to configuring voice commands.
Background
Behaviors defined in software may specify a task, where a behavior is a class that controls the lifecycle of task execution. The intent is a passive data structure that can specify particular behaviors and applications associated with particular behaviors. The intent may be triggered by the application and the behavior specified by the intent may be caused to be performed on or by the target application specified by the intent.
Disclosure of Invention
A passive data structure, referred to as an intent, may specify an application and a behavior to be performed on or by the application, where the behavior is a task performed on or by the application. The intent may specify one or more trigger phrases that may be used to trigger the intent, such that the triggering of the intent results in the execution of an action by the specified application. For example, the intent may be associated with an application for a taxi service and one or more trigger phrases operating as voice actions may be specified such that when the user speaks, detection of one of the trigger phrases triggers initiation of the taxi service application. In addition to the application specified by the intent, the intent can be triggered by the application or the operating system such that the triggering of the application causes the specified behavior to be performed on or by the particular application.
Services or tools provided by the host may enable application developers to request new ideas that specify the particular application they are developing. The submission of the new intent may be in the form of a grammar, where a developer may submit the grammar to a service or tool, and the service or tool may generate an intent for a particular application based on the grammar.
The creation of new intents may enhance the flexibility of the particular application being developed by increasing the number and scope of inputs that may be provided to the application such that a particular result is achieved. Thus, the reliability of the application being developed may be improved, as it may be ensured that a particular application will respond to user input in a correct manner, and the handling of user input is improved.
In some implementations, the syntax submitted by the developer can specify one or more of an application, a trigger phrase, a behavior or composite behavior, a context, or other information. The grammar is submitted by a developer and received by a developer voice action service or tool over a network. A developer voice action service or tool may validate the grammar. For example, validation of a grammar can involve determining whether a trigger phrase has been associated with an intent for an operating system or another application, whether a behavior specified by the grammar can be performed by the specified application, whether the trigger phrase meets certain criteria, whether a format of the submitted grammar is syntactically valid, or whether the grammar submitted by a developer is otherwise a viable grammar that can be used to create new intents.
The valid grammar can be guided by a developer voice action service or tool to convert the grammar to intent and extend the trigger phrase specified in the grammar. For example, the directing of the grammar can involve an intent to convert a grammar submitted in a first format to a second format. In addition to converting grammars into intents, trigger phrases submitted by developers can be extended to include other related trigger phrases. For example, one or more trigger phrase extension rules, synonym rules, term-alternative rules, or other rules may be applied to the trigger phrase to produce one or more related trigger phrases associated with the intent. The generated intent may be stored in a contextual intent database along with one or more other intents, each of which is associated with an application, one or more trigger phrases, and optionally behavior, composite behavior, context, or other information related to the intent or a trigger of the intent.
The generation of new intentions based on grammar verification means that the developer does not need to generate new intentions by directly inputting the intentions themselves. As described above, the grammar can have a specific format that can be converted in the induction process, and the conversion process can expand the number of grammars that can be suitable candidates for new intentions because the requirements for the input grammar format are relaxed. Thus, an application may be developed to include new intents with less user input.
After the grammar submitted by the developer has been established as an intent, the user may provide speech input to the client device. The user's speech input may be transcribed to produce a transcription of the speech input, and the transcription may be matched with one or more trigger phrases associated with one or more other databases stored in the contextual intent database or specifying intent (such as a database including intent associated with a system operating the client device). Based on determining that a portion of the transcription matches a trigger phrase associated with a particular intent, the intent can be triggered such that a behavior associated with the intent is executed on or by an application specified by the particular intent.
For example, the intent associated with launching a taxi service application may be associated with the trigger phrase "call taxi". Based on the client device receiving user voice input including the phrase "call taxi," the client device or other system may trigger the launch of a taxi service application.
The innovative aspects of the subject matter described in this specification can be embodied in methods that include the actions of: the method includes receiving data identifying an application and a voice command trigger term, validating the received data, directing the received data to produce an intent specifying the application, the voice command trigger term, and one or more other voice command trigger terms determined based at least on the voice command trigger term, and storing the intent in a contextual intent database, wherein the contextual intent database includes the one or more other intents. Other embodiments of these aspects include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods encoded on computer storage devices.
These and other embodiments may each optionally include one or more of the following features. In various examples, validating the received data includes determining that the voice command trigger term is not associated with an application different from the identified application. The action includes receiving data identifying the action, wherein validating the received data includes determining that the identified action is compatible with the identified application. Directing the received data to produce an intent specifying the application, the voice command trigger term and the one or more other voice command trigger terms determined based at least on the voice command trigger term including converting the received data identifying the application and the voice command trigger term to different data formats. Directing receipt of data to produce an intent specifying an application, a voice command trigger term, and one or more other voice command trigger terms determined based at least on the voice command trigger term, including: one or more other voice command trigger terms determined based at least on the voice command trigger term are generated by applying one or more synonym rules, optional rules, or extended rules to the voice command trigger term.
The actions include: obtaining audio data comprising speech spoken by a user, generating a transcription of the speech spoken by the user based at least on the audio data, determining that at least a portion of the transcription matches a particular voice command trigger term specified by an intent stored in a contextual intent database, wherein the intent specifies a particular application and the particular voice command trigger term, and causing a task to be performed based on determining that the at least a portion of the transcription matches the particular voice command trigger term specified by the intent. Causing the task to be performed includes causing a particular application to perform the task. The actions may also include: for each of one or more intents that each specify (i) an application, and (ii) a voice command trigger term, determining that at least a portion of the transcription matches the voice command trigger term specified by the intent; the method further includes requesting a selection of a particular application from a set of applications that includes the application specified by each of the one or more intents, receiving data indicative of the selection of the particular application from the set of applications that includes the application specified by each of the one or more intents, and adjusting a strength of an association between a particular voice command trigger term and the intent specifying the particular application in response to receiving the data indicative of the selection of the particular application.
The actions include: for each of one or more intents that each specify (i) an application and (ii) a voice command trigger term, determining that at least a portion of the transcription matches the voice command trigger term specified by the intent, for each of the one or more intents, determining an affinity of the intent, the affinity indicating an affinity of the voice command trigger term specified by the intent to the application specified by the intent, selecting a particular intent based at least on the affinity for each of the one or more intents, and causing a task specified by the particular intent to be performed. The actions include: for each of one or more intents that each specify (i) an application and (ii) a voice command trigger term, determining that at least a portion of a transcription matches the voice command trigger term specified by the intent, ranking the one or more intents, selecting a particular intent based at least on the ranking of the one or more intents, and causing a task specified by the particular intent to be performed.
The innovative aspects of the subject matter described in this specification can be embodied in a computer-implemented method that includes: receiving, by a voice-action service system, a spoken utterance provided at a computing device of a user, the spoken utterance including a voice command trigger phrase; processing, by the voice action service system, the spoken utterance to determine an intent associated with the voice command trigger phrase; identifying, by the voice action service system, two or more applications that are each capable of satisfying the intent, wherein identifying the two or more applications is based on determining that the two or more applications are associated with intents in one or more databases; selecting, by the voice action service system, only one of the two or more applications as compared to the remainder of the two or more applications, wherein the only one of the two or more applications is selected based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of a user; and providing, by the voice action service system and in response to the spoken utterance, an indication of only the selected one application to a computing device of the user.
The innovative aspects of the subject matter described in this specification can be embodied in a system that includes: at least one processor; and at least one memory including instructions that, when executed, cause the at least one processor to: receiving a spoken utterance provided at a computing device of a user, the spoken utterance including a voice command trigger phrase; processing the spoken utterance to determine that the spoken utterance includes the voice command trigger phrase; identifying two or more applications, wherein identifying the two or more applications is based on determining that the two or more applications are mapped to voice command trigger phrases in one or more databases; selecting only one of the two or more applications as compared to the rest of the two or more applications, wherein the only one of the two or more applications is selected based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of the user; and providing an indication of only the selected one application to a computing device of the user in response to the utterance.
Innovative aspects of the subject matter described in this specification can be embodied in a computer-readable storage medium that includes instructions that, when executed, cause at least one processor to: receiving a spoken utterance provided at a computing device of a user, the spoken utterance including a voice command trigger phrase; processing the spoken utterance to determine an intent associated with the voice command trigger phrase; identifying two or more applications that are each capable of satisfying the intent, wherein identifying the two or more applications is based on determining that the two or more applications are associated with intents in one or more databases; selecting only one of the two or more applications as compared to the rest of the two or more applications, wherein only one of the two or more applications is selected based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of the user; and providing an indication of only the selected one application to a computing device of the user in response to the utterance.
The innovative aspects of the subject matter described in this specification can be embodied in a computer-implemented method that includes: receiving, by a voice action service system, an intent associated with a voice command trigger phrase included in a spoken utterance provided at a computing device of a user; receiving, by the voice action service system, contextual information associated with a computing device of a user; identifying, by the voice action service system, based on the intent associated with the voice command trigger phrase, an application installed on the user's computing device that, when executed by the user's computing device, satisfies the intent associated with the voice command trigger phrase; determining whether the computing device of the user is capable of executing the application to satisfy the intent of the user based on contextual information associated with the computing device of the user; and in response to determining that the computing device of the user is capable of executing the application to satisfy the user's intent: causing, by the voice action service system, a computing device of a user to execute the application to satisfy the intent associated with the voice command trigger phrase.
Innovative aspects of the subject matter described in this specification can be embodied in a voice action service system that includes: at least one processor; and at least one memory storing instructions that, when executed, cause the at least one processor to: receiving an intent associated with a voice command trigger phrase included in a spoken utterance provided at a computing device of a user; receiving contextual information associated with a computing device of a user; determining whether the user's computing device is capable of executing the application to satisfy the user's intent based on the intent associated with the voice command trigger phrase and based on contextual information associated with the user's computing device; and in response to determining that the computing device of the user is capable of executing the application to satisfy the user's intent: causing a computing device of the user to execute the application to satisfy the intent associated with the voice command trigger phrase.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 depicts an example system for a developer voice action system and service.
FIG. 2 depicts an example system for using voice action systems and services.
FIG. 3 is a flow diagram of an example process associated with a developer voice action system and service.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
FIG. 1 shows an example of a developer voice action service system 100. In general, the developer voice action service enables the application developer 101 to submit a grammar that is considered a candidate for new intent through the developer voice action service system 100.
The developer voice action service system 100 validates the grammar submitted by the application developer 101, directs the grammar by converting it from its submitted format to an intent format if valid, and extends the trigger phrase specified by the grammar to include additional trigger phrases related to the specified trigger phrase. The developer voice action service system 100 stores the new intent in a contextual intent database 140 that includes other intents that specify various applications.
Briefly, the developer voice action service system 100 includes an ingestion engine 110, a verification engine 120, a grammar guidance engine 130, and a contextual intent database 140. Any of the intake engine 110, the verification engine 120, the grammar guidance engine 130, or the contextual intent database 140 can be implemented as a component of the developer voice action service system 100 that is independent of other systems that may be in communication with the developer voice action service system 100, such as another voice action service system, or can be implemented as a component of such other systems. The validation engine 120 may access validation criteria available at the validation criteria database 125 and the grammar boot engine 130 may access boot rules available at the boot rules database 135. The application developer 101 can submit the grammar to the developer voice action service system 100 using a computing device 102 that communicates with the developer voice action service system 100 over one or more networks 105.
The application developer 101 can submit a grammar to the developer voice action service system 100 to request new intents for a particular application. In particular, the application developer 101 can submit grammars using a computing device 102 that can communicate with the developer voice action service system 100 over one or more networks 105.
The grammar submitted by the application developer 101 may specify various information related to the generation of new ideas. In particular, the grammar can specify a particular application, such as the particular application being developed by the application developer 101, for which new intents are being produced in an attempt. Alternatively, the grammar can specify a particular application that is different from the application being developed by the application developer 101, such as a grammar that specifies a particular application that is related to or in communication with the application being developed. In other examples, the grammar may specify particular aspects of the application, such as particular sub-processes, threads, or other segmentations associated with the application being developed by the application developer 101 or another application. The application developer 101 may additionally specify a trigger phrase in the grammar. The trigger phrase includes one or more terms that trigger the intent when detected from a voice input provided by the user. In some cases, the trigger phrase may include one or more objections (entries), wherein an objection acts as a placeholder in the trigger phrase that may be replaced by one or more terms of the user's speech input. For example, a trigger phrase "take me to $ position" may enable a user voice input such as "take us to northwest street of washington, dc 1425K" to be recognized as matching the trigger phrase. Other objections may be detected, such as an objection specifying a location, day, date or time, person, contact stored at the user's client device, content item such as a song, movie or other content item, or other objection.
The grammar can also optionally specify particular behaviors defined as tasks performed on or by a particular application specified by the grammar. Additionally, in some cases, the application developer 101 can specify a context associated with a grammar, where the context defines additional conditions that must exist for triggering the intent specified by the grammar, or where there is a modification to the selection of the triggering intent. In some implementations, the grammar submitted by the application developer 101 can specify developer-defined types, where a type is defined as one or more terms that should be detected if a trigger phrase is included. For example, a developer-defined type identified by one or more terms may be associated with a particular type of application-specific mode (e.g., "sleep" or "lock"), or may be identified by a particular term that is often used in conjunction with input provided to a particular application, such as a car model name for a used car search application, or may be a developer-defined type that is related to certain applications or trigger phrases. When speech input is received from a user, the speech action service system may be more likely to detect a type of term than other terms. For example, based on "sleep" being a type defined by one or more grammars that have been submitted by application developers, the voice action service system may be more likely to detect the term "sleep" from the voice input than "sleep".
In some examples, a type may be associated with or used to reference a particular entity. For example, the type defined by the term "my house" may be associated with the property associated with the user, e.g., based on the user having registered an address associated with an attribute of a voice action service system, an application, or based on the application developer 101 specifying in a grammar that the type defined by the term "my house" should be associated with the property of the user or the address of the property. Detecting a term defining a type in a user speech input may cause an action specified by a trigger intent to be performed with respect to an entity associated with the type.
For example, based on the user providing a voice input to the voice action service system "call taxi cab me home", the voice action service system may determine that the voice input matches a trigger phrase specified by an intent for a taxi service application, and may also determine that the term "my house" included in the voice input corresponds to a type associated with the user's home address. Thus, the triggering of the intent may cause the taxi service application to request a taxi to bring the user from their current location to their home address.
In such an example, the information specifying the home address of the user may be stored in association with an instance of a taxi service application running on the user's client device, may be stored at the developer voice action service system 100, may be specified by an intent stored in the contextual intent database 140, or may be otherwise stored. In some examples, the entities associated with a type are specific to a particular user, such that the term "my house" will reference different addresses of properties associated with different users, and the correct entities associated with that type may be selected for the user providing the voice input that includes that type.
As an example, the application developer 101 may submit the syntax shown in fig. 1. The grammar may specify an application named "taxi Caller" (Cab Caller) as the taxi service application. Further, the grammar can specify a trigger phrase "hire taxi (Hail a Cab)" that the application developer 101 has indicated as the trigger phrase that should cause the intent resulting from the grammar to be triggered. The grammar also specifies the specific behavior, i.e., launching the specified application. Thus, the triggering of an intent resulting from the grammar will result in the launching of a "taxi caller" application. Again, the triggering of intent resulting from grammar is not limited to launching third party applications, but may trigger other applications or functions that may be specified by the developer. The application developer 101 additionally specifies a context for the grammar. In particular, the context indicates that the intent resulting from the grammar will be triggered only when the client device providing the voice input triggering the intent hosts "version 1.2" of the "taxi caller" application. Many other contexts are contemplated in this disclosure, such as time of day, location, device orientation, weather, user authentication, proximity to an item or person carrying an item, presence (or absence) of particular software or components on a user's client device, and so forth.
The grammar submitted by the application developer 101 is received by the developer voice action service system 100 over one or more networks 105 on the ingestion engine 110. The ingestion engine 110 provides an interface for computing devices, such as the computing device 102, to enable the developer voice action service system 100 to receive grammars submitted by application developers.
The ingest engine 110 receives the grammar submitted by the application developer 101 and provides the grammar to the validation engine 120. Validation engine 120 receives the grammar and validates the grammar to determine whether a new intent can be generated based on the submitted grammar. To verify the grammar, the verification engine 120 may access verification criteria accessible at a verification criteria database 125. The validation criteria database 125 may include validation criteria that have been predetermined or generated in response to a received grammar.
For example, the validation engine 120 can validate the grammar by determining whether a trigger phrase specified by the grammar has been associated with the intent. Additionally or alternatively, validating the grammar can involve determining whether an application specified by the grammar is capable of performing a particular behavior specified by the grammar or another default behavior. For example, the validation engine 120 may access a manifest or other information associated with the application specified in the syntax indicating information about the application, such as information indicating which behaviors may be performed on or by the application, which data is available to or within the application, or other information. The validation engine 120 may consult a manifest or other information associated with the identified application to validate the grammar.
Validating the grammar can also involve determining whether the context specified in the grammar is an appropriate context, for example by ensuring that the context can be satisfied, e.g., "version 1.2" of the "taxi caller" application exists. Validating the grammar may also involve determining that the grammar is provided in the correct format so that the grammar can be converted to the intended format. Additionally or alternatively, validating the grammar can involve determining that a trigger phrase submitted for the grammar has an appropriate format, such as by determining that an objection included in the trigger phrase is correctly formatted or is a supported objection type. Validating the grammar can also include identifying one or more behaviors related to or needed to perform the behavior specified in the grammar received from the application developer 101. For example, if the application developer 101 specifies a behavior in the grammar to request a taxi using a taxi service application, the verification engine 120 may determine that requesting a taxi using a taxi service application also involves a behavior that launches the taxi service application on the user's client device, and determine the user's location using a positioning system application running on the user's client device.
In addition, the validation engine 120 may validate the received grammar by determining that the formatting or portions of the grammar are grammatically valid. In some cases, grammar guidance engine 130 may be able to guide a received grammar only if the grammar is valid, that is, if its data structure is correctly formatted according to the expected grammar format and includes only terms that may be processed or converted by grammar guidance engine 130.
In some implementations, feedback can be provided to the application developer 101 by the developer voice action service system 100. For example, the validation engine 120 can provide information to the computing device 102 of the application developer 101 indicating whether the grammar submitted by the application developer 101 has been validated. Additional or different feedback may also be provided to the application developer 101, such as information indicating whether the intent has been successfully generated using the grammar, information specifying one or more other related trigger phrases generated based on the trigger phrase specified in the grammar, or other information.
Based on the validation engine 120 validating the grammar submitted by the application developer 101, the validation engine 120 can submit the grammar to the grammar guidance engine 130. Grammar guidance engine 130 may receive the grammar and may guide the grammar to produce a grammar-based intent. The guide grammar may include converting the grammar from the received format to an intended format by the developer voice action service system 100. The guidance grammar can also include extending a specified trigger phrase in the grammar by generating one or more other trigger phrases associated with the specified trigger phrase based on the trigger phrase specified in the grammar.
For example, the developer voice action service system 100 may enable an application developer to submit grammars in a format that is more easily understood by the application developer or other user, such as by enabling the application developer to submit grammars in a format that includes textual objections. The grammar guidance engine 130 may receive the grammar in this format and may convert the grammar to an intended format, where the intended format may be a format that is more difficult for an application developer or other user to parse or understand or may be a format specific to a voice action service.
Extending the trigger phrase specified in the grammar submitted to the developer voice action service system 100 allows the developer voice action service system 100 to generate one or more related trigger phrases based on the specified trigger phrase. Various extension rules may be applied to the trigger phrase to produce a related trigger phrase. For example, one or more term-selectable rules, synonym rules, expansion rules, or other revision rules (such as other query term revision rules) may be applied to the trigger phrase specified in the grammar to generate one or more other trigger phrases that are related to and based at least in part on the trigger phrase specified in the grammar.
An optional rule may be applied to the trigger phrase that specifies whether one or more particular terms in the trigger phrase are optional in the related trigger phrase generated based on the trigger phrase specified in the grammar. The synonym rule may identify one or more synonyms for one or more terms in the grammar that may be included in a related trigger phrase generated based on the trigger phrase specified in the grammar. The expansion rules may enable a trigger phrase specified in the grammar to be expanded to include one or more additional terms, such as one or more terms identified as similar or related to the term or terms in the trigger phrase specified in the grammar. Other rules for generating related trigger phrases based on the trigger phrases specified in the grammar can be applied.
Additionally, in some examples, grammar guidance engine 130 may generate one or more trigger phrases that are related to the trigger phrase specified in the received grammar by converting the received trigger phrase or one or more related trigger phrases generated for the specified trigger phrase to one or more other languages. For example, the trigger phrase "hiring a taxi" may be converted from english to one or more of mandarin, spanish, arabic, french, hindi, german, russian, or other languages, and one or more conversions may be identified as the trigger phrase related to the trigger phrase "hiring a taxi" specified in the grammar.
Additionally, in some cases, grammar guidance engine 130 may be capable of determining variations and permutations of terms or objections included in trigger phrases specified in the received grammar. For example, if a trigger phrase specified in a grammar associated with the shopping application specifies an objection to the garment size parameter in a first cell, such as a suit size in U.S. size, the grammar guidance engine 130 may be able to determine a deformation or permutation of the objection, such as by including the objection to the garment size parameter in a second cell in another trigger phrase, such as a suit size in European size.
In some cases, grammar guidance engine 130 may determine a strength of the association score and associate it with one or more of the trigger phrases specified in the grammar and the trigger phrase determined based on the trigger phrase specified in the grammar. The strength of the association score may indicate an estimate of the applicability or uniqueness of the application specified in the grammar by the trigger phrase. For example, based on receiving a grammar specifying a trigger phrase "hiring a taxi" from the application developer 101, the grammar guidance engine 130 may determine a strength of association between the trigger phrase "hiring a taxi" and the trigger phrase "calling a taxi" that is identified as a related trigger phrase based at least on the trigger phrase specified in the received grammar.
In some cases, the validation engine 120 and/or grammar guidance engine 130 may determine the strength of the association between the trigger phrase and the application to determine whether the requested grammar is spam. For example, grammar guidance engine 130 may determine a strength of association between the trigger phrase "hiring a taxi" and the taxi caller's taxi service application or an intent to specify the taxi caller application, and may assign a score for the strength of association to the trigger phrase "hiring a taxi" based on the determination. Further, in some examples, an advertiser or application developer may attempt to submit a grammar to a developer voice action service that specifies the application and trigger phrase they are attempting to promote. The validation engine 120 and/or grammar guidance engine 130 may determine that the trigger phrase does not have a sufficiently strong association with the identified application and may therefore refuse to generate new intentions based on the received grammar and/or may provide feedback to the user submitting the grammar indicating that they must provide a trigger phrase more strongly related to the application.
For example, an advertiser of a sports news application may submit a grammar that specifies the sports news application and the trigger phrase "call taxi. The verification engine 120 and/or grammar guidance engine 130 may determine that the trigger phrase "make a call" is not strongly enough associated with the sports news application to potentially reject new intentions generated based on the grammar.
In some implementations, grammar guidance engine 130 may also provide feedback to application developer 101 at computing device 102. For example, based on directing the received grammar to generate one or more other trigger phrases that are determined based at least on the trigger phrase identified by the grammar submitted by the application developer 101, feedback identifying the one or more other trigger phrases can be presented to the application developer 101 to allow the application developer 101 to evaluate the one or more other trigger phrases to determine whether the other trigger phrases are acceptable substitute trigger phrases for the identified trigger phrase. In some implementations, the application developer 101 may be able to provide a response to the feedback, e.g., to indicate which of the one or more other trigger phrases generated by the grammar guidance engine 130 should be specified in the new idea.
Additionally, in some cases, the grammar guidance engine 130 or another component of the developer voice action service system 100 may generate one or more example queries or voice inputs based on the received intent, one or more other trigger phrases determined based on trigger phrases specified by the received grammar, or an intent generated from the received intent. Example queries or voice inputs may be used as examples to teach a user how to use a voice action service that can access intents stored in the contextual intent database 140. For example, when a user first provides a speech input that triggers an intent resulting from a grammar submitted by the application developer 101, an example query or trigger phrase may be provided for output to the user.
Based on the grammar guidance engine 130 having extended the trigger phrases specified in the grammar by generating one or more related trigger phrases and converting the grammar including the one or more related trigger phrases into the format of the intent, the developer voice operation service system 100 can store the new intent in the contextual intent database 140. For example, a new intent generated based on the grammar submitted by the application developer 101 can be added as "intent Y" to a set of intents stored in the contextual intent database, the set of intents also including at least "intent X" and "intent Z". For example, "intent Y" and "intent Z" can be intents generated based on syntax submitted by one or more other application developers in association with one or more applications that can be different from or the same as the application specified in the syntax submitted by the application developer 101.
The intents stored in the contextual intent database may specify parameters in an intent format specified by the grammar submitted by the application developer 101. For example, "intent Y" generated based on the syntax submitted by the application developer 101 may specify a "taxi caller" application, may specify a launch application behavior that will be executed on or by the "taxi caller" application if the intent is triggered, and may specify a context that enforces additional conditions when the intent is triggered, i.e., the client device on which the intent may be triggered must have the feature of "version 1.2" of the "taxi caller" application. Further, "intent Y" may specify a trigger phrase submitted by the application developer 101 and one or more additional trigger phrases determined by the developer voice action service system 100 based at least on the trigger phrase submitted by the application developer 101. For example, the trigger phrase "hiring a taxi" may be extended to also include the trigger phrases "Call taxi (Call a cab)", "get car", and "Call taxi (Call taxi)", as long as additional context is met, detection of any of these trigger phrases will result in triggering "intent Y".
Once the intent is generated based on the grammar submitted by the application developer 101 and stored in the contextual intent database 140, the user may be able to trigger the intent by providing a speech input that includes a trigger phrase associated with the intent. For example, based on a user of a client device hosting "version 1.2" of a "taxi caller" application providing a voice input "call taxi" to the client device, the client device may determine that the voice input includes the trigger phrase "call taxi", may determine that the mobile device hosts "version 1.2" of the "taxi caller" application, and may trigger an "intent Y" that causes the "taxi caller" application to launch on the user client device in response.
In some implementations, the process of storing newly generated intents in the contextual intent database 140 can be a process supported to quickly update push or deployment support. For example, once the speech developer voice action service system 100 has guided the received grammar, it may be able to quickly store the newly created intent in the contextual intent database 140 and make the newly generated intent available to the user of the voice action service who may access the intent stored in the contextual intent database 140.
FIG. 2 illustrates an example use case of the voice action service system 200 that enables a user to submit voice input that matches an intent stored by the contextual intent database 240. Matching the speech input to the intent results in the behavior specified by the intent being executed on or by an application of the user's client device 202, where the application is also specified by the intent.
Briefly, as shown in fig. 2, the voice action service system 200 includes a sound recognition engine 210, a matcher 220, a disambiguation engine 230, and an execution engine 250. The voice recognition engine 210 receives audio data over one or more wired or wireless networks 205, the wired or wireless networks 205 including voice input from the user 201 of the client device 202. For example, the user 201 may provide voice input to a voice interface application hosted on the client device 202, and the voice interface application hosted on the client device 202 may send audio data including the voice input to the voice action service system 200 over the one or more networks 205. Although shown in fig. 2 as being separate from the client device 202 in fig. 2, in certain embodiments, one or more of the sound recognition engine 210, matcher 220, disambiguation engine 230 and execution engine 250, as well as the other systems and subsystems discussed herein, may alternatively be implemented or form part of the client device 202 associated with the user 201.
In some cases, the voice recognition engine 210 may have access to one or more types defined by the voice action service system 200. For example, the voice action service system 200 may access one or more knowledge bases that identify people, places, items, or other defined types of entities such as content, e.g., movies or songs. The voice recognition engine 210 or other components of the voice action service system 200 may access one or more knowledge bases when performing voice recognition on the audio data 240 or when performing other operations related to the selection and execution of intent based on the user's voice input.
The matcher 220 can access a contextual intent database 240, the contextual intent database 240 including one or more intents generated based on a grammar submitted by an application developer (such as the application developer 101 of fig. 1). The matcher 220 may also access an Operating System (OS) intent database 245, the Operating System (OS) intent database 245 including one or more intents that are specific to one or more particular operating systems, or that are designated by an operating system as belonging to a particular application. For example, intents stored in OS intent database 245 may include intents for actions performed by an operating system, such as actions to reboot or lock a device, join a wireless network, etc., or actions performed by an operating system belonging to one or more applications, such as actions to launch a messaging, telephony, or email application. In addition, the disambiguation engine 230 may access a user personalization database 235 that includes information specific to a particular user or client device. The user personalization data may be used to disambiguate intent determined to match speech input received from the client device 202.
As shown in fig. 2, a user 201 may have a client device 202 capable of receiving voice input, such as voice commands provided by the user 201 to perform one or more actions. In some cases, the client device 202 includes a microphone or other device for obtaining voice input from a user, and an operating system or application running on the client device 202 may include software for receiving the voice input. The client device 202 may receive a voice input from the user 201, such as a voice input "call taxi. Based on the client device 202 receiving the voice input, the client device may send audio data 204, the audio data 204 including voice input to the voice action service system 200 over one or more networks 205.
The voice recognition engine 210 of the voice action service system 200 receives the audio data 204 and performs a voice recognition process on the audio data 204 to obtain a transcription of the speech input included in the audio data 204. For example, the sound recognition engine 210 may receive the audio data 204 and may optionally filter the audio data 204 to remove noise or background audio from the audio data 204 and produce a clean version of the audio data 204. The voice recognition engine 210 may then perform speech recognition processing on the clean version of the audio data 204 to obtain a transcription of the speech input. For example, the voice recognition engine 210 may generate a transcription "call taxi" from the audio data 204 received from the client device 202.
The voice recognition engine 210 provides the transcription of the audio data 204 to the matcher 220. The matcher 220 matches the transcription to one or more trigger phrases specified by the intent stored in the contextual intent database 240 or the OS intent database 245. In some cases, determining that a transcription matches a particular trigger phrase associated with a particular intent may involve determining that at least a portion of the transcription matches the particular trigger phrase, such as determining that one or more terms in the transcription match one or more terms in the particular trigger phrase. Thus, a particular transcription may be determined as a potential call to multiple intents, such that the voice action service system 200 must determine the particular intent to trigger from among the candidate intents. Based on determining that the transcription matches one or more trigger phrases specified by the one or more candidate intents, the matcher 220 may provide information specifying the one or more candidate trigger phrases to the disambiguation engine 230.
In some cases, the user 201 may provide the text input to the client device 201 such that the text input may be provided directly to the matcher 220 without being provided to the sound recognition engine 210. The matcher 220 may receive the text input and may perform the matching operations described above to identify one or more candidate intents based on the text input. In some cases, matcher 220 may be able to identify the type or other characteristics included in the text input, such as objections.
In some examples, matcher 220 may extend the transcription of audio data 204 to produce one or more related transcriptions of audio data 204. For example, the transcript "call taxi" may be extended using one or more query term expansion rules, synonym rules, term optional rules, or other rules to produce a related transcript "call taxi", "request taxi", "hire car", and the like. The matcher 220 may match one or more related transcriptions to a trigger phrase specified by an intent stored in the contextual intent database 240 or the OS intent database 245 to determine a trigger phrase for the transcription or related transcription match.
In some examples, the transcription of the extended audio data 204 requires the matcher 220 to determine whether the relevant transcription will conflict with a trigger phrase that already exists or is associated with a different application. For example, the trigger phrase "call taxi" may be specified by an intent associated with the taxi service application "taxi caller", and the related trigger phrase "request taxi" may be specified by an intent associated with the taxi service application "call taxi now". Matcher 220 may determine that the transcription and related transcription match a trigger phrase specified by the intent relating to the different application, so in some examples matcher 220 may ignore or remove the related transcription "request taxi" from the matching process. Thus, the matcher 220 or another component of the voice action service system 200 may be able to adjust the transcription of the user input to increase the chance of a match being detected while also handling any conflicts resulting from the transcription of the expanded audio data 204.
The disambiguation engine 230 may receive information specifying one or more candidate intents, and may perform operations to identify a particular intent from the candidate intents that triggered the call. Based on the request and receipt of user feedback selecting particular candidate intents to trigger or based on other information, the disambiguation engine 230 may identify particular intents to trigger from among the candidate intents based on the confidence scores associated with each candidate intention based on information accessed at the user personalization database 235.
For example, the voice action service system 200 may receive or be capable of receiving information identifying the user 201 or the client device 202 that the user 201 is using, e.g., based on the user 201 being logged into a particular application hosted by the client device 202, based on the client device 202 submitting the information identifying the user 201 or the client device 202 to the voice action service system 200. The disambiguation engine 230 may identify the user 201 or the client device 202 based on the identifying information, and may access information at the user personalization database 235 that specifies information related to the intent of the particular user 201 or client device 202. For example, information accessed by the disambiguation engine 230 at the user personalization database 235 may specify that the user 201 typically provides speech input or specific text input included in the audio data 204 to trigger a specific intent, such as opening a specific application. Similarly, the information accessed at the user personalization database 235 may specify preferences provided by the user 201 or determined based on past behavior of the user 201. For example, voice action service system 200 may determine that user 201 may be inclined to a particular taxi service application compared to the other based on past user behavior, or may determine that user 201 generally uses a particular term when referring to a particular application.
Based on the information accessed at the user personalization database 235, the disambiguation engine 230 may select a particular intent from the candidate intentions and provide information identifying the particular intent to the enforcement engine 250. For example, the disambiguation engine 230 may receive information identifying two candidate intents identified based on the transcription "call taxi (Cab Called"). One candidate intent may specify a taxi service application of "taxi call" and a second intent may specify a taxi service application called "taxi now (taxi)". The disambiguation engine 230 may access the information at the user personalization database 235 and determine that the user 201 or the user of the client device 202 generally uses a "taxi call" taxi service application as compared to a "taxi now" taxi service application, and thus may select a candidate intent specifying a "taxi caller" application from two candidate intents. The disambiguation engine 230 may then send information identifying the intent of the designated "taxi caller" application to the enforcement engine 250. In this way, the disambiguation engine 230 may rely on the personalization data of the user 201 to select a particular candidate intent without requiring additional input from the user 201.
Alternatively, the disambiguation engine 230 may identify a particular intent from a set of candidate intents based on the confidence score associated with each candidate intent. For example, each candidate intent may be associated with a confidence score that approximates the confidence that the candidate intent is an intent that the user 201 intends to trigger using a voice action. In some cases, the disambiguation engine 230 or the matcher 220 may determine a confidence score for each of the candidate intents.
The confidence score assigned to a candidate intent may be based on one or more factors. For example, a confidence score may be determined based at least in part on the exact nature of the match between the transcription and the trigger phrase specified by the intent. In such an example, a candidate intent that specifies a trigger phrase that exactly matches the transcription may be assigned a confidence score that indicates a high confidence that the candidate intent is the intent of the user 201.
In other implementations, different trigger phrases specified by the intent may be associated with a score indicative of the strength of association between the trigger phrase and the intent, and a confidence score may be determined based at least in part on the score of the strength of association of the matched trigger phrase. For example, a score for the strength of association of the trigger phrase "hire a taxi" may indicate that the association between the trigger phrase and the intent to specify the "taxi caller" application is stronger than a score for the strength of association of the trigger phrase "get a car" specified by the same intent.
Other factors may be considered in determining the confidence score for a candidate intent, such as how often a candidate intent has been triggered, the most recent time a candidate intent was triggered, other applications that may be running on the client device 202 at the time a voice action is received that may be associated with or frequently used with an application specified by a candidate intent, the time a voice action is received that may be relevant or frequently used with an application specified by a candidate intent based on the user's location or behavior that the user is identified to engage in, or based on other information. In some embodiments, a combination of these factors may be considered in determining the confidence score for a particular candidate intent. Based on the confidence scores for one or more candidate intents, the disambiguation engine 230 may select a particular candidate intent, and the disambiguation engine may send information identifying the selected candidate intent to the enforcement engine 250.
In addition, when determining the confidence score for the candidate intent, actions of other users may be considered, such as the number of other users who are actively using the application specified by the candidate intent, the number of contacts or friends of the user who are using the application specified by the candidate intent. For example, based on the user providing the voice input "call taxi," the voice action service system 200 may determine that the voice input may trigger two candidate taxi service applications and may further determine that more taxi drivers are using one of the applications than the other. Thus, the confidence score assigned to an intent for a taxi service application having a driver who has more aggressive use of the application may be higher than the confidence score assigned to an intent for a taxi service application having a driver who has less aggressive use of the application.
In yet further examples, the disambiguation engine 230 may provide the client device 202 with a request to select a particular candidate intent from the one or more candidate intents for the user 201. For example, based on receiving information from the matcher 220 specifying one or more candidate intents, the disambiguation engine may cause a request to be provided at the client device 202 for the user 201 to select a particular candidate intent, or a particular application specified by a particular candidate intent. The disambiguation engine 230 may receive information indicative of the user 201's selection and may provide information identifying a particular candidate intent selected by the user 201 to the enforcement engine 250.
For example, based on determining that the transcription "call taxi" matches a trigger phrase associated with an intent of the designated "taxi caller" application and also matches a trigger phrase associated with an intent of the designated "call now" application, the disambiguation engine 230 may provide a user interface to the user 201 at the client device 202 requesting the user 201 to select the "taxi caller" application or the "call now" application. The disambiguation engine 230 may receive information indicating that the user selected the "taxi caller" application, and in response may provide information identifying a candidate intent for the designated "taxi caller" application to the execution engine 250.
In some implementations, the disambiguation engine 230 may rank the candidate intents based on the confidence scores or other information, and may select a particular candidate intent or display the candidate intent in a user interface presented to the user 201 based on the ranking. For example, the disambiguation engine 230 may rank the candidate intents based on the confidence scores determined for each candidate intent, e.g., such that a candidate intent with a higher confidence score is ranked above a candidate intent with a lower confidence score. Other factors may affect the ranking of the candidate intent, such as how often or how recently the candidate intent was triggered, how often or how recently the application specified by the candidate intent was selected by the user in response to similar speech input.
In some cases, the user interface may present the candidate intent or application to the user differently based on the ranking, confidence score, or other information or factors associated with the candidate intent or ranking, the user interface requesting user input selecting a particular candidate intent from a plurality of candidate intents, or requesting selection of a particular application specified by the candidate intent from a set of applications, each of the set of applications specified by at least one candidate intent. For example, as shown in fig. 2, based on determining that the "taxi caller" application specified by the candidate intent is selected by the user 201 more frequently than the "summoning now" application, presenting the user 201 with a user interface requesting selection of one of the "taxi caller" or "summoning now" applications may display the option of the "taxi caller" as a selectable icon larger than the option of the "summoning now" application, may display the option of the "taxi caller" application above the option of the "summoning now" application, or may otherwise display the options differently or at different locations of the user interface.
In some cases, the disambiguation engine 230 may also update the information at the user personalization database 235 based on the disambiguation determination or receiving information selecting a particular candidate intent from the one or more candidate intents. For example, based on the disambiguation engine 230 receiving a selection of the user 201 of the "taxi caller" application in response to providing the speech input "call taxi," the disambiguation engine 230 may update the user personalization information at the user personalization database and/or the contextual intent database 240. Updating the personalization information may involve increasing the strength of an association score between the trigger phrase "call taxi" and the candidate intent specifying the "taxi caller" application, may involve adjusting a confidence score of the candidate intent specifying the "taxi caller" to indicate a stronger confidence, adjusting a confidence score of the candidate intent specifying the "now taxi" application to indicate a reduced confidence, or may otherwise update the personalization information or information associated with the intent stored at the user personalization database 235 or the contextual intent database 240.
In other examples, the disambiguation engine may also be able to access intents stored in the contextual intent database 140 or the OS intent database, and may be able to modify or adjust parameters associated with the intents or information related to the intents. For example, based on determining that the user 201 has "called a taxi" in response to the user's voice input, a "taxi caller" application is selected at the user interface provided to the user 201 instead of the "call now" application, the disambiguation engine 230 may update the candidate intents that specify the "call now" application and are stored in the contextual intent database 140 to remove the trigger phrase "call a taxi" from the set of trigger phrases specified for the intents, or to adjust the strength of the association score for the trigger phrase "call a taxi" specified by the intents. This and similar processes may enable the voice action service system 200 to evaluate the revocation capability of a service when triggering an appropriate intent, and adjust the intent or information associated with the intent to improve the accuracy or efficiency of the voice action service.
In some examples, feedback from multiple users may be aggregated and analyzed in determining how to adjust parameters associated with an intent or information related to an intent. For example, based on feedback aggregated from the user's body indicating that the trigger phrase "call taxi" typically results in the user typically selecting a "taxi caller" application as compared to a "call taxi now" application, the voice action service system 200 may determine to add an intent to the trigger phrase "call taxi" and the designated "taxi caller" application or an associated strength or confidence score of the "taxi caller" application itself.
Although discussed separately above, one or more methods available to the disambiguation engine 230 may be used to select a particular intent from the candidate intentions. For example, the disambiguation engine 230 may receive data from the matcher 220 identifying a set of five candidate intents, and may disambiguate three of the five candidate intents based on confidence scores associated with or determined for the candidate intents. The disambiguation engine 230 may then output a request to the client device 202 requesting the user 201 to select from the applications specified by the remaining two candidate intents to determine a particular candidate intent. The disambiguation engine 230 may then send information identifying candidate intents that specify the intent selected by the user to the enforcement engine 250.
Additionally, in some cases, the voice action service system 200 may present a different user interface to the user based on the analysis of the candidate intent through one or more methods used by the disambiguation engine 230. For example, if the disambiguation engine 230 selects a particular candidate intent based only on the confidence score of the candidate intent, the disambiguation engine 230 may provide a user interface to the user 201 at the client device 202 that identifies the particular intent or the application specified by the particular intent. Alternatively, if the disambiguation engine 230 determines, based on the confidence scores of the candidate intentions, that the disambiguation engine 230 should request the user to select the candidate intent or the application specified by the candidate intent, the disambiguation engine 230 may provide a different user interface for output at the client device 202 requesting the user's selection.
The execution engine 250 may receive information from the disambiguation engine 230 identifying a particular intent, and may trigger the intent to cause the behavior specified by the intent to be executed on or by the application specified by the intent. For example, the execution engine 250 may access the identified intent at the contextual intent database 240, or may otherwise access or receive the identified intent. The execution engine 250 can trigger the intent such that the behavior specified by the intent is executed on or by the application specified by the intent. For example, the execution engine 250 may cause the behavior to execute on a specified application, may provide the operating system or other application with control over the execution of the behavior on the application, or may cause the application to execute the specified behavior. In other examples, the execution engine 250 may provide information identifying the intent to trigger or the behavior specified by the intent to trigger to the client device 202, and the client device 202 may receive the information and trigger the intent or the behavior based on the received information. In some cases, the execution engine 250 may be implemented at the client device 202.
As an example, based on receiving information from the disambiguation engine 230 identifying candidate intents for a specified "taxi caller" application, the execution engine 250 may access an intent to specify the "taxi caller" application at the contextual intent database 240 and may trigger an intent to specify the "taxi caller" application. Triggering an intent to specify a "taxi caller" application may involve determining an action specified by the intent, such as an action to launch a "taxi caller" application. Based on determining that the behavior specified by the intent is a behavior to launch a "taxi caller" application, execution engine 250 may send information that causes client device 202, or an application, operating system, or other software hosted by the client, to launch the "taxi caller" application on client device 202.
In some cases, triggering the intent may involve sending the intent or information associated with the intent without determining the behavior specified by the intent. For example, the execution engine 250 may send data associated with the identified intent to the client device 202 without determining the behavior specified by the intent. In this case, the client device 202, or an application, operating system, or other software hosted by the client device, may determine the behavior to perform based on the intent, and may cause the behavior to be performed on or by the application specified in the intent.
The client device 202 may receive the information from the execution engine 250 and, based on the information, may perform operations to implement the behavior specified by the identified intent. For example, the client device 202 may receive an intent identified by the disambiguation engine 230, or may receive information teaching the client device 202 or software hosted by the client device 202 to perform an action specified by the identified intent, such as an action to launch a "taxi caller" application. Based on the received information, the client device 202 or software hosted by the client device 202 may perform the action. For example, the operating system of client device 202, the "taxi caller" application itself, or another application hosted on client device 202 may launch the "taxi caller" application in response to the received information. In this manner, the "call taxi" voice input provided by the user 201 may cause a "taxi caller" taxi service application to be launched such that the user 201 will then be able to use the "taxi caller" application to request a taxi.
FIG. 3 depicts a flowchart of an example process 300 for a developer voice action service. In some implementations, the process 300 of FIG. 3 may be performed by the developer voice action service system 100 of FIG. 1. Or may be performed by another system local to or remote from the client device, such as a system accessible and configured to receive information from the client device over one or more networks.
The process 300 begins by receiving data identifying at least an application and a trigger phrase (302). For example, the ingestion engine 110 of fig. 1 may receive a grammar from the computing device 102 associated with the application developer 101 over the one or more networks 105, and the received grammar may specify at least one application and a trigger phrase. The received data may be considered a candidate for a new idea to be registered with the developer voice action service system 100. In some examples, as described, other information may be specified in the grammar received from the application developer 101, such as information specifying a behavior or compound behavior to be performed on or by a specified application, a context associated with a new intent, one or more objections associated with a trigger phrase, or other information.
The information identifying the application and the trigger phrase may be verified to determine whether the information may be used to generate a new intent (304). For example, the verification engine 120 of the developer voice action service system 100 may verify the received information by determining that the trigger phrase identified by the data is an acceptable trigger phrase, e.g., that the trigger phrase has not yet been associated with another intent, or that the trigger phrase has insufficient relevance to the identified application. Verifying the received information may also include determining whether the format of the received information may be converted to produce a new intent, may involve determining whether an action identified in the application or received data is a valid application or may be performed by the identified application, or may involve other verification processes.
In some cases, feedback may be provided to the computing device 102 associated with the application developer 101 that includes information determined based on or related to the verification process performed on the received information. For example, the feedback may indicate whether the grammar submitted by the application developer 101 has passed the validation process, so that the grammar may be converted to produce a new intent, or may indicate a reason why the grammar submitted by the application developer 101 failed the validation process.
The received information may be directed to generate an intent that specifies at least the application, the trigger phrase identified by the received data, and one or more other trigger phrases determined based at least on the trigger phrase identified by the received data (306). Directing the received information involves determining one or more additional trigger phrases determined based at least on the identified trigger phrase by extending the identified trigger phrase to produce one or more related trigger phrases. For example, as described, the trigger phrase identified in the received data may be expanded by using one or more expansion rules, synonym rules, term-alternative rules, or other rules, and may be expanded to include conversions of the identified trigger phrase and related trigger phrases to other languages.
Directing the received information may also involve converting the received information from a grammar format it received from the computing device 102 to an intent-for-format to produce a new intent that specifies at least the application, the identified trigger phrase, and one or more additional trigger phrases determined based at least on the identified trigger phrase. In some cases, converting the received information may further include converting one or more additional trigger phrases determined based at least on the identified trigger phrase from a first format (such as a grammatical format of the received information) to an intent format to generate a new intent
New intents specifying at least the application, the trigger phrase identified in the received information, and one or more additional trigger phrases are stored in a contextual intent database (308). For example, based on the guidance engine 130 of FIG. 1 guiding the grammar received from the application developer 101 to generate a new intent, the developer voice action service system 100 can store the new intent in the contextual intent database 140. The stored intent may specify the identified application, the identified trigger phrase, and one or more trigger phrases determined based at least on the identified trigger phrase. The new intent stored in the contextual intent database 140 can also specify other information, such as an action performed on or by the application when the intent is triggered, one or more contexts associated with the intent, one or more association strength scores that estimate the strength of association between the trigger phrase and the application or action, or other information. The new intent may be stored in the contextual intent database 140 along with one or more other intents to enable future speech input provided by the user to be matched with a trigger phrase specified by one of the stored intents. In response to detecting the match, the matched intent may be triggered to cause a behavior specified by the matched intent to be executed on or by the application specified by the matched intent.
A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. For example, various forms of the flows shown above may be used, with steps reordered, added, or deleted. Accordingly, other implementations are within the scope of the following claims.
For the examples of systems and/or methods discussed herein, personal information about a user may be collected or may be utilized, the user may be provided with an opportunity to control whether programs or functions collect personal information, e.g., information about the user's social network, social actions or behaviors, profession, preferences, or current location, or whether (and/or how) the control system and/or method may perform operations more relevant to the user. In addition, certain data may be anonymized in one or more ways prior to storage or use, thereby eliminating personally identifiable information. For example, the identity of the user may be anonymized such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized when obtaining location information, such as on a city, zip code, or state level, such that the particular location of the user cannot be determined. Thus, the user may control how information about him or her is collected and used.
The embodiments and all of the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them. The term "data processing apparatus" encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both.
The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer does not require such a device. Further, the computer may be embedded in another device, e.g., a tablet computer, a mobile phone, a Personal Digital Assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name a few. Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices (e.g., EPROM, EEPROM, and flash memory devices), magnetic disks (e.g., an internal hard disk or a removable disk), magneto-optical disks, and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments may be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user, a keyboard, and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with the user, for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback, and input from the user may be received in any form, including auditory, acoustic, or tactile input.
Embodiments may be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), such as the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The association between a client and a server occurs as a result of computer programs running on the respective computers and having a client-server association with each other.
While this specification contains many specifics, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features of a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
In each of the examples that refer to HTML files, other file types or formats may be substituted. For example, the HTML file may be replaced by an XML, JSON, plain text, or other type of file. Further, where a table or hash table is mentioned, other data structures (e.g., spreadsheets, relational databases, or structured files) may be used.
Thus, particular embodiments have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results.
Claims (21)
1. A computer-implemented method, comprising:
receiving, by a voice-action service system, a spoken utterance provided at a computing device of a user, the spoken utterance including a voice command trigger phrase;
processing, by the voice action service system, the spoken utterance to determine an intent associated with the voice command trigger phrase;
identifying, by the voice action service system, two or more applications that are each capable of satisfying the intent, wherein identifying the two or more applications is based on determining that the two or more applications are associated with intents in one or more databases;
selecting, by the voice action service system, only one of the two or more applications as compared to the remainder of the two or more applications, wherein the only one of the two or more applications is selected based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of a user; and
providing, by the voice-action service system and in response to the spoken utterance, an indication of only the selected one application to a computing device of the user.
2. The computer-implemented method of claim 1, wherein selecting the only one of the two or more applications based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of a user comprises: determining that the only one application was most recently selected by the user in response to the voice command trigger phrase.
3. The computer-implemented method of claim 1, wherein the only one of the two or more applications is selected further based at least in part on a strength of an association score between the only one application and at least one of a voice command trigger phrase or intent.
4. The computer-implemented method of claim 1, wherein the only one of the two or more applications is further selected based at least in part on executing the only one application on a computing device of the user while receiving the spoken utterance.
5. The computer-implemented method of claim 1, wherein providing an indication of only the selected one application comprises providing an audible indication of only the selected one application.
6. The computer-implemented method of claim 1, further comprising:
receiving, by the voice-action service system, additional spoken utterances at the computing device of the user, the additional spoken utterances including confirmation of the selected only one application; and
in response to receiving the additional spoken utterance, executing the only one application to satisfy the intent.
7. The computer-implemented method of claim 1, wherein processing the spoken utterance to determine intent comprises:
voice recognizing, by the voice action service system, the spoken utterance to obtain a transcription of the spoken utterance; and
determining, by the voice action service system, that at least a portion of the transcription includes a voice command trigger phrase and that the voice command trigger phrase matches an intent.
8. A system, comprising:
at least one processor; and
at least one memory comprising instructions that, when executed, cause the at least one processor to:
receiving a spoken utterance provided at a computing device of a user, the spoken utterance including a voice command trigger phrase;
processing the spoken utterance to determine that the spoken utterance includes the voice command trigger phrase;
identifying two or more applications, wherein identifying the two or more applications is based on determining that the two or more applications are mapped to voice command trigger phrases in one or more databases;
selecting only one of the two or more applications as compared to the rest of the two or more applications, wherein the only one of the two or more applications is selected based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of the user; and
an indication of only the selected one application is provided to the user's computing device in response to the utterance.
9. The system of claim 8, wherein the instructions for selecting the only one of the two or more applications based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of a user comprise: instructions for determining that the only one application is the most recently selected by the user in response to the voice command trigger phrase.
10. The system of claim 8, wherein the instructions for selecting the only one of the two or more applications further comprise: instructions for selecting the only one of the two or more applications based at least in part on a strength of an association score between the only one application and a voice command trigger phrase.
11. The system of claim 8, wherein the instructions for selecting the only one of the two or more applications further comprise: instructions for selecting the only one of the two or more applications based at least in part on executing the only one application on the user's computing device while receiving the spoken utterance.
12. The system of claim 8, wherein the instructions to provide an indication of only the selected one application further comprise: instructions for providing an audible indication of the selected only one application.
13. The system of claim 8, further comprising instructions to:
receiving additional spoken utterances at the user's computing device, the additional spoken utterances including confirmation of the selected only one application; and
executing the only one application in response to receiving the additional spoken utterance.
14. A computer-readable storage medium comprising instructions that, when executed, cause at least one processor to:
receiving a spoken utterance provided at a computing device of a user, the spoken utterance including a voice command trigger phrase;
processing the spoken utterance to determine an intent associated with the voice command trigger phrase;
identifying two or more applications that are each capable of satisfying the intent, wherein identifying the two or more applications is based on determining that the two or more applications are associated with intents in one or more databases;
selecting only one of the two or more applications as compared to the rest of the two or more applications, wherein only one of the two or more applications is selected based at least in part on (i) recency of use of the only one application by a user and (ii) past behavior of the user; and
an indication of only the selected one application is provided to the user's computing device in response to the utterance.
15. A computer-implemented method, comprising:
receiving, by a voice action service system, an intent associated with a voice command trigger phrase included in a spoken utterance provided at a computing device of a user;
receiving, by the voice action service system, contextual information associated with a computing device of a user;
identifying, by the voice action service system, based on the intent associated with the voice command trigger phrase, an application installed on the user's computing device that, when executed by the user's computing device, satisfies the intent associated with the voice command trigger phrase;
determining whether the computing device of the user is capable of executing the application to satisfy the intent of the user based on contextual information associated with the computing device of the user; and
in response to determining that the computing device of the user is capable of executing the application to satisfy the user's intent:
causing, by the voice action service system, a computing device of a user to execute the application to satisfy the intent associated with the voice command trigger phrase.
16. The method of claim 15, wherein the context information associated with the user's computing device includes context information associated with applications installed on the user's computing device.
17. The method of claim 16, wherein the contextual information associated with the application comprises a version of the application installed on the user's computing device.
18. The method of claim 15, wherein the contextual information associated with the user's computing device includes an operating system utilized by the user's computing device.
19. The method of claim 15, wherein the intent associated with the voice command trigger phrase is determined locally at the computing device based on the computing device processing the spoken utterance.
20. The method of claim 19, wherein processing the utterance to determine intent comprises:
processing the speech to generate a transcription corresponding to the speech;
determining that at least a portion of the transcription matches a voice command trigger phrase;
identifying an intent based on the intent being associated with the voice command trigger phrase; and
sending the intent to a voice action service system.
21. A voice action service system comprising:
at least one processor; and
at least one memory storing instructions that, when executed, cause the at least one processor to:
receiving an intent associated with a voice command trigger phrase included in a spoken utterance provided at a computing device of a user;
receiving contextual information associated with a computing device of a user;
determining whether the user's computing device is capable of executing the application to satisfy the user's intent based on the intent associated with the voice command trigger phrase and based on contextual information associated with the user's computing device; and
in response to determining that the computing device of the user is capable of executing the application to satisfy the user's intent:
causing a computing device of the user to execute the application to satisfy the intent associated with the voice command trigger phrase.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/693,330 US9472196B1 (en) | 2015-04-22 | 2015-04-22 | Developer voice actions system |
US14/693,330 | 2015-04-22 | ||
CN201680019717.8A CN107408385B (en) | 2015-04-22 | 2016-04-12 | Developer voice action system |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680019717.8A Division CN107408385B (en) | 2015-04-22 | 2016-04-12 | Developer voice action system |
Publications (1)
Publication Number | Publication Date |
---|---|
CN113851120A true CN113851120A (en) | 2021-12-28 |
Family
ID=55953380
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680019717.8A Active CN107408385B (en) | 2015-04-22 | 2016-04-12 | Developer voice action system |
CN202111019888.XA Pending CN113851120A (en) | 2015-04-22 | 2016-04-12 | Developer voice action system |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680019717.8A Active CN107408385B (en) | 2015-04-22 | 2016-04-12 | Developer voice action system |
Country Status (8)
Country | Link |
---|---|
US (4) | US9472196B1 (en) |
EP (1) | EP3286633B1 (en) |
JP (2) | JP6538188B2 (en) |
KR (2) | KR102038074B1 (en) |
CN (2) | CN107408385B (en) |
DE (1) | DE112016001852T5 (en) |
GB (1) | GB2553234B (en) |
WO (1) | WO2016171956A1 (en) |
Families Citing this family (157)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
WO2016151699A1 (en) * | 2015-03-20 | 2016-09-29 | 株式会社 東芝 | Learning apparatus, method, and program |
US9472196B1 (en) | 2015-04-22 | 2016-10-18 | Google Inc. | Developer voice actions system |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
GB2544543B (en) * | 2015-11-20 | 2020-10-07 | Zuma Array Ltd | Lighting and sound system |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
JP6620934B2 (en) * | 2016-01-29 | 2019-12-18 | パナソニックＩｐマネジメント株式会社 | Translation support method, translation support apparatus, translation apparatus, and translation support program |
US9947316B2 (en) | 2016-02-22 | 2018-04-17 | Sonos, Inc. | Voice control of a media playback system |
US9965247B2 (en) | 2016-02-22 | 2018-05-08 | Sonos, Inc. | Voice controlled media playback system based on user profile |
US10743101B2 (en) | 2016-02-22 | 2020-08-11 | Sonos, Inc. | Content mixing |
US10509626B2 (en) | 2016-02-22 | 2019-12-17 | Sonos, Inc | Handling of loss of pairing between networked devices |
US10264030B2 (en) | 2016-02-22 | 2019-04-16 | Sonos, Inc. | Networked microphone device control |
US10095470B2 (en) | 2016-02-22 | 2018-10-09 | Sonos, Inc. | Audio response playback |
US9922648B2 (en) * | 2016-03-01 | 2018-03-20 | Google Llc | Developer voice actions system |
US10049670B2 (en) | 2016-06-06 | 2018-08-14 | Google Llc | Providing voice action discoverability example for trigger term |
US9978390B2 (en) | 2016-06-09 | 2018-05-22 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
AU2017100670C4 (en) | 2016-06-12 | 2019-11-21 | Apple Inc. | User interfaces for retrieving contextually relevant media content |
US10152969B2 (en) | 2016-07-15 | 2018-12-11 | Sonos, Inc. | Voice detection by multiple devices |
US10134399B2 (en) | 2016-07-15 | 2018-11-20 | Sonos, Inc. | Contextualization of voice inputs |
US10403275B1 (en) * | 2016-07-28 | 2019-09-03 | Josh.ai LLC | Speech control for complex commands |
US10115400B2 (en) | 2016-08-05 | 2018-10-30 | Sonos, Inc. | Multiple voice services |
US9942678B1 (en) | 2016-09-27 | 2018-04-10 | Sonos, Inc. | Audio playback settings for voice interaction |
US9743204B1 (en) | 2016-09-30 | 2017-08-22 | Sonos, Inc. | Multi-orientation playback device microphones |
US10181323B2 (en) | 2016-10-19 | 2019-01-15 | Sonos, Inc. | Arbitration-based voice recognition |
US10824798B2 (en) | 2016-11-04 | 2020-11-03 | Semantic Machines, Inc. | Data collection for a new conversational dialogue system |
WO2018148441A1 (en) | 2017-02-08 | 2018-08-16 | Semantic Machines, Inc. | Natural language content generator |
CN110301004B (en) * | 2017-02-23 | 2023-08-08 | 微软技术许可有限责任公司 | Extensible dialog system |
WO2018156978A1 (en) | 2017-02-23 | 2018-08-30 | Semantic Machines, Inc. | Expandable dialogue system |
US11069340B2 (en) | 2017-02-23 | 2021-07-20 | Microsoft Technology Licensing, Llc | Flexible and expandable dialogue system |
US10762892B2 (en) | 2017-02-23 | 2020-09-01 | Semantic Machines, Inc. | Rapid deployment of dialogue system |
US11183181B2 (en) | 2017-03-27 | 2021-11-23 | Sonos, Inc. | Systems and methods of multiple voice services |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
CN109102802B (en) * | 2017-06-21 | 2023-10-17 | 三星电子株式会社 | System for processing user utterances |
KR102007478B1 (en) * | 2017-06-28 | 2019-08-05 | 크리스토퍼 재현 윤 | Device and method for controlling application using speech recognition under predetermined condition |
CN107316643B (en) * | 2017-07-04 | 2021-08-17 | 科大讯飞股份有限公司 | Voice interaction method and device |
US20190027149A1 (en) * | 2017-07-20 | 2019-01-24 | Nuance Communications, Inc. | Documentation tag processing system |
US10475449B2 (en) | 2017-08-07 | 2019-11-12 | Sonos, Inc. | Wake-word detection suppression |
KR102411766B1 (en) * | 2017-08-25 | 2022-06-22 | 삼성전자주식회사 | Method for activating voice recognition servive and electronic device for the same |
US11132499B2 (en) | 2017-08-28 | 2021-09-28 | Microsoft Technology Licensing, Llc | Robust expandable dialogue system |
US10311874B2 (en) * | 2017-09-01 | 2019-06-04 | 4Q Catalyst, LLC | Methods and systems for voice-based programming of a voice-controlled device |
US10048930B1 (en) | 2017-09-08 | 2018-08-14 | Sonos, Inc. | Dynamic computation of system response volume |
US10446165B2 (en) | 2017-09-27 | 2019-10-15 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US10482868B2 (en) | 2017-09-28 | 2019-11-19 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US10621981B2 (en) | 2017-09-28 | 2020-04-14 | Sonos, Inc. | Tone interference cancellation |
US10466962B2 (en) * | 2017-09-29 | 2019-11-05 | Sonos, Inc. | Media playback system with voice assistance |
CN107886948A (en) * | 2017-11-16 | 2018-04-06 | 百度在线网络技术（北京）有限公司 | Voice interactive method and device, terminal, server and readable storage medium storing program for executing |
US10880650B2 (en) | 2017-12-10 | 2020-12-29 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US10818290B2 (en) | 2017-12-11 | 2020-10-27 | Sonos, Inc. | Home graph |
US10878808B1 (en) * | 2018-01-09 | 2020-12-29 | Amazon Technologies, Inc. | Speech processing dialog management |
WO2019152722A1 (en) | 2018-01-31 | 2019-08-08 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10936822B2 (en) * | 2018-05-04 | 2021-03-02 | Dell Products L.P. | Linguistic semantic analysis alert correlation system |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US10847178B2 (en) | 2018-05-18 | 2020-11-24 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
KR20190133100A (en) * | 2018-05-22 | 2019-12-02 | 삼성전자주식회사 | Electronic device and operating method for outputting a response for a voice input, by using application |
US10959029B2 (en) | 2018-05-25 | 2021-03-23 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10811009B2 (en) * | 2018-06-27 | 2020-10-20 | International Business Machines Corporation | Automatic skill routing in conversational computing frameworks |
US10681460B2 (en) | 2018-06-28 | 2020-06-09 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
US10461710B1 (en) | 2018-08-28 | 2019-10-29 | Sonos, Inc. | Media playback system with maximum volume setting |
US11076035B2 (en) | 2018-08-28 | 2021-07-27 | Sonos, Inc. | Do not disturb feature for audio notifications |
US10587430B1 (en) | 2018-09-14 | 2020-03-10 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US10878811B2 (en) | 2018-09-14 | 2020-12-29 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US10811015B2 (en) | 2018-09-25 | 2020-10-20 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US10692518B2 (en) | 2018-09-29 | 2020-06-23 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
KR102620705B1 (en) | 2018-10-11 | 2024-01-04 | 삼성전자주식회사 | Electronic device and operating method thereof |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
US11527265B2 (en) * | 2018-11-02 | 2022-12-13 | BriefCam Ltd. | Method and system for automatic object-aware video or audio redaction |
KR20200055202A (en) * | 2018-11-12 | 2020-05-21 | 삼성전자주식회사 | Electronic device which provides voice recognition service triggered by gesture and method of operating the same |
EP3654249A1 (en) | 2018-11-15 | 2020-05-20 | Snips | Dilated convolutions and gating for efficient keyword spotting |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11132989B2 (en) | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US10602268B1 (en) | 2018-12-20 | 2020-03-24 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US10867604B2 (en) | 2019-02-08 | 2020-12-15 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
WO2020166183A1 (en) * | 2019-02-13 | 2020-08-20 | ソニー株式会社 | Information processing device and information processing method |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11120794B2 (en) | 2019-05-03 | 2021-09-14 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US10586540B1 (en) | 2019-06-12 | 2020-03-10 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US11138975B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US10871943B1 (en) | 2019-07-31 | 2020-12-22 | Sonos, Inc. | Noise classification for event detection |
US11138969B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US11184298B2 (en) * | 2019-08-28 | 2021-11-23 | International Business Machines Corporation | Methods and systems for improving chatbot intent training by correlating user feedback provided subsequent to a failed response to an initial user intent |
CN110718221A (en) * | 2019-10-08 | 2020-01-21 | 百度在线网络技术（北京）有限公司 | Voice skill control method, voice equipment, client and server |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
CN110808051A (en) * | 2019-10-30 | 2020-02-18 | 腾讯科技（深圳）有限公司 | Skill selection method and related device |
US20210158803A1 (en) * | 2019-11-21 | 2021-05-27 | Lenovo (Singapore) Pte. Ltd. | Determining wake word strength |
US11482214B1 (en) * | 2019-12-12 | 2022-10-25 | Amazon Technologies, Inc. | Hypothesis generation and selection for inverse text normalization for search |
US11450325B1 (en) | 2019-12-12 | 2022-09-20 | Amazon Technologies, Inc. | Natural language processing |
US11380308B1 (en) | 2019-12-13 | 2022-07-05 | Amazon Technologies, Inc. | Natural language processing |
US11551681B1 (en) * | 2019-12-13 | 2023-01-10 | Amazon Technologies, Inc. | Natural language processing routing |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
CN111240478B (en) * | 2020-01-07 | 2023-10-13 | 百度在线网络技术（北京）有限公司 | Evaluation method, device, equipment and storage medium for equipment response |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11089440B1 (en) | 2020-03-02 | 2021-08-10 | International Business Machines Corporation | Management of geographically and temporarily distributed services |
EP4139784A1 (en) * | 2020-04-21 | 2023-03-01 | Google LLC | Hierarchical context specific actions from ambient speech |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
US11501762B2 (en) * | 2020-07-29 | 2022-11-15 | Microsoft Technology Licensing, Llc | Compounding corrective actions and learning in mixed mode dictation |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
CN112786040A (en) * | 2020-10-22 | 2021-05-11 | 青岛经济技术开发区海尔热水器有限公司 | Voice control method, device and equipment applied to intelligent household electrical appliance |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
US11862175B2 (en) * | 2021-01-28 | 2024-01-02 | Verizon Patent And Licensing Inc. | User identification and authentication |
US11908452B1 (en) * | 2021-05-20 | 2024-02-20 | Amazon Technologies, Inc. | Alternative input representations for speech inputs |
US20220406301A1 (en) * | 2021-06-16 | 2022-12-22 | Google Llc | Passive disambiguation of assistant commands |
Family Cites Families (113)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CA2119397C (en) | 1993-03-19 | 2007-10-02 | Kim E.A. Silverman | Improved automated voice synthesis employing enhanced prosodic treatment of text, spelling of text and rate of annunciation |
WO1998033129A1 (en) * | 1997-01-28 | 1998-07-30 | Casio Computer Co., Ltd. | Data processing apparatus used for communication network |
JP4267081B2 (en) | 1997-10-20 | 2009-05-27 | コーニンクレッカ フィリップス エレクトロニクス エヌ ヴィ | Pattern recognition registration in distributed systems |
US6604075B1 (en) * | 1999-05-20 | 2003-08-05 | Lucent Technologies Inc. | Web-based voice dialog interface |
US7069220B2 (en) * | 1999-08-13 | 2006-06-27 | International Business Machines Corporation | Method for determining and maintaining dialog focus in a conversational speech system |
US6748361B1 (en) | 1999-12-14 | 2004-06-08 | International Business Machines Corporation | Personal speech assistant supporting a dialog manager |
US20020072914A1 (en) * | 2000-12-08 | 2002-06-13 | Hiyan Alshawi | Method and apparatus for creation and user-customization of speech-enabled services |
JP4155383B2 (en) | 2001-03-05 | 2008-09-24 | アルパイン株式会社 | Voice recognition device operation device |
US7398209B2 (en) | 2002-06-03 | 2008-07-08 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US7502737B2 (en) | 2002-06-24 | 2009-03-10 | Intel Corporation | Multi-pass recognition of spoken dialogue |
JP4107093B2 (en) | 2003-01-30 | 2008-06-25 | 株式会社日立製作所 | Interactive terminal device and interactive application providing method |
US7013282B2 (en) | 2003-04-18 | 2006-03-14 | At&T Corp. | System and method for text-to-speech processing in a portable device |
JP2005017974A (en) * | 2003-06-30 | 2005-01-20 | Noritz Corp | Warm water system |
US7363228B2 (en) | 2003-09-18 | 2008-04-22 | Interactive Intelligence, Inc. | Speech recognition system and method |
JP4377718B2 (en) * | 2004-02-27 | 2009-12-02 | 富士通株式会社 | Dialog control system and method |
US7624018B2 (en) * | 2004-03-12 | 2009-11-24 | Microsoft Corporation | Speech recognition using categories and speech prefixing |
CN100424630C (en) * | 2004-03-26 | 2008-10-08 | 宏碁股份有限公司 | Operation method of web page speech interface |
US20060116880A1 (en) * | 2004-09-03 | 2006-06-01 | Thomas Gober | Voice-driven user interface |
JP4405370B2 (en) | 2004-11-15 | 2010-01-27 | 本田技研工業株式会社 | Vehicle equipment control device |
US7653546B2 (en) * | 2004-11-18 | 2010-01-26 | Nuance Communications, Inc. | Method and system for efficient voice-based programming |
JP3984988B2 (en) | 2004-11-26 | 2007-10-03 | キヤノン株式会社 | User interface design apparatus and control method thereof |
WO2006077942A1 (en) * | 2005-01-19 | 2006-07-27 | Brother Kogyo Kabushiki Kaisha | Radio tag information management system, read device, tag label creation device, radio tag circuit element cartridge, and radio tag |
JP4628803B2 (en) | 2005-01-25 | 2011-02-09 | 本田技研工業株式会社 | Voice recognition type device controller |
US7640160B2 (en) | 2005-08-05 | 2009-12-29 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US7949529B2 (en) | 2005-08-29 | 2011-05-24 | Voicebox Technologies, Inc. | Mobile systems and methods of supporting natural language human-machine interactions |
US9703892B2 (en) * | 2005-09-14 | 2017-07-11 | Millennial Media Llc | Predictive text completion for a mobile communication facility |
JP4260788B2 (en) | 2005-10-20 | 2009-04-30 | 本田技研工業株式会社 | Voice recognition device controller |
JP4878471B2 (en) | 2005-11-02 | 2012-02-15 | キヤノン株式会社 | Information processing apparatus and control method thereof |
JP2008076811A (en) | 2006-09-22 | 2008-04-03 | Honda Motor Co Ltd | Voice recognition device, voice recognition method and voice recognition program |
US7840409B2 (en) | 2007-02-27 | 2010-11-23 | Nuance Communications, Inc. | Ordering recognition results produced by an automatic speech recognition engine for a multimodal application |
US7877258B1 (en) | 2007-03-29 | 2011-01-25 | Google Inc. | Representing n-gram language models for compact storage and fast retrieval |
US8285329B1 (en) * | 2007-04-02 | 2012-10-09 | Sprint Communications Company L.P. | Mobile device-based control of smart card operation |
US8396713B2 (en) | 2007-04-30 | 2013-03-12 | Nuance Communications, Inc. | Method and system for using a statistical language model and an action classifier in parallel with grammar for better handling of out-of-grammar utterances |
US8028042B2 (en) * | 2007-06-15 | 2011-09-27 | Amazon Technologies, Inc. | System and method of managing media content |
US8239239B1 (en) * | 2007-07-23 | 2012-08-07 | Adobe Systems Incorporated | Methods and systems for dynamic workflow access based on user action |
US8165886B1 (en) * | 2007-10-04 | 2012-04-24 | Great Northern Research LLC | Speech interface system and method for control and interaction with applications on a computing system |
US8359204B2 (en) * | 2007-10-26 | 2013-01-22 | Honda Motor Co., Ltd. | Free-speech command classification for car navigation system |
US9241063B2 (en) * | 2007-11-01 | 2016-01-19 | Google Inc. | Methods for responding to an email message by call from a mobile device |
US8219407B1 (en) * | 2007-12-27 | 2012-07-10 | Great Northern Research, LLC | Method for processing the output of a speech recognizer |
US8370160B2 (en) | 2007-12-31 | 2013-02-05 | Motorola Mobility Llc | Methods and apparatus for implementing distributed multi-modal applications |
US20090171663A1 (en) | 2008-01-02 | 2009-07-02 | International Business Machines Corporation | Reducing a size of a compiled speech recognition grammar |
US7917368B2 (en) | 2008-02-25 | 2011-03-29 | Mitsubishi Electric Research Laboratories, Inc. | Method for interacting with users of speech recognition systems |
US8418076B2 (en) * | 2008-05-15 | 2013-04-09 | Microsoft Corporation | Managing inputs from a plurality of user input device actuators |
KR101545582B1 (en) * | 2008-10-29 | 2015-08-19 | 엘지전자 주식회사 | Terminal and method for controlling the same |
US8479051B2 (en) * | 2009-01-23 | 2013-07-02 | Microsoft Corporation | System and method for customized error reporting |
US9755842B2 (en) * | 2009-01-28 | 2017-09-05 | Headwater Research Llc | Managing service user discovery and service launch object placement on a device |
TWI420433B (en) * | 2009-02-27 | 2013-12-21 | Ind Tech Res Inst | Speech interactive system and method |
US9684741B2 (en) | 2009-06-05 | 2017-06-20 | Microsoft Technology Licensing, Llc | Presenting search results according to query domains |
US10540976B2 (en) * | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US20130219333A1 (en) * | 2009-06-12 | 2013-08-22 | Adobe Systems Incorporated | Extensible Framework for Facilitating Interaction with Devices |
US9111538B2 (en) | 2009-09-30 | 2015-08-18 | T-Mobile Usa, Inc. | Genius button secondary commands |
US20110099507A1 (en) | 2009-10-28 | 2011-04-28 | Google Inc. | Displaying a collection of interactive elements that trigger actions directed to an item |
US8868427B2 (en) * | 2009-12-11 | 2014-10-21 | General Motors Llc | System and method for updating information in electronic calendars |
US20110288859A1 (en) * | 2010-02-05 | 2011-11-24 | Taylor Andrew E | Language context sensitive command system and method |
US8515734B2 (en) | 2010-02-08 | 2013-08-20 | Adacel Systems, Inc. | Integrated language model, related systems and methods |
US8694313B2 (en) * | 2010-05-19 | 2014-04-08 | Google Inc. | Disambiguation of contact information using historical data |
KR101699720B1 (en) * | 2010-08-03 | 2017-01-26 | 삼성전자주식회사 | Apparatus for voice command recognition and method thereof |
US8731939B1 (en) | 2010-08-06 | 2014-05-20 | Google Inc. | Routing queries based on carrier phrase registration |
US8682661B1 (en) | 2010-08-31 | 2014-03-25 | Google Inc. | Robust speech recognition |
US8719727B2 (en) * | 2010-12-15 | 2014-05-06 | Microsoft Corporation | Managing an immersive environment |
KR101828273B1 (en) * | 2011-01-04 | 2018-02-14 | 삼성전자주식회사 | Apparatus and method for voice command recognition based on combination of dialog models |
US8929591B2 (en) * | 2011-03-08 | 2015-01-06 | Bank Of America Corporation | Providing information associated with an identified representation of an object |
US9104440B2 (en) * | 2011-05-27 | 2015-08-11 | Microsoft Technology Licensing, Llc | Multi-application environment |
US8818994B2 (en) * | 2011-06-27 | 2014-08-26 | Bmc Software, Inc. | Mobile service context |
US8707289B2 (en) * | 2011-07-20 | 2014-04-22 | Google Inc. | Multiple application versions |
US8997171B2 (en) * | 2011-08-19 | 2015-03-31 | Microsoft Technology Licensing, Llc | Policy based application suspension and termination |
US8806369B2 (en) * | 2011-08-26 | 2014-08-12 | Apple Inc. | Device, method, and graphical user interface for managing and interacting with concurrently open software applications |
US8762156B2 (en) * | 2011-09-28 | 2014-06-24 | Apple Inc. | Speech recognition repair using contextual information |
CN102520788B (en) * | 2011-11-16 | 2015-01-21 | 歌尔声学股份有限公司 | Voice identification control method |
WO2013101051A1 (en) | 2011-12-29 | 2013-07-04 | Intel Corporation | Speech recognition utilizing a dynamic set of grammar elements |
US9418658B1 (en) * | 2012-02-08 | 2016-08-16 | Amazon Technologies, Inc. | Configuration of voice controlled assistant |
US8902182B2 (en) * | 2012-02-24 | 2014-12-02 | Blackberry Limited | Electronic device and method of controlling a display |
US20130238326A1 (en) * | 2012-03-08 | 2013-09-12 | Lg Electronics Inc. | Apparatus and method for multiple device voice control |
US9503683B2 (en) * | 2012-03-27 | 2016-11-22 | Google Inc. | Providing users access to applications during video communications |
US20140365884A1 (en) * | 2012-03-30 | 2014-12-11 | Google Inc. | Voice command recording and playback |
US8881269B2 (en) * | 2012-03-31 | 2014-11-04 | Apple Inc. | Device, method, and graphical user interface for integrating recognition of handwriting gestures with a screen reader |
JP6012237B2 (en) * | 2012-04-18 | 2016-10-25 | キヤノン株式会社 | Information processing apparatus, control method, and program |
KR101944414B1 (en) * | 2012-06-04 | 2019-01-31 | 삼성전자주식회사 | Method for providing voice recognition service and an electronic device thereof |
US9317709B2 (en) * | 2012-06-26 | 2016-04-19 | Google Inc. | System and method for detecting and integrating with native applications enabled for web-based storage |
US8532675B1 (en) * | 2012-06-27 | 2013-09-10 | Blackberry Limited | Mobile communication device user interface for manipulation of data items in a physical space |
US8965759B2 (en) * | 2012-09-01 | 2015-02-24 | Sarah Hershenhorn | Digital voice memo transfer and processing |
US20150088523A1 (en) * | 2012-09-10 | 2015-03-26 | Google Inc. | Systems and Methods for Designing Voice Applications |
CN103674012B (en) * | 2012-09-21 | 2017-09-29 | 高德软件有限公司 | Speech customization method and its device, audio recognition method and its device |
KR101407192B1 (en) * | 2012-09-28 | 2014-06-16 | 주식회사 팬택 | Mobile terminal for sound output control and sound output control method |
KR20140089861A (en) * | 2013-01-07 | 2014-07-16 | 삼성전자주식회사 | display apparatus and method for controlling the display apparatus |
US9172747B2 (en) * | 2013-02-25 | 2015-10-27 | Artificial Solutions Iberia SL | System and methods for virtual assistant networks |
US10102845B1 (en) * | 2013-02-25 | 2018-10-16 | Amazon Technologies, Inc. | Interpreting nonstandard terms in language processing using text-based communications |
US9454957B1 (en) * | 2013-03-05 | 2016-09-27 | Amazon Technologies, Inc. | Named entity resolution in spoken language processing |
JP6236805B2 (en) * | 2013-03-05 | 2017-11-29 | 日本電気株式会社 | Utterance command recognition system |
US9530160B2 (en) * | 2013-03-14 | 2016-12-27 | Nanigans, Inc. | System and method for an affinity capture, user feedback and affinity analysis |
WO2014157903A1 (en) * | 2013-03-27 | 2014-10-02 | Samsung Electronics Co., Ltd. | Method and device for displaying service page for executing application |
US9875494B2 (en) * | 2013-04-16 | 2018-01-23 | Sri International | Using intents to analyze and personalize a user's dialog experience with a virtual personal assistant |
US20140324856A1 (en) * | 2013-04-27 | 2014-10-30 | Microsoft Corporation | Application discoverability |
US9292254B2 (en) * | 2013-05-15 | 2016-03-22 | Maluuba Inc. | Interactive user interface for an intelligent assistant |
JP2015011170A (en) | 2013-06-28 | 2015-01-19 | 株式会社ＡＴＲ−Ｔｒｅｋ | Voice recognition client device performing local voice recognition |
US9443507B2 (en) * | 2013-07-15 | 2016-09-13 | GM Global Technology Operations LLC | System and method for controlling a speech recognition system |
US20150024721A1 (en) * | 2013-07-22 | 2015-01-22 | Nvidia Corporation | Automatically connecting/disconnecting an incoming phone call to a data processing device based on determining intent of a user thereof to respond to the incoming phone call |
US9343068B2 (en) | 2013-09-16 | 2016-05-17 | Qualcomm Incorporated | Method and apparatus for controlling access to applications having different security levels |
CN103794214A (en) * | 2014-03-07 | 2014-05-14 | 联想(北京)有限公司 | Information processing method, device and electronic equipment |
US9639412B1 (en) * | 2014-03-11 | 2017-05-02 | Apteligent, Inc. | Application performance management tools with a service monitor for collecting network breadcrumb data |
US10249296B1 (en) * | 2014-05-27 | 2019-04-02 | Amazon Technologies, Inc. | Application discovery and selection in language-based systems |
US10592080B2 (en) * | 2014-07-31 | 2020-03-17 | Microsoft Technology Licensing, Llc | Assisted presentation of application windows |
US9548066B2 (en) * | 2014-08-11 | 2017-01-17 | Amazon Technologies, Inc. | Voice application architecture |
US20160103793A1 (en) * | 2014-10-14 | 2016-04-14 | Microsoft Technology Licensing, Llc | Heterogeneous Application Tabs |
US9116768B1 (en) * | 2014-11-20 | 2015-08-25 | Symantec Corporation | Systems and methods for deploying applications included in application containers |
US9812126B2 (en) * | 2014-11-28 | 2017-11-07 | Microsoft Technology Licensing, Llc | Device arbitration for listening devices |
US10248192B2 (en) * | 2014-12-03 | 2019-04-02 | Microsoft Technology Licensing, Llc | Gaze target application launcher |
US10460720B2 (en) * | 2015-01-03 | 2019-10-29 | Microsoft Technology Licensing, Llc. | Generation of language understanding systems and methods |
US9472196B1 (en) | 2015-04-22 | 2016-10-18 | Google Inc. | Developer voice actions system |
US20170075985A1 (en) * | 2015-09-16 | 2017-03-16 | Microsoft Technology Licensing, Llc | Query transformation for natural language queries |
KR20180046208A (en) * | 2016-10-27 | 2018-05-08 | 삼성전자주식회사 | Method and Apparatus for Executing Application based on Voice Command |
US11468881B2 (en) * | 2019-03-29 | 2022-10-11 | Samsung Electronics Co., Ltd. | Method and system for semantic intelligent task learning and adaptive execution |
KR20210045241A (en) * | 2019-10-16 | 2021-04-26 | 삼성전자주식회사 | Electronic device and method for sharing voice command thereof |
-
2015
- 2015-04-22 US US14/693,330 patent/US9472196B1/en active Active
-
2016
- 2016-04-12 CN CN201680019717.8A patent/CN107408385B/en active Active
- 2016-04-12 GB GB1715580.5A patent/GB2553234B/en active Active
- 2016-04-12 WO PCT/US2016/027113 patent/WO2016171956A1/en active Application Filing
- 2016-04-12 EP EP16721548.2A patent/EP3286633B1/en active Active
- 2016-04-12 CN CN202111019888.XA patent/CN113851120A/en active Pending
- 2016-04-12 JP JP2017550871A patent/JP6538188B2/en active Active
- 2016-04-12 KR KR1020177028031A patent/KR102038074B1/en active IP Right Grant
- 2016-04-12 KR KR1020197031169A patent/KR102173100B1/en active IP Right Grant
- 2016-04-12 DE DE112016001852.5T patent/DE112016001852T5/en active Pending
- 2016-09-07 US US15/258,084 patent/US10008203B2/en active Active
-
2018
- 2018-05-23 US US15/987,509 patent/US10839799B2/en active Active
-
2019
- 2019-05-30 JP JP2019101151A patent/JP6873188B2/en active Active
-
2020
- 2020-11-16 US US17/099,130 patent/US11657816B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
EP3286633B1 (en) | 2022-07-06 |
US10839799B2 (en) | 2020-11-17 |
CN107408385B (en) | 2021-09-21 |
JP2019144598A (en) | 2019-08-29 |
GB201715580D0 (en) | 2017-11-08 |
KR102038074B1 (en) | 2019-10-29 |
JP6873188B2 (en) | 2021-05-19 |
US11657816B2 (en) | 2023-05-23 |
US20180374480A1 (en) | 2018-12-27 |
GB2553234A (en) | 2018-02-28 |
KR20170124583A (en) | 2017-11-10 |
US20170186427A1 (en) | 2017-06-29 |
CN107408385A (en) | 2017-11-28 |
GB2553234B (en) | 2022-08-10 |
US10008203B2 (en) | 2018-06-26 |
US20210082430A1 (en) | 2021-03-18 |
JP2018511831A (en) | 2018-04-26 |
EP3286633A1 (en) | 2018-02-28 |
KR102173100B1 (en) | 2020-11-02 |
JP6538188B2 (en) | 2019-07-03 |
US20160314791A1 (en) | 2016-10-27 |
US9472196B1 (en) | 2016-10-18 |
DE112016001852T5 (en) | 2018-06-14 |
KR20190122888A (en) | 2019-10-30 |
WO2016171956A1 (en) | 2016-10-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107408385B (en) | Developer voice action system | |
JP6942841B2 (en) | Parameter collection and automatic dialog generation in the dialog system | |
EP3424045B1 (en) | Developer voice actions system | |
EP3627500B1 (en) | Voice action biasing system | |
US10331784B2 (en) | System and method of disambiguating natural language processing requests | |
US9606986B2 (en) | Integrated word N-gram and class M-gram language models | |
US9026431B1 (en) | Semantic parsing with multiple parsers | |
CN110291582B (en) | Language model biasing system | |
US11016968B1 (en) | Mutation architecture for contextual data aggregator |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |