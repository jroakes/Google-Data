US20230104228A1 - Joint Unsupervised and Supervised Training for Multilingual ASR - Google Patents
Joint Unsupervised and Supervised Training for Multilingual ASR Download PDFInfo
- Publication number
- US20230104228A1 US20230104228A1 US17/929,934 US202217929934A US2023104228A1 US 20230104228 A1 US20230104228 A1 US 20230104228A1 US 202217929934 A US202217929934 A US 202217929934A US 2023104228 A1 US2023104228 A1 US 2023104228A1
- Authority
- US
- United States
- Prior art keywords
- latent
- speech
- loss
- contrastive
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0495—Quantised networks; Sparse networks; Compressed networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/088—Non-supervised learning, e.g. competitive learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/0895—Weakly supervised learning, e.g. semi-supervised or self-supervised learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
Definitions
- This disclosure relates to joint unsupervised and supervised training for multilingual automated speech recognition (ASR).
- ASR automated speech recognition
- ASR Automatic speech recognition
- one challenge in developing deep learning-based ASR models is that parameters of the ASR models tend to over fit the training data, thereby resulting in the ASR models having difficulties generalizing unseen data when the training data is not extensive enough.
- training ASR models on larger training datasets improves the accuracy of the ASR model.
- Unlabeled training data and labeled training data can be incorporated to increase the volume of training data used to train the ASR models.
- JUST joint unsupervised and supervised. training
- ASR multilingual automatic speech recognition
- the JUST framework includes a feature encoder configured to receive audio features corresponding to an utterance of speech as input and generate a latent speech representation at each of a plurality of time steps.
- the JUST framework also includes a quantizer configured to receive, as input, the latent speech representations generated by the feature encoder at each of the plurality of time steps and generate, at each of the plurality of time steps, a target quantized vector token and a target token index for a corresponding latent speech representation generated by the feature encoder.
- the target token index maps the corresponding latent speech representation to the target quantized vector token stored in a codebook.
- the JUST framework also includes a contrastive net configured to: receive, as input, the latent speech representations generated by the feature encoder at each of the plurality of time steps after masking a subset of the latent speech representations; generate, at each of the plurality of time steps, a contrastive context vector for the corresponding unmasked or masked latent speech representation; and derive, at each of the plurality of time steps, a contrastive self-supervised loss based on the corresponding contrastive context vector and the corresponding target quantized vector token generated by the quantizer for the corresponding latent speech representation.
- the JUST framework also includes a masked language modeling (MLM) module configured to: receive, as input, the contrastive context vector generated by the contrastive net at each of the plurality of time steps; generate, at each of the plurality of time steps, a high-level context vector; and for each high-level context vector, learn to predict the target token index at the corresponding time step using a cross-entropy loss based on the target token index generated by the quantizer at the corresponding time step.
- the JUST framework also includes a decoder configured to receive, as input, the high-level context vector generated by the MLM module at each of the plurality of time steps and predict speech recognition hypotheses for the utterance.
- the JUST framework trains the multilingual ASR model on an unsupervised loss based on the cross-entropy loss and the contrastive self-supervised loss and a supervised loss based on the predicted speech recognition hypotheses and a ground-truth transcription of the utterance.
- Implementations of the disclosure may include one or more of the following optional features.
- the feature encoder includes two convolutional neural network (CNN) blocks.
- masking the subset of the latent speech representations includes randomly replacing each latent speech representation in the subset of latent speech representations with a corresponding random vector.
- the contrastive self-supervised loss derived by the contrastive net may further be based on K negative samples/distractors uniformly sampled from the target quantized vector token stored in the codebook that correspond to masked latent representations from the masked subset of latent representations.
- the supervised loss is further based on an entropy-based diversity loss associated with the codebook.
- the multilingual ASR model is trained on training utterances spoken in a plurality of different languages.
- training the multilingual ASR model includes training the multilingual ASR model having no prior pretraining.
- training the multilingual ASR model includes finetuning the multilingual ASR model from a pretrained checkpoint.
- the training the multilingual ASR model includes jointly training the multilingual ASR model on the unsupervised loss and the supervised loss.
- the supervised loss may include a Recurrent Neural Network-Transducer (RNN-T) loss.
- RNN-T Recurrent Neural Network-Transducer
- Another aspect of the disclosure provides a system that includes data processing hardware and memory hardware storing instructions that when executed on the data processing hardware causes the data processing hardware to perform operations.
- the operations include receiving audio features that correspond to an utterance of speech and generating, at each of a plurality of time steps, a latent speech representation based on the audio features.
- the operations also include generating, at each of the plurality of time steps, a target quantized vector token and a target token index for a corresponding latent speech representation.
- the target token index maps the corresponding latent speech representation to the target quantized vector token stored in a codebook.
- the operations also include generating, at each of the plurality of time steps, a contrastive context vector for a corresponding unmasked or masked latent speech representation.
- the operations also include deriving, at each of the plurality of time steps, a contrastive self-supervised loss based on the corresponding contrastive vector and the corresponding target quantized vector token.
- the operations also include generating, at each of the plurality of time steps, a high-level context vector based on the contrastive context vector and, for each high-level context vector, learning to predict the target token index at the corresponding time step using a cross-entropy loss based on the target token index.
- the operations also include predicting speech recognition hypotheses for the utterance based on the high-level context vectors and training a multilingual automatic speech recognition (ASR) model using an unsupervised loss based on the contrastive self-supervised losses and the cross-entropy losses and a supervised loss based on the predicted speech recognition hypotheses and a ground-truth transcription of the utterance.
- ASR multilingual automatic speech recognition
- generating the latent speech representation includes generating, by a feature encoder, the latent speech representation at each of the plurality of time steps.
- the feature encoder includes two convolutional neural network (CNN) blocks.
- the operations may further include masking a subset of the latent speech representations by randomly replacing each latent speech representation in the subset of latent speech representations with a corresponding random vector.
- the contrastive self-supervised loss is further based on K negative samples/distracters uniformly samples from the target quantized vector token stored in the codebook that correspond to masked latent representations from a masked subset of latent representations.
- the unsupervised loss is further based on an entropy-based diversity loss associated with the codebook.
- the multilingual ASR model may be trained on training utterances spoken in a plurality of different languages.
- training the multilingual ASR model includes training the multilingual ASR model having no prior pretraining.
- Training the multilingual ASR model may include finetuning the multilingual ASR model from a pretrained checkpoint.
- training the multilingual ASR model includes jointly training the multilingual ASR model on the unsupervised and the supervised loss.
- the supervised loss includes a Recurrent Neural Network-Transducer (RNN-T) loss.
- RNN-T Recurrent Neural Network-Transducer
- FIG. 1 is a schematic view of an example speech recognition system.
- FIG. 2 is a schematic view of a Recurrent Neural Network-Transducer (RNN-T) model architecture.
- RNN-T Recurrent Neural Network-Transducer
- FIGS. 3 A and 3 B are schematic views of an example training process for training a speech recognition model.
- FIG. 4 is a flowchart of an example arrangement of operations for a method of jointly training an automatic speech recognition model using unsupervised and supervised training.
- FIG. 5 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- pretraining automatic speech recognition (ASR) models has demonstrated an effective method for learning general latent representations from large-scale unlabeled training data.
- Pretraining ASR models significantly reduces the training complexity for downstream fine-tuning.
- fine-tuning refers to performing supervised training on a pretrained ASR model using a small labeled training data set because the ASR model has already been pretrained using unlabeled training data.
- the pretrained ASR model may train (i.e., fine-tune train) using only a smaller and/or less diverse labeled training data set.
- fine-tuning the pretrained ASR model using the smaller labeled training data set still achieves similar (or better) performance than a ASR model that receives no pretraining and trains using a larger and/or more diverse set of labeled training data.
- Pretraining ASR models usually includes a two-stage approach.
- the ASR model trains using a self-supervised loss derived from unlabeled training data to learn general latent representations.
- the ASR model fine-tunes its training based on a supervised loss.
- the second stage of training only requires a small set of labeled training data (i.e., audio data with corresponding labeled transcriptions) because the ASR model has already been pretrained using the unlabeled training data.
- This two stage training approach has proven successful for sequence modeling, but there are some issues with this approach. For instance, a pretrained model is susceptible to catastrophic forgetting.
- the pretrained model may forget the latent representations previously learned during the first stage of training using unlabeled training data. Stated differently, training the ASR model using a supervised loss in the second stage may overwrite the latent representations learned from the first stage of training, thereby diminishing any benefit received from pretraining the ASR model in the first stage. Forgetting previously learnt latent representations is especially prevalent when the labeled training data set is large.
- pretrained checkpoint selection is where the pretraining (i.e., first stage) of the ASR model ends and the fine-tune training (i.e., second stage) begins.
- the pretrained checkpoint varies based on how much pretraining the ASR model receives, In particular, the issue is determining when to stop pretraining and begin fine-tune training. Notably, performing too much pretraining can actually lead to a degradation in performance of the ASR model. On the other hand, performing too little pretraining can also lead to a degradation in performance of the ASR model.
- the issue of pretrained checkpoint selection is even more severe in multilingual ASR models because the different languages of the multilingual training dataset are often imbalanced.
- implementations herein are directed toward training an automatic speech recognition (ASR) model using a joint unsupervised and supervised training (JUST) process.
- the JUST process may train ASR models from scratch (i.e., ASR models that did not receive any pretraining) or train pretrained ASR models from a pretrained checkpoint.
- the JUST process may train monolingual or multilingual ASR models.
- the JUST process trains the ASR model using an unsupervised loss derived from a cross-entropy loss and a self-supervised loss and a supervised loss derived from a predicted speech recognition hypotheses and a ground-troth transcription.
- FIG. 1 illustrates an automated speech recognition (ASR) system 100 implementing an ASR model 200 that resides on a user device 102 of a user 104 and/or on a remote computing device 201 (e.g., one or more servers of a distributed system executing in a cloud-computing; environment) in communication with the user device 102 .
- ASR automated speech recognition
- the user device 102 is depicted as a mobile computing device (e.g., a smart phone), the user device 102 may correspond to any type of computing device such as, without limitation, a tablet device, a laptop/desktop computer, a wearable device, a digital assistant device, a smart speaker/display, a smart appliance, an automotive infotainment system, or an Internet-of-Things (IoT) device, and is equipped with data processing hardware 111 and memory hardware 113 .
- IoT Internet-of-Things
- the user device 102 includes an audio subsystem 108 configured to receive an utterance 106 spoken by the user 104 (e.g., the user device 102 may include one or more microphones for recording the spoken utterance 106 ) and convert the utterance 106 into a corresponding digital format associated with input acoustic frames (i.e., audio features) 110 capable of being processed by the ASR system 100 .
- the user speaks a respective utterance 106 in a natural language of English for the phrase “What is the weather in New York City?” and the audio subsystem 108 converts the utterance 106 into corresponding acoustic frames 110 for input to the ASR system 100 .
- the ASR model 200 receives, as input, the acoustic frames 110 corresponding to the utterance 106 , and generates/predicts, as output, a corresponding transcription 120 (e.g., recognition result/hypothesis) of the utterance 106 .
- the user device 102 and/or the remote computing device 201 also executes a user interface generator 107 configured to present a representation of the transcription 120 of the utterance 106 to the user 104 of the user device 102 .
- the transcription 120 output from the ASR system 100 is processed, e.g., by a natural language understanding (NLU) module executing on the user device 102 or the remote computing device 201 , to execute a user command.
- NLU natural language understanding
- a text-to-speech system may convert the transcription into synthesized speech for audible output by another device.
- the original utterance 106 may correspond to a message the user 104 is sending to a friend in which the transcription 120 is converted to synthesized speech for audible output to the friend to listen to the message conveyed in the original utterance 106 .
- an example ASR model 200 may be a frame alignment-based transducer model 200 that includes a Recurrent Neural Network-Transducer (RNN-T) model architecture which adheres to latency constraints associated with interactive applications.
- RNN-T Recurrent Neural Network-Transducer
- the use of the RNN-T model architecture is exemplary, and the frame alignment-based transducer model 200 may include other architectures such as transformer-transducer, conformer-transducer, and conformer-encoder model architectures among others.
- the RNN-T model 200 provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the RNN-T model architecture suitable for performing speech recognition entirely on the user device 102 (e.g., no communication with a remote server is required).
- the RNN-T model 200 includes an encoder network 210 , a prediction network 220 , and a joint network 230 .
- acoustic frames 110 FIG. 1
- This higher-order feature representation is denoted as h 1 enc , . . . , h T enc .
- the prediction network 220 is also an LSTM network, which, like a language model (LM), processes the sequence of non-blank symbols output by a final Softmax layer 240 so far, y 0 , . . . , y ui ⁇ 1 , into a dense representation p u i .
- LM language model
- the representations produced by the encoder and prediction/decoder networks 210 , 220 are combined by the joint network 230 .
- the prediction network 220 may be replaced by an embedding look-up table to improve latency by outputting looked-up sparse embeddings in lieu of processing dense representations.
- the joint network 230 then predicts P(y i
- the “possible speech recognition hypotheses” correspond to a set of output labels each representing a symbol/character in a specified natural language. For example, when the natural language is English, the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26-letters in the English alphabet and one label designating a space.
- the joint network 230 may output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels.
- This set of values can be a vector and can indicate a probability distribution over the set of output labels.
- the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited.
- the set of output labels can include wordpieces and/or entire words, in addition to or instead of graphemes.
- the output distribution of the joint network 230 can include a posterior probability value for each of the different output labels.
- the output y i of the joint network 230 can include 100 different probability values, one for each output label.
- the probability distribution can then be used to select and assign scores to candidate orthgraphic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (by the Softmax layer 240 ) for determining the transcription 120 .
- the Softmax layer 240 may employ any technique to select the output label/symbol with the highest probability in the distribution as the next output symbol predicted by the RNN-T model 200 at the corresponding output step. In this manner, the RNN-T model 200 does not make a conditional independence assumption, rather the prediction of each symbol is conditioned not only on the acoustics but also on the sequence of labels output so far. The RNN-T model 200 does assume an output symbol is independent of future acoustic frames 110 , which allows the RNN-T model to be employed in a streaming fashion.
- the encoder network 210 of the RNN-T model 200 includes a stack of self-attention layers/blocks, such as conformer blocks,
- each conformer block includes a series of multi-headed self attention, depth wise convolution and teed-forward layers.
- the encoder network 210 may include LSTM layers in lieu of self-attention layers/blocks.
- the prediction network 220 may have two 2,048-dimensional LSTM layers, each of which is also followed by 640-dimensional projection layer.
- the prediction network 220 may include a stack of transformer or conformer blocks, or an embedding look-up table in lieu of LSTM layers.
- the joint network 230 may also have 640 hidden units.
- the softmax layer 240 may be composed of a unified word piece or grapheme set that is generated using all unique word pieces or graphemes in a plurality of training data sets.
- FIGS. 3 A and 3 B illustrate an example JUST training process 300 for training the ASR model 200 ( FIG. 2 ).
- the ASR model 200 may be a multilingual ASR. model or a monolingual ASR model.
- the example JUST training process 300 (also referred to as simply “training process 300 ”) trains an un-trained ASR. model 200 . That is, the training process 300 trains an ASR model 200 that has not been pretrained yet. In other examples, the training process 300 trains a pretrained ASR model 200 from a pretrained checkpoint. Here, the ASR model 200 has been pretrained already and the training process 300 fine-tunes the pre-trained ASR model from the pretrained checkpoint.
- the training process 300 may train the ASR model 200 using available training data that includes a set of un-transcribed speech utterances (i.e., unlabeled training data) 302 and a set of transcribed speech utterances (i.e., labeled training data) 304 .
- Each un-transcribed speech utterance 302 includes audio-only data (i.e., unpaired data) such that the un-transcribed speech utterance 302 is not paired with any corresponding transcription.
- each respective transcribed speech utterance 304 includes a corresponding ground-truth transcription 306 paired with a corresponding speech representation of the respective transcribed speech utterance 304 (i.e., paired data).
- the set of un-transcribed speech utterances 302 and the set of transcribed speech utterances 304 may each respectively include either non-synthetic speech representations, synthetic speech representations generated by a text-to-speech (TTS) system using textual utterances (not shown), or some combination thereof.
- TTS text-to-speech
- the set of un-transcribed speech utterances 302 and the set of transcribed speech utterances 304 each include utterances spoken in a plurality of different languages for training a multilingual ASR model.
- the training process 300 includes an unsupervised loss part 300 a ( FIG. 3 A ) and a supervised loss part 300 b ( FIG. 3 B ).
- the training process 300 trains the ASR model 200 based on an unsupervised loss 355 ( FIG. 3 A ) from the unsupervised loss part 300 a and on a supervised loss 365 ( FIG. 3 B ) from the supervised loss part 300 b.
- the training process 300 trains the ASR model 200 jointly using the unsupervised loss 355 and the supervised loss 365 .
- the unsupervised loss part 300 a and the supervised loss part 300 b both include a feature encoder 311 , a masking module 215 , a contrastive net 320 , and a masked language modeling (MLM) module 330 .
- the training process 300 i.e., unsupervised loss part 300 a and supervised loss part 3001
- the unsupervised loss part 300 a trains the ASR model 200 using the un-transcribed speech utterances 302
- the supervised loss part 300 b trains the ASR model 200 using the transcribed speech utterances 304 .
- the sequence of input audio features/vectors are associated. with one of the un-transcribed speech utterances 302 or one of the transcribed speech utterances 304 .
- the latent speech representation 212 corresponds to a respective one of: one of the un-transcribed speech utterances 302 ; or one of the transcribed speech utterances 304 .
- the feature encoder 311 includes two convolutional neural network (CNN) blocks (e.g., two convolutional layers) each having a 3 ⁇ 3 filter size and a stride of (2, 2).
- CNN convolutional neural network
- a first CNN block of the two CNN blocks may include 128 channels and a second CNN block of the two CNN blocks includes 32 channels.
- the feature encoder 311 includes a subsampling block with 4 ⁇ reduction in feature dimensionality and sequence length.
- the latent speech representations 212 output from the feature encoder 311 may be fed to a masking module 215 where some of the latent speech representations 212 are randomly chosen and replaced with a trained feature vector shared between all masked time steps to provide corresponding masked latent speech representations 212 , 212 m, Alternatively, the randomly chosen latent speech representations 212 may be replaced by random feature vectors to generate the corresponding masked latent speech representations 212 m.
- the masking module 215 masks the randomly chosen latent speech representations 212 by randomly sampling, without replacement, a certain proportion p of all time steps T to be start indices and then mask the subsequent M consecutive time steps from every sample index, whereby some spans may overlap.
- the masking module 215 only masks a subset of the entire set of latent speech representations 212 resulting in a masked subset of latent speech representations 212 m and a subset of unmasked latent speech representations 212 , 212 u.
- the contrastive net 320 may include a stack of conformer blocks each with multi-headed self-attention, depth-wise convolution and feed-forward layers.
- the contrastive net 320 may include 8 conformer blocks where each conformer block includes a hidden dimensionality 1024, 8 attention heads, and a convolution kernels size of 5.
- the unsupervised loss part 300 a includes a quantizer 310 that also receives the latent speech representations 212 .
- the quantizer 310 is configured to receive, as input, the latent speech representations 212 generated by the feature encoder 311 at each of the plurality of time steps and generate, at each of the plurality of time steps, a target quantized vector token 312 and a target token index 314 for a corresponding latent speech representation 212 generated by the feature encoder 311 as output.
- the quantizer 310 generates the target quantized vector token (q i ) 312 and the target token index (y i ) 314 using latent speech representations 212 that do not include any masking.
- the quantizer 310 summarizes all of the latent speech representations 212 into representative target quantized vector tokens (i.e., discriminative speech tokens) 312 .
- the representative target quantized vector tokens 312 generated by the quantizer 310 represent a finite set of representative target quantized vector tokens referred to as a codebook 315 .
- the codebook 315 is stored at the quantizer 310 and represents the size of the codebook 315 .
- the target token index 314 maps each corresponding latent speech representation 212 to a respective one of the target quantized vector tokens 312 stored in the codebook 315 .
- all of the representative target quantized vector tokens 312 in the codebook 315 are learnable during the training process 300 .
- the unsupervised loss part 300 a is dependent upon the codebook 315 to represent both positive and negative training examples. Accordingly, the training process 300 uses an entropy-based diversity loss d associated with the codebook 315 to increase the use of the representative target quantized vector tokens 312 in the codebook 315 . That is, the training process 300 encourages equal use of the V entries in each codebook (G) 315 thereby maximizing the entropy of the averaged softmax distribution over the codebook entries for each codebook p g across a batch of training utterances represented by:
- the training process 300 uses a single codebook 315 rather than multiple codebooks 315 .
- the MLM module 330 may include a stack of 16 conformer blocks each having a hidden dimensionality of 1024, 8 attention heads, and a convolution kernel size of 5.
- the unsupervised loss part 300 a includes an unsupervised loss module 350 that derives an unsupervised loss 355 .
- the unsupervised loss module 350 may reside on the contrastive net 320 (not shown), reside on the MLM module 330 , or be an independent module (e.g., reside on neither the contrastive net 320 nor the MLM module 330 ).
- the unsupervised loss module 350 receives the contrastive context vectors 322 and the target quantized vector token 312 .
- the unsupervised loss module 350 derives a contrastive self-supervised loss 355 , 355 a based on the corresponding contrastive context vector 322 from the contrastive net 320 and the corresponding target quantized vector token 312 generated by the quantizer 310 .
- the unsupervised loss module 350 may derive the contrastive self-supervised loss 355 a by:
- Equation 2 c represents the contrastive self-supervise loss 355 a
- q represents the target quantized vector token 312 as the positive sample
- sim(a, h) represents the exponential of the cosine similarity between a and b.
- the contrastive self-supervised loss 355 a derived by the unsupervised loss module 350 is further based on K negative samples/distractors uniformly sampled from the target quantized vector token 312 stored in the codebook 315 that correspond to masked latent speech representations 212 m from the masked subset of latent speech representations 212 m.
- the unsupervised loss module 350 also receives the target token index 324 and the high-level context vector 334 .
- the unsupervised loss module 350 determines a cross-entropy loss ( m ) 355 , 355 b by comparing the corresponding target token index 324 with the corresponding high-level context vector (i.e., target token index prediction) 334 .
- the MLM module 330 learns to predict the target token index 314 at the corresponding time step using the cross-entropy loss 355 b based on the target token index 314 generated by the quantizer 310 at the corresponding time step.
- the unsupervised loss module 350 provides the unsupervised loss 355 including the contrastive self-supervised loss 355 a and the cross-entropy loss 355 b as feedback to the ASR model 200 .
- the unsupervised loss 355 is based on the contrastive self-supervised loss 355 a and on the cross-entropy loss 355 b represented by:
- Equation 3 represents the unsupervised loss 355 and ⁇ represents a weighting parameter.
- the unsupervised loss part 300 a of the training process 300 may update parameters of the ASR model 200 based on the unsupervised loss 355 .
- the supervised loss part 300 b further includes a decoder 340 and a supervised loss module 360 .
- Each high-level context vector 334 represents a target token index prediction generated by a linear layer.
- the decoder 340 is configured to receive, as input, the high-level context vector 334 generated by the MLM module 330 at each of the plurality of time steps and predict speech recognition hypotheses 342 for the utterance.
- the decoder 340 may include a two layer 768 dimension long short-term memory (LSTM) based Recurrent Neural Network-Transducer (RNN-T) with 3072 hidden units.
- LSTM long short-term memory
- RNN-T Recurrent Neural Network-Transducer
- the decoder 340 may generate a probability distribution over possible speech recognition hypotheses 342 for the corresponding high-level context vector 334 .
- the supervised loss module 360 generates a supervised loss ( s ) 365 by comparing the probability distribution over possible speech recognition hypotheses 342 and the ground-truth transcription 306 .
- the supervised loss module 360 compares the probability distribution over possible speech recognition hypotheses 342 for a respective transcribed speech utterance 304 with the ground-truth transcription corresponding to the respective transcribed speech utterance 304 .
- the supervised loss 365 includes a RNN-T loss.
- the supervised loss part 300 b of the training process 300 may provide the supervised loss 365 as feedback to the ASR model 200 .
- the supervised loss part 300 b of the training process 300 may updated parameters of the ASR model 200 based on the supervised loss 365 .
- the training process 300 determines a total loss based on the unsupervised loss 355 and the supervised loss 365 represented by:
- the training process 300 may train the ASR model 200 using the total loss L such that the training process 300 jointly trains the ASR model 200 using the unsupervised loss 355 and the supervised loss 365 .
- jointly training the ASR model 200 with total loss eliminates the risk of the ASR model 200 forgetting the latent representations previously learned during pretraining because the training process 300 jointly (i.e., concurrently) trains the ASR model 200 using both the unsupervised loss 355 and the supervised loss 365 .
- the training process 300 eliminates the pretrained checkpoint selection because of the joint training approach. That is, the training process 300 jointly trains the ASR model 200 using the unsupervised loss 355 and the supervised loss 365 in a single-stage approach thereby eliminating the issues of pretraining the ASR model 200 using the two-stage approach.
- FIG. 4 is a flowchart of an example arrangement of operations for a method 400 of jointly training an ASR model 200 using unsupervised and supervised training.
- the method 400 may execute on data processing hardware 510 ( FIG. 5 ) using instructions stored on memory hardware 520 ( FIG. 5 ).
- the data processing hardware 510 and the memory hardware 520 may reside on the remote computer/server 201 of FIG. 1 corresponding to a computing device 500 ( FIG. 5 ).
- the method 400 includes receiving audio features 110 corresponding to one of the un-transcribed speech utterances 302 or one of the transcribed speech utterances 304 .
- the method 400 includes generating, at each of a plurality of time steps, a latent speech representation 212 based on the audio features 110 .
- the method 400 includes generating, at each of the plurality of time steps, a target quantized vector token 312 and a target token index 314 for a corresponding latent speech representation 212 .
- the target token index 314 maps the corresponding latent speech representation 212 to the target quantized vector token 312 stored in a codebook 315 .
- the method 400 includes generating, at each of the plurality of time steps, a contrastive context vector 322 for a corresponding unmasked or masked latent speech representation 212 u, 212 m.
- the method 400 includes deriving, at each of the plurality of time steps, a contrastive self-supervised loss 355 a based on the corresponding contrastive context vector 322 and the corresponding target quantized vector token 312 .
- the method 400 includes generating, at each of the plurality of time steps, a high-level context vector 334 based on the contrastive context vector 322 .
- the method 400 includes learning to predict the target token index 314 at the corresponding time step using a cross-entropy loss 355 b based on the target token index 314 .
- the method 400 includes predicting speech recognition hypotheses 342 for the utterance based on the high-level context vectors 334 .
- the method 400 includes training a multilingual ASR model 200 using an unsupervised loss 355 based on the contrastive self-supervised losses 355 a and the cross-entropy losses 355 b and a supervised loss 365 based on the predicted speech recognition hypotheses 342 and a ground-truth transcription 306 of the utterance 302 , 304 .
- FIG. 5 is schematic view of an example computing device 500 that may be used to implement the systems and methods described in this document.
- the computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 500 includes a processor 510 , memory 520 , a storage device 530 , a high-speed interface/controller 540 connecting to the memory 520 and high-speed expansion ports 550 , and a low speed interface/controller 560 connecting to a low speed bus 570 and a storage device 530 .
- Each of the components 510 , 520 , 530 , 540 , 550 , and 560 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 510 can process instructions for execution within the computing device 500 , including instructions stored in the memory 520 or on the storage device 530 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 580 coupled to high speed interface 540 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 520 stores information non-transitorily within the computing device 500 .
- the memory 520 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 520 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 500 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 530 is capable of providing mass storage for the computing device 500 .
- the storage device 530 is a computer-readable medium.
- the storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 520 , the storage device 530 , or memory on processor 510 .
- the high speed controller 540 manages bandwidth-intensive operations for the computing device 500 , while the low speed controller 560 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 540 is coupled to the memory 520 , the display 580 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 550 , which may accept various expansion cards (not shown).
- the low-speed controller 560 is coupled to the storage device 530 and a low-speed expansion port 590 .
- the low-speed expansion port 590 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 500 a or multiple times in a group of such servers 500 a, as a laptop computer 500 b, or as part of a rack server system 500 c.
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
A method includes receiving audio features and generating a latent speech representation based on the audio features. The method also includes generating a target quantized vector token and a target token index for a corresponding latent speech representation. The method also includes generating a contrastive context vector for a corresponding unmasked or masked latent speech representation and deriving a contrastive self-supervised loss based on the corresponding contrastive context vector and the corresponding target quantized vector token. The method also include generating a high-level context vector based on the contrastive context vector and, for each high-level context vector, learning to predict the target token index at the corresponding time step using a cross-entropy loss based on the target token index. The method also includes predicting speech recognition hypotheses for the utterance and training a multilingual automatic speech recognition (ASR) model using an unsupervised loss and a supervised loss.
Description
- This U.S. Patent Application claims priority under 35 U.S.C. § 119(e) to U.S. Provisional Application 63/262,174, filed on Oct. 6, 2021. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.
- This disclosure relates to joint unsupervised and supervised training for multilingual automated speech recognition (ASR).
- Automatic speech recognition (ASR), the process of taking an audio input and transcribing it into text, has greatly been an important technology that is used in mobile devices and other devices. In general, automatic speech recognition attempts to provide accurate transcriptions of what a person has said by taking an audio input (e.g., speech utterance) and transcribing the audio input into text. Modern ASR models continue to improve performance in both accuracy (e.g., a low word error rate (WER)) and latency (e.g., delay between the user speaking and the transcription) based on the ongoing development of deep neural networks. However, one challenge in developing deep learning-based ASR models is that parameters of the ASR models tend to over fit the training data, thereby resulting in the ASR models having difficulties generalizing unseen data when the training data is not extensive enough. As a result, training ASR models on larger training datasets improves the accuracy of the ASR model. Unlabeled training data and labeled training data can be incorporated to increase the volume of training data used to train the ASR models.
- One aspect of the disclosure provides a joint unsupervised and supervised. training (JUST) framework for training a multilingual automatic speech recognition (ASR) model. The JUST framework includes a feature encoder configured to receive audio features corresponding to an utterance of speech as input and generate a latent speech representation at each of a plurality of time steps. The JUST framework also includes a quantizer configured to receive, as input, the latent speech representations generated by the feature encoder at each of the plurality of time steps and generate, at each of the plurality of time steps, a target quantized vector token and a target token index for a corresponding latent speech representation generated by the feature encoder. Here, the target token index maps the corresponding latent speech representation to the target quantized vector token stored in a codebook. The JUST framework also includes a contrastive net configured to: receive, as input, the latent speech representations generated by the feature encoder at each of the plurality of time steps after masking a subset of the latent speech representations; generate, at each of the plurality of time steps, a contrastive context vector for the corresponding unmasked or masked latent speech representation; and derive, at each of the plurality of time steps, a contrastive self-supervised loss based on the corresponding contrastive context vector and the corresponding target quantized vector token generated by the quantizer for the corresponding latent speech representation. The JUST framework also includes a masked language modeling (MLM) module configured to: receive, as input, the contrastive context vector generated by the contrastive net at each of the plurality of time steps; generate, at each of the plurality of time steps, a high-level context vector; and for each high-level context vector, learn to predict the target token index at the corresponding time step using a cross-entropy loss based on the target token index generated by the quantizer at the corresponding time step. The JUST framework also includes a decoder configured to receive, as input, the high-level context vector generated by the MLM module at each of the plurality of time steps and predict speech recognition hypotheses for the utterance. Here, the JUST framework trains the multilingual ASR model on an unsupervised loss based on the cross-entropy loss and the contrastive self-supervised loss and a supervised loss based on the predicted speech recognition hypotheses and a ground-truth transcription of the utterance.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, the feature encoder includes two convolutional neural network (CNN) blocks. In some examples, masking the subset of the latent speech representations includes randomly replacing each latent speech representation in the subset of latent speech representations with a corresponding random vector. The contrastive self-supervised loss derived by the contrastive net may further be based on K negative samples/distractors uniformly sampled from the target quantized vector token stored in the codebook that correspond to masked latent representations from the masked subset of latent representations.
- In some implementations, the supervised loss is further based on an entropy-based diversity loss associated with the codebook. The multilingual ASR model is trained on training utterances spoken in a plurality of different languages. In some examples, training the multilingual ASR model includes training the multilingual ASR model having no prior pretraining. In other examples, training the multilingual ASR model includes finetuning the multilingual ASR model from a pretrained checkpoint. In some implementations, the training the multilingual ASR model includes jointly training the multilingual ASR model on the unsupervised loss and the supervised loss. The supervised loss may include a Recurrent Neural Network-Transducer (RNN-T) loss.
- Another aspect of the disclosure provides a system that includes data processing hardware and memory hardware storing instructions that when executed on the data processing hardware causes the data processing hardware to perform operations. The operations include receiving audio features that correspond to an utterance of speech and generating, at each of a plurality of time steps, a latent speech representation based on the audio features. The operations also include generating, at each of the plurality of time steps, a target quantized vector token and a target token index for a corresponding latent speech representation. The target token index maps the corresponding latent speech representation to the target quantized vector token stored in a codebook. The operations also include generating, at each of the plurality of time steps, a contrastive context vector for a corresponding unmasked or masked latent speech representation. The operations also include deriving, at each of the plurality of time steps, a contrastive self-supervised loss based on the corresponding contrastive vector and the corresponding target quantized vector token. The operations also include generating, at each of the plurality of time steps, a high-level context vector based on the contrastive context vector and, for each high-level context vector, learning to predict the target token index at the corresponding time step using a cross-entropy loss based on the target token index. The operations also include predicting speech recognition hypotheses for the utterance based on the high-level context vectors and training a multilingual automatic speech recognition (ASR) model using an unsupervised loss based on the contrastive self-supervised losses and the cross-entropy losses and a supervised loss based on the predicted speech recognition hypotheses and a ground-truth transcription of the utterance.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, generating the latent speech representation includes generating, by a feature encoder, the latent speech representation at each of the plurality of time steps. The feature encoder includes two convolutional neural network (CNN) blocks. The operations may further include masking a subset of the latent speech representations by randomly replacing each latent speech representation in the subset of latent speech representations with a corresponding random vector. In some examples, the contrastive self-supervised loss is further based on K negative samples/distracters uniformly samples from the target quantized vector token stored in the codebook that correspond to masked latent representations from a masked subset of latent representations.
- In some implementations, the unsupervised loss is further based on an entropy-based diversity loss associated with the codebook. The multilingual ASR model may be trained on training utterances spoken in a plurality of different languages. In some examples, training the multilingual ASR model includes training the multilingual ASR model having no prior pretraining. Training the multilingual ASR model may include finetuning the multilingual ASR model from a pretrained checkpoint. In some implementations, training the multilingual ASR model includes jointly training the multilingual ASR model on the unsupervised and the supervised loss. In some examples, the supervised loss includes a Recurrent Neural Network-Transducer (RNN-T) loss.
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view of an example speech recognition system. -
FIG. 2 is a schematic view of a Recurrent Neural Network-Transducer (RNN-T) model architecture. -
FIGS. 3A and 3B are schematic views of an example training process for training a speech recognition model. -
FIG. 4 is a flowchart of an example arrangement of operations for a method of jointly training an automatic speech recognition model using unsupervised and supervised training. -
FIG. 5 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- The use of pretraining automatic speech recognition (ASR) models has demonstrated an effective method for learning general latent representations from large-scale unlabeled training data. Pretraining ASR models significantly reduces the training complexity for downstream fine-tuning. Here, fine-tuning refers to performing supervised training on a pretrained ASR model using a small labeled training data set because the ASR model has already been pretrained using unlabeled training data. Thus, after an ASR model is pretrained, the pretrained ASR model may train (i.e., fine-tune train) using only a smaller and/or less diverse labeled training data set. Notably, fine-tuning the pretrained ASR model using the smaller labeled training data set still achieves similar (or better) performance than a ASR model that receives no pretraining and trains using a larger and/or more diverse set of labeled training data.
- Pretraining ASR models usually includes a two-stage approach. In a first stage, the ASR model trains using a self-supervised loss derived from unlabeled training data to learn general latent representations. Thereafter, in a second stage, the ASR model fine-tunes its training based on a supervised loss. Here, the second stage of training only requires a small set of labeled training data (i.e., audio data with corresponding labeled transcriptions) because the ASR model has already been pretrained using the unlabeled training data. This two stage training approach has proven successful for sequence modeling, but there are some issues with this approach. For instance, a pretrained model is susceptible to catastrophic forgetting. That is, the pretrained model may forget the latent representations previously learned during the first stage of training using unlabeled training data. Stated differently, training the ASR model using a supervised loss in the second stage may overwrite the latent representations learned from the first stage of training, thereby diminishing any benefit received from pretraining the ASR model in the first stage. Forgetting previously learnt latent representations is especially prevalent when the labeled training data set is large.
- Another issue with the two-stage training approach is the pretrained checkpoint selection. A pretrained checkpoint is where the pretraining (i.e., first stage) of the ASR model ends and the fine-tune training (i.e., second stage) begins. As such, the pretrained checkpoint varies based on how much pretraining the ASR model receives, In particular, the issue is determining when to stop pretraining and begin fine-tune training. Notably, performing too much pretraining can actually lead to a degradation in performance of the ASR model. On the other hand, performing too little pretraining can also lead to a degradation in performance of the ASR model. Moreover, the issue of pretrained checkpoint selection is even more severe in multilingual ASR models because the different languages of the multilingual training dataset are often imbalanced.
- Accordingly, implementations herein are directed toward training an automatic speech recognition (ASR) model using a joint unsupervised and supervised training (JUST) process. The JUST process may train ASR models from scratch (i.e., ASR models that did not receive any pretraining) or train pretrained ASR models from a pretrained checkpoint. Moreover, the JUST process may train monolingual or multilingual ASR models. As will become apparent, the JUST process trains the ASR model using an unsupervised loss derived from a cross-entropy loss and a self-supervised loss and a supervised loss derived from a predicted speech recognition hypotheses and a ground-troth transcription.
-
FIG. 1 illustrates an automated speech recognition (ASR) system 100 implementing anASR model 200 that resides on auser device 102 of auser 104 and/or on a remote computing device 201 (e.g., one or more servers of a distributed system executing in a cloud-computing; environment) in communication with theuser device 102. Although theuser device 102 is depicted as a mobile computing device (e.g., a smart phone), theuser device 102 may correspond to any type of computing device such as, without limitation, a tablet device, a laptop/desktop computer, a wearable device, a digital assistant device, a smart speaker/display, a smart appliance, an automotive infotainment system, or an Internet-of-Things (IoT) device, and is equipped withdata processing hardware 111 andmemory hardware 113. - The
user device 102 includes anaudio subsystem 108 configured to receive anutterance 106 spoken by the user 104 (e.g., theuser device 102 may include one or more microphones for recording the spoken utterance 106) and convert theutterance 106 into a corresponding digital format associated with input acoustic frames (i.e., audio features) 110 capable of being processed by the ASR system 100. In the example shown, the user speaks arespective utterance 106 in a natural language of English for the phrase “What is the weather in New York City?” and theaudio subsystem 108 converts theutterance 106 into correspondingacoustic frames 110 for input to the ASR system 100. Thereafter, theASR model 200 receives, as input, theacoustic frames 110 corresponding to theutterance 106, and generates/predicts, as output, a corresponding transcription 120 (e.g., recognition result/hypothesis) of theutterance 106. In the example shown, theuser device 102 and/or theremote computing device 201 also executes a user interface generator 107 configured to present a representation of thetranscription 120 of theutterance 106 to theuser 104 of theuser device 102. In some configurations, thetranscription 120 output from the ASR system 100 is processed, e.g., by a natural language understanding (NLU) module executing on theuser device 102 or theremote computing device 201, to execute a user command. Additionally or alternatively, a text-to-speech system (e.g., executing on any combination of theuser device 102 or the remote computing device 201) may convert the transcription into synthesized speech for audible output by another device. For instance, theoriginal utterance 106 may correspond to a message theuser 104 is sending to a friend in which thetranscription 120 is converted to synthesized speech for audible output to the friend to listen to the message conveyed in theoriginal utterance 106. - Referring to
FIG. 2 , anexample ASR model 200 may be a frame alignment-basedtransducer model 200 that includes a Recurrent Neural Network-Transducer (RNN-T) model architecture which adheres to latency constraints associated with interactive applications. The use of the RNN-T model architecture is exemplary, and the frame alignment-basedtransducer model 200 may include other architectures such as transformer-transducer, conformer-transducer, and conformer-encoder model architectures among others. The RNN-T model 200 provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the RNN-T model architecture suitable for performing speech recognition entirely on the user device 102 (e.g., no communication with a remote server is required). The RNN-T model 200 includes anencoder network 210, a prediction network 220, and ajoint network 230. Theencoder network 210, which is roughly analogous to an acoustic model (AM) in a traditional ASR system, includes a stack of self-attention layers (e.g., Conformer or Transformer layers) or a recurrent network of stacked Long Short-Term Memory (LSTM) layers. For instance, theencoder network 210 reads a sequence of d-dimensional feature vectors (e.g., acoustic frames 110 (FIG. 1 )) x=(x1, x2, . . . , xT), where xt∈ - Similarly, the prediction network 220 is also an LSTM network, which, like a language model (LM), processes the sequence of non-blank symbols output by a
final Softmax layer 240 so far, y0, . . . , yui−1, into a dense representation pui . Finally, with the RNN-T model architecture, the representations produced by the encoder and prediction/decoder networks 210, 220 are combined by thejoint network 230. The prediction network 220 may be replaced by an embedding look-up table to improve latency by outputting looked-up sparse embeddings in lieu of processing dense representations. Thejoint network 230 then predicts P(yi|xti , y0, . . . , yui−1 ), which is a distribution over the next output symbol. Stated differently, thejoint network 230 generates, at each output step (e.g., time step), a probability distribution over possible speech recognition hypotheses. Here, the “possible speech recognition hypotheses” correspond to a set of output labels each representing a symbol/character in a specified natural language. For example, when the natural language is English, the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26-letters in the English alphabet and one label designating a space. Accordingly, thejoint network 230 may output a set of values indicative of the likelihood of occurrence of each of a predetermined set of output labels. This set of values can be a vector and can indicate a probability distribution over the set of output labels. In some cases, the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited. For example, the set of output labels can include wordpieces and/or entire words, in addition to or instead of graphemes. The output distribution of thejoint network 230 can include a posterior probability value for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output yi of thejoint network 230 can include 100 different probability values, one for each output label. The probability distribution can then be used to select and assign scores to candidate orthgraphic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (by the Softmax layer 240) for determining thetranscription 120. - The
Softmax layer 240 may employ any technique to select the output label/symbol with the highest probability in the distribution as the next output symbol predicted by the RNN-T model 200 at the corresponding output step. In this manner, the RNN-T model 200 does not make a conditional independence assumption, rather the prediction of each symbol is conditioned not only on the acoustics but also on the sequence of labels output so far. The RNN-T model 200 does assume an output symbol is independent of futureacoustic frames 110, which allows the RNN-T model to be employed in a streaming fashion. - In some examples, the
encoder network 210 of the RNN-T model 200 includes a stack of self-attention layers/blocks, such as conformer blocks, Here, each conformer block includes a series of multi-headed self attention, depth wise convolution and teed-forward layers. Theencoder network 210 may include LSTM layers in lieu of self-attention layers/blocks. - The prediction network 220 may have two 2,048-dimensional LSTM layers, each of which is also followed by 640-dimensional projection layer. Alternatively, the prediction network 220 may include a stack of transformer or conformer blocks, or an embedding look-up table in lieu of LSTM layers. Finally, the
joint network 230 may also have 640 hidden units. Thesoftmax layer 240 may be composed of a unified word piece or grapheme set that is generated using all unique word pieces or graphemes in a plurality of training data sets. -
FIGS. 3A and 3B illustrate an exampleJUST training process 300 for training the ASR model 200 (FIG. 2 ). TheASR model 200 may be a multilingual ASR. model or a monolingual ASR model. In some implementations, the example JUST training process 300 (also referred to as simply “training process 300”) trains an un-trained ASR.model 200. That is, thetraining process 300 trains anASR model 200 that has not been pretrained yet. In other examples, thetraining process 300 trains apretrained ASR model 200 from a pretrained checkpoint. Here, theASR model 200 has been pretrained already and thetraining process 300 fine-tunes the pre-trained ASR model from the pretrained checkpoint. - The
training process 300 may train theASR model 200 using available training data that includes a set of un-transcribed speech utterances (i.e., unlabeled training data) 302 and a set of transcribed speech utterances (i.e., labeled training data) 304. Eachun-transcribed speech utterance 302 includes audio-only data (i.e., unpaired data) such that theun-transcribed speech utterance 302 is not paired with any corresponding transcription. On the other hand, each respective transcribed speech utterance 304 includes a corresponding ground-truth transcription 306 paired with a corresponding speech representation of the respective transcribed speech utterance 304 (i.e., paired data). Moreover, the set ofun-transcribed speech utterances 302 and the set of transcribed speech utterances 304 may each respectively include either non-synthetic speech representations, synthetic speech representations generated by a text-to-speech (TTS) system using textual utterances (not shown), or some combination thereof. In some examples, the set ofun-transcribed speech utterances 302 and the set of transcribed speech utterances 304 each include utterances spoken in a plurality of different languages for training a multilingual ASR model. - For simplicity, the
training process 300 includes anunsupervised loss part 300 a (FIG. 3A ) and a supervised loss part 300 b (FIG. 3B ). Thetraining process 300 trains theASR model 200 based on an unsupervised loss 355 (FIG. 3A ) from theunsupervised loss part 300 a and on a supervised loss 365 (FIG. 3B ) from the supervised loss part 300 b. In some examples, thetraining process 300 trains theASR model 200 jointly using the unsupervised loss 355 and thesupervised loss 365. - Continuing with
FIGS. 3A and 3B , theunsupervised loss part 300 a and the supervised loss part 300 b both include afeature encoder 311, amasking module 215, acontrastive net 320, and a masked language modeling (MLM)module 330. Here, the training process 300 (i.e.,unsupervised loss part 300 a and supervised loss part 3001) may use training data that includes the set ofun-transcribed speech utterances 302, the set of transcribed speech utterances 304, and/or some combination thereof (not shown). In other instances, theunsupervised loss part 300 a trains theASR model 200 using theun-transcribed speech utterances 302 and the supervised loss part 300 b trains theASR model 200 using the transcribed speech utterances 304. - The
feature encoder 311 is configured to receive, as input, a sequence of input audio features/vectors {xi}i=1 L (e.g., mel-frequency spectrograms such as theacoustic frames 110 ofFIG. 1 ) corresponding to one of theun-transcribed speech utterances 302 or one of the transcribed speech utterances 304 where L represents the original length of a training speech utterance. The sequence of input audio features/vectors are associated. with one of theun-transcribed speech utterances 302 or one of the transcribed speech utterances 304. After, receiving the sequence of input audio features/vectors, thefeature encoder 311 generates, as output, at each of a plurality of time steps T, a latent speech representation ({zi}i=1 T) 212. Here, thelatent speech representation 212 corresponds to a respective one of: one of theun-transcribed speech utterances 302; or one of the transcribed speech utterances 304. In some implementations, thefeature encoder 311 includes two convolutional neural network (CNN) blocks (e.g.., two convolutional layers) each having a 3×3 filter size and a stride of (2, 2). In these implementations, a first CNN block of the two CNN blocks may include 128 channels and a second CNN block of the two CNN blocks includes 32 channels. In other implementations, thefeature encoder 311 includes a subsampling block with 4× reduction in feature dimensionality and sequence length. - The
latent speech representations 212 output from thefeature encoder 311 may be fed to amasking module 215 where some of thelatent speech representations 212 are randomly chosen and replaced with a trained feature vector shared between all masked time steps to provide corresponding maskedlatent speech representations latent speech representations 212 may be replaced by random feature vectors to generate the corresponding maskedlatent speech representations 212 m. In some examples, themasking module 215 masks the randomly chosenlatent speech representations 212 by randomly sampling, without replacement, a certain proportion p of all time steps T to be start indices and then mask the subsequent M consecutive time steps from every sample index, whereby some spans may overlap. As such, themasking module 215 only masks a subset of the entire set oflatent speech representations 212 resulting in a masked subset oflatent speech representations 212 m and a subset of unmaskedlatent speech representations - The
contrastive net 320 is configured to receive, as input, thelatent speech representations 212 generated by thefeature encoder 311 at each of the plurality of time steps after masking the subset of thelatent speech representations 212 m. Stated differently, thecontrastive net 320 receives both the subset of maskedlatent speech representations 212 m and the subset of unmaskedlatent speech representations 212 u. Thereafter, thecontrastive net 320 generates, at each of the plurality of time steps, a contrastive context vector ({ci}i=1 T) 322 for the corresponding unmaskedlatent speech representation 212 u or the corresponding maskedlatent speech representation 212 m. Thecontrastive net 320 may include a stack of conformer blocks each with multi-headed self-attention, depth-wise convolution and feed-forward layers. For example, thecontrastive net 320 may include 8 conformer blocks where each conformer block includes a hidden dimensionality 1024, 8 attention heads, and a convolution kernels size of 5. - Referring now to
FIG. 3A , in some implementations theunsupervised loss part 300 a includes aquantizer 310 that also receives thelatent speech representations 212. In particular, thequantizer 310 is configured to receive, as input, thelatent speech representations 212 generated by thefeature encoder 311 at each of the plurality of time steps and generate, at each of the plurality of time steps, a target quantizedvector token 312 and a targettoken index 314 for a correspondinglatent speech representation 212 generated by thefeature encoder 311 as output. As such, thequantizer 310 generates the target quantized vector token (qi) 312 and the target token index (yi) 314 usinglatent speech representations 212 that do not include any masking. Here, thequantizer 310 generates the target quantizedvector tokens 312 according to qi∈{ej}j=1 V. Thequantizer 310 summarizes all of thelatent speech representations 212 into representative target quantized vector tokens (i.e., discriminative speech tokens) 312. The representative target quantizedvector tokens 312 generated by thequantizer 310 represent a finite set of representative target quantized vector tokens referred to as acodebook 315. Thecodebook 315 is stored at thequantizer 310 and represents the size of thecodebook 315. The targettoken index 314 maps each correspondinglatent speech representation 212 to a respective one of the target quantizedvector tokens 312 stored in thecodebook 315. Moreover, all of the representative target quantizedvector tokens 312 in thecodebook 315 are learnable during thetraining process 300. - The
unsupervised loss part 300 a is dependent upon thecodebook 315 to represent both positive and negative training examples. Accordingly, thetraining process 300 uses an entropy-based diversity losscodebook 315 to increase the use of the representative target quantizedvector tokens 312 in thecodebook 315. That is, thetraining process 300 encourages equal use of the V entries in each codebook (G) 315 thereby maximizing the entropy of the averaged softmax distribution over the codebook entries for each codebookp g across a batch of training utterances represented by: -
- In some examples, the
training process 300 uses asingle codebook 315 rather thanmultiple codebooks 315. - The
MLM module 330 is configured to receive, as input, thecontrastive context vector 322 generated by thecontrastive net 320 at each of the plurality of time steps, and generate, as output, a high-level context vector ({mi}i=1 T) 334 at each of the plurality of time steps. That is, theMLM module 330 generates the high-level context vectors 334 by extracting high-level contextualized speech representations from thecontrastive context vectors 322. Each high-level context vector 334 represents a target token index prediction generated by a linear layer. TheMLM module 330 may include a stack of 16 conformer blocks each having a hidden dimensionality of 1024, 8 attention heads, and a convolution kernel size of 5. - In some implementations, the
unsupervised loss part 300 a includes anunsupervised loss module 350 that derives an unsupervised loss 355, Theunsupervised loss module 350 may reside on the contrastive net 320 (not shown), reside on theMLM module 330, or be an independent module (e.g., reside on neither thecontrastive net 320 nor the MLM module 330). Theunsupervised loss module 350 receives thecontrastive context vectors 322 and the target quantizedvector token 312. For each respectivelatent speech representation 212, theunsupervised loss module 350 derives a contrastive self-supervised loss 355, 355 a based on the correspondingcontrastive context vector 322 from thecontrastive net 320 and the corresponding target quantizedvector token 312 generated by thequantizer 310. Theunsupervised loss module 350 may derive the contrastive self-supervised loss 355 a by: -
- In Equation 2,
vector token 312 as the positive sample, and {{tilde over (q)}i}i=1 K represents K negative samples/distracters uniformly sampled from target quantized vector tokens (qj) of other maskedlatent speech representations 212 m in the same utterance. Moreover, in Equation 2 sim(a, h) represents the exponential of the cosine similarity between a and b. Accordingly, the contrastive self-supervised loss 355 a derived by theunsupervised loss module 350 is further based on K negative samples/distractors uniformly sampled from the target quantizedvector token 312 stored in thecodebook 315 that correspond to maskedlatent speech representations 212 m from the masked subset oflatent speech representations 212 m. - The
unsupervised loss module 350 also receives the target token index 324 and the high-level context vector 334. Here, for each respectivelatent speech representation 212, theunsupervised loss module 350 determines a cross-entropy loss (level context vector 334, theMLM module 330 learns to predict the targettoken index 314 at the corresponding time step using the cross-entropy loss 355 b based on the targettoken index 314 generated by thequantizer 310 at the corresponding time step. - The unsupervised loss module 350 provides the unsupervised loss 355 including the contrastive self-supervised loss 355 a and the cross-entropy loss 355 b as feedback to the ASR model 200. The unsupervised loss 355 is based on the contrastive self-supervised loss 355 a and on the cross-entropy loss 355 b represented by:
-
- Referring now to
FIG. 3B , in some implementations the supervised loss part 300 b further includes adecoder 340 and asupervised loss module 360. TheMLM module 330 is configured to receive, as input, thecontrastive context vector 322 generated by thecontrastive net 320 at each of the plurality of time steps, and generate, as output, a high-level context vector ({mi}i=1 T) 334 at each of the plurality of time steps. That is, theMLM module 330 generates the high-level context vectors 334 by extracting high-level contextualized speech representations from thecontrastive context vectors 322. Each high-level context vector 334 represents a target token index prediction generated by a linear layer. - The
decoder 340 is configured to receive, as input, the high-level context vector 334 generated by theMLM module 330 at each of the plurality of time steps and predictspeech recognition hypotheses 342 for the utterance. Thedecoder 340 may include a two layer 768 dimension long short-term memory (LSTM) based Recurrent Neural Network-Transducer (RNN-T) with 3072 hidden units. Here, thedecoder 340 may generate a probability distribution over possiblespeech recognition hypotheses 342 for the corresponding high-level context vector 334. Thesupervised loss module 360 generates a supervised loss (speech recognition hypotheses 342 and the ground-truth transcription 306. That is, thesupervised loss module 360 compares the probability distribution over possiblespeech recognition hypotheses 342 for a respective transcribed speech utterance 304 with the ground-truth transcription corresponding to the respective transcribed speech utterance 304. In some examples, thesupervised loss 365 includes a RNN-T loss. The supervised loss part 300 b of thetraining process 300 may provide thesupervised loss 365 as feedback to theASR model 200. Thus, the supervised loss part 300 b of thetraining process 300 may updated parameters of theASR model 200 based on thesupervised loss 365. - Referring back to
FIGS. 3A and 3B , in some implementations the training process 300 determines a total loss based on the unsupervised loss 355 and the supervised loss 365 represented by: - In Equation 4,
training process 300 may train theASR model 200 using the total loss L such that thetraining process 300 jointly trains theASR model 200 using the unsupervised loss 355 and thesupervised loss 365. Notably, jointly training theASR model 200 with total lossASR model 200 forgetting the latent representations previously learned during pretraining because thetraining process 300 jointly (i.e., concurrently) trains theASR model 200 using both the unsupervised loss 355 and thesupervised loss 365. Moreover, thetraining process 300 eliminates the pretrained checkpoint selection because of the joint training approach. That is, thetraining process 300 jointly trains theASR model 200 using the unsupervised loss 355 and thesupervised loss 365 in a single-stage approach thereby eliminating the issues of pretraining theASR model 200 using the two-stage approach. -
FIG. 4 is a flowchart of an example arrangement of operations for amethod 400 of jointly training anASR model 200 using unsupervised and supervised training. Themethod 400 may execute on data processing hardware 510 (FIG. 5 ) using instructions stored on memory hardware 520 (FIG. 5 ). Thedata processing hardware 510 and thememory hardware 520 may reside on the remote computer/server 201 ofFIG. 1 corresponding to a computing device 500 (FIG. 5 ). - At
operation 402, themethod 400 includes receiving audio features 110 corresponding to one of theun-transcribed speech utterances 302 or one of the transcribed speech utterances 304. Atoperation 404, themethod 400 includes generating, at each of a plurality of time steps, alatent speech representation 212 based on the audio features 110, Atoperation 406, themethod 400 includes generating, at each of the plurality of time steps, a target quantizedvector token 312 and a targettoken index 314 for a correspondinglatent speech representation 212. Here, the targettoken index 314 maps the correspondinglatent speech representation 212 to the target quantizedvector token 312 stored in acodebook 315. Atoperation 408, themethod 400 includes generating, at each of the plurality of time steps, acontrastive context vector 322 for a corresponding unmasked or maskedlatent speech representation operation 410, themethod 400 includes deriving, at each of the plurality of time steps, a contrastive self-supervised loss 355 a based on the correspondingcontrastive context vector 322 and the corresponding target quantizedvector token 312. - At
operation 412, themethod 400 includes generating, at each of the plurality of time steps, a high-level context vector 334 based on thecontrastive context vector 322. Atoperation 414, for each high-level context vector 334, themethod 400 includes learning to predict the targettoken index 314 at the corresponding time step using a cross-entropy loss 355 b based on the targettoken index 314. Atoperation 416, themethod 400 includes predictingspeech recognition hypotheses 342 for the utterance based on the high-level context vectors 334. Atoperation 416, themethod 400 includes training amultilingual ASR model 200 using an unsupervised loss 355 based on the contrastive self-supervised losses 355 a and the cross-entropy losses 355 b and asupervised loss 365 based on the predictedspeech recognition hypotheses 342 and a ground-truth transcription 306 of theutterance 302, 304. -
FIG. 5 is schematic view of anexample computing device 500 that may be used to implement the systems and methods described in this document. Thecomputing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 500 includes aprocessor 510,memory 520, astorage device 530, a high-speed interface/controller 540 connecting to thememory 520 and high-speed expansion ports 550, and a low speed interface/controller 560 connecting to a low speed bus 570 and astorage device 530. Each of thecomponents processor 510 can process instructions for execution within thecomputing device 500, including instructions stored in thememory 520 or on thestorage device 530 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 580 coupled tohigh speed interface 540. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 520 stores information non-transitorily within thecomputing device 500. Thememory 520 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 520 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 500. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 530 is capable of providing mass storage for thecomputing device 500. In some implementations, thestorage device 530 is a computer-readable medium. In various different implementations, thestorage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 520, thestorage device 530, or memory onprocessor 510. - The
high speed controller 540 manages bandwidth-intensive operations for thecomputing device 500, while thelow speed controller 560 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 540 is coupled to thememory 520, the display 580 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 550, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 560 is coupled to thestorage device 530 and a low-speed expansion port 590. The low-speed expansion port 590, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 500 a or multiple times in a group ofsuch servers 500 a, as alaptop computer 500 b, or as part of arack server system 500 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (20)
1. A joint unsupervised and supervised training (JUST) framework for training a multilingual automatic speech recognition (ASR) model, the JUST framework comprising:
a feature encoder configured to:
receive, as input, audio features corresponding to an utterance of speech; and
generate, at each of a plurality of time steps, a latent speech representation;
a quantizer configured to:
receive, as input, the latent speech representations generated by the feature encoder at each of the plurality of time steps; and
generate, at each of the plurality of time steps, a target quantized vector token and a target token index for a corresponding latent speech representation generated by the feature encoder, wherein the target token index maps the corresponding latent speech representation to the target quantized vector token stored in a codebook;
a contrastive net configured to:
receive, as input, the latent speech representations generated by the feature encoder at each of the plurality of time steps after masking a subset of the latent speech representations;
generate, at each of the plurality of time steps, a contrastive context vector for the corresponding unmasked or masked latent speech representation; and
derive, at each of the plurality of time steps, a contrastive self-supervised loss based on the corresponding contrastive context vector and the corresponding target quantized vector token generated by the quantizer for the corresponding latent speech representation;
a masked language modeling (MLM) module configured to:
receive, as input, the contrastive context vector generated by the contrastive net at each of the plurality of time steps;
generate, at each of the plurality of time steps, a high-level context vector; and
for each high-level context vector, learn to predict the target token index at the corresponding time step using a cross-entropy loss based on the target token index generated by the quantizer at the corresponding time step; and
a decoder configured to:
receive, as input, the high-level context vector generated by the MLM module at each of the plurality of time steps; and
predict speech recognition hypotheses for the utterance,
wherein the multilingual ASR model is trained on:
an unsupervised loss based on the contrastive self-supervised loss and the cross-entropy; and
a supervised loss based on the predicted speech recognition hypotheses and a ground-truth transcription of the utterance.
2. The JUST framework of claim 1 , wherein the feature encoder comprises two convolutional neural network (CNN) blocks.
3. The JUST framework of claim 1 , wherein masking the subset of the latent speech representations comprises randomly replacing each latent speech representation in the subset of latent speech representations with a corresponding random vector.
4. The JUST framework of claim 1 , wherein the contrastive self-supervised loss derived by the contrastive net is further based on K negative samples/distractors uniformly sampled from the target quantized vector token stored in the codebook that correspond to masked latent representations from the masked subset of latent representations.
5. The JUST framework of claim 1 , wherein the unsupervised loss is further based. on an entropy-based diversity loss associated with the codebook.
6. The JUST framework of claim 1 , wherein the multilingual ASR model is trained on training utterances spoken in a plurality of different languages.
7. The JUST framework of claim 1 , wherein training the multilingual ASR model comprises training the multilingual ASR model having no prior pretraining.
8. The JUST framework of claim 1 , wherein training the multilingual ASR model comprises fine-tuning the multilingual ASR model from a pretrained checkpoint.
9. The JUST framework of claim 1 , wherein training the multilingual ASR model comprises jointly training the multilingual ASR model on the unsupervised loss and the supervised loss.
10. The JUST framework of claim 1 , wherein the supervised loss comprises a Recurrent Neural Network-Transducer (RNN-T) loss.
11. A computer-implemented method when executed by data processing hardware causes the data processing hardware to perform operations comprising:
receiving audio features corresponding to an utterance of speech;
generating, at each of a plurality of time steps, a latent speech representation based on the audio features;
generating, at each of the plurality of time steps, a target quantized vector token and a target token index for a corresponding latent speech representation, wherein the target token index maps the corresponding latent speech representation to the target quantized vector token stored in a codebook;
generating, at each of the plurality of time steps, a contrastive context vector for a corresponding unmasked or masked latent speech representation;
deriving, at each of the plurality of time steps, a contrastive self-supervised loss based on the corresponding contrastive context vector and the corresponding target quantized vector token;
generating, at each of the plurality of time steps, a high-level context vector based on the contrastive context vector;
for each high-level context vector, learning to predict the target token index at the corresponding time step using a cross-entropy loss based on the target token index;
predicting speech recognition hypotheses for the utterance based on the high-level context vectors; and
training a multilingual automatic speech recognition (ASR) model using an unsupervised loss based on the contrastive self-supervised losses and the cross-entropy losses and a supervised loss based on the predicted speech recognition hypotheses and a ground-truth transcription of the utterance.
12. The computer-implemented method of claim 11 , wherein generating the latent speech representation comprises generating, by a feature encoder, the latent speech representation at each of the plurality of time steps, the feature encoder comprising two convolutional neural network (CNN) blocks.
13. The computer-implemented method of claim 11 , wherein the operations further comprise masking a subset of the latent speech representations by randomly replacing each latent speech representation in the subset of latent speech representations with a corresponding random vector.
14. The computer-implemented method of claim 11 , wherein the contrastive self-supervised loss is further based on K negative samples/distractors uniformly sampled from the target quantized vector token stored in the codebook that correspond to masked latent representations from a masked subset of latent representations.
15. The computer-implemented method of claim 11 , wherein the unsupervised loss is further based on an entropy-based diversity loss associated with the codebook.
16. The computer-implemented method of claim 11 , wherein the multilingual ASR model is trained on training utterances spoken in a plurality of different languages.
17. The computer-implemented method of claim 11 , wherein training the multilingual ASR model comprises training the multilingual ASR model having no prior pretraining.
18. The computer-implemented method of claim 11 , wherein training the multilingual ASR model comprises fine-tuning the multilingual ASR model from a pretrained checkpoint.
19. The computer-implemented method of claim 11 , wherein training the multilingual ASR model comprises jointly training the multilingual ASR model on the unsupervised loss and the supervised loss.
20. The computer-implemented method of claim 11 , wherein the supervised loss comprises a Recurrent Neural Network-Transducer (RNN-T) loss.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/929,934 US20230104228A1 (en) | 2021-10-06 | 2022-09-06 | Joint Unsupervised and Supervised Training for Multilingual ASR |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163262174P | 2021-10-06 | 2021-10-06 | |
US17/929,934 US20230104228A1 (en) | 2021-10-06 | 2022-09-06 | Joint Unsupervised and Supervised Training for Multilingual ASR |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230104228A1 true US20230104228A1 (en) | 2023-04-06 |
Family
ID=83447762
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/929,934 Pending US20230104228A1 (en) | 2021-10-06 | 2022-09-06 | Joint Unsupervised and Supervised Training for Multilingual ASR |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230104228A1 (en) |
WO (1) | WO2023059969A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230154188A1 (en) * | 2021-11-16 | 2023-05-18 | Salesforce.Com, Inc. | Systems and methods for video and language pre-training |
CN116434759A (en) * | 2023-04-11 | 2023-07-14 | 兰州交通大学 | Speaker identification method based on SRS-CL network |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11238845B2 (en) * | 2018-11-21 | 2022-02-01 | Google Llc | Multi-dialect and multilingual speech recognition |
WO2020242580A1 (en) * | 2019-05-28 | 2020-12-03 | Google Llc | Large-scale multilingual speech recognition with a streaming end-to-end model |
-
2022
- 2022-09-06 US US17/929,934 patent/US20230104228A1/en active Pending
- 2022-09-06 WO PCT/US2022/075998 patent/WO2023059969A1/en active Application Filing
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230154188A1 (en) * | 2021-11-16 | 2023-05-18 | Salesforce.Com, Inc. | Systems and methods for video and language pre-training |
CN116434759A (en) * | 2023-04-11 | 2023-07-14 | 兰州交通大学 | Speaker identification method based on SRS-CL network |
Also Published As
Publication number | Publication date |
---|---|
WO2023059969A1 (en) | 2023-04-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN113692616B (en) | Phoneme-based contextualization for cross-language speech recognition in an end-to-end model | |
US11929060B2 (en) | Consistency prediction on streaming sequence models | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
US11610586B2 (en) | Learning word-level confidence for subword end-to-end automatic speech recognition | |
US20230377564A1 (en) | Proper noun recognition in end-to-end speech recognition | |
US11961515B2 (en) | Contrastive Siamese network for semi-supervised speech recognition | |
US20220310065A1 (en) | Supervised and Unsupervised Training with Contrastive Loss Over Sequences | |
US20220310080A1 (en) | Multi-Task Learning for End-To-End Automated Speech Recognition Confidence and Deletion Estimation | |
US20230317059A1 (en) | Alignment Prediction to Inject Text into Automatic Speech Recognition Training | |
US20220310067A1 (en) | Lookup-Table Recurrent Language Model | |
US20220310097A1 (en) | Reducing Streaming ASR Model Delay With Self Alignment | |
US11823697B2 (en) | Improving speech recognition with speech synthesis-based model adapation | |
US20230107475A1 (en) | Exploring Heterogeneous Characteristics of Layers In ASR Models For More Efficient Training | |
US20240029715A1 (en) | Using Aligned Text and Speech Representations to Train Automatic Speech Recognition Models without Transcribed Speech Data | |
US20240153484A1 (en) | Massive multilingual speech-text joint semi-supervised learning for text-to-speech | |
US20240013777A1 (en) | Unsupervised Data Selection via Discrete Speech Representation for Automatic Speech Recognition | |
US20230103722A1 (en) | Guided Data Selection for Masked Speech Modeling | |
US20230017892A1 (en) | Injecting Text in Self-Supervised Speech Pre-training | |
US20230298591A1 (en) | Optimizing Personal VAD for On-Device Speech Recognition | |
US20230107493A1 (en) | Predicting Word Boundaries for On-Device Batching of End-To-End Speech Recognition Models | |
US20220310081A1 (en) | Multilingual Re-Scoring Models for Automatic Speech Recognition | |
US20240153495A1 (en) | Multi-Output Decoders for Multi-Task Learning of ASR and Auxiliary Tasks | |
US20240029720A1 (en) | Context-aware Neural Confidence Estimation for Rare Word Speech Recognition | |
US20230298565A1 (en) | Using Non-Parallel Voice Conversion for Speech Conversion Models | |
US20230013587A1 (en) | Advancing the Use of Text and Speech in ASR Pretraining With Consistency and Contrastive Losses |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LI, BO;BAI, JUNWEN;ZHANG, YU;AND OTHERS;REEL/FRAME:061107/0058Effective date: 20220906 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |