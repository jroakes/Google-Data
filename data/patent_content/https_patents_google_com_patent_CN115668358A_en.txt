CN115668358A - Method and system for user interface adaptation for text-to-speech synthesis - Google Patents
Method and system for user interface adaptation for text-to-speech synthesis Download PDFInfo
- Publication number
- CN115668358A CN115668358A CN202080101685.2A CN202080101685A CN115668358A CN 115668358 A CN115668358 A CN 115668358A CN 202080101685 A CN202080101685 A CN 202080101685A CN 115668358 A CN115668358 A CN 115668358A
- Authority
- CN
- China
- Prior art keywords
- speech
- text
- identified portion
- rate
- synthesized speech
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/033—Voice editing, e.g. manipulating the voice of the synthesiser
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/06—Elementary speech units used in speech synthesisers; Concatenation rules
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
- G10L13/10—Prosody rules derived from text; Stress or intonation
- G10L2013/105—Duration
Abstract
A method and system for adapting speech synthesis based on user interface input is disclosed. In synthesizing speech from a text passage using a text-to-speech (TTS) system and simultaneously displaying the text passage in a display device, the system can receive a tracking action input that tracks a portion of the text undergoing synthesis and identifies a contextual portion of the text for which previously synthesized speech has been synthesized at a standard speech rate. The tracking information may be used to adjust a speech rate of the TTS synthesis of the portion from a standard speech rate to an adapted speech rate, and the speech characteristics of the synthesized speech of the portion may be adapted by applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion to the TTS synthesis process of the portion. The identified portion of synthesized speech may be output at the adapted speech rate and the adapted speech characteristic.
Description
Background
Unless otherwise indicated herein, the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.
The goal of Automatic Speech Recognition (ASR) technology is to map a particular utterance or speech sample to an accurate textual or other symbolic representation of that utterance. For example, an ASR performed on the utterance "my dog has fleas" would ideally map to the text string "my dog has fleas" rather than the meaningless text string "my dog has frozen", or the reasonable but inaccurate text string "my marshy has trees".
The goal of speech synthesis technology is to convert written language into speech that can be output in an audio format, e.g., directly for audio output or stored as an audio file suitable for audio output. Such speech synthesis may be performed by a text-to-speech (TTS) system. The written language may take the form of a textual or symbolic language representation. Speech may be generated as waveforms by a speech synthesizer that produces artificial human speech. Human speech that sounds natural may also be the target of speech synthesis systems.
Various technologies including computers, web servers, telephones, and Personal Digital Assistants (PDAs) may be used to implement ASR systems and/or speech synthesis systems, or one or more components of such systems. The communication network may in turn provide communication paths and links between some or all of such devices, support speech synthesis system capabilities and services that may utilize ASR and/or speech synthesis system capabilities.
Disclosure of Invention
In one aspect, example embodiments presented herein provide a method comprising: while synthesizing speech from a text segment using a text-to-speech (TTS) system and while displaying the text segment in a display device, receiving input indicative of a position and motion of a tracking operation relative to the text segment displayed in the display device; using the indicated location of the tracking operation to identify both a portion of the text passage undergoing the TTS synthesis process near the time the tracking operation input was received and a contextual portion for which the TTS system has synthesized a text passage of previously synthesized speech at a standard (canonical) speech rate, wherein the contextual portion includes text immediately preceding and immediately following the identified portion; adjusting a speech rate of the TTS synthesis of the identified portion from a standard speech rate to an adapted speech rate determined based on the indicated motion using the indicated motion of the tracking operation; adapting the speech characteristics of the synthesized speech of the identified portion by applying the adjusted speech rate and synthesized speech characteristics of the previously synthesized speech of the contextual portion to a TTS synthesis process of the identified portion; and outputting the synthesized speech of the identified portion at the adapted speech rate and the adapted speech characteristic. In some embodiments, the portion of text undergoing the TTS synthesis process may optionally be identified at a time proximate to receiving the tracking operation input or at a location proximate to the indicated location. The tracking operation may indicate or include a change in an indicator of the location of the (user) input or potential input, and may be associated with a visual indicator (although not necessarily). For example, when the user input is a touch input (or stylus input), the tracking operation may indicate a change in the touch input; the touch input may not necessarily be rendered on the display device, but there may still be a visual indicator (in the form of a finger or stylus) to indicate the location of the user input for the tracking operation. The indicator may be a movable visual indicator, such as a rendered pointer, when the user enters via a physical input device, such as a mouse or keyboard. In the following description, indicators associated with tracking operations may be referred to by the terms virtual pointing indicator and/or "cursor," which terms encompass both visual (rendered and physical) and other non-visual indicators of the location of user input or potential user input; the position and motion of the tracking operation may then be received as part of a virtual pointing indicator input or "cursor input" for use in the methods described herein. For example, as a user moves their finger (along a touchscreen or in front of a gesture-based interface) or moves a mouse, the position and motion of the indicator relative to a segment of text displayed on the display device may be provided as input (such as a virtual pointing indicator or cursor input) indicative of a tracking operation.
In another aspect, example embodiments presented herein provide a system comprising a text-to-speech (TTS) system, the system comprising: one or more processors; a memory; and machine-readable instructions stored in the memory, which when executed by the one or more processors, cause the system to perform operations comprising: while synthesizing speech from a text segment using a TTS system and simultaneously displaying the text segment in a display device, receiving input indicative of a position and a motion of a tracking operation relative to the text segment displayed in the display device; using the indicated location of the tracking operation to identify both a portion of the text passage undergoing the TTS synthesis process near the time the tracking operation input was received and a contextual portion for which the TTS system has synthesized a text passage of previously synthesized speech at a standard speech rate, wherein the contextual portion includes text immediately preceding and immediately following the identified portion; adjusting a speech rate of the TTS synthesis of the identified portion from a standard speech rate to an adapted speech rate determined based on the indicated motion using the indicated motion of the tracking operation; adapting the speech characteristics of the synthesized speech of the identified portion by applying the adapted speech rate and synthesized speech characteristics of the previously synthesized speech of the contextual portion to a TTS synthesis process of the identified portion; and outputting the synthesized speech of the identified portion at the adapted speech rate and the adapted speech characteristic. In some embodiments, the portion of text undergoing the TTS synthesis process may optionally be identified at a time proximate to receiving the tracking operation input or at a location proximate to the indicated location. The system may be configured to perform any of the methods described or claimed herein.
In yet another aspect, example embodiments presented herein provide an article of manufacture comprising a computer-readable storage medium having stored thereon program instructions that, when executed by one or more processors of a system comprising a text-to-speech (TTS) system, cause the system to perform operations comprising: while synthesizing speech from a text segment using a TTS system and simultaneously displaying the text segment in a display device, receiving input indicative of a position and a motion of a tracking operation relative to the text segment displayed in the display device; using the indicated location of the tracking operation to identify both a portion of the text passage undergoing the TTS synthesis process near the time the tracking operation input was received and a contextual portion of the text passage for which the TTS system has synthesized previously synthesized speech at a standard speech rate, wherein the contextual portion includes text immediately preceding and immediately following the identified portion; adjusting a speech rate of the TTS synthesis of the identified portion from a standard speech rate to an adapted speech rate determined based on the indicated motion using the indicated motion of the tracking operation; adapting speech characteristics of the synthesized speech of the identified portion by applying the adapted speech rate and synthesized speech features of the previously synthesized speech of the contextual portion to a TTS synthesis process of the identified portion; and outputting the synthesized speech of the identified portion at the adapted speech rate and the adapted speech characteristic. In some embodiments, optionally, the portion of the text undergoing the TTS synthesis process is identified proximate in time when the tracking operation input is received or proximate in position to the indicated location. In some embodiments, the portion of text undergoing the TTS synthesis process may optionally be identified at a time proximate to receiving the tracking operation input or at a location proximate to the indicated location. A computer program may also be provided that comprises instructions which, when executed by one or more processors, cause the one or more processors to carry out any of the methods described or claimed herein.
These and other aspects, advantages, and alternatives will become apparent to one of ordinary skill in the art by reading the following detailed description, with appropriate reference to the accompanying drawings. Further, it is to be understood that this summary and other descriptions and drawings provided herein are intended to illustrate embodiments by way of example only and that, as such, many variations are possible. For example, structural elements and process steps may be rearranged, combined, distributed, eliminated, or otherwise altered while remaining within the scope of the claimed embodiments.
Drawings
FIG. 1 depicts a simplified block diagram of an example text-to-speech system according to an example embodiment.
Fig. 2 is a block diagram of an example network and computing architecture, according to an example embodiment.
Fig. 3A is a block diagram of a server device according to an example embodiment.
Fig. 3B depicts a cloud-based server system, according to an example embodiment.
Fig. 4 depicts a block diagram of a client device, according to an example embodiment.
FIG. 5 depicts an example process flow for text-to-speech synthesis according to an example embodiment.
Fig. 6 illustrates an exemplary waveform and pitch profile (profile) of a voice according to an exemplary embodiment.
FIG. 7 is a conceptual illustration of an example text-to-speech application with user interface adaptation according to an example embodiment.
FIG. 8 depicts an example process flow for text-to-speech synthesis with user interface adaptation according to an example embodiment.
Fig. 9 is a flowchart illustrating an example method according to an example embodiment.
Detailed Description
1. Overview
The speech synthesis system may be a processor-based system configured to convert written language into artificially generated speech or spoken speech. The written language may be written text, such as one or more written sentences or text strings. The written language may also take the form of other symbolic representations, such as a speech synthesis markup language, which may include information indicating the mood of the speaker, the gender of the speaker, the identity of the speaker, and the style of speaking. The source of written text may be entered from a keyboard or keypad of a computing device, such as a portable computing device (e.g., PDA, smart phone, etc.), or may be from a file stored on one form or another of computer-readable storage medium. The artificially generated speech may be generated as a waveform from a signal generation device or module (e.g., a speech synthesizer device) and output and/or formatted by an audio playback device and recorded as an audio file on a tangible recording medium. Such systems may also be referred to as "text-to-speech" (TTS) systems, although written forms are not necessarily limited to text only.
The speech synthesis system may operate by receiving input text (or other form of written language) and translating the written text into a "phonetic transcription" corresponding to a symbolic representation of how the spoken presentation of the text sounds or should sound. The phonetic transcription can then be mapped to speech features that parameterize the acoustic rendering of the phonetic transcription, and which are then used as input data to a signal generation module device or element that can produce an audio waveform suitable for playback by an audio output device. For example, a human voice (voice) is played that may sound like a word (or sound) speaking an input text string. In the case of speech synthesis, the more natural the sound (e.g., to the human ear) of the synthesized speech, the better the speech quality ranking of the system in general. In some cases, a more natural sound may also reduce computing resources, as subsequent communication with the user to clarify the output meaning may be reduced. The audio waveform may also be generated as an audio file that may be stored or recorded on a storage medium suitable for subsequent playback.
In operation, the TTS system may be used to communicate information from a device (e.g., a processor-based device or system) to a user, such as messages, prompts, answers to questions, instructions, news, email, and speech-to-speech translations. The speech signal itself may carry various forms or types of information including linguistic content, emotional states (e.g., mood and/or mood), physical states (e.g., physical voice characteristics), and speaker identity, among others.
In an example embodiment, speech synthesis may use parametric representations of speech with symbolic descriptions of speech and linguistic content of text. A TTS system may be trained using data consisting essentially of a large number of speech samples and corresponding text strings (or other symbol renderings). For practical reasons, speech samples are usually recorded, although in principle this is not required. By construction, the corresponding text string is, or generally conforms to, the written storage format. Thus, the recorded speech samples and their corresponding text strings may constitute training data for a TTS system.
One example of TTS is based on Hidden Markov Models (HMMs). In this approach, the HMM is used to model the following statistical probabilities: the statistical probability associates a phonetic transcription of the input text string with a parametric representation of the corresponding speech to be synthesized. As another example, TTS may generate a parametric representation of speech based on some form of machine learning to synthesize the speech. For example, speech parameters are generated using an Artificial Neural Network (ANN) by training the ANN to associate known phonetic transcriptions with known parametric representations of speech sounds. Both HMM-based speech synthesis and ANN-based speech synthesis can facilitate changing or adjusting the characteristics of synthesized speech using one or another form of statistical adaptation. Other forms of TTS systems are possible.
One area of TTS and audio applications of interest is related to human learning. In particular, it is known that some people tend to be "visual learners" while others prefer auditory patterns to absorb information. The advent and popularity of audio books is an example of a market that serves those who like to listen rather than read. TTS may also be used as a user that is compliant with reduced literacy or a user with disabilities such as reading disabilities.
The inventors have realized that converting text to audio by TTS presents a number of usability related challenges, especially when applied in the human learning context. Two particular usability problems relate to reduced attention due to human physiological factors and linearity. In short, some users who consume text content audibly through TTS report a decrease in attention as compared to reading. This seems to be because listening may be considered a more passive form of participation than reading, requiring a reduction in user interaction with the tone generator once it is activated. Traditional TTS-based audio systems do not incorporate interactivity into TTS processing and synthesis by design. In contrast, for example, conventional systems may provide a simple start-stop-volume-slide (scrub) control.
Linearity describes the generally straightforward "read-through" User Interface (UI) imposed by traditional TTS audio. Conventional TTS systems provide limited ability to perform equivalent "rereading". While a user may be able to slide (or jump) back and forth through an audio work (e.g., slide left on a touch screen system to back 10 seconds), the granularity of control in conventional TTS systems typically does not match the fine-grained, natural experience they have while reading; while reading, they may re-read and re-parse a complex portion of a sentence, either back by one phrase, or re-read a sentence to help better understand it.
To the extent that conventional TTS systems have any degree of capability to provide UI control over TTS, these capabilities are typically far from generating natural sounding speech, in pitch and pace (pace), among other perceptible audio features. For example, a human recorded text-to-speech provided recording may not typically be paced without pitch distortion (e.g., deepen speech pitch for slower paces, and vice versa). And conventional TTS synthesis systems similarly lack UI controls integrated with speech synthesis algorithms that synthesize speech from text, which typically results in speech that sounds unnatural.
To address these and other challenges, the inventors have devised techniques for adapting speech synthesis that overcome the above-referenced problems, as well as others, and thereby provide improved reading and learning understanding for users consuming text through a text-to-speech interface. Accordingly, example embodiments described herein provide systems and methods that employ techniques such as machine learning to deliver effective UI adaptation of synthesized speech and an enhanced audio plus text experience to a user.
According to an example embodiment, the position and motion of an indicator, such as a cursor or other visually rendered pointer, that tracks text displayed in a display device may be used as input to a TTS system that synthesizes speech from the displayed text. In some embodiments, the indicator or tracking indicator may not necessarily have a visually rendered icon or graphic, although it still tracks the displayed text. More specifically, when a portion or segment of displayed text is being processed by the TTS system and synthesized as speech, the "tracking operation" may provide as input to the TTS system the position and movement of the indicator relative to the portion or segment. The tracking operation may be a user tracking operation. The tracking operation may take various forms. Non-limiting examples include: a cursor or pointer graphically rendered under interactive control of a computer mouse device or other interactive tracking device; a finger of a user providing a tactile input when moving on the touch screen device; a virtual pointer crossing or traversing the displayed text along a preprogrammed path at a preprogrammed pace; and input indicative of a virtual projection of the user's eye gaze direction and motion onto the displayed text derived from an eye tracking apparatus, such as an eye tracking camera on interactive virtual and/or augmented reality glasses (or other interactive head-mounted display device). The input position and motion can be integrated into the speech synthesis process in a manner that accommodates feedback of the synthesis process so as to cause the generation of synthesized speech that both fits the tracking information and is faithful to the speech and prosodic context of the tracked text rendered as synthesized speech. Thus, the generated or synthesized speech may be mapped to the user's input, while improving the match between the user's intended output and actual output and preventing user's misconvergence. Thus, the audio output generated as described herein may be provided to the user in a manner that improves the physiological response of the user.
2. Example text-to-Speech System
A TTS synthesis system (or more generally, a speech synthesis system) may operate by: the method includes receiving input text, processing the text into a symbolic representation of phonetic and linguistic content of a text string, generating a sequence of speech features corresponding to the symbolic representation, and providing the speech features as input to a speech synthesizer to produce a spoken representation of the input text. The symbolic representation of the phonetic and linguistic content of the text may take the form of a sequence of tags, each tag identifying a phonetic speech unit (such as a phoneme) and further identifying or encoding a linguistic and/or syntactic context, a temporal parameter, and other information for specifying how to render the sound of the symbolic representation into meaningful speech of a given language. Other speech characteristics may include pitch, frequency, pace, and intonation (e.g., statement tone, question tone, etc.). At least some of these features are sometimes referred to as "prosody".
According to an example embodiment, the phonetic units of the phonetic transcription may be phonemes. A phoneme can be considered a minimal acoustic segment of speech in a given language that contains meaningful contrasts with other speech segments in the given language. Thus, a word typically includes one or more phonemes. For simplicity, phonemes can be considered as the pronunciation of letters, although this is not a perfect analogy, as some phonemes may represent multiple letters. In written form, a phoneme is typically represented as one or more letters or symbols within some type of separator that represent the text that represents the phoneme. For example, the phone spelling of the American English pronunciation of the word "cat" is/k// ae// t/, and consists of the phones/k/,/ae/and/t/. Another example is that the phoneme spelling of the word "dog" is/d// aw// g/, and consists of the phonemes/d/,/aw/and/g/. Different phonemic letters exist and other phonemic representations are possible. The phonemic alphabet commonly used in american english contains about 40 different phones. Other languages may be described by different phone letters containing different phones.
The phonetic properties of the phonemes in an utterance may depend on or be influenced by the context in which the utterance was spoken (or intended to be spoken). For example, a "triphone" is a triplet of phonemes in which the spoken rendering of a given phoneme is formed from a temporally preceding phoneme (referred to as the "left context") and a temporally following phoneme (referred to as the "right context"). Thus, the phoneme order of the english triphone corresponds to the direction of english reading. Other phone contexts, such as pentaphone, are also contemplated.
The speech features represent acoustic properties of speech as parameters and, in the context of speech synthesis, can be used to drive the generation of a synthesized waveform corresponding to the output speech signal. In general, the features of speech synthesis take into account the three main components of the speech signal, namely, the spectral envelope resembling vocal tract effects, the excitation of simulated glottal sources, and the prosody describing pitch contour ("melody") and tempo (rhythm) as described above. In practice, the features may be represented as a multi-dimensional feature vector corresponding to one or more time frames. One of the basic operations of a TTS synthesis system is to map phonetic transcription (e.g., sequences of tags) to appropriate sequences of feature vectors.
For example, the features may include mel-filter cepstral coefficient (MFCC) coefficients. The MFCC may represent a short-term power spectrum of a portion of an input utterance and may be based on, for example, a linear cosine transform of a logarithmic power spectrum on a non-linear mel-frequency scale. (the Mel scale may be a scale of pitches subjectively perceived by a listener as being approximately equal distances from each other, even though the actual frequencies of the pitches are not equal distances from each other.)
In some embodiments, the feature vector may include the MFCC, first order cepstral coefficient derivatives, and second order cepstral coefficient derivatives. For example, the feature vector may contain 13 coefficients, 13 first order derivatives ("δ"), and 13 second order derivatives ("δ - δ"), and thus be 39 in length. However, in other possible embodiments, the feature vector may use different combinations of features. As another example, the feature vector may include Perceptual Linear Prediction (PLP) coefficients, relative spectral (RASTA) coefficients, filter bank logarithmic energy coefficients, or some combination thereof. Each feature vector may be considered to comprise a quantitative representation of the acoustic content of the corresponding time frame of the utterance (or, more generally, the audio input signal).
FIG. 1 depicts a simplified block diagram of an example text-to-speech (TTS) synthesis system 100, according to an example embodiment. In addition to functional components, FIG. 1 also shows selected example inputs, outputs, and intermediates of an example operation. The functional components of TTS synthesis system 100 include: a text analysis module 102, configured to convert an input text 101 into a phonetic transcription 103; a TTS subsystem 104 for generating data representing acoustic characteristics 105 of the speech to be synthesized from the phonetic transcription 103; and a speech generator 106 for generating a synthesized speech 107 from the acoustic characteristics 105. These functional components may be implemented as machine language instructions on one or more computing platforms or systems, such as those described above, in a centralized and/or distributed fashion. The machine language instructions may be stored in one or another form of tangible, non-transitory computer-readable medium (or other article of manufacture), such as a magnetic or optical disk; and made available to processing elements of the system as part of, for example, a manufacturing process, a configuration process, and/or an execution start-up process.
It should be noted that the discussion in this section and the figures are given for illustrative purposes. For example, the TTS subsystem 104 may be implemented using HMM models for generating speech features at runtime based on learning (training) associations between known labels and known parameterized speech. As another example, the TTS subsystem 104 may be implemented using a machine learning model, such as an Artificial Neural Network (ANN), for generating speech features at runtime from associations between known tags and known parameterized speech, where the associations are learned through training with known associations. In yet another example, the TTS subsystem may employ a hybrid HMM-ANN model.
According to an example embodiment, the text analysis module 102 may receive input text 101 (or other form of text-based input) and generate as output phonetic transcription 103. For example, the input text 101 may be a text message, an email, a chat input, a book paragraph, an article, or other text-based communication. As described above, phonetic transcription may correspond to a sequence of tags that identify phonetic units (such as phonemes), possibly together with contextual information.
As shown, the TTS subsystem 104 can employ HMM-based or ANN-based speech synthesis to generate feature vectors corresponding to the phonetic transcription 103. The feature vector may comprise quantities representing acoustic properties 105 of the speech to be generated. For example, the acoustic characteristics may include pitch, fundamental frequency, pace (e.g., speech rate), and prosody. Other acoustic characteristics are also possible.
The acoustic characteristics may be input to a speech generator 106, which speech generator 106 generates as output a synthesized speech 107. The synthesized speech 107 may be generated as actual audio output, such as from an audio device having one or more speakers (e.g., headphones, earphones, speakers, or the like); and/or digital data that may be recorded and played from a data file (e.g., a waveform file, etc.).
Such a high-level description of an example TTS system may be used to provide context for the description of TTS synthesis with user interface inputs for adapting how speech is synthesized. First, a discussion of example communication systems and device architectures in which example embodiments of TTS synthesis with user interface adaptation may be implemented is presented.
3. Example communication System and device architecture
According to the method of an example embodiment, such as the one described above, the device may be implemented using so-called "thin client" and "cloud-based" server devices, as well as other types of client and server devices. Under aspects of this example, client devices such as mobile phones and tablet computers may load transfer some processing and storage responsibilities to a remote server device. At least at some times, these client services can communicate with a server device over a network, such as the internet. As a result, applications running on the client device may also have permanent, server-based components. It should be noted, however, that at least some of the methods, processes, and techniques disclosed herein may be capable of operating entirely on a client device or a server device.
This section describes a general system and device architecture for such client devices and server devices. However, the methods, devices and systems presented in the following sections may also operate under different paradigms. Thus, the embodiments of this section are merely examples of how the methods, apparatus and systems can be performed.
a. Example communication System
Fig. 2 is a simplified block diagram of a communication system 200 in which various embodiments described herein may be employed. Communication system 200 includes client devices 202, 204, and 206, which represent a desktop Personal Computer (PC), a tablet computer, and a mobile phone, respectively. The client device may also include, for example, a wearable computing device, such as a head-mounted display and/or an augmented reality display. Each of these client devices may be capable of communicating with other devices (including with each other) via the network 208 using wired connections (represented by solid lines) and/or wireless connections (represented by dashed lines).
The network 208 may be, for example, the Internet, or some other form of public or private Internet Protocol (IP) network. Thus, client devices 202, 204, and 206 may communicate using packet-switched technology. Nonetheless, the network 208 may also incorporate at least some circuit-switched technology, and the client devices 202, 204, and 206 may communicate via circuit-switching instead of, or in addition to, packet-switching.
Although only three client devices, one server device, and one server data store are shown in fig. 2, communication system 200 may include any number of each of these components. For example, communication system 200 may include millions of client devices, thousands of server devices, and/or thousands of server data stores. Further, the client device may take forms other than that shown in fig. 2.
b. Example Server device and Server System
Fig. 3A is a block diagram of a server device according to an example embodiment. In particular, the server device 300 shown in fig. 3A may be configured to perform one or more functions of the server device 210 and/or the server data store 212. The server device 300 may include a user interface 302, a communication interface 304, a processor 306, and a data store 308, all of which may be linked together via a system bus, network, or other connection mechanism 314.
The user interface 302 may include a user input device such as a keyboard, keypad, touch screen, computer mouse, trackball, joystick, and/or other similar device now known or later developed. The user interface 302 may also include a user display device such as one or more Cathode Ray Tubes (CRTs), liquid Crystal Displays (LCDs), light Emitting Diodes (LEDs), displays using Digital Light Processing (DLP) technology, printers, light bulbs, and/or other similar devices now known or later developed. Additionally, user interface 302 may be configured to generate aural outputs via speakers, speaker jacks, audio output ports, audio output devices, headphones, and/or other similar devices now known or later developed. In some embodiments, user interface 302 may include software, circuitry, or another form of logic capable of transmitting data to and/or receiving data from an external user input/output device.
The communication interface 304 may include one or more wireless interfaces and/or wired interfaces that may be configured to communicate via a network, such as the network 208 shown in fig. 2. The wireless interface, if present, may include one or more wireless transceivers, such as Bluetooth
In some embodiments, the communication interface 304 may be configured to provide reliable, secure, and/or authenticated communication. For each communication described herein, information for ensuring reliable communication (e.g., guaranteed messaging) may be provided, possibly as part of a message header and/or trailer (e.g., packet/message ordering information, encapsulation header and/or trailer, size/time information, and transmission verification information such as Cyclic Redundancy Check (CRC) and/or parity values). Communications may be secured (e.g., encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms such as, but not limited to, the Data Encryption Standard (DES), advanced Encryption Standard (AES), rivest, shamir and Adleman (RSA) algorithms, diffie-Hellman algorithms, and/or Digital Signature Algorithms (DSA). Other encryption protocols and/or algorithms may be used in place of or in addition to those listed herein to secure (and then decrypt/decode) communications.
The processor 306 may include one or more general-purpose processors (e.g., a microprocessor) and/or one or more special-purpose processors (e.g., a Digital Signal Processor (DSP), a Graphics Processing Unit (GPU), a floating point processing unit (FPU), a network processor, or an Application Specific Integrated Circuit (ASIC)). The processor 306 may be configured to execute computer-readable program instructions 310 and/or other instructions contained in the data store 308 to perform various functions described herein.
The data store 308 may include one or more non-transitory computer-readable storage media that may be read or accessed by the processor 306. One or more computer-readable storage media may include volatile and/or nonvolatile storage components, such as optical, magnetic, organic, or other memory or disk storage, which may be integrated in whole or in part with processor 306. In some embodiments, data storage 308 may be implemented using a single physical device (e.g., one optical, magnetic, organic, or other memory or disk storage unit), while in other embodiments, data storage 308 may be implemented using two or more physical devices.
The data store 308 can also include program data 312, which program data 312 can be used by the processor 306 to perform the functions described herein. In some embodiments, the data store 308 may include or have access to additional data storage components or devices (e.g., a cluster data store described below).
Referring briefly again to fig. 2, the server device 210 and the server data storage device 212 may store applications and application data at one or more locations accessible via the network 208. These sites may be data centers containing a large number of servers and storage devices. The exact physical location, connectivity, and configuration of server device 210 and server data storage device 212 may be unknown and/or unimportant to the client device. Thus, server device 210 and server data storage device 212 may be referred to as "cloud-based" devices, which are housed at various remote locations. One possible advantage of such "cloud-based" computing is load shifting processing and data storage from client devices, thereby simplifying the design and requirements of these client devices.
In some embodiments, server device 210 and server data storage device 212 may be a single computing device residing in a single data center. In other embodiments, server device 210 and server data storage device 212 may comprise multiple computing devices in a data center, or even multiple computing devices in multiple data centers, where the data centers are located in different geographic locations. For example, fig. 2 depicts each of the server device 210 and the server data storage device 212 as possibly being located in different physical locations.
Fig. 3B depicts an example of a cloud-based server cluster. In FIG. 3B, the functionality of the server device 210 and the server data storage device 212 may be distributed among three server clusters 320A, 320B, and 320C. The server cluster 320A may include one or more server devices 300A, a cluster data store 322A, and a cluster router 324A connected by a local cluster network 326A. Similarly, server cluster 320B may include one or more server devices 300B, cluster data store 322B, and cluster router 324B connected by a local cluster network 326B. Likewise, server cluster 320C may include one or more server devices 300C, cluster data store 322C, and cluster router 324C connected by a local cluster network 326C. Server clusters 320A, 320B, and 320C may communicate with network 308 via communication links 328A, 328B, and 328C, respectively.
In some embodiments, each of server clusters 320A, 320B, and 320C may have an equal number of server devices, an equal number of cluster data stores, and an equal number of cluster routers. However, in other embodiments, some or all of the server clusters 320A, 320B, and 320C may have a different number of server devices, a different number of cluster data stores, and/or a different number of cluster routers. The number of server devices, cluster data stores, and cluster routers in each server cluster may depend on the computing tasks and/or applications assigned to each server cluster.
For example, in server cluster 320A, server device 300A may be configured to perform various computing tasks for a server, such as server device 210. In one embodiment, these computing tasks may be distributed among one or more of the server devices 300A. Server devices 300B and 300C in server clusters 320B and 320C may be configured the same as or similar to server device 300A in server cluster 320A. On the other hand, in some embodiments, server devices 300A, 300B, and 300C may each be configured to perform different functions. For example, server device 300A may be configured to perform one or more functions of server device 210, while server device 300B and server device 300C may be configured to perform functions of one or more other server devices. Similarly, the functionality of the server data storage device 212 may be dedicated to a single server cluster, or distributed across multiple server clusters.
The cluster data stores 322A, 322B, and 322C of server clusters 320A, 320B, and 320C, respectively, may be data storage arrays comprising disk array controllers configured to manage reading, writing, and access to groups of hard disk drives. The disk array controller, alone or in combination with its respective server device, may also be configured to manage backup or redundant copies of data stored in the cluster data store to prevent: a disk drive failure or other type of failure that prevents one or more server devices from accessing one or more clustered data stores.
Similar to the manner in which the functionality of server device 210 and server data storage device 212 may be distributed across server clusters 320A, 320B, and 320C, various active portions and/or backup/redundant portions of these components may be distributed across cluster data stores 322A, 322B, and 322C. For example, some cluster data stores 322A, 322B, and 322C may be configured to store backup versions of data stored in other cluster data stores 322A, 322B, and 322C.
The cluster routers 324A, 324B, and 324C in the server clusters 320A, 320B, and 320C, respectively, may include networking equipment configured to provide internal and external communications to the server clusters. For example, cluster router 324A in server cluster 320A may include one or more packet switching and/or routing devices configured to provide network communication between (i) server device 300A and cluster data store 322A via cluster network 326A, and/or (ii) server cluster 320A and other devices via communication link 328A to network 308. The cluster routers 324B and 324C may comprise network devices similar to the cluster router 324A, and the cluster routers 324B and 324C may perform the networking functions for the server clusters 320B and 320C that the cluster router 324A performs for the server cluster 320A.
Additionally, the configuration of cluster routers 324A, 324B, and 324C may be based at least in part on the data communication requirements of the server devices and the cluster storage arrays, the data communication capabilities of the network devices in cluster routers 324A, 324B, and 324C, the latency and throughput of local cluster networks 326A, 326B, 326C, the latency, throughput, and cost of wide area network connections 328A, 328B, and 328C, and/or other factors that may contribute to cost, speed, fault tolerance, resiliency, efficiency, and/or other design goals of the system architecture.
c. Example client device
Fig. 4 is a simplified block diagram illustrating some of the components of an example client device 400. The client device 400 may be configured to perform one or more functions of the client devices 202, 204, 206. By way of example, and not limitation, client device 400 may be or include a "plain old telephone system" (POTS) phone, a cellular mobile phone, a still camera, a video camera, a facsimile machine, an answering machine, a computer (such as a desktop, notebook, or tablet computer), a personal digital assistant, a wearable computing device, a home automation component, a Digital Video Recorder (DVR), a digital television, a remote control, or some other type of device equipped with one or more wireless or wired communication interfaces. Client device 400 may also take the form of interactive virtual and/or augmented reality glasses, such as a head-mounted display device, sometimes referred to as a "heads-up" display device. Although not necessarily shown in fig. 4, the head mounted device may include a display component for displaying images on the display component of the head mounted device. The head-mounted device may also include one or more eye-facing cameras or other devices configured to track eye movements of a wearer of the head-mounted device. An eye tracking camera may be used to determine the eye gaze direction and the movement of the wearer's eyes in real time. The eye gaze direction may be provided as an input to various operations, functions, and/or applications, such as tracking the gaze direction of the wearer and movement on text displayed in the display device.
As shown in fig. 4, client device 400 may include a communication interface 402, a user interface 404, a processor 406, and a data store 408, all of which may be communicatively linked together via a system bus, network, or other connection mechanism 410.
In general, the processor 406 may be capable of executing program instructions 418 (e.g., compiled or non-compiled program logic and/or machine code) stored in the data store 408 to perform various functions described herein. The data store 408 can include a non-transitory computer-readable medium having stored thereon program instructions that, when executed by the client device 400, cause the client device 400 to perform any of the methods, processes, or functions disclosed in this specification and/or the drawings. Execution of program instructions 418 by processor 406 may result in processor 406 using data 412.
By way of example, the program instructions 418 may include an operating system 422 (e.g., an operating system kernel, device drivers, and/or other modules) and one or more application programs 420 (e.g., an address book, email, web browsing, social networking, and/or gaming applications) installed on the client device 400. Similarly, data 412 may include operating system data 416 and application data 414. Operating system data 416 may be primarily accessible to operating system 422, and application data 414 may be primarily accessible to one or more application programs 420. Application data 414 may be arranged in a file system that is visible or hidden from the user of client device 400.
The application programs 420 may communicate with the operating system 412 through one or more Application Programming Interfaces (APIs). These APIs may facilitate, for example, the reading and/or writing of application data 414 by application programs 420, the transmission or reception of information via communication interface 402, the reception or display of information on user interface 404, and so forth.
In some jargon, the application 420 may be referred to simply as an "app (application)". Additionally, the application programs 420 may be downloaded to the client device 400 through one or more online application stores or application marketplaces. However, the application may also be installed on the client device 400 in other manners, such as via a web browser or through a physical interface (e.g., a USB port) on the client device 400.
4. Example systems and operations
Text-to-speech synthesis with user interface adaptation can be viewed as incorporating manual pacing (pacing) by design and as a whole process into a system and method for text-to-speech synthesis. According to an example embodiment, speech output may be incrementally generated when a user performs a focusing activity, such as using a mouse to hover a cursor over words one after another on a desktop computer, or using their fingers to track text on a tablet-type device, or some other form of manual text tracking on a device with an interactive display. As the user does so, their tracked movement may be translated into input (e.g., cursor input) to the text-to-speech module or subsystem. The text-to-speech module may use the input tracking information to adapt the generation of speech features that drive the speech generator. The result may be synthesized speech of the spoken text consistent with the user's tracking of the text. In particular, the TTS module and speech generator may speak (or otherwise output) the text that the user is pointing at, at the rate that the user moves their finger or cursor over the displayed text, and in a manner that sounds natural.
The technical challenge of implementing text-to-speech synthesis for manual pacing, delivering high-quality speech output while following the user's tracking of the location and velocity of the text, can be illustrated by considering the following somewhat naive approach: just observe what word the user is currently pointing to and send that word to the TTS engine for pronunciation. While this may seem reasonable in principle, the effect is sub-optimal, lacking prosody that makes the synthesized voice lifelike. For understanding reasons, consider first an example of a nominal TTS operation, which is manually paced controlled without any user interaction. For example, when a sentence is sent to a TTS engine for pronunciation, a number of linguistic operations are performed first, before synthesizing the speech. These include grapheme to phoneme conversions that are used to disambiguate words that are spelled the same but that are pronounced differently (such as the present and past times of the word read). Each phoneme in a sentence is given a duration (and therefore a rate) and a volume. The overall meaning of the sentence is inferred (e.g., whether it is a question or exclamation point) and a pitch contour is generated.
An example process flow 500 for nominal TTS synthesis is shown in FIG. 5. For example, process flow 500 may be performed by TTS system 100 of FIG. 1. As shown, the process flow 500 begins with phoneme extraction 502 of the input text 501, which may be performed by the text analysis module 102, for example. The result of phoneme extraction 502 may be, for example, phonetic transcription 103, which may include a given phoneme for the synthesis process, as well as one or more preceding and following phonemes that provide a "context" for how the given phoneme should be pronounced. As used herein, the terms "before", "after", and the like as applied to context refer to a reading order. Thus, a preceding phoneme (or other language unit) refers to a phoneme corresponding to a preceding text that was read prior to a given text associated with the given phoneme (or other language unit). Similarly, a later phoneme (or other language unit) refers to a phoneme corresponding to a later text that is read after a given text associated with the given phoneme (or other language unit).
The next operation in the example process flow 500 is operation 504, determining phoneme duration and volume; and an operation 506 of determining a pitch contour. For example, these operations may be performed by the text-to-speech subsystem 104 of fig. 1. In an example embodiment, the phoneme duration and volume and pitch contour may correspond to or be considered aspects of "speech characteristics". In the example of fig. 1, these may be acoustic properties 105 and may include or be represented as data, for example, in the form of feature vectors.
As shown, the final operation 508 is speech generation, using phoneme duration and volume and pitch contour as input and producing as output synthesized speech 503. More generally, speech generation may use speech characteristics (or acoustic characteristics) as input. Referring again to FIG. 1, this operation may be performed by the speech generator 106.
Referring again to the TTS subsystem 104 in fig. 1 and the description of the operation of the TTS system 110, in an example embodiment, operations 504 and 506 may be characterized as determining a mapping of the extracted phonemes (including context) to speech features that parameterize the verbal rendering of the extracted phonemes. As described above, this may entail using HMM-based and/or ANN-based models to identify speech features associated with phonetic transcription of written text. The recognition may be probabilistic, e.g., having an achievable confidence level or some other likelihood metric depending on the degree of training.
In such an arrangement, recognition may further involve applying both a speech model and a language model in order to predict the correct pronunciation of each phoneme. For example, the pronunciation describes the tone, duration, and volume of the beginning, middle, and end, among other characteristics. That is, the pronunciation may use the determinations made in operations 504 and 506. These determined speech and linguistic contexts may be associated with the pace at which text is "read" and processed into speech. Without user interaction to control or adjust the pace, "standard" paces of speech and language interpretation can be used to generate natural sounding speech at standard speech rates.
For example, a standard pace (or standard speech rate) may be the pace at which a human may speak when reading a passage of text to a listener or viewer. It should be understood, however, that a standard pace may not correspond to an exact speaking rate, but may generally describe a natural and largely consistent pace that a typical listener would find clear and easy to understand. It may also be considered an initial or default cadence defined as opposed to an adjustable or manual cadence that a user may indicate via an interaction track with displayed text, as described in accordance with example embodiments herein. In some examples, the default or standard pace or pace may be predetermined or may be user defined.
FIG. 6 illustrates an exemplary speech waveform and pitch distribution that may be synthesized according to exemplary process flow 500, according to an exemplary embodiment. The top panel in fig. 6 shows the simple sentence "do you want to here? "from the waveform 602. Each phoneme of the sentence is also tagged at the top of the waveform, and the horizontal spacing between the vertical lines separating each phoneme represents the duration of each spoken segment. The bottom panel in fig. 6 shows the corresponding pitch distribution 604, overlaid with the same phoneme label and duration interval. Note that the pitch distribution may correspond to the pitch distribution of the synthesized speech specified by the synthesized waveform, or may be generated in a speech synthesis process performed by the TTS subsystem, for example. In either case, the pitch distribution represents an aspect of the speech characteristics. The two panels of fig. 6 show examples of speech that can be synthesized at standard (or default) cadence.
The above-mentioned problem with the simplified or naive approach to synthesizing speech by interactively identifying one word at a time or even one phoneme at a time from the displayed text is that the context information may be misinterpreted or may be lost completely. The result may be a pronunciation that sounds word by word or phoneme by phoneme, that sounds unnatural or difficult to understand, and in any case does not correspond to the following: the manner in which the user pronounces the text himself if the user visually scans the text at the same or similar adjusted pace at which the finger (or cursor) crosses the text display.
As an example, consider the word "to" in the simple sentence above. When spoken at standard pace, it may not sound at all. This is indicated by the relatively short duration of phoneme/t/in the waveform and pitch profile. Thus, nominal TTS synthesis modeling can process the word "to" at a standard pace in the context of sentences and surrounding words. For example, a simplified or naive approach may result in a fully pronounced pronunciation of the word "to" that sounds inconsistent and discontinuous with surrounding works and phonemes. In this example, the full pronunciation of "to" may be appropriate for manual pacing as determined from user input, but the tone, volume and pitch, as well as other speech characteristics, may sound unnatural to the human ear.
To overcome problems such as those described above, and to address the challenge of converting user interactions that provide manual pacing of TTS synthesis, the inventors devised techniques that can incorporate as feedback context determined from standard paced TTS synthesis into manually paced TTS synthesis. By doing so, the TTS system can synthesize speech characteristics and output the synthesized speech at a pace that faithfully reflects the interactive user's control of pacing, while producing synthesized speech that sounds natural and fits the manually controlled pace set by the user interaction.
a. Example text-to-speech conversion with user interface Adaptation
FIG. 7 is a conceptual illustration of an example text-to-speech application with user interface adaptation according to an example embodiment. By way of example, an example TTS application is shown as it might appear on a user device 700 (e.g., which can be implemented as user device 400) having a display screen, such as a laptop computer or tablet computer. A visual display 702 of text (text segment) appears on a portion of the display screen. The user interaction is represented by an icon of a pointing finger shown at two consecutive positions on the displayed text. The curved dashed arrow between the two locations indicates that the user is tracking on a portion of text. As a visual cue, most of the text is shown grayed out, with only the portion of the sentence currently being speech synthesized shown in black, enlarged font. However, in example embodiments, graying or other visual defocus effects may be an explicit feature of the actual display. This may be done as a real-time visual aid to the user to help the user focus on the tracked text while attenuating surrounding text that might otherwise distract visually. In other words, the identified text (or text of the identified portion) may be presented or displayed visually clearly and centrally as compared to the remainder of the text — this may include actively emphasizing the identified text and/or actively minimizing the remainder of the text by displaying the text of the text segment in visual defocus rather than the text of the identified portion. In the example shown, the defocus may be graying out, or text blurring. Other possible defocus effects may also be used. Such visual "defocus" and "focus" may be beneficial because displaying the text portions identified by the user more prominently may help the user keep the identified text segments within their visual focus or field of interest.
The movement of the cursor may be used to adapt the standard pace of speech synthesis. For example, the indicator indicated by the cursor input or the motion of the cursor corresponds to the user's real-time tracking of text in the identified portion. According to an example embodiment, the speed (velocity) and/or acceleration of the user's tracked motion may determine the pace of the synthesis process and the pronunciation of speech. For example, the first step cadence and/or acceleration may translate to a first cadence that is above a first threshold cadence but below a standard cadence, where the standard cadence is a second threshold cadence. Upon detecting the first step key, the example system may synthesize and pronounce speech at a word-by-word pace (i.e., the pace at which a TTS system synthesizes a single word); the word-by-word tracking rate corresponds to a tracking rate above a first (predetermined) threshold and below a second (predetermined) threshold, which corresponds to a default or standard pace. As another example, the second step and/or acceleration may be converted to a second cadence that is at or below the first threshold, in which case the example system may synthesize and pronounce the speed at a phoneme-by-phoneme cadence (i.e., the cadence at which the TTS system synthesizes a single phoneme); the phoneme-by-phoneme tracking rate corresponds to a tracking rate equal to or lower than a first threshold. Other arrangements are possible that convert the user tracked speed and/or acceleration into synthesized and spoken speed, including a user interface or application "dashboard" for setting the desired speed.
For illustrative purposes and as an example, the user tracking in fig. 7 may be considered slow enough (e.g., below a first threshold pace) to signal to the synthesis application to generate and output synthesized speech 707 at a phoneme-by-phoneme pace. The synthesized speech may thus be emitted from the speaker component 704 of the user device 700. Also for illustrative purposes, speaker 704 is explicitly shown as a visual cue that it is a component of the user device. In an actual user device, the speaker component may be physically internal and invisible.
In this example, the phoneme-by-phoneme rendering of the synthesized speech 707 is represented by the hyphen and enlarged font of the word "tr-ans-formed". According to an example embodiment, a manually paced TTS system, such as the system conceptually shown in fig. 7, may render the pronunciation of "tr-ans-formed" in a natural-sounding manner, e.g., as if the user were slowly reading the word "transformated". This capability, and more generally the ability to adapt pacing TTS synthesis to pronounce naturally, can be achieved by modifying the nominal speech synthesis process, as will now be described by way of example. In an example embodiment, the synthesis of a phoneme-by-phoneme cadence of a word or phrase may be followed by automatically re-reading the word or phrase at a standard cadence. As can be further seen in fig. 7, the word "himself" is output at a faster pace than the pronunciation of "conversion"; the methods described herein facilitate a smooth transition between two different cadences, which may improve the user's human auditory perception. By improving user perception, computing resources may also be reduced, as the user may not need to repeat TTS requests, and subsequent interactions with the client device may be reduced.
According to an example embodiment, standard paced TTS synthesis of text may provide "previously synthesized speech" that may be used as input and/or feedback for speech synthesis of text being tracked by cursor input or the like while being displayed in an interactive display device. The prior synthesis may be performed during a time interval (e.g., a few seconds, minutes, or even earlier) that is completely before the TTS synthesis is paced according to the tracking, or during an interval that is substantially simultaneous with the pacing of the TTS synthesis according to the tracking, which is paced according to the tracking. In the latter case, for example, the prior synthesis may be performed one or several steps before the adaptive pacing synthesis; e.g. with sufficient preamble time as feedback.
The process flow of TTS, which accommodates pacing or manual pacing, can be described in at least two ways according to modifications of the example nominal synthesis process. First, it preserves the context of previously generated speech (e.g., before the current word or phoneme) so that it can iteratively generate speech without making each synthesized sample appear to come from a new synthesized instance. In other words, it passes the context of the previously spoken text to the synthesizer so that the synthesizer generates a "continuation" of the speech signal rather than a new speech signal. Second, it adapts the volume, rate and pitch contour of the generated speech to follow the rate or pace of the tracking instructions.
Fig. 8 depicts an example process flow 800 for text-to-speech synthesis with user interface adaptation according to an example embodiment. As with the example nominal process flow of FIG. 5, process flow 800 may be performed by, for example, TTS system 100 of FIG. 1. As shown, the process flow 800 begins with phoneme extraction 802 of the input text 801. In process flow 800, phoneme extraction corresponds to a glyph to phoneme conversion for an entire paragraph using the entire text, referred to hereinafter as the global context. The global context can be used to disambiguate the pronunciation and infer the standard phoneme duration, volume and pitch contour. That is, the global context includes the entire text and the elements of the TTS that make up the entire text as if the user tracking did not exist-i.e., standard processing. From this point, process flow 800 may be considered to follow two substantially parallel process "pipes". The first generally corresponds to standard processing that generates the previous synthesis described above. The second corresponds to a manual pacing or adaptive composition process that uses previously composed aspects for the context.
For the previously synthesized (canonical) processing pipeline, the extracted phonemes are input to operation 804 for determining a canonical phoneme duration and volume. This is followed by operation 808 of determining the current phoneme duration and volume according to standard processing. Next, operation 810 determines the current word pitch contour, again at standard pacing. Next is speech generation 812. However, standard speech is not output audibly. Instead, in operation 814, it is provided as previously synthesized audio, which is used in the adaptation process performed according to the manually paced synthesis pipeline, as now described.
For a manual pacing or adaptive synthesis processing pipeline, the user tracking input is used to determine the current word 803 and the current rate 805. The current word 803 may correspond to a current tracked location within the displayed text, while the current rate 805 may correspond to first and second time derivatives of the tracked location (i.e., the velocity and acceleration of the tracked location). By way of example in fig. 8, these inputs may correspond to cursor inputs, as indicated. However, as noted above, the position and motion (e.g., velocity and acceleration) inputs may more generally be the position and motion of any suitable tracking operation, non-limiting examples of which are also described above. Accordingly, it should be understood that references herein to "cursor input" in the context of manually pacing or adaptive TTS synthesis may be considered to apply to one or more of any of the non-limiting tracking operation examples or the like provided.
The tracked locations may then be used to index the current word within the globally extracted phonemes determined in phoneme extraction 802. This gives the synthesizer a subset of the phonemes to be spoken at this time, which is represented as the phonemes of the current word 806. Meanwhile, the previously synthesized audio 814 provides a phoneme input for the phoneme of the current word 806, which is then processed to determine the current phoneme duration and volume 808.
The current word 803 is also used to determine a local context 807, which is input to both the current phoneme duration and volume 808 and the current word pitch contour 810. The current phoneme duration and volume 808 also receives as input the standard phoneme duration and volume 804 and the current rate 805. These three inputs, local context 807, canonical phone duration and volume 804, and current rate 805, are used to determine a current phone duration and volume 808, where the duration corresponds to the current rate and the phone volume coincides with the canonical phone volume.
The next operation 810 determines the current word pitch. The operation also receives as input the local context 807, the current rate 805 and a previous (previously synthesized) pitch 816, the pitch 816 being derived from the previously synthesized audio 814, as indicated. These three inputs, local context 807, current rate 805, and previous pitch 816, are used to determine that the current word pitch 810 is continuous and/or consistent with a previously synthesized pitch. For example, the pitch and prosody of the synthesized speech of the text of the identified portion may be continuous with the pitch and prosody of the synthesized speech of the text of the context portion both immediately preceding and immediately following the identified portion. Such smooth transitions may reduce dissonances and improve the human auditory perception of the user. By improving user perception, computing resources may also be reduced, as the user may not need to repeat TTS requests.
The local context 807 is used to help ensure prosodic consistency. For example, the possibly start-and-stop and/or discontinuous nature of speech output resulting from manual pacing may cause differences in how the user understands the text as a whole. For example, certain words that would otherwise be skipped, connected, or simply spoken very briefly, such as the word "to" in the sentence "I wan to go home," may have to be pronounced in their entirety when the user explicitly tracks them. However, such words should still be given a pitch, duration and loudness value that is related to the prosodic importance of the word immediately adjacent to it. The language information from the global context next to the current word is used to derive the local context. For example, the local context 807 may be based on context portions of text surrounding the identified text portion; the contextual part of the text may be one or more phonemes, one or more words, or one or more sentences, as the case may be. The context section includes text immediately preceding and immediately following the identified section (i.e., at least one phoneme, optionally at least one language unit, of the text segment adjacent either end of the identified section).
The current phoneme duration and volume 808 and current word pitch 810 are then processed by the speech generator to produce adapted synthesized speech 809. This is synthesized speech that is pronounced at a pace corresponding to the user-tracked text and has a pitch and other speech characteristics that sound natural.
An example process flow 800 for text-to-speech synthesis with user interface adaptation may be summarized as shown in table 1. The entire process flow 800 and/or the summaries in table 1 may be repeated until the entire audio rendering of a text passage or other text portion is fully spoken. As described above, the reference to "cursor input" is exemplary. Various other forms of trace input, including the examples described above, may be used in the flow outlined in table 1.
TABLE 1
The example process flow 800 may be considered to describe word tracking, such as reading word by word. For phone-by-phone factor tracking, the process flow may be similar, but the word-based local context is now also augmented with a phone-based (sub-word-based) local context. In this case, when the user tracks the text, the system can track which syllable or phoneme they are pointing to, and can therefore generate a particular reading unit.
Note that even if the user is in a phoneme mode, generating a particular phoneme may not be appropriate in all cases. Thus, above a certain velocity of the tracked text, the system may fall back to word mode, leaving phoneme modes (determined by their velocity and acceleration patterns) only when the user intentionally attempts to read the words phoneme.
Example embodiments of text-to-speech applications with user interface adaptation advantageously link tracking and reading in a manner that may significantly increase a user's attention to text. The nature of vision is that the user's visual field can contribute a great deal of stimulation to the brain. Keeping a tracking operation indicator (e.g., such as a mouse-controlled pointer/cursor, finger tracking on a touch screen, or measured/monitored eye gaze direction mapped to a text display) pointing to the next word to be read may ensure that the majority of the stimulus comes from the user's text, rather than from distractions in the user's environment.
This focusing can be further enhanced by intentionally de-focusing, de-saturating, or blurring portions of text that are away from the user's cursor, thereby reducing the irritation of visually distracting areas of the screen before the user's cursor reaches these areas. This may further enhance the user's focused experience when manually pacing the TTS. For example, various defocus techniques and/or operations may be used in addition to blurring or fading. Non-limiting examples of visual defocus may include visual effects such as fading or blurring or display with a different background or foreground color than text outside of the user cursor (or other interactive focus/tracking). Other defocusing operations may include changing the font or font size of the defocused text (e.g., reducing the font size relative to the text outside of the user's interactive focusing/tracking, and/or displaying the defocused text in a relatively less authoritative font style than the text outside of the user's interactive focusing/tracking). Additionally, text within the user's interactive focus/tracking may be displayed or underlined within the text box, while out-of-focus text is displayed or not underlined from the text box. Other defocus effects are also possible.
One common challenge associated with reading words by manually tracking phonemes or words is that while this focuses attention, it may also place higher demands on the learner's working memory. For example, since the pace of reading may be slow, users may have to retain words in their memory for an earlier time in order to successfully decode a sentence.
Example embodiments described herein address this challenge by modeling TTS synthesis that a user paces on the visual processing of saccades of eye-read text. When reading fluently, the eye typically does not move character to character, or even word to word. Instead, it jumps from one set of words to another, receiving multiple words at a time. For example, when reading text arranged in narrow columns, the eye may simply move down through the rows, receiving the entire column at a glance. If desired, the eye can move back and forth to re-read.
Thus, the example embodiments described herein may be viewed as implementing a "tone panning" that functions in a manner complementary to the manual tracking described above. That is, the input text is divided into words and phonemes for TTS, and dependency parsing and other language tagging methods are used to identify larger groups of related words. For example, TTS technology can be used to divide input text into noun phrases, clauses, and sentences. These different representations can then be arranged hierarchically, wherein the text consists of sentences, the sentences consist of sentence fragments, the sentence fragments consist of clauses, the clauses consist of phrases, the phrases consist of word collocations, and the word collocations consist of words.
Thus, when a user initiates a glance-like user interaction, a higher level representation of the hierarchy may be spoken as a possible language-aware re-reading by the example embodiments described herein. Such interaction may also be initiated implicitly by the system when a unit of the representation at a certain level is completed. For example, the system may be configured to re-read words after phoneme read-out. When a user manually reads the phonemes of a word, the phonemes are spoken, and then the complete word may be spoken by the system at normal (e.g., standard) cadence. In another embodiment, the system may be configured to re-read the sentence after reading a single word. In this case, the user may control the pace at which individual words are read, but once the entire sentence is tracked, the sentence may be spoken.
Such audio saccade simulation may also be initiated manually, for example, by special gestures, keys, or other application dashboard controls. Thus, a user who may forget the tracking of a sentence when reading a single word may initiate re-reading of the entire sentence (or the phrase of which the word is a part). Such re-reading may help improve understanding while at the same time helping the user's working memory by only reading words that are relevant to the current context in which the user is located.
In this way, by combining intelligent TTS and language-aware re-reading, reading comprehension can be greatly enhanced, and the text and audio representations can work together to improve user engagement and saving of the text.
b. Example method
In an example embodiment, the example methods may be implemented as machine-readable instructions that, when executed by one or more processors of a system, cause the system to perform the various functions, operations, and tasks described herein. In addition to one or more processors, the system may include one or more forms of memory for storing machine-readable instructions (and possibly other data) of the example methods, as well as one or more input devices/interfaces, one or more output devices/interfaces, and possibly other components. Some or all aspects of the example method may be implemented in a TTS synthesis system, which may include functionality and capabilities specific to TTS synthesis. However, not all aspects of the example method are necessarily dependent on implementation in a TTS synthesis system.
In an example embodiment, a TTS synthesis system may include one or more processors, one or more forms of memory, one or more input devices/interfaces, one or more output devices/interfaces, and machine-readable instructions that, when executed by the one or more processors, cause the TTS synthesis system to perform the various functions and tasks described herein. The TTS synthesis system may also include an implementation based on one or more hidden markov models. In particular, a TTS synthesis system may employ a method that incorporates HMM-based speech synthesis as well as other possible components. Additionally or alternatively, the TTS synthesis system may also include one or more Artificial Neural Network (ANN) based implementations. In particular, a TTS synthesis system may employ a method that incorporates ANN-based speech synthesis, as well as other possible components.
Fig. 9 is a flowchart illustrating an example method according to an example embodiment. At step 902, a text-to-speech (TTS) system that synthesizes speech from a text segment using the TTS system and simultaneously displays the text segment in a display device may receive input indicative of a position and motion of a tracking operation relative to the text segment displayed in the display device. In embodiments where the TTS system is implemented on a server remote from the display device, step 902 may include, while synthesizing speech from a text segment displayed on the display device using a text-to-speech (TTS) system, receiving input indicative of a location and motion of a tracking operation at the display device displaying the text segment.
At step 904, the TTS system may use the indicated location of the tracking operation to identify the portion of the text segment that is undergoing the TTS synthesis process. The portion may be identified as a portion of text being processed near the time the tracking operation input (or input indicating the location and motion of the tracking operation) was received, or the portion may be identified as a portion of a text segment located near the indicated location of the tracking operation. The TTS system may also use the indicated location of the tracking operation to identify a context portion of the text passage for which the TTS system has synthesized previously synthesized speech at a standard speech rate, step 904. The context portion can include text immediately preceding and immediately following the identified portion.
At step 906, the TTS system may use the indicated motion of the tracking operation to adjust a speech rate of the TTS synthesis of the identified portion from a standard speech rate to an adapted speech rate determined based on the indicated motion.
At step 908, the TTS system can adapt the speech characteristics of the synthesized speech of the identified portion by applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion to the TTS synthesis process of the identified portion.
Finally, in step 910, the TTS system may output the synthesized speech of the identified portion at the adapted speech rate and the adapted speech characteristics. The synthesized speech may be rendered or played locally or output by the TTS system to a remote device.
According to an example embodiment, the example method may further comprise synthesizing previously synthesized speech of the contextual part during a time interval concurrent with synthesizing speech of the part. In this case, applying the adapted speech rate and synthesized speech characteristics of the previously synthesized speech of the contextual part may require applying the adapted speech rate and synthesized speech characteristics of the previously synthesized speech of the contextual part as feedback to the TTS synthesis process of the identified part.
According to an example embodiment, the example method may further comprise synthesizing previously synthesized speech of the contextual part during a time interval prior to synthesizing speech of the part.
According to an example embodiment, receiving a tracking operation input may entail receiving input of a virtual pointing indicator input (or cursor input) via a user interface communicatively connected to or part of the display device. Furthermore, receiving a tracking operation input indicating a position and motion of the virtual pointing indicator in the display device relative to the displayed text segment in the display device may require receiving a tracking of the position, velocity, and acceleration of the virtual pointing indicator relative to the displayed text segment in real-time. Further according to an example embodiment, the virtual pointing indicator may be any one or more of a (graphically) rendered cursor, a tactile input of position and motion on a touch screen, or an input of eye-tracking position and motion of a user projected on a display screen. The virtual pointing indicator may or may not be graphically rendered, as appropriate.
According to an example embodiment, identifying the portion of the text snippet that is undergoing the TTS synthesis process using the indicated location of the tracking operation may involve identifying language units in the identified portion that is undergoing the TTS synthesis process using the location of the tracking operation. Further, the language units may be one or more of phonemes, words or phrases corresponding to the text in the identified portion undergoing the TTS synthesis process.
According to an example embodiment, the indicated movement of the tracking operation may correspond to real-time tracking of the text in the identified portion at a word-by-word or phoneme-by-phoneme tracking rate. Thus, if the tracking rate is word-by-word, a TTS synthesis process that applies the adapted speech rate to the identified portion may require word-by-word synthesis of the spoken speech. Similarly, if the tracking rate is phone by phone, then applying the adapted speech pacing to the TTS synthesis process for the identified portion may require synthesizing the spoken speech phone by phone.
According to an example embodiment, the speech characteristics of the identified portion of synthesized speech may include a pitch and a prosody of the identified portion of synthesized speech. Similarly, the speech characteristics of the previously-synthesized speech of the contextual part include pitch and prosody of the previously-synthesized speech of the contextual part. As such, applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion to the TTS synthesis processing of the identified portion may involve generating synthesized speech of the identified portion spoken at the adapted speech rate, and the pitch and prosody of the synthesized speech of the text of the identified portion is continuous with the pitch and prosody of the synthesized speech of the text of the contextual portion immediately preceding and immediately following the identified portion. This continuous pitch and prosody may ensure smooth transitions during speech synthesis, which may reduce user dissonance.
According to an example embodiment, the indicated movement of the tracking operation may correspond to tracking text in the identified portion in real-time at a tracking rate. In this way, synthesized speech of the identified portion spoken at the adapted speech rate is generated, and the pitch and prosody of the synthesized speech of the text of the identified portion is continuous with the pitch and prosody of the synthesized speech of the text of the contextual portion immediately preceding and immediately following the identified portion, which may involve synthesizing speech of the identified portion, which speaks one language unit at a time. In particular, each given spoken language unit of the identified portion may also be spoken with a pitch and prosody that is continuous with the pitch and prosody of the synthesized speech of the language units of the contextual portion that immediately precedes and succeeds both the given spoken language unit of the identified portion. Further, each language unit may be a phoneme if the tracking rate is not greater than the first threshold rate, or a word if the tracking rate is greater than the first threshold rate and less than the second threshold rate.
According to an example embodiment, the example method may further include, immediately after outputting the synthesized speech of the identified portion at the adapted speech rate and with the adapted speech characteristics, repeatedly outputting the synthesized speech of the identified portion at the standard speech rate and with the standard speech characteristics. This operation may help improve and/or enhance user comprehension and/or read-write capabilities by associating the repetitive output with speech synthesized at an adapted speech rate and adapted speech characteristics.
According to an example embodiment, simultaneously, displaying a text segment in a display device may require displaying text of the identified portion in visual clarity and focus; and employing visual defocus to display text of the text segment in addition to text of the identified portion. Non-limiting examples of visual defocus may include visual effects such as fading or blurring, or display with different background or foreground colors. Other defocusing operations may include changing the font or font size of the defocused text (e.g., reducing the font size relative to the text of the identified portion, and/or displaying the defocused text in a relatively less authoritative font style than the identified portion). Additionally, the identified portion may be displayed within the text box or underlined, while the defocused text is excluded from the text box or displayed without underlining. Other defocus effects are also possible.
It will be understood that the steps shown in fig. 9 are intended to illustrate a method according to an example embodiment. In this way, various steps may be changed or modified, the order of certain steps may be changed, and additional steps may be added, while still achieving the overall desired operation. The method may be performed by a client device, or by a server, or by a combination of a client device and a server. The method may be performed by any suitable computing device.
Conclusion
Illustrative embodiments have been described herein by way of example. Those skilled in the art will understand, however, that changes and modifications may be made thereto without departing from the true scope and spirit of the elements, products and methods to which the embodiments are directed as defined by the claims.
Claims (20)
1. A method, comprising:
while synthesizing speech from a text segment using a text-to-speech (TTS) system and while displaying the text segment in a display device, receiving input indicative of a position and a motion of a tracking operation relative to the text segment displayed in the display device;
using the indicated location of the tracking operation to identify both a portion of the text passage undergoing the TTS synthesis process near the time the tracking operation input was received and a contextual portion for which the TTS system has synthesized a text passage of previously synthesized speech at a standard speech rate, wherein the contextual portion includes text immediately preceding and immediately following the identified portion;
adjusting a speech rate of the TTS synthesis of the identified portion from a standard speech rate to an adapted speech rate determined based on the indicated motion using the indicated motion of the tracking operation;
adapting speech characteristics of the synthesized speech of the identified portion by applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion to a TTS synthesis process of the identified portion; and
the synthesized speech of the identified portion is output at the adapted speech rate and the adapted speech characteristic.
2. The method of claim 1, further comprising:
synthesizing previously synthesized speech of the contextual part during a time interval concurrent with synthesizing speech of the part,
and wherein applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion to the TTS synthesis process for the identified portion comprises applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion as feedback to the TTS synthesis process for the identified portion.
3. The method of claim 1, further comprising:
synthesizing previously synthesized speech of the contextual part during a time interval prior to synthesizing the part of speech.
4. The method of any preceding claim, wherein receiving tracking operation input comprises receiving input of a virtual pointing indicator via a user interface communicatively connected to the display device or a portion of the display device, wherein the virtual pointing indicator is at least one of: a rendered cursor, a tactile input of position and motion on a touch screen, or an input of eye-tracking position and motion of a user projected on a display screen.
5. The method of any preceding claim, wherein receiving input indicative of a position and motion of a tracking operation in the display device relative to a displayed text segment in the display device comprises receiving tracking of the position, velocity and acceleration of the tracking operation relative to the displayed text segment in real-time.
6. The method of any preceding claim, wherein using the indicated location of the tracking operation to identify the portion of the text segment undergoing TTS synthesis processing comprises:
the location of the tracking operation is used to identify the language units in the identified portion of the TTS synthesis process being performed,
wherein the language unit is at least one of: phonemes, words, phrases or sentences corresponding to the text in the identified portion being subjected to the TTS synthesis process.
7. The method of any preceding claim, wherein the indicated movement of the tracking operation corresponds to real-time tracking of text in the identified portion at a word-by-word or phoneme-by-phoneme tracking rate,
and wherein the TTS synthesis process of applying the adapted speech rate to the identified portion comprises:
synthesizing the spoken speech word by word if the tracking rate is word by word; and
if the tracking rate is phoneme by phoneme, the spoken speech is synthesized phoneme by phoneme.
8. The method of any of claims 1-6, wherein the speech characteristics of the identified portion of synthesized speech include pitch and prosody of the identified portion of synthesized speech,
wherein the speech characteristics of the previously-synthesized speech of the contextual part include pitch and prosody of the previously-synthesized speech of the contextual part,
and wherein applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the context portion to the TTS synthesis processing of the identified portion comprises:
generating synthesized speech of the identified portion, the synthesized speech of the identified portion being spoken at the adapted speech rate, and a pitch and a prosody of the synthesized speech of the text of the identified portion being continuous with a pitch and a prosody of the synthesized speech of the text of the context portion both immediately before and after the identified portion.
9. The method of claim 8, wherein the indicated motion of the tracking operation corresponds to tracking text in the identified portion in real-time at a tracking rate,
and wherein generating the identified portion of synthesized speech, the identified portion of synthesized speech being spoken at the adapted speech rate and having a pitch and prosody of the synthesized speech of the text of the identified portion continuous with a pitch and prosody of the synthesized speech of the text of the contextual portion both immediately preceding and following the identified portion comprises:
synthesizing speech of the identified portion, the speech of the identified portion uttering one language unit at a time, wherein each given uttered language unit of the identified portion is further uttered with a pitch and a prosody that are continuous with a pitch and a prosody of synthesized speech of language units of the context portion that are both immediately before and immediately after the given uttered language unit of the identified portion,
wherein each language unit is: a phoneme if the tracking rate is not greater than a first threshold rate, or a word if the tracking rate is greater than the first threshold rate and less than a second threshold rate.
10. The method of claim 9, further comprising:
immediately after outputting the identified portion of synthesized speech at the adapted speech rate and the adapted speech characteristic, the identified portion of synthesized speech is repeatedly output at the standard speech rate and the standard speech characteristic to associate the repeated output with the synthesized speech having the adapted speech rate and the adapted speech characteristic.
11. The method of any preceding claim, wherein simultaneously displaying text segments in a display device comprises:
displaying text of the identified portion in visual clarity and focus; and
displaying text of the text segment other than the text of the identified portion with visual defocus, wherein the visual defocus includes a visual effect that is at least one of: fade out, blur, display with a different background or foreground color, change font and/or font size, or exclude out-of-focus text from a text box drawn around the identified portion.
12. A system comprising a text-to-speech (TTS) system, comprising:
one or more processors;
a memory; and
machine-readable instructions stored in the memory, which when executed by the one or more processors, cause the system to perform operations comprising:
while synthesizing speech from a text segment using a TTS system and simultaneously displaying the text segment in a display device, receiving input indicative of a position and a motion of a tracking operation relative to the text segment displayed in the display device;
using the indicated location of the tracking operation to identify both a portion of the text passage undergoing the TTS synthesis process near the time when the tracking operation input was received and a contextual portion for which the TTS system has synthesized a text passage of previously synthesized speech at a standard speech rate, wherein the contextual portion includes text immediately preceding and immediately following the identified portion;
adjusting a speech rate of the TTS synthesis of the identified portion from a standard speech rate to an adapted speech rate determined based on the indicated motion using the indicated motion of the tracking operation;
adapting speech features of the synthesized speech of the identified portion by applying both the adapted speech rate and synthesized speech characteristics of the previously synthesized speech of the contextual portion to a TTS synthesis process of the identified portion; and
the synthesized speech of the identified portion is output at the adapted speech rate and the adapted speech characteristic.
13. The system of claim 12, wherein the operations further comprise:
synthesizing previously synthesized speech of the contextual part during a time interval concurrent with synthesizing speech of the part,
and wherein applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion to the TTS synthesis process for the identified portion comprises applying both the adapted speech rate and the synthesized speech characteristics of the previously synthesized speech of the contextual portion as feedback to the TTS synthesis process for the identified portion.
14. The system of claim 12, wherein the operations further comprise:
synthesizing previously synthesized speech of the contextual part during a time interval prior to synthesizing the part of speech.
15. The system of any of claims 12 to 14, wherein receiving input indicative of a position and a motion of a tracking operation in the display device relative to a displayed text segment in the display device comprises receiving a tracking of a position, a velocity, and an acceleration of the tracking operation relative to the displayed text segment in real-time.
16. The system of any of claims 12 to 14, wherein the speech characteristics of the identified portion of synthesized speech include a pitch and a prosody of the identified portion of synthesized speech,
wherein the speech characteristics of the previously synthesized speech of the contextual part include pitch and prosody of the previously synthesized speech of the contextual part,
and wherein applying both the adapted speech level and the synthesized speech characteristics of the previously synthesized speech of the context portion to the TTS synthesis processing of the identified portion comprises:
generating synthesized speech of the identified portion, the synthesized speech of the identified portion spoken at the adapted speech rate, and a pitch and a prosody of the synthesized speech of the text of the identified portion continuous with a pitch and a prosody of the synthesized speech of the text of the contextual portion both immediately preceding and immediately following the identified portion.
17. The system of claim 16, wherein the indicated motion of the tracking operation corresponds to real-time tracking of text in the identified portion at a tracking rate,
and wherein generating the identified portion of synthesized speech, the identified portion of synthesized speech being spoken at the adapted speech rate and having a pitch and prosody that is continuous with a pitch and prosody of synthesized speech of text of the contextual portion that is both immediately preceding and immediately following the identified portion, comprises:
synthesizing speech of the identified portion, the speech of the identified portion uttering one language unit at a time, wherein each given uttered language unit of the identified portion is further uttered with a pitch and a prosody continuous with a pitch and a prosody of synthesized speech of language units of the context portion that are both immediately before and immediately after the given uttered language unit of the identified portion,
wherein each language unit is: a phoneme if the tracking rate is not greater than a first threshold rate, or a word if the tracking rate is greater than the first threshold rate and below a second threshold rate.
18. The system of claim 17, wherein the operations further comprise:
immediately after outputting the identified portion of synthesized speech at the adapted speech rate and the adapted speech characteristic, the identified portion of synthesized speech is repeatedly output at the standard speech rate and the standard speech characteristic to associate the repeated output with the synthesized speech having the adapted speech rate and the adapted speech characteristic.
19. The system of any of claims 12 to 18, wherein simultaneously displaying text segments in a display device comprises:
displaying text of the identified portion in visual clarity and focus; and
employing a visual defocus to display text of the text segment other than the text of the identified portion, wherein the visual defocus includes a visual effect that is at least one of: faded or blurred or displayed with different background or foreground colors.
20. An article of manufacture comprising a computer-readable storage medium having stored thereon program instructions that, when executed by one or more processors of a system comprising a text-to-speech (TTS) system, cause the system to perform operations comprising:
while synthesizing speech from a text segment using a TTS system and simultaneously displaying the text segment in a display device, receiving input indicative of a position and a motion of a tracking operation relative to the text segment displayed in the display device;
using the indicated location of the tracking operation to identify both a portion of the text passage undergoing the TTS synthesis process near the time when the tracking operation input was received and a contextual portion for which the TTS system has synthesized a text passage of previously synthesized speech at a standard speech rate, wherein the contextual portion includes text immediately preceding and immediately following the identified portion;
adjusting a speech rate of the TTS synthesis of the identified portion from a standard speech rate to an adapted speech rate determined based on the indicated motion using the indicated motion of the tracking operation;
adapting speech characteristics of the synthesized speech of the identified portion by applying the adapted speech rate and synthesized speech characteristics of the previously synthesized speech of the contextual portion to a TTS synthesis process of the identified portion; and
the identified portion of synthesized speech is output at the adapted speech rate and the adapted speech characteristic.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/035861 WO2021247012A1 (en) | 2020-06-03 | 2020-06-03 | Method and system for user-interface adaptation of text-to-speech synthesis |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115668358A true CN115668358A (en) | 2023-01-31 |
Family
ID=71842800
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080101685.2A Pending CN115668358A (en) | 2020-06-03 | 2020-06-03 | Method and system for user interface adaptation for text-to-speech synthesis |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230122824A1 (en) |
EP (1) | EP4143820A1 (en) |
CN (1) | CN115668358A (en) |
WO (1) | WO2021247012A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116645954A (en) * | 2023-07-27 | 2023-08-25 | 广东保伦电子股份有限公司 | IP broadcasting system adopting AI (analog input) sound |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2022132752A1 (en) * | 2020-12-14 | 2022-06-23 | Speech Morphing Systems, Inc. | Method and system for synthesizing cross-lingual speech |
US20230134226A1 (en) * | 2021-11-03 | 2023-05-04 | Accenture Global Solutions Limited | Disability-oriented font generator |
CN114373445B (en) * | 2021-12-23 | 2022-10-25 | 北京百度网讯科技有限公司 | Voice generation method and device, electronic equipment and storage medium |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090313020A1 (en) * | 2008-06-12 | 2009-12-17 | Nokia Corporation | Text-to-speech user interface control |
US9262063B2 (en) * | 2009-09-02 | 2016-02-16 | Amazon Technologies, Inc. | Touch-screen user interface |
-
2020
- 2020-06-03 CN CN202080101685.2A patent/CN115668358A/en active Pending
- 2020-06-03 WO PCT/US2020/035861 patent/WO2021247012A1/en unknown
- 2020-06-03 US US17/914,033 patent/US20230122824A1/en active Pending
- 2020-06-03 EP EP20747219.2A patent/EP4143820A1/en active Pending
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN116645954A (en) * | 2023-07-27 | 2023-08-25 | 广东保伦电子股份有限公司 | IP broadcasting system adopting AI (analog input) sound |
CN116645954B (en) * | 2023-07-27 | 2023-11-17 | 广东保伦电子股份有限公司 | IP broadcasting system adopting AI (analog input) sound |
Also Published As
Publication number | Publication date |
---|---|
EP4143820A1 (en) | 2023-03-08 |
US20230122824A1 (en) | 2023-04-20 |
WO2021247012A1 (en) | 2021-12-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11527174B2 (en) | System to evaluate dimensions of pronunciation quality | |
US20230122824A1 (en) | Method and system for user-interface adaptation of text-to-speech synthesis | |
KR102496817B1 (en) | Method and apparatus for training model, method and apparatus for synthesizing speech, device and storage medium | |
US8527276B1 (en) | Speech synthesis using deep neural networks | |
US20210350786A1 (en) | Speech Recognition Using Unspoken Text and Speech Synthesis | |
US8571871B1 (en) | Methods and systems for adaptation of synthetic speech in an environment | |
US9183830B2 (en) | Method and system for non-parametric voice conversion | |
US10043519B2 (en) | Generation of text from an audio speech signal | |
US20050182630A1 (en) | Multilingual text-to-speech system with limited resources | |
US20220392430A1 (en) | System Providing Expressive and Emotive Text-to-Speech | |
CN108520650A (en) | A kind of intelligent language training system and method | |
JP7228998B2 (en) | speech synthesizer and program | |
WO2021212954A1 (en) | Method and apparatus for synthesizing emotional speech of specific speaker with extremely few resources | |
US11676572B2 (en) | Instantaneous learning in text-to-speech during dialog | |
US8452603B1 (en) | Methods and systems for enhancement of device accessibility by language-translated voice output of user-interface items | |
Wang et al. | Computer-assisted audiovisual language learning | |
US20230111824A1 (en) | Computing system for unsupervised emotional text to speech training | |
CN113421550A (en) | Speech synthesis method, device, readable medium and electronic equipment | |
Seljan et al. | Automatic word-level evaluation and error analysis of formant speech synthesis for Croatian | |
KR20030079497A (en) | service method of language study | |
US20230335111A1 (en) | Method and system for text-to-speech synthesis of streaming text | |
Houidhek et al. | Dnn-based speech synthesis for arabic: modelling and evaluation | |
CN112750423B (en) | Personalized speech synthesis model construction method, device and system and electronic equipment | |
TWM621764U (en) | A system for customized speech | |
CN114420084A (en) | Voice generation method, device and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |