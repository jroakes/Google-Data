CN115699021A - Subtask adaptive neural network - Google Patents
Subtask adaptive neural network Download PDFInfo
- Publication number
- CN115699021A CN115699021A CN202080101932.9A CN202080101932A CN115699021A CN 115699021 A CN115699021 A CN 115699021A CN 202080101932 A CN202080101932 A CN 202080101932A CN 115699021 A CN115699021 A CN 115699021A
- Authority
- CN
- China
- Prior art keywords
- subtask
- combined
- neural network
- specific
- mask
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/764—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using classification, e.g. of video objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
Abstract
In training, the base neural network may be trained to perform each of a plurality of base subtasks included in the overall set of base subtasks (e.g., individually or in some combination thereof). Next, a description of the desired combined subtasks may be obtained. Based on the description of the combined subtasks, the mask generator may generate a pruning mask that is used to prune the underlying neural network into a smaller combined subtask-specific network that only performs two or more of the basic subtasks included in the combined subtask.
Description
Technical Field
The present disclosure relates generally to machine learning. More particularly, the present disclosure relates to systems and methods for generating, training and using subtask-adaptive neural networks that include an underlying network of dynamic and controllable subsets that may be adapted to efficiently perform different subtasks.
Background
Over the past few years, several neural network architectures have been proposed that consistently improve the prior art performance of various computer vision tasks. In general, the size of the deep network for a task depends on the complexity of the task. Thus, a significantly larger network is required to identify 1000 classes than to identify only 10 classes.
However, given a deep network that has been trained to recognize a large number of classes, people need to run the entire network even when they are interested in only a small subset of the classes that can potentially be recognized using a much smaller network. This often occurs in several real-world use cases of deep networks, where a unified model is built to serve multiple applications.
Thus, while having a unified model provides flexibility to serve several applications, this results in an inference time inefficiency for applications that are only interested in a subset of the categories. This often prevents these applications from using large unified models due to computational budget or other resource constraints.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a computing system having a subtask-adaptive neural network. The computing system includes one or more processors and one or more non-transitory computer-readable media collectively storing: a subtask-adaptive neural network comprising a base neural network, the base neural network comprising a plurality of parameters, wherein the base neural network has been trained to perform each of a plurality of base subtasks included in a total set of base subtasks; and instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations include receiving data describing a combined subtask that includes two or more basic subtasks, the combined subtask being a subset of a total set of basic subtasks for which the underlying neural network has been trained. The operations include generating a combined subtask-specific pruning mask based on the combined subtask, where the combined subtask-specific pruning mask identifies a subset of the plurality of parameters of the underlying neural network to be pruned. The operations include pruning a subset of the plurality of parameters of the underlying neural network identified by the combined subtask-specific pruning mask to generate a combined subtask-specific neural network. The operations include, after pruning the subset of the plurality of parameters, providing a combined subtask-specific neural network for execution of the combined subtask.
Another example aspect of the disclosure relates to a computer-implemented method for training a subtask-adaptive neural network. The method includes obtaining, by a computing system including one or more computing devices, a subtask-adaptive neural network that includes an underlying neural network, the underlying neural network including a plurality of parameters. Portions of the method may be performed for multiple training iterations. The method includes receiving, by a computing system, data describing a combined subtask that includes two or more basic subtasks, the combined subtask being a subset of an overall set of the plurality of basic subtasks. The method includes generating, by the computing system, a combined subtask-specific pruning mask based on the combined subtask, wherein the combined subtask-specific pruning mask identifies a subset of the plurality of parameters of the underlying neural network to prune. The method includes pruning, by the computing system, a subset of the plurality of parameters of the underlying neural network identified by the combined subtask-specific pruning mask to generate a combined subtask-specific neural network. The method includes generating, by the computing system, an output based on the training input using the combined subtask-specific neural network after pruning the subset of the plurality of parameters. The method includes evaluating, by a computing system, a loss function that evaluates an output. The method includes modifying, by the computing system, one or more values of one or more of the parameters of the underlying neural network based at least in part on the loss function. The respective combined subtasks for at least two of the plurality of training iterations may be different from each other.
Another example aspect of the present disclosure is directed to one or more non-transitory computer-readable media collectively storing a combined subtask-specific neural network generated from an underlying neural network. The combined subtask-specific neural network is configured to perform a combined subtask that includes two or more basic subtasks. The combined subtasks are a subset of the total set of basic subtasks on which the underlying neural network is trained. The combined subtask-specific neural network is generated via pruning a subset of the plurality of parameters of the underlying neural network identified by the combined subtask-specific pruning mask associated with the combined subtask. The non-transitory computer-readable medium also collectively stores instructions that, when executed by the one or more computing devices, cause the one or more computing devices to use the combined subtask-specific neural network for execution of the combined subtask.
Other aspects of the disclosure relate to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification with reference to the drawings, in which:
1A-C provide graphical depictions of example pruning of an underlying network for different subtasks, according to example embodiments of the present disclosure.
FIG. 2 provides a block diagram of an example subtask adaptive network, according to an example embodiment of the present disclosure.
Fig. 3 provides a block diagram of an example convolutional layer in accordance with an example embodiment of the present disclosure.
Fig. 4 provides a flowchart of an example method for generating a combined subtask-specific neural network, according to an example embodiment of the present disclosure.
Fig. 5 provides a flowchart of an example method of training a subtask adaptive neural network, according to an example embodiment of the present disclosure.
Fig. 6A depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Fig. 6B depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Fig. 6C depicts a block diagram of an example computing device, according to an example embodiment of the present disclosure.
Repeated reference numbers in multiple figures are intended to identify similar features in various embodiments.
Detailed Description
1. Summary of the invention
In general, the present disclosure is directed to a Subtask Adaptive Network (SAN) that includes a unified underlying neural network and a subtask-specific pruning mask generator. In training, the base neural network may be trained to perform each of a plurality of base subtasks included in the overall set of base subtasks (e.g., individually or in some combination thereof). As one example, the plurality of basic subtasks may be a plurality of recognition tasks respectively associated with a plurality of different object classes (e.g., giraffes, elephants, ostriches, dinosaurs, and parachutes). Next, a description of one or more desired subtasks may be obtained. For example, a combined subtask may include two or more of the basic subtasks (e.g., a subset of the categories of interest, such as giraffes and elephants only, rather than ostriches, dinosaurs, and parachutes). Based on the description of the combined subtasks, the mask generator may generate a pruning mask that is used to prune the underlying neural network into a smaller combined subtask-specific network that only performs one or more of the basic subtasks included in the subtasks. In this way, the combined subtask-specific network may be used to more efficiently perform inference of only the desired task. Further, the base recognition network and the mask generator may be learned together using end-to-end training.
1A-C provide example illustrations of dynamic subtask-specific network pruning in an example context of object recognition. In particular, fig. 1A depicts an underlying neural network configured to perform a plurality of basic subtasks, which in this example correspond to different object recognition tasks. Fig. 1B shows a first subtask-specific network that has been pruned to perform only a first subtask, which in this example corresponds to recognizing dogs and cats. As another example, fig. 1C shows a second subtask-specific network that has been pruned to perform a second set of subtasks, which in this example corresponds to recognizing a dog, a cat, and a person. Thus, the underlying network may be dynamically pruned based on a particular and dynamically selectable set of basic subtasks.
The system and method of the present disclosure first provides a sub-task specific pruning method that is dynamic and controllable. More specifically, while the possibility of task-specific network pruning has been explored in earlier work, most of these work focused on pruning for a single defined task, rather than on a dynamic and controllable subset of tasks. Thus, in contrast to existing approaches, the systems and methods described herein can train a single network that can be pruned (e.g., not retrained) for different subtasks using a learned subtask-specific mask generator at inference time.
Another relevant but distinct work route is multi-task learning (MTL), which focuses on training a single large network for several different tasks. Unlike MTL, the systems and methods of the present disclosure reduce computational cost by pruning the network when an application is interested in only a subset or proper subset (e.g., a subset of classes or categories) of tasks that can be performed by the entire network.
The system and method of the present disclosure provide a number of technical effects and benefits. As one example, the systems and methods of the present disclosure provide a significantly more efficient inference-time model than typical MTL methods. More specifically, past MTL methods train a single large model to perform multiple different tasks. However, at the time of inference, the entire large model is then used to perform the inference, even when it is actually desirable to perform only a subset of the multiple different tasks, which is an inefficient approach and undesirably consumes computational resources such as processor usage, memory usage, network bandwidth, and the like. In contrast, the present disclosure provides techniques to generate pruning masks for pruning the underlying neural network into smaller combined subtask-specific networks that perform only a subset of the tasks included in a given combined subtask, thereby saving computational resources such as processor usage, memory usage, network bandwidth, and the like. Thus, a smaller combined subtask-specific network may be suitable for resource-constrained devices (e.g., portable computing devices including smartphones, tablets, wearable pieces, etc.) on which it is not possible or appropriate to use a fully-based neural network.
As another example technical effect and benefit, the present disclosure provides significant resource savings as compared to methods that train, store, and use separate models for each individual task. For example, for 100 tasks, one existing approach would be to train, store and use 100 different models. The present disclosure represents a significant advance over this approach by performing only a single training of a single underlying network and then using the proposed pruning approach to generate a model (e.g., in advance or in real-time) that is capable of performing any subset of the tasks. Thus, the computational resource allocation for model training may be reduced (e.g., from one hundred individual training events to a single training event).
As another example, the present disclosure also provides an end result that is more flexible and has higher performance than existing approaches: models may be generated for any combined subset of one or more tasks, and the generated models will benefit from cross-task learning, potentially resulting in higher performance.
Referring now to the drawings, example embodiments of the disclosure will be discussed in more detail.
2. Example subtask adaptive network
This section describes an example Subtask Adaptive Network (SAN). Specifically, by way of example, FIG. 2 depicts an example SAN 200 that includes the following two primary components: a unified basic neural network 202, which may be referred to as BaseNet, and a subtask-specific pruning mask generator 204. The same underlying neural network 202 may be pruned (e.g., using masks generated by the generator 204) for different subtasks without retraining.
In the following section, an example SAN is described in an example context/use case, where different basic subtasks correspond to different classes of objects to be recognized. This is just one example arrangement. The systems and methods described herein may be applied to any different form of task and any different form of input data modality (e.g., input data representing text, input data representing audio, etc.).
2.1 example basic neural networks
One example, baseNet 202, is a convolutional neural network that uses a class-specific (e.g., sigmoid) function in the output layer to produce a score for each of a plurality of classes (e.g., in the range of [0,1 ]). Let N denote the total number of convolution filters in all layers of the basic discriminatory network.
2.2 example pruning mask Generator
For the example SAN 200, basic subtasks may be defined as a subset of tasks (e.g., categories) that will always be performed (e.g., recognized) together. Thus, a basic subtask may include one or more tasks, such as one or more (e.g., two, three, etc.) different sets of objects that will always be recognized together. A combined subtask may be defined as a combination of two or more basic subtasks.
Based on these definitions, all subtasks supported by the SANSet of (2)
Referring again to the example shown in FIG. 2, for the subtasksmask generator 204 produces a binary mask 208 that consists of a binary value for each convolution filter in the BaseNet 202 that indicates whether that particular filter should be on or off for the subtask 206. Although mask 208 is described herein as providing a value for each filter of the network, other example masks may provide a corresponding value for each layer of the network, each parameter of the network, or other division of parameters of the network.
In some example embodiments, to produce the binary mask 208, the generator 204 first produces a positive-valued mask called a utility mask
in some example embodiments, mask generator 204 may generate a separately learned utility mask u for each basic subtask T . Mask generator 204 may combine these base utility masks to generate utility masks for the combined subtasks. For example, given for the subtasks T respectively 1 And T 2 Utility mask of
Given the binarized utility mask 208 for task T206, a pruning operation 210 may be performed to turn off (or omit) the filter whose corresponding mask elements are zero, thereby obtaining a corresponding subtask-specific network 212. After pruning, the subtask-specific network 212 may be used to process the input image 214 to generate the label scores 216 only for the categories included in the subtask 206.
In some embodiments, since the filters in the shutdown layer affect the total amplitude of the input to the next layer, to compensate for this, a subtask-specific scaling mask S is used T Which adds a per-channel scaling factor to the input of all convolutional layers (e.g., except the first convolutional layer that takes the image as input), as shown in fig. 3. In particular, as shown in fig. 3, in some embodiments, each convolutional layer may have a subtask-specific scaling and a utility mask associated therewith. In some implementations, in a residual network, the same utility mask may be used for all inputs that contribute to the residual operation.
Thus, referring again to fig. 2, similar to utility masks, in some example embodiments, pruning mask generator 204 may use a separate learned scaling mask S for each basic subtask T And a scaling mask for the combined subtask can be generated by combining the basic scaling masks. For example, given for the subtasks T respectively 1 And T 2 Scaled mask of
where MaxAbs represents an element-by-element operation that selects a value with a larger magnitude while preserving its sign. Thus, in some embodiments, the overall pruning mask M generated by the mask generator 204 for the subtask T206 T 208 may include a utility mask U T And scaling the mask S T 。
While this section describes example embodiments of pruning mask generator 204 that directly optimize/learn the per-task utility mask and/or the scaling mask, other example embodiments of the present disclosure include more complex mask generator 204. As one example, the example pruning mask generator 204 may include one or more neural networks that directly regress the subtask-specific masks. For example, the optimal values of the parameters of such a mask generating neural network may be learned in a similar manner as how the optimal values for the mask are learned in the direct learning embodiment.
In some embodiments, the generation of the subtask-specific network 212 shown in FIG. 2 may be performed in real-time in response to the receipt of the input image 214 and the description of the subtask 206. In other words, the generation/pruning process may be performed while it is desirable to run the network 212 to generate the label score 216 from the input image 214. The generating/pruning may be performed by the same device or a different device that runs the network 212 to generate the label score 216 from the input image 214.
In other embodiments, the generation of the subtask-specific network 212 shown in FIG. 2 may be performed before the planned use of the network 212 for inference, and then stored for later use. In other words, the generation/pruning process may be performed at some time prior to the time at which execution of the network 212 is desired to generate the label score 216 from the input image 214, and the network 212 may be stored and then used later. The generating/pruning may be performed by the same device or a different device that runs the network 212 to generate the label score 216 from the input image 214. For example, the generating/pruning may be performed by a central entity (e.g., a server device or a service layer or platform) as a service for a client (e.g., a remote device, an application, etc.). For example, the server device may perform the generation/pruning process and provide a resulting network for use at the remote device. In another example, the generation/pruning process may be performed by a service layer or component of the device (which may be referred to as a "pruning engine") and may be provided for use by applications that also reside on the device.
2.3 example loss function:
let C denote the set of all tasks (e.g., categories) and | C | denote their cardinality. Is provided with
An example task loss for a subtask T may be defined as
Where α = (| C | + (β -1) | T |)/| C | is the normalization factor and CE is the standard binary cross-entropy loss.
While the full task penalty gives equal weight to all categories, the subtask penalty gives higher weight to the category that defines the subtask (e.g., β is set to a large value). Including categories that are not part of subtasks with small weights improves the stability and speed of training.
2.4 example training protocol
During training, by training the SAN in an end-to-end manner, the utility and scaling masks for the underlying subtasks can be learned together { (U) T ，S T ) Parameter W for BaseNet. Order to
wherein, | | U T || 1 Is a sparsity induction that encourages network pruning 1 Loss, and γ is a hyper-parameter that can be used to vary the amount of clipping.
Note that the proposed mask generator uses a threshold operation, the derivative of which is zero everywhere. Thus, to train the network using a standard gradient-based approach, a straight-through estimator can be used while propagating the gradient backwards. Further, in some embodiments, utility mask U T Is limited to positive. This may be accomplished by replacing the positive value U with a real-valued variable T And handled using absolute value operators in the forward and backward propagation.
In some embodiments, at the start of training, a zero-mean gaussian with standard deviation σ may be used to initialize the scaling mask with all 1's and the utility mask. The value of σ that affects the magnitude of the initial utility mask can thus bias the model toward high/low clipping quantities. Therefore, together with γ, σ can also be used to change the clipping amount.
In some implementations, the loss function (e.g., equation (5)) can be propagated back through any combination of the subtask specific network 212, the base network 202, and/or the pruning mask generator 204. In some embodiments, the modifications to the subtask-specific network 212 and the base network 202 may be averaged. Modifications to the subtask-specific network 212 may be returned or passed to the base network 202.
2.4.1 exemplary lost sampling technique
In some scenarios, the above-described penalty function may include a large number (an index of the number of basic subtasks) of penalty terms
Example inference schemes
When BaseNet uses the batch normalization layer, the mean and variance statistics used for batch normalization vary on a subtask basis due to turning off some filters. Therefore, before using the network to infer a subtask, subtask-specific batch normalized mean and variance statistics need to be computed by running the corresponding pruning network on the set of images. While this adds an extra step between the training and inference phases, the additional overhead is negligible compared to training separate models for each subtask.
3. Example method
Fig. 4 depicts a flowchart of an example method for generating a combined subtask private network according to an example embodiment of the present disclosure. Although fig. 4 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of the method 400 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 402, a computing system may receive data describing a combined subtask including two or more basic subtasks, the combined subtask being a subset of a total set of basic subtasks for which an underlying neural network has been trained.
In some implementations, the plurality of basic subtasks are a plurality of recognition tasks respectively associated with a plurality of different object classes. In some such embodiments, the data describing the combined subtask identifies two or more of the plurality of different object classes. In some embodiments, at least one of the plurality of basic subtasks includes classifying the input into two or more different classes.
At 404, the computing system may generate a combined subtask-specific pruning mask based on the combined subtask, where the combined subtask-specific pruning mask identifies a subset of the plurality of parameters of the underlying neural network to be pruned.
In some implementations, generating the combined subtask-specific pruning mask based on the combined subtask may include accessing, from storage, two or more predetermined utility masks respectively associated with two or more basic subtasks included in the combined subtask. In some embodiments, generating the combined subtask-specific pruning mask based on the combined subtask may include combining two or more predetermined utility masks to generate the combined subtask-specific utility mask. In some implementations, combining the two or more predetermined utility masks to generate the combined subtask-specific utility mask includes performing a maximum operator for the two or more predetermined utility masks on an element-by-element basis.
In some embodiments, generating the combined subtask-specific pruning mask based on the combined subtask may include binarizing the combined subtask-specific utility mask to generate the combined subtask-specific pruning mask. In some implementations, binarizing the combined subtask-specific utility mask to generate the combined subtask-specific pruning mask may include comparing, on an element-by-element basis, each value of the combined subtask-specific utility mask to a threshold, where values less than the threshold are set to 0 and values greater than the threshold are set to 1.
In some embodiments, the subtask-adaptive neural network further includes a pruning-mask-generating neural network. In some implementations, generating the combined subtask-specific pruning mask based on the combined subtask may include: inputting data describing the combined subtask into a pruning mask to generate a neural network; and receiving the combined subtask-specific pruning mask as an output of a pruning mask generating neural network, the pruning mask generating neural network generating the combined subtask-specific pruning mask based on data describing the combined subtask.
In some embodiments, the underlying neural network may be a convolutional neural network that includes a plurality of filters. In some embodiments, the combined subtask-specific pruning mask identifies a subset of the plurality of filters of the underlying neural network to prune on a filter-by-filter basis.
In some implementations, the method 400 may further include generating a combined subtask-specific scaling mask based on the combined subtask, where the combined subtask-specific scaling mask scales one or more of the plurality of parameters of the underlying neural network to offset an effect of the combined subtask-specific pruning mask. In some such implementations, generating the combined subtask-specific pruning mask at 404 may include scaling an intermediate version of the combined subtask-specific pruning mask according to the combined subtask-specific scaling mask.
Referring again to fig. 4, at 406, the computing system may prune a subset of the plurality of parameters of the underlying neural network identified by the combined subtask-specific pruning mask to generate a combined subtask-specific neural network. After pruning 406 the subset of the plurality of parameters, the computing system may provide 408 a combined subtask-specific neural network for execution of the combined subtask. In some embodiments, as a result of pruning, the data size of the combined subtask-specific neural network may be reduced compared to the underlying neural network because the combined subtask-specific neural network may omit the pruned parameters. This is beneficial for both storage and transmission of the combined subtask-specific neural network.
In some embodiments, providing the combined subtask-specific neural network for execution of the combined subtask includes transmitting the combined subtask-specific neural network to a computer application for storage and use by the computer application.
In some embodiments, method 400 is performed in real-time in response to an input signal describing a combined subtask. In some such embodiments, providing a combined subtask-specific neural network for performing the combined subtask may include using the combined subtask-specific neural network to perform the combined subtask in response to and concurrently with the receipt of the input signal.
Fig. 5 depicts a flowchart of an example method for training a subtask adaptive network according to an example embodiment of the present disclosure. Although fig. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of the method 500 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 502, a computing system may receive data describing a combined subtask that includes two or more basic subtasks, the combined subtask being a subset of a total set of basic subtasks that have trained an underlying neural network.
In some implementations, the plurality of basic subtasks are a plurality of recognition tasks respectively associated with a plurality of different object classes. In some such embodiments, the data describing the combined subtask identifies two or more of a plurality of different object classes. In some embodiments, at least one of the plurality of basic subtasks includes classifying the input into two or more different classes.
At 504, the computing system may generate a combined subtask-specific pruning mask based on the combined subtask, where the combined subtask-specific pruning mask identifies a subset of the plurality of parameters of the underlying neural network to be pruned.
In some implementations, generating the combined subtask-specific pruning mask based on the combined subtask may include accessing, from a store, two or more predetermined utility masks respectively associated with two or more basic subtasks included in the combined subtask. In some embodiments, generating the combined subtask-specific pruning mask based on the combined subtask may include combining two or more predetermined utility masks to generate the combined subtask-specific utility mask. In some implementations, combining the two or more predetermined utility masks to generate the combined subtask-specific utility mask includes performing a maximum operator for the two or more predetermined utility masks on an element-by-element basis.
In some implementations, generating the combined subtask-specific pruning mask based on the combined subtask may include binarizing the combined subtask-specific utility mask to generate the combined subtask-specific pruning mask. In some implementations, binarizing the combined subtask-specific utility mask to generate the combined subtask-specific pruning mask may include comparing, on an element-by-element basis, each value of the combined subtask-specific utility mask to a threshold, where values less than the threshold are set to 0 and values greater than the threshold are set to 1.
In some embodiments, the subtask-adaptive neural network further includes a pruning-mask-generating neural network. In some implementations, generating the combined subtask-specific pruning mask based on the combined subtask may include: inputting data describing the combined subtasks into a pruning mask generation neural network; the combined subtask-specific pruning mask is received as an output of a pruning mask generating neural network that generates the combined subtask-specific pruning mask based on data describing the combined subtask.
In some embodiments, the underlying neural network may be a convolutional neural network that includes a plurality of filters. In some embodiments, the combined subtask-specific pruning mask identifies a subset of the plurality of filters of the underlying neural network to prune on a filter-by-filter basis.
In some embodiments, the method 500 may further include generating a combined subtask-specific scaling mask based on the combined subtask, wherein the combined subtask-specific scaling mask scales one or more of the plurality of parameters of the underlying neural network to offset an effect of the combined subtask-specific pruning mask. In some such implementations, generating 504 the combined subtask-specific pruning mask may include scaling an intermediate version of the combined subtask-specific pruning mask according to the combined subtask-specific scaling mask.
Referring again to fig. 5, at 506, the computing system may prune a subset of the plurality of parameters of the underlying neural network identified by the combined subtask-specific pruning mask to generate a combined subtask-specific neural network. After pruning 506 the subset of the plurality of parameters, the computing system may generate 508 an output based on the training inputs using the combined subtask-specific neural network.
At 510, the computing system may evaluate a loss function that evaluates the output. In some embodiments, the loss function may include a sparsity inducing term that encourages network pruning.
In some embodiments, the output generated by the combined subtask-specific neural network may include a plurality of outputs for a plurality of basic subtasks, respectively. In some embodiments, the loss function may include: a subtask loss item that evaluates two or more outputs of the plurality of outputs respectively associated with two or more basic subtasks included in the combined subtask; and/or a residual penalty term that evaluates outputs associated with the base subtasks not included in the combined subtask, respectively. In some implementations, the subtask loss terms are weighted more heavily than the remaining loss terms.
At 512, the computing system may modify one or more values of one or more of the parameters of the underlying neural network based at least in part on the loss function.
At 514, the computing system may modify one or more values of one or more parameters of the pruning mask generator based at least in part on the loss function.
In some implementations, modifying one or more values of one or more parameters of the pruning mask generator may include modifying one or more values of two or more utility masks respectively associated with two or more basic subtasks included in the combined subtask based at least in part on a loss function.
In some implementations, modifying one or more values of one or more parameters of the pruning mask generator may include modifying one or more values of the pruning mask generation neural network based at least in part on the loss function.
After 514, the method 500 may optionally return to 512. Thus, in some embodiments, method 500 may be performed iteratively. The iterative loop may stop when one or more stopping criteria are met. The stopping criteria may be any number of different criteria including, for example, the loop counter reaching a predetermined maximum, iterations of the iterative change in parameter adjustment falling below a threshold, a gradient below a threshold, and/or various other criteria.
4. Example apparatus and System
FIG. 6A depicts a block diagram of an example computing system 100 providing a subtask adaptive network, according to an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled through a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming machine or game controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be one processor or operatively connected processors. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, etc., and combinations thereof. The memory 114 may store data 116 and instructions 118 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 can store or include one or more machine learning models 120. For example, the machine learning model 120 may be or may otherwise include various machine learning models, such as a neural network (e.g., a deep neural network) or other types of machine learning models, including non-linear models and/or linear models. The neural network may include a feed-forward neural network, a recurrent neural network (e.g., a long-short term memory recurrent neural network), a convolutional neural network, or other form of neural network. An example machine learning model 120 is discussed with reference to fig. 1A-5.
In some implementations, the one or more machine learning models 120 can be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of a single machine learning model 120 (e.g., perform parallel tasks across multiple instances of input).
Additionally or alternatively, one or more machine learning models 140 may be included in the server computing system 130, or stored and implemented by the server computing system 130, the server computing system 130 communicating with the user computing device 102 according to a client-server relationship. For example, the machine learning model 140 may be implemented by the server computing system 140 as part of a web service (e.g., a network adaptation service). Thus, one or more models 120 may be stored and implemented at the user computing device 102, and/or one or more models 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device by which a user may provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and may be an operatively connected processor or processors. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, disks, etc., and combinations thereof. Memory 134 may store data 136 and instructions 138 that are executed by processor 132 to cause server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances where the server computing system 130 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine learning models 140. For example, the model 140 may be or may otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layered nonlinear models. Example neural networks include feed-forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Example models are discussed with reference to fig. 1A-5.
The user computing device 102 and/or the server computing system 130 may train the models 120 and/or 140 via interaction with a training computing system 150, the training computing system 150 communicatively coupled through a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
In some implementations, performing back propagation of errors can include performing truncated back propagation through time. The model trainer 160 may perform a variety of generalization techniques (e.g., weight decay, tentative regression, etc.) to improve the generalization capability of the trained model.
In particular, the model trainer 160 may train the machine-learned models 120 and/or 140 based on a set of training data 162. In some implementations, the training examples may be provided by the user computing device 102 if the user has provided consent. Thus, in such implementations, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some instances, this process may be referred to as a personalization model.
The model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some embodiments, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other embodiments, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
In some implementations, the input to the machine learning model of the present disclosure may be image data. The machine learning model may process the image data to generate an output. As an example, the machine learning model may process the image data to generate an image recognition output (e.g., recognition of the image data, potential embedding of the image data, an encoded representation of the image data, a hash of the image data, etc.). As another example, the machine learning model may process the image data to generate an image segmentation output. As another example, the machine learning model may process the image data to generate an image classification output. As another example, the machine learning model may process the image data to generate an image data modification output (e.g., a change in the image data, etc.). As another example, the machine learning model may process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine learning model may process the image data to generate an enlarged image data output. As another example, the machine learning model may process the image data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure may be text or natural language data. The machine learning model may process text or natural language data to generate an output. As an example, the machine learning model may process natural language data to generate a language coded output. As another example, the machine learning model may process text or natural language data to generate potential text-embedded output. As another example, a machine learning model may process textual or natural language data to generate translation output. As another example, the machine learning model may process text or natural language data to generate a classification output. As another example, the machine learning model may process text or natural language data to generate a text segmentation output. As another example, a machine learning model may process text or natural language data to generate a semantic intent output. As another example, the machine learning model may process text or natural language data to generate an amplified text or natural language output (e.g., higher quality text or natural language data than input text or natural language, etc.). As another example, a machine learning model may process textual or natural language data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure may be speech data. The machine learning model may process the speech data to generate an output. As an example, the machine learning model may process speech data to generate a speech recognition output. As another example, the machine learning model may process speech data to generate a speech translation output. As another example, the machine-learned model may process speech data to generate potential embedded output. As another example, the machine learning model may process speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.). As another example, the machine learning model may process the speech data to generate an amplified speech output (e.g., higher quality speech data than the input speech data, etc.). As another example, the machine learning model may process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, a machine learning model may process speech data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure may be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model may process the latent coding data to generate an output. As an example, the machine learning model may process the latent coding data to generate the recognition output. As another example, the machine learning model may process the potential encoding data to generate a reconstructed output. As another example, the machine learning model may process the latent coding data to generate a search output. As another example, the machine learning model may process the latent coding data to generate a re-aggregated output. As another example, the machine learning model may process the potentially encoded data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be statistical data. The machine learning model may process the statistical data to generate an output. As an example, the machine learning model may process the statistics to generate the recognition output. As another example, the machine learning model may process the statistics to generate a prediction output. As another example, the machine learning model may process the statistics to generate a classification output. As another example, the machine learning model may process the statistics to generate a segmented output. As another example, the machine learning model may process the statistics to generate a segmented output. As another example, the machine learning model may process the statistics to generate a visual output. As another example, the machine learning model may process the statistics to produce a diagnostic output.
In some embodiments, the input to the machine learning model of the present disclosure may be sensor data. The machine learning model may process the sensor data to generate an output. As one example, a machine-learned model may process sensor data to generate a recognition output. As another example, the machine learning model may process sensor data to generate a predicted output. As another example, the machine learning model may process the sensor data to generate a classification output. As another example, the machine learning model may process the sensor data to generate a segmented output. As another example, the machine learning model may process the sensor data to generate a segmented output. As another example, the machine-learned model may process sensor data to generate a visual output. As another example, the machine learning model may process sensor data to generate a diagnostic output. As another example, the machine learning model may process sensor data to generate a detection output.
FIG. 6A illustrates one example computing system that may be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such implementations, the model 120 may be trained and used locally at the user computing device 102. In some of these implementations, the user computing device 102 may implement a model trainer 160 to personalize the model 120 based on user-specific data.
Fig. 6B depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
As shown in fig. 6B, each application can communicate with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some embodiments, the APIs used by each application are specific to that application.
Fig. 6C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
The central smart inlay includes a number of machine learning models. For example, as shown in fig. 6C, a respective machine learning model (e.g., model) may be provided to each application and managed by the central intelligence layer. In other embodiments, two or more applications may share a single machine learning model. For example, in some embodiments, a central smart inlay may provide a single model (e.g., a single model) for all applications. In some embodiments, the central smart inlay is included within or implemented by the operating system of the computing device 50.
The central smart inlay may communicate with a central device data plane. The central device data layer may be a centralized data repository for the computing device 50. As shown in fig. 1C, the central device data layer may communicate with many other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).
5. Additional disclosure
The technology discussed herein relates to servers, databases, software applications, and other computer-based systems, and the actions taken and information sent to and received from these systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between and among the various components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with reference to various specific exemplary embodiments thereof, each example is provided by way of illustration and not limitation of the present disclosure. Those skilled in the art, having the benefit of the foregoing description, may effect numerous variations, modifications, and equivalents from such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Thus, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computing system having a subtask-adaptive neural network, the computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media collectively storing:
a subtask-adaptive neural network including a base neural network having a plurality of parameters, wherein the base neural network has been trained to perform each of a plurality of base subtasks included in a total set of base subtasks; and
instructions that, when executed by the one or more processors, cause the computing system to perform operations comprising:
receiving data describing a combined subtask consisting of two or more basic subtasks, the combined subtask being a subset of the total set of basic subtasks for which the basic neural network has been trained;
generating a combined subtask-specific pruning mask based on the combined subtask, wherein the combined subtask-specific pruning mask identifies a subset of the plurality of parameters of the underlying neural network to prune;
pruning the subset of the plurality of parameters of the base neural network identified by the combined subtask-specific pruning mask to generate a combined subtask-specific neural network; and
after pruning the subset of the plurality of parameters, providing the combined subtask-specific neural network for execution of the combined subtask.
2. The computing system of claim 1, wherein to generate the combined subtask-specific pruning mask based on the combined subtask comprises to:
accessing, from storage, two or more predetermined utility masks respectively associated with the two or more basic subtasks included in the combined subtask;
combining the two or more predetermined utility masks to generate a combined subtask-specific utility mask; and
binarizing the combined subtask-specific utility mask to generate the combined subtask-specific pruning mask.
3. The computing system of claim 2, wherein combining the two or more predetermined utility masks to generate the combined subtask-specific utility mask includes performing a maximum operator for the two or more predetermined utility masks on an element-by-element basis.
4. The computing system of claim 2 or claim 3, wherein binarizing the combined subtask-specific utility mask to generate the combined subtask-specific pruning mask includes comparing, on an element-by-element basis, each value of the combined subtask-specific utility mask to a threshold, wherein values less than the threshold are set to 0 and values greater than the threshold are set to 1.
5. The computing system of claim 1, wherein:
the subtask adaptive neural network further comprises a pruning mask generation neural network; and
generating the combined subtask-specific pruning mask based on the combined subtask comprises:
inputting the data describing the combined subtasks into the pruning mask generating neural network; and
receiving the combined subtask-specific pruning mask as an output of the pruning mask generating neural network, the combined subtask-specific pruning mask being generated by the pruning mask generating neural network based on the data describing the combined subtask.
6. The computing system of any preceding claim, wherein:
the base neural network comprises a convolutional neural network comprising a plurality of filters; and
the combined subtask-specific pruning mask identifies a subset of the plurality of filters of the base neural network to prune on a filter-by-filter basis.
7. The computing system of any preceding claim, wherein:
the operations further comprise generating a combined subtask-specific scaling mask based on the combined subtask, wherein the combined subtask-specific scaling mask scales one or more of the plurality of parameters of the underlying neural network to offset an effect of the combined subtask-specific pruning mask; and
generating the combined subtask-specific pruning mask based on the combined subtask includes scaling an intermediate version of the combined subtask-specific pruning mask according to the combined subtask-specific scaling mask.
8. The computing system of any preceding claim, wherein:
the plurality of basic subtasks includes a plurality of recognition tasks respectively associated with a plurality of different object classes; and
the data describing the combination subtask identifies two or more of the plurality of different object classes.
9. The computing system of any preceding claim, wherein at least one of the plurality of basic subtasks comprises classifying an input into two or more different classes.
10. The computing system of any preceding claim, wherein providing the combined subtask-specific neural network for execution of the combined subtask comprises communicating the combined subtask-specific neural network to a computer application for storage and use by the computer application.
11. The computing system of any preceding claim, wherein:
the operations are performed in real-time in response to an input signal describing the combined subtask; and
providing the combined subtask-specific neural network for execution of the combined subtask includes using the combined subtask-specific neural network to execute the combined subtask in response to and concurrently with receipt of the input signal.
12. A computer-implemented method for training a subtask-adaptive neural network, the method comprising:
obtaining, by a computing system comprising one or more computing devices, a subtask adaptive neural network comprising a base neural network, the base neural network comprising a plurality of parameters;
for each of a plurality of training iterations:
receiving, by the computing system, data describing a combined subtask that includes two or more basic subtasks, the combined subtask being a subset of a total set of the plurality of basic subtasks;
generating, by the computing system, a combined subtask-specific pruning mask based on the combined subtask, wherein the combined subtask-specific pruning mask identifies a subset of the plurality of parameters of the underlying neural network to prune;
pruning, by the computing system, the subset of the plurality of parameters of the base neural network identified by the combined subtask-specific pruning mask to generate a combined subtask-specific neural network;
after pruning the subset of the plurality of parameters, generating, by the computing system, an output based on training inputs using the combined subtask-specific neural network;
evaluating, by the computing system, a loss function that evaluates the output; and
modifying, by the computing system, one or more values of one or more of the parameters of the base neural network based at least in part on the loss function;
wherein the respective combined subtasks for at least two of the plurality of training iterations are different from each other.
13. The computer-implemented method of claim 12, wherein, for each training iteration:
generating, by the computing system, the combined subtask-specific pruning mask based on the combined subtask includes:
accessing, by the computing system from storage, two or more utility masks respectively associated with the two or more basic subtasks included in the combined subtask;
combining, by the computing system, the two or more utility masks to generate a combined subtask-specific utility mask; and
binarizing, by the computing system, the combined subtask-specific utility mask to generate the combined subtask-specific pruning mask; and
the method further comprises the following steps: modifying, by the computing system, one or more values of the two or more utility masks respectively associated with the two or more basic subtasks included in the combined subtask based at least in part on the loss function.
14. The computer-implemented method of claim 12, wherein:
the subtask adaptive neural network further comprises a pruning mask generation neural network;
for each training iteration, generating, by the computing system, the combined subtask-specific pruning mask based on the combined subtask includes:
inputting, by the computing system, the data describing the combined subtasks into the pruning mask generating neural network; and
receiving, by the computing system, the combined subtask-specific pruning mask as an output of the pruning mask generating neural network, the combined subtask-specific pruning mask being generated by the pruning mask generating neural network based on the data describing the combined subtask; and
the method further comprises the following steps: modifying, by the computing system, one or more values of one or more parameters of the pruning mask-generating neural network based at least in part on the loss function.
15. The computer-implemented method of any of claims 12-14, wherein the loss function includes a sparsity inducing term that encourages network pruning.
16. The computer-implemented method of any of claims 12-15, wherein:
the output generated by the combined subtask-specific neural network includes a plurality of outputs for the plurality of basic subtasks, respectively;
the loss function includes:
a subtask loss item that evaluates the two or more of the plurality of outputs respectively associated with the two or more basic subtasks included in the combined subtask; and
a residual loss item that evaluates outputs respectively associated with basic subtasks that are not included in the combined subtask; and
the subtask lost items are weighted more heavily than the remaining lost items.
17. The computer-implemented method of any of claims 12-16, wherein:
the base neural network comprises a convolutional neural network comprising a plurality of filters; and
the combined subtask-specific pruning mask identifies a subset of the plurality of filters of the underlying neural network to prune on a filter-by-filter basis.
18. The computer-implemented method of any of claims 12-17, wherein:
for each training iteration, the method further includes generating, by the computing system, a combined subtask-specific scaling mask based on the combined subtask, wherein the combined subtask-specific scaling mask scales one or more of the plurality of parameters of the underlying neural network to offset an effect of the combined subtask-specific pruning mask; and
generating, by the computing system, the combined subtask-specific pruning mask based on the combined subtask comprises scaling, by the computing system, an intermediate version of the combined subtask-specific pruning mask according to the combined subtask-specific scaling mask.
19. The computer-implemented method of any of claims 12-18, wherein:
the plurality of basic subtasks includes a plurality of recognition tasks respectively associated with a plurality of different object classes; and
the data describing the composition subtask identifies two or more of the plurality of different object classes.
20. One or more tangible, non-transitory computer-readable media collectively storing:
a combined subtask-specific neural network generated from the base neural network;
wherein the combined subtask-specific neural network is configured to perform a combined subtask that includes two or more basic subtasks;
wherein the combined subtasks are a subset of a total set of basic subtasks on which the base neural network was trained; and
wherein the combined subtask-specific neural network is generated via pruning of a subset of the plurality of parameters of the base neural network, the subset identified by a combined subtask-specific pruning mask associated with the combined subtask; and
instructions that, when executed by one or more computing devices, cause the one or more computing devices to use the combined subtask-specific neural network for execution of the combined subtask.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/036973 WO2021251960A1 (en) | 2020-06-10 | 2020-06-10 | Subtask adaptable neural network |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115699021A true CN115699021A (en) | 2023-02-03 |
Family
ID=71944234
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080101932.9A Pending CN115699021A (en) | 2020-06-10 | 2020-06-10 | Subtask adaptive neural network |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230214656A1 (en) |
EP (1) | EP4136585A1 (en) |
CN (1) | CN115699021A (en) |
WO (1) | WO2021251960A1 (en) |
-
2020
- 2020-06-10 CN CN202080101932.9A patent/CN115699021A/en active Pending
- 2020-06-10 WO PCT/US2020/036973 patent/WO2021251960A1/en unknown
- 2020-06-10 EP EP20750832.6A patent/EP4136585A1/en active Pending
- 2020-06-10 US US18/009,629 patent/US20230214656A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2021251960A1 (en) | 2021-12-16 |
EP4136585A1 (en) | 2023-02-22 |
US20230214656A1 (en) | 2023-07-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11922322B2 (en) | Exponential modeling with deep learning features | |
Simeone | A brief introduction to machine learning for engineers | |
CN111368996B (en) | Retraining projection network capable of transmitting natural language representation | |
US11475277B2 (en) | Accurate and interpretable classification with hard attention | |
KR20210029785A (en) | Neural network acceleration and embedding compression system and method including activation sparse | |
US20180268286A1 (en) | Neural network cooperation | |
CN111523640B (en) | Training method and device for neural network model | |
CN113692594A (en) | Fairness improvement through reinforcement learning | |
CN114830133A (en) | Supervised contrast learning with multiple positive examples | |
US11423307B2 (en) | Taxonomy construction via graph-based cross-domain knowledge transfer | |
CN114424215A (en) | Multitasking adapter neural network | |
CN113826125A (en) | Training machine learning models using unsupervised data enhancement | |
US20190228297A1 (en) | Artificial Intelligence Modelling Engine | |
US11934416B2 (en) | Task and process mining by robotic process automations across a computing environment | |
Li et al. | Halo: Hardware-aware learning to optimize | |
EP3916597A1 (en) | Detecting malware with deep generative models | |
US20230267315A1 (en) | Diffusion Models Having Improved Accuracy and Reduced Consumption of Computational Resources | |
CN116264847A (en) | System and method for generating machine learning multitasking models | |
US20210383237A1 (en) | Training Robust Neural Networks Via Smooth Activation Functions | |
US11790236B2 (en) | Minimum deep learning with gating multiplier | |
WO2021012263A1 (en) | Systems and methods for end-to-end deep reinforcement learning based coreference resolution | |
Eremeev et al. | Integrated approach for data mining based on case-based reasoning and neural networks | |
US20240029088A1 (en) | System for customer churn prediction and prevention | |
US20220004904A1 (en) | Deepfake detection models utilizing subject-specific libraries | |
US20220269936A1 (en) | Knowledge graphs in machine learning decision optimization |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |