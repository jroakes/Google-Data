US9176889B1 - Virtual machine memory management - Google Patents
Virtual machine memory management Download PDFInfo
- Publication number
- US9176889B1 US9176889B1 US13/837,303 US201313837303A US9176889B1 US 9176889 B1 US9176889 B1 US 9176889B1 US 201313837303 A US201313837303 A US 201313837303A US 9176889 B1 US9176889 B1 US 9176889B1
- Authority
- US
- United States
- Prior art keywords
- memory
- memory page
- host machine
- contents
- host
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/455—Emulation; Interpretation; Software simulation, e.g. virtualisation or emulation of application or operating system execution engines
- G06F9/45533—Hypervisors; Virtual machine monitors
- G06F9/45558—Hypervisor-specific management and integration aspects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5005—Allocation of resources, e.g. of the central processing unit [CPU] to service a request
- G06F9/5011—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resources being hardware resources other than CPUs, Servers and Terminals
- G06F9/5016—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resources being hardware resources other than CPUs, Servers and Terminals the resource being the memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0866—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches for peripheral storage systems, e.g. disk cache
- G06F12/0871—Allocation or management of cache space
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0893—Caches characterised by their organisation or structure
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
- G06F12/1027—Address translation using associative or pseudo-associative address translation means, e.g. translation look-aside buffer [TLB]
- G06F12/1036—Address translation using associative or pseudo-associative address translation means, e.g. translation look-aside buffer [TLB] for multiple virtual address spaces, e.g. segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
- G06F12/109—Address translation for multiple virtual address spaces, e.g. segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1668—Details of memory controller
- G06F13/1673—Details of memory controller using buffers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1668—Details of memory controller
- G06F13/1689—Synchronisation and timing concerns
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/38—Information transfer, e.g. on bus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/38—Information transfer, e.g. on bus
- G06F13/382—Information transfer, e.g. on bus using universal interface adapter
- G06F13/385—Information transfer, e.g. on bus using universal interface adapter for adaptation of a particular data processing system to different peripheral devices
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/455—Emulation; Interpretation; Software simulation, e.g. virtualisation or emulation of application or operating system execution engines
- G06F9/45533—Hypervisors; Virtual machine monitors
- G06F9/45558—Hypervisor-specific management and integration aspects
- G06F2009/45583—Memory management, e.g. access or allocation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0602—Interfaces specially adapted for storage systems specifically adapted to achieve a particular effect
- G06F3/0608—Saving storage space on storage systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0638—Organizing or formatting or addressing of data
- G06F3/064—Management of blocks
- G06F3/0641—De-duplication techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0646—Horizontal data movement in storage systems, i.e. moving data in between storage devices or systems
- G06F3/065—Replication mechanisms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0653—Monitoring storage devices or systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0662—Virtualisation aspects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0668—Interfaces specially adapted for storage systems adopting a particular infrastructure
- G06F3/067—Distributed or networked storage systems, e.g. storage area networks [SAN], network attached storage [NAS]
Definitions
- This specification relates to cloud computing and, in particular, to virtual machine memory management.
- Cloud computing is network-based computing in which typically large collections of servers housed in data centers or “server farms” provide computational resources and data storage as needed to remote end users.
- Some cloud computing services provide access to software applications such as word processors and other commonly used applications to end users who interface with the applications through web browsers or other client-side software. Users' electronic data files are usually stored in the server farm rather than on the users' computing devices. Maintaining software applications and user data on a server farm simplifies management of end user computing devices.
- Some cloud computing services allow end users to execute software applications in virtual machines.
- Virtual machines in a datacenter commonly have guest physical memory pages whose contents are identical to the contents of other guest physical memory pages in use by the same or other virtual machines in the datacenter. Rather than maintaining a separate copy of the contents of these guest physical memory pages for each guest physical memory page, the guest physical memory pages whose contents are the same can be coalesced, such that the datacenter as a whole maintains fewer copies of the contents of these guest physical memory pages.
- a virtual machine monitor can request copies of a coalesced page as needed, typically when accessed by the software running within the virtual machine, or in anticipation of such access.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a notification that contents of a first memory page for a first virtual machine on a first host machine are the same as contents of a second memory page for a second virtual machine on a second different host machine, wherein a memory page is a segment of computer-executable instructions or data; deallocating storage space occupied by the first memory page on the first host machine based on the notification; receiving a request from the first virtual machine for the first memory page; and in response to the request, obtaining a copy of contents of the second memory page from the second host machine.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- the foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination.
- the first memory page and the second memory page are guest physical memory pages.
- Receiving the notification comprises receiving the notification from a service that manages virtual machine memory pages in a datacenter.
- the actions include receiving an identification of the second host machine, and wherein obtaining a copy of contents of the second memory page from the second host machine comprises requesting the copy from the second host machine based on the identification.
- the actions include in response to obtaining a copy of the second memory page, loading the second memory page into random access memory.
- the actions include providing a notification to a virtual machine monitor that the second memory page is available for access.
- the actions include computing usage statistics for one or more virtual machine memory pages used by one or more virtual machines on the first host machine; and identifying one or more virtual machine memory pages with usage that is less than a threshold based at least in part on the computed usage statistics.
- the actions include receiving a request for usage statistics on one or more virtual machine memory pages; and providing usage statistics for the identified one or more virtual machine memory pages in response to the request.
- another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of determining that multiple memory pages whose contents are the same are used by multiple different virtual machines, wherein each of the virtual machines executes on a different respective host machine in a datacenter; storing, on a first host machine, contents of a memory page from one of the multiple memory pages whose contents are the same; receiving a request for the stored memory page from a second host machine; and providing information identifying a datacenter location of the first host machine to the second host machine in response to on the request.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- the actions include computing usage statistics for virtual machine memory pages used by one or more virtual machines in a datacenter; and determining that the multiple memory pages whose contents are the same each have a usage statistic that satisfies a threshold.
- the actions include providing a notification to each of the one or more host machines that the respective one or more memory pages can be deallocated.
- the actions include storing multiple copies of the contents of the memory page, wherein each copy of the multiple copies is maintained on a different host machine.
- Coalescing guest physical memory pages across host machines allows a datacenter to store fewer copies of guest physical memory pages whose contents are the same.
- the number of copies of the contents of guest physical memory pages stored across the datacenter may be fewer than the number of physical host machines with virtual machines using those guest physical memory.
- the storage space savings from coalescing virtual memory pages can additionally allow more virtual machines or non-virtual workloads to be loaded onto host machines in the datacenter.
- FIG. 1 is a schematic illustration of an example virtual machine system.
- FIG. 2A illustrates management of virtual machine memory pages.
- FIG. 2B is a sequence diagram of an example process for requesting a coalesced memory page.
- FIG. 3 is a flow chart of an example process 300 for coalescing memory pages in a datacenter.
- FIG. 4 is a flow chart of an example process 400 for coalescing memory pages in a datacenter.
- FIG. 1 is a schematic illustration of an example virtual machine system 100 .
- the system 100 includes one or more host machines such as, for example, host machine 102 and host machine 104 .
- a host machine is one or more data processing apparatus such as rack mounted servers or other computing devices.
- the data processing apparatus can be in different physical locations and can have different capabilities and computer architectures.
- Host machines can communicate with each other through an internal data communications network 116 .
- the internal network 116 can include one or more wired, e.g., Ethernet, or wireless, e.g., WI-FI, networks, for example.
- the internal network 116 is an intranet.
- Host machines can also communicate with devices on external networks, such as the Internet 122 , through one or more gateways 120 which are data processing apparatus responsible for routing data communication traffic between the internal network 116 and the external network 122 . Other types of external networks are possible.
- Each host machine 102 , 104 executes a host operating system 106 , 108 .
- a host operating system 106 , 108 manages host machine resources.
- host operating systems 106 , 108 run software, e.g. a virtual machine monitor (VMM) or a hypervisor, that virtualizes the underlying host machine hardware and manages concurrent execution of one or more VMs.
- the host operating system 106 manages two VMs, VM 110 and VM 112 , while a different host operating system 108 manages a single VM 114 .
- VMs can be migrated from one host machine to another host machine.
- a single VM can be managed by multiple host machines.
- a host machine can, in general, manage multiple virtual machines, however, the quantity may be limited based on physical resources of the host machine.
- Each VM provides an emulation of a physical hardware system which may, but need not, be based on the host machine hardware architecture.
- the simulated version of the hardware is referred to as virtual hardware, e.g., virtual hardware 110 a , 112 a , and 114 a .
- Software that is executed by the virtual hardware is referred to as guest software.
- guest software cannot determine if it is being executed by virtual hardware or by a physical host machine. If guest software executing in a VM, or the VM itself, is compromised, malfunctions, or aborts, other VMs executing on the host machine may not be affected.
- a host machine's microprocessor(s) can include processor-level mechanisms to enable virtual hardware to execute software applications efficiently by allowing guest software instructions to be executed directly on the host machine's microprocessor without requiring code-rewriting, recompilation, or instruction emulation.
- each VM e.g., VMs 110 , 112 , and 114 , has access to some amount of guest physical memory.
- the VMM may implement this guest physical memory in various ways, for example, by mapping pages of guest physical memory to pages of host physical memory, by simulating pages of guest physical memory, or by dynamically loading contents of guest physical memory from a storage device.
- Each VM also has access to virtual storage hardware, for example virtual disk blocks from one or more virtual disk drives for use by the guest software executing on the VM.
- a virtual memory page can store a segment of computer-executable instructions, data, or both. The contents of a memory page can be read from computer-readable storage, loaded into random access memory, and in some instances executed by the VM.
- host operating system 106 allocates memory pages and disk blocks to VM 110 and VM 112
- host operating system 108 allocates memory pages and disk blocks to VM 114 .
- a given VM cannot access the guest physical virtual memory pages assigned to other VMs.
- VM 110 cannot access memory pages that have been assigned to VM 112 .
- a virtual disk drive can be persisted across VM restarts.
- Virtual disk blocks are allocated on physical disk drives coupled to host machines or available over the internal network 116 , for example.
- VMs can be allocated network addresses through which their respective guest software can communicate with other processes reachable through the internal network 116 or the Internet 122 .
- guest software executing on VM 110 can communicate with guest software executing on VM 112 or VM 114 .
- each VM is allocated one or more unique Internet Protocol (IP) version 4 or version 6 addresses and one or more User Datagram Protocol (UDP) port numbers. Other address schemes are possible.
- IP Internet Protocol
- UDP User Datagram Protocol
- the VM IP addresses are visible on the internal network 116 and, in some implementations, may be visible on the Internet 122 if the addresses are advertised using a suitable routing protocol, for instance.
- a VM's guest software can include a guest operating system, e.g., guest operating systems 110 b , 112 b , and 114 b , which is software that controls the execution of respective guest software applications, e.g., guest applications 110 c , 112 c , and 114 c , within the VM and provides services to those applications.
- a guest operating system could be a version of the UNIX operating system. Other operating systems are possible.
- Each VM can execute the same guest operating system or different guest operating systems.
- a VM does not require a guest operating system in order to execute guest software applications.
- a guest operating system's access to resources such as networks and virtual disk storage is controlled by the underlying host operating system.
- the guest application 110 c or guest operating system 110 b attempts to perform an input/output operation on a virtual disk, initiate network communication, or perform a privileged operation, for example, the virtual hardware 110 a is interrupted so that the host operating system 106 can perform the action on behalf of the virtual machine 110 .
- the host operating system 106 can perform these actions with a process that executes in kernel process space 106 b , user process space 106 a , or both.
- the kernel process space 106 b is virtual memory reserved for the host operating system 106 's kernel 106 d which can include kernel extensions and device drivers, for instance.
- the kernel process space has elevated privileges, sometimes referred to as “supervisor mode”; that is, the kernel 106 d can perform certain privileged operations that are off limits to processes running in the user process space 106 a . Examples of privileged operations include access to different address spaces, access to special functional processor units in the host machine such as memory management units, and so on.
- the user process space 106 a is a separate portion of virtual memory reserved for user mode processes. User mode processes cannot perform privileged operations directly.
- a portion of VM network communication functionality is implemented in a communication process, e.g., communication process 106 c .
- the communication process executes in the user process space, e.g., user process space 106 a , of a host operating system, e.g., host operating system 106 .
- the communication process can execute in the kernel process space, e.g., kernel process space 106 d of the host operating system.
- some portion of the communication process executes in the user process space and another portion executes in the kernel process space.
- the system 100 includes a memory service 130 .
- the memory service 130 is a process that can manage and maintain information about memory pages in the system 100 .
- the memory service 130 can determine that multiple VMs in the system 100 are maintaining guest physical memory pages whose contents are the same. In other words, the guest physical memory pages are storing the same bytes, which are encoding instructions, data, or both.
- the memory service 130 can instead maintain the contents of a single guest physical memory page that corresponds to the guest physical memory pages that are common to all of the multiple VMs.
- the multiple VMs can deallocate storage space, e.g. hard disk space or RAM, that was previously allocated to maintaining the guest physical memory page.
- FIG. 2A illustrates management of virtual machine memory pages.
- a memory service 230 manages virtual machine memory pages in the system by identifying guest physical memory pages that can be coalesced into a single guest physical memory page. Coalescing two or more pages of memory means maintaining a single copy of the contents of two or more memory pages whose contents are the same. Two host machines 211 and 221 can thus share a copy of the contents of a guest physical memory page, which can either be stored on memory or on disk of a host machine or stored in a storage system 240 accessible on a network.
- host machine 211 runs a host OS 213 , which runs a VMM 214 that hosts a virtual machine 215 , and a memory manager 216 .
- the memory manager 216 can be a process executing on the host machine 211 , for example.
- the VMM 214 and memory manager 216 run directly on host machine 211 without a separate host OS 213 .
- the memory manager 216 coordinates with the memory service 230 and other host machines, e.g. host machine 221 , to identify, locate, and maintain coalesced virtual machine memory pages.
- the virtual machine 215 of host machine 211 accesses guest physical memory pages, which can be emulated by the VMM 214 and stored as host physical memory pages 212 a - d in host machine memory 212 .
- the host physical memory pages 212 a - d can be stored either in random access memory or in other computer-readable storage, e.g. a hard disk, associated with host machine 211 .
- the memory manager 216 can coalesce a guest physical memory page of a particular virtual machine by storing a copy of the contents of the guest physical memory page elsewhere, for example, on disk or in local or networked computer-readable storage.
- the contents of a guest physical memory page for virtual machine 215 can be stored on a different host machine 221 , e.g. in host physical memory page 222 b of memory 222 .
- the memory manager 216 can locate the requested page and load the requested page into a host physical memory page in memory 212 for access by virtual machine 215 .
- Host machine 221 also runs a host OS 223 , which runs a VMM 224 that hosts a virtual machine 225 , and a memory manager 226 .
- the host OS 223 includes a memory manager 226 that communicates with memory service 230 .
- the virtual machine 225 accesses guest physical memory pages, which can be stored as host physical memory pages 222 a - d in memory 222 .
- the memory service 230 can determine that the contents of two guest physical memory pages used by virtual machine 215 and virtual machine 225 are the same. For example, the memory service 230 can determine that the contents of a guest physical memory page used by virtual machine 215 and stored in host physical memory page 212 b of host machine 211 are the same as the contents of a guest physical memory page used by virtual machine 225 and stored in host physical memory page 222 b of host machine 221 .
- Two memory pages can be the same when they include the same set of computer-executable instructions, data, or both.
- the memory service 230 determines that two memory pages are the same by hashing the contents of the memory pages and comparing the resulting hash values. If the hash values are the same, the memory service 230 can optionally do a second bitwise comparison of the two memory pages to ensure that the contents of the memory pages are the same.
- the memory service 230 can maintain a single copy of the contents of the guest physical memory pages that were previously stored in both host physical memory pages 212 b and 222 b .
- the memory service 230 can, for example, record an association between a first guest physical memory page and a location of the contents of the page, e.g. host physical memory page 222 b .
- the memory service 230 can then instruct memory manager 216 where to find the contents of the guest physical memory page when needed by virtual machine 215 .
- the memory service 230 can also identify memory sub-pages that are the same, and similarly deallocate storage space for and store a representative copy of the contents of duplicate sub-pages.
- Memory manager 216 can deallocate storage space previously occupied by host physical memory page 212 b . Deallocation of the storage space allows other memory pages to be loaded into the storage space previously occupied by host physical memory page 212 b . In some implementations, the storage space savings can additionally allow more virtual machines or other non-virtual workloads to be loaded onto host machine 211 or host machine 221 .
- the host machine 211 includes a host translation lookaside buffer, “TLB,” which maps host virtual addresses to host physical addresses.
- TLB host translation lookaside buffer
- the host TLB translates the requested host virtual address to a host physical address. If the host virtual address does not exist in the host TLB, the host OS 213 can walk a set of page tables, which can be stored in host machine memory 212 or on disk, to find the corresponding host physical address.
- the virtual machine 215 includes a guest TLB 218 , which maps guest virtual addresses to host physical addresses.
- a guest application executing on the VM 215 requests access to a guest virtual address
- the guest OS of the VM 215 translates the requested guest virtual address to a guest physical address using the guest TLB 218 .
- the VM 215 requests a translation from the VMM 214 .
- the VMM 214 can perform the translation to a guest physical address by a number of methods. For example, the VMM 214 can build a “shadow TLB” that translates guest virtual addresses to host physical addresses. Alternatively, the VMM 214 can also build guest-physical to host-physical page tables and then let the guest OS of the VM 215 build guest-virtual to guest-physical page tables.
- the VMM 214 can simulate a hardware exception in the guest OS of the VM 215 . This exception can trigger memory manager 216 to request the required guest physical memory page.
- the VMM 214 can suspend guest OS execution and issue a request to memory manager 216 to retrieve the missing page. When the requested page is retrieved, the VMM 214 can then resume guest OS execution.
- Memory manager 216 can request a copy of a guest physical memory page directly from host machine 221 when the memory page is needed by virtual machine 215 .
- memory manager 216 can query memory service 230 for the location of the requested memory page, e.g. if host machine 220 no longer stores a copy of the guest physical memory page in its host physical memory pages, e.g. host physical memory page 222 b.
- memory service 230 provides host machine 211 with information identifying a datacenter location of a second host machine where the contents of a particular page is located. Memory service 230 can migrate the contents of a particular memory page from machine to machine in a datacenter as needed, meanwhile maintaining information about the location of the particular memory page.
- memory service 230 can maintain at least a certain number of copies of the contents of each page that has been coalesced in the datacenter, e.g. at least three.
- memory service 230 can provide a list of all locations where the page is stored. The memory manager 216 can then request a copy of the page from each location on the list in sequence until the page is found.
- FIG. 2B is a sequence diagram of an example process for requesting a coalesced memory page.
- a VMM 213 requests a memory page from memory manager 216 .
- VMM 213 can request a guest physical memory page from memory manager 216 in simulating a hardware exception in a guest OS or in response to an exception raised when a guest physical to host physical page table does not include a particular entry.
- Memory manager 216 communicates with memory service 230 to locate the requested memory page on another host machine. Memory manager 216 then requests a copy of the memory page directly from a memory manager 226 of the other host machine.
- VMM 213 makes a page request 202 for a particular memory page.
- the page request 202 is a request to access a particular memory page, e.g. a request to load a particular memory page into random access memory.
- the memory manager 216 receives the request 202 and determines that the requested memory page is not located on the same host machine as VMM 213 . In response, the memory manager 216 makes a page host request 204 to memory service 230 to locate a host machine that is storing the requested memory page.
- the memory service 230 receives the page host request 204 and determines on which host machine the requested memory page is stored.
- the memory service 230 provides information identifying the location 206 of the host machine to memory manager 216 .
- the memory manager 216 then makes a page request 208 to the memory manager 226 running on the host machine that is storing the requested memory page.
- the memory service 230 and second memory manager 226 may be running on the same or on different host machines.
- the memory manager 226 responds with a copy of the contents of the requested memory page 210 .
- the memory manager 216 then provides the copy of the contents of the memory page 210 to the VMM 213 .
- the VMM 213 can then load the contents of the memory page into a page of guest physical memory, e.g. by loading the page into random access memory
- FIG. 3 is a flow chart of an example process 300 for coalescing memory pages in a datacenter.
- the process 300 can be performed by an appropriately programmed computer system of one or more computers.
- the process 300 will be described as being performed by a memory manager, e.g. memory manager 216 as shown in FIG. 2A , installed on a particular host machine in a datacenter of multiple host machines.
- the memory manager receives a notification that a first memory page is the same as a second memory page ( 310 ). For example, the memory manager can receive a notification that a first guest physical memory page used by a first virtual machine on a first host machine has the same contents as a second guest physical memory page used by a second virtual machine on a second different host machine. The memory manager can receive the notification from a memory service that has analyzed memory page usage in a datacenter.
- the memory manager deallocates storage space for the first memory page ( 320 ). For example, the memory manager can deallocate a particular host physical memory page that stored the contents of a guest physical memory page. Deallocating storage space occupied by a guest physical memory page can allow other pages of memory to be loaded into host physical memory pages
- the memory manager receives a request for the first memory page ( 330 ).
- a virtual machine can request a guest physical memory page. If the contents of the requested guest physical memory page are not stored in a host physical memory page on the same host machine, a VMM can ask the memory manager to request the contents of the guest physical memory page from another host machine.
- the memory manager obtains a copy of the contents of the second memory page from a different host machine ( 340 ).
- the memory manager can request a copy of the contents of the second memory page directly from another host machine, or the memory manager can query a memory service to locate a host machine on which the contents of the second memory page are stored.
- the memory manager can request a copy of the second memory page and provide the copy of the second memory page to the virtual machine that requested the copy. For example, the memory manager can load the copy of the second memory page into random access memory and notify the virtual machine monitor that the second memory page is available for access by the virtual machine.
- FIG. 4 is a flow chart of an example process 400 for coalescing memory pages in a datacenter.
- the process 400 can be performed by an appropriately-programmed computer system of one or more computers.
- the process 400 will be described as being performed by a memory service, e.g. memory service 230 as shown in FIG. 2A , installed on a particular host machine in a datacenter of multiple host machines.
- the memory service determines that multiple memory pages whose contents are the same are used by multiple different virtual machines ( 410 ). For example, the memory service can compare guest physical memory pages used by virtual machines in a datacenter and determine that the virtual machines all use a guest physical memory page having the same contents.
- the memory service identifies memory pages that are relatively rarely used, e.g. memory pages that relate to exception handling.
- the memory service can maintain usage statistics for pages of guest physical memory used by virtual machines in a datacenter. For example, the memory service can compute an access frequency for pages of guest physical memory and compare the access frequency to a threshold to identify rarely used pages.
- the memory service can also rank memory pages by usage statistics and identify a number of pages with the lowest usage statistics to be coalesced, for example, the bottom 10%.
- the memory managers installed on individual host machines can maintain usage statistics about memory pages used by virtual machines on that host machine. The memory service can then query the memory managers for the collected usage statistics and aggregate usage statistics for common memory pages in order to identify memory pages that are rarely used.
- the memory service maintains a memory page corresponding to the one or more same memory pages ( 420 ).
- the memory service can for example designate a particular host physical memory page of a single host machine as the memory page that the virtual machines will use to access contents of the guest physical memory page.
- the memory service can also maintain multiple copies of the page on different host machines for redundancy and reliability.
- the memory service can store a copy of the contents of the memory page in computer-readable storage that can be provided to virtual machines upon request.
- the memory page can be stored in a compressed form to save storage space, and the memory service or the VMM can decompress the page into its original form when needed.
- the memory service receives a request for the memory page ( 430 ).
- the memory service can provide information identifying the datacenter location of a host machine where the memory page is stored ( 440 ).
- the memory service can provide a network address, e.g. an Internet Protocol address, of a host machine where the memory page is stored.
- the memory service can provide its own maintained copy of the page in computer-readable storage.
- the memory service can, for example, maintain a database of associations between particular memory pages and host machines where they are stored. When a request is received for a particular memory page, the memory service can look up information about a host machine where the memory page is located and provide the information in response to the request.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
Claims (25)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/837,303 US9176889B1 (en) | 2013-03-15 | 2013-03-15 | Virtual machine memory management |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/837,303 US9176889B1 (en) | 2013-03-15 | 2013-03-15 | Virtual machine memory management |
Publications (1)
Publication Number | Publication Date |
---|---|
US9176889B1 true US9176889B1 (en) | 2015-11-03 |
Family
ID=54352720
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/837,303 Active 2033-12-27 US9176889B1 (en) | 2013-03-15 | 2013-03-15 | Virtual machine memory management |
Country Status (1)
Country | Link |
---|---|
US (1) | US9176889B1 (en) |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160179396A1 (en) * | 2014-12-19 | 2016-06-23 | International Business Machines Corporation | Method to improve page out mechanism with compressed memory pools |
US20170003997A1 (en) * | 2015-07-01 | 2017-01-05 | Dell Products, Lp | Compute Cluster Load Balancing Based on Memory Page Contents |
US9836357B1 (en) * | 2014-04-25 | 2017-12-05 | Veritas Technologies Llc | Systems and methods for backing up heterogeneous virtual environments |
US10419344B2 (en) | 2016-05-31 | 2019-09-17 | Avago Technologies International Sales Pte. Limited | Multichannel input/output virtualization |
US10439960B1 (en) * | 2016-11-15 | 2019-10-08 | Ampere Computing Llc | Memory page request for optimizing memory page latency associated with network nodes |
US11061711B2 (en) | 2019-09-23 | 2021-07-13 | Red Hat, Inc. | Storage deduplication for virtual machines with encrypted storage |
US11232030B2 (en) * | 2019-09-27 | 2022-01-25 | Red Hat Inc. | Storage deduplication for virtual machines with encrypted storage |
US11656891B2 (en) | 2019-09-27 | 2023-05-23 | Red Hat, Inc. | Copy-on-write for virtual machines with encrypted storage |
Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP0969380A2 (en) | 1990-06-11 | 2000-01-05 | Cray Research, Inc. | Method for efficient non-virtual main memory management |
US20050108496A1 (en) | 2003-11-13 | 2005-05-19 | International Business Machines Corporation | Hardware support for superpage coalescing |
US7437529B2 (en) | 2005-06-16 | 2008-10-14 | International Business Machines Corporation | Method and mechanism for efficiently creating large virtual memory pages in a multiple page size environment |
US20110010515A1 (en) * | 2009-07-09 | 2011-01-13 | Microsoft Corporation | Backup of virtual machines using cloned virtual machines |
US20110179413A1 (en) | 2010-01-15 | 2011-07-21 | Vmware, Inc. | Guest/Hypervisor Interrupt Coalescing for Storage Adapter Virtual Function in Guest Passthrough Mode |
US20110219447A1 (en) * | 2010-03-08 | 2011-09-08 | Vmware, Inc. | Identification of Unauthorized Code Running in an Operating System's Kernel |
US8131972B2 (en) | 2007-09-19 | 2012-03-06 | International Business Machines Corporation | Method and apparatus for improving memory coalescing in a virtualized hardware environment |
US20120221800A1 (en) | 2009-02-27 | 2012-08-30 | Izik Eidus | Memory sharing among computer programs |
US20130159649A1 (en) * | 2011-08-31 | 2013-06-20 | Ibm Corporation | Selecting a Primary-Secondary Host Pair for Mirroring Virtual Machines |
US20130246685A1 (en) * | 2011-09-09 | 2013-09-19 | Mcafee, Inc. | System and method for passive threat detection using virtual memory inspection |
US8800009B1 (en) * | 2011-12-30 | 2014-08-05 | Google Inc. | Virtual machine service access |
US8958293B1 (en) * | 2011-12-06 | 2015-02-17 | Google Inc. | Transparent load-balancing for cloud computing services |
-
2013
- 2013-03-15 US US13/837,303 patent/US9176889B1/en active Active
Patent Citations (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP0969380A2 (en) | 1990-06-11 | 2000-01-05 | Cray Research, Inc. | Method for efficient non-virtual main memory management |
US20050108496A1 (en) | 2003-11-13 | 2005-05-19 | International Business Machines Corporation | Hardware support for superpage coalescing |
US8417913B2 (en) | 2003-11-13 | 2013-04-09 | International Business Machines Corporation | Superpage coalescing which supports read/write access to a new virtual superpage mapping during copying of physical pages |
US7437529B2 (en) | 2005-06-16 | 2008-10-14 | International Business Machines Corporation | Method and mechanism for efficiently creating large virtual memory pages in a multiple page size environment |
US8131972B2 (en) | 2007-09-19 | 2012-03-06 | International Business Machines Corporation | Method and apparatus for improving memory coalescing in a virtualized hardware environment |
US20120221800A1 (en) | 2009-02-27 | 2012-08-30 | Izik Eidus | Memory sharing among computer programs |
US20110010515A1 (en) * | 2009-07-09 | 2011-01-13 | Microsoft Corporation | Backup of virtual machines using cloned virtual machines |
US8291135B2 (en) | 2010-01-15 | 2012-10-16 | Vmware, Inc. | Guest/hypervisor interrupt coalescing for storage adapter virtual function in guest passthrough mode |
US8392623B2 (en) | 2010-01-15 | 2013-03-05 | Vmware, Inc. | Guest/hypervisor interrupt coalescing for storage adapter virtual function in guest passthrough mode |
US20110179413A1 (en) | 2010-01-15 | 2011-07-21 | Vmware, Inc. | Guest/Hypervisor Interrupt Coalescing for Storage Adapter Virtual Function in Guest Passthrough Mode |
US20110219447A1 (en) * | 2010-03-08 | 2011-09-08 | Vmware, Inc. | Identification of Unauthorized Code Running in an Operating System's Kernel |
US20130159649A1 (en) * | 2011-08-31 | 2013-06-20 | Ibm Corporation | Selecting a Primary-Secondary Host Pair for Mirroring Virtual Machines |
US20130246685A1 (en) * | 2011-09-09 | 2013-09-19 | Mcafee, Inc. | System and method for passive threat detection using virtual memory inspection |
US8958293B1 (en) * | 2011-12-06 | 2015-02-17 | Google Inc. | Transparent load-balancing for cloud computing services |
US8800009B1 (en) * | 2011-12-30 | 2014-08-05 | Google Inc. | Virtual machine service access |
Cited By (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9836357B1 (en) * | 2014-04-25 | 2017-12-05 | Veritas Technologies Llc | Systems and methods for backing up heterogeneous virtual environments |
US20160179396A1 (en) * | 2014-12-19 | 2016-06-23 | International Business Machines Corporation | Method to improve page out mechanism with compressed memory pools |
US9483184B2 (en) * | 2014-12-19 | 2016-11-01 | International Business Machines Corporation | Method to improve page out mechanism with compressed memory pools |
US10001925B2 (en) | 2014-12-19 | 2018-06-19 | International Business Machines Corporation | Method to improve page out mechanism with compressed memory pools |
US20170003997A1 (en) * | 2015-07-01 | 2017-01-05 | Dell Products, Lp | Compute Cluster Load Balancing Based on Memory Page Contents |
US10419344B2 (en) | 2016-05-31 | 2019-09-17 | Avago Technologies International Sales Pte. Limited | Multichannel input/output virtualization |
US10797999B2 (en) | 2016-05-31 | 2020-10-06 | Avago Technologies International Sales Pte. Limited | Multichannel input/output virtualization |
US10439960B1 (en) * | 2016-11-15 | 2019-10-08 | Ampere Computing Llc | Memory page request for optimizing memory page latency associated with network nodes |
US11061711B2 (en) | 2019-09-23 | 2021-07-13 | Red Hat, Inc. | Storage deduplication for virtual machines with encrypted storage |
US11232030B2 (en) * | 2019-09-27 | 2022-01-25 | Red Hat Inc. | Storage deduplication for virtual machines with encrypted storage |
US11656891B2 (en) | 2019-09-27 | 2023-05-23 | Red Hat, Inc. | Copy-on-write for virtual machines with encrypted storage |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9176889B1 (en) | Virtual machine memory management | |
US9720952B2 (en) | Virtual block devices | |
US11824926B2 (en) | Efficient live-migration of remotely accessed data | |
US10620932B2 (en) | Replacing an accelerator firmware image without operating system reboot | |
US9292319B2 (en) | Global computing interface | |
US9870248B2 (en) | Page table based dirty page tracking | |
US20180046581A1 (en) | Page-Fault Latency Directed Virtual Machine Performance Monitoring | |
US20120185688A1 (en) | Processor mode locking | |
US20160350010A1 (en) | Providing block size compatibility with a storage filter | |
US9569223B2 (en) | Mixed shared/non-shared memory transport for virtual machines | |
US9342450B2 (en) | On-demand hypervisor memory mapping | |
US20160239324A1 (en) | Watchdog code for virtual machine functions | |
US20160267021A1 (en) | Managing Storage Block of a Data Volume | |
US10552374B2 (en) | Minimizing file creation and access times using skip optimization | |
US20230185593A1 (en) | Virtual device translation for nested virtual machines | |
US9483300B2 (en) | Importing a running VM | |
US10747567B2 (en) | Cluster check services for computing clusters | |
US9430255B1 (en) | Updating virtual machine generated metadata to a distribution service for sharing and backup | |
US9336024B1 (en) | Clustering for parallel processing | |
US20200076920A1 (en) | Distributed computing systems having capability services | |
US20220358049A1 (en) | Memory access handling for peripheral component interconnect devices |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EARHART, ROBERT H., III;REEL/FRAME:030447/0484Effective date: 20130519 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |