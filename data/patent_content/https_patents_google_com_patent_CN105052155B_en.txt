CN105052155B - Interpolation video label - Google Patents
Interpolation video label Download PDFInfo
- Publication number
- CN105052155B CN105052155B CN201480016856.6A CN201480016856A CN105052155B CN 105052155 B CN105052155 B CN 105052155B CN 201480016856 A CN201480016856 A CN 201480016856A CN 105052155 B CN105052155 B CN 105052155B
- Authority
- CN
- China
- Prior art keywords
- video
- frame
- video frame
- label
- sequence
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related
Links
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/8126—Monomedia components thereof involving additional data, e.g. news, sports, stocks, weather forecasts
- H04N21/8133—Monomedia components thereof involving additional data, e.g. news, sports, stocks, weather forecasts specifically related to the content, e.g. biography of the actors in a movie, detailed information about an article seen in a video program
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/439—Processing of audio elementary streams
- H04N21/4394—Processing of audio elementary streams involving operations for analysing the audio stream, e.g. detecting features or characteristics in audio streams
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/44—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs
- H04N21/44008—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics in the video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/472—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content
- H04N21/4722—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content for requesting additional data associated with the content
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/472—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content
- H04N21/4722—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content for requesting additional data associated with the content
- H04N21/4725—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content for requesting additional data associated with the content using interactive regions of the image, e.g. hot spots
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/85—Assembly of content; Generation of multimedia applications
- H04N21/858—Linking data to content, e.g. by linking an URL to a video object, by creating a hotspot
- H04N21/8583—Linking data to content, e.g. by linking an URL to a video object, by creating a hotspot by creating hot-spots
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N7/00—Television systems
- H04N7/025—Systems for the transmission of digital non-picture data, e.g. of text during the active part of a television frame
Abstract
Describe a kind of interpolation video label technique.A kind of exemplary method includes the sequence of video frame that multiple video frame of identification video include, wherein object is present at least one of sequence of video frame video frame, determine with the object in the sequence of the video frame there are associated interpolating functions, the wherein interpolating function specifies the time domain of interpolating function effective video wherein, and transmits the video, the instruction of the object and the instruction of the interpolating function.
Description
Background technology
The various equipment of such as desktop computer and laptop computer, tablet computer, mobile phone and television set etc all may be used
With being capable of playing video data.Such equipment can by various means obtain video data, such as via the Internet download,
Internet streams or passes through removable medium.In this example, show video data identical equipment can also can calculate with
The relevant other data of video.
Invention content
In one example, a kind of method includes video included in multiple video frame by computing device identification video
The sequence of frame, wherein object are present at least one of sequence of video frame video frame；By the computing device determine with
The object in the sequence of the video frame there are associated interpolating function, the wherein interpolating function specifies the interpolating function to exist
The wherein time domain of effective video, and the video, the instruction of the object and the interpolating function are transmitted by the computing device
Instruction.
In another example, a kind of equipment includes memory and one or more programmable processors, is configured
The sequence of included video frame, wherein object are present in the sequence of the video frame in multiple video frame to identify video
In at least one video frame；Determine with the object in the sequence of the video frame there are associated interpolating function, wherein this
Interpolating function specifies the time domain of interpolating function effective video wherein.The equipment further comprises interface, is configured as
Transmit the video, the instruction of the object and the instruction of the interpolating function.
In another example, a kind of computer readable storage devices coding has instruction, which causes when executed
The sequence of included video frame in multiple video frame of one or more programmable processors identification video of computing device,
Middle object is present at least one of sequence of video frame video frame, determines with the object in the sequence of the video frame
There are associated interpolating function, wherein the interpolating function specifies the time domain of interpolating function effective video wherein, and
And transmit the video, the instruction of the object and the instruction of the interpolating function.
In another example, a kind of method includes including video, the video of multiple video frame by computing device reception
The instruction of at least one object of middle expression, label associated with the object, at least one for the video frame in the video
The interpolating function associated with the object of a sequence and knowledge data associated with the object, the wherein interpolating function
Specify the time domain of interpolating function effective video wherein；Institute is based on by the computing device and for the video frame of the video
The interpolating function of reception determines that wherein the label is with being associated with for the position of label associated with the object in the video frame
The knowledge data of the object is associated；And by the computing device output video frame and the mark positioned according to identified position
At least one of label are with display.
In another example, a kind of equipment includes memory and interface, which, which is configured as receiving, includes multiple regard
The instruction of at least one object that is represented in the video of frequency frame, the video, for the video video frame at least one sequence
Interpolating function associated with the object instruction and knowledge data associated with the object, the wherein interpolating function
Specify interpolating function effective video time domain wherein.The equipment further comprises one or more programmable processors,
It is configured as determining in the video frame for display and the object video frame of the video based on the interpolating function received
The position of associated label, the wherein label are associated with the knowledge data for being associated with the object；And output video frame with
And at least one of label positioned according to identified position is with display.
In another example, a kind of computer readable device coding has instruction, which causes to calculate when executed
One or more programmable processors reception of equipment includes representing at least one right in the video of multiple video frame, the video
The instruction of elephant, for the video frame in the video at least one sequence interpolating function associated with the object instruction,
And knowledge data associated with the object, wherein the interpolating function specify the interpolating function wherein effective video when
Domain；For the video video frame based on the interpolating function received determine in the video frame for show it is associated with the object
Label position, the wherein label is associated with the knowledge data for being associated with the object；And output video frame and according to
At least one of label of identified position positioning is with display.
The exemplary details of one or more of the disclosure provides in the accompanying drawings and the description below.Other features, target and
Advantage will be due to the description and attached drawing and due to claim but obvious.
Description of the drawings
Fig. 1 is diagram according to the server apparatus of the one or more aspects of the disclosure, client device and by the visitor
The concept map for the user interface that family end equipment is provided.
Fig. 2 is the service that diagram is configured as performing one or more server side interpolation video label techniques of the disclosure
The block diagram of the details of device equipment.
Fig. 3 is the client that diagram is configured as performing one or more client-side interpolation video label techniques of the disclosure
The block diagram of the details of end equipment.
Fig. 4 A-4C are the concepts of the user interface with interpolation video label of the one or more aspects according to the disclosure
Figure.
Fig. 5 is to illustrate server apparatus to implement one or more server side interpolation videos of the disclosure by it
The flow chart of the instantiation procedure of label technique.
Fig. 6 is to illustrate client device to implement one or more client-side interpolation videos of the disclosure by it
The flow chart of the instantiation procedure of label technique.
Fig. 7 is to illustrate server apparatus to implement one or more server side interpolation videos of the disclosure by it
The flow chart of another instantiation procedure of label technique.
Fig. 8 is to illustrate client device to implement one or more client-side interpolation videos of the disclosure by it
The flow chart of another instantiation procedure of label technique.
Specific embodiment
The program that computing device either performs on the computing device can play or export in other ways video data.
More specifically, computing device can play video by showing image (or " video frame ") sequence.In addition, computing device can be with
Audio is exported with reference to video is played.In various scenarios, user may want to pair represented in viewing and the video played
As associated data.For example, user may want to viewing it is related to appearing in the performer on the picture in played video
The data of such as movie listings etc of connection.As another example, user may want to viewing and nothing represented in video
The associated data of life entity, the title of such as specific buildings or the title of position or song.
Usually, the technology of the disclosure is related to object addition label represented in the frame to video.The label can make
User is able to access that in the frame with video the associated information (such as " metadata ") of represented various objects.For example, it retouches
The technology for calculating the position that label associated with object is shown in spatial redundancy of video in given frame is stated.In addition, it describes based on
Calculate the property of label to display and/or the technology of size.
In an exemplary aspect, such as implement the computing device of the server of the technology etc and can detect and identify to regard
One or more objects of such as face in frequency.In addition, the server can determine to include the one of the successive video frames of the object
A or multiple sequences, are referred to as being segmented.For each segmentation, which can determine the object rising in corresponding sequence
Position in beginning frame and end frame.Based on object across the starting and ending position of the sequence of video frame (" endpoint "), the server
The interpolating function of movement of the object across the sequence of the video frame can be determined to express.Express the accurate interior of movement of the object across frame
Slotting function can be nonlinear, interpolating function such as associated with the secondary motion of object.The server can utilize production
The linear interpolation function of the raw result in permitted error tolerance carries out non-linear interpolation function approximate.For example, the line
Property interpolating function can simply specify position and the interpolation in start frame and end frame of the object in video segmentation
Function effective time domain wherein.
The server is also based on approximated function, and label is associated with each sequence so that in the video frame
The position of label approx tracks movement of the object across sequence.More specifically, the server can store the expression of the object
And the information in relation to the object.In the example for being face image associated with performer in the wherein object, which can
With storage and the relevant information of the performer, such as movie listings of the performer so far.As another example, if this is right
As if the expression of the position of such as City Skyline etc, the server can store and the relevant information in the city, such as trip
It swims relevant data and is directed toward the link of tour site.In addition, the server can store label function, by special object
Label mapping to object-related information.In addition, the server can be together with right based on one or more including interpolating function
The presence of elephant and indicate that video is sent to one or more client devices by the data of various segmentations together, such as accessing
The client device of video.
And then from server receive the video client device can implement one or more technologies of the disclosure with to
User provides the access based on label of the lightweight for the information in relation to the object in the video.For example, from server
It downloads or receives in other ways after some or all video files or simultaneously, which can be that user prepare this
One or more parts of the file of sample have for its viewing and access the option of object for information about.More specifically, client
Equipment can identify all " activity " labels at the particular instance of the video.As used herein, term " activity " can be with
Label associated with object shown in current video frame is described.Spatial redundancy of video in given frame can be based on the object identified in frame
Quantity and including multiple active tags.For example, client device can be for each recognition sequence server of video file
Label associated with object represented in the sequence.In addition, the client device can with object-based Attributions selection or
Calculate label property.As several examples, which can select face round or ellipse, for building or
Building cluster selection customization polygon selects circumflex shape for the audio object of such as song etc.
The interpolating function provided using server for special object, client device can determine the object across sequence
Approximate motion (such as path).More specifically, the client device can apply the interpolating function with the every of the approximate sequence
The position of object in a video frame.Apparent position based on object in frame, client device can be with corresponding video frame
Appropriate label is exported simultaneously, so that user can access object based on the object's position in currently displayed video frame
Label.
For example, the client device can export video, and it can show video in response to receiving pause request
Frame, the video frame are based on phase covered with label associated with the object in the video frame, the wherein shape of the label and position
Interpolating function is answered to calculate.In addition, the client device can be configured label to be directed toward specific to corresponding object
Information link.For example, the client device can enable a user to interact with label it is (such as defeated by touching
Enter), thus call the link for being directed toward object-related information.
The technology of the disclosure can provide a kind of more potential advantage.For example, by exporting mark in the manner described
Label, client device can save the object in each frame that the equipment originally will be by identifying video and correspondingly output mark
The spent resource of label.For example, the client device can only be downloaded by being based on segment end points and storage object information and mark
Label interpolation functions rather than each and whole frame of video is downloaded and store label position data and to save vast resources.
The example of resource that client device can be saved by implementing the technology of the disclosure includes process resource, data storage capacity
With the calculating time.For example, the technology of the disclosure includes several computing devices in video provides the object of metadata for it
Can be particularly useful in situation.
Fig. 1 is diagram according to the server apparatus 2 of the one or more aspects of the disclosure, client device 12 and by visitor
The concept map of user interface (UI) 26 shown for video that family end equipment 12 is provided.Server apparatus 2 can be with client
Couple to end equipment 12 and other computing device communications of such as various other client devices.As some examples, service
Device equipment 2 can use one or more types communication connection communicate with client device 12, such as via including internet,
LAN (LAN), wide area network (WAN), Metropolitan Area Network (MAN) (MAN) system, such as third generation (3G) and forth generation (4G) cellular network
Etc wireless protocols, etc..
According to the one or more aspects of the disclosure, server apparatus 2 and client device 12 can not be required to keep
Continued communication connects to perform and utilize interpolation video label technique as described herein.On the contrary, for example, server apparatus 2 can
To perform one or more server side technologies of the disclosure, and data are transmitted to client device 12 by communication connection
24.When receiving data 24, client device 12 can perform one or more client-side technologies of the disclosure to perform
Interpolation video label is further interacted without requiring with server apparatus.In this way, it is even if inactive in client device
In the case that ground is coupled to server apparatus 2, such as when client device 12 does not access internet connection, client device 12
It can also show tape label video and label associated metadata.
Server apparatus 2 can implement one or more server sides of interpolation video label technique as described herein
Part.Although being illustrated as individual equipment for illustration purposes only, server apparatus 2 in various embodiments can be with
The computing device of combination including equipment, such as various host equipments and/or other interconnection.As shown in Figure 1, server apparatus 2
It can include various assemblies, including Object Identification Module 4, interpolating function module 6, video segmentation module 8 and mapping block 10.
In various embodiments, individual module can be combined into herein in relation to two or more module described functions.On the contrary,
It can be divided between two or more modules about any one module described function.
Server apparatus 2 can store either access in other ways various numerical datas such as video file (or letter
Referred to as " video ") and various knowledge datas.As an example, server apparatus 2 can access with it is various right in video
As relevant knowledge data, such as with the relevant information of performer in video occur, building in video such as occur
It to be described with geographical location etc without inanimate object and as the audio data of the part output of video, such as song
Or perform the identity of the performer of offscreen voice.The video accessed by server apparatus 2 can include video frame, can include
Image and/or picture.The particular order that the video can be arranged to be presented according to video frame is configured, such as root
It is to be output for the sequence of display according to video frame.
Object Identification Module 4 can be configured as or can be operated in other ways included in video to identify
Successive video frames set, each video frame includes the expression of object.For example, Object Identification Module 4 can identify it is each
The set of the successive frame of the expression including same object, the subset of the frame in such as entire video.The object can be at this
The performer that is represented in a little frames with visual manner or without inanimate object or can be related to being associated with the audio data of these frames
Connection.As used herein, term " sequence " can define the output of Object Identification Module 4.Sequence can be with pair that is identified
As associated, and can include being identified to represent the frame that continuously occurs of the object in the audio data of video or video
Known in the time data (for example, millisecond and/or frame count) of related data in set, such as these frames and these frames
The shape of other object, size, position simply exist.
Based on one or more parameters, recognition sequence can be the video frame block for representing object by Object Identification Module 4,
Even if certain frames in the sequence do not include the expression of the object.For example, in one example, Object Identification Module 4 can incite somebody to action
Recognition sequence is the performer represented on screen, even if the sequence includes the frame that one or more does not indicate that the performer, it is assumed that this is drilled
The situation that the expression of member does not show is not occurred with being more than in the successive frame of number of thresholds (such as two) in the sequence.
In another example, Object Identification Module 4 can identify sequence associated with offscreen voice, even if the offscreen voice is in the sequence
It can't hear during the display of certain frames, it is assumed that the situation that can't hear the offscreen voice is no more than the threshold duration of the sequence
(such as two seconds).
In some embodiments, Object Identification Module 4 can be the frame (such as one of the limited quantity after endpoint frame
A frame, two frames etc.) determine the presence of object, so that client device can be after object stops being present in video
The label of the object is exported in some time.In this way, Object Identification Module 4 can enable a user to stop in object
Label and/or associated knowledge data are accessed in some time after being present in video.For example, Object Identification Module 4
It can determine exist during extra frame of the song in the range of the several seconds after the song actually terminates, to consider such as
At the end of song being mistakenly considered by fade out or being inhibited with the simultaneous other sound of song or in song
Be video track in Insert Fragment and user still it is expected situation when the song continues to play, and therefore its label is
It is visible.By allowing object of short duration missing from sequence, Object Identification Module 4 is it is considered that the light in such as video dodges
Sparkle the situation of the foreground object of visual object either blocked in frame etc or in view of during offscreen voice or playback of songs
The pause of generation.Such object missing can also be referred to as " gap " or " hole " herein.In this way, Object identifying mould
Block 4 can implement the technology of the disclosure to consider the object caused by the common condition in video and associated audio
Missing.
In this example, the continuous representation segmentation or " cutting " that Object Identification Module 4 can be along video time by object is cover
Cover multiple sequences of the different piece of the duration continuously occurred.For example, Object Identification Module 4 can determine that object represents
There is " jump " in some frame, so as to substantive and/or rapidly related to the expression of (multiple) previous frame, such movement will
It will not logically be identified as being continuous, and be therefore defined on the sequence terminated before the frame and start in the frame
New sequence.As used herein, term " movement " can with the size variation of object, change in location, change in shape and
There are any one or more associated in variation.For example, substantive movement can (width, height, x be sat with dimensional parameter
Cursor position, y-coordinate position, diameter, radius etc.) to change be more than number of thresholds (such as the 30% of the width of video frame or height
(0.3)) it is associated.For example, jump can be associated with the error amount more than threshold error value, above-mentioned threshold error value and object
Discontinuous movement be associated.
In this example, Object Identification Module 4 can detect one or more of audio data of video and/or video pair
As, and tracking object is represented along the continuously or substantially continuous of video time.Object Identification Module 4 can be for each base
It is continuous in sheet to represent output per one sequence of object, and some such sequences overlap each other in time.For example,
Object Identification Module 4 can identify the sequence for covering frame 500-600 such as associated with the appearance of a performer and covering such as
Another sequence of frame 550-650 associated with the broadcasting of song.In this example, sequence associated with performer and song
It is overlapping in the frame 550-600 of the video.
In some embodiments, if at least one of continuous appearance frame, object is associated with multiple expressions,
Then it is included in Object Identification Module 4 or serializing device module (not shown in figure 1) coupled thereto can be from the one of object
It is a continuously to occur forming multiple sequences.For example, Object Identification Module 4, which can identify, continues through drilling for frame 500-650 appearance
The expression of member.In addition, in this example, each frame in range 550-600, the performer may have there are two expression (for example, should
The portrait of performer and the performer all represent on the screen), and each other frames in range 500-650 include the performer only
One expression.Based on various standards, which can determine closer to screen left side edge to determine in frame 550-600
Single expression before the expression of position and frame 550 belongs to identical sequence due to continuity.In addition, the serializing device module can
To determine single to represent to belong to another closer to after representing of positioning of screen right side edge and frame 600 in frame 550-600
One sequence.In other words, Object Identification Module 4 can determine " virtual objects " or object identical in other ways not
It is associated with example with sequences different from what is be overlapped on the time.
In some instances, the video segmentation module 8 of server apparatus 2 can be configured as or can be in other ways
Operated with determine first time of the sequence of the video frame there are object that Object Identification Module 4 identified and it is last when
Between, and in other examples, being described as can be impliedly by interpolating function module as the operation performed by video segmentation module 8
6 perform.In various embodiments, video segmentation module 8 can determine to deposit based on frame count, timestamp or other standards
In first and final time of the sequence of object.For example, video segmentation module 8 can determine the 500th and 600 frames pair of video frame
There is first on the screen and final time during the particular sequence of video frame in Ying Yuyi performers.Show as another
Example, video segmentation module 8 can determine the part of song from 20 corresponding to video:21 (point:Second) timestamp to correspondence
In the 25 of the video:32 timestamp plays out.The sequence of frame associated with object can be referred to as the video herein
" segmentation ", and the object identified such as video segmentation module 8 is present in first and final time in sequence herein
It can be referred to as " endpoint " of the segmentation.In all cases, video segmentation module 8 can be based on all as previously discussed with respect to other
The heuristic data of segment data etc determined by video determines segmentation.
In this example, each sequence can be divided into segmentation by video segmentation module 8 in a manner that calculating is above feasible so that
Movement of the virtual objects in each such segmentation can be described with simple mathematical function.Video segmentation module 8 can
Multiple segmentations are exported with each sequence.It is necessary that video segmentation module 8 such as can be enabled to processing based on server apparatus 2
Inspiration in the situation of integrality and provide and the wherein relevant certain details of cutting section.As an alternative, video segmentation module 8
The definition of the valid time domain of the interpolating function provided such as interpolating function module 6 can be based simply on to determine wherein to cut
Segmentation.
As used herein, the movement and/or size variation that term " segmentation " can be with wherein object can use letter
The division for the sequence that singular mathematic(al) function is indicated is associated.Term " sequence " can with virtual objects in video continuous or
It is substantially continuous to be associated.Term " endpoint " can be opened with what first in beginning-such as sequence of sequence was segmented
Head, sequence ending-such as sequence in the ending that is finally segmented or the point for occurring being segmented among sequence be associated.And then
Sequence can include one or more segmentations.Can be overlapping (example in time from the associated sequence of different virtual objects
Such as pass through shared one or more shared frames), but do not weighed in time from the associated different segmentations of single virtual object
It is folded.
As described, Object Identification Module 4 can identify multiple and different objects in video.And then in some examples
In, video segmentation module 8 can be segmented each identified Object identifying one or more.In this example, single object
Can be associated with multiple segmentations, the not a sequence of appearance of a such as performer in video, the wherein appearance are by a period of time
And/or frame is separated.In addition, video segmentation module 8 can identify Overlapping Fragment associated from different objects and/or can be with
Identification Overlapping Fragment associated with the different instances of object or " virtual " object, " virtual " object for example represent and phase
With associated multiple " virtual " objects for representing (for example, portrait of performer and performer) of object.For example, video segmentation module 8
It can identify the segmentation of the endpoint with frame 500-600 associated with appearances of a performer and have and be broadcast with song
Put another segmentation of the endpoint of associated frame 550-650.In this case, segmentation associated with performer and song exists
The frame 550-600 of the video is overlapping.In this way, Object Identification Module 4 and video segmentation module 8 can identify multiple
Object and multiple segmentations corresponding to multiple objects in video.
The interpolating function module 6 of server apparatus 2 can determine to object in each corresponding segmentation there are related
The interpolating function of connection.More specifically, interpolating function can be with the interpolating function in designated as defined in interpolating function module 6
Effective time domain.In each example, which can be the line of the position at each endpoint of segmentation based on object
Property function.For example, in object is the situation of performer on the screen occur, interpolating function module 6 can determine the performer's
Represent the position in each endpoint frame of segmentation.Position based on the expression of performer in each endpoint frame of segmentation, interpolation
Function module 6 can determine that the interpolating function is to represent object across the linear of the movement of the segmentation between two endpoint locations
Function.
In the example of the object of the visual representation of the performer such as on screen, interpolating function module 6 can should
Interpolating function is determined as linear function to represent movement of the object across segmentation.If for example, the object is visually expressed
In the lower left corner of first end point frame, and the upper right corner of rearmost end point frame is indicated on, then interpolating function module 6 can be with proximal line
Property interpolating function, assumes that the object is moved to the upper right corner with constant rate of speed across the segmentation from the lower left corner of display.It is even if right
As not meeting any linear function, and identified caused by approximate linear interpolation function across the practical movement of segmentation
Mobile to deviate with practical movement of the object across segmentation, interpolating function module 6 can also determine that the interpolating function should for approximate representation
The linear function of the movement of object, this allows the tolerable margin of error with the movement of the object.Interpolating function module 6 can be
Simple object of listing appears in each object's position (such as the position at the center of object in first and final time in segmentation
Put) aspect the linear function is expressed.
In some embodiments, the threshold value of tolerable error can be arranged with respect to video by interpolating function module 6
Size and with the practical mobile phase difference 0.8% (or 0.008) of object or 2% (or 0.02).In some embodiments, interpolation
Function module 6 can the various standard scores based on such as object size etc with different error thresholds (for example, with progressive or cumulative
Mode).For example, the object with smaller size can be assigned the frame (and/or for other frames) in framing
Smaller error threshold, and the object with larger size can be assigned the frame larger error threshold in framing
Value.
In each example, interpolating function module 6 can about object across the object linear movement of segmentation, object it is secondary
Mobile either object determines interpolating function across the change in shape or the two of segmentation.More specifically, linear movement can with it is right
As the length and/or width of the respective frame about the segmentation are associated with the movement of the object.In some instances, such as only
In the case of the linear movement for having object, interpolating function module 6 can determine interpolating function only to include the number of endpoint of the object
According to.On the other hand, secondary movement can be associated about the movement of the depth perspective of the frame in the segmentation with object.For example, two
Secondary movement can represent the variation of the perceived distance between object and the image-capturing apparatus of captured frame.Secondary movement can be originated from
In moving closer to or movement far from the image-capturing apparatus or moved by the movement of the image-capturing apparatus and/or scaling
Dynamic object, and can be expressed as representing the linear size variation of the label of the object.
In some instances, interpolating function module 6 can use other types of simple mathematical function, such as secondary letter
Number, exponential function, trigonometric function etc., with description object across segmentation movement.Interpolating function module 6 can be for will be by client
Each one or more simple mathematical functions of segmentation generation that equipment 12 receives.In addition, interpolating function module 6 can be each segmented
The one or more interpolating functions of output, with the mobile various aspects (size variation, change in location etc.) of description.In various realities
It applies in mode, interpolating function module 6 can share one or more functions with video segmentation module 8, more easily to carry
The data or feedback that can be utilized for video segmentation module 8 when determining segmentation scheme.
The mapping block 10 of server apparatus 2 can be configured as or can be operated in other ways with identify with
The associated various knowledge datas of object that Object Identification Module 4 is identified.For example, mapping block 10 can be identified including right
Knowledge data including the movie listings information of performer for being identified as being present in the frame of video as identification module 4.In various realities
It applies in mode, mapping block 10 can identify the knowledge data being locally stored on server apparatus 2 or server apparatus 2
Such as connected by internet and via the accessed knowledge data of communication connection.
In addition, the specific part in the knowledge data identified can be mapped to Object Identification Module 4 by mapping block 10
The special object identified in video.As an example, Object Identification Module 4 can be by performer and such as city etc
Geographical location be identified as in video each frame screen on.In this example, mapping block 10 can identify and performer
With the associated knowledge data in both cities.In addition, mapping block 10 can map to the specific part in the knowledge data
Each object such as maps to the movie listings information MAP of the performer to the performer, and by history and/or travel information
The city.In this way, mapping block 10 can pacify according to the related object that Object Identification Module 4 is identified in video
Arrange specific knowledge data.
In this example, mapping block 10 can be used available for the various technologies of mapping block 10 or based on being manually entered
And the sequence of each virtual objects sequence is mapped into real object and relevant knowledge data.For example, mapping block 10 is in the rank
Different virtual objects in identical time domain can be mapped to identical real object by section.In addition, mapping block 10 can be made
Obtaining can work independently of one or two in video segmentation module 8 and interpolating function module 6, because of mapping block 10
The data that these modules are provided can not be needed to when determining mapping data.
The instruction for the object that server apparatus 2 can be identified video, Object Identification Module 4 in the video and work
One or more of instruction for interpolating function is sent to client device 12 as a part for data 24.Although herein
The information that can be transmitted about server apparatus 2 to client device 12 is described, but will be appreciated that, data 24
It can be included between server 2 and client device 12 by communicating to connect the various information exchanged.Via such as servicing
The corresponding interface of the network interface of device equipment 2 and client device 12 etc, server apparatus 2 can transmit and client is set
Standby 12 can receive the instruction of the video and interpolating function of the part as the data 24 for going to client device 12.
Client device 12 can receive one represented in the video of the part as data 24, video from server 2
The indicating of a or multiple objects, the instruction of one or more interpolating functions and knowledge associated with each indicated object
Data.In various scenarios, which can include multiple video frame, and the interpolating function can specify this in the video interior
Insert the effective time domain of function.As described, in addition to other options, which can be in terms of frame count and/or timestamp
It is expressed.
Interpolation module 15 can be configured as or can be operated in other ways with for given in the video
Time (for example, for giving framing) determines the position of label associated with each indicated object.More specifically, interpolation mould
Block 15 can determine the position of the label based on the application of the interpolating function received from server apparatus 2.For example, interpolation module
15 check the data downloaded from server apparatus 2 to determine which interpolating function defined for given time.Interpolation module
15 are then input to the selected time (such as the video suspend frame number) for each defined in the time
In interpolating function, it is directed to obtain and wherein can shows corresponding label for each object of its display label in the frame
As a result.
In some instances, interpolation module 15 also calculated using interpolating function label to display shape and/or
Size.For example, being defined as in label circular, the radius or diameter of the label can be defined as by the interpolating function
The frame number of time-such as-function.As an alternative or in addition to this, in some instances, interpolation module 15 can for example based on
The attribute of the object identified in segmentation determines the shape of label.For example, in the example of face, interpolation module 15 can incite somebody to action
The shape of label is determined as round or ellipse, and in the geographical location in the city such as identified by City Skyline etc
In the case of, the shape of label can be determined as being generated using Bézier curve or other modeling techniques more by interpolation module 15
Side shape.
Interpolation module 15 can cause client device 12 to export one or more frames of video with such as in user interface
(UI) equipment 14 is shown and is for example shown by using the video player application run on client device 12
Show.Although being illustrated as a part for client device 12 for exemplary purposes, UI equipment 14 is in various embodiments
It can be detached with client device 12, and be optionally coupled to client device 12 and enable to communicate.Such as Fig. 1 institutes
Show, UI equipment 14 exports user interface (26) to show.UI 26 includes several UI elements, including video frame 27, face's object 28
With label 30.For example, face's object 28 can be with the performer, the sportsman or other that are represented on the screen during video segmentation
People is associated.In addition, interpolation module 15 can based on for face's object 28 define and from server apparatus 2 download in
Slotting function determines the position of label 30, size and shape.In addition, interpolation module 15 determines it the every of position in segmentation
A label can be associated with the knowledge data corresponding to the special object identified for the segmentation.
As shown in Figure 1, in some instances, interpolation module 15 can position label 30 based on interpolating function, make
Label 30 is crooked compared with face's object 28, i.e., do not precisely aligned with face object 28.In the particular example of Fig. 1, face
The multiple portions of portion's object 28 are located at except 30 boundary of label, such as the multiple portions of the auris dextra of face's object 28 and chin.This
Outside, label 30 covers the substantial portions that face's object 28 is not indicated that in video frame 27.As described, 12 institute of client device
The interpolating function received can within threshold error limit approximate representation face object 28 across the movement of the segmentation of video frame.
And then video frame sequence in framing, the position of label 30 that interpolating function is exported can be in the margin of error
Physical location compared with face's object 28 is deviated.
Similarly, interpolation module 15 can determine one or more sizes of label 30 using interpolating function.It is for example, interior
Diameter or radius of the label 30 in video frame 27 can be determined using interpolating function by inserting module 15.In addition, in the example of Fig. 1
In, such as by searching object type in the database that object type is mapped to label shape, interpolation module 15 can be with base
In determine face's object 28 represent face and be label 30 select circular shape.
UI 26 further includes audio tag 32.Audio tag 32 can be combined with client device 12 output video frame 27 with
The song of display is associated.Client device 12 can download with for the associated interpolation of song object defined in the song
Function, the wherein interpolating function are defined as the static or non-interpolation letter of the video segmentation played out therebetween about the song
Number.For example, the interpolating function for song object can simply indicate to whether there is any given time in video
Song object.For example, interpolation module 15 can apply received interpolating function with determine the song object not with segmentation
Specific position in any frame is associated.In some instances, interpolation module 15 can be about related to song object in segmentation
Each frame of connection determines identical position for audio tag 32.In some instances, interpolation module 15 can be by audio tag 32
Be placed in video frame 27 so that audio tag 32 do not include with current fragment the label 30 of other labels-such as-frame in
Any other label overlapping, position can change according to the placement of other labels with frame.In the particular example of Fig. 1, sound
Frequency marking label 32 are expressed as the icon of similar note.In this way, interpolation module 15 can implement the client-side of the disclosure
Technology and multiple labels are exported in a manner that label is visually separated, which improve the bodies of the user of client device 12
It tests.
In some instances, interpolation module 15 can so that UI equipment 14 is defeated in response to certain conditions-such as reception user
Enter-and one or two in display label 30 and audio tag 32.For example, UI equipment 14 includes such as touch screen wherein
Etc the embodiment there are sensitive equipment in, interpolation module 15 can detect user input, such as utilize finger or touch-control
The touch input that pen is provided in UI equipment 14 corresponding to the region of pause button 34.In response to detecting at pause button 34
Touch input, interpolation module 15 can cause client device 12 pause play video, such as pass through client device
The video player application run on 12 stops showing in video frame 27.In addition, interpolation module 34 may operate in video frame 27
Locate the interpolating function of definition to determine one or two in wherein output label 30 and audio tag 32 for display.Another
In one embodiment, interpolation module 15 can in response to the touch input at pause button 34 output label 30, while
Video exports audio tag 32 during showing.
The label selecting module 20 of client device 12 can be configured as or can be operated in other ways with
Detection is to the selection of one or two in label 30 and audio tag 32.For example, include the implementation of touch screen in UI equipment 14
In mode, label selecting module 20 can be based on using finger or stylus or substantially in label 30 and/or audio tag
The touch input provided at 32 detects the selection.In response to detecting the selection to label 30 and/or audio tag 32, label
The instruction that user selects can be transmitted to knowledge data output module 22 by selecting module 20.
And then knowledge data output module 22 can be exported and is mapped in received knowledge data and selected mark
A part for associated special object is signed to be shown.For example, it is examined in response to being received from label selecting module 20
The instruction of the selection to label 30 measured, knowledge data output module 22 can cause 4 explicit knowledge's data frame 38 of UI equipment.
In the example of fig. 1, knowledge data frame 38 can include it is associated with the performer represented by face object 28 partly or entirely
It movie listings and/or is directed toward associated with performer additional knowledge data and links.Similarly, if label selecting module
The instruction of the selection of 20 relaying audio tags 32, knowledge data output module 22 can export title of song, album and/or art
Family's information and/or with being associated with the relevant other knowledge datas of the song of audio tag 32.
Using interpolating function, object data and the knowledge data downloaded from server 2, even if client device 12 is in client
End equipment 12 is not currently connected to provide the video of tape label in the case of server apparatus 2.In addition to this, by making
It is determined how with interpolating function as video display label, the technology of the disclosure can enable client device 12 to save visitor
Family end equipment 12 is originally by the data and/or audio tag by the way that identification face object 28 is downloaded and stored in each video frame
32 song and correspondingly export the resource spent by corresponding label.It is carried in video including client device 12 for it
For such as face object 28 of knowledge data, song object and possible other objects several objects in the case of, client
End equipment 12 can be by only being stored and being downloaded based on segment end points rather than each and each video frame label position data
Object information and label function and save greater amount of resource.Client device 12 can by implement the technology of the disclosure and
The example of the resource of saving includes process resource, data storage capacity and calculates the time.
Fig. 2 is the service that diagram is configured as performing one or more server side interpolation video label techniques of the disclosure
The block diagram of the details of the example embodiment of device equipment 2.In particular implementation shown in Fig. 2, server apparatus 2 can wrap
Include one or more processors 70, one or more communication units 72 and one or more storage devices 78.
In each example, one or more processors 70 are configured as implementing to perform in server apparatus 2
Function and/or process instruction.For example, processor 70 can pass through in storage device 78 or in other ways to being stored in
The instruction that storage device 78 accesses is handled.Such instruction can include one or more operating systems 80, Object identifying
One or more of module 4, interpolating function module 6, audio parsing module 8 and mapping block 10 component.
In the embodiment shown in figure 2, server 2 further includes one or more communication units 72.In one example,
Server apparatus 2 is using communication unit 72 via the one or more networks and external equipment of such as one or more wireless networks
It communicates.Communication unit 72 can include such as Ethernet card network interface card, optical transceiver, RF transceiver or
Person is other types of can to send and receive one or more of equipment of information.The other examples packet of such communication unit
It includes in such as mobile host equipment3G, 4G andThe network interface and universal serial bus of radio
(USB).In some instances, server apparatus 2 is set using communication unit 72 by network and such as one or more clients
Standby external equipment carries out wireless communication.
One or more storage devices 78 can be configured as the information stored in server apparatus 2 during operation.
In some examples, storage device 78 is described as computer readable storage medium and/or one or more computer-readable storages
Medium.In some instances, storage device 78 includes temporary storage, it means that the main application of storage device 78 is not long
Phase stores.In some instances, storage device 78 is described as volatile memory, it means that storage device 78 is in computer
Stored content is not preserved during shutdown.The example of volatile memory includes random access memory (RAM), dynamic random is deposited
The volatile storage of access to memory (DRAM), static RAM (SRAM) and other forms known in the art
Device.In some instances, storage device 78 is used to storage program instruction to be performed by processor 40.In one example,
The software or be used for the interim storage information during program performs that storage device 78 is run on server apparatus 2.
Each in component 70,72 and 78 can be interconnected (physically, communicatedly and/or operatively) to carry out
Inter-component communication.In some instances, one or more communication channels 76 can include system bus, network connection, interprocedual
Communication data structure or any other channel for being used for transmission data.As an example in Fig. 2, component 70,72 and 78
It can be coupled by one or more communication channels 76.
One or more operating systems 80 can control server apparatus 2 and/or one or more functions of its component.Example
Such as, operating system 80 can in Object Identification Module 4, interpolating function module 6, video segmentation module 8 and mapping block 10
One or more interacts, and can facilitate and be passed through between these modules and one or more processors 70 and communication unit 72
The one or more interactions carried out by communication channel 76.In some instances, Object Identification Module 4, interpolating function module 6,
One or more of video segmentation module 8 and mapping block 10 can be included in operating system 80.In other examples, it is right
As one or more of identification module 4, interpolating function module 6, video segmentation module 8 and mapping block 10 can be in servers
Implement except equipment 2, such as implement in network site.In some such examples, server apparatus 2 can use communication unit
By being used as, " way access and implementation well known to cloud computing are illustrated member 72 about server apparatus 2 and its corresponding assembly
The function that various modules provide.
As described, Object Identification Module 4 can be operated to identify special object represented in video, such as
Include the people of such as performer or sportsman, the specific building facilities of the audio data of such as song or offscreen voice, such as building,
And the geographical location of such as city or country.In order to identify such object in video, Object Identification Module 4 can be implemented all
Such as the various identification technologies of face recognition, speech recognition, audio fingerprint and pattern-recognition etc.In addition, Object Identification Module 4 can
With by the way that the image in video or audio data and the image confirmed or audio data are compared in identification video
Object, above-mentioned confirmed image or audio data such as store storage device 78 and/or server apparatus 2 communicates coupling
The part of the data of the remote equipment of conjunction.In some embodiments, Object Identification Module 4 is also based on user's input to know
Not one or more objects.
Interpolating function module 6 can be operated determines interpolating function with the object identified for Object Identification Module 4,
And video segmentation module 8 can be that object determines corresponding video segmentation based on the interpolating function.As described, it is interior
Slotting function can express the attribute of label associated with object.The example of such attribute of label can include with Descartes
The position of coordinate expression and the size data and shape data of label.
In ellustrative example, interpolating function module 6 can receive the finger of face's object from Object Identification Module 4
Show, the performer being present in video segmentation such as identified.The phase of face's object in each endpoint frame based on the segmentation
Answer position and size, interpolating function module 6 can generate expression as the object of the function of time across segmentation linear movement with/
Or the interpolating function of the shape of object.
Use the example value of cartesian coordinate and the dimension data of circular object, in one example, server apparatus 2
Interpolating function module 6 interpolating function is determined as to linear function between following endpoint location：
In above example endpoint (1) and (2), constant is represented about expressed by the score of the specific dimensions of quadrilateral frame
Numerical value.More specifically, x attributes can be denoted as the horizontal position at the circle center of the score of video frame width, from video frame
Left side edge offset, and y attributes can be denoted as the upright position at the circle center of the score of the height of video frame, from
The top edge offset of video frame.In addition, d attributes can be denoted as the circular diameter of the score of video frame catercorner length.It should
Example endpoint can with video from frame 500 to frame 600 and by the circle it is general mobile be expressed as to the left (fromIt arrives)、
Downwards (fromIt arrives) and become larger (fromIt arrives).The interpolating function module 6 of server 2 can be based on endpoint (1) and (2) obtain
Cutting edge aligned function is mobile to describe the approximation of the circle so that all properties (x, y and d) are all across the time model from frame 500 to frame 600
It encloses and is changed with constant rate of speed.
In the case of not represented visually in frame of the wherein object in segmentation such as in the case of narration, interpolation letter
Digital-to-analogue block 6 can determine the object across segmentation be static, thus generate static interpolating function.
Equation (3) is the example of static interpolating function.More specifically, the object that equation (3) is provided in particular frame exists
Within the segmentation or except binary system instruction.In the example of equation (3), function S indicate object frame number 500 to
600 exist, and the frame before frame number 500 or after frame number 600 is not present.
In some instances, interpolating function module 6 can generate the interpolation letter for considering perceived object size difference
Number.For example, interpolating function module 6 can determine the diameter change of the round or substantially circular object of such as face.Directly
Diameter increase may with as object camera may be moved close to or caused by camera is towards the amplification of object
The reduction of the distance perceived between object and image-capturing apparatus is associated.In other examples, interpolating function module 6 can be with
Based on other sizes come the difference of determining perceived size, above-mentioned other sizes such as square sides, arbitrary polygon
Perimeter, one or two oval axis etc..And then the shape and/or size of identified object can be with being associated with this
The shape and/or size of the label of object are associated.In this way, interpolating function module 6 can determine to consider for object
The shape and/or size variation of the linear movement of the object and the object and the static attribute for also considering certain object types
Interpolating function.
It is associated for interpolating function determined by the object at the frame count represented by " f " with interpolating function module 6
Example original data set can be as follows：
F=100, Circle (x=0.3, y=0.2, d=0.1)；
F=101, Circle (x=0.31, y=0.21, d=0.1)；
F=102, Circle (x=0.321, y=0.22, d=0.1)；
F=103, Circle (x=0.329, y=0.23, d=0.1)；
F=104, Circle (x=0.34, y=0.24, d=0.1)；
F=105, Circle (x=0.352, y=0.25, d=0.1)；
F=106, Circle (x=0.357, y=0.26, d=0.1)；
F=107, Circle (x=0.371, y=0.27, d=0.1)；
F=108, Circle (x=0.381, y=0.28, d=0.1)；
F=109, Circle (x=0.39, y=0.29, d=0.1)；
F=110, Circle (x=0.40, y=0.30, d=0.1)；
F=119, Circle (x=0.49, y=0.39, d=0.1)；
F=120, Circle (x=0.6, y=0.4, d=0.1)；
F=121, Square (x=0.5, y=0.4, d=0.1)；
F=122, Square (x=0.5, y=0.4, d=0.11)；
As used herein, item " R " may refer to being produced to framing in the range of frame 100-122 according to data above
The lookup function of raw corresponding initial data.R (120)=Circle (x=0.6, y=0.4, d=0.1).
In this example, become with from frame 100 to the associated circle of the object of frame 110 approximately according to the following formula
Change：
X=0.3+0.1p,
Y=0.2+0.1p,
D=0.1,
In this example, p represents ratio/percentage/progress.The value of p for f=100 to 110 can from 0.0 to 1.0 line
Property variation.More specifically, the subregion for corresponding to frame 100 to 110 in the domain of initial data in frame 100 to 122 can be approx
Meet with minor function：
S(f:100...110)=Circle (x=0.3+0.1p, y=0.2+0.1p, d=0.1)
When compared with described initial data, which can include one or more errors.It for example, should
Interpolating function can appear in f=106 about the worst error of data above set, and wherein function S generates Circle (x=
0.36, y=0.26, d=0.1), but R (106) generates Circle (x=0.357, y=0.26, d=0.1), this instruction is in x
Coordinate has 0.003 error, and available fall into carries out approximately the value of R for the tolerable error in the special domain of S (f)
Within the threshold value of the tolerable error of function S (f).In other words, S can be defined as the segmentation in R.However, in some feelings
Under condition, S can not be included among the final segmentation of R determined by video segmentation module 8.
Above example elaborates function S using three linear subfunctions of p, x and y and the constant subfunction of d.Interpolation letter
Digital-to-analogue block can utilize any mathematically simple type function, as long as the function can facilitate the expression of segmentation.Such letter
Single function can including but not limited to such as 0 to 2 lower order polynomial expressions, wherein exponent number 0 is constant function (y (x)=c), rank
Number 1 is linear (y (x)=ax+b), and exponent number 2 is secondary (y (x)=ax2+ bx+c), inverse proportion function (y (x)=c/x),
Etc..
As described with reference to fig. 1, in some cases, video segmentation module 8 can determine and 4 institute of Object Identification Module
The associated particular video frequency segmentation of object of identification.For example, video segmentation module 8 can identify the continuous sequence of video frame, make
The each frame for obtaining the sequence includes the expression of object.As another example, video segmentation module 8 can independently or with it is right
As identification module 4 is cooperated come the continuous sequence for identifying video frame so that object do not lack the sequence more than number of thresholds
Successive video frames.
In addition, sequence caused by Object Identification Module 4 can be divided into multiple segmentations by video segmentation module 8.One
In a little examples, video segmentation module 8 can pass through the threshold value for the tolerable " jump " for setting movement of the object between successive frame
It is horizontal and so do.Sequence cutting based on jump can be cut independently of above with respect to 4 described sequence of Object Identification Module
Undercutting row carries out in addition.For example, video segmentation module 8 can determine maximum tolerable jump corresponding to 25%
Value, the i.e. factor of 0.25 parasang (for example, centimetre or inch).In some embodiments, which can represent
The score of the score of frame size, such as frame height or width.In the example of the secondary movement of object, video segmentation module 8 can
With such as corresponding to the face's object of the object identified-such as-the round particular measurement of diameter represented of estimation two
Occur detecting the error beyond threshold value in the case of the deviation more than the factor of 0.25 parasang between a successive video frames.
In one example, diameter associated with object can become in 0.3 parasang in the frame number 550 of video
0.5 parasang being changed in frame number 551.In this example, video segmentation module 8 can be single as 0.2 distance using determination deviation
Position, thus falls within tolerable error threshold.As a comparison, if video segmentation module 8 determines the diameter of face's object
From frame number 600 to the factor of 601 deviation of frame number, 0.3 parasang, then video segmentation module 8 can determine segmentation transformation.
More specifically, video segmentation module 8 can determine that frame number 600 forms the endpoint of the segmentation of face's object (for example, last
Frame), and frame number 601 forms the endpoint (for example, initial frame) of the subsequent segment of face's object.For example, video segmentation mould
Block 8 determines the diameter from frame number 601 to the factor of 602 deviation of frame number, 0.4 parasang, and video segmentation module 8 can be true
Fixed another segmentation transformation again, so that frame number 601 is segmented about the object as single frames.In addition, video segmentation module 8
Can detect corresponding to successive frame it is multiple subsequently match be more than threshold value mobile jump.In this case, video segmentation
Module 8 can be that the object generates a series of single frames segmentations.In this way, it is quick or discontinuous in video in visual object
In the case of movement, video segmentation module 8 can generate single frames segmentation or include the segmentation of relatively low amount of frame.
Whether video segmentation module 8 can use mathematically more simple function progress based on the data in segmentation
It describes to determine segment end points.In various embodiments, video segmentation module 8 can be become based on the variation of such as shape type or movement
The standard of gesture variation etc detects segment end points.In the determining example changed based on shape type, different shape type
It can not be usually described in a manner of mathematically simple jointly.For example, in raw data sample listed above, shape
Type becomes square at the conversion from f=120 to f=121 from circle.Video segmentation module 8 can dissolve this become
It is interpreted as the signal that new segmentation starts in f=121.However, video segmentation module 8 can be by by more special shape generalization
The variation of certain shape types is identified as " compatibility " for more general shape.For example, if circular shape becomes revolving
The ellipse or vice versa turned, video segmentation module 8 can upgrade to the circle with mutually isometric and with arbitrary rotation
The ellipse turned.In this way, video segmentation module 8 can mitigate the shape type variation at transition point.
In the situation of mobile trend variation, if video segmentation module 8 determines specific shape attribute (such as round center
X position) it is increased, but become reducing from particular frame suddenly, then video segmentation module 8 can determine that the particular frame may be
Start the data point of new segmentation, otherwise interpolation functions function module may be no longer able to adjust previously identified interpolating function
To include the data at new frame in the case where not generating bigger error.
In the example of coordination technique implemented in interpolation function module 6 and video segmentation module 8, video segmentation module 8
It determines such as in frame f0The segmentation at place starts, wherein f0It is time/frame number of the beginning of entire sequence or makes immediately previously
The last segmentation limited with the workflow.In addition, candidate segment is defined as from f by video segmentation module 80To f, wherein f quilts
It is initialized as being equal to f0.Next, video segmentation module 8 feeds f to interpolation function module 60Original frame data between f, and
And the interpolation letter of all data of the generation in certain errors tolerance threshold in these approximate frames is submitted to interpolation function module 6
Number.The example collaboration technology can be iterative process, if for example, interpolating function module 6 can store previously provided from
f0To the data close to the frame before f, then video segmentation module 8 can need only provide for the data at f.In addition, interpolating function
Access can be directed to original frame data by module 6, and in this case, video segmentation module can be only needed for interpolating function
Module 6 is provided obtains the range " f of frame data for it0To f ".
If interpolating function module 6 can generate requested interpolating function, video segmentation module 8 can be by by f
Being incremented to next can obtain it frame of data and candidate segment is extended, and ask interpolating function module 6 to elder generation
Preceding generated function is adjusted or provides new function to include the data at new frame.The process continues up to
Interpolating function module 6 cannot generate appropriate function in the case of no more than error tolerance threshold, at this time interpolation work function module 6
The particular frame f' for terminating candidate segment to video segmentation module 8 can be provided, be in f0Between the last frame before f.F it
Preceding last frame is typically f-1, is missing from the case of arbitrary frame-skip as described above.
Process described above can also determine that f has been in entire sequence when video segmentation module 8 is causing f to be incremented by
Determine that the data at new f cannot in the case that the ending of row cannot be incremented by so as to frame count or in video segmentation module 8
It represents to come since f0Play and exclude the nature of the data point of f (for example, by checking change in shape and substantive displacement)
It is terminated in the case of continuous.If the process terminates in this way, the segmentation definition of the sequence is by video segmentation module 8
From f0To the preceding value of f, the interpolating function returned recently with interpolating function module 6.
In a manner of described above, which can be with the segmentation in link definition sequence, until in the sequence
All frames all handled.The workflow can last frame be included in segmentation in terminate at once or alternatively to
Best segmentation scheme is found about data approximation accuracy, interpolating function simplicity or other desired attributes and is used not
Same parameter restarts.Different parameters can for example include only as the different decisions made by video segmentation module 8, such as whether
It is opened and segmented in the point off of mobile jump, change in shape or mobile trend variation.
It is described in following sample workflow in the case where not cooperateed with other modules of server apparatus 2 by interior
Insert the example technique that function module 6 is implemented.In the example technique, interpolating function module 6 can be as precisely as possible to regarding
Piecewise approximation interpolating function defined in frequency division module 8.In examples described below, interpolating function module 6 can make
Interpolating function is determined with simple embodiment, the endpoint based on the candidate segment that video segmentation module 8 is provided.
When receiving with data R (f0) first frame f0When, interpolating function module 6 can use received number
According to defining the shape type of final interpolating function.And then interpolating function module 6 can return and always generate the unmodified frame
Data function, i.e.,
S(fs：f0..f0)=R (f0)
Wherein " fs" be the function parameter, but its range is restricted to f0.When receiving with the arbitrary of data R (f)
During subsequent frame f, if R (f) is not indicated that and R (f0) identical shape type, then interpolating function module 6 can indicate video segmentation
Module 8 terminates the segmentation in the previous frame by described workflow.
Interpolating function module 6 can be in domain " f0...f”(f0≤fs≤ f) defined in interpolating function S (fs) so that the process
And all shape attributes are all linear-interpolated.Interpolating function module 6 can be used only from R (f0) and the data of R (f) it is as follows
It is determined：
S(fs：f0..f)=X (as, bs, cs...), wherein
as=a0+(a-a0) p,
bs=b0+(b-b0) p,
cs=c0+(c-c0)p
Etc., whereinAnd X, a0, a ... matching
R(f0)=X (a0, b0, c0...), R (f)=X (a, b, c ...)
Item X can represent R (f0) in shape title, such as " Circle "；Item a, b, c... can represent shape attribute
Value, such as x coordinate, y-coordinate and diameter of a circle.
For each frame fm, wherein f0＜ fm＜ f, and wherein R (fm) be defined and be not included in caused by the jump of frame
" gap " among, interpolating function module 6 can be used calculates a with superior functions、bs、csDeng interpolation results, and by institute
The value of calculating and R (fm) actual value be compared.If being not above the difference of error threshold, interpolating function module 6 can
To return to function S defined above as interpolating function.Otherwise, interpolating function module 6 can be indicated to video segmentation module 8
The segmentation is terminated at the previous frame Jing Guo the example workflow.
It is performed using the example of the workflow of the initial data in R defined above as presented below.Assuming that frame
100 to 119 interpolating functions that are processed and finally being generated are：
S(fs：100..119)=Circle (x=0.3+0.19p, y=0.2+0.19p, d=0.1),
Wherein p=(fs-100)/19
With R (100)=Circle (x=0.3, y=0.2, d=0.1)；| following end-point data：
R (119)=Circle (x=0.49, y=0.39, d=0.1)
Video segmentation module 8 can be indicated to interpolation function module 6 by R (120)=Circle (x=0.6, y=0.4, d
=0.1) it is included in the interpolating function to be generated.In this example, every frame increment of x is significantly bigger than previously (being in 0.01)
(being in 0.11).And then interpolating function module 6 can generate the following tentative function corresponding to frame 100 to 120：
S(fs：100..120)=Circle (x=0.3+0.3p, y=0.2+0.2p, d=0.1),
Wherein p=(fs-100)/20
When being verified to frame 119, interpolating function module 6 can determine S (119)=Circle (x=0.585, y=
0.39, d=0.1) it, is compared with R (119)=Circle (x=0.49, y=0.39, d=0.1), x deviations 0.095.Example
Such as, if error tolerance is arranged on 0.05, interpolating function module 6 can determine that the candidate functions of covering frame 100 to 120 are more than
It threshold value and should be rejected.As a result, in this example, interpolating function module 6 can be indicated to video segmentation module
Frame 119 terminates the segmentation, and starts new segmentation in frame 120.In this example, new segmentation can only include frame 120, because
There is the shape (square) incompatible with frame 120 (circle) for frame 121.
The mapping block 10 of server apparatus 2 can be operated with by the specific part and Object identifying of knowledge data 82
The special object that module 4 is identified is associated.As an example, knowledge data 82 can include performer and the artistical shadow of offscreen voice
Piece directory information, the stroke analysis of sportsman, musician discography etc..And then as an example, mapping block
10 can be associated with specific actors by particular film catalogue.In addition, mapping block 10 can be stored such as knowledge data 82
Specific part and corresponding object between the map information being associated with etc storage to mapping data 84.In this way, it reflects
Penetrate module 10 can utilize the relevant portion of knowledge data 82 is corresponding right with being identified in video by video identification module 4
Mapping data 84 are expanded as associated information.The interpolation letter that mapping block 10 can also be generated interpolating function module 6
Number maps to corresponding object, and the information is stored to mapping data 84.
And then server apparatus 2 can will be identified via communication unit 72 in video, video by Object Identification Module 4
Each object instruction and the instruction of each interpolation module that is generated of interpolating function module be sent to client device.
In addition, the specific part of knowledge data 82 and mapping data 84 can be sent to client device by server apparatus 2.With this
Kind mode, server apparatus 2 can implement one or more server side technologies of the disclosure to provide phase to client device
Information is closed so as to provide interpolation video label.By providing relevant information to client device, even if not living in client device
In the case of server apparatus 2 is coupled to dynamicly to be communicated, server apparatus 2 is it is also possible that client device can
Provide a user the experience of interpolation video label.
Fig. 3 is the client that diagram is configured as performing one or more client-side interpolation video label techniques of the disclosure
The block diagram of the details of end equipment 12.Client device 12 can include, as one or more in various types of equipment or
Person is as part of it, apart from the others, above equipment such as mobile phone (including smart phone), tablet computer, online
Sheet, laptop computer, desktop computer, personal digital assistant (PDA), set-top box, television set and wrist-watch.
In each example, one or more processors 40 are configured as implementing function and/or process instruction so as in visitor
It is performed in family end equipment 12.It is stored in storage device 48 for example, processor 40 can be handled or can pass through in other ways
Its instruction to access.Such instruction can include component, the object UI EM equipment modules of one or more operating systems 54
52nd, interpolation module 15 and its component, label selecting module and knowledge data output module 22.
Client device 12 can also include one or more communication units 44.In one example, client device 12
It is communicated using communication unit 44 via one or more networks of such as one or more wireless networks with external equipment.It is logical
Letter unit 44 can include the network interface card of such as Ethernet card, optical transceiver, RF transceiver or can send simultaneously
Receive one or more of any other type of equipment of information.The other examples of such communication unit can include all
In mobile host equipment3G, 4G andThe network interface and universal serial bus (USB) of radio.
In some instances, client device 12 passes through network and such as one or more servers and/or master using communication unit 44
The external equipment of machine equipment carries out wireless communication.
In each example, input equipment 42 is configured as from the user by tactile, audio or video feedback reception
Input.The example of input equipment 42 includes there are sensitive display (such as touch screen), mouse, keyboard, voice response system, regards
Frequency camera, microphone or any other type of equipment for detecting order from the user.In some instances, exist
Sensitive display includes touching sensitive screen curtain.
One or more output equipments 206 can also be included in client device 12.In some instances, output equipment
46 are configured with tactile, audio or video stimulation provides a user output.In each example, output equipment 46 includes depositing
In sensitive display (for example, touch screen), sound card, video graphics adaptor card or for converting a signal into people or machine
It will be appreciated that appropriate form any other type of equipment.The other example of output equipment 46 includes loud speaker, cathode
Ray tube (CRT) monitor, liquid crystal display (LCD) or any other type that output can be appreciated that user's generation
Equipment.
Each in component 40,42,44,46,14 and 48 can interconnect (physically, communicatedly and/or operatively) with
Just inter-component communication is carried out.In some instances, one or more communication channels 50 can include system bus, network connection,
Interprocess communication data structure or any other channel for being used for transmission data.As an example in Fig. 3, component 40,
42nd, it 44,46,14 and 48 can be coupled by one or more communication channels 50.
One or more operating systems 54 can control client device 12 and/or one or more functions of its component.
For example, operating system 54 can be with UI EM equipment modules 52, interpolation module 15, label selecting module 20 and knowledge data output module
One or more of 22 interact, and can facilitate these modules and one or more processors 40 and communication unit 44
Between one or more interactions for being carried out via communication channel 50.In some instances, UI EM equipment modules 52, interpolation module
15th, one or more of label selecting module 20 and knowledge data output module 22 can be included in operating system 54.
In other examples, one in UI EM equipment modules 52, interpolation module 15, label selecting module 20 and knowledge data output module 22
Or it is multiple can implement except client device 12, such as network site implement.In some such examples, client
Equipment 12 can use communication unit 44 by be known as " way access of cloud computing and implement about client device 12 and
The function that the various modules that its corresponding assembly is illustrated provide.
Client device 12 can also include application module 56.Application module 56 can be configured as or can be with other
Mode is operated to perform operation based on the various conditions for such as detecting user's input etc.As an example, one
A or multiple application modules 12 can export UI 26 in various embodiments using UI equipment 14.In addition, related application module 56 can
To receive user's input, and the user received by the end equipment 12 based on client inputs generation output.Application module 56 is also
Output can be transferred to user by the various tools that mobile client device 12 is provided, such as by via UI equipment 4
(for example, in UI 26) shows the output with the form that can be read.In various embodiments, application module 56 can implement client
The function that the function and/or the opposing party's (being frequently referred to as " third-party application ") that the manufacturer of end equipment 12 is provided are provided.
Client device 12 such as can receive institute in such as video, video via communication unit 44 from server apparatus 2
The data of instruction, interpolating function, knowledge data and the mapping data of the object of expression etc.In addition, client device can incite somebody to action
Received information stores the downloading data 58 to storage device 48.And then modules shown in Fig. 3 can implement this
In described client-side interpolation video label technique when access the relevant portions of downloaded data 58.
For example, interpolation module 15 can determine the instruction of each object in downloaded video from the data 58 downloaded,
And interpolating function associated with each such object.More specifically, each interpolating function can specify corresponding interpolation letter
Number time domain effective wherein.In other words, interpolation module 15 can determine particular fragments based on specified time domain, the object
It is continuously or substantially continuously represented in video during the segmentation.
In addition, interpolation module 15 can cause 52 output label of UI EM equipment modules so as to combine the segmentation corresponding frame into
Row display.In one example, which can be embedded in corresponding video frame by interpolation module 15 so that UI equipment moulds
Block 52 via one or more conditions of the pause request received by input equipment 42 etc in response to such as exporting the mark
Label.
As described in Figure 1 on, label selecting module 20 can receive the selection of label to being shown with reference to present frame.
More specifically, label selecting module 20 can receive selection request via input equipment 42, such as by input equipment 42
The touch input provided at touch screen.In addition, knowledge data output module 22 can be examined in response to label selecting module 20
Measure select and so that UI EM equipment modules 52 export correlation via output equipment 46 and/or UI equipment 14 for tape label object knows
Know data.
By implementing client-side interpolation video label technique described above, client device 12 can provide so that
The user experience of knowledge data access can be carried out to tape label object, while reduce computing resource consumption.More specifically, it substitutes
Based on each frame store label shape, size and location data, client device 12 can implement the technology with for
Endpoint (or " boundary ") the storage label data of video segmentation.Using received interpolating function, client device 12 can be with
For given object using the interpolating function with determine in the given time unit of a such as frame how display label.Replacement is deposited
Intermediate data points in storage segmentation, client device 12 can be estimated using interpolating function as described herein and Technique dynamic
Shape, the size and location data of label.
Fig. 4 A-4C are the concepts of the user interface with interpolation video label of the one or more aspects according to the disclosure
Figure.More specifically, Fig. 4 A-4C illustrate three frames of video segmentation that the variation including face's object 28 shown in FIG. 1 represents.
In addition, the variation that Fig. 4 A-4C include label 30 shown in FIG. 1 represents.The change aspect of face object 28A-28C may be due to
Caused by the attribute of video segmentation, and the change aspect of label 30A-30C may be then due to the interpolating function mould of such as server 2
Application of the block 6 about interpolating function determined by the video segmentation.For exemplary purposes, Fig. 4 A-4C have herein been described as
Initial frame, intermediate frame (for example, positioned at middle part of the video segmentation) and the last frame in video segmentation are represented respectively.In concept
On, linear interpolation function can be considered as the straight line at the center of each object substantially in the video frame of connection figure 4A-4C.This
Outside, which can be scaled up to greater amount of face's object and represent, such as across the end of face's object of video segmentation
It puts to the progress of endpoint.Moreover, for exemplary purposes, Fig. 4 A-4C are herein about equipment illustrated in Fig. 1-3 and component
It is described.
Fig. 4 A are illustrated positioned at the first face object 28A of first position 100 and the first label 30A.Interpolation module 15
Label 30B is calculated based on received interpolating function and shows label 30B associated with face object 28B.
Fig. 4 B illustrate the second face object 28B in the second position 102, and said second position 102 is positioned essentially at
The left side of first position 100 indicates linear movement of the face's object 28 along trunnion axis.In addition, the second face object 28B can be with
It is bigger than the first face object 28A, movement of the instruction face towards image-capturing apparatus.It is interior based on received interpolating function
Module 15 is inserted to calculate label 30B and show label 30B associated with face object 28B.
As shown, when with the first label 30A about the placement of the first face object 28A compared with when, the second label 30B
Placement can be with the right side of oblique second face label 28B.For example, in the second face object 28B such as chin towards left half
It may be not included in the second label 30B.In addition, the second label 30B can include not representing second face's object in present frame
The substantial portions of 28B are located at the left side of the second face object 28B.Such inaccuracy may be due to interpolation module
15 are calculated the position of label using interpolating function rather than exact label position are stored based on each frame.However, in this way
The inaccuracy reduction that described interpolation video label technique is provided herein computing resource consumption in terms of be can be with
Tolerance.
Fig. 4 C illustrate the face object 28C in the third place 104.Interpolation module 15 is based on received interpolation letter
Number calculates label 30B and shows label 30B associated with face object 28B.As shown, the third place 104 is in frame
The interior left side that can be located substantially on the second position 102 and upward, instruction face object 28 is along the linear of horizontal and vertical axis
It is mobile.In addition, third face object 28C can be bigger than the second face object 28B, instruction face object 28 is set towards picture catching
Standby further movement.
On client device 12, calculated by application interpolating function module 6 about the particular frame of video segmentation in
Function is inserted, interpolation module 15 can determine shape, position and the size of label and be carried out to combine particular frame or other time unit
Display.In the described example of face's object of circular shape is determined to it about interpolation module 15, interpolation module 15 can be with
The attribute of circular tag is determined to be shown with reference to the particular frame.More specifically, interpolation module 15 can be based on received
Interpolating function determine the position of label, shapes and sizes.Interpolation module 15, which can be configured as, will be expressed as two ends
The interpolating function of point is construed to the linear function between the endpoint.
Fig. 5 is to illustrate server apparatus to implement the interpolation video of one or more server sides of the disclosure by it
The flow chart of the instantiation procedure 110 of label technique.Merely for exemplary purpose, server apparatus 2 of the process 110 about Fig. 1-2
It is described.Process 110 can identify one or more of video object as starting (112) using Object Identification Module 4.Such as
Described, Object Identification Module 4 can be identified including people (for example, performer or sportsman), such as geographical location, song and picture
Various types of objects of the visual representation of audio object, the building of outer sound etc etc..In addition, mapping block 10 can will be all
The knowledge data of information etc as that can be obtained from various internet sites map to Object Identification Module 4 identified it is each right
As (114).In this example, video segmentation module 8 can determine original tag data, such as in relation to there are Object identifyings in video
The information frame by frame for one or more objects that module 4 is identified.
In some embodiments, the expression of the object identified can be divided into and have by the component of server apparatus 2
The virtual objects of non-overlapping appearance.It is regarded more specifically, video segmentation module 8 can determine that identified object is appeared in by it
The continuous appearance of one or more of frequency or continuous time range.In this example, continuous appearance can be restricted to the single time
Example, the single frame of any consecutive frame such as occurred wherein without the object.Such as all can not be to regard in any frame
Audio data that feel mode represents or for geographical location, such as the appearance of object defined in Object Identification Module 4
Definition can be abstract.
In addition, Object Identification Module 4 can identify the overlapping appearance of the same object in frame, such as wherein each piece display
4 × 4 video walls of same person, this people are the performers visually represented on the screen also described, are appeared in identical
Portrait of people and this people in frame, etc..Virtual objects can also be identified as the " secondary of object by Object Identification Module 4
This ", it may be used as the differentiation between multiple examples of same object.Video segmentation module 8 and interpolation functions module 6 can be
The each of object identifies copy generation segmentation and interpolative data.
Video segmentation module 8 can determine one or more by each object that Object Identification Module 4 identifies in video
A frame sequence or segmentation.In addition, server apparatus 2 can by the instruction of each object, instruction segmentation data, with each being known
It the relevant knowledge data 82 of other object and relevant knowledge data 82 is linked to the mapping data 84 of corresponding object stores
To storage device 78.In this way, server apparatus 2 can implement the technology of one or more server sides of the disclosure,
So that client device is capable of providing the user experience of interpolation video label, while save computing resource.
Fig. 6 is to illustrate client device to implement the interpolation video of one or more client-sides of the disclosure by it
The flow chart of the instantiation procedure 130 of label technique.For exemplary purposes, the client about Fig. 1 and 3 herein of process 130
Equipment 12 and its component are described.Process 130 can connect in client device 12 via communication unit 44 from server apparatus 2
Start (312) when rating frequency and relevant information, the instruction of above-mentioned relevant information such as object, segment end points data, relevant knowledge
Data, mapping data and interpolating function.In some cases, client device 12 can receive video and data when downloading,
Such as by the way that received data and file are saved in storage device 48.In other cases, client device 12 can be with
" streaming " video and data, such as by receiving multiple videos and data portion with lasting basis so as to via output equipment 46
Output.
Client device 12 can prepare received video and data just to be presented (134).For example, client is set
Standby 12 can actively load relevant one or more of the time range with video of the current output to show in received data
A part (for example, file).In this example, client device 12 (installment) can load received file in batches
Part, each in batches be cover video particular time range separate unit.By loading received file in batches,
Client device 12 may not necessarily load entire file, and This further reduces file data required storage devices 48 originally
In memory portion consumption.
Interpolation module 15 can identify the label (136) of received video.In this example, interpolation module 15 can be regarding
All " activity " labels of specific time or time range identification of frequency.More specifically, active tags can in the specific of video
The indicated object that time or time range represent is associated.In this example, interpolation module 15 can be based on such as pause request
Etc user input identification specific time.In addition, interpolation module 15 can be based on the finger in the object represented by specific time
Show and determine that one or more labels about the specific time (for example, identified static frames are asked in pause) are movable.
In some examples, interpolation module 15 is by determining which interpolation functions is defined as effectively specific time to identify movable mark
Label.
In addition, interpolation module 15 can be for each identified object generation label shape (138).For example, interpolation mould
Block 15 can such as by will in object type database associated with label shape search object type will be certain pre-
The shape that shapes is associated with object type, such as circular tag is associated with face object.As another example, interpolation module
15 can be that some object type generation of such as building such as customizes the specific shape of polygon.In again other example
In, interpolation module 15 can such as determine binary presence/missing standard about audio object.In some instances, it is interior
Module 15 is inserted based on interpolating function associated with object to generate label shape.
Each received interpolating function can be applied to each corresponding video segmentation and institute by interpolation module 15
The object (140) of identification.By applying the interpolating function, interpolation module 15 can determine each object in each frame in sequence
Position and size.In addition, UI EM equipment modules 52 can be in each corresponding frame (or segmentation) of the video received with every
Each label that a identified object output interpolation module 15 is generated is to be shown (142).
Fig. 7 is to illustrate server apparatus to implement the interpolation video of one or more server sides of the disclosure by it
The flow chart of another instantiation procedure 160 of label technique.Although process 160 can be performed by various equipment, merely for
The purpose of explanation is described herein about the server apparatus of Fig. 1-2 2 and its various assemblies.Process 160 can be in video
Segmentation module 8 starts (162) when identifying the frame sequence in video.For example, video segmentation module 8 can identify that the multiple of video regard
Included sequence of frames of video in frequency frame.In this example, all or part of video frame in the sequence can include Object identifying
The expression for the object that module 4 is identified.
In some instances, video segmentation module 8 can determine that there are first and final times of object in frame sequence
(164).As indicated by dashed boundaries, (164) can be formed can selection operation.In other examples, video segmentation module 8 can be with
Point frame or the presence of object is determined in use using other technologies as described herein.As an example, video segmentation module 8
It can determine the associated timestamp of first and final time of the object with being identified in frame sequence there are Object Identification Module 4
And/or frame count.
Additionally or alternatively, interpolating function module 6 can determine interpolating function for the object in frame sequence
(166).For example, interpolating function module 6 can determine that there are phases with the object that is confirmed as in the sequence of frames of video there are object
The effective time domain of the interpolating function in associated interpolating function, wherein the interpolating function designated.In some instances, interpolation
Function module 6 can not as independently of the operation of determining interpolating function come determine in frame sequence there are object first and it is last
Time, but alternatively can determine time domain as a part for the process for determining interpolating function.It is in addition, one or more logical
Letter unit 72 can transmit video, the instruction of object and the instruction of interpolating function (168).
Fig. 8 is to illustrate client device to implement the interpolation video of one or more client-sides of the disclosure by it
The flow chart of the instantiation procedure 180 of label technique.For exemplary purposes, the client about Fig. 1 and 3 herein of process 180
Equipment 12 and its component are described.Client device 12 can use communication unit 44 to receive video, one or more objects
Instruction, one or more interpolating functions associated with each object and knowledge data associated with each object
(182).The video received can include multiple video frame, and interpolating function can specify the interpolating function in the video to have
The time domain of effect.
Interpolation module 15 can determine position (184) based on the interpolating function for the label in the video frame of video.The mark
Label can the knowledge data of object corresponding with being associated be associated.In addition, UI EM equipment modules 52 can be exported according to interpolation mould
The label that block 15 is positioned about position determined by particular frame is to be shown (186).
Technology as described herein at least partly can combine to implement with hardware, software, firmware or the arbitrary of them.
For example, described various aspects can be implemented in one or more processors, including one or more microprocessors, number
Word signal processor (DSP), application-specific integrated circuit (ASIC), field programmable gate array (FPGA) or other equivalent collection
Into or discrete logic and such component arbitrary combination.Term " processor " or " processing circuit " generally can be single
It solely either combines other logic circuits and refers to arbitrary above-mentioned logic circuit either any other logic circuit or any other
Equivalent circuit.Control unit including hardware can also perform one or more technologies of the disclosure.
Such hardware, software and firmware can be implemented to support institute in the disclosure in identical equipment or in separation equipment
The various technologies of description.In addition, any described unit, module or component can collectively or individually be implemented as it is discrete but
It being capable of interactive operation.By different characteristic be described as module or unit be intended to it is not necessarily dark in terms of emphasizing different functions
Show that such module or unit must be realized by the hardware, firmware or component software that detach.On the contrary, with one or more modules
Or the associated function of unit can perform or be integrated in shared or list by individual hardware, firmware or component software
Within only hardware, firmware or component software.
The various technologies of the disclosure can be further by (or its arbitrary group of one or more examples discussed below
Close) it is described.
Example 1.
1. a kind of method, including：The sequence of video frame included by computing device identification in multiple video frame of video,
Wherein object is present at least one of the sequence of video frame video frame；By the computing device determine with it is described right
As in the sequence of the video frame there are associated interpolating function, wherein the interpolating function specifies the interpolating function
The time domain of the effective video wherein；And by the computing device transmit the video, the object instruction and
The instruction of the interpolating function.
Example 2.
2. according to the method described in example 1, wherein, the interpolating function is by the size of label associated with the object
It is expressed as the function of the time in the video.
Example 3.
3. according to the method for example 1, any one of 2, wherein, the interpolating function will be in video frame
Function of the location presentation of display label associated with the object for the time in the video.
Example 4.
4. according to any one of example 1-3 methods, wherein it is determined that the interpolating function includes：
Determine that the sequence by the video frame is divided into one or more segmentation sides being segmented of the video frame by the computing device
Case；And number associated with each segmentation of video frame according to determined by the segmentation scheme is determined as the computing device
Simple subfunction on.
Example 5.
5. according to the method described in example 1, wherein it is determined that the interpolating function includes：By the computing device at least portion
Point ground by determine the multiple video frame in follow in the sequence not include the object final video frame after
Video frame determines the interpolating function effective time domain wherein.
Example 6.
6. according to any one of example 1-5 methods, further comprise：Known by the computing device
It is each in the set not with the set for continuously matching associated error amount of the video frame in the sequence of the video frame
Error amount and it is associated from the existing deviation in sequence of the object in the video frame；Institute is determined by the computing device
State whether the error amount in set is more than threshold error value；It is more than the threshold in response to the error amount determined in the set
It is worth error amount, the continuous pairing for determining video frame associated with the error amount by the computing device represents sequence transitions；
The continuous more early video frame with centering is identified as to the final video frame of the sequence；And continuously match centering by described
Later video frame is identified as the first video frame of the second sequence of frames of video.
Example 7.
7. according to the method described in example 6, wherein it is determined that whether the error amount includes more than the threshold error value：
Determine with the object about the video frame continuously match there are associated value, wherein described value and object pass
At least one of size, location and shape in continuous each video frame with centering of the video frame are associated；It is based on
With the object about the video frame continuously match there are associated value determines the threshold error value so that it is super
The error amount for crossing the threshold error value is associated with the discontinuous movement of the object.
Example 8.
8. the method according to any one of example 1-7, wherein, the object include people, building,
The expression of at least one of geographical location and audio data.
Example 9.
9. according to the method described in example 8, wherein, identify that the object of expression for including people includes the use of face recognition
To identify people.
Example 10.
10. according to the method for example 8, any one of 9, wherein, the audio data includes song
With narration at least one of.
Example 11.
11. according to any one of example 1-10 methods, further comprise：By the computing device
Determine knowledge data associated with the object；And the knowledge data identified mapped to by the computing device described
Object.
Example 12.
12. a kind of method, including：Include what is represented in the video of multiple video frame, the video by computing device reception
The instruction of at least one object, for the video video frame at least one sequence interpolation associated with the object
The instruction of function and knowledge data associated with the object, wherein the interpolating function specifies the interpolating function to exist
The time domain of the wherein effective video；By the computing device and for the video video frame based on being received in
Slotting function is determined for showing the position of associated with object label in the video frame, wherein the label and pass
Being coupled to the knowledge data of the object is associated；And the video frame is exported and according to identified by the computing device
At least one of label of position positioning is with display.
Example 13.
13. according to the method described in example 12, wherein, the interpolating function is big by label associated with the object
The function of the small time being expressed as in the video.
Example 14.
14. the method according to example 12, any one of 13, wherein, the interpolating function is by video
Function of the location presentation of label associated with the object for the time in the video is shown in frame.
Example 15.
15. according to any one of example 12-14 methods, wherein, the interpolating function is wherein
Two or more discrete times that effective time domain includes the video are segmented.
Example 16.
16. according to any one of example 12-15 methods, wherein, by interpolating function meaning
Fixed time domain is associated with the sequence of video frame that the multiple video frame of the video includes.
Example 17.
17. according to the method described in example 16, further comprise receiving the instruction of the second object represented in the video,
For the finger of the second interpolating function associated with second object of at least the second sequence of the video frame in the video
Show and knowledge data associated with second object, wherein second interpolating function specifies the second interpolation letter
Count the second time domain of the effective video wherein.
Example 18.
18. according to any one of example 12-17 methods, wherein, identified video frame is packet
The first video frame in the sequence of the video frame is included, and the position determined for label is for first of the label
It puts, the method further includes：It is by the computing device and true for the second video frame in the sequence of the video frame
The second position of label associated with the object is used in the fixed video frame；And basis is exported by the computing device
The label of the second position positioning for the second video frame being associated in the sequence of the video frame is to show.
Example 19.
19. according to any one of example 12-18 methods, further comprise：It is set by the calculating
It is standby to receive the selection of label shown to being associated with identified video frame by exporting；And in response to receiving the choosing
It selects, at least part of knowledge data associated with the object is exported by the computing device.
Example 20.
20. according to the method described in example 19, wherein, at least part for exporting the knowledge data includes：By described
Computing device exports at least part of the knowledge data and is shown with being associated with identified video frame.
Example 21.
21. according to any one of example 12-20 methods, wherein, the label is exported to show
It is in response in receiving one or more inputs.
Example 22.
22. according to the method described in example 21, wherein, one or more of inputs include and identified video frame phase
At least one of associated pause request and Object Selection associated with identified video frame.
Example 23.
23. according to any one of example 12-22 methods, wherein, the object includes face, body
Body and the expression without at least one of life entity.
Example 24.
24. according to the method described in example 23, wherein, the no life entity includes building, geographical location and packet
Include at least one of audio data of at least one of song and offscreen voice.
Example 25.
25. a kind of computing device is performed including being used for according to example 1-24 or its arbitrary combination described any side
The device of method.
Example 26.
26. a kind of encode the computer readable storage medium for having instruction, described instruction causes computing device when executed
One or more processors perform the method according to example 1-24 or its arbitrary combination.
Example 27.
A kind of equipment, including：Memory；One or more programmable processors, are configured as：Identify the multiple of video
The sequence for the video frame that video frame includes, wherein object are present at least one of the sequence of video frame video frame
In；And determine with the object in the sequence of the video frame there are associated interpolating function, wherein the interpolation
Function specifies the time domain of the interpolating function effective video wherein；And interface, it is configured as regarding described in transmission
Frequently, the instruction of the object and the instruction of the interpolating function.
Example 28.
A kind of to encode the computer readable storage devices for having instruction, described instruction causes the one of computing device upon being performed
A or multiple programmable processors：The sequence of video frame that multiple video frame of identification video include, wherein object are present in
In at least one of the sequence of video frame video frame；It determines and presence of the object in the sequence of the video frame
Associated interpolating function, wherein the interpolating function specifies the time domain of the interpolating function effective video wherein；
And the transmission video, the instruction of the object and the instruction of the interpolating function.
Example 29.
A kind of equipment, including：Memory；Interface is configured as reception and includes the video of multiple video frame, the video
The instruction of at least one object of middle expression, for the video video frame at least one sequence it is related to the object
The instruction of the interpolating function of connection and knowledge data associated with the object, wherein the interpolating function is specified in described
The time domain of the slotting function effective video wherein；And one or more processors, it is configured as：For the video
Video frame determined based on the interpolating function received in the video frame for showing associated with object label
Position, wherein the label is associated with the knowledge data for being associated with the object；And export the video frame and root
At least one of label according to the positioning of identified position is with display.
Example 30.
A kind of to encode the computer readable storage devices for having instruction, which causes one of computing device when executed
Or multiple programmable processors：Receive the finger of at least one object represented in the video for including multiple video frame, the video
Show, for the video video frame at least one sequence interpolating function associated with the object instruction and
Knowledge data associated with the object, wherein the interpolating function specifies the interpolating function is effectively described wherein to regard
The time domain of frequency；The video frame of the video is determined based on the interpolating function received in the video frame for show with
The position of the associated label of object, wherein the label is associated with the knowledge data for being associated with the object；And
At least one of the video frame and the label positioned according to identified position are exported with display.
Technology described in the disclosure can also be to include the product that coding has the computer readable storage medium of instruction
To embody or encode.Instruction that is embedded or encoding in the product including the computer readable storage medium being encoded can cause
One or more programmable processors or other processors implement one or more technologies as described herein, such as including or
When instruction of the coding in the computer readable storage medium is executed by one or more processors.Computer readable storage medium
It can include random access memory (RAM), read-only memory (ROM), programmable read only memory (PROM), erasable compile
Journey read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), flash memory, hard disk, compact-disc ROM (CD-
ROM), floppy disk, cartridge, magnetic medium, optical medium or other computer-readable mediums.In some instances, product can be with
Including one or more computer readable storage mediums.
In some instances, computer readable storage medium can include tangible or non-transitory media, such as one or more
A computer readable storage devices.Term " non-transient " can indicate that the storage medium is not embodied with carrier wave or transmitting signal.
In some examples, can store being capable of time to time change for non-transitory storage media or non-transient computer readable storage devices
Data (such as in RAM or cache).
Various aspects of the disclosure is described.Exemplary aspect and feature as described herein can with it is another
Any other aspect or feature described in one example are combined.These and other example is in appended claims
Within the scope of.
Claims (26)
1. a kind of method for identifying object, including：
By the first computing device identify sequence of frames of video, wherein at least one of described sequence of frames of video video frame exist pair
As the sequence of frames of video is included in multiple video frame of video；
Error amount collection associated with the continuously pairing of the video frame of the sequence of frames of video is identified by first computing device
Close, each error amount in the error value sets and from the object in the sequence of frames of video existing for corresponding deviation
It is associated；
Determine whether the error amount in the error value sets is more than threshold error value by first computing device；
It is more than the threshold error value in response to the error amount determined in the error value sets, is calculated and set by described first
The standby continuous pairing for determining video frame associated with the error amount represents sequence transitions；
Based on the sequence transitions, point of object described in the presence for identify the sequence of frames of video as first computing device
Section；
By first computing device determine with the object in the segmentation of the sequence of frames of video there are associated
Interpolating function, wherein the interpolating function specifies the time domain of the interpolating function effective video wherein；And
The video, described is transmitted from first computing device to the second computing device for being detached with first computing device
The instruction of object and the instruction of the interpolating function.
2. according to the method described in claim 1, wherein, the interpolating function is by the size of label associated with the object
It is expressed as the function of the time in the video.
3. according to the method described in claim 1, wherein, the interpolating function will show associated with the object in video frame
Label function of the location presentation for the time in the video.
4. according to the method described in claim 1, wherein it is determined that the interpolating function includes：
It is determined by first computing device associated with the segmentation according to the sequence of frames of video mathematically simple
Subfunction.
5. according to the method described in claim 1, wherein it is determined that the interpolating function includes：
By first computing device at least partially through determine the multiple video frame in follow in the sequence of frames of video
In not the video frame after the final video frame including the object come when determining that the interpolating function is effective described wherein
Domain.
6. according to the method described in claim 1, further comprise：
The final video frame continuous more early video frame with centering being identified as in the sequence；And
The the first video frame continuous later video frame with centering being identified as in the second sequence of frames of video.
7. according to the method described in claim 6, wherein it is determined that whether the error amount includes more than the threshold error value：
Determine with the object about the video frame continuously match there are associated value, wherein described value with it is described right
At least one of size, location and shape as continuous each video frame with centering about the video frame are associated；
And
Based on the object about the video frame continuously match there are associated value determines the threshold error
Value so that associated with the discontinuous movement of the object more than the error amount of the threshold error value.
8. according to the method described in claim 1, wherein, the object is included in people, building, geographical location and audio data
At least one expression.
9. according to the method described in claim 8, wherein described object includes the expression of people, the method further includes using
Face recognition identifies the people,
Wherein identify that the sequence of frames of video includes：Based on the people is identified using face recognition, to identify the video frame
Sequence.
10. according to the method described in claim 8, wherein, the audio data includes at least one of song or narration.
11. according to the method described in claim 1, further comprise：
Knowledge data associated with the object is determined by first computing device；And
The knowledge data identified is mapped into the object by first computing device.
12. one kind is used to add object tagged method, including：
Include multiple video frame from the second computing device reception detached with first computing device by the first computing device
The instruction of at least one object that is represented in video, the video, for the video video frame at least one sequence
It is associated with the object and by the second computing device previously instruction of identified interpolating function and with it is described
The associated knowledge data of object, wherein the interpolating function specify the interpolating function wherein the effective video when
Domain；
In response to detecting that user inputs at first computing device, regard by first computing device and for described
The video frame of frequency is determined in the video frame based on the interpolating function received for display mark associated with the object
The position of label, wherein the label is associated with the knowledge data for being associated with the object；And
The video frame is exported by first computing device and the label that is positioned according to identified position at least one
It is a with display.
13. according to the method for claim 12, wherein, the interpolating function is big by label associated with the object
The function of the small time being expressed as in the video.
14. according to the method for claim 12, wherein, the interpolating function expresses display and the object phase in video frame
How the position of associated label changes as the function of the time in the video.
15. according to the method for claim 12, wherein, effective time domain includes the video to the interpolating function wherein
Two or more discrete times segmentation.
16. the method according to claim 11, wherein, as the time domain specified by the interpolating function and the institute of the video
Stating the sequence of frames of video that multiple video frame include is associated.
17. according to the method for claim 16, further comprise receiving the instruction of the second object represented in the video,
Instruction for the second interpolating function associated with second object of at least the second sequence of frames of video in the video,
And knowledge data associated with second object, wherein second interpolating function specifies second interpolating function to exist
Second time domain of the wherein effective video.
18. according to the method for claim 12, wherein, identified video frame is included in the sequence of frames of video
First video frame, and the position determined for label be for the first position of the label, the method further includes：
It determines to use in the video frame by first computing device and for the second video frame in the sequence of frames of video
In the second position of label associated with the object；And
Determined by first computing device output according to the second position of the second video frame being associated in the sequence of frames of video
The label of position is to show.
19. according to the method for claim 12, further comprise：
The selection of label shown to being associated with identified video frame by exporting is received by first computing device；And
In response to receiving the selection, knowledge data associated with the object is exported extremely by first computing device
A few part.
20. according to the method for claim 19, wherein, at least part for exporting the knowledge data includes：
At least part of the knowledge data is exported by first computing device and is carried out with being associated with identified video frame
Display.
21. it according to the method for claim 12, wherein, exports the label and is in response to showing in receiving one or more
A input.
22. according to the method for claim 21, wherein, one or more of inputs include and identified video frame phase
At least one of associated pause request and Object Selection associated with identified video frame.
23. according to the method for claim 12, wherein, the object include face, body and without in life entity extremely
Expression one few.
24. according to the method for claim 23, wherein, the no life entity includes building, geographical location and packet
Include at least one of audio data of at least one of song and offscreen voice.
25. a kind of computing device performs the device according to any method described in claim 1-24 including being used for.
26. a kind of encode the computer readable storage medium for having instruction, described instruction causes the one of computing device when executed
A or multiple processors perform the method according to claim 1-24.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/847,947 | 2013-03-20 | ||
US13/847,947 US9294712B2 (en) | 2013-03-20 | 2013-03-20 | Interpolated video tagging |
PCT/US2014/022694 WO2014150240A1 (en) | 2013-03-20 | 2014-03-10 | Interpolated video tagging |
Publications (2)
Publication Number | Publication Date |
---|---|
CN105052155A CN105052155A (en) | 2015-11-11 |
CN105052155B true CN105052155B (en) | 2018-06-22 |
Family
ID=50549407
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480016856.6A Expired - Fee Related CN105052155B (en) | 2013-03-20 | 2014-03-10 | Interpolation video label |
Country Status (4)
Country | Link |
---|---|
US (2) | US9294712B2 (en) |
EP (1) | EP2976886A1 (en) |
CN (1) | CN105052155B (en) |
WO (1) | WO2014150240A1 (en) |
Families Citing this family (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8793582B2 (en) | 2012-08-22 | 2014-07-29 | Mobitv, Inc. | Personalized timeline presentation |
US10009579B2 (en) | 2012-11-21 | 2018-06-26 | Pelco, Inc. | Method and system for counting people using depth sensor |
US9367733B2 (en) * | 2012-11-21 | 2016-06-14 | Pelco, Inc. | Method and apparatus for detecting people by a surveillance system |
US9639747B2 (en) | 2013-03-15 | 2017-05-02 | Pelco, Inc. | Online learning method for people detection and counting for retail stores |
US9294712B2 (en) | 2013-03-20 | 2016-03-22 | Google Inc. | Interpolated video tagging |
US9589595B2 (en) | 2013-12-20 | 2017-03-07 | Qualcomm Incorporated | Selection and tracking of objects for display partitioning and clustering of video frames |
US10346465B2 (en) | 2013-12-20 | 2019-07-09 | Qualcomm Incorporated | Systems, methods, and apparatus for digital composition and/or retrieval |
GB2528893B (en) * | 2014-08-01 | 2019-06-05 | Mirada Medical Ltd | Method and apparatus for delineating an object within a volumetric medical image |
CN105373938A (en) * | 2014-08-27 | 2016-03-02 | 阿里巴巴集团控股有限公司 | Method for identifying commodity in video image and displaying information, device and system |
US10299012B2 (en) * | 2014-10-28 | 2019-05-21 | Disney Enterprises, Inc. | Descriptive metadata extraction and linkage with editorial content |
US10264329B2 (en) * | 2014-10-28 | 2019-04-16 | Disney Enterprises, Inc. | Descriptive metadata extraction and linkage with editorial content |
EP3286620A4 (en) | 2015-04-20 | 2018-12-05 | Tiltsta Pty Ltd. | Interactive media system and method |
US10629166B2 (en) * | 2016-04-01 | 2020-04-21 | Intel Corporation | Video with selectable tag overlay auxiliary pictures |
WO2017210601A1 (en) * | 2016-06-02 | 2017-12-07 | Stc.Unm | System and methods for parallel processing motion estimation of digital videos |
CN107770614A (en) * | 2016-08-18 | 2018-03-06 | 中国电信股份有限公司 | The label producing method and device of content of multimedia |
CN106303726B (en) * | 2016-08-30 | 2021-04-16 | 北京奇艺世纪科技有限公司 | Video tag adding method and device |
CN108174270B (en) * | 2017-12-28 | 2020-12-01 | Oppo广东移动通信有限公司 | Data processing method, data processing device, storage medium and electronic equipment |
KR20200040097A (en) * | 2018-10-08 | 2020-04-17 | 삼성전자주식회사 | Electronic apparatus and method for controlling the electronicy apparatus |
WO2020107327A1 (en) * | 2018-11-29 | 2020-06-04 | Beijing Didi Infinity Technology And Development Co., Ltd. | Systems and methods for target identification in video |
US11080532B2 (en) * | 2019-01-16 | 2021-08-03 | Mediatek Inc. | Highlight processing method using human pose based triggering scheme and associated system |
CN110493661B (en) * | 2019-06-26 | 2021-11-16 | 中电万维信息技术有限责任公司 | Video file processing method and server |
CN111314759B (en) * | 2020-03-02 | 2021-08-10 | 腾讯科技（深圳）有限公司 | Video processing method and device, electronic equipment and storage medium |
CN113079417B (en) * | 2021-03-25 | 2023-01-17 | 北京百度网讯科技有限公司 | Method, device and equipment for generating bullet screen and storage medium |
US11347565B1 (en) * | 2021-06-30 | 2022-05-31 | United Services Automobile Association (Usaa) | System and method for app-to-app content reconfiguration |
CN113852776A (en) * | 2021-09-08 | 2021-12-28 | 维沃移动通信有限公司 | Frame insertion method and electronic equipment |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN101506828A (en) * | 2006-06-09 | 2009-08-12 | 索尼爱立信移动通讯股份有限公司 | Media identification |
US8037496B1 (en) * | 2002-12-27 | 2011-10-11 | At&T Intellectual Property Ii, L.P. | System and method for automatically authoring interactive television content |
Family Cites Families (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6819783B2 (en) | 1996-09-04 | 2004-11-16 | Centerframe, Llc | Obtaining person-specific images in a public venue |
KR19980073964A (en) * | 1997-03-20 | 1998-11-05 | 김영환 | Contour Matched Motion Vector Estimation Method for Shape Information Coding of Image Signal |
US6724915B1 (en) * | 1998-03-13 | 2004-04-20 | Siemens Corporate Research, Inc. | Method for tracking a video object in a time-ordered sequence of image frames |
JP3413720B2 (en) * | 1998-06-26 | 2003-06-09 | ソニー株式会社 | Image encoding method and apparatus, and image decoding method and apparatus |
US6968004B1 (en) * | 1999-08-04 | 2005-11-22 | Kabushiki Kaisha Toshiba | Method of describing object region data, apparatus for generating object region data, video processing method, and video processing apparatus |
CA2279797C (en) * | 1999-08-06 | 2010-01-05 | Demin Wang | A method for temporal interpolation of an image sequence using object-based image analysis |
GB2395779A (en) | 2002-11-29 | 2004-06-02 | Sony Uk Ltd | Face detection |
US20100002070A1 (en) * | 2004-04-30 | 2010-01-07 | Grandeye Ltd. | Method and System of Simultaneously Displaying Multiple Views for Video Surveillance |
US8412021B2 (en) | 2007-05-18 | 2013-04-02 | Fall Front Wireless Ny, Llc | Video player user interface |
US7881505B2 (en) | 2006-09-29 | 2011-02-01 | Pittsburgh Pattern Recognition, Inc. | Video retrieval system for human face content |
US8204273B2 (en) | 2007-11-29 | 2012-06-19 | Cernium Corporation | Systems and methods for analysis of video content, event notification, and video content provision |
US8170280B2 (en) * | 2007-12-03 | 2012-05-01 | Digital Smiths, Inc. | Integrated systems and methods for video-based object modeling, recognition, and tracking |
US8699858B2 (en) * | 2008-08-29 | 2014-04-15 | Adobe Systems Incorporated | Combined visual and auditory processing |
US8527537B2 (en) * | 2009-02-19 | 2013-09-03 | Hulu, LLC | Method and apparatus for providing community-based metadata |
US20120167145A1 (en) | 2010-12-28 | 2012-06-28 | White Square Media, LLC | Method and apparatus for providing or utilizing interactive video with tagged objects |
US20120198496A1 (en) * | 2011-01-31 | 2012-08-02 | Modiotek Co., Ltd. | Video Related Tag Generating Apparatus, Video Related Tag Generating Method, Video Interacting Method, Video Interacting System and Video Interacting Apparatus |
US8983189B2 (en) * | 2011-02-08 | 2015-03-17 | Algotec Systems Ltd. | Method and systems for error correction for three-dimensional image segmentation |
US20120239690A1 (en) | 2011-03-16 | 2012-09-20 | Rovi Technologies Corporation | Utilizing time-localized metadata |
US8649558B2 (en) | 2011-05-31 | 2014-02-11 | Wisconsin Alumni Research Foundation | Video processing with region-based warping |
US9294712B2 (en) | 2013-03-20 | 2016-03-22 | Google Inc. | Interpolated video tagging |
-
2013
- 2013-03-20 US US13/847,947 patent/US9294712B2/en not_active Expired - Fee Related
-
2014
- 2014-03-10 EP EP14719404.7A patent/EP2976886A1/en not_active Withdrawn
- 2014-03-10 CN CN201480016856.6A patent/CN105052155B/en not_active Expired - Fee Related
- 2014-03-10 WO PCT/US2014/022694 patent/WO2014150240A1/en active Application Filing
-
2016
- 2016-03-21 US US15/076,136 patent/US9992554B2/en not_active Expired - Fee Related
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8037496B1 (en) * | 2002-12-27 | 2011-10-11 | At&T Intellectual Property Ii, L.P. | System and method for automatically authoring interactive television content |
CN101506828A (en) * | 2006-06-09 | 2009-08-12 | 索尼爱立信移动通讯股份有限公司 | Media identification |
Also Published As
Publication number | Publication date |
---|---|
US20140285717A1 (en) | 2014-09-25 |
EP2976886A1 (en) | 2016-01-27 |
WO2014150240A1 (en) | 2014-09-25 |
CN105052155A (en) | 2015-11-11 |
US20160205446A1 (en) | 2016-07-14 |
US9294712B2 (en) | 2016-03-22 |
US9992554B2 (en) | 2018-06-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN105052155B (en) | Interpolation video label | |
US10726836B2 (en) | Providing audio and video feedback with character based on voice command | |
CN105518783B (en) | Video segmentation based on content | |
US10324619B2 (en) | Touch-based gesture recognition and application navigation | |
US10388077B2 (en) | Three-dimensional environment authoring and generation | |
CN104350495B (en) | Object is managed in panorama is shown with navigation through electronic form | |
WO2020056903A1 (en) | Information generating method and device | |
CN103631768A (en) | Collaborative data editing and processing system | |
US20210051374A1 (en) | Video file playing method and apparatus, and storage medium | |
JP2020516107A (en) | Video content summarization | |
WO2022048329A1 (en) | Menu display method and apparatus | |
US20160005207A1 (en) | Method and system for generating motion sequence of animation, and computer-readable recording medium | |
US10699746B2 (en) | Control video playback speed based on user interaction | |
US20200186869A1 (en) | Method and apparatus for referencing, filtering, and combining content | |
CN110516749A (en) | Model training method, method for processing video frequency, device, medium and calculating equipment | |
US20220345797A1 (en) | System for summary segment association and dynamic selection for viewing with a content item of interest | |
CN115205925A (en) | Expression coefficient determining method and device, electronic equipment and storage medium | |
CN109298806A (en) | A kind of long-range quick interface exchange method and device based on Object identifying | |
US20180189988A1 (en) | Chart-type agnostic scene graph for defining a chart | |
US20230281221A1 (en) | Method for content synchronization and replacement | |
CN111135558B (en) | Game synchronization method, game client, computer storage medium and electronic device | |
CN112752132A (en) | Cartoon picture bullet screen display method and device, medium and electronic equipment | |
US20180190000A1 (en) | Morphing chart animations in a browser | |
US20160018982A1 (en) | Touch-Based Gesture Recognition and Application Navigation | |
US9607573B2 (en) | Avatar motion modification |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
GR01 | Patent grant | ||
GR01 | Patent grant | ||
CF01 | Termination of patent right due to non-payment of annual fee | ||
CF01 | Termination of patent right due to non-payment of annual fee |
Granted publication date: 20180622Termination date: 20200310 |