CN115769202A - Transient personalization mode for guest users of automated assistants - Google Patents
Transient personalization mode for guest users of automated assistants Download PDFInfo
- Publication number
- CN115769202A CN115769202A CN202080102329.2A CN202080102329A CN115769202A CN 115769202 A CN115769202 A CN 115769202A CN 202080102329 A CN202080102329 A CN 202080102329A CN 115769202 A CN115769202 A CN 115769202A
- Authority
- CN
- China
- Prior art keywords
- computing device
- user
- data
- helper
- assistant
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/30—Authentication, i.e. establishing the identity or authorisation of security principals
- G06F21/31—User authentication
- G06F21/32—User authentication using biometric data, e.g. fingerprints, iris scans or voiceprints
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/606—Protecting data by securing the transmission between two devices or processes
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/027—Concept to speech synthesisers; Generation of natural phrases from machine-based concepts
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L9/00—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols
- H04L9/32—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols including means for verifying the identity or authority of a user of the system or for message authentication, e.g. authorization, entity authentication, data integrity or data verification, non-repudiation, key authentication or verification of credentials
- H04L9/3226—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols including means for verifying the identity or authority of a user of the system or for message authentication, e.g. authorization, entity authentication, data integrity or data verification, non-repudiation, key authentication or verification of credentials using a predetermined code, e.g. password, passphrase or PIN
- H04L9/3231—Biological data, e.g. fingerprint, voice or retina
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/227—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of the speaker; Human-factor methodology
Abstract
Embodiments set forth herein relate to an automated assistant that is capable of operating in an instant personalization mode and/or assisting a separate automated assistant in providing output according to an instant personalization mode. The transient personalization mode can allow a guest user of an assistant-enabled device to receive a personalized response from the assistant-enabled device even if not logged into the assistant-enabled device. The host automation assistant of the assistant-enabled device is able to securely communicate with the automation assistant of the guest user through a backend process. In this way, incoming queries from the guest user to the host automation assistant can be personalized according to the guest automation assistant — without requiring the guest user to directly use their own personal device.
Description
Background
Humans may conduct a person's conversation with a computer using interactive software applications, which are referred to herein as "automated assistants" (also referred to as "digital agents," "chat robots," "interactive personal assistants," "intelligent personal assistants," and "conversation agents," etc.). For example, a human being (which may be referred to as a "user" when they interact with the automated assistant) may provide commands and/or requests using spoken natural language input (i.e., an utterance) and/or by providing textual (e.g., typed) natural language input, which may be converted to text and then processed in some cases.
In some cases, the automated assistant can be available to the user via each of a plurality of different automated assistant devices (i.e., computing devices that each provide access to the automated assistant), each in the user's login mode. In the login mode, the computing device is able to utilize credentials of the user to enable an automated assistant accessible via the computing device to at least selectively access (e.g., in response to speaker verification and/or facial verification of the user) various data specific to the user. Further, the automation assistant is able to utilize such data in processing user requests submitted to the automation assistant via the computing device. Such data can be utilized, for example, in performing speech recognition of an utterance from a user (e.g., utilized in selecting a speech recognition language, in favoring term(s), etc.), in determining underlying content responsive to the utterance (e.g., determining content from such data, or identifying content using such data), and/or in determining at which speech synthesized speech (e.g., speech readily understood by the user) the response is audibly presented. Thus, utilizing an automated assistant in login mode provides various technical benefits, such as ensuring accurate speech processing of a user's request, generating a response related to the request, and/or presenting the response in a manner that is readily understood by the user.
However, for a given computing device, multiple user device interactions may often be required to at least selectively be in a user's login mode. These interactions can include multiple touch inputs to the automated helper application to add the user as an authorized user of the computing device. Further, for computing devices where the user is not an administrator, the user may need to interact with the administrator to have the administrator add the user as an authorized user. Furthermore, for a given computing device that is only utilized instantaneously by a user, data security issues can arise when the user is operating in a login mode.
In view of these and other considerations, there are a number of benefits to operating in login mode for the user's personal computing device(s) and/or the computing device(s) with which the user permanently interacts (e.g., the computing device in the user's home). However, for computing devices with which the user interacts only momentarily (e.g., only a limited number of interactions and/or for a limited duration), the user may not be in login mode (e.g., a given user may lack authorization to be added as a logged-in user). Additionally or alternatively, adding the user as multiple inputs required to log in to the user may not be guaranteed for instantaneous interaction, and-furthermore, providing multiple inputs would require a delay for instantaneous interaction. As one example, when a user utilizes a computing device at a friend's home or at a business (e.g., a hotel), the user may only be able to operate the automated assistant of the computing device in guest mode. The functionality of the automated helper can be limited in guest mode and/or various benefits of login mode may not be available in guest mode.
Disclosure of Invention
Embodiments set forth herein relate to various techniques for instantaneously adjusting the processing of automated helper request(s) based on data personal to the user, particularly when the request(s) are provided by the user at an automated helper device for which the user is not a login/authentication user. Such transient adaptation is sometimes referred to herein as operating according to a transient personalization mode. Operating in the transient personalization mode allows a guest user request received at the host automated assistant device to be processed using the user's personal data, even though the user is not authenticated on the host automated assistant device. This can include, for example, using the data when performing speech recognition where the request is an utterance, using the data when determining the underlying content of a response to the request, and/or using the data when determining in which speech synthesized speech the response is audibly presented. Although in some cases, the guest user has no prior interaction with the host automated helper device, some embodiments enable transient personalization.
As used herein, a "host automation helper" will be used to refer to an instance of an automation helper that a guest user utilizing the host automation helper device is not a logged-on user of the automation helper and that is accessible to the host automation helper device. As used herein, a "guest automation assistant" will be used to refer to an instance of an automation assistant that is accessible to the guest automation assistant device of which the guest user is a logged-in user. In other words, the guest user is not an authenticated user of the host device, and thus the host automated helper device cannot be used to directly access the user's personal automated helper data. On the other hand, the guest user is an authenticated user of the guest automation assistant apparatus and thus the guest automation assistant apparatus is able to provide direct access to automation assistant data personal to the guest user and/or stored in association with the account of the guest user.
In some embodiments, for the host automation assistant to operate in an instant personalization mode for the guest user, the host automation assistant may determine that the guest user is associated with the guest automation assistant. For example, various users can have assistant accounts associated with their own respective automation assistants (i.e., guest users can have their own personal automation assistants). However, when a particular user is considered a guest user with respect to a host automation assistant (e.g., an automation assistant accessible via a host device), such host automation assistant is able to determine that the user has an account established with the guest automation assistant (e.g., an automation assistant accessible via the user's personal computing device).
In some embodiments, the host automated assistant can ensure that there is a correlation between the guest user and a particular input before operating in the transient personalization mode. For example, a guest user's relevance determination can be initiated in response to the host automated helper apparatus receiving input from the guest user who may be out-of-work. The input can be an utterance, such as "assistant, what is scheduled on my calendar? ", which can be provided by a guest user to a host automated helper device, for example, in a hotel room. In response to receiving the utterance, the host automated assistant can initially determine whether the origin of the utterance corresponds to an existing authenticated user (e.g., a hotel owner). For example, the host automated assistant device or another network device can determine whether a biometric signature (e.g., voice, face, fingerprint, pupil, etc.) of the person providing the utterance matches a biometric signature of any existing authenticated user(s) (e.g., employees of a hotel). Based on the host automation assistant determining that the utterance was provided by an unauthenticated user (e.g., not matching any logged-in users of the device), the host automation assistant can identify a nearby device associated with the user providing the utterance or other input to the host automation assistant.
For example, in some implementations, the host automated assistant is able to confirm that the utterance corresponds to a user within a vicinity of the host automated assistant device. The host automation assistant can generate: voice embedding and/or voice vectors based on voice signatures embodied in utterances, face embedding and/or face vectors based on one or more images, fingerprint embedding and/or fingerprint vectors based on user finger scans, and/or any other information that can be used for biometric authentication if previously approved by a user. Voice embedding can be used to encrypt an authentication value (e.g., a secret string or other data), and the encrypted value can be shared with one or more nearby devices. For example, one or more devices, including the guest device, can receive the cryptographic authentication value via a bluetooth, ultrasonic, local Area Network (LAN), wide Area Network (WAN), internet, intranet, and/or Wi-Fi connection. In some embodiments, devices eligible to receive the cryptographic authentication value can be limited to certain devices within a threshold distance from the host device. In response, the guest device can attempt to decrypt the encrypted authentication value using the same or similar voice embedding accessible to the guest device. Because the host device and the guest device each have received an utterance from the guest user, their respective embeddings can have a similar arrangement in the potential space. Thus, a guest device having voice embedding corresponding to a guest user providing the utterance will be able to decrypt the encrypted authentication value. In this way, the host device is able to ensure that the utterance corresponds to the nearby user and the nearby device, thereby preserving the transient personalization mode for those users that are truly near the host device.
In some embodiments, when the guest device decrypts the encrypted authentication value, the guest device can transmit the authentication value back to the host device to indicate to the host device that the guest user has authenticated on the guest device. In response to receiving the correct authentication value, the host device can transmit the utterance to the guest device. For example, the host device can generate encrypted query data embodying the utterance, and can transmit the encrypted query data to the guest device. The transmitted query data can include audio data, text data (e.g., text from speech to text processing performed at the host device), and/or natural language processing data (e.g., an identifier of an action intent and/or parameters of the action intent). The guest device can then generate response data based on the encrypted query data and share the response data with the host device. Alternatively or additionally, the host device can provide the encrypted challenge data with an encrypted authentication value so that only guest devices with correct voice embedding can decrypt the helper challenge and authentication value. The response data and the authentication value can then be provided back to the host device, which can render an output based on the response data.
According to the foregoing example, the guest device is able to decrypt the encrypted query data to determine that the guest user is requesting the host automation assistant to tell the guest user what schedule is on the guest user's calendar. Based on this determination, the guest device (e.g., the guest user's handset) can cause the guest automation assistant or a separate application to access the guest user's calendar application in order to generate response data for the host automation assistant to present. When the guest device and/or associated device generates response data that can correspond to a description of a predetermined event (e.g., "6 pm today, you have 'and dad' a schedule to have dinner altogether"), the guest device can transmit the response data to the host device. Alternatively or additionally, the guest apparatus can communicate one or more user preferences of the guest user, such as a preferred voice profile of the automated assistant. The host device can optionally receive the response data as encrypted response data. The host device can then process the response data to present corresponding outputs at one or more interfaces of the host device. For example, as a result of this process, the host device can provide an audible response to the guest user, such as "according to your calendar, at 6 pm today, you have' and dad have a dinner together. In this way, the guest user does not have to rely exclusively on their personal device to receive a personalized response from the automated assistant. This can allow guest users to conserve computing resources, such as battery life and network usage, of their personal devices when away from home.
The host automation assistant can determine that the utterance is appropriate for the personalized response based on, for example, determining that the utterance includes content that is only accessible by a person having access to a calendar application managed by the guest user. Alternatively or additionally, the automated assistant can determine that the voice is appropriate for a personalized response based on determining that the subject matter (e.g., calendar) of the utterance is related to user-customizable information, and/or that the utterance includes an owner pronoun (e.g., "my"). Alternatively or additionally, one or more trained machine learning models can be used to determine whether an utterance includes a query suitable for a personalized response. Alternatively or additionally, the host automation assistant can omit determining whether the utterance is appropriate for a personalized response, but rather determine whether the guest user is associated with the guest automation assistant. As used herein, a guest automation assistant can be (i) another automation assistant provided by the same entity that provides the host automation assistant, (ii) another automation assistant provided by a different entity, and/or (iii) associated with a particular automation assistant that is accessible via an Application Programming Interface (API) available to the host automation assistant.
The host automation assistant may initiate operation in the transient personalization mode when the host automation assistant determines that the utterance includes a query suitable for a personalized response, and/or when the host automation assistant determines that the user is associated with a separate automation assistant. However, to transition to the transient personalization mode, the host automated assistant may initially confirm whether the utterance is relevant to the nearby user and/or the nearby assistant-enabled device. In some implementations, the host automated assistant is able to provide a non-personalized response when the host device receives an utterance that includes a personal query but the host device is unable to authenticate any nearby devices. Alternatively or additionally, the host automation assistant can provide a response that explicitly states that the response from the host automation assistant is not personalized for the guest user providing the personal query and/or that the host automation assistant cannot identify the account and/or device associated with the guest user. This can be noticeable to some guest users, although they may realize that they can receive personalized results from the host automated assistant, the responses they currently receive are not personalized to them. In these cases, such notification can eliminate false communications with any host automation assistant that is capable of operating in the transient personalization mode.
In some embodiments, the user is able to provide the host automation assistant and the guest automation assistant with permission to coordinate the personalized response before the host automation assistant processes the query from the user. Alternatively or additionally, the user may be able to limit the privileges of the host automation assistant based on time, context, subject matter, and/or any other parameter suitable to limit the responsiveness of the automation assistant. For example, when a guest user initially provides a personal query to the host automation assistant, the host automation assistant can request the guest automation assistant to process the personal query. In response to receiving the request from the host automation assistant, the guest automation assistant can present a prompt to the guest user in order to obtain permission for the guest automation assistant to coordinate the personalized response with the host automation assistant. Alternatively or additionally, the guest automation assistant or another application can prompt the guest user as to whether the guest user wishes to restrict the host automation assistant's transient personalization mode. In response, the guest user can choose to limit the host automated assistant's instantaneous personalization mode to a particular time period (e.g., the next 24 hours), a particular place (e.g., when the guest user is near a threshold of the host automated assistant device), and/or a particular context (e.g., when the guest user's calendar indicates that the guest user is on business).
In some embodiments, the host automated assistant is further operable to provide personalized suggestions to the guest user when the guest user has given permission for the host automated assistant to provide a personalized response. For example, when a guest user is resident in a hotel room that includes the host automated assistant device, and the user has given permission to receive a personalized response, the host automated assistant can render particular content based on the user's personal preferences. For example, when a guest user provides an utterance, or whether or not the guest user provides an automated helper query, the guest device can share user preferences with the host automated helper when the guest device has authorized permission for such sharing. Using this user preference data, the host automated assistant can select and/or organize certain search results to present personalized content to the user. For example, the user preferences can characterize the user's language preferences, the user's food preferences, music preferences, event preferences, and/or any other preferences that can be characterized in the data. In this way, when a host automated assistant at a host device, such as in a hotel room, is presenting restaurant suggestions to a guest user, the host automated assistant will be able to filter the suggested content according to the user preferences identified by the guest automated assistant. Alternatively or additionally, when the host device is processing utterances from a guest user, the host device is able to perform the processing using an Automatic Speech Recognition (ASR) model employed by the guest automation assistant. Alternatively or additionally, when the host device presents audible output in response to an utterance from the guest user, the host device is capable of presenting the audible output in accordance with a preferred text-to-speech (TTS) profile selected by the guest automated assistant.
The above description is provided as an overview of some embodiments of the disclosure. Further description of these and other embodiments will be described in more detail below.
Other embodiments may include a non-transitory computer-readable storage medium storing instructions that are executable by one or more processors (e.g., central processing unit(s) (CPU), graphics processing unit(s) (GPU), and/or tensor processing unit(s) (TPU)) to perform a method, such as one or more of the methods described above and/or elsewhere herein. Other embodiments may include one or more computer systems comprising one or more processors operable to execute stored instructions to perform methods, such as one or more of the methods described above and/or elsewhere herein.
It should be understood that all combinations of the above concepts and further concepts described in greater detail herein are considered part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
Fig. 1A and 1B respectively illustrate views of a user interacting with a host automation assistant that is capable of invoking the guest automation assistant when operating in the guest user's transient personalization mode.
Fig. 2A and 2B illustrate views of a user interacting with a host automation assistant that is capable of employing guest user preferences when operating in a guest user's transient personalization mode.
FIG. 3 illustrates a system for providing an automated assistant that is capable of operating in and/or communicating with another automated assistant operating in an instant personalization mode.
FIG. 4 illustrates a method of processing a request from a host automation assistant when the host automation assistant attempts to operate in a transient personalization mode.
FIG. 5 illustrates a method for operating an automated assistant in an instant personalization mode when one or more guest users interact with the automated assistant.
FIG. 6 is a block diagram of an example computer system.
Detailed Description
Fig. 1A and 1B illustrate a view 100 and a view 120, respectively, of a user 102 interacting with a host automation assistant that is capable of invoking the guest automation assistant when operating in the guest user's transient personalization mode. For example, the user 102 can travel outside of their respective country and live in a particular hotel room 118. User 102 can arrive at hotel room 118 with their personal device 110, and personal device 110 can be a portable computing device, such as a cellular telephone. In addition, hotel room 118 can include one or more assistant-enabled devices, such as host device 108 and host television 106.
Initially, when user 102 arrives at hotel room 118, host device 108 and host television 106 are able to operate in accordance with an account corresponding to an entity separate from user 102, such as a business hotel. Thus, initially, host device 108 and host television 106 will not have access to different accounts corresponding to user 102, and thus initially may not be able to provide a personalized response for user 102. For example, a personal device 110 owned by the user 102 can provide access to a guest automation assistant that can provide a personalized response to the user 102 based on previous interactions with the user 102 and/or other data. However, although host device 108 and host television 106 may provide access to the host automation assistant, the host automation assistant may not be able to provide personalized information to user 102 without interacting with the guest automation assistant.
To interact with the guest automation assistant, the host automation assistant can operate in an instant personalization mode. This mode can allow the host automation assistant to provide a personalized response to a guest user associated with another automation assistant. For example, the user 102 can provide an utterance 104 to the host device 108, such as "assistant, which are my favorite restaurants here? "in response to receiving the utterance 104, a host automated assistant accessible via the host device 108 can optionally determine whether the utterance 104 includes one or more assistant queries that can have personalized responses. For example, the host automation assistant can determine whether the utterance embodies at least one assistant query that can be personalized using data that is not currently accessible to the host automation assistant. Alternatively or additionally, the host automated assistant can omit determining whether the utterance 104 embodies a query that may have a personalized response, but rather determine whether the source of the utterance 104 is associated with another automated assistant.
For example, in some implementations, the host device 108 can provide the host-relevance request 112 to the personal device 110 of the user 102 before or after receiving the utterance 104. Host correlation request 112 can be a request for personal device 110 or a guest automation assistant to provide information to the host automation assistant indicating that the guest automation assistant is related to user 102 providing utterance 104 and/or that the device that enabled the guest automation assistant is related to an operating vicinity of the guest automation system. In some embodiments, host device 108 or an associated device is capable of generating embedded data or other authentic data, and using this data to encrypt secret data that personal device 110 will have access to, but not any other device that does not have specific permissions from a guest user. The embedded data can be, for example, speech embedding or speech vectors based on at least some amount of audio captured when the user 102 provided the utterance 104. In this way, because the guest automation assistant has previously received an utterance from the user 102, the guest automation assistant will be able to decrypt the secret data using the same embedding or similar embedding. For example, when personal device 110 receives host dependency request 112, personal device 110 or another associated personal device can decrypt the host dependency request to identify the secret data. Personal device 110 can then generate a guest dependency response 114 that identifies or is otherwise based on the secret data. As illustrated in fig. 1A, an indication that the secret data has been successfully decrypted by the personal device 110 can be embodied in the guest-relevancy response 114 and provided back to the host device 108 via a network connection (e.g., wi-Fi, bluetooth, ultrasonic connection, zigBee, etc.).
When the host device 108 determines that the nearby personal device 110 is relevant to the user 102, the host device 108 can provide host query data 122 to the personal device 110. Alternatively or additionally, the host query data 122 can be provided to the personal device 110 along with the host dependency request 112. In some implementations, the host device 108 is capable of providing raw audio data of the utterance provided by the user 102. Alternatively or additionally, host device 108 can provide encrypted audio data that can be decrypted by personal device 110. Alternatively or additionally, host device 108 can provide Natural Language Understanding (NLU) data characterizing one or more actions requested by user 102. Alternatively or additionally, host device 108 can provide personal device 110 with a textual record of one or more portions of utterance 104.
In response to receiving the host query data 122, the personal device 110 and/or the guest automation assistant can generate guest query response data 124. The guest query response data 124 can characterize one or more automated helper outputs responsive to one or more queries embodied in the utterance 104 from the user 102. In some embodiments, the guest query response data 124 can be encrypted in a manner that allows the host device 108 to decrypt the automated helper output. In some embodiments, the guest query response data 124 can include natural language content characterizing the output 128 to be presented by the host automated assistant. For example, when host device 108 receives guest query response data 124 from personal device 110, host device 108 can present audible output 128 using guest query response data 124. For example, the host automation assistant of the host device 108 can render natural language content, such as "these are some personalized results provided for you on a television. "
Alternatively or additionally, the guest query response data 124 can characterize data that is responsive to the utterance 104 but not embodied in a natural language sentence format. For example, the guest query response data 124 can include a list 126, and the host device 108 can cause the list 126 to be presented at the host television 106. In this manner, the user 102 is able to seamlessly interact with the host device in order to receive the personalized response without requiring the user to specifically conduct an extended authentication process.
In some implementations, the personal device 110 can prompt the user 102 as to whether the user 102 wishes the host device 108 to no longer use the personal device 110 in the transient personalization mode. Alternatively or additionally, the personal device 110 and/or the host device 108 can prompt the user as to whether the user 102 wishes to limit the instant personalization mode to a certain time period, a certain location, and/or any other identifiable limit. In this manner, the user 102 is able to allow the host device 108 to operate strictly in the transient personalization mode during the user's vacation without having to constantly confirm approval for the host device 108 to operate in the transient personalization mode. This can conserve computing resources that may be consumed during interactions in which the user 102 repeats certain permissions with the host automation assistant.
Fig. 2A and 2B illustrate a view 200 and a view 220 of a user 202 interacting with a host automation assistant that is capable of adopting guest user preferences when operating in a guest user's transient personalization mode. In some embodiments, the interaction illustrated in fig. 2A and 2B can be a continuation of the interaction between the user 102 and the host device 108 illustrated in fig. 1A and 1B. Further, the functions described with respect to fig. 1A and 1B can be applied to the features illustrated in fig. 2A and 2B.
In some embodiments, the user 202 can travel outside and live in a guest room 218 that includes one or more host devices that provide access to the host automation assistant. For example, the one or more host devices can include a host device 208 and a host television 206. When the user 202 is outside their home, they may carry their personal device 210, the personal device 210 can be a cellular phone or other device that provides access to a guest automated assistant, or-that is, an automated assistant that has prior permission to access the user's 202 account.
In some embodiments, because the user 202 is traveling and the host device 208 may not be personalized for the user 202, the host device 208 may request user preference data from one or more devices and/or applications associated with the user 202. Such a request can be provided in response to the user 202 providing an utterance 204, such as "assistant i am now sleeping". In response to receiving the utterance 204, a host automation assistant accessible via the host device 208 can determine that the utterance 204 embodies a request for the automation assistant to perform one or more actions and/or routines. Alternatively or additionally, the host automated assistant can determine that the utterance 204 embodies one or more queries suitable for personalized response.
In response to receiving the utterance 204, the host device 208 and/or the host automation assistant can provide a host relevance request 212, the host relevance request 212 can be based on one or more of the implementations discussed with respect to the host relevance request 112. Further, the personal device 210 is capable of providing a guest relevance response 214 in accordance with one or more embodiments discussed with respect to the guest relevance response 114 of fig. 1A and 1B. Based on successfully receiving guest dependency response 214, host device 208 and/or the host automation assistant can provide host query data 222 to personal device 210. Host query data 222 can include requests for personal device 210 and/or guest automation assistants to provide data that can be used to generate a response to utterance 204.
For example, the requested data can include user preference data, ASR data, TTS data, one or more trained machine learning models, and/or any other information that can be used to generate a response to the utterance 204. For example, the personal device 210 and/or the guest automated assistant can provide guest helper data 224 to the host device 208. The guest helper data 224 can indicate one or more user preferences associated with one or more queries embodied in the utterance 204. For example, because the utterance 204 refers to one or more helper actions that will assist the user 202 (e.g., a routine of one or more helper actions that the guest automated helper performs at night in response to the user 202 saying "i am going to sleep"), the user preferences identified in the guest helper data 224 can include one or more preferred parameters for the user by the host automated helper when performing the one or more helper actions.
For example, one or more helper actions can include setting up a thermostat and playing certain music or other audio. Thus, in this case, the guest helper data 224 can identify the particular temperature setting of the thermostat and the particular radio station to play. In response to receiving the utterance 204 and based on the guest helper data 224, the host automated helper can provide an output 228, such as "good, i will play some natural sounds and set the temperature to 70 degrees. "additionally, based on the guest helper data 224, the host automation helper can cause the thermostat in the room 218 to change the temperature setting to 70 degrees and can also render additional audio from a natural sounding radio station. In this way, computing resources can be conserved when a user is able to bypass entering certain preferences directly to each helper device that the user wishes to temporarily personalize. Bypassing such operations can reduce the amount of audio processing or other input processing that is otherwise performed in order for the host automation assistant to capture all of the guest user's preferences.
FIG. 3 illustrates a system 300 for providing an automated assistant 304, automated assistant 304 being capable of operating in and/or assisting another automated assistant operating in an instant personalization mode. Automated assistant 304 can operate as part of an assistant application provided at one or more computing devices, such as computing device 302 and/or a server device. The user can interact with the automation assistant 304 via assistant interface(s) 320, the assistant interface 320 can be a microphone, a camera, a touch screen display, a user interface, and/or any other device capable of providing an interface between the user and an application. For example, the user can initialize the automation assistant 304 by providing verbal, textual, and/or graphical input to the assistant interface 320 to cause the automation assistant 304 to initialize one or more actions (e.g., provide data, control a peripheral device, access an agent, generate input and/or output, etc.). Optionally, automated assistant 304 can be initialized based on the processing of context data 336 using one or more trained machine learning models. Context data 336 can characterize one or more characteristics of the environment accessible to automation assistant 304 and/or one or more characteristics of a user predicted to be intending to interact with automation assistant 304.
The computing device 302 and/or other third party client devices can communicate with the server device over a network, such as the internet. Further, computing device 302 and any other computing devices can communicate with each other over a Local Area Network (LAN), such as a Wi-Fi network. The computing device 302 is able to offload computing tasks to a server device in order to conserve computing resources at the computing device 302. For example, the server device can host automation assistant 304, and/or computing device 302 can transmit input received at one or more assistant interfaces 320 to the server device. However, in some embodiments, automated assistant 304 can be hosted at computing device 302 and can perform various processes associated with enabling automated assistant operations at computing device 302.
In various implementations, all or less than all aspects of automated assistant 304 can be implemented on computing device 302 (e.g., at a client computing device or a server computing device). Such implementations can be based on whether the response from automation assistant 304 corresponds to data that is not stored at the client computing device and/or whether the response corresponds to an operation that should be performed by a separate computing device. In some of these embodiments, aspects of automation assistant 304 are implemented via computing device 302 and can connect with server devices that can implement other aspects of automation assistant 304. The server device is able to optionally serve multiple users and their associated helper applications via multiple threads. In embodiments in which all or fewer than all aspects of automation assistant 304 are implemented via computing device 302, automation assistant 304 can be an application separate from the operating system of computing device 302 (e.g., installed "on top of") or can alternatively be implemented directly by the operating system of computing device 302 (e.g., an application considered to be an operating system but integrated with the operating system).
In some embodiments, automation assistant 304 can include input processing engine 306, and input processing engine 306 can employ a number of different modules to process inputs and/or outputs of computing device 302 and/or the server device. For example, the input processing engine 306 can include a speech processing engine 308, and the speech processing engine 308 can process audio data received at the helper interface 320 to identify text embodied in the audio data. The audio data can be transmitted from, for example, the computing device 302 to a server device in order to conserve computing resources at the computing device 302. Additionally or alternatively, the audio data can be specifically processed at the computing device 302.
The process of converting audio data to text can include a speech recognition algorithm that can employ a neural network and/or a statistical model to identify sets of audio data corresponding to words or phrases. The text converted from the audio data can be parsed by data parsing engine 310 and made available to automation assistant 304 as text data that can be used to generate and/or identify command phrase(s), intent(s), action(s), slot value(s), and/or any other content specified by the user. In some embodiments, output data provided by data parsing engine 310 can be provided to parameter engine 312 to determine whether a user provides input corresponding to a particular intent, action, and/or routine that can be performed by automation assistant 304 and/or an application or agent that can be accessed via automation assistant 304. For example, helper data 338 can be stored at server device and/or computing device 302 and can include data defining one or more actions that can be performed by automated helper 304, as well as parameters required to perform the actions. The parameter engine 312 can generate one or more parameters for the intent, action, and/or time slot values and provide the one or more parameters to the output generation engine 314. The output generation engine 314 can communicate with the helper interface 320 to provide output to a user and/or communicate with one or more applications 334 to provide output to one or more applications 334 using the one or more parameters.
In some embodiments, automation assistant 304 can be an application that can be installed "on top of" the operating system of computing device 302 and/or can itself form a portion (or all) of the operating system of computing device 302. The automated helper app includes and/or has access to on-device speech recognition, on-device natural language understanding, and on-device fulfillment. For example, on-device speech recognition can be performed using an on-device speech recognition module that processes audio data (detected by the microphone (s)) using an end-to-end speech recognition machine learning model stored locally at computing device 302. On-device speech recognition generates recognized text of utterances (if any) present in the audio data. Further, for example, on-device Natural Language Understanding (NLU) can be performed using an on-device NLU module that processes recognition text generated using on-device speech recognition and optionally context data to generate NLU data.
The NLU data can include parameter(s) (e.g., slot values) corresponding to the intent(s) and optionally the intent(s) of the utterance. The on-device fulfillment can be performed using an on-device fulfillment module that utilizes NLU data (from the on-device NLU) and optionally other local data to determine the action(s) to take to resolve the intent(s) (and optionally the parameter(s) of the intent) of the utterance. This can include determining local and/or remote responses (e.g., answers) to the utterance, interaction(s) with locally installed application(s) performed based on the utterance, commands transmitted to internet of things (IoT) device(s) based on the utterance (either directly or via corresponding remote system (s)), and/or other analytic action(s) performed based on the utterance. Fulfillment on the device can then initiate local and/or remote execution/performance of the determined action(s) to resolve the utterance.
In various embodiments, at least remote speech processing, remote NLU, and/or remote fulfillment can be selectively utilized. For example, the recognition text can be at least selectively transmitted to a remote automation helper component(s) for remote NLU and/or remote fulfillment. For example, the identification text can be transmitted for remote execution, optionally in parallel with on-device execution or in response to a fault implemented on the device and/or NLU. However, on-device speech processing, on-device NLUs, on-device fulfillment, and/or on-device fulfillment can be prioritized at least due to the latency reduction they provide in resolving utterances (since client-server round trip(s) are not required to resolve the utterances). Further, the on-device function can be the only function available without or with limited network connectivity.
In some embodiments, computing device 302 can include one or more applications 334, applications 334 can be provided by a third party entity that is different from the entity that provides computing device 302 and/or automation assistant 304. Automation assistant 304 and/or an application state engine of computing device 302 can access application data 330 to determine one or more actions that can be performed by one or more applications 334, as well as a state of each of one or more applications 334 and/or a state of a respective device associated with computing device 302. Device data 332 can be accessed by automation assistant 304 and/or a device state engine of computing device 302 to determine one or more actions that can be performed by computing device 302 and/or one or more devices associated with computing device 302. Further, the application data 330 and/or any other data (e.g., device data 332) can be accessed by the automation assistant 304 to generate context data 336, and the context data 336 can characterize the context in which a particular application 334 and/or device is executing, and/or the context in which a particular user is accessing the computing device 302, accessing the application 334, and/or any other device or module.
When one or more applications 334 are executing at computing device 302, device data 332 can characterize the current operating state of each application 334 executing at computing device 302. Further, application data 330 can characterize one or more features of executing application 334, such as the content of one or more graphical user interfaces presented under direction of one or more applications 334. Alternatively or additionally, application data 330 can characterize a pattern of action that can be updated by the respective application and/or by automation assistant 304 based on the current operating state of the respective application. Alternatively or additionally, one or more action modes of one or more applications 334 can remain static, but can be accessed by an application state engine to determine the appropriate action to be initiated via automation assistant 304.
When training one or more trained machine learning models from these instances of training data, helper invocation engine 322 can cause automated helper 304 to detect or limit detection of spoken invocation phrases from the user based on characteristics of the context and/or environment or non-verbal activity of the user. Additionally or alternatively, helper invocation engine 322 can cause automated helper 304 to detect or limit detection of one or more helper commands from the user based on the context and/or characteristics of the environment. In some implementations, the helper invocation engine 322 can be disabled or restricted based on the computing device 302 detecting a helper suppression output from another computing device. In this way, when computing device 302 detects that the helper suppresses output, automation helper 304 will not be invoked based on context data 336 — otherwise if no helper-inhibited output is detected, context data 236 may cause automation helper 304 to be invoked.
In some embodiments, system 300 can include a guest dependency engine 316. Guest relevance engine 316 can be used to employ one or more operations to determine whether the user providing input to automation assistant 304 is a guest user or a host user. Alternatively or additionally, when the guest user provides input to the automation assistant 304 indirectly or directly, the guest relevance engine 316 can determine whether the guest user is within a threshold proximity of the computing apparatus 302 or an associated computing apparatus. For example, guest relevance engine 316 can determine that a voice signature or facial embedding associated with a user who has provided input does not correspond to a user logged into automation assistant 304, or otherwise has particular access permission(s) to automation assistant 304. The guest relevance engine 316 can then include that the user is a guest user. When guest correlation engine 316 determines that a guest user is using automation helper 304, either directly or indirectly, guest correlation engine 304 can invoke guest signature engine 318 in order to identify another helper device that is relevant to the guest user interacting with automation helper 304.
Guest signature engine 318 can use a true signature and/or embedding associated with the guest user in order to identify one or more other devices that may be relevant to the guest user. For example, guest signature engine 318 can encrypt communications that can be sent to one or more other devices using voice embedding. It can be considered that an apparatus that is capable of decrypting the communication and indicating to the automation assistant 304 that the apparatus successfully decrypted the communication is relevant to the guest user. For example, the guest apparatus can decrypt the communication using the same or similar voice embedding generated from one or more previous interactions between the guest apparatus and the guest user. Alternatively or in addition, the guest signature engine 318 can identify secrets that only certain devices have access to (e.g., such as a personal identification code presented at the user interface for pairing purposes), and the secrets can be used to relate a particular guest device to a guest user. When the automation assistant 304 determines that the guest apparatus is relevant to the guest user providing the input, the automation assistant 304 can further communicate with the guest apparatus in order for the guest automation assistant associated with the guest user to assist in processing the input received from the guest user. The guest device can then provide response data in response to the request from host automation assistant 304.
In some embodiments, automation assistant 304 can include a pattern preference engine 324 that can determine one or more preferences of a guest user or an acquaintance of the guest user interacting with the host automation assistant. For example, automation assistant 304 may be able to receive a request or provide a request to identify one or more preferences that a user may have when interacting with their own respective automation assistant. Such preferences can include preferences that are explicitly identified by the user or preferences that are appropriate for the user over time. For example, the automated assistant can provide preference data that identifies one or more trained machine learning models that can be used in processing input from or output to the user. For example, the trained machine learning models can include ASR models, speech-to-text models, text-to-speech models, and/or any other type of trained computer learning model that can be used during one or more operations of an automated assistant. This can allow the host automation assistant to provide responses that can be more easily interpreted by the guest user, as these responses can be pronounced, for example, in some manner that the host automation assistant typically does not pronounce to the host user.
In some embodiments, automated assistant 304 can include personal query engine 326, where personal query engine 326 can determine whether input from a user is associated with information that can be personalized for a particular user. For example, personal query engine 326 may use one or more trained machine learning models to determine whether input to automation assistant 304 and/or other interactions with automation assistant 304 are associated with information that can be personalized for a particular user. In some embodiments, personal query engine 326 can be optional, and can optionally cause automated assistant 304 to transition to the transient personalization mode when the guest user provides input determined to be associated with personalization information. Alternatively or additionally, when personal query engine 326 determines that the input or interaction is not associated with personal information (e.g., the input is a request that can be satisfied using public data that is not associated with a particular user account), personal query engine 326 can omit transitioning automated assistant 304 to the transient personalization mode.
FIG. 4 illustrates a method 400 of processing a request from a host automation assistant when the host automation assistant attempts to operate in a transient personalization mode. The method 400 can be performed by one or more applications, devices, and/or any other devices or modules capable of performing operations associated with an automation assistant. The method 400 can include an operation 402 of determining whether a dependency request has been received from a host automation assistant. This determination can be made at a guest apparatus that provides access to a guest automation assistant that can be associated with a user in the vicinity of another helper-enabled apparatus.
Upon receiving a dependency request from the host automation assistant, the method 400 can proceed from operation 402 to operation 404, which can include determining whether the guest user can be correlated with the input of the host automation assistant. In some embodiments, the guest device can receive encrypted data from the host device and can encrypt the encrypted data using a value generated based on a unique input from a user. For example, the value can be a speech vector or speech embedding that is based on the user's voice feature(s) as the user provides spoken input to the host automation assistant. In this way, the guest automation assistant will be able to decrypt the encrypted data transmitted from the host automation assistant because the guest automation assistant has previously received an utterance from the guest user.
The method 400 can proceed to operation 406 when the host automation assistant determines that the guest device or guest automation assistant is associated with a user providing input to the host automation assistant. Otherwise, the method 400 can return to operation 402. Operation 406 can be an optional operation that includes transmitting the authentication value to the host automation assistant. The authentication value can be, for example, a secret generated by a host automation assistant, desiring that only the guest device to which the user is logged in can decrypt the encrypted data and identify the authentication value. Alternatively or additionally, query data characterizing one or more requests embodied in input from a user can be received by the guest automation assistant and acted upon without communicating authentication values back to the host device.
From operation 404 or operation 406, the method 400 may proceed to operation 408, and operation 408 may include processing the request to identify one or more helper queries from the user. One or more helper queries can be embodied in an utterance from the user to the host automated helper. However, the host automation assistant can communicate a request to the guest automation assistant characterizing one or more assistant queries. In response to receiving the request, the guest automated assistant or guest apparatus can generate response data based on one or more assistant queries. For example, the guest automation assistant can process the queries as if the user provided the queries directly to the guest automation assistant. As a result, the guest automation assistant can generate response data that can characterize the output and/or other data that the host automation assistant processes in order to fulfill the input from the user to the host automation assistant.
From operation 410, the method 400 can proceed to operation 412, operation 412 can include causing the host automation assistant to present an output based on the response data. For example, the response data can characterize natural language content that can be presented at one or more interfaces of the host device. The natural language content can be responsive to an utterance provided by the user to the host automated assistant. In this way, when a user is outside their home, the user can quickly personalize nearby automated assistants, which can operate in an instant personalization mode.
FIG. 5 illustrates a method 500 for operating an automated assistant in an instant personalization mode when one or more guest users interact with the automated assistant. Method 500 can be performed by one or more applications, devices, and/or any other device or module capable of providing access to an automation assistant. The method 500 can include an operation 502 of determining whether an input from a guest user is received at the host automation assistant. The guest user can be a person who is not logged into the host automated assistant and/or who currently has access to the owner's account of the host automated assistant device that provides access to the host automated assistant. Upon determining that input has been received from the guest user, the method 500 can proceed from operation 502 to operation 504. Otherwise, the host automation assistant can proceed to determine if the guest user provided input.
Operation 504 can include providing a dependency request to a guest device operating within a vicinity of the host device. The dependency request can be a request for a nearby device to indicate that the device is associated with a guest user providing input to the host automation assistant. From operation 504, the method 500 can proceed to operation 506, operation 506 can include determining whether the guest apparatus can be correlated to input from the guest user. In some embodiments, the guest device can be associated with the input when the guest device is able to decrypt an authentication value that has been encrypted with the input information from the guest user. For example, the authentication value can be encrypted using face embedding, voice embedding, image embedding, video embedding, and/or any other signature of the guest user. Thus, when the guest device is able to decrypt the authentication value using similar embedding and transmit the authentication value back to the host device, the method 500 can proceed to operation 510. Otherwise, the method 500 can proceed to operation 508, operation 508 can include responding to the guest user without relying on a guest automation assistant.
From operation 510, the method 500 can proceed to operation 512, operation 512 can include processing the response data based on the one or more helper queries. For example, in some embodiments, the response data can embody audio data, text data, natural Language Processing (NLP) data, such as action intent and/or parameters, and/or any other data that can be used as a basis for generating automated helper responses. From operation 512, the method 500 can proceed to operation 514, operation 514 can include causing the host automation assistant to present an output based on the response data. For example, when the host automation assistant receives the NLP data, the host automation assistant can perform one or more actions identified by the NLP data using any parameters also identified in the NLP data.
FIG. 6 is a block diagram 600 of an example computer system 610. Computer system 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via a bus subsystem 612. These peripheral devices can include a storage subsystem 624, which storage subsystem 624 includes, for example, a memory 625 and a file storage subsystem 626, a user interface output device 620, a user interface input device 622, and a network interface subsystem 616. Input devices and output devices allow a user to interact with computer system 610. Network interface subsystem 616 provides an interface to external networks and couples to corresponding interface devices in other computer systems.
The user interface input devices 622 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into the display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the word "input device" is intended to include all possible types of devices and ways to input information to computer system 610 or over a communication network.
User interface output device 620 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual displays, such as via audio output devices. In general, use of the word "output device" is intended to include all possible types of devices and ways to output information from computer system 610 to a user or to another machine or computer system.
These software modules are typically executed by processor 614 alone or in combination with other processors. Memory 625 used in storage subsystem 624 can include a number of memories, including a main Random Access Memory (RAM) 630 for storing instructions and data during program execution and a Read Only Memory (ROM) 632 that stores fixed instructions. File storage subsystem 626 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 626 in storage subsystem 624, or in other machines accessible to processor(s) 614.
Where the systems described herein collect personal information about a user (or "participant" as often referred to herein) or may use personal information, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social behavior or activity, profession, user preferences, or the user's current geographic location), or whether and/or how to receive content from a content server that may be more relevant to the user. Further, before certain data is stored or used, the data may be processed in one or more ways to delete personally identifiable information. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the user's geographic location (such as a city, zip code, or state level) may be summarized where geographic location information is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information about the user is collected and/or usage information.
While several embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the results and/or one or more advantages described herein may be used, and each of these variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend on the particular application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
In some embodiments, a method implemented by one or more processors is set forth as including operations such as: the method includes receiving, at a first computing device, a request for the first computing device to process an utterance submitted by a user to a second computing device, wherein each of the first computing device and the second computing device are located in a common environment and provide access to a respective automated assistant, and wherein the second computing device encrypts the request using signature data generated by the second computing device using a biometric signature corresponding to the user. The operations can further include: the request from the second computing device is processed by the first computing device to identify one or more helper requests embodied in the request. The operations can further include: helper response data is generated by the first computing device that characterizes one or more automated helper responses in response to one or more helper requests. The operations can further include: causing, by the first computing device, the second computing device to present one or more automated helper responses to the user using the helper response data.
In some implementations, processing the request from the second computing device includes: the method may further include accessing, by the first computing device, other signature data associated with the user, and using the other signature data to identify an authentication value embodied in the request or other data from the second computing device. In some embodiments, causing the second computing device to present one or more automated helper responses comprises: the method further includes providing, from the first computing device, an authentication value to the second computing device, wherein the authentication value was generated by the second computing device in response to the second computing device receiving the utterance from the user. In some embodiments, generating helper response data comprises: when the second computing device receives the utterance from the user, stored content that is not stored at the second computing device is accessed by the first computing device.
In some embodiments, generating helper response data comprises: accessing content associated with an account of the user, wherein the second computing device is not authenticated to directly access the account of the user. In some embodiments, causing the second computing device to present the one or more automated helper responses comprises: transmitting helper response data from the first computing device to the second computing device via a local area network, a bluetooth connection, or a wide area network, wherein transmitting the helper response data causes the second computing device to present one or more automated helper responses. In some embodiments, the method can further comprise the operations of: at an interface of the first computing device and in response to receiving a request from the second computing device, a prompt is provided that allows a user to select whether to permit the first computing device to respond to the request or a subsequent request from the second computing device. In some embodiments, the method can further comprise the operations of: at an interface of the first computing device and in response to receiving a request from the second computing device, a prompt is provided that allows a user to limit when the first computing device is permitted to respond to the request or a subsequent request from the second computing device.
In other embodiments, a method implemented by one or more processors is set forth as including operations such as: the method includes receiving an utterance from a user associated with a first computing device, wherein the utterance is received at a second computing device in a common environment with the first computing device and the user, and wherein each of the first computing device and the second computing device provides access to a respective automated assistant. The operations can further include: providing, by the second computing device, to the first computing device, a first request for the first computing device to confirm that the user has authenticated on the first computing device, wherein the first request embodies an authentication value accessible by one or more devices authenticated by the user. The operations can further include: the method further includes receiving, by the second computing device, an authentication value indicating to the second computing device that the first computing device is able to access the authentication value. The operations can further include: providing, by the second computing device and based on the authentication value, a second request responsive to the one or more helper requests embodied in the dialog of the first computing device. The operations can further include: the method further includes receiving, by the second computing device and in response to providing the second request, helper response data responsive to one or more helper requests embodied in the utterance. The operations can further include: causing, by the second computing device, one or more interfaces of the second computing device to present automated helper output based on the helper response data.
In some embodiments, the operations can further comprise: identifying, by the second computing device, a genuine signature of the user; the first request is generated by the second computing device by encrypting the authentication value using the authentic signature. In some embodiments, the operations can further comprise: the helper response data is processed by the second computing device using the true signature, wherein the helper response data is encrypted by the first computing device using the true signature. In some embodiments, the authentic signature of the user corresponds to an audio-based signature or an image-based signature. In some embodiments, the operations can further comprise: in response to receiving the utterance, it is determined that the utterance embodies one or more requests to access content that the second computing device is not currently permitted to access. In some embodiments, providing the second request for the first computing device to respond to the one or more helper requests comprises: audio data or text data characterizing one or more portions of an utterance provided by a user to a second computing device is provided to the first computing device. In some embodiments, providing the second request for the first computing device to respond to the one or more helper requests comprises: in response to a user providing an utterance to the second computing device, action data characterizing one or more automated helper actions to be performed by the automated helper is provided to the first computing device.
In yet other embodiments, a method implemented by one or more processors is set forth as including operations such as: the method includes receiving an utterance from a user associated with a first computing device, wherein the utterance is received at a second computing device in a common environment with the first computing device and the user, and wherein each of the first computing device and the second computing device provides access to a respective automated assistant. The operations can further include: providing, by the second computing device, to the first computing device, a first request for the first computing device to confirm authentication of the user on the first computing device, wherein the first request embodies an authentication value accessible by one or more devices authenticated by the user. The operations can further include: when the first computing device has access to the authentication value: the authentication data is received by the second computing device, the authentication data indicating to the second computing device that the first computing device is able to access the authentication value. The operations can further include: providing, by the second computing device and based on the authentication value being accessible to the first computing device, a second request to provide the user preference data for the first computing device to respond to one or more helper requests embodied in the utterance. The operations can further include: receiving, by the second computing device and in response to providing the second request, user preference data identifying one or more user preferences to be employed by an automated assistant of the second computing device when responding to one or more assistant requests submitted by the user. The operations can further include: causing, by the second computing device, one or more interfaces of the second computing device to present automated helper output based on the user preference data.
In some embodiments, the method can further comprise the operations of: generating automated assistant output based on the user preference data further based on the automated assistant output data, wherein the user preference data identifies one or more automated speech recognition models to be used when processing utterances from the user. In some embodiments, the operations can further comprise: generating automation helper output based on the user preference data further based on the automation helper output, wherein the user preference data identifies one or more text-to-speech models to be used when presenting the automation helper output to the user. The operations can further include: automated helper output data is generated in response to the one or more helper requests based on the user preference data, wherein the user preference data identifies a content ranking of candidate content identified by the second computing device when generating the automated helper output data. The operations can further include: when the first computing device is unable to access the authentication value: causing, by the second computing device, one or more interfaces of the second computing device to present different automated assistant outputs that are not based on the user preference data.
Claims (22)
1. A method implemented by one or more processors, the method comprising:
receiving, at a first computing device, a request for the first computing device to process an utterance submitted by a user to a second computing device,
wherein each of the first computing device and the second computing device are located in a common environment and provide access to a respective automation assistant, an
Wherein the second computing device encrypts the request using signature data generated by the second computing device using a biometric signature corresponding to the user;
processing, by the first computing device, the request from the second computing device to identify one or more helper requests embodied in the request;
generating, by the first computing device, helper response data characterizing one or more automated helper responses in response to the one or more helper requests; and
causing, by the first computing device, the second computing device to present the one or more automated helper responses to the user using the helper response data.
2. The method of claim 1, wherein processing the request from the second computing device comprises:
accessing, by the first computing device, other signature data associated with the user, an
Using the other signature data to identify an authentication value embodied in the request or other data from the second computing device.
3. The method of claim 2, wherein causing the second computing device to present the one or more automated helper responses comprises:
providing the authentication value from the first computing device to the second computing device,
wherein the authentication value is generated by the second computing device in response to the second computing device receiving the utterance from the user.
4. The method of any of the preceding claims, wherein generating the helper response data comprises:
accessing, by the first computing device, stored content that is not stored at the second computing device when the second computing device receives the utterance from the user.
5. The method of any of the preceding claims wherein generating the helper response data comprises:
accessing content associated with the user's account,
wherein the second computing device is not authenticated to directly access the account of the user.
6. The method of any of the preceding claims, wherein causing the second computing device to present the one or more automated helper responses comprises:
transmitting the helper response data from the first computing device to the second computing device via a local area network, a Bluetooth connection, or a wide area network,
wherein transmitting the helper response data causes the second computing device to present one or more automated helper responses.
7. The method of any of the preceding claims, further comprising:
providing, at an interface of the first computing device and in response to receiving the request from the second computing device, a prompt allowing the user to select whether to permit the first computing device to respond to the request or a subsequent request from the second computing device.
8. The method of any one of the preceding claims, further comprising:
providing, at an interface of the first computing device and in response to receiving the request from the second computing device, a prompt that allows the user to limit when the first computing device is permitted to respond to the request or a subsequent request from the second computing device.
9. A method implemented by one or more processors, the method comprising:
receiving an utterance from a user associated with a first computing device,
wherein the utterance is received at a second computing device in a common environment with the first computing device and the user, and
wherein each of the first computing device and the second computing device provides access to a respective automation assistant;
providing, by the second computing device to the first computing device, a first request for the first computing device to confirm that the user is authenticated on the first computing device,
wherein the first request embodies an authentication value accessible by one or more devices authenticated by the user;
receiving, by the second computing device, the authentication value indicating to the second computing device that the first computing device has access to the authentication value;
providing, by the second computing device and based on the authentication value, a second request for the first computing device to respond to one or more helper requests embodied in the utterance;
receiving, by the second computing device and in response to providing the second request, helper response data in response to the one or more helper requests embodied in the utterance; and
causing, by the second computing device, one or more interfaces of the second computing device to present automated helper outputs based on the helper response data.
10. The method of claim 9, further comprising:
identifying, by the second computing device, a genuine signature of the user; and
generating, by the second computing device, the first request by encrypting the authentication value using the real signature.
11. The method of claim 10, further comprising:
processing, by the second computing device, the helper response data using the true signature,
wherein the helper response data is encrypted by the first computing device using the authentic signature.
12. The method of claim 10 or claim 11, wherein the authentic signature of the user corresponds to an audio-based signature or an image-based signature.
13. The method of any of claims 9 to 12, further comprising:
in response to receiving the utterance, determining that the utterance embodies one or more requests to access content that the second computing device is not currently permitted to access.
14. The method of any of claims 9 to 13, wherein providing the second request for the first computing device to respond to one or more helper requests comprises:
providing, to the first computing device, audio data or text data characterizing one or more portions of the utterance provided by the user to the second computing device.
15. The method of any of claims 9 to 14, wherein providing the second request for the first computing device to respond to one or more helper requests comprises:
in response to the user providing the utterance to the second computing device, providing, to the first computing device, action data characterizing one or more automated helper actions to be performed by the automated helper.
16. A method implemented by one or more processors, the method comprising:
receiving an utterance from a user associated with a first computing device,
wherein the utterance is received at a second computing device in a common environment with the first computing device and the user, and
wherein each of the first computing device and the second computing device provides access to a respective automation assistant;
providing, by the second computing device to the first computing device, a first request for the first computing device to confirm that the user has been authenticated on the first computing device,
wherein the first request embodies an authentication value accessible by one or more devices authenticated by the user;
when the first computing device has access to the authentication value:
receiving, by the second computing device, authentication data indicating to the second computing device that the first computing device has access to the authentication value;
providing, by the second computing device and based on the authentication value being accessible to the first computing device, a second request to provide user preference data for the first computing device to respond to one or more helper requests embodied in the utterance;
receiving, by the second computing device and in response to providing the second request, the user preference data identifying one or more user preferences to be employed by an automated assistant of the second computing device when responding to the one or more assistant requests submitted by the user; and
causing, by the second computing device, one or more interfaces of the second computing device to present an automated helper output based on the user preference data.
17. The method of claim 16, further comprising:
automated helper output data based on which the automated helper output is generated based on the user preference data,
wherein the user preference data identifies one or more automatic speech recognition models to be used when processing the utterance from the user.
18. The method of claim 16 or claim 17, further comprising:
generating automated assistant output data based on the user preference data further based on the automated assistant output data,
wherein the user preference data identifies one or more text-to-speech models to be used when presenting the automated assistant output to the user.
19. The method of any of claims 16 to 18, further comprising:
generating automated helper output data responsive to the one or more helper requests based on the user preference data,
wherein the user preference data identifies a content ranking of candidate content identified by the second computing device when generating the automated assistant output data.
20. The method of any of claims 16 to 19, further comprising:
when the first computing device is unable to access the authentication value:
causing, by the second computing device, the one or more interfaces of the second computing device to present different automated assistant outputs that are not based on the user preference data.
21. A system comprising one or more processors and memory operably coupled with the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to perform the method of any of claims 1-20.
22. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform the method of any one of claims 1-20.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/947,753 US11798546B2 (en) | 2020-08-14 | 2020-08-14 | Transient personalization mode for guest users of an automated assistant |
US16/947,753 | 2020-08-14 | ||
PCT/US2020/064944 WO2022035456A1 (en) | 2020-08-14 | 2020-12-14 | Transient personalization mode for guest users of an automated assistant |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115769202A true CN115769202A (en) | 2023-03-07 |
Family
ID=74181332
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080102329.2A Pending CN115769202A (en) | 2020-08-14 | 2020-12-14 | Transient personalization mode for guest users of automated assistants |
Country Status (6)
Country | Link |
---|---|
US (2) | US11798546B2 (en) |
EP (2) | EP4258257A3 (en) |
KR (1) | KR20230038771A (en) |
CN (1) | CN115769202A (en) |
CA (1) | CA3182638A1 (en) |
WO (1) | WO2022035456A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20210089295A (en) * | 2020-01-07 | 2021-07-16 | 엘지전자 주식회사 | Data processing method based on artificial intelligence |
Family Cites Families (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8477940B2 (en) | 2005-07-15 | 2013-07-02 | Tyfone, Inc. | Symmetric cryptography with user authentication |
CN104518875B (en) * | 2013-09-27 | 2018-12-11 | 深圳市腾讯计算机系统有限公司 | A kind of method that authentication and account obtain, mobile terminal |
KR102065029B1 (en) * | 2014-03-28 | 2020-01-10 | 삼성전자주식회사 | Method for sharing data of electronic device and electronic device thereof |
US9257120B1 (en) * | 2014-07-18 | 2016-02-09 | Google Inc. | Speaker verification using co-location information |
EP3396667A1 (en) * | 2017-04-24 | 2018-10-31 | Koninklijke Philips N.V. | Personal voice assistant authentication |
WO2018213415A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Far-field extension for digital assistant services |
KR102392717B1 (en) | 2017-12-08 | 2022-04-29 | 구글 엘엘씨 | Distributed identification of network systems |
DK201870355A1 (en) * | 2018-06-01 | 2019-12-16 | Apple Inc. | Virtual assistant operation in multi-device environments |
CN112513845A (en) | 2018-06-06 | 2021-03-16 | 亚马逊技术股份有限公司 | Transient account association with voice-enabled devices |
US11425118B2 (en) * | 2018-08-06 | 2022-08-23 | Giesecke+Devrient Mobile Security America, Inc. | Centralized gateway server for providing access to services |
US20200127988A1 (en) * | 2018-10-19 | 2020-04-23 | Apple Inc. | Media intercom over a secure device to device communication channel |
US11393478B2 (en) * | 2018-12-12 | 2022-07-19 | Sonos, Inc. | User specific context switching |
US11069363B2 (en) * | 2018-12-21 | 2021-07-20 | Cirrus Logic, Inc. | Methods, systems and apparatus for managing voice-based commands |
US10693872B1 (en) | 2019-05-17 | 2020-06-23 | Q5ID, Inc. | Identity verification system |
US20210090561A1 (en) * | 2019-09-24 | 2021-03-25 | Amazon Technologies, Inc. | Alexa roaming authentication techniques |
US11043220B1 (en) * | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
-
2020
- 2020-08-14 US US16/947,753 patent/US11798546B2/en active Active
- 2020-12-14 CA CA3182638A patent/CA3182638A1/en active Pending
- 2020-12-14 WO PCT/US2020/064944 patent/WO2022035456A1/en unknown
- 2020-12-14 CN CN202080102329.2A patent/CN115769202A/en active Pending
- 2020-12-14 EP EP23179364.7A patent/EP4258257A3/en active Pending
- 2020-12-14 EP EP20839475.9A patent/EP3983915B1/en active Active
- 2020-12-14 KR KR1020237005408A patent/KR20230038771A/en unknown
-
2023
- 2023-09-18 US US18/369,610 patent/US20240005924A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022035456A1 (en) | 2022-02-17 |
KR20230038771A (en) | 2023-03-21 |
CA3182638A1 (en) | 2022-02-17 |
EP3983915A1 (en) | 2022-04-20 |
EP4258257A3 (en) | 2023-12-06 |
EP4258257A2 (en) | 2023-10-11 |
EP3983915B1 (en) | 2023-06-28 |
US20240005924A1 (en) | 2024-01-04 |
US20220051663A1 (en) | 2022-02-17 |
US11798546B2 (en) | 2023-10-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11289100B2 (en) | Selective enrollment with an automated assistant | |
US11704940B2 (en) | Enrollment with an automated assistant | |
EP3920181B1 (en) | Text independent speaker recognition | |
US11902275B2 (en) | Context-based authentication of a user | |
US11727925B2 (en) | Cross-device data synchronization based on simultaneous hotword triggers | |
US20240046935A1 (en) | Generating and/or utilizing voice authentication biasing parameters for assistant devices | |
US20240005924A1 (en) | Transient personalization mode for guest users of an automated assistant | |
US20220094650A1 (en) | Asynchronous resumption of dialog session(s) between a user and an automated assistant based on intermediate user interaction(s) | |
US11315575B1 (en) | Automatic generation and/or use of text-dependent speaker verification features | |
WO2023091171A1 (en) | Shared assistant profiles verified via speaker identification | |
KR20230147157A (en) | Contextual suppression of assistant command(s) | |
US20230409277A1 (en) | Encrypting and/or decrypting audio data utilizing speaker features | |
US11984128B2 (en) | Automatic generation and/or use of text-dependent speaker verification features | |
US20240087564A1 (en) | Restricting third party application access to audio data content | |
US20240111811A1 (en) | Selecting a device to respond to device-agnostic user requests |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |