US8209183B1 - Systems and methods for correction of text from different input types, sources, and contexts - Google Patents
Systems and methods for correction of text from different input types, sources, and contexts Download PDFInfo
- Publication number
- US8209183B1 US8209183B1 US13/177,821 US201113177821A US8209183B1 US 8209183 B1 US8209183 B1 US 8209183B1 US 201113177821 A US201113177821 A US 201113177821A US 8209183 B1 US8209183 B1 US 8209183B1
- Authority
- US
- United States
- Prior art keywords
- input
- corrected
- segment
- text
- displaying
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/232—Orthographic correction, e.g. spell checking or vowelisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/26—Techniques for post-processing, e.g. correcting the recognition result
- G06V30/262—Techniques for post-processing, e.g. correcting the recognition result using context analysis, e.g. lexical, syntactic or semantic context
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
Definitions
- Word can be used to create, edit, and/or view documents that include text.
- Text can be used in other applications as well, such as web pages and messaging services.
- Additional software and/or hardware applications can be used to convert various inputs, such as speech, video, and paper documents, to computer-usable text. These applications can recognize words in input and generate corresponding text as output. In some cases, the output text includes errors.
- a method is provided. Inputs having a plurality of input types are received at a wearable computing device. A text string corresponding to the inputs is generated using the wearable computing device. The text string includes a plurality of segments. Each segment is associated with an input type of the plurality of input types. The text string is displayed using the wearable computing device. For a given segment of the text string, one or more corrected segments are generated by applying an error-correction filter to the given segment using the wearable computing device, wherein the error-correction filter is configured to correct errors based on an input type associated with the given segment and on a location-sensitive context. At least one of the one or more corrected segments is displayed using the wearable computing device. A selected corrected segment of the one or more corrected segments is selected using the wearable computing device. A corrected text string including the selected corrected segment is displayed using the wearable computing device.
- a wearable computing device in a second aspect of the disclosure of the application, includes a display; a plurality of input devices, wherein each input device is associated with an input type; a processor; and memory.
- the memory has one or more instructions that, in response to execution by the processor, causes the wearable computing device to perform functions.
- the functions include: (i) receiving inputs from at least some of the plurality of input devices, (ii) generating a text string corresponding to the inputs, wherein the text string includes a plurality of segments and each segment is associated with an input type of the plurality of input types, (iii) displaying the text string using the display, (iv) for a given segment of the text string, generating one or more corrected segments by applying an error-correction filter to the given segment using the wearable computing device, where the error-correction filter is configured to correct errors based on an input type associated with the given segment and on a location-sensitive context, (v) displaying at least the one or more corrected segments using the display, (vi) selecting a selected corrected segment of the one or more corrected segments; and (vii) displaying a corrected text string comprising the selected corrected segment using the display.
- an article of manufacture includes a computer-readable storage medium having instructions stored thereon that, in response to execution by a processor, cause the processor to perform operations.
- the instructions include: (i) instructions for receiving inputs having a plurality of input types, (ii) instructions for generating a text string corresponding to the inputs, wherein the text string includes a plurality of segments and each segment is associated with an input type of the plurality of input types, (iii) instructions for displaying the text string, (iv) instructions for generating, for a given segment of the text string, one or more corrected segments by applying an error-correction filter to the given segment, wherein the error-correction filter is configured to correct errors based on an input type associated with the given segment and on a location-sensitive context, (v) instructions for displaying at least the one or more corrected segments, (vi) instructions for selecting a selected corrected segment of the one or more corrected segments, and (vii) instructions for displaying a corrected text
- FIG. 1 is a first view of an example system for receiving, transmitting and displaying data, in accordance with example embodiments.
- FIG. 2 is a second view of the example system of FIG. 1 , in accordance with example embodiments.
- FIG. 3 is an example schematic drawing of computer network infrastructure, in accordance with an example embodiment.
- FIG. 4 is a functional block diagram for a wearable computing system, in accordance with an example embodiment.
- FIG. 5A depicts a first scenario of error correction in accordance with an example embodiment.
- FIGS. 5B , 5 C, and 5 D depict processing by an error correction module for the inputs received in the first scenario in accordance with an example embodiment.
- FIG. 6 depicts a second scenario of error correction in accordance with an example embodiment.
- FIG. 7 depicts a third scenario of error correction in accordance with an example embodiment.
- FIG. 8A depicts a fourth scenario of error correction in accordance with an example embodiment.
- FIG. 8B depicts a firth scenario of error correction in accordance with an example embodiment.
- FIG. 8C depicts a sixth scenario of error correction in accordance with an example embodiment.
- FIG. 9 is a flow chart of a method in accordance with an example embodiment.
- Such computing devices including wearable computing devices, commonly receive inputs from a number of sources, such as, but not limited to, keyboards, touch screens, paper documents, writing surfaces for hand-written input, microphones, and network interfaces.
- Computer-readable text, or “text” for short can be generated from these input sources, and it is not unusual that this text includes errors.
- text correction it is well known to perform text correction; for example, an e-mail interface receiving text input can provide suggestions for completing and/or correcting text as entered.
- Text received by a computing device can include errors of at least three different types: input type errors, input source errors, and input context errors.
- Input type errors are errors that can be associated with an input source for the text. For example, text from a keyboard often has the word “the” misspelled as “teh”. As another example, a speech-to-text converter could convert the spoken word “virus” erroneously into the text “wire us.” Similar examples for errors in text conversion can be found for text based on optical character recognition (OCR), handwritten input, and other input sources.
- OCR optical character recognition
- some errors in text can be linked to specific input sources. For example, suppose two users, User A and User B, are providing spoken input via a microphone to a speech-to-text program. When both users speak the word “virus”, the speech-to-text program may erroneously generate the text “wire us” for User A, while correctly generating the text “virus” for User B. However, when both users speak the words “you all”, the speech-to-text program may erroneously generate the text “yawl” for User B, while correctly generating the text “you all” for User A.
- the context can be “location sensitive” or depend on location.
- a context can include associations to people, buildings, activities, speech, video, and/or other inputs that are proximate to the wearable computing device.
- the context can be based on a time of the year; for example, the context can include information that phrases associated with Christmas such as “Santa Claus”, “Yuletide”, “Jingle Bells”, etc. are more common in December than in July.
- Location-sensitive context can include documents, location, contact information, etc., just as with any other type of context.
- devices and methods can involve an auto-correction technique that corrects errors based on, e.g., the input type, input source or context.
- the device can determine the input types(s), input source(s) and context(s) and scan for the text for errors using one or more different types of error-correction filters: input-type, input-device, and input-context.
- These devices and methods can perform error correction using at least two different techniques: an automatic technique, where the text is corrected without external input, and a manual technique, where the text is corrected by providing a number of correction options, and then corrected based on a selection of one of the correction options.
- FIG. 1 illustrates an example system 100 for receiving, transmitting, and displaying data.
- the system 100 is shown in the form of a wearable computing device. While FIG. 1 illustrates eyeglasses 102 as an example of a wearable computing device, other types of wearable computing devices could additionally or alternatively be used.
- the eyeglasses 102 comprise frame elements including lens-frames 104 and 106 and a center frame support 108 , lens elements 110 and 112 , and extending side-arms 114 and 116 .
- the center frame support 108 and the extending side-arms 114 and 116 are configured to secure the eyeglasses 102 to a user's face via a user's nose and ears, respectively.
- Each of the frame elements 104 , 106 , and 108 and the extending side-arms 114 and 116 may be formed of a solid structure of plastic or metal, or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through the eyeglasses 102 .
- Each of the lens elements 110 and 112 may include a material on which an image or graphic can be displayed. Each of the lens elements 110 and 112 may also be sufficiently transparent to allow a user to see through the lens element. These two features of the lens elements could be combined; for example, to provide an augmented reality or heads-up display where the projected image or graphic can be superimposed over or provided in conjunction with a real-world view as perceived by the user through the lens elements.
- the extending side-arms 114 and 116 are each projections that extend away from the frame elements 104 and 106 , respectively, and can be positioned behind a user's ears to secure the eyeglasses 102 to the user.
- the extending side-arms 114 and 116 may further secure the eyeglasses 102 to the user by extending around a rear portion of the user's head.
- the system 100 may be connected to or be integral to a head-mounted helmet structure. Other possibilities exist as well.
- the system 100 may also include an on-board computing system 118 , a video camera 120 , a sensor 122 , and finger-operable touch pads 124 , 126 .
- the on-board computing system 118 is shown to be positioned on the extending side-arm 114 of the eyeglasses 102 ; however, the on-board computing system 118 may be provided on other parts of the eyeglasses 102 .
- the on-board computing system 118 may include a processor and memory, for example.
- the on-board computing system 118 may be configured to receive and analyze data from sensor 118 , video camera 120 and finger-operable touch pads 124 , 126 (and possibly from other sensory devices, user interfaces, or both) and generate images for output to the lens elements 110 and 112 .
- touch pads 124 and/or 126 can be configured to process handwriting inputs.
- the video camera 120 is shown to be positioned on the extending side-arm 114 of the eyeglasses 102 ; however, the video camera 120 may be provided on other parts of the eyeglasses 102 .
- the video camera 120 may be configured to capture images at various resolutions or at different frame rates. Many video cameras with a small form-factor, such as those used in cell phones or webcams, for example, may be incorporated into an example of the system 100 .
- FIG. 1 illustrates one video camera 120 , more video cameras may be used, and each may be configured to capture the same view, or to capture different views.
- the video camera 120 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by the video camera 120 may then be used to generate an augmented reality where computer generated images appear to interact with the real-world view perceived by the user.
- the sensor 122 is shown mounted on the extending side-arm 116 of the eyeglasses 102 ; however, the sensor 122 may be provided on other parts of the eyeglasses 102 .
- the sensor 122 may include one or more motion sensors, such as a gyroscope and/or an accelerometer. Other sensing devices may be included within the sensor 122 and other sensing functions may be performed by the sensor 122 .
- the finger-operable touch pads 124 , 126 are shown mounted on the extending side-arms 114 , 116 of the eyeglasses 102 . Each of finger-operable touch pads 124 , 126 may be used by a user to input commands.
- the finger-operable touch pads 124 , 126 may sense at least one of a position and a movement of a finger via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities.
- the finger-operable touch pads 124 , 126 may be capable of sensing finger movement in a direction parallel to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied.
- the finger-operable touch pads 124 , 126 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pads 124 , 126 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when the user's finger reaches the edge of the finger-operable touch pads 124 , 126 . Each of the finger-operable touch pads 124 , 126 may be operated independently, and may provide a different function.
- FIG. 2 illustrates another view of the system 100 of FIG. 1 .
- the lens elements 110 and 112 may act as display elements.
- the eyeglasses 102 may include a first projector 128 coupled to an inside surface of the extending side-arm 116 and configured to project a display 130 onto an inside surface of the lens element 112 .
- a second projector 132 may be coupled to an inside surface of the extending side-arm 114 and configured to project a display 134 onto an inside surface of the lens element 110 .
- the lens elements 110 and 112 may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors 128 and 132 . In some embodiments, a special coating may not be used (e.g., when the projectors 128 and 132 are scanning laser devices).
- the lens elements 110 , 112 themselves may include: a transparent or semi-transparent matrix display, such as an electroluminescent display or a liquid crystal display; one or more waveguides for delivering an image to the user's eyes; and/or other optical elements capable of delivering an in focus near-to-eye image to the user.
- a corresponding display driver may be disposed within the frame elements 104 and 106 for driving such a matrix display.
- a scanning laser device such as low-power laser or LED source and accompanying scanning system, can draw a raster display directly onto the retina of one or more of the user's eyes. The user can then perceive the raster display based on the light reaching the retina.
- system 100 can be configured for audio output.
- system 100 can be equipped with speaker(s), earphone(s), and/or earphone jack(s).
- audio output can be provided via the speaker(s), earphone(s), and/or earphone jack(s).
- FIG. 3 is a schematic drawing of a system 136 illustrating an example computer network infrastructure.
- a device 138 communicates using a communication link 140 (e.g., a wired or wireless connection) to a remote device 142 .
- the device 138 may be any type of device that can receive data and display information corresponding to or associated with the data.
- the device 138 may be a heads-up display system, such as the eyeglasses 102 described with reference to FIGS. 1 and 2 .
- the device 138 may include a display system 144 comprising a processor 146 and a display 148 .
- the display 148 may be, for example, an optical see-through display, an optical see-around display, or a video see-through display.
- the processor 146 may receive data from the remote device 142 , and configure the data for display on the display 148 .
- the processor 146 may be any type of processor, such as a micro-processor or a digital signal processor, for example.
- the device 138 may further include on-board data storage, such as memory 150 shown coupled to the processor 146 in FIG. 3 .
- the memory 150 may store software and/or data that can be accessed and executed by the processor 146 , for example.
- the remote device 142 may be any type of computing device or transmitter including a laptop computer, a mobile telephone, etc., that is configured to transmit data to the device 138 .
- the remote device 142 and the device 138 may contain hardware to enable the communication link 140 , such as processors, transmitters, receivers, antennas, etc.
- the communication link 140 is illustrated as a wireless connection.
- the wireless connection could use, e.g., Bluetooth® radio technology, communication protocols described in IEEE 802.11 (including any IEEE 802.11 revisions), Cellular technology (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), or Zigbee® technology, among other possibilities.
- wired connections may be used.
- the communication link 140 may be a wired link via a serial bus such as a universal serial bus or a parallel bus.
- a wired connection may be a proprietary connection as well.
- the remote device 142 may be accessible via the Internet and may comprise a computing cluster associated with a particular web service (e.g., social-networking, photo sharing, address book, etc.).
- FIG. 4 is a functional block diagram for a wearable computing system 400 in accordance with an example embodiment.
- System 400 is configured to monitor incoming data from a number of input devices 404 and display information related to the incoming data on Head Mounted Display (HMD) 401 .
- HMD Head Mounted Display
- system 400 can monitor speech received via microphone 408 and, may convert the speech to text using speech-to-text module 426 .
- the input speech can include instructions that specify actions and objects for the actions.
- system 400 can be configured to detect instructions, and to responsively initiate the actions specified in the instructions.
- system 400 includes one or more input-device interfaces 402 for receiving data from input devices 404 and one or more output devices, such as HMD 401 , for presenting information related to the data from input devices.
- the input devices 404 include, for example, an application 406 , a microphone 408 , a keyboard 410 , a camera 412 , a touchpad 414 , and a location sensor 434 .
- a given input-device interface 402 may be configured to interface with and receive data from a single input device, such as microphone 408 .
- a given input-device interface 402 may be configured to simultaneously interface with multiple input devices, such as some or all of input devices 406 - 414 .
- System 400 can receive a number of different types of input data from input devices 404 .
- system 400 may receive, for example, audio data from microphone 408 , text data from keypad 410 , video data and/or image data from camera(s) 412 , and/or gesture data from touchpad 414 .
- multiple inputs can be received simultaneously.
- a system may be configured to receive other modalities of data, in addition or in the alternative to those described, without departing from the scope of the invention.
- some or all types of input data can be converted to text.
- input data such as input data from keyboard 410 and touchpad 414
- conversion modules such as speech-to-text module 426 and/or a video-to-text module (not shown in FIG. 4 )
- speech-to-text module 426 and/or a video-to-text module can be used to convert input data to text.
- Applications such as application 406 , may generate text, audio input, video input, and/or other types of input (e.g., encrypted input, compressed input, other types of binary input, etc.).
- application-specific techniques can be used generate input text from inputs derived from application 406 .
- Location sensor 434 can utilize one or more technologies and sensors to determine and provide a location of system 400 .
- Example technologies include, but are not limited to, Global Positioning System (GPS) technologies and sensors, other satellite-based technologies and sensors, inertial navigation technologies, timing circuitry, accelerometers, compasses, velocity sensors, and gyroscopes.
- location sensor 434 can determine and provide related information to system 400 , such as velocity including both speed and direction(s) of travel, acceleration, distance(s) traveled, and timing information. Many other types of related information are possible as well.
- system 400 can be configured with one or more input and/or output ports or jacks configured for communicating with input and/or output devices.
- system 400 includes an input selection module 416 , which generally functions to evaluate the input data from the various input devices 404 .
- input selection module 416 may be configured to receive input data from the input devices 404 via input device interfaces 402 and detect one or more data patterns in the input data.
- input selection module 416 may detect multiple concurrent data patterns in the input data. For example, input selection module 416 may detect a first data pattern in data from a first source and, simultaneously, detect a second data pattern in data from a second source. As such, selection criteria 418 may provide input-selection rules that prioritize certain data patterns and/or certain input devices.
- selection criteria 418 may prioritize detection of speech in audio data from microphone 408 over other data patterns detected in video data from camera 412 . Accordingly, some embodiments may be configured to display a text conversion of speech whenever speech matching a data pattern is detected in incoming audio data, regardless of whether there is also a matching data pattern in incoming video data. Similarly, if input selection module 416 detects that a user is entering text via a keyboard 410 , this text may be displayed, even when there is a matching data pattern in incoming audio data and/or in incoming video data; for example, where keyboard data is given priority over audio data and video data by selection criteria 418 .
- selection criteria 418 may provide input-selection rules that prioritize certain data patterns when multiple matching data patterns are detected from a common input device. For instance, when explicit commands are received in audio data, the explicit commands may be given priority over implicit information in the audio data from input devices 404 . As one specific example, input-selection criteria 418 may specify that when a user says “show video” (e.g., when “show video” is detected in audio data from microphone 408 ), then this should be interpreted as an explicit command to select camera 412 as the input device and display video from camera 412 .
- selection criteria 418 may specify other hierarchies and/or other prioritizations of input devices and/or data patterns, without departing from the scope of the invention. Thus, selection criteria 418 may be based on one or more objectives in a specific implementation.
- the selection criteria 418 indicate that multiple input devices 404 should be selected.
- a scenario may exist where text is detected in input data from keyboard 410 and speech is detected in audio data from microphone 408 .
- speech-to-text module 426 may convert the speech from the audio data to text, and this text may be merged with the text from the keyboard for display.
- scenarios may exist where video or an image from camera 412 is displayed, and text is overlaid on top of the video or image.
- the text may be obtained from the keyboard 410 and/or obtained via speech-to-text module 426 converting speech in audio data from microphone 408 .
- a location input from location sensor 434 can be combined with text and/or video derived from one or more of input devices 406 - 414 .
- Many other examples of combinations of multiple input devices, which combine a variable number of input devices, are also possible.
- the selection criteria 418 can indicate that inputs are to be corrected by error correction module 430 .
- Error correction module 430 can be configured to receive one or more segments of text related to any of the input devices 404 , evaluate the segments of text, and responsively generate one or more corrected segments of text.
- Error correction module 430 can utilize error correction data 432 , historical context 424 , and/or selection criteria 418 to correct received text.
- Error correction data 432 can include one or more filters 432 a - 432 c for correcting errors.
- the one or more filters can include: (a) one or more filters to correct errors based on input type 432 a , shown in FIG. 4 as “F(type)”, (b) one or more filters to correct errors based on input source 432 b , shown in FIG. 4 as “F(src)”, and/or (c) one or more filters to correct errors based on input context 432 c , shown in FIG. 4 as “F(con).”
- some or all of filters 432 a - 432 c can be combined; for example, a source-context filter can combine filter(s) based on input source and filter(s) based on input context. Other filters are possible as well.
- the received text can be evaluated for applying type-based corrections using input type filters 432 a .
- This filter can include a list, table, tree, trie, dictionary, database, and/or other data structure(s) configured to store at least one input word and corresponding correction(s) for the input word.
- error correction module 430 can find possible corrections for each word in the input text by looking up the each word in the input text in the data structure(s) storing the known corrections, and replacing the input word with any correction(s).
- speech of “Get dog picture” can be received at microphone 408 and converted to text by speech-to-text module 426 .
- the text generated by text-to-speech module 426 is “Get dog pitcher.”
- error correction module 430 can evaluate the received text and provide corrected text in response.
- the input type is speech received via microphone 408 .
- error correction module 430 can then determine that the appropriate input type filter 432 a is a microphone-related filter and then provide each input word to the microphone-related filter. For instance, suppose that speech-to-text module 426 frequently generated the word “pitcher” instead of the word “picture” for input coming from microphone 408 . Then, in response to input text with the word “pitcher,” the microphone-related filter can provide a possible correction of “picture.”
- Input source filter(s) 432 b include filters that take an input source, such as a particular speaker, typist, or touch-pad user, into account.
- Input context filter(s) 432 c can evaluate the input text in view of historical context 424 to provide possible corrections to the text. Contexts are discussed in more detail in at least the “Selection of Content Based on Context Information” section below.
- input context filters 432 c can be utilized by context evaluation module 420 to correct documents and/or other components of historical context 424 .
- the possible correction(s) can be applied automatically or manually.
- Automatic correction techniques involve text correction without external input. For example, suppose the only correction to the “Get dog pitcher” input was to change the word “pitcher” to “picture.” An example automatic correction technique can replace the word “pitcher” with “picture” without prompting a user or otherwise receiving external input. Upon applying the automatic correction technique, the corresponding text segment of “Get dog picture” can be output as corrected text.
- Manual correction techniques involve providing a number of correction options, and then correcting text based on a selection of one of the correction options. For example, suppose the only correction to the “Get dog pitcher” input was to change the word “pitcher” to “picture.” An example manual technique can generate a prompt with the input text of “Get dog pitcher” and a request for input such as “Do you want to replace ‘pitcher’ with ‘picture’?” In response to displaying the prompt, system 400 can receive input, via one or more of input devices 404 , related to whether or not to replace the word “pitcher” with “picture” and generate the corresponding output text segment based on the decision. In some embodiments, the prompt may not include the input text; rather the prompt may provide option(s) for correction only.
- part or all of the functionality of one or more of the herein-described modules 416 , 420 , 426 , 430 , selection criteria 418 , and historical context 424 can be combined with one or more other modules.
- the part or all of the functionality of error correction module 430 can be combined with context evaluation module 420 (or vice versa).
- Prompts, corrections, and/or text segments can be displayed using HMD 401 and/or on another display device (not shown in FIG. 4 ).
- output can be provided to other devices than HMD 401 ; for example, output can be communicated via communication link 140 .
- system 400 is equipped with speaker(s), earphone(s), and/or earphone jack(s)
- audio output can be provided via the speaker(s), earphone(s), and/or earphone jack(s).
- Many other examples and/or outputs are possible as well.
- System 400 can select an input based on implicit information extracted from input data from the various possible input devices. This implicit information may correspond to certain data patterns in the input data.
- input selection module 416 may monitor incoming audio data for various data patterns, according to the input-selection criteria.
- the input-selection criteria may specify numerous types of data patterns, which may vary in complexity and/or form.
- input selection module 416 may monitor audio data for: (i) patterns that are indicative of human speech in general, (ii) patterns that are indicative of human speech by a particular person (e.g., the owner of the device, or a friend or spouse of the owner), (iii) patterns that are indicative of a certain type of human speech (e.g., a question or a command), (iv) patterns that are indicative of human speech inflected with a certain emotion (e.g., angry speech, happy speech, sad speech, and so on), (v) patterns that are indicative of human speech associated with a certain context (e.g., a pre-recorded announcement on a subway car or a statement typically given by a flight attendant on an airplane), (vi) patterns that are indicative of a certain type of human speech (e.g., speech that is not in a speaker's native language), (vii) patterns indicative of certain types of non-speech audio (e.g., music) and/or of non-speech audio with certain
- a system may be configured to monitor audio data for data patterns that include or are indicative of speech by a particular user associated with the system (e.g., the owner of a wearable computer). Accordingly, the speech-to-text module 426 may convert the speech to corresponding text, which may then be displayed.
- a particular user associated with the system e.g., the owner of a wearable computer.
- the speech-to-text module 426 may convert the speech to corresponding text, which may then be displayed.
- the audio data in which speech is detected may be analyzed in order to verify that the speech is actually that of the user associated with the system. For example, the audio data can be compared to previously-received samples of audio data known to be utterances of the user associated with the system to verify that a speaker is (or is not) the user associated with the system.
- a “voiceprint” or template of the voice of the user associated with the system can be generated, and compared to a voiceprint generated from input audio data. Other techniques for verifying speaker(s) are possible as well.
- error correction module 430 can generate command(s) to search various sources for the named person's contact information or other information related to the named person.
- Error correction module 430 may perform one or more implicit searches, for example, when the person's name is stated in the midst of a conversation, and the user does not explicitly request the information about the person. Implicit searches can be performed for other types of content, such as other proper nouns, repeated words, unusual words, and/or other words.
- error correction module 430 can indicate that the contact information may be displayed.
- the contact information can include phone number(s), email address(es), mailing address(es), images/video related to the contact, and/or social networking information.
- the contact information may be displayed in various forms—the contact information can be displayed visually (e.g., using HMD 401 ) and/or audibly (e.g., using a text-to-speech module, not shown in FIG. 4 , in combination with an audio output, such as a speaker or earphone not shown in FIG. 4 ). Many other types of contact information are possible as well.
- text corresponding to the detected speech can be displayed.
- the default action may be not to display anything related to the detected speech. Other default actions are also possible.
- input selection module 416 may be configured to select an input device and/or to select input content based on context.
- input selection module 416 may coordinate with context evaluation module 420 , which is configured to evaluate context signals from one or more context information sources 422 .
- context evaluation module 420 may determine a context, and then relay the determined context to input selection module 416 .
- input selection module 416 can provide the context to another module; e.g., speech evaluation module 430 .
- the context can be a location-sensitive context.
- context evaluation module 420 may determine contexts, including location-sensitive contexts, using various “context signals,” which may be any signals or information pertaining to the state or the environment surrounding the system or a user associated with the system.
- a wearable computer may be configured to receive one or more context signals, such as location signals, time signals, environmental signals, and so on. These context signals may be received from, or derived from information received from, context information sources 422 and/or other sources.
- context signals may include: (a) the current time, (b) the current date, (c) the current day of the week, (d) the current month, (e) the current season, (f) a time of a future event, (g) a date of a future event or future user-context, (h) a day of the week of a future event or future user-context, (i) a month of a future event or future user-context, (j) a season of a future event or future user-context, (k) a time of a past event or past user-context, (l) a date of a past event or past user-context, (m) a day of the week of a past event or past user-context, (n) a month of a past event or past user-context, (o) a season of a past event or past user-context, ambient temperature
- context evaluation module 420 may identify the context as a quantitative or qualitative value of one context signal (e.g., the time of the day, a current location, a user status). The context may also be determined based on a plurality of context signals (e.g., the time of day, the day of the week, and the location of the user). In other embodiments, the context evaluation module 420 may extrapolate from the information provided by context signals. For example, a determined user-context may be determined, in part, based on context signals that are provided by a user (e.g., a label for a location such as “work” or “home”, or user-provided status information such as “on vacation”).
- context information sources 422 may include various sensors that provide context information. These sensors may be included as part of or communicatively coupled to system 400 . Examples of such sensors include, but are not limited to, a temperature sensor, an accelerometer, a gyroscope, a compass, a barometer, a moisture sensor, one or more electrodes, a shock sensor, one or more chemical sample and/or analysis systems, one or more biological sensors, an ambient light sensor, a microphone, and/or a digital camera, among others.
- System 400 may also be configured to acquire context signals from various data sources.
- context evaluation module 420 can be configured to derive information from network-based weather-report feeds, news feeds and/or financial-market feeds, and/or a system clock providing a reference for time-based context signals.
- a location-sensitive context can determine a location for the context using a location-determining system (e.g., location sensor 434 ), among others.
- system 400 may also be configured to learn over time about a user's preferences in certain contexts, and to update selection criteria 418 accordingly. For example, whenever an explicit input-content instruction is received, a corresponding entry may be created in historical context database 424 . This entry may include the input device and/or input content indicated by the input-content instruction, as well as context information that is available at or near the receipt of the input-content instruction. Context evaluation module 420 may periodically evaluate historical context database 424 and determine a correlation exists between explicit instructions to select a certain input device and/or certain input content, and a certain context. When such a correlation exists, selection criteria 418 may be updated to specify that the input device should be automatically selected, and/or that the input content should be automatically displayed, upon detection of the corresponding context.
- system 400 may be configured for an “on-the-fly” determination of whether a current context has historically been associated with certain input devices and/or certain input content.
- context evaluation module 420 may compare a current context to historical context data in historical context database 424 , and determine whether certain content historically has been correlated with the current context. If a correlation is found, then context evaluation module 420 may automatically display the associated input content.
- context evaluation module 420 can determine that the context include (a) a location of system 400 is related to “work” (b) a time just before or at 12:00, (c) a history of ordering lunch from the aforementioned seven restaurants, and (c) that six of the seven restaurants are open at this time, based on online listings. Then, the context evaluation module 420 can generate a command to display a reminder to “Order Lunch” with a list of the six open restaurants for order selection, and perhaps including information indicating that the seventh restaurant is closed. In response, the user can select a restaurant from the list using input devices 404 , choose another restaurant, dismiss/postpone the order, or perhaps, perform some other action.
- error correction module 430 may select the particular application that is appropriate to open the file as the input device, launch the selected application in the multimode input field, and then open the named file using the application.
- the user may say “search” and then state or type the terms to be searched, or identify other content to be searched, such as an image, for example.
- error correction module 430 may responsively form a query to a search engine, provide the query with subsequently stated terms or identified content, and receive search results in response to the query.
- Implicit searches also can be performed by this technique of forming a query based on identified content; e.g., the word(s) that provoked the implicit search, providing the query with identified content to a search engine, and receiving search results in response to the query.
- context evaluation module 420 and/or error correction module 430 can be provide predictive corrections based on context. For example, suppose system 400 is traveling northbound on Highway 101 in San Mateo, Calif., and receives text input of “Franciso airport.” Then, error correction module 430 can predict that, based on the location and direction of travel, that possible text corrections can include “San Francisco airport” or “SFO.”
- system 400 can weight grocery-related corrections higher between 5 PM and 6:30 PM. For example, in this scenario, suppose system 400 receives text input of “stak” at 3 PM and again receives text input of “stak” at 5:10 PM.
- the input of “stak” may lead to a text correction of “stake” being more highly weighted than a text correction of “steak”, while at 5:10 PM, the input of “stak” may have a text correction of “steak” being more highly weighted than a text correction of “stake.”
- Many other examples of predictive correction are possible as well.
- Historical context database 424 can also, or instead, include information about one or more documents for inclusion in a context.
- a document can be a bounded physical or digital representation of a body of information, or content.
- Content of the document can include text, images, video, audio, multi-media content, and/or other types of content.
- Document-property information can be associated with a document, such as, but not limited to, document names, sizes, locations, references, partial or complete content of documents, criteria for selecting documents to form a context and/or to locate a document. Other types of content and document-property information are possible as well.
- documents, including content and/or document-property information can be stored in historical context database 424 for at least use in context generation.
- a context may involve information derived from collection(s) of documents, such as, but not limited to, related collections of documents and past documents that have been created by the user and/or by other users. For example, based on the fact that a user has created a number of purchase order documents in the past, a background process may interpret the document in the context of a purchase order agreement, perhaps searching for supplier names and/or supplier part numbers as data for search request(s).
- a document can be accessed via one or more document references such as, but not limited to, a Uniform Resource Locator (URL), a Uniform Resource Identifier (URI), a volume name/number, a title, a page number, an address, a storage address, such as a memory address or disk sector, a library index number, an International Standard Book Number (ISBN), a bar code, and/or other identifying information.
- a document references such as, but not limited to, a Uniform Resource Locator (URL), a Uniform Resource Identifier (URI), a volume name/number, a title, a page number, an address, a storage address, such as a memory address or disk sector, a library index number, an International Standard Book Number (ISBN), a bar code, and/or other identifying information.
- One or more document references can be included with the document-property information about the document. Other document references are possible as well.
- FIG. 5A depicts a scenario 500 of error correction in accordance with an example embodiment.
- Scenario 500 begins at 500 A where keyboard 510 is used to generate text input 512 of “Teh grape wsa.” Text input 512 is received at device 520 (shown in FIG. 5A at 500 C and 500 E).
- An example wearable computing device that could be utilized as device 520 is system 400 , described in detail above with reference to FIG. 4 .
- Text input 512 is processed by error correction module 530 to generate corrected text segment 516 , shown at 500 B of FIG. 5 .
- corrected text segment 516 of “The grape was” is shown as being added to open document 514 , shown in FIG. 5A as entitled as “Grape Story 1.”
- FIG. 5A shows document 514 with corrected text segment 516 displayed on display 518 .
- Display 518 can be controlled by device 520 ; for example, using a wireless display controller on device 520 configured to send commands to drive display 518 equipped with a compatible wireless receiver.
- document 514 can be displayed using a projector of device 520 , such as projector 128 and/or 132 discussed above in the context of FIG. 1 .
- speaker 530 wearing device 520 speaks utterance 532 of “purple.”
- a microphone or other sound sensor associated with device 520 can receive utterance 532 and convert received utterance 532 into text; e.g., using speech-to-text module 426 .
- speech-to-text module 426 generates text 534 a of “purble” corresponding to utterance 532 .
- error correction module 530 can generate a corrected text segment corresponding to text input 534 a , and add the corrected text segment to document 514 .
- FIG. 5A shows corrected text segment as a list 534 b of three possible corrected text segments: 1. purple, 2. burping, and 3. other . . . . That is, list 534 b is part of an example of a manual correction technique, where speaker 530 is asked to choose between a number of different possible error corrections.
- corrected text segment 516 is shown as being generated using an automated correction technique, as no additional input was required to generate corrected text segment 516 from input text 512 .
- text related to keyboard input is shown using a bold font
- text related to speech input both as input and as corrected
- underlining text of various input types and/or input sources can be distinguished from each other using different fonts, color, font sizes, and/or spacing.
- text of one input type can be displayed using a first color (e.g., blue) and text of a second input type can be displayed using a second color (e.g., red).
- distinguishing text from different input types and/or input sources can be temporary; for example, as inputs are received they can be displayed using their distinguishing characteristics of font, color, size, etc. for a pre-determined amount of time; e.g., 5 or 10 seconds, before the text is shown as non-distinguished text.
- Other techniques for distinguishing text are possible as well.
- speaker 530 speaks utterance 540 of “one.”
- Utterance 540 can be converted to text via speech-to-text-module 426 and text input of “one” can be provided to error correction module 430 .
- error correction module 430 Upon receiving text input of “one,” error correction module 430 , performing a manual correction technique, can associate the text input with the first displayed item in the list 534 a , which is the word “purple.” Then, error correction module 430 can request the word “purple” be added to segment 534 b of document 514 .
- FIG. 5C shows the result of the manual correction technique with document 514 including phrase 542 of “The grape was purple.”
- FIGS. 5B , 5 C, and 5 D depict processing by error correction module 430 for inputs received in scenario 500 in accordance with an example embodiment.
- error correction module 430 determines an input type for input text of “Teh grape wsa”.
- the input type is determined as a “keyboard.”
- the input type can be determined by adding a input-type tag and/or other data to input data received at input sources 404 —e.g., for input received by application 406 , the input-type tag can be “application,” for input received by microphone 408 , the input-type tag can be “microphone,” and so on. Many other techniques for determining an input type associated with an input are possible as well.
- FIG. 5B shows, at block 554 , that error correction module 430 applies a correction based on the keyboard input type.
- the correction can involve utilizing a filter for the keyboard input type on each of the three input words: “Teh”, “grape”, and “wsa.” After applying the filter, FIG. 5B shows an output of “The grape was”.
- error correction module 430 can determine an input source for the input of “Teh grape wsa”.
- the input source can be determined using an input-source tag similar to the input-type tag described above. For example, if a user logs in or is otherwise identified, an input-type tag can be populated with the identified user information.
- an input source can be determined by data associated with the input, metadata associated with the input, speech and/or facial recognition techniques applied to the input audio and/or video.
- the input source for the “Teh grape wsa” input is unknown, as indicated in block 558 . As the input source is unknown, error correction module 430 may not apply any corrections to this input based on input source.
- an input context is determined.
- the input context can include context signals, context information derived from sensors, user preferences, documents, document-property information, and/or historical context database.
- the input context is merely that of a current document “Grape Story 1”, as indicated at block 562 . As this input context is an empty document, no corrections are performed based on the input context.
- the determination to apply an automatic correction technique can be based on input type, input source, input context, user preferences, and/or other information. For example, a number of possible corrections for the input text can be determined, and automatic techniques can be used when there is only one or fewer possible corrections per word to be applied, while manual techniques can be used when there are multiple possible corrections to be applied to at least one word, such as shown at 500 D of FIG. 5A .
- FIG. 5B shows that an automatic correction technique is to be used at block 566 .
- a corrected text segment of “The grape was”, which is the output of block 554 is the output of error correction module 430 at block 568 .
- the corrected text segment can replace the input text of “Teh grape wsa” in the “Grape Story 1” document.
- FIG. 5C shows an input to error correction module 430 of “purble” that was generated from utterance 532 of FIG. 5A .
- error correction module 430 determines an input type for the “purble” input text.
- the input type is determined to be “speech.”
- error correction module 430 applies a correction based on the speech input type.
- the correction can involve utilizing a filter for the speech input type for the input word of “purble.” After applying the input-type filter, FIG. 5B shows an output of “purple.”
- error correction module 430 can determine an input source for the input of “purble.”
- the input source for the “purble” input is a user with username of “GG 1 ” as shown in block 578 of FIG. 5C .
- An input-source filter based on the GG 1 input source can be applied to the input at block 580 .
- the input-source filter can take both the input source (e.g., GG 1 ) and the input type (e.g., speech) into account—that is, the input-source filter can account for errors based on both source and type.
- the filter act as a reverse filter; that is, taking output text, such as output from a speech-to-text or video-to-text module, as an input and then determining the corresponding input that could lead to the “output” text.
- output text such as output from a speech-to-text or video-to-text module
- speech-to-text conversion typically converts the word “burping” spoken by GG 1 to “purple.”
- an output list of corrections can have at least two words: “purple” as generated at block 574 , and “burping” as generated at block 580 .
- FIG. 5B shows that an input context of a current document “Grape Story 1” is formed at block 584 . As this input context now only includes the words “The grape was”, no corrections are performed based on the input context.
- FIG. 5C shows that at block 586 , a determination is made as to whether or not to apply an automatic correction technique.
- the determination is not to use the automatic correction technique; therefore, a manual correction technique is used.
- an output list is presented as a manual correction technique is used.
- FIG. 5C shows that the output list includes three options: “purple,” “burping,” and “other.” In some manual correction technique scenarios, the “other” option can be omitted.
- the “other” option is selected.
- additional input can be requested to correct the text; e.g., a prompt such as “Replace purble with what word(s)?”
- error correction module 430 can correct the input text using the additional text.
- an input of “one” is received.
- error correction module 430 can apply the input as part of the manual correction technique.
- the input type is determined, which is shown at block 594 to be “speech.” In scenario 500 , consider that the input of “one” is correctly converted to input text of “one.”
- the manual correction technique is applied using input text of “one.”
- the list generated at block 590 is retrieved.
- the input text applied to the list to determine the correction; that is, the word in the list associated with the input text, which is “purple,” is determined to be the correction word.
- FIG. 5D shows that, at block 598 , the word “purple” is output as the output of the manual correction technique.
- multiple output words can be generated for one input word.
- error correction module 430 can generate corrections with multiple output words, such as “can you dig it” and “can you digit”, as possible replacements for the “canyoudigit” input.
- FIG. 6 depicts a scenario 600 of error correction in accordance with an example embodiment.
- Scenario 600 begins at 600 A with speaker 520 instructing device 530 using utterance 610 of “Order 400 vires.”
- error correction module 430 of device 520 can receive input text corresponding to utterance 610 .
- the input text is “Order 400 vires.”
- Error correction module can apply input-type and/or input-source filters to the input text, as discussed above with reference to FIGS. 5B , 5 C, and 5 D.
- the input context of scenario 600 appears to be empty, so error correction module 430 may determine that input-context filters are not to be applied.
- the input text can be corrected, and the corresponding text of “Order 400 wires” can be output.
- FIG. 6 shows that device 520 shows corrected text 612 of “Order 400 wires” displayed on lens/display 614 of device 520 .
- FIG. 6 shows device 520 with camera 618 a and microphone 618 b observing speaker 620 speaking utterance 622 of “Orduh three hunnert resistahs, too.”
- Input text can be generated from inputs from both camera 618 a and microphone 618 b .
- input text can be generated from video input generated by camera 618 a using automated lip-reading and/or facial motion techniques. Other techniques can be used as well.
- Input texts from camera 618 a and microphone 618 b can be compared to find similarities and differences. Where the two input texts are similar, error correction module 430 can determine that the corrected output is likely to involve similar words; while error correction module 430 can determine that errors are more likely where the two input texts are different. Error correction module 430 can apply input-type and/or input-source filters to the input text, as discussed above with reference to FIGS. 5B , 5 C, and 5 D.
- an input-context filter can be used to correct utterance 622 .
- error correction module 430 can determine the word “orduh” of utterance 622 is more likely to be the word “order.” Further, in the context of an already-begun order, error correction module 430 can indicate that the final word of utterance 622 is the word “too” as opposed to “to” or “two.”
- Figure 600D shows that, after applying input-type, input-source filters, and/or input-context filters, text 624 of “300 resistors” is generated and displayed on lens/display 616 of device 520 .
- FIG. 7 depicts a scenario 700 of error correction in accordance with an example embodiment.
- Scenario 700 begins at 700 A with speaker 710 composing a document via utterance 712 of “Wiruses can be deadly.”
- Utterance 710 can be received by wearable computing device 720 , which can be an embodiment of system 400 configured with an output port configured to connect to and drive device 716 via connection 722 .
- utterance 710 can be received by a microphone of device 720 and converted to text by a speech-to-text module.
- Error correction module 430 of device 720 can receive the text corresponding to utterance 712 , which in scenario 700 is “Wiruses can be deadly.”
- Figure 700B shows that the document 714 being composed is entitled “Influenza Paper23.”
- the input context can include document 714 as well as other papers on influenza (e.g., Influenza Papers 1 through 22) edited by speaker 710 , papers not edited by speaker 710 , additional documents on influenza, web pages on viruses (including the influenza viruses), and other types of information about influenza and viruses.
- Error correction module 430 can apply input-type, input-source, and/or input-context filters to the input text of “Wiruses can be deadly,” as discussed above with reference to FIGS. 5B , 5 C, and 5 D.
- input-source and/or input-context filters can correct the word “Wiruses” to be “Viruses.”
- the input text can be corrected, and the corresponding text of “Viruses can be deadly” can be output to document 714 displayed on display 716 .
- corrected text 718 is shown as being distinguished in document 714 via use of bold and underlined font.
- Other techniques for distinguishing corrected text are possible as well; for example as discussed above in the context of FIG. 5A for distinguishing text of different input sources and/or types.
- distinguishing corrected text can be temporary; for example, the corrected text 718 of “Viruses” can appear with bold and underlined font for a predetermined period of time; e.g., 5 or 10 seconds, before the corrected text 718 is shown as not-distinguished text.
- speaker 710 speaks utterance 730 of “Some Orthomyxowirids” which is received at a microphone of device 720 and converted to text and provided to error correction module 430 as “Some Orthomyxowirids.”
- Error correction module 430 can apply input-type, input-source, and/or input-context filters to the input text of “Some Orthomyxowirids”, as discussed above with reference to FIGS. 5B , 5 C, and 5 D.
- error correction module 430 can determine based on input context that the word “Orthomyxowirids” corresponds to the plural of the influenza virus, or Orthomyxoviridae, and correspondingly correct utterance 730 to be “Some Orthomyxoviridae.”
- corrected text 734 of “Orthomyxoviridae” is shown inserted into document 714 and distinguished via use of bold and underlined font.
- FIGS. 8A , 8 B, and 8 C each depict scenarios for corrections utilizing contexts.
- FIG. 8A depicts a scenario 800 of error correction in accordance with an example embodiment.
- Scenario 800 begins with wearer 810 of wearable computing device 812 at movie theater 802 .
- utterance 816 is spoken at movie theater 802 and captured by wearable computing device 812 .
- utterance 816 is “YogeeBear says funny things!”
- Wearable computing device 812 can utilize a location-sensitive context to correct utterance 816 .
- wearable-computing device 812 can determine a location of wearable computing device 812 utilizing location sensor 434 , and further determine that the location of wearable computing device 812 is within university 802 .
- the location-sensitive context can include time-sensitive information, such as schedules of movies being played at movie theater 802 .
- wearable computing device 812 can determine one or more activities associated with the building(s) as part of the location-sensitive context. For example, activities associated with movie theater 802 can include watching and discussing movies, listening to and discussing movie-related music, talking about activities of other patrons, and enjoying popcorn and other confections.
- activities associated with movie theater 802 can include watching and discussing movies, listening to and discussing movie-related music, talking about activities of other patrons, and enjoying popcorn and other confections.
- wearable computing device 812 can determine that utterance 816 is likely associated with a movie.
- wearable computing device can determine that the movie “Yogi Bear” and/or cartoons with the Yogi Bear character are playing at the time utterance 816 is captured.
- wearable computing device 812 can determine one or more speech patterns related to academic exercises, classes, etc., that can indicate the word “YogeeBear” that is part of utterance 816 is likely related to “Yogi Bear”.
- lens/display 814 of wearable computing device 812 is shown displaying corrected text 818 for utterance 816 .
- FIG. 8A shows that corrected text 816 has corrected “YogeeBear” to be “Yogi Bear”, so that corrected text 816 reads “Yogi Bear says funny things!”
- Scenario 800 continues with wearer 810 of wearable computing device 812 traveling to sports bar 822 .
- utterance 826 is received at wearable computing device 812 .
- utterance 826 is “YogeeBeara says funny things.”
- wearable computing device 812 can determine a location of wearable computing device 812 utilizing location sensor 434 , as discussed above. Then, wearable computing device 812 can determine the location is a location of a sports bar building; e.g., sports bar 822 , and determine activities associated with the building. For example, wearable computing device 812 can determine that sports, such as football, baseball, basketball, soccer, hockey, and drinking are activities associated with a sports bar such as sports bar 822 .
- wearable computing device 812 can determine corrections to utterance 826 using a location-sensitive context that are based on both the location (e.g., sports bar 822 ) and activities associated with the location (e.g., various sports, drinking, etc.). For example, in the context of a sports bar and the activity of “baseball”, wearable computing device 812 can determine one or more baseball-related speech patterns. For example, a baseball-related speech pattern can indicate the word “YogeeBeara” that is part of utterance 816 are likely related to “Yogi Berra,” who is a Hall of Frame baseball player known for colorful sayings.
- wearable computing device 812 can generate corrected text 828 of utterance 826 using the location-sensitive context and baseball-related speech patterns.
- FIG. 8A shows corrected text 828 as “Yogi Berra says funny things!”
- wearable computing device 812 can be at a university and capture an utterance of “LBJ wuz a pivotal figure of the 60s.”
- wearable computing device 812 can determine one or more activities with the university as part of the location-sensitive context.
- one activity associated with university 802 could be United States history, among others.
- Wearable computing device 812 can determine one or more speech patterns related to United States history can indicate the initials “LBJ” are likely related to “Lyndon B. Johnson”, who was the 36 th President of the United States. Indeed, this utterance provides context in by saying LBJ was “a pivotal figure of the 60s”, as Lyndon B. Johnson was U.S. President from 1963 to 1969.
- corrected text based on this utterance examples include: “LBJ was a pivotal figure of the 60s,” with spelling corrections only, “Lyndon B. Johnson was a pivotal figure of the 60s” with the expansion of the initials “LBJ”, and “Lyndon B. Johnson was a pivotal figure of the 1960s” with the expansion of the term “60s.”
- this corrected text can be added to notes and/or other information about the activity at the university; e.g., a U.S. History lecture.
- user controls can be provided to instruct wearable computing device 812 whether or not verbatim text capture is to be performed, initials and/or other terms are to be expanded, spelling and/or grammar corrections are to be applied, slang/jargon is to be converted to slang/jargon-free language, and other controls as well.
- an utterance of “LBJ just posterized #38 fer an and one” is captured.
- wearable computing device 812 can determine that basketball is associated with sports bar 822 , and perhaps determine time-sensitive information such as a basketball game is being played.
- a basketball-related speech pattern can indicate the initials “LBJ” that are part of the utterance are likely related to “LeBron James”, the slang word “posterize” is likely related to dunking a basketball over another player, and the slang phrase “and one” relates to getting fouled as part of a successful shot and getting a free throw for the foul.
- examples of corrected text of this utterance include “LBJ just posterized #38 for an and one,” “LeBron James just posterized #38 for an and one”, and “LeBron James just scored over #38 and was fouled during the score.”
- industry-specific jargon is commonly used in the work place.
- a wearable computing device can access information, such as industry-specific dictionaries and/or databases, to suggest corrections based on industry-specific jargon when located at a work place and/or when proximate to one or more identified co-workers.
- Such access to industry-specific information can be useful to employees learning work-related jargon while breaking into a new industry and/or to ensure jargon is used correctly.
- Many other examples are possible as well.
- FIG. 8B shows an example scenario 830 for error correction in accordance with an example embodiment.
- speaker 832 associated with wearable computing device 834 makes utterance 838 of “Wazzup, Dawg?”
- speaker 842 makes utterance 844 of “Chillin' man.”
- wearable computing device 834 can identify a speaker associated with utterance 838 (e.g., speaker 832 ) using the techniques for verifying speech discussed above. Upon determining that speaker 832 is associated with utterance 838 , wearable computing device 834 can generate a context that includes information about speaker 832 , including speech patterns for speaker 832 , related documents, information about location(s) associated with wearable computing device 834 , and additional information.
- a speaker associated with utterance 838 e.g., speaker 832
- wearable computing device 834 can generate a context that includes information about speaker 832 , including speech patterns for speaker 832 , related documents, information about location(s) associated with wearable computing device 834 , and additional information.
- wearable computing device 834 can determine that another speaker (e.g., speaker 842 ) is within range of wearable computing device 834 .
- a location-sensitive context can be used to account for both speakers 832 and 842 .
- a location-sensitive context can indicate that speakers 832 and 842 are within a threshold distance, or proximity, of wearable computing device 834 .
- wearable computing device 834 can generate respective corrected text segments for each utterance.
- the corrected text segments can capture the text verbatim (modulo some spelling and perhaps grammatical corrections) or can apply corrections on a per-speaker basis.
- corrected text segments 846 a are shown on lens/display 840 of wearable computing device 834 .
- Corrected text segments 846 a show text as captured verbatim without specific speaker identification.
- corrected text segments 846 a include a first segment “S 1 : Wazzup, Dawg?” and a second segment “S 2 : Chillin', man.”
- the labels S 1 and S 2 in corrected text segments 846 a indicate speaker 834 as unidentified speaker 1 and speaker 842 as unidentified speaker 2 , respectively. That is, corrected text segments 846 a show that the speaker of the first segment differs from the speaker of the second text segment, but neither speaker is specifically identified.
- corrected text segments 846 b are shown on lens/display 840 of wearable computing device 834 .
- Corrected text segments 846 b show text with speaker identification and taking into account speaker-specific speech patterns.
- corrected text segments 846 b include a first segment “Bill: How are you doing, Joe?” and a second segment “Joe: Not bad, Bill.”
- the labels S 1 and S 2 in corrected text segments 846 a have been replaced by identification information for each speaker; e.g., identifying speaker 834 as “Bill” and speaker 842 as “Joe.”
- replacement of identification information during text correction can be controlled as part of the user controls mentioned above in the context of FIG. 8A .
- wearable computing device 834 can correct text based on speaker-specific speech patterns. For example, suppose that Bill (speaker 834 ) always says “How you doin', Dawg?” when he greets Joe (speaker 842 ) and that Joe always makes a brief statement of condition (e.g., OK, Not Bad, Not my best, Chillin') in response. Thus, wearable computing device 834 can infer or otherwise determine, such as by performing searches of data related to speech patterns, that “How you doin', Dawg” can be corrected to be “How are you doing?” or “How are you doing, Joe” once utterance 844 has been identified to be associated with Joe.
- condition e.g., OK, Not Bad, Not my best, Chillin'
- wearable computing device 834 can infer or otherwise determine that “Chillin' man” can be corrected to be “Not bad, Bill.”
- FIG. 8B shows that second segment of corrected text segments 846 b with an underline font to clearly indicate that corrected text segment 846 b includes text from two separate speakers.
- other information such as color, other fonts, font sizes, spacing, etc. can be used instead or as well as an underlined font to show indicate text from multiple speakers.
- Speaker-specific speech patterns can be based on personal conversation histories. For example, suppose that wearer Ace of a wearable computing device, such as system 400 , regularly converses with Bull, Cam, Dominic, and Eve and that Ace's conversations with Bull commonly involve the U.S. Civil War, Ace's conversations with Cam commonly involve photography, Ace's conversations with Dominic involve comparative religion, and Ace's conversations with Eve involve music.
- system 400 can maintain a history of terminology used as part of a personal conversation history between Ace and each of Ace's conversational partners. Then, upon hearing the utterance “cannon”, system 400 can generate the appropriate corrected text. In these examples, for a conversation about the U.S. Civil War with Bull, the corrected text is likely to be “cannon” (a type of artillery piece used during the U.S.
- the corrected text is likely to be “Canon” (a manufacturer of cameras), for a conversation about comparative religion with Dominic, the corrected text is likely to be “canon” (the laws for the Roman Catholic Church) or “Kannon” (one name for a Buddhist bodhisattva), and for a conversation about music with Eve, the corrected text is likely to be “canon” (a composition with imitations of a melody each played after a given duration).
- Canon a manufacturer of cameras
- the corrected text is likely to be “canon” (the laws for the Roman Catholic Church) or “Kannon” (one name for a Buddhist bodhisattva)
- the corrected text is likely to be “canon” (a composition with imitations of a melody each played after a given duration).
- Other examples of personal conversational histories are possible as well.
- speaker 832 is accompanied by a dog, Rover, as shown in FIG. 8B .
- Scenario 800 shows speaker 830 composing a message using a speech interface to wearable computing device 834 .
- speaker 834 emits utterance 850 of “Howz the dawg doin'?”
- wearable computing device 834 can generate respective corrected text segments for each utterance. As indicated at 830 E 1 of FIG. 8B , corrected text segment 852 a for utterance 850 is “How is the dog doing?” which includes spelling corrections for the words “Howz” and “dawg”.
- wearable-computing device 834 can use determine that another entity, a dog named Rover, is proximate to speaker 832 .
- Rover may have an identifying chip, or speaker 832 may have indicated that Rover is proximate by a previous utterance (e.g., “Come here, Rover.”)
- wearable-computing device 834 can maintain a location-sensitive context that includes the information that one dog, Rover, is proximate to speaker 832 .
- wearable computing device 834 can utilize social-networking and/or other network-available information (e.g., general and/or specialized search engines) to identify proximate entities. For example, wearable computing device 834 can identify an image of Rover as an image of a dog and perform searches of social-networking and/or other network-available information about speaker 832 and dogs. In this example, wearable computing device 834 can determine that speaker 832 is associated with a dog named Rover.
- network-available information e.g., general and/or specialized search engines
- Wearable computing device 834 can generate corrected text segment 852 b based on this location-sensitive context and speech patterns of speaker 832 . As shown at 830 E 2 of FIG. 8B , display/lens 840 of wearable computing device 834 displays corrected text segment 852 b of “How is Rover doing?”
- FIG. 8C shows an example scenario 860 for error correction in accordance with an example embodiment.
- the date is identified as December 21, and speaker 832 , associated with wearable computing device 834 , is proximate to a dog, Rover and a nephew, Alex.
- speaker 832 emits utterance 862 of “My dawg and nefew here should get toys from Sanda Claws.”
- Wearable computing device 834 can generate a corrected text segment based on utterance 862 that take timing information, such as the day of the year, into account. For example, as the day of the year at 860 A is December 21 which is four days before Christmas, wearable computing device 834 can determine that text has a higher probability of Christmas-related words than at other times of the year. Then, as part of a context, wearable computing device 834 can include a dictionary of seasonal or holiday-related words, such as “Christmas”, “Santa Claus”, “gifts”, etc.
- wearable computing device 834 can determine the input that the text of “Sanda Claws” on December 21 is likely to be “Santa Claus.” Also, wearable computing device 834 can correct misspellings like “dawg” and “nefew.” Accordingly, at 860 B 1 of FIG. 8A , wearable computing device 834 is shown as having generated corrected text segment 864 of “My dog and nephew here should get toys from Santa Claus.”
- wearable computing device 834 can take proximate entities into account as well, such as Rover the dog and Alex the nephew. After taking proximate entities into account at 860 B 2 of FIG. 8A , wearable computing device 834 is shown as having generated corrected text segment 866 of “Rover and Alex should get toys from Santa Claus.”
- the date is identified as July 21, and speaker 832 , associated with wearable computing device 834 , is proximate to a dog, Rover.
- speaker 832 emits utterance 872 of “Rover gets sanda claws at the beach.”
- Scenario 860 has a date of July 21, which is during summer in the Northern Hemisphere.
- wearable computing device 834 can determine that a location of wearable computing device 834 is within the Northern Hemisphere and that the season is summer. Taking the day of the year at 860 C of July 21 into account, wearable computing device 834 can determine that text has a higher probability of summer-related words than at other times of the year. Then, as part of a context, wearable computing device 834 can include a dictionary of seasonal words, such as “sun,” “hiking,” “swimming,” “sand,” “beach,” “sun tan,” “boating,” etc.
- wearable computing device 834 can determine the input that the text of “sanda claws” on July 21 is likely to be “sandy claws” (or perhaps “sand in his claws”) and accordingly generate corrected text segment 874 .
- display/lens 840 of wearable computing device 834 displays corrected text segment 874 of “Rover gets sandy claws at the beach.”
- information about current topics, news, or trends can be used in a context to correct text segments. For example, suppose a popular new singer, Singer YourA, was recently involved in an accident. Then, a wearable computing device can use public search information, such as trending searches from popular web sites, public electronic communications (e.g., tweets, blog entries, comments), news searches, and additional information to correct input text of “Did you know singer your A was in an accident?” to be “Did you know Singer YourA was in an accident?”
- public search information such as trending searches from popular web sites, public electronic communications (e.g., tweets, blog entries, comments), news searches, and additional information to correct input text of “Did you know singer your A was in an accident?” to be “Did you know Singer YourA was in an accident?”
- FIG. 9 is a flow chart of an example method 900 in accordance with an example embodiment.
- inputs having a plurality of input types can be received at a wearable computing device.
- the plurality of input types include speech, text entry, handwriting, and optical character recognition (OCR).
- OCR optical character recognition
- a text string corresponding to the inputs can be generated using the wearable computing device.
- the text string can include a plurality of segments, where each segment is associated with an input type of the plurality of input types. Generating text strings related to inputs is described above with reference to at least FIGS. 4-8C .
- the text string can be displayed using the wearable computing device. Displaying text strings is described above with reference to at least FIGS. 2 and 4 - 8 C.
- displaying the text string includes: (a) associating a first color with a first input type of the plurality of input types, (b) associating a second color with a second input type of the plurality of input types, where the first and second input types differ, and where the first and second colors differ, (c) displaying each segment of the text string associated with the first input type using the first color; and (d) displaying each segment of the text string associated with the second input type using the second color. Displaying text strings distinguished by input type and/or source is described above with reference to at least FIG. 5A .
- one or more corrected segments can be generated by applying an error-correction filter to the given segment using the wearable computing device.
- the error-correction filter can be configured to correct errors based on an input type associated with the given segment. Correcting errors using error-correction filters is described above with reference to at least FIGS. 4-8C .
- the error-correction filter can be further configured to correct errors based on a given input source.
- the input type associated with the segment is speech
- the given input source is a speaker of the speech. Correcting errors based on given input types and sources is described above with reference to at least FIGS. 4-7 .
- the error-correction filter can be further configured to correct errors based on a location-sensitive context associated with the given segment. Correcting errors based on a location-sensitive context is described above with reference to at least FIGS. 8A-8C .
- the location-sensitive context can include information about one or more entities proximate to a location of the wearable computing device, such as discussed above in more detail with respect to at least FIGS. 8B and 8C .
- the one or more entities are people.
- the information about the one or more entities comprises information about one or more speech patterns of the people, such as discussed above in more detail with respect to at least FIG. 8B .
- the information about the one or more entities includes identification information about the one or more entities, such as discussed above in more detail with respect to at least FIGS. 8B and 8C .
- the location-sensitive context can include information about one or more speech patterns associated with a location of the wearable computing device, such as discussed above in the context of FIG. 8A .
- the location of the wearable computing device can be associated with a building, and the one or more speech patterns associated with the location can include one or more speech patterns associated with an activity associated with the building, such as discussed above in the context of FIG. 8A .
- the location-sensitive context can be associated with one or more documents.
- displaying the least the one or more corrected segments can include: identifying a document of the one or more documents associated with a corrected segment of the one or more corrected segments, and updating the corrected segment based on information in the identified document. Contexts that include documents are discussed above in the context of at least FIGS. 5A-7 .
- the location-sensitive context can include information about current topics, news, and trends, such as discussed above in more detail with respect to at least FIG. 8C .
- At block 950 at least the one or more corrected segments can be displayed using the wearable computing device. Displaying corrected segments is described above with reference to at least FIGS. 5A-8C .
- displaying the corrected segments includes displaying corrected segments associated with a first input type using the first color. Displaying distinguished corrected segments is described above with reference to at least FIGS. 5A and 7 .
- displaying the corrected segments includes: identifying a person of the one or more entities associated with a corrected segment of the one or more corrected segments and updating the corrected segment based on the speech pattern of the identified person, such as discussed above in more detail in the context of at least FIG. 8B .
- displaying the corrected segments includes: identifying an entity of the one or more entities associated with a corrected segment of the one or more corrected segments; and updating the corrected segment based on the identification information about the entity, such as discussed above in more detail in the context of at least FIG. 8B .
- updating the corrected segment based on the identification information about the entity includes inserting at least part of the identification information into the corrected segment, such as discussed above in more detail in the context of at least FIG. 8B .
- displaying the corrected segments includes: identifying a location associated with a corrected segment of the one or more corrected segment and updating the corrected segment based on a speech pattern of the one or more speech patterns associated with the location.
- the location-sensitive context comprises a phrase associated with the time of the year.
- displaying the least the one or more corrected segments can include: (a) determining that the time of the year is associated with a corrected segment of the one or more corrected segments, (b) determining that the phrase associated with the time of the year is also associated with the corrected segment, and (c) updating the corrected segment based on the phrase associated with the time of the year.
- a corrected segment of the one or more corrected segments can be selected using the wearable computing device.
- a manual correction technique can be used for selecting a corrected segment. Selecting corrected segments is discussed above with reference to at least FIGS. 4-5C .
- a corrected text string comprising the selected corrected segment can be displayed using the wearable computing device. Displaying corrected text strings is discussed above with reference to at least FIGS. 4-8C .
- each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments.
- Alternative embodiments are included within the scope of these example embodiments.
- functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved.
- more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- a block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique.
- a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data).
- the program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique.
- the program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- the computer readable medium may also include non-transitory computer readable media such as computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM).
- the computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- a computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
Abstract
Description
Claims (19)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/177,821 US8209183B1 (en) | 2011-07-07 | 2011-07-07 | Systems and methods for correction of text from different input types, sources, and contexts |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/177,821 US8209183B1 (en) | 2011-07-07 | 2011-07-07 | Systems and methods for correction of text from different input types, sources, and contexts |
Publications (1)
Publication Number | Publication Date |
---|---|
US8209183B1 true US8209183B1 (en) | 2012-06-26 |
Family
ID=46272959
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/177,821 Expired - Fee Related US8209183B1 (en) | 2011-07-07 | 2011-07-07 | Systems and methods for correction of text from different input types, sources, and contexts |
Country Status (1)
Country | Link |
---|---|
US (1) | US8209183B1 (en) |
Cited By (204)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120054284A1 (en) * | 2010-08-25 | 2012-03-01 | International Business Machines Corporation | Communication management method and system |
US20130044130A1 (en) * | 2011-08-17 | 2013-02-21 | Kevin A. Geisner | Providing contextual personal information by a mixed reality device |
US20130191739A1 (en) * | 2012-01-25 | 2013-07-25 | International Business Machines Corporation | Intelligent automatic expansion/contraction of abbreviations in text-based electronic communications |
US20130289976A1 (en) * | 2012-04-30 | 2013-10-31 | Research In Motion Limited | Methods and systems for a locally and temporally adaptive text prediction |
US20140081633A1 (en) * | 2012-09-19 | 2014-03-20 | Apple Inc. | Voice-Based Media Searching |
US20140152696A1 (en) * | 2012-12-05 | 2014-06-05 | Lg Electronics Inc. | Glass type mobile terminal |
WO2014116323A1 (en) * | 2013-01-23 | 2014-07-31 | Syntellia, Inc. | User interface for text input |
US20140244260A1 (en) * | 2006-05-18 | 2014-08-28 | Nuance Communications, Inc. | Method and apparatus for recognizing and reacting to user personality in accordance with speech recognition system |
US20140314216A1 (en) * | 2013-04-22 | 2014-10-23 | Ge Aviation Systems Limited | Unknown speaker identification system |
US20150121285A1 (en) * | 2013-10-24 | 2015-04-30 | Fleksy, Inc. | User interface for text input and virtual keyboard manipulation |
US9213405B2 (en) | 2010-12-16 | 2015-12-15 | Microsoft Technology Licensing, Llc | Comprehension and intent-based content for augmented reality displays |
US20160035351A1 (en) * | 2014-07-31 | 2016-02-04 | Seiko Epson Corporation | Display device, method of controlling display device, and program |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US20160078019A1 (en) * | 2014-09-12 | 2016-03-17 | Microsoft Corporation | Actions on digital document elements from voice |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
GB2533842A (en) * | 2014-10-20 | 2016-07-06 | Lenovo Singapore Pte Ltd | Text correction based on context |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US9535906B2 (en) | 2008-07-31 | 2017-01-03 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US9536350B2 (en) | 2011-08-24 | 2017-01-03 | Microsoft Technology Licensing, Llc | Touch and social cues as inputs into a computer |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
US9620104B2 (en) | 2013-06-07 | 2017-04-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9626955B2 (en) | 2008-04-05 | 2017-04-18 | Apple Inc. | Intelligent text-to-speech conversion |
US9633660B2 (en) | 2010-02-25 | 2017-04-25 | Apple Inc. | User profiling for voice input processing |
US9633674B2 (en) | 2013-06-07 | 2017-04-25 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US9646614B2 (en) | 2000-03-16 | 2017-05-09 | Apple Inc. | Fast, language-independent method for user authentication by voice |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US9672202B2 (en) | 2014-03-20 | 2017-06-06 | Microsoft Technology Licensing, Llc | Context-aware re-formating of an input |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US9760559B2 (en) | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US9798393B2 (en) | 2011-08-29 | 2017-10-24 | Apple Inc. | Text correction processing |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US20170339091A1 (en) * | 2016-05-20 | 2017-11-23 | International Business Machines Corporation | Cognitive communication assistant to bridge incompatible audience |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US9860200B1 (en) * | 2014-08-27 | 2018-01-02 | Google Llc | Message suggestions |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9946699B1 (en) * | 2012-08-29 | 2018-04-17 | Intuit Inc. | Location-based speech recognition for preparation of electronic tax return |
US9953088B2 (en) | 2012-05-14 | 2018-04-24 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9966068B2 (en) | 2013-06-08 | 2018-05-08 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US10019962B2 (en) | 2011-08-17 | 2018-07-10 | Microsoft Technology Licensing, Llc | Context adaptive user interface for augmented reality display |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US10089072B2 (en) | 2016-06-11 | 2018-10-02 | Apple Inc. | Intelligent device arbitration and control |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US10169329B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Exemplar-based natural language processing |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10185542B2 (en) | 2013-06-09 | 2019-01-22 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10255907B2 (en) | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
US10269345B2 (en) | 2016-06-11 | 2019-04-23 | Apple Inc. | Intelligent task discovery |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10283110B2 (en) | 2009-07-02 | 2019-05-07 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US10297253B2 (en) | 2016-06-11 | 2019-05-21 | Apple Inc. | Application integration with a digital assistant |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10318871B2 (en) | 2005-09-08 | 2019-06-11 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10354011B2 (en) | 2016-06-09 | 2019-07-16 | Apple Inc. | Intelligent automated assistant in a home environment |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10521466B2 (en) | 2016-06-11 | 2019-12-31 | Apple Inc. | Data driven natural language event detection and classification |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US10568032B2 (en) | 2007-04-03 | 2020-02-18 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10607140B2 (en) | 2010-01-25 | 2020-03-31 | Newvaluexchange Ltd. | Apparatuses, methods and systems for a digital conversation management platform |
US20200117268A1 (en) * | 2017-04-27 | 2020-04-16 | Siemens Aktiengesellschaft | Authoring augmented reality experiences using augmented reality and virtual reality |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10970482B2 (en) | 2015-05-28 | 2021-04-06 | Advanced New Technologies Co., Ltd. | Assisted data input |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11002965B2 (en) * | 2018-06-25 | 2021-05-11 | Apple Inc. | System and method for user alerts during an immersive computer-generated reality experience |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11061744B2 (en) * | 2018-06-01 | 2021-07-13 | Apple Inc. | Direct input from a remote device |
US11062704B1 (en) | 2018-12-21 | 2021-07-13 | Cerner Innovation, Inc. | Processing multi-party conversations |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11244679B2 (en) * | 2017-02-14 | 2022-02-08 | Samsung Electronics Co., Ltd. | Electronic device, and message data output method of electronic device |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11275757B2 (en) | 2015-02-13 | 2022-03-15 | Cerner Innovation, Inc. | Systems and methods for capturing data, creating billable information and outputting billable information |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11398232B1 (en) | 2018-12-21 | 2022-07-26 | Cerner Innovation, Inc. | Natural language understanding of conversational sources |
US11410650B1 (en) | 2018-12-26 | 2022-08-09 | Cerner Innovation, Inc. | Semantically augmented clinical speech processing |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11422764B1 (en) * | 2018-06-03 | 2022-08-23 | Epic Optix, Inc. | Multi-platform integrated display |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
CN115314757A (en) * | 2022-08-05 | 2022-11-08 | 聚好看科技股份有限公司 | Display device and guide window display method |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11501504B2 (en) | 2018-12-20 | 2022-11-15 | Samsung Electronics Co., Ltd. | Method and apparatus for augmented reality |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11875883B1 (en) | 2018-12-21 | 2024-01-16 | Cerner Innovation, Inc. | De-duplication and contextually-intelligent recommendations based on natural language understanding of conversational sources |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
Citations (40)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6236768B1 (en) | 1997-10-14 | 2001-05-22 | Massachusetts Institute Of Technology | Method and apparatus for automated, context-dependent retrieval of information |
US20020044104A1 (en) | 1999-03-02 | 2002-04-18 | Wolfgang Friedrich | Augmented-reality system for situation-related support of the interaction between a user and an engineering apparatus |
US20020167463A1 (en) | 2001-02-05 | 2002-11-14 | Kazutaka Inoguchi | Optical system and image displaying apparatus using the same |
US20030046401A1 (en) | 2000-10-16 | 2003-03-06 | Abbott Kenneth H. | Dynamically determing appropriate computer user interfaces |
US6731253B1 (en) | 1999-08-05 | 2004-05-04 | Honeywell International Inc. | Ambient adaptable optical combiner |
US20050195129A1 (en) | 1998-08-31 | 2005-09-08 | Semiconductor Energy Laboratory Co., Ltd. | Portable information processing system |
US20060017657A1 (en) | 2004-07-20 | 2006-01-26 | Olympus Corporation | Information display system |
US20060028400A1 (en) | 2004-08-03 | 2006-02-09 | Silverbrook Research Pty Ltd | Head mounted display with wave front modulator |
EP1635328A1 (en) | 2004-09-14 | 2006-03-15 | Swisscom AG | Speech recognition method constrained with a grammar received from a remote system. |
US7058888B1 (en) | 2000-04-25 | 2006-06-06 | Microsoft Corporation | Multi-modal text editing correction |
EP1705554A2 (en) | 2005-03-25 | 2006-09-27 | AT&T Corp. | System and method for dynamically adapting performance of interactive dialog system basd on multi-modal confirmation |
US7149970B1 (en) | 2000-06-23 | 2006-12-12 | Microsoft Corporation | Method and system for filtering and selecting from a candidate list generated by a stochastic input method |
US20060282575A1 (en) | 2005-04-22 | 2006-12-14 | Microsoft Corporation | Auto-suggest lists and handwritten input |
US7164753B2 (en) | 1999-04-08 | 2007-01-16 | Ultratec, Incl | Real-time transcription correction system |
US7200555B1 (en) | 2000-07-05 | 2007-04-03 | International Business Machines Corporation | Speech recognition correction for devices having limited or no display |
US20070277103A1 (en) | 2003-09-18 | 2007-11-29 | Wolfgang Theimer | Method, device, and input element for selecting the functional mode thereof |
US20070296646A1 (en) | 2006-06-27 | 2007-12-27 | Kakuya Yamamoto | Display apparatus and control method thereof |
US7319957B2 (en) | 2004-02-11 | 2008-01-15 | Tegic Communications, Inc. | Handwriting and voice input with automatic correction |
US20080024391A1 (en) | 2006-07-31 | 2008-01-31 | Manuel Oliver | Image alignment method for binocular eyewear displays |
US20080030429A1 (en) | 2006-08-07 | 2008-02-07 | International Business Machines Corporation | System and method of enhanced virtual reality |
US20080084362A1 (en) | 2002-08-12 | 2008-04-10 | Scalar Corporation | Image display device |
US7372429B1 (en) | 2004-02-27 | 2008-05-13 | Atr Mission Concepts | Information module assembly and method of controlling access to different levels of information |
US7380203B2 (en) | 2002-05-14 | 2008-05-27 | Microsoft Corporation | Natural input recognition tool |
US20080122736A1 (en) | 1993-10-22 | 2008-05-29 | Kopin Corporation | Portable communication display device |
US20080198097A1 (en) | 2007-02-20 | 2008-08-21 | Canon Kabushiki Kaisha | Head-mounted display and head-mounted video display |
US20080270128A1 (en) | 2005-11-07 | 2008-10-30 | Electronics And Telecommunications Research Institute | Text Input System and Method Based on Voice Recognition |
US20090018830A1 (en) | 2007-07-11 | 2009-01-15 | Vandinburg Gmbh | Speech control of computing devices |
US20090144056A1 (en) * | 2007-11-29 | 2009-06-04 | Netta Aizenbud-Reshef | Method and computer program product for generating recognition error correction information |
US7587308B2 (en) | 2005-11-21 | 2009-09-08 | Hewlett-Packard Development Company, L.P. | Word recognition using ontologies |
US20100023320A1 (en) | 2005-08-10 | 2010-01-28 | Voicebox Technologies, Inc. | System and method of supporting adaptive misrecognition in conversational speech |
US20100041000A1 (en) | 2006-03-15 | 2010-02-18 | Glass Andrew B | System and Method for Controlling the Presentation of Material and Operation of External Devices |
US7668718B2 (en) | 2001-07-17 | 2010-02-23 | Custom Speech Usa, Inc. | Synchronized pattern recognition source data processed by manual or automatic means for creation of shared speaker-dependent speech user profile |
US20100309097A1 (en) | 2009-06-04 | 2010-12-09 | Roni Raviv | Head mounted 3d display |
EP2264563A1 (en) | 2009-06-19 | 2010-12-22 | Tegic Communications, Inc. | Virtual keyboard system with automatic correction |
US7881936B2 (en) | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
US20110034176A1 (en) | 2009-05-01 | 2011-02-10 | Lord John D | Methods and Systems for Content Processing |
US7899674B1 (en) * | 2006-08-11 | 2011-03-01 | The United States Of America As Represented By The Secretary Of The Navy | GUI for the semantic normalization of natural language |
US20110098029A1 (en) | 2009-10-28 | 2011-04-28 | Rhoads Geoffrey B | Sensor-based mobile search, related methods and systems |
US20110143811A1 (en) | 2009-08-17 | 2011-06-16 | Rodriguez Tony F | Methods and Systems for Content Processing |
US20120050144A1 (en) | 2010-08-26 | 2012-03-01 | Clayton Richard Morlock | Wearable augmented reality computing apparatus |
-
2011
- 2011-07-07 US US13/177,821 patent/US8209183B1/en not_active Expired - Fee Related
Patent Citations (41)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080122736A1 (en) | 1993-10-22 | 2008-05-29 | Kopin Corporation | Portable communication display device |
US6236768B1 (en) | 1997-10-14 | 2001-05-22 | Massachusetts Institute Of Technology | Method and apparatus for automated, context-dependent retrieval of information |
US20050195129A1 (en) | 1998-08-31 | 2005-09-08 | Semiconductor Energy Laboratory Co., Ltd. | Portable information processing system |
US7881936B2 (en) | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
US20020044104A1 (en) | 1999-03-02 | 2002-04-18 | Wolfgang Friedrich | Augmented-reality system for situation-related support of the interaction between a user and an engineering apparatus |
US7164753B2 (en) | 1999-04-08 | 2007-01-16 | Ultratec, Incl | Real-time transcription correction system |
US6731253B1 (en) | 1999-08-05 | 2004-05-04 | Honeywell International Inc. | Ambient adaptable optical combiner |
US7058888B1 (en) | 2000-04-25 | 2006-06-06 | Microsoft Corporation | Multi-modal text editing correction |
US7149970B1 (en) | 2000-06-23 | 2006-12-12 | Microsoft Corporation | Method and system for filtering and selecting from a candidate list generated by a stochastic input method |
US7200555B1 (en) | 2000-07-05 | 2007-04-03 | International Business Machines Corporation | Speech recognition correction for devices having limited or no display |
US20030046401A1 (en) | 2000-10-16 | 2003-03-06 | Abbott Kenneth H. | Dynamically determing appropriate computer user interfaces |
US20020167463A1 (en) | 2001-02-05 | 2002-11-14 | Kazutaka Inoguchi | Optical system and image displaying apparatus using the same |
US7668718B2 (en) | 2001-07-17 | 2010-02-23 | Custom Speech Usa, Inc. | Synchronized pattern recognition source data processed by manual or automatic means for creation of shared speaker-dependent speech user profile |
US7380203B2 (en) | 2002-05-14 | 2008-05-27 | Microsoft Corporation | Natural input recognition tool |
US20080084362A1 (en) | 2002-08-12 | 2008-04-10 | Scalar Corporation | Image display device |
US20070277103A1 (en) | 2003-09-18 | 2007-11-29 | Wolfgang Theimer | Method, device, and input element for selecting the functional mode thereof |
US7319957B2 (en) | 2004-02-11 | 2008-01-15 | Tegic Communications, Inc. | Handwriting and voice input with automatic correction |
US7372429B1 (en) | 2004-02-27 | 2008-05-13 | Atr Mission Concepts | Information module assembly and method of controlling access to different levels of information |
US20060017657A1 (en) | 2004-07-20 | 2006-01-26 | Olympus Corporation | Information display system |
US20060028400A1 (en) | 2004-08-03 | 2006-02-09 | Silverbrook Research Pty Ltd | Head mounted display with wave front modulator |
EP1635328A1 (en) | 2004-09-14 | 2006-03-15 | Swisscom AG | Speech recognition method constrained with a grammar received from a remote system. |
EP1705554A2 (en) | 2005-03-25 | 2006-09-27 | AT&T Corp. | System and method for dynamically adapting performance of interactive dialog system basd on multi-modal confirmation |
US20060282575A1 (en) | 2005-04-22 | 2006-12-14 | Microsoft Corporation | Auto-suggest lists and handwritten input |
US20100023320A1 (en) | 2005-08-10 | 2010-01-28 | Voicebox Technologies, Inc. | System and method of supporting adaptive misrecognition in conversational speech |
US20080270128A1 (en) | 2005-11-07 | 2008-10-30 | Electronics And Telecommunications Research Institute | Text Input System and Method Based on Voice Recognition |
US7587308B2 (en) | 2005-11-21 | 2009-09-08 | Hewlett-Packard Development Company, L.P. | Word recognition using ontologies |
US20100041000A1 (en) | 2006-03-15 | 2010-02-18 | Glass Andrew B | System and Method for Controlling the Presentation of Material and Operation of External Devices |
US20070296646A1 (en) | 2006-06-27 | 2007-12-27 | Kakuya Yamamoto | Display apparatus and control method thereof |
US7928926B2 (en) | 2006-06-27 | 2011-04-19 | Panasonic Corporation | Display apparatus and method for hands free operation that selects a function when window is within field of view |
US20080024391A1 (en) | 2006-07-31 | 2008-01-31 | Manuel Oliver | Image alignment method for binocular eyewear displays |
US20080030429A1 (en) | 2006-08-07 | 2008-02-07 | International Business Machines Corporation | System and method of enhanced virtual reality |
US7899674B1 (en) * | 2006-08-11 | 2011-03-01 | The United States Of America As Represented By The Secretary Of The Navy | GUI for the semantic normalization of natural language |
US20080198097A1 (en) | 2007-02-20 | 2008-08-21 | Canon Kabushiki Kaisha | Head-mounted display and head-mounted video display |
US20090018830A1 (en) | 2007-07-11 | 2009-01-15 | Vandinburg Gmbh | Speech control of computing devices |
US20090144056A1 (en) * | 2007-11-29 | 2009-06-04 | Netta Aizenbud-Reshef | Method and computer program product for generating recognition error correction information |
US20110034176A1 (en) | 2009-05-01 | 2011-02-10 | Lord John D | Methods and Systems for Content Processing |
US20100309097A1 (en) | 2009-06-04 | 2010-12-09 | Roni Raviv | Head mounted 3d display |
EP2264563A1 (en) | 2009-06-19 | 2010-12-22 | Tegic Communications, Inc. | Virtual keyboard system with automatic correction |
US20110143811A1 (en) | 2009-08-17 | 2011-06-16 | Rodriguez Tony F | Methods and Systems for Content Processing |
US20110098029A1 (en) | 2009-10-28 | 2011-04-28 | Rhoads Geoffrey B | Sensor-based mobile search, related methods and systems |
US20120050144A1 (en) | 2010-08-26 | 2012-03-01 | Clayton Richard Morlock | Wearable augmented reality computing apparatus |
Non-Patent Citations (4)
Title |
---|
Hollerer, "User Interfaces for Mobile Augmented Reality Systems," PhD Thesis, Columbia University, Dept. of Computer Science, New York, NY, 2004, available at http://www.cs.ucsb.edu/~holl/pubs/hollerer-2004-diss.pdf (last visited Jul. 7, 2011). |
Hollerer, "User Interfaces for Mobile Augmented Reality Systems," PhD Thesis, Columbia University, Dept. of Computer Science, New York, NY, 2004, available at http://www.cs.ucsb.edu/˜holl/pubs/hollerer-2004-diss.pdf (last visited Jul. 7, 2011). |
Suhm, "Empirical Evaluation of Interactive Multimodal Error Correction," Proceedings of 1997 IEEE Workshop on Automatic Speech Recognition and Understanding, Santa Barbara, CA, Dec. 14-17, 1997, available at http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.30.4188&rep=rep1&type=pdf (last visited Jul. 7, 2011). |
Taghva et al. "OCRSpell: an interactive spelling correction system for OCR errors in text," International Journal on Document Analysis and Recognition, 2001, vol. 3, pp. 125-137, available at http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.2509&rep=rep1&type=pdf (last visited Jul. 7, 2011). |
Cited By (351)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9646614B2 (en) | 2000-03-16 | 2017-05-09 | Apple Inc. | Fast, language-independent method for user authentication by voice |
US10318871B2 (en) | 2005-09-08 | 2019-06-11 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US9576571B2 (en) * | 2006-05-18 | 2017-02-21 | Nuance Communications, Inc. | Method and apparatus for recognizing and reacting to user personality in accordance with speech recognition system |
US20140244260A1 (en) * | 2006-05-18 | 2014-08-28 | Nuance Communications, Inc. | Method and apparatus for recognizing and reacting to user personality in accordance with speech recognition system |
US10568032B2 (en) | 2007-04-03 | 2020-02-18 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11012942B2 (en) | 2007-04-03 | 2021-05-18 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US9626955B2 (en) | 2008-04-05 | 2017-04-18 | Apple Inc. | Intelligent text-to-speech conversion |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US9535906B2 (en) | 2008-07-31 | 2017-01-03 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US11900936B2 (en) | 2008-10-02 | 2024-02-13 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10475446B2 (en) | 2009-06-05 | 2019-11-12 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US10795541B2 (en) | 2009-06-05 | 2020-10-06 | Apple Inc. | Intelligent organization of tasks items |
US11080012B2 (en) | 2009-06-05 | 2021-08-03 | Apple Inc. | Interface for a virtual digital assistant |
US10283110B2 (en) | 2009-07-02 | 2019-05-07 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US9548050B2 (en) | 2010-01-18 | 2017-01-17 | Apple Inc. | Intelligent automated assistant |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US10706841B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Task flow identification based on user intent |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US10984326B2 (en) | 2010-01-25 | 2021-04-20 | Newvaluexchange Ltd. | Apparatuses, methods and systems for a digital conversation management platform |
US10607140B2 (en) | 2010-01-25 | 2020-03-31 | Newvaluexchange Ltd. | Apparatuses, methods and systems for a digital conversation management platform |
US11410053B2 (en) | 2010-01-25 | 2022-08-09 | Newvaluexchange Ltd. | Apparatuses, methods and systems for a digital conversation management platform |
US10984327B2 (en) | 2010-01-25 | 2021-04-20 | New Valuexchange Ltd. | Apparatuses, methods and systems for a digital conversation management platform |
US10607141B2 (en) | 2010-01-25 | 2020-03-31 | Newvaluexchange Ltd. | Apparatuses, methods and systems for a digital conversation management platform |
US9633660B2 (en) | 2010-02-25 | 2017-04-25 | Apple Inc. | User profiling for voice input processing |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US20120054284A1 (en) * | 2010-08-25 | 2012-03-01 | International Business Machines Corporation | Communication management method and system |
US9455944B2 (en) | 2010-08-25 | 2016-09-27 | International Business Machines Corporation | Reply email clarification |
US8775530B2 (en) * | 2010-08-25 | 2014-07-08 | International Business Machines Corporation | Communication management method and system |
US9213405B2 (en) | 2010-12-16 | 2015-12-15 | Microsoft Technology Licensing, Llc | Comprehension and intent-based content for augmented reality displays |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US10102359B2 (en) | 2011-03-21 | 2018-10-16 | Apple Inc. | Device access using voice authentication |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US9153195B2 (en) * | 2011-08-17 | 2015-10-06 | Microsoft Technology Licensing, Llc | Providing contextual personal information by a mixed reality device |
US10223832B2 (en) | 2011-08-17 | 2019-03-05 | Microsoft Technology Licensing, Llc | Providing location occupancy analysis via a mixed reality device |
US10019962B2 (en) | 2011-08-17 | 2018-07-10 | Microsoft Technology Licensing, Llc | Context adaptive user interface for augmented reality display |
US20130044130A1 (en) * | 2011-08-17 | 2013-02-21 | Kevin A. Geisner | Providing contextual personal information by a mixed reality device |
US11127210B2 (en) | 2011-08-24 | 2021-09-21 | Microsoft Technology Licensing, Llc | Touch and social cues as inputs into a computer |
US9536350B2 (en) | 2011-08-24 | 2017-01-03 | Microsoft Technology Licensing, Llc | Touch and social cues as inputs into a computer |
US9798393B2 (en) | 2011-08-29 | 2017-10-24 | Apple Inc. | Text correction processing |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US9311286B2 (en) * | 2012-01-25 | 2016-04-12 | International Business Machines Corporation | Intelligent automatic expansion/contraction of abbreviations in text-based electronic communications |
US20130191739A1 (en) * | 2012-01-25 | 2013-07-25 | International Business Machines Corporation | Intelligent automatic expansion/contraction of abbreviations in text-based electronic communications |
US20130191738A1 (en) * | 2012-01-25 | 2013-07-25 | International Business Machines Corporation | Intelligent automatic expansion/contraction of abbreviations in text-based electronic communications |
US9817802B2 (en) * | 2012-01-25 | 2017-11-14 | International Business Machines Corporation | Intelligent automatic expansion/contraction of abbreviations in text-based electronic communications |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US20140257797A1 (en) * | 2012-04-30 | 2014-09-11 | Blackberry Limited | Methods and systems for a locally and temporally adaptive text prediction |
US8756052B2 (en) * | 2012-04-30 | 2014-06-17 | Blackberry Limited | Methods and systems for a locally and temporally adaptive text prediction |
US20130289976A1 (en) * | 2012-04-30 | 2013-10-31 | Research In Motion Limited | Methods and systems for a locally and temporally adaptive text prediction |
US9953088B2 (en) | 2012-05-14 | 2018-04-24 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11321116B2 (en) | 2012-05-15 | 2022-05-03 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US9946699B1 (en) * | 2012-08-29 | 2018-04-17 | Intuit Inc. | Location-based speech recognition for preparation of electronic tax return |
US9547647B2 (en) * | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
US9971774B2 (en) * | 2012-09-19 | 2018-05-15 | Apple Inc. | Voice-based media searching |
US20140081633A1 (en) * | 2012-09-19 | 2014-03-20 | Apple Inc. | Voice-Based Media Searching |
US20170161268A1 (en) * | 2012-09-19 | 2017-06-08 | Apple Inc. | Voice-based media searching |
US20140152696A1 (en) * | 2012-12-05 | 2014-06-05 | Lg Electronics Inc. | Glass type mobile terminal |
US9330313B2 (en) * | 2012-12-05 | 2016-05-03 | Lg Electronics Inc. | Glass type mobile terminal |
WO2014116323A1 (en) * | 2013-01-23 | 2014-07-31 | Syntellia, Inc. | User interface for text input |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US11557310B2 (en) | 2013-02-07 | 2023-01-17 | Apple Inc. | Voice trigger for a digital assistant |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US11636869B2 (en) | 2013-02-07 | 2023-04-25 | Apple Inc. | Voice trigger for a digital assistant |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US9083808B2 (en) * | 2013-04-22 | 2015-07-14 | Ge Aviation Systems Limited | Unknown speaker identification system |
US20140314216A1 (en) * | 2013-04-22 | 2014-10-23 | Ge Aviation Systems Limited | Unknown speaker identification system |
US9633674B2 (en) | 2013-06-07 | 2017-04-25 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9620104B2 (en) | 2013-06-07 | 2017-04-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US9966068B2 (en) | 2013-06-08 | 2018-05-08 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10185542B2 (en) | 2013-06-09 | 2019-01-22 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US20150121285A1 (en) * | 2013-10-24 | 2015-04-30 | Fleksy, Inc. | User interface for text input and virtual keyboard manipulation |
US9176668B2 (en) * | 2013-10-24 | 2015-11-03 | Fleksy, Inc. | User interface for text input and virtual keyboard manipulation |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US9672202B2 (en) | 2014-03-20 | 2017-06-06 | Microsoft Technology Licensing, Llc | Context-aware re-formating of an input |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US11670289B2 (en) | 2014-05-30 | 2023-06-06 | Apple Inc. | Multi-command single utterance input method |
US9760559B2 (en) | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US10714095B2 (en) | 2014-05-30 | 2020-07-14 | Apple Inc. | Intelligent assistant for home automation |
US11699448B2 (en) | 2014-05-30 | 2023-07-11 | Apple Inc. | Intelligent assistant for home automation |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US10169329B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Exemplar-based natural language processing |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US11810562B2 (en) | 2014-05-30 | 2023-11-07 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US10497365B2 (en) | 2014-05-30 | 2019-12-03 | Apple Inc. | Multi-command single utterance input method |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9668024B2 (en) | 2014-06-30 | 2017-05-30 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10904611B2 (en) | 2014-06-30 | 2021-01-26 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US20160035351A1 (en) * | 2014-07-31 | 2016-02-04 | Seiko Epson Corporation | Display device, method of controlling display device, and program |
US9972319B2 (en) * | 2014-07-31 | 2018-05-15 | Seiko Epson Corporation | Display device, method of controlling display device, and program having display of voice and other data |
US20180109476A1 (en) * | 2014-08-27 | 2018-04-19 | Google Llc | Message Suggestions |
US9860200B1 (en) * | 2014-08-27 | 2018-01-02 | Google Llc | Message suggestions |
US11252114B2 (en) * | 2014-08-27 | 2022-02-15 | Google Llc | Message suggestions |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
US20160078019A1 (en) * | 2014-09-12 | 2016-03-17 | Microsoft Corporation | Actions on digital document elements from voice |
US9582498B2 (en) * | 2014-09-12 | 2017-02-28 | Microsoft Technology Licensing, Llc | Actions on digital document elements from voice |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
GB2533842A (en) * | 2014-10-20 | 2016-07-06 | Lenovo Singapore Pte Ltd | Text correction based on context |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US11556230B2 (en) | 2014-12-02 | 2023-01-17 | Apple Inc. | Data detection |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
US11275757B2 (en) | 2015-02-13 | 2022-03-15 | Cerner Innovation, Inc. | Systems and methods for capturing data, creating billable information and outputting billable information |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US11842734B2 (en) | 2015-03-08 | 2023-12-12 | Apple Inc. | Virtual assistant activation |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US10970482B2 (en) | 2015-05-28 | 2021-04-06 | Advanced New Technologies Co., Ltd. | Assisted data input |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10255907B2 (en) | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11947873B2 (en) | 2015-06-29 | 2024-04-02 | Apple Inc. | Virtual assistant for media playback |
US11550542B2 (en) | 2015-09-08 | 2023-01-10 | Apple Inc. | Zero latency digital assistant |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US11853647B2 (en) | 2015-12-23 | 2023-12-26 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US11205057B2 (en) * | 2016-05-20 | 2021-12-21 | International Business Machines Corporation | Communication assistant to bridge incompatible audience |
US20170339091A1 (en) * | 2016-05-20 | 2017-11-23 | International Business Machines Corporation | Cognitive communication assistant to bridge incompatible audience |
US10579743B2 (en) * | 2016-05-20 | 2020-03-03 | International Business Machines Corporation | Communication assistant to bridge incompatible audience |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US10354011B2 (en) | 2016-06-09 | 2019-07-16 | Apple Inc. | Intelligent automated assistant in a home environment |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US11657820B2 (en) | 2016-06-10 | 2023-05-23 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10297253B2 (en) | 2016-06-11 | 2019-05-21 | Apple Inc. | Application integration with a digital assistant |
US10521466B2 (en) | 2016-06-11 | 2019-12-31 | Apple Inc. | Data driven natural language event detection and classification |
US10089072B2 (en) | 2016-06-11 | 2018-10-02 | Apple Inc. | Intelligent device arbitration and control |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US11749275B2 (en) | 2016-06-11 | 2023-09-05 | Apple Inc. | Application integration with a digital assistant |
US10269345B2 (en) | 2016-06-11 | 2019-04-23 | Apple Inc. | Intelligent task discovery |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10553215B2 (en) | 2016-09-23 | 2020-02-04 | Apple Inc. | Intelligent automated assistant |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US11244679B2 (en) * | 2017-02-14 | 2022-02-08 | Samsung Electronics Co., Ltd. | Electronic device, and message data output method of electronic device |
US20200117268A1 (en) * | 2017-04-27 | 2020-04-16 | Siemens Aktiengesellschaft | Authoring augmented reality experiences using augmented reality and virtual reality |
US11099633B2 (en) * | 2017-04-27 | 2021-08-24 | Siemens Aktiengesellschaft | Authoring augmented reality experiences using augmented reality and virtual reality |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10847142B2 (en) | 2017-05-11 | 2020-11-24 | Apple Inc. | Maintaining privacy of personal information |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11538469B2 (en) | 2017-05-12 | 2022-12-27 | Apple Inc. | Low-latency intelligent automated assistant |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US11675829B2 (en) | 2017-05-16 | 2023-06-13 | Apple Inc. | Intelligent automated assistant for media exploration |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US11217255B2 (en) | 2017-05-16 | 2022-01-04 | Apple Inc. | Far-field extension for digital assistant services |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US11487364B2 (en) | 2018-05-07 | 2022-11-01 | Apple Inc. | Raise to speak |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11900923B2 (en) | 2018-05-07 | 2024-02-13 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11074116B2 (en) * | 2018-06-01 | 2021-07-27 | Apple Inc. | Direct input from a remote device |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11360577B2 (en) | 2018-06-01 | 2022-06-14 | Apple Inc. | Attention aware virtual assistant dismissal |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US11630525B2 (en) | 2018-06-01 | 2023-04-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11061744B2 (en) * | 2018-06-01 | 2021-07-13 | Apple Inc. | Direct input from a remote device |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10504518B1 (en) | 2018-06-03 | 2019-12-10 | Apple Inc. | Accelerated task performance |
US11422764B1 (en) * | 2018-06-03 | 2022-08-23 | Epic Optix, Inc. | Multi-platform integrated display |
US10944859B2 (en) | 2018-06-03 | 2021-03-09 | Apple Inc. | Accelerated task performance |
US11002965B2 (en) * | 2018-06-25 | 2021-05-11 | Apple Inc. | System and method for user alerts during an immersive computer-generated reality experience |
US11828940B2 (en) | 2018-06-25 | 2023-11-28 | Apple Inc. | System and method for user alerts during an immersive computer-generated reality experience |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11501504B2 (en) | 2018-12-20 | 2022-11-15 | Samsung Electronics Co., Ltd. | Method and apparatus for augmented reality |
US11862164B2 (en) | 2018-12-21 | 2024-01-02 | Cerner Innovation, Inc. | Natural language understanding of conversational sources |
US11062704B1 (en) | 2018-12-21 | 2021-07-13 | Cerner Innovation, Inc. | Processing multi-party conversations |
US11875883B1 (en) | 2018-12-21 | 2024-01-16 | Cerner Innovation, Inc. | De-duplication and contextually-intelligent recommendations based on natural language understanding of conversational sources |
US11869509B1 (en) | 2018-12-21 | 2024-01-09 | Cerner Innovation, Inc. | Document generation from conversational sources |
US11869501B2 (en) | 2018-12-21 | 2024-01-09 | Cerner Innovation, Inc. | Processing multi-party conversations |
US11398232B1 (en) | 2018-12-21 | 2022-07-26 | Cerner Innovation, Inc. | Natural language understanding of conversational sources |
US11875794B2 (en) | 2018-12-26 | 2024-01-16 | Cerner Innovation, Inc. | Semantically augmented clinical speech processing |
US11410650B1 (en) | 2018-12-26 | 2022-08-09 | Cerner Innovation, Inc. | Semantically augmented clinical speech processing |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11783815B2 (en) | 2019-03-18 | 2023-10-10 | Apple Inc. | Multimodality in digital assistant systems |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11675491B2 (en) | 2019-05-06 | 2023-06-13 | Apple Inc. | User configurable task triggers |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11705130B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | Spoken notifications |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11888791B2 (en) | 2019-05-21 | 2024-01-30 | Apple Inc. | Providing message response suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11360739B2 (en) | 2019-05-31 | 2022-06-14 | Apple Inc. | User activity shortcut suggestions |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11924254B2 (en) | 2020-05-11 | 2024-03-05 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
CN115314757A (en) * | 2022-08-05 | 2022-11-08 | 聚好看科技股份有限公司 | Display device and guide window display method |
CN115314757B (en) * | 2022-08-05 | 2023-10-24 | 聚好看科技股份有限公司 | Display device and guide window display method |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8209183B1 (en) | Systems and methods for correction of text from different input types, sources, and contexts | |
US9911418B2 (en) | Systems and methods for speech command processing | |
US11670289B2 (en) | Multi-command single utterance input method | |
US20220199079A1 (en) | Systems and Methods for Providing User Experiences on Smart Assistant Systems | |
US11853647B2 (en) | Proactive assistance based on dialog communication between devices | |
US8519909B2 (en) | Multimode input field for a head-mounted display | |
US20170263248A1 (en) | Dictation that allows editing | |
US20130275899A1 (en) | Application Gateway for Providing Different User Interfaces for Limited Distraction and Non-Limited Distraction Contexts | |
US11861315B2 (en) | Continuous learning for natural-language understanding models for assistant systems | |
WO2014197730A1 (en) | Application gateway for providing different user interfaces for limited distraction and non-limited distraction contexts | |
US11567788B1 (en) | Generating proactive reminders for assistant systems | |
US20220284904A1 (en) | Text Editing Using Voice and Gesture Inputs for Assistant Systems | |
JP7279636B2 (en) | Information processing device, information processing method, and program | |
US20230128422A1 (en) | Voice Command Integration into Augmented Reality Systems and Virtual Reality Systems | |
US20220279051A1 (en) | Generating Proactive Reminders for Assistant Systems | |
US20220366904A1 (en) | Active Listening for Assistant Systems | |
US20220374645A1 (en) | Task Execution Based on Real-world Text Detection for Assistant Systems | |
TW202307643A (en) | Auto-capture of interesting moments by assistant systems | |
CN116888661A (en) | Reading of communication content including non-Latin or non-resolvable content items for auxiliary systems | |
CN109841209A (en) | Speech recognition apparatus and system | |
US20230419952A1 (en) | Data Synthesis for Domain Development of Natural Language Understanding for Assistant Systems | |
US20240119932A1 (en) | Systems and Methods for Implementing Smart Assistant Systems | |
TW202240461A (en) | Text editing using voice and gesture inputs for assistant systems | |
WO2022235274A1 (en) | Message based navigational assistance | |
Taylor | “Striking a healthy balance”: speech technology in the mobile ecosystem |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PATEL, NIRMAL;STARNER, THAD;WEAVER, JOSH;REEL/FRAME:026555/0004Effective date: 20110706 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0405Effective date: 20170929 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20200626 |