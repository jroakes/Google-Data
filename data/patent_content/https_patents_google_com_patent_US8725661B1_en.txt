US8725661B1 - Growth and use of self-terminating prediction trees - Google Patents
Growth and use of self-terminating prediction trees Download PDFInfo
- Publication number
- US8725661B1 US8725661B1 US13/082,231 US201113082231A US8725661B1 US 8725661 B1 US8725661 B1 US 8725661B1 US 201113082231 A US201113082231 A US 201113082231A US 8725661 B1 US8725661 B1 US 8725661B1
- Authority
- US
- United States
- Prior art keywords
- tree
- prediction
- function
- node
- real
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/01—Dynamic search techniques; Heuristics; Dynamic trees; Branch-and-bound
Definitions
- Prediction trees can include a type of decision tree used in machine learning and data mining applications, among others.
- a prediction tree can be a decision tree in which each node has a real value associated with it, in addition to a branching variable as in a conventional decision tree.
- Prediction trees may be built or learned by using a first set of training data, which is then used to construct the decision and prediction values. A tree may be then applied against a second set of validation data, and the results are used to fine-tune the tree.
- Various computer-implemented techniques are known for growing and applying prediction trees to arbitrary data sets.
- Conventional techniques for building prediction trees include two phases: a growing phase and a pruning phase.
- the growing phase nodes are added to the tree to match a known set of data, such as a training set.
- the tree may be overgrown, often to the point of fitting some noise in the data as well as real trends and patterns in the data.
- a tree can be constructed for a set of data in which each data point is associated with an individual leaf, i.e., the tree is fit exactly to the data set so that no two examples or data points result in the same end leaf or path through the tree.
- such an overgrown tree may exactly fit known data, but could be ineffective or useless at predicting outcomes for other examples or data points.
- a second pruning phase may be employed in which sections of the tree that provide little or no additional predictive power are removed or collapsed. For example, a portion of the tree that fails to distinguish further among most of the examples that lead to that portion of the tree may be removed, thus terminating that portion of the tree at a higher node.
- Various pruning and validation techniques are known. For example, validation data may be applied to the tree to determine whether the tree provides equivalent or better predictions in the absence of certain nodes. Such nodes may then be pruned from the tree.
- the two-step growing and pruning process is computationally expensive.
- Some tree learning and application techniques associate a prediction with internal nodes of prediction trees; such techniques have been used for the estimation and learning of context trees for compression and classification.
- Measure-based regularization of prediction trees has been used to penalize a Hilbert norm of the gradient of a prediction function ⁇ .
- Some tree growing techniques have made use of self-controlled learning for online learning of self-bounded suffix trees. The learning procedure can be viewed as the task of estimating the parameters of a prediction tree of a fixed structure using the hinge loss for assessing the empirical risk along with an l2-norm variation penalty. In the context of online learning, this setting may lead to distilled analysis that implies sub-linear growth of the suffix tree. However, such approaches may not migrate directly to other settings.
- Various Bayesian approaches have also been used for tree induction and pruning.
- a computer-implemented method of constructing a self-terminating prediction tree may include constructing a piecewise-continuous function representative of a prediction tree that maps an input space to real prediction values, determining a complexity function for the prediction tree based upon the variation norm of the real-valued prediction values, where the complexity function includes a regularizer that indicates when each child of a node should not be grown, and constructing a weighted risk function based upon the piecewise-continuous function.
- a variable that minimizes a combination of the complexity function and the weighted risk function for a root node may be identified, and a real value for each child node of the root node determined.
- the combination of the complexity function and the weighted risk function for each child node may be minimized, so as to obtain a real value for each child node of the child node.
- An input that includes a request for a prediction of a real value may be received from a user, and the tree may be traversed to obtain the requested prediction.
- a computer-implemented method of constructing a self-terminating prediction tree may include determining a complexity function for the prediction tree, constructing a weighted risk function for the prediction tree, and minimizing a combination of the complexity function and the weighted risk function to obtain a real-valued prediction for a plurality of nodes in the tree, where nodes having a real-valued prediction of zero are not added to the tree.
- a system may include a processor configured to construct a piecewise-continuous function representative of a prediction tree, where the function maps an input space to real prediction values, determine a complexity function for the prediction tree based upon the variation norm of the real-valued prediction values, that includes a regulator to indicate when a node should not be grown, and construct a weighted risk function based upon the piecewise-continuous function.
- the processor may determine a variable that minimizes a combination of the complexity function and the weighted risk function for the root node, determine a real value for each child node of the root node, and, for each child node of the root node having a non-zero real value, minimize the combination of the complexity function and the weighted risk function for the child node to obtain a real value for each child node of the child node.
- the system also may include an input configured to receive a request for a prediction of a real value based upon the prediction tree from a user, and an output configured to provide a prediction obtained by traversing the tree based upon the request.
- a system may include a processor configured to determine a complexity function for a prediction tree, construct a weighted risk function for the prediction tree, and minimize a combination of the complexity function and the weighted risk function to obtain a real-valued prediction for a plurality of nodes in the tree. Nodes in which the optimization method yields no change in the real-valued prediction relative to the parent need not be added to the tree.
- methods and systems as disclosed above may be implemented on or in conjunction with a computer-readable medium that causes a processor to perform the disclosed methods and/or to implement the disclosed systems.
- FIG. 1A shows a schematic representation of an example prediction tree according to an embodiment of the disclosed subject matter.
- FIG. 1B shows a specific example of a prediction tree according to an embodiment of the disclosed subject matter.
- FIG. 2A shows an example correspondence between and tree size according to an embodiment of the disclosed subject matter.
- FIG. 2B shows validation loss at various tree sizes according to an embodiment of the disclosed subject matter.
- FIG. 3 shows example convex loss functions suitable for use with various embodiments of the disclosed subject matter.
- FIG. 4 shows an example logistic loss function suitable for use with various embodiments of the disclosed subject matter.
- FIG. 5 shows an example difference of hinge loss function suitable for use with various embodiments of the disclosed subject matter.
- FIG. 6 shows a comparison of the difference of hinge loss optimizers according to an embodiment of the disclosed subject matter with Cart results.
- FIG. 7 shows a comparison of the hinge loss optimizers according to an embodiment of the disclosed subject matter with Cart results.
- FIG. 8 shows experimental results in which uniform label noise is injected in 20% of the training and validation data according to an embodiment of the disclosed subject matter.
- FIG. 9 shows a scatter plot comparing the log loss between Cart and the log loss optimizer with an l 1 regularizer according to an embodiment of the disclosed subject matter.
- FIG. 10 shows a plot of error rates for various noise rates for the difference of hinge and log loss functions according to embodiments of the disclosed subject matter.
- FIG. 11 shows an example device according to an embodiment of the disclosed subject matter.
- FIG. 12 shows techniques for node-based optimization techniques for classification and regression according to embodiments of the disclosed subject matter.
- FIG. 13 shows an example technique for a generalized solution for a dual optimization method according to an embodiment of the disclosed subject matter.
- FIG. 14 shows an example of node-level optimizations for a classification setting according to an embodiment of the disclosed subject matter.
- FIG. 15 shows an example of node-level optimizations for a regression setting according to an embodiment of the disclosed subject matter.
- FIG. 16 shows an example prediction tree according to an embodiment of the disclosed subject matter.
- Self-terminating prediction trees are a generalization of decision trees in which each node is associated with a real-valued prediction. Instead of having a separate pruning phase, a self-terminating tree may be constructed by applying various limits during tree growth that prevent nodes that add little or no additional decision power from being grown within the tree. For example, a parent node that would only have a single child node that provides little or no additional information relative to the parent's real-value prediction value may not be grown.
- any tree or tree structure that could be created using a conventional growing/pruning technique also may be created using embodiments of the disclosed subject matter.
- growing/pruning techniques normally expand either all children no children of a node in the tree
- embodiments of the disclosed subject matter allow for development of the same or equivalent tree structures directly during tree growth.
- an SPT can be viewed as a piecewise-constant function from an input space into a set of real values. Therefore, the children of a node in an SPT split the portion of the input feature space that is defined by the parent node into disjoint partitions, where each of the partitions is associated with a different prediction value.
- the complexity of the tree may be measured by the variation norm of the piecewise-constant function it induces.
- SPTs may be applied to obtain prediction values for base inputs, such as prediction request and/or initial data supplied by a user of a system configured to generate and/or use the SPT.
- a base prediction for an input instance is formed by summing the individual predictions at the nodes traversed from the root node to a leaf by applying a sequence of branching predicates.
- the final predicted value may be obtained by applying a transfer function to the base prediction.
- a suitable transfer function may be the inverse logit function 1/(1+e ⁇ x ).
- the identity may be used as a suitable transfer function.
- the logical problem of learning the prediction tree may be cast as a penalized empirical risk minimization task, based upon the use of prediction values and functional tree complexity described above. For instance, for prediction trees with the inverse logit transfer, a natural choice for the risk is the log-likelihood of the examples. Variation penalties based on l 1 and l ⁇ , norms may be used. It has been found that these norms may promote sparse solutions that, in the context of SPTs, correspond to self-terminating of a tree-growing phase, meaning that no separate pruning phase is required. These norms also may facilitate parameter estimation of the prediction values.
- Embodiments of the presently disclosed subject matter may be “backward compatible” with existing tree learning procedures. That is, other tree learning procedures may be used, and caused to self-terminate using the techniques disclosed herein. Efficient tree growing algorithms may be derived for a variety of loss functions, including some non-convex losses such as the difference of hinge functions, which may provide a tighter bound to the 0-1 loss.
- techniques disclosed herein may provide other growing criteria such as the information gain and the Gini index.
- an optimization method employing a dual representation of the (primal) penalized risk may be used, which may enable a unified treatment of different variational norms through their dual norms.
- a combined primal-dual procedure also may provide an algorithmic skeleton independent of the empirical loss.
- Embodiments of the presently disclosed subject matter may diverge from conventional tree construction methods, which require two uncoupled phases of growing and then pruning the tree.
- a prediction tree is a generalization of a decision tree in which each node s is assigned a predicate ⁇ s that is used for branching, as well as a real value ⁇ s .
- FIG. 1A shows a schematic representation of an example prediction tree where each node s 100 has a real value ⁇ s associated with it.
- the bias b s is the sum of real values from the root node 110 to the node s 100 , and provides a confidence value for each prediction. Confidence values may be calculated for both internal nodes and for leaves.
- FIG. 1B shows a specific example of a prediction tree. As in FIG. 1A , the bias provides a confidence value for each prediction.
- the path P s (x) is defined as the path of nodes from the root node to the node s when evaluating x.
- the norm variation complexity V P (T) is defined as ⁇ s ⁇ T ⁇ (s) ⁇ C(s) ⁇ p , where C(s) is the set of children of the node s and ⁇ (s) is a penalty for node s, e.g., the depth of node s.
- the real value ⁇ is set to 0 for null children.
- the penalties ⁇ (s) and ⁇ tilde over ( ⁇ ) ⁇ (s) may be used to encourage small decision trees.
- the regularization constant ⁇ provides a control for the degree of sparsity of the prediction tree.
- FIG. 2A shows an example correspondence between ⁇ and tree size according to embodiments of the presently disclosed subject matter. As shown, the tree size may be constrained by selecting an appropriate value of the regularization constant. For comparison, the validation loss at various tree sizes is shown in FIG. 2B . In some configurations, a more strict regularization constant, i.e., one that results in a smaller tree, also may increase the prediction error.
- the use of the l ⁇ regularizer above may provide a sparse solution, in which children C(s) of a node s are zero. If the optimal solution is such that at least some ⁇ s′ for s′ ⁇ C(s) is non-zero, then the rest of the children can be non-zero as well without incurring further penalty.
- the tree learning process can be performed as a penalized empirical risk minimization task.
- the tree is modeled as a piecewise-continuous function and a risk function is applied.
- a function ⁇ T may be defined for a prediction tree T.
- ⁇ T (x) may be the sum of the ⁇ values along the path from the root of the tree T to the leaf reached by x.
- An empirical risk function ⁇ circumflex over (R) ⁇ (L, F, w) may be defined for the function ⁇ with loss L weighted by w ⁇ 0.
- Equation 1 incorporates sparsity-promoting regulation and, therefore, the learning technique encourages small trees that naturally terminate growth.
- the optimization procedure used to select the variable to place at the node simultaneously determines the value ⁇ j for each of the branches defined by the selected variable. For each branch for which s ⁇ s is non-zero, the process is recursively applied. That is, embodiments of the presently disclosed subject matter may learn a prediction tree by first determining a variable that minimizes a combination of the complexity function and the weighted risk function at a root node, which also provides a real value for each child node of the root node. Similarly, these techniques may then determine a variable that minimizes a combination of the complexity function and the weighted risk function for each child node having a non-zero real value, which provides a real value for each child node of the root node. The process may be recursively applied for each child level having at least node with a non-zero real value.
- the regularizer used in the objective determines when to stop growing the tree, i.e., when the tree will self-terminate.
- the regularization constant ⁇ provides a control for the degree of sparsity for the prediction tree, as shown and described with respect to FIG. 1 previously.
- embodiments of the presently disclosed subject matter may be used to boost shallow, non-fixed depth trees.
- embodiments of the presently disclosed subject matter may be considered as including several components: associating each node of a prediction tree with a confidence value and a real-valued prediction, and learning the tree by minimizing a penalized empirical risk function.
- the risk function may be applied, for example, to a piecewise-continuous model of the tree.
- the complexity measure of the tree may be defined as the variation of the real-valued predictions.
- various loss functions may be used with the penalized empirical risk tree learning technique.
- node expansion may be performed through a variety of techniques. Techniques for learning sparse real ⁇ values for a node's children according to an embodiment of the disclosed subject matter will now be described.
- the predicate ⁇ to use within a node s is chosen by greedily selecting the predicate minimizing the penalized loss (1). More specifically, the loss obtained when s is associated with a k-ary predicate ⁇ may be derived, which in turn may create k children with values ⁇ 1 , . . . , ⁇ k .
- ⁇ j 0 for all j, this objective reduces to a standard information gain.
- a general-purpose solution for the dual case may be obtained as described herein.
- the closed-form solution for ⁇ j requires knowledge of s j ; however, the sign of ⁇ j may be determined from known quantities:
- s j and ⁇ j can be determined based upon only the known quantities ⁇ , ⁇ , ⁇ , and b.
- a more complex method may be applied to all convex losses and for both l 1 and l ⁇ regularizers, as described in further detail herein.
- another loss of interest may be the difference of hinge loss.
- an exponential loss function may be used.
- L( ⁇ (x),y) exp( ⁇ (x) y), so the objective function is
- a squared loss may be applied for regression problems (where y ⁇ ).
- the technique attempts to find ⁇ that minimizes
- FIG. 12 shows node-based optimization techniques for classification and regression according to embodiments of the presently disclosed subject matter.
- FIG. 13 shows an example technique for such a solution.
- FIG. 14 shows an example embodiment of the disclosed subject matter of node-level optimizations for a classification setting.
- FIG. 15 shows an example of node-level optimizations for a regression setting according to an embodiment of the disclosed subject matter. Further details regarding the dual technique, including derivation and solutions for an l 1 constraint, are provided in the appendix included herewith.
- FIG. 3 shows example convex loss functions suitable for use with embodiments of the presently disclosed subject matter.
- Example hinge 510 , logistic 520 , and exponential 530 functions are shown.
- FIGS. 4-5 show example non-convex loss functions suitable for use with embodiments of the presently disclosed subject matter, including the difference of logistic loss (4) and difference of hinge loss (5) functions. It will be understood that the specific functions shown are illustrative only, and other variations and other loss functions may be used.
- the standard University of California-Irvine (UCI) data sets as commonly used in the field were used to grow and test a self-terminating tree.
- the results obtained with this embodiment demonstrate that the self-terminating tree techniques disclosed herein provide results competitive with a sophisticated Cart implementation that uses validation data in a post-pruning process.
- embodiments of the presently disclosed subject matter allow for trees to self-terminate during the growing phase, with validation data only needed to select the value of 2.
- the standard UCI training data was used, with 1 ⁇ 6 of the training data used as test data. The remaining 5 ⁇ 6 was provided as training data (with a fraction set aside as designated by the algorithm for cross validation).
- the classification results were averaged over 200 repetitions of this process, and the results for regression averaged over 50 repetitions. The results are shown below:
- FIGS. 6-7 compare the hinge loss and difference of hinge loss optimizers according to embodiments of the presently disclosed subject matter with Cart results in a scatter plot.
- the techniques disclosed herein are naturally suited to making binary predictions and, because the optimization technique itself is based on minimizing a given loss function, it is appropriate that the disclosed techniques perform well when compared with Cart with respect to the loss being optimized.
- FIG. 9 shows a scatter plot comparing the log loss between Cart (using the confidence measure as its real-valued prediction), and the log loss optimizer with an l 1 regularizer according to embodiments of the presently disclosed subject matter.
- FIG. 8 and the following table present data from embodiments of the presently disclosed subject matter in which uniform label noise has been injected in 20% of the training and validation data.
- SPTs use empirical risk minimization with respect to a real-valued prediction associated with each node in the tree, it would be expected that as with minimizing the log loss, the techniques disclosed herein will perform well for regression as compared with Cart.
- the following table shows a comparison between Cart and SPTs according to embodiments of the presently disclosed subject matter using the squared loss with au L 1 regularizer. As expected, SPTs according to embodiments of the presently disclosed subject matter may significantly outperform Cart on these data sets.
- FIG. 10 shows a plot of error rates for various noise rates for the difference of hinge and log loss functions.
- Embodiments of the presently disclosed subject matter also may be extended and generalized to multiclass problems.
- some embodiments of the presently disclosed subject matter may provide techniques to solve multiclass problems using an l 1 regularizer at the node level. Using this restriction, an estimation procedure for each child of a node may be individually performed.
- a derivation of an example multiclass technique and solution according to an embodiment of the presently disclosed subject matter is disclosed in the appendix provided herewith.
- Embodiments of the presently disclosed subject matter may be used to construct and use self-terminating trees in a variety of contexts.
- self-terminating trees may be used to automatically classify or rank various items within a computer system. Specific examples include assigning a likelihood that a file is corrupt, identifying a desired file or component, ranking cost or value of a set of items, attributes, or conditions, assigning a probability that a user's provided identity is correct, determining a likelihood that a security measure has been breached, and the like, as well as various other ranking and/or classification applications.
- the real value at each node may provide, for example, an indication of whether a user is likely to perform a specific action, if an analysis of the user's history or attributes leads to that node of the tree.
- Each node may indicate an attribute the user may have, the value of which for the particular user indicates which branch or path through the tree should be followed.
- the tree may provide a prediction that the user's data is inaccurate, that the file is corrupt, or the like.
- FIG. 16 shows an example prediction tree for predicting the political party of a political representative based on the representative's votes.
- a prediction value (b, “output”) closer to 1 indicates a higher likelihood or confidence that the representative is a Democrat, while a prediction value of 0 indicates a high confidence that the representative is a Republican.
- the bias b (equal to the sum of the real values ⁇ along the path to each node) is shown for each node.
- the tree structure shown in FIG. 16 may be obtained according to embodiments of the disclosed subject matter by applying an optimization procedure that selects a variable to place at each node. As previously described, the optimization procedure will also determine the real values ⁇ for each branch at the node. Only branches with non-zero ⁇ values are grown. In the example, this process results in a prediction tree that includes the large nodes 1610 , 1611 , 1612 , 1613 , 1614 , 1615 , where each node indicates a vote (variable) that was found to improve the predictive power of the tree.
- FIG. 16 also shows a prediction tree that may be obtained for the same value using a conventional growth/pruning technique, before it has been pruned.
- a conventional growth/pruning technique for each node typically either all children are expanded or no children are expanded.
- the only child of the “immigration” vote 1611 found by an SPT technique as disclosed herein to have further predictive power is the “yes” branch” along the outside edge, as shown by the 0-valued ⁇ values for the other branches.
- a conventional growth/pruning technique may expand the “no” and “no vote” branches as well, resulting in the sub-tree structure 1620 as shown.
- a growth/pruning technique may expand other branches 1630 , 1640 , 1650 , 1660 that would not be grown by an SPT technique as disclosed herein.
- additional nodes are shown much smaller for ease of illustration and understanding, it will be apparent to one of skill in the art that the intermediate fully-grown tree results in a much larger tree than the SPT techniques disclosed herein.
- these nodes may then be pruned based upon the performance of the full tree when applied to validation data.
- the validation data may show that the additional branches 1620 - 1660 provide little or no additional predictive power, or that a tree without one or more of these branches performs better than the fully-grown tree that includes these branches.
- the branches 1620 - 1660 may be removed from the tree, resulting in a similar or identical tree to that obtained by an SPT technique as disclosed herein.
- the additional growth of branches that are later pruned 1620 - 1660 causes computational inefficiencies, especially for larger trees and data sets.
- embodiments of the disclosed subject matter may provide improved processing time relative to growth/pruning-type techniques for tree growth.
- FIG. 11 is an example device 200 suitable for implementing embodiments of the presently disclosed subject matter.
- the computer system 200 includes a bus 212 which interconnects major subsystems of the computer system 210 , such as a central processor 214 , a system memory 217 (typically RAM, but which may also include ROM, flash RAM, or the like), an input/output controller 218 , a user display 224 , such as a display screen via a display adapter, a user input subsystem, which may include one or more controllers and associated user input devices such as a keyboard, mouse, and the like, fixed storage 224 , such as a hard drive, flash storage, Fibre Channel network, SCSI device, and the like, and a removable media subsystem 237 operative to control and receive an optical disk, flash drive, and the like.
- a bus 212 which interconnects major subsystems of the computer system 210 , such as a central processor 214 , a system memory 217 (typically RAM, but which may also include
- the bus 212 allows data communication between the central processor 214 and the system memory 217 , which may include read-only memory (ROM) or flash memory (neither shown), and random access memory (RAM) (not shown), as previously noted.
- the RAM is generally the main memory into which the operating system and application programs are loaded.
- the ROM or flash memory can contain, among other code, the Basic Input-Output system (BIOS) which controls basic hardware operation such as the interaction with peripheral components.
- BIOS Basic Input-Output system
- Applications resident with the computer system 200 are generally stored on and accessed via a computer readable medium, such as a hard disk drive (e.g., fixed storage 224 ), an optical drive, floppy disk, or other storage medium 237 .
- the fixed storage 224 may be integral with the computer system 200 or may be separate and accessed through other interface systems.
- the network interface 208 may provide a direct connection to a remote server via a telephone link, to the Internet via an internet service provider (ISP), or a direct connection to a remote server via a direct network link to the Internet via a POP (point of presence) or other technique.
- ISP internet service provider
- POP point of presence
- the network interface 208 may provide such connection using wireless techniques, including digital cellular telephone connection, Cellular Digital Packet Data (CDPD) connection, digital satellite data connection or the like.
- CDPD Cellular Digital Packet Data
- Embodiments of the presently disclosed subject matter may include or be embodied in the form of computer-implemented processes and apparatuses for practicing those processes.
- Embodiments also may be embodied in the form of a computer program product having computer program code containing instructions embodied in non-transitory and/or tangible media, such as floppy diskettes, CD-ROMs, hard drives, USB (universal serial bus) drives, or any other machine readable storage medium, wherein, when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing embodiments of the disclosed subject matter.
- Embodiments also may be embodied in the form of computer program code, for example, whether stored in a storage medium, loaded into and/or executed by a computer, or transmitted over some transmission medium, such as over electrical wiring or cabling, through fiber optics, or via electromagnetic radiation, wherein when the computer program code is loaded into and executed by a computer, the computer becomes an apparatus for practicing embodiments of the disclosed subject matter.
- the computer program code segments configure the microprocessor to create specific logic circuits.
- a set of computer-readable instructions stored on a computer-readable storage medium may be implemented by a general-purpose processor, which may transform the general-purpose processor or a device containing the general-purpose processor into a special-purpose device configured to implement or carry out the instructions.
- Embodiments may be implemented using hardware that may include a processor, such as a general purpose microprocessor and/or an Application Specific Integrated Circuit (ASIC) that embodies all or part of the method in accordance with embodiments of the disclosed subject matter in hardware and/or firmware.
- the processor may be coupled to memory, such as RAM, ROM, flash memory, a hard disk or any other device capable of storing electronic information.
- the memory may store instructions adapted to be executed by the processor to perform the method in accordance with an embodiment of the disclosed subject matter.
- ⁇ j v j - ⁇ j ⁇ j - b .
- Lemmas 1 and 3 suggest an efficient algorithm that iteratively considers candidate ⁇ values.
- computing the optimal ⁇ j is easy using Eq. (2) or Eq. (4).
- ⁇ ( ⁇ ) denote the optimal ⁇ .
- ⁇ j ⁇ j - v j ⁇ e b + ⁇ ⁇ ⁇ s j c b + 0 ⁇ ⁇ s j + 1 .
- the key to the algorithm is to find the optimal partition of ⁇ into I + , I ⁇ , and I 0 .
- ⁇ must satisfy ⁇ i-1 > ⁇ i .
- the multiclass penalized risk minimization for the logistic loss amount to minimizing ⁇ i q i log p i + ⁇ ⁇ where p i ⁇ e ⁇ i +b i .
- Finding the optimal solution of this problem is not an easy task due to the l ⁇ penalty.
- Eq. (1) underscores the relation between ⁇ and p. Specifically, Eq. (1) implies that when ⁇ i >0, u i ⁇ p i ⁇ q i , and for ⁇ i ⁇ 0, u i ⁇ p i >q i . In words, the solution p lies between q and u where the lower and upper bounds on each coordinate in p depends on the relation between the corresponding components in q and u. This characterization facilitates the efficient procedure for finding the optimum which we describe in the sequel.
- I + be the set of indices for which ⁇ i >0
- I ⁇ be the set of indices for which ⁇ i ⁇ 0
- This condition can be checked in constant time as well by simply examining the largest and smallest ratios q i /u i for i ⁇ I 0 .
- the time complexity of this procedure for finding the optimum is O(n 2 ) since we might need to examine all possible pairs (r, s) such that 1 ⁇ r ⁇ s ⁇ n and q r /u r ⁇ 1 and q s /u s >1. Since typically the label set is not large and we can quickly disqualify candidate partitions we found that this procedure is in practice very fast.
Abstract
Description
where {tilde over (λ)}(s) is the penalty for the parent of node s. The penalties λ(s) and {tilde over (λ)}(s) may be used to encourage small decision trees. In general, the regularization constant λ provides a control for the degree of sparsity of the prediction tree. For example,
Then the goal is to minimize the penalized weighted empirical risk (Equation 1):
μj=Σy
νj=Σy
In terms of ν and μ, this becomes (Equation 2):
It can be shown that this generalizes a conventional greedy tree building using information gain by first determining the dual of
where
Thus, rj and αj are:
The closed-form solution for αj requires knowledge of sj; however, the sign of αj may be determined from known quantities:
For p=1, the loss is piecewise-linear, so the objective may be determined at the three inflection points α=0, α=1−b, and α=−1−b. The objective values may then be compared to find the minimum. The dual approach described herein may also be used, such as when p=∞.
For this loss, a solution may be obtained using the primal for both p=1 and p=∞. When p=1, the loss is piecewise-linear as with the hinge loss, allowing for the objective to be determined at the inflection points α=0, α=1−b, α=−1−b, and α=−b. Similarly, when p=∞, the loss is piecewise-linear with inflection points at αj=0, for αjε{−r, r}, where r=min(|1−b|, |1+b|) and αjε{1−b,−(1+b)} for all j.
For p=1, setting the sub-gradient with respect to αj to zero yields
−μiexp(−(αi +b))+νiexp(αi +b)+λsign(αi)=0.
The equation is a second-order polynomial in eα and the solution is the root of the equation. Just as the logistic loss generalizes a standard information gain measure for tree growing, the exponential loss generalizes the Gini index.
where C is a constant independent if α. Defining
and
gives the equivalent
The saddle point for αj is defined by wj(αj+b)−νj+λsign(αj)=0. So when αj>0, at the saddle point, αj=(νj−λ)/μj−b. This occurs if and only if (νj−λ)/μj−b>0, or equivalently when
νj/μj −b>λ/μ j.
Similarly, when αj<0, αj=(νj+λ)/μj−b, if and only if (νj+λ)/μj−b<0, or equivalently when
νj/μj −b<−λ/μ j.
If neither conditions hold, then αj=0.
Data Set | Cart | LogLoss l1 | DiffHinge l1 | HingeLoss l1 | HingeLoss l∞ |
breast-cancer | 0.297 ± 0.059 | 0.286 ± 0.061 | 0.310 ± 0.062 | 0.309 ± 0.062 | 0.324 ± 0.057 |
breast-w | 0.063 ± 0.022 | 0.089 ± 0.026 | 0.061 ± 0.021 | 0.061 ± 0.021 | 0.052 ± 0.019 |
credit-g | 0.269 ± 0.032 | 0.283 ± 0.036 | 0.288 ± 0.031 | 0.288 ± 0.031 | 0.289 ± 0.031 |
diabetes | 0.259 ± 0.037 | 0.248 ± 0.033 | 0.261 ± 0.033 | 0.261 ± 0.033 | 0.261 ± 0.033 |
haberman | 0.271 ± 0.062 | 0.284 ± 0.068 | 0.254 ± 0.061 | 0.254 ± 0.061 | 0.251 ± 0.063 |
heart-c | 0.225 ± 0.058 | 0.249 ± 0.059 | 0.274 ± 0.050 | 0.274 ± 0.050 | 0.275 ± 0.030 |
heart-h | 0.212 ± 0.050 | 0.225 ± 0.055 | 0.199 ± 0.052 | 0.199 ± 0.052 | 0.198 ± 0.053 |
heart-statlog | 0.223 ± 0.061 | 0.244 ± 0.061 | 0.263 ± 0.057 | 0.263 ± 0.058 | 0.265 ± 0.057 |
hepatitis | 0.202 ± 0.079 | 0.215 ± 0.078 | 0.201 ± 0.072 | 0.201 ± 0.072 | 0.201 ± 0.072 |
labor | 0.251 ± 0.135 | 0.273 ± 0.118 | 0.220 ± 0.127 | 0.223 ± 0.127 | 0.236 ± 0.129 |
liver-disorders | 0.350 ± 0.057 | 0.344 ± 0.061 | 0.354 ± 0.072 | 0.334 ± 0.072 | 0.354 ± 0.072 |
lung-cancer | 0.186 ± 0.201 | 0.234 ± 0.189 | 0.148 ± 0.186 | 0.148 ± 0.186 | 0.148 ± 0.186 |
solar-flare-1 | 0.025 ± 0.021 | 0.022 ± 0.019 | 0.023 ± 0.019 | 0.023 ± 0.019 | 0.022 ± 0.019 |
solar-flare-2 | 0.005 ± 0.005 | 0.004 ± 0.004 | 0.005 ± 0.004 | 0.005 ± 0.004 | 0.005 ± 0.005 |
sonar | 0.266 ± 0.075 | 0.281 ± 0.074 | 0.285 ± 0.076 | 0.285 ± 0.076 | 0.285 ± 0.076 |
vote | 0.051 ± 0.024 | 0.048 ± 0.024 | 0.043 ± 0.022 | 0.043 ± 0.022 | 0.043 ± 0.022 |
Data Set | Cart | LogLoss l1 | DiffHinge l1 | HingeLoss l1 | HingeLoss l∞ |
breast-cancer | 0.327 ± 0.077 | 0.333 ± 0.073 | 0.320 ± 0.077 | 0.318 ± 0.068 | 0.325 ± 0.063 |
breast-w | 0.080 ± 0.027 | 0.080 ± 0.028 | 0.079 ± 0.025 | 0.080 ± 0.026 | 0.076 ± 0.028 |
credit-g | 0.293 ± 0.036 | 0.304 ± 0.041 | 0.296 ± 0.036 | 0.295 ± 0.034 | 0.297 ± 0.036 |
diabetes | 0.272 ± 0.039 | 0.277 ± 0.042 | 0.268 ± 0.039 | 0.265 ± 0.036 | 0.263 ± 0.036 |
haberman | 0.300 ± 0.070 | 0.319 ± 0.068 | 0.294 ± 0.068 | 0.281 ± 0.065 | 0.283 ± 0.073 |
heart-c | 0.273 ± 0.068 | 0.276 ± 0.067 | 0.277 ± 0.067 | 0.273 ± 0.060 | 0.269 ± 0.065 |
heart-h | 0.218 ± 0.064 | 0.230 ± 0.066 | 0.214 ± 0.057 | 0.211 ± 0.053 | 0.212 ± 0.057 |
heart-statlog | 0.265 ± 0.075 | 0.281 ± 0.073 | 0.271 ± 0.065 | 0.277 ± 0.069 | 0.271 ± 0.071 |
hepatitis | 0.243 ± 0.095 | 0.256 ± 0.100 | 0.211 ± 0.082 | 0.216 ± 0.084 | 0.216 ± 0.079 |
labor | 0.329 ± 0.157 | 0.283 ± 0.158 | 0.265 ± 0.165 | 0.293 ± 0.166 | 0.290 ± 0.166 |
liver-disorders | 0.391 ± 0.068 | 0.392 ± 0.069 | 0.413 ± 0.059 | 0.416 ± 0.063 | 0.406 ± 0.075 |
lung-cancer | 0.372 ± 0.238 | 0.386 ± 0.225 | 0.330 ± 0.250 | 0.366 ± 0.260 | 0.349 ± 0.247 |
solar-flare-1 | 0.064 ± 0.043 | 0.029 ± 0.029 | 0.032 ± 0.026 | 0.032 ± 0.028 | 0.031 ± 0.025 |
solar-Rare-2 | 0.017 ± 0.016 | 0.005 ± 0.006 | 0.006 ± 0.006 | 0.007 ± 0.007 | 0.007 ± 0.007 |
sonar | 0.333 ± 0.088 | 0.334 ± 0.089 | 0.340 ± 0.087 | 0.334 ± 0.087 | 0.327 ± 0.091 |
vote | 0.057 ± 0.028 | 0.050 ± 0.027 | 0.018 ± 0.025 | 0.048 ± 0.025 | 0.048 ± 0.024 |
Data Set | Cart | SquaredLoss l1 | ||
abalone | 4.669 ± 0.368 | 2.553 ± 0.209 | ||
autoMpg | 12.038 ± 3.560 | 6.613 ± 1.424 | ||
breastTumor | 103 ± 16 | 53 ± 8 | ||
cpu | 1322 ± 1772 | 4787 ± 5669 | ||
diabetes_numeric | 0.625 ± 1.056 | 0.186 ± 0.073 | ||
housing | 33.583 ± 73.934 | 9.290 ± 4.233 | ||
kdd_coil | 208.685 ± 377.111 | 17.555 ± 11.409 | ||
mbagrade | 0.123 ± 0.055 | 0.061 ± 0.024 | ||
servo | 0.568 ± 0.454 | 0.349 ± 0.234 | ||
vineyard | 13.995 ± 33.863 | 5.591 ± 2.833 | ||
wisconsin | 2972 ± 7118 | 531 ± 88 | ||
κj>θ iff γj>0, κj <−θ iff γ j<0, and −θ≦κj≦θ iff γj=0.
Proof. Let sjε∂|γj|. Then the subgradient condition for optimality of the dual (1) is
Let κj>θ and assume that γj≦0. Then sjε[−1,0], and
contradicting the subgradient conditions for optimality. The case for κj<−θ is similar, and when κjε[−θ, θ], then setting γj=0 gives sj=[−1,1] and
which satisfies the subgradient conditions for optimality.
and the case for γj<0 is similar. If γj=0, then by Eq. (2) there is some sjε[−1,1] for which κj+θsj=0, or κjε[−θ, θ]. □
Further, given the optimal dual variable γ, the optimal α is
The structure of the solution is given by the following lemma.
Then
κj>θ iff γj>0, κj<−θ iff γj>0, and −θ≦κj≦θ iff γj=0.
Proof. Let sjε∂|γj|. Then the subgradient condition for optimality of the dual (3) is
Let κj>θ and assume for the sake of contradiction that γj≦0. Then sjε[−1,0], and
a contradiction to the fact that
Conversely, Eq. (4) implies that when γj>0,
The proof for the case that κj<−θ is similar. When κjε[−θ,θ], there is some sjε[−1,1] such that
so that Eq. (4) is satisfied. Conversely, if γj=0 is optimal, then Eq. (4) implies
Thus, with the l∞-constraint added, the solution γ*j=max{min{{circumflex over (γ)}j, λ}, −λ} is immediate.
Solving the Dual with l1 Constraints
Let t=e0. Then to find the θ such that ∥γ(θ)∥1=λ, assuming the partition of γ into the index sets I is correct, we solve
We can solve the above for t as follows. Let σμ +=ΣjεI+μj,σμ −=Σjε−μj,σν +=ΣjεI+νj, and σν −=ΣjεI−νj. Then a bit of algebra yields
−(σν ++σμ −+λ)t 2+(e b(σμ +−σμ −−λ)+(σν −−σν +−λ)e −b)t+(σμ ++σν −−λ)=0. (6)
Clearly Eq. (6) is a quadratic in t, and we can solve for θ=log t (where we take the positive root, and if there is none, the algorithm simply continues). For the regression problem, we see that solving for γj in Eq. (4) gives γj(θ)=νj−μj(b+sjθ). Thus, setting the σ values as before for logistic regression, we require that
Solving for θ yields
such ∥γ∥1≦λ and Σiγi=0. To solve the dual form we introduce a Lagrange multiplier θ≧0 for the l1 constraint and δ for the constraint that Σiγi=0, and obtain the following Lagrangian,
Denoting si=sign(γi), and using the sub-gradient optimality condition with respect to γ yields that,
where z is the standard normalization (partition function) which ensures that p is a proper distribution. Eq. (1) underscores the relation between γ and p. Specifically, Eq. (1) implies that when γi>0, ui≦pi<qi, and for γi<0, ui≧pi>qi. In words, the solution p lies between q and u where the lower and upper bounds on each coordinate in p depends on the relation between the corresponding components in q and u. This characterization facilitates the efficient procedure for finding the optimum which we describe in the sequel.
and similarly,
Combining Eq. (1) with the constraint that Σiγi=0 (which stems from the requirement Σipi=1) yields
(e θ U + +e −θ U −)/z=Q + +Q −. (2)
Similarly, combining Eq. (1) with the constraint Σi|γi|=λ yields
(−e θ U + +e −θ U −)/z=λ−Q + +Q −. (3)
Combining the last two equalities gives a close form solution for θ and z,
|log(q i /u i)+log z|<θ
We now combine these properties to obtain an efficient algorithm for finding the optimal partition into I+, I− and I0 in the optimal solution. First observe we can sort the components according to the ratios qi/ui. Without loss of generality and for clarity of our derivation, let us assume that q1/u1≦q2/u2≦ . . . ≦qn/un, where n is the number of different labels. From Eq. (4) we know that there must exist two indices r and s such that 1≦r<s≦n and qr/ur<1 and qs/us>1. In turn, these ratio properties imply that that for j≦r, γj<0, γr+1= . . . =γs-1=0, and for j≧s, γj>0. The next key observation is that had we were given the partition, then we could have computed the solution corresponding to that partition using the from the equations for z and θ. Finally, from Eq. (4), it is clear that a candidate partition is optimal iff θ>0 and for all i such that |log(qi/ui)+log z|<θ, the value of γi is zero.
Claims (16)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/082,231 US8725661B1 (en) | 2011-04-07 | 2011-04-07 | Growth and use of self-terminating prediction trees |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/082,231 US8725661B1 (en) | 2011-04-07 | 2011-04-07 | Growth and use of self-terminating prediction trees |
Publications (1)
Publication Number | Publication Date |
---|---|
US8725661B1 true US8725661B1 (en) | 2014-05-13 |
Family
ID=50635746
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/082,231 Expired - Fee Related US8725661B1 (en) | 2011-04-07 | 2011-04-07 | Growth and use of self-terminating prediction trees |
Country Status (1)
Country | Link |
---|---|
US (1) | US8725661B1 (en) |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10013655B1 (en) | 2014-03-11 | 2018-07-03 | Applied Underwriters, Inc. | Artificial intelligence expert system for anomaly detection |
CN110110225A (en) * | 2019-04-17 | 2019-08-09 | 重庆第二师范学院 | Online education recommended models and construction method based on user behavior data analysis |
US10402406B2 (en) * | 2016-12-19 | 2019-09-03 | Amadeus S.A.S. | Predictive database for computer processes |
US20200372400A1 (en) * | 2019-05-22 | 2020-11-26 | The Regents Of The University Of California | Tree alternating optimization for learning classification trees |
US11481580B2 (en) * | 2018-05-31 | 2022-10-25 | Fujitsu Limited | Accessible machine learning |
-
2011
- 2011-04-07 US US13/082,231 patent/US8725661B1/en not_active Expired - Fee Related
Non-Patent Citations (1)
Title |
---|
Goldman et al., "Self-Pruning Prediction Trees", Feb. 10, 2010, pp. 1-2. * |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10013655B1 (en) | 2014-03-11 | 2018-07-03 | Applied Underwriters, Inc. | Artificial intelligence expert system for anomaly detection |
US10402406B2 (en) * | 2016-12-19 | 2019-09-03 | Amadeus S.A.S. | Predictive database for computer processes |
US11481580B2 (en) * | 2018-05-31 | 2022-10-25 | Fujitsu Limited | Accessible machine learning |
CN110110225A (en) * | 2019-04-17 | 2019-08-09 | 重庆第二师范学院 | Online education recommended models and construction method based on user behavior data analysis |
US20200372400A1 (en) * | 2019-05-22 | 2020-11-26 | The Regents Of The University Of California | Tree alternating optimization for learning classification trees |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9990558B2 (en) | Generating image features based on robust feature-learning | |
US10936949B2 (en) | Training machine learning models using task selection policies to increase learning progress | |
US20190362222A1 (en) | Generating new machine learning models based on combinations of historical feature-extraction rules and historical machine-learning models | |
US10031945B2 (en) | Automated outlier detection | |
WO2020114022A1 (en) | Knowledge base alignment method and apparatus, computer device and storage medium | |
WO2020098606A1 (en) | Node classification method, model training method, device, apparatus, and storage medium | |
CN110674880A (en) | Network training method, device, medium and electronic equipment for knowledge distillation | |
US8725661B1 (en) | Growth and use of self-terminating prediction trees | |
US7809665B2 (en) | Method and system for transitioning from a case-based classifier system to a rule-based classifier system | |
WO2021089013A1 (en) | Spatial graph convolutional network training method, electronic device and storage medium | |
US20230267381A1 (en) | Neural trees | |
CN110197207B (en) | Method and related device for classifying unclassified user group | |
US11544561B2 (en) | Task-aware recommendation of hyperparameter configurations | |
JP2023138736A (en) | Predictive design space metrics for material development | |
US20180150785A1 (en) | Interaction based machine learned vector modelling | |
US20230027427A1 (en) | Memory-augmented graph convolutional neural networks | |
WO2022227217A1 (en) | Text classification model training method and apparatus, and device and readable storage medium | |
JP2020144852A (en) | Device and method for mixed training meta learning network | |
US20180150784A1 (en) | Machine learned vector modelling for recommendation generation | |
US20220058448A1 (en) | Image selection from a database | |
US20220405570A1 (en) | Post-hoc loss-calibration for bayesian neural networks | |
US9929909B2 (en) | Identifying marginal-influence maximizing nodes in networks | |
US20210192032A1 (en) | Dual-factor identification system and method with adaptive enrollment | |
CN111161238A (en) | Image quality evaluation method and device, electronic device, and storage medium | |
US20220270155A1 (en) | Recommendation with neighbor-aware hyperbolic embedding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GOLDMAN, SALLY;SINGER, YORAM;REEL/FRAME:026135/0985Effective date: 20110407 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20220513 |