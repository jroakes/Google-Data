CN109479098A - Multiple view scene cut and propagation - Google Patents
Multiple view scene cut and propagation Download PDFInfo
- Publication number
- CN109479098A CN109479098A CN201780034730.5A CN201780034730A CN109479098A CN 109479098 A CN109479098 A CN 109479098A CN 201780034730 A CN201780034730 A CN 201780034730A CN 109479098 A CN109479098 A CN 109479098A
- Authority
- CN
- China
- Prior art keywords
- video flowing
- mask
- segmentation
- boundary
- reference picture
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/194—Segmentation; Edge detection involving foreground-background segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
- G06T11/60—Editing figures and text; Combining figures or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/11—Region-based segmentation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/222—Studio circuitry; Studio devices; Studio equipment
- H04N5/262—Studio circuits, e.g. for mixing, switching-over, change of character of image, other special effects ; Cameras specially adapted for the electronic generation of special effects
- H04N5/2625—Studio circuits, e.g. for mixing, switching-over, change of character of image, other special effects ; Cameras specially adapted for the electronic generation of special effects for obtaining an image which is composed of images from a temporal image sequence, e.g. for a stroboscopic effect
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/222—Studio circuitry; Studio devices; Studio equipment
- H04N5/262—Studio circuits, e.g. for mixing, switching-over, change of character of image, other special effects ; Cameras specially adapted for the electronic generation of special effects
- H04N5/272—Means for inserting a foreground image in a background image, i.e. inlay, outlay
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10004—Still image; Photographic image
- G06T2207/10012—Stereo images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
- G06T2207/10021—Stereoscopic video; Stereoscopic image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10024—Color image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10028—Range image; Depth image; 3D point clouds
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10052—Images from lightfield camera
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20076—Probabilistic image processing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20092—Interactive image processing based on input by user
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20092—Interactive image processing based on input by user
- G06T2207/20104—Interactive definition of region of interest [ROI]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2215/00—Indexing scheme for image rendering
- G06T2215/16—Using real world measurements to influence rendering
Abstract
Modified multi-view video stream can be generated based on the effect of depth to multi-view video stream application.The foreground area and the boundary between the background area at the foreground area different depth that user's input can specify the reference picture of the video flowing.It is inputted based on the user, reference mask can be generated to indicate the foreground area and the background area.The reference mask can be used for generating one or more of the other mask, the foreground area of one or more different images of one or more of other different frames and/or different views of the mask instruction from the reference picture and background area.The reference mask and other masks can be used for for the effect being applied to the multi-view video stream to generate the modified multi-view video stream.
Description
Cross reference to related applications
This application claims " the Multi-View Scene Segmentation and submitted on March 17th, 2017
The priority of the S. Utility application serial 15/462,752 of Propagation " (attorney docket LYT287), disclosure
Content is totally integrating herein by reference.
This application claims " the Using Light-Field Image Data for submitted on June 9th, 2016
The U.S. Provisional Application serial number of Background Color Spill Suppression " (attorney docket LYT255-PROV)
62/347,734 equity, the disclosure of which are totally integrating herein by reference.
" the Depth-Based Application of Image submitted this application involves on August 27th, 2015
The US application serial No. 14/837,465 of Effects " (attorney docket LYT203), the disclosure of which is by quoting integrally simultaneously
Enter herein.
This application involves " the Capturing Light-Field Volume Images and submitted on March 29th, 2016
The S. Utility Shen of Video Data Using Tiled Light-Field Cameras " (attorney docket LYT217)
Please serial number 15/084,326, the disclosure of which by reference is totally integrating herein.
Technical field
This disclosure relates to the system and method for handling image data, and be caught more particularly, to for dividing
The system and method for obtaining the light field image to be used in virtual reality or augmented reality application and/or the scene in volume video.
Background technique
It edits routine or light field image and such as changes coloring to provide effect, changes contrast or insertion and/or removal
Object in image can be challenging.In general, how user must be answered on selecting object boundary with control effect with caution
With.Therefore, the application of the effect based on depth can be time-consuming and labour-intensive effort.
Another challenge is presented by making video based on the needs of the modification of depth such as background replacement.When being related to
The process that the difference between foreground elements and background element is drawn when multiple frames can promptly become trouble.For making this segmentation
The known method of automation is restricted significantly.For example, the edge detection and alpha estimation in fringe region, which rely on, makes background face
Color and foreground color separation, this is inaccurate in low contrast regions or in the case where foreground color is similar with background color
True.
As light-field camera or tiling camera array, challenge is aggravated with the video of multiple viewpoints.Such as
Each view in fruit video flowing requires to divide, then must correspondingly repetitive process.The result is that very labour-intensive process.
Summary of the invention
According to various embodiments, the system and method for technology described herein handle image data, such as light field image
Data, so that depth characteristic based on view data realizes various effects.This effect can include but is not limited to replacement one
Or multiple objects, modification exposure levels, modification contrast level, modification saturation levels, modify image color set and/or change
Become the background of image data.The accurate extraction of foreground scene element can be able to achieve more effectively synthesis and/or other views
Feel effect, such as to allow to be mixed into new background (and/or other elements), while overflows color and being minimized with other artifacts.
Specifically, modified video flowing can be generated based on the effect of depth to video stream application.User's input can
With the boundary between the foreground area of the reference picture of specified video stream and the background area of the depth different from foreground area.
This can carry out specified boundary by using bounded frame etc. to complete.For example, 3D bounded frame can be used for the prospect of reference picture
Region is appointed as the part of the image in 3D bounded frame.It can be background area in the part of the image in 3D bounded outer frame portion.
It is inputted based on user, reference mask (mask) can be generated to indicate foreground area and background area.It can refine
Boundary is to calculate the fine boundary of fine reference mask.Fine reference mask can be optionally include foreground area and background area
The ternary diagram (trimap) of zone of ignorance between domain.Zone of ignorance, which may include, may belong to background area or foreground area
Pixel.Zone of ignorance can have based on boundary whether the confidence level in lucky position appropriate can be greater or lesser
Adaptive width.Optionally, it can be inputted via other user, the application of algorithm based on depth and/or based on color
Other refinement is made in the application of algorithm to zone of ignorance.Other views and/or frame can be analyzed in combination with reference picture
Belong to foreground area or background area to assist in the pixel of zone of ignorance.
Reference mask can be used for generating the one or more non-reference different from reference picture that instruction is used for video flowing
The foreground area of image and one or more non-reference masks of background area.Therefore, reference mask (for example, ternary diagram) can be with
Different views and/or different frames are applied to facilitate and generate ternary diagram for the frame.Therefore can via propagated forward (
On time forward) and/or back-propagating (in time backward) from one or more key frames propagate three-dimensional figure.Similarly, may be used
Ternary diagram to be traveled to the different views of frame identical from the image for being computed ternary diagram.Therefore, video can be directed to
All views and/or frame of stream calculate three-dimensional figure, provide input without user for each frame and/or view.
Ternary diagram can be used for generating alpha masking-out (alpha matte) for each view and/or frame.Alpha masking-out can
How to be applied to video flowing for control effect.If video flowing is light field video, can choose alpha masking-out is anti-
Light field is projected to, so that light field can be used to project the new view for applying alpha masking-out.
Therefore, modified video flowing can be generated.Modified video flowing can be shown for viewer, for example, conduct
Virtual reality or a part of augmented reality experience.
Detailed description of the invention
Several embodiments of drawing illustration.Together with specification, they are used to illustrate the principle of embodiment.The technology of this field
Personnel are not intended to limit range it will be recognized that particular embodiment illustrated is only exemplary in attached drawing.
Fig. 1 is the figure of the pipeline according to the description of one embodiment for executing video effect application.
Fig. 2 is the flow chart of the method according to the description of one embodiment for executing video effect application.
Fig. 3 is the figure that the propagation of consecutive frame and/or view of video flowing is divided into according to the description of one embodiment.
Fig. 4 is the propagation according to the consecutive frame and/or view for being further depicted as being divided into video flowing of one embodiment
Figure.
Fig. 5 is the screenshot capture that can specify a mode of foreground object according to the description user of one embodiment.
Fig. 6 is the screen for specifying foreground and background material using the stroke that user draws according to the description of one embodiment
Screenshot.
Fig. 7 is one according to the ternary diagram of the description prospect, background and zone of ignorance (that is, unknown segmentation) of one embodiment
Part.
Fig. 8 is the set for describing the preceding image propagated to ternary diagram using light stream according to one embodiment.
Fig. 9 is the set for the image propagated after being described using light stream according to another embodiment to ternary diagram.
Figure 10 is the combination of the ternary diagram obtained according to the description of one embodiment via propagated forward and back-propagating
The set of image.
Figure 11 be using i at key frame and k at key frame describe the chart of the interpolation of the ternary diagram for frame j.
Figure 12 is being depicted according to one embodiment | i-j |≤| k-j | in the case of combination ternary map values table.
Figure 13 is being depicted according to one embodiment | i-j | > | k-j | in the case of combination ternary map values table.
Figure 14 A to Figure 14 D is according to the top view for adjacent (non-reference) view VA of one embodiment, preliminary ternary
Figure, binary segmentation and final ternary diagram.
Figure 15 is the top view for adjacent (non-reference view) VA according to one embodiment.
Figure 16 is the description alpha masking-out according to one embodiment from multiple views to the step in the back projection of light field
The set of screen shot image.
Specific embodiment
Definition
For the purpose of description provided herein, use is defined below:
Alpha masking-out: the output of figure (alpha matting) process is scratched from alpha.
Alpha scratches figure: may include to these pixels for distinguishing the process of background and foreground pixel in image
Assign transparency.The transparency level assigned can be for example encoded to alpha associated with image channel (alpha
channel)。
A part of the image of the specified theme far from camera in background area-.
Calculate equipment: any equipment comprising processor.
Data storage: any equipment of numerical data is stored.
Depth: the expression being displaced between object and/or correspondence image sample and the microlens array of camera.
Depth map: X-Y scheme corresponding with light field image indicates every in multiple pixel samples in light field image
One depth.
Display screen: any kind of hardware of image can be shown above.
Extended depth-of-field (EDOF) image: the image for focusing object along bigger depth bounds has been processed into it.
A part of the image of prospect segmentation or " foreground area "-specified theme closer to camera.
Frame: one or more images of scene have been used as a part of captured time of video flowing specified.
Image: the respectively pixel value of regulation color or the two-dimensional array of pixel.
Input equipment: any equipment that user inputs can be received to calculate equipment
Light field image: the image of the expression of the light field data captured at sensor.
Lenticule: lenslet, usually similar to one in the array of lenticule.
Multi-view video stream: with the video flowing of multiple views at least one frame.
Modified video flowing: the video flowing that result obtains after to video stream application one or more effect.
Processor: any equipment that can be determined based on data.
Reference mask: instruction is directed to specified any data structure of each of multiple portions of image, including
Ternary diagram.
Ternary diagram: the reference mask in the region of specified image.Ternary diagram for example can specify foreground area, background area
And the zone of ignorance between foreground area and background area.
Zone of ignorance-includes a part that may belong to the image of pixel of foreground area or background area.
Video flowing: a series of images including frame in different times.
View: the viewpoint and/or view orientation of scene are observed.
In addition, being easy for nomenclature, in this paper, we refer to image capture device or other numbers for term " camera "
According to acquisition equipment.This data acquisition facility, which can be, indicates field for obtaining, recording, measure, estimate, determine and/or calculate
Any equipment or system of the data of scape, including but not limited to two-dimensional image data, 3 d image data and/or light field data.
This data acquisition facility may include the data for obtaining expression scene for using technology well known in the art
Optical device, sensor and image procossing electronic device.Those skilled in the art will recognize that can make together with the disclosure
With the data acquisition facility of many types, and the present disclosure is not limited to cameras.Therefore, term " camera " use purport herein
To be illustrative and exemplary, and should not be considered as limiting the scope of the present disclosure.Specifically, this term is herein
Any use any suitable equipment for being considered as referring to for obtaining image data.
In the following description, several technology and methods for handling light field image are described.The technology of this field
Personnel are it will be recognized that these various technology and methods can be executed individually and/or according to any suitable combination each other.
Framework
In at least one embodiment, it can be realized together with the light field image captured by light-field capture equipment herein
Described in system and method, the light-field capture equipment includes but is not limited to Ng et al., Light-field photography
with a hand-held plenoptic capture device,Technical Report CSTR 2005-02,
Described in Stanford Computer Science those.In " the Depth-Based submitted on the 27th of August in 2015
It is shown in the US application serial No. 14/837,465 of Application of Image Effects " (attorney docket LYT203)
And exemplary light field concept, optical hardware and computing hardware are described, the disclosure of which is integrally incorporated in herein by reference
In.Hardware such as light-field camera that can be disclosed herein receives and processes and realizes this on the after-treatment system of light field data
The system and method for described in the text.
Additionally or alternatively, the video that can be such as captured by the tiled arrays of camera together with volume video data
Data realize system and method described herein together.In " the Capturing Light- that on March 29th, 2016 submits
Field Volume Images and Video Data Using Tiled Light-Field Cameras " (attorney
Number LYT217) S. Utility application serial 15/084,326 in exemplary camera array concept, light has shown and described
Hardware and computing hardware are learned, the disclosure of which is totally integrating herein by reference.System and method described herein
It can such as be tiled by associated hardware and camera array or capture systems and/or receive and process the rear of volume video data
Reason system is realized.
Effect application
In the presence of can be based on many effects that the depth of the object in image is valuably applied.For example, it may be desired to replace figure
The presence closer to or far from the additional source of light of the object of camera is only irradiated in the background or prospect of picture, or simulation.
The application of this effect may be challenging for single image.To such as light field image or by tiling phase
There are additional challenges for the multi-view image of machine array institute captured image.Change when to be applied to multiple frames so as to video
When the format such as video presentation used in virtual reality or augmented reality application, these challenges are aggravated.
Disclosure offer can be used to the system and method based on depth segmentation image.For example, in order to execute background replacement, it can
To divide the image into the foreground part comprising the object in display foreground, the background parts comprising the object in image background
And/or comprising can be in the unknown portions of the object in the prospect and/or background of image.Then segmentation can be traveled to and is come from
The image of alternative view and/or alternative frame in order to provide video flowing segmentation.Upon splitting, it can be easily performed and such as carry on the back
The operation of scape replacement.
Multiple view divides pipeline and method
Fig. 1 is the figure of the pipeline according to the description of one embodiment for executing video effect application.As elucidated earlier
, it can be to the multi-view video application effect of such as light field video and/or the video captured with tiling camera array.As institute
Show, video flowing 100 can be optionally multi-view video stream, and may include multiple images, each of these is corresponding
In particular frame and/or view.
Video flowing 100 may include various types of data, the data can include but is not limited to color data 102,
Depth data 104 and exercise data 106.Color data 102 can be to each pixel of each image for video flowing 100
Color is encoded, and can have the suitable format of RGB or any other.Depth data 104 may include instruction video
The each pixel and/or partial distance of each image of stream 100 capture the depth of the depth of the imaging sensor of video flowing 100
Figure or other data structures.Exercise data 106 can indicate the variation of the correspondence image from the image of each frame to frame in succession.
One or more images of video flowing 100 can undergo segmentation, and wherein the different segmentations of video flowing 100 are carved fixed
To facilitate the application of effect.For example, segmentation can be binary segmentation 110, wherein image is divided into two segmentations, such as shows
Foreground area relatively close to the object of camera and the background area for being relatively far from the object of camera is shown.User's input
120 can be used for facilitating this delimitation process.
Binary segmentation 110 may include initial binary segmentation 130 for each image, wherein in foreground area and background area
Boundary 132 is described between domain.In addition, binary segmentation 110 may include fine binary segmentation 140 for each image, wherein side
Boundary 132 is refined the form to take the fine boundary 142 between foreground area and background area.
Binary segmentation 110 can be used for constructing ternary diagram 150, the ternary diagram 150 can be for each image from
Adapt to ternary diagram.Ternary diagram 150, which can be, to be made image be divided into foreground area and background area and is located at foreground area and background
The figure of the image of zone of ignorance between region.Zone of ignorance may include can be suitably by foreground area or background area point
The pixel of group.In some embodiments, ternary diagram 150 can be specified prospect (such as being encoded to 255), background (such as encodes
0) and the mask in unknown (such as being encoded to 128) region for.Zone of ignorance, which can be defined on, makes foreground pixel and background pixel point
From boundary around pixel piece, wherein the foreground/background about respective pixel specify exist it is uncertain.
Figure 160 can be scratched to the application of ternary diagram 150, this can lead to the generation of foreground image 165, the foreground image
The alpha for the degree that 165 images and/or indicating effect that can be only expression foreground area will be applied to each image is covered
Version 170.Foreground image 165 and/or alpha masking-out 170 can be used for effect being applied to video flowing 100, so as to cause modification
The generation of video flowing 180 afterwards.The generation of alpha masking-out 170 can be the main target of image segmentation.
In some embodiments, alpha masking-out 170 can be the floating-point of the part for belonging to the following terms of identification image
Image:
Prospect (alpha value 1)；
Background (alpha value 0)；And
" soft " edge between prospect and background.Due to motion blur or material properties, some marginal values will have
Partial transparency.It that case, can estimate the alpha value between 0 and 1.
Fig. 2 is the flow chart of the method according to the description of one embodiment for executing video effect application.Optionally may be used
To use data structure shown in Fig. 1 the method that executes Fig. 2.Alternatively, it can use different data structures to hold
The method of row Fig. 2.Similarly, the data structure of Fig. 1 can be used together with the method different from those of Fig. 2 method.With
Lower description assumes that the method for Fig. 2 utilizes the data structure of Fig. 1.
For example, can use the post processing circuitry for handling light field image and/or volume video data to execute Fig. 2's
Method.In some embodiments, this method can be executed by calculating equipment；This calculating equipment may include desktop computer,
One or more of laptop computer, smart phone, plate, camera and/or the other equipment for handling digital information.
Method can be since step 200, and video flowing 100 is for example by camera or the sensing of camera array in step 200
Device capture, as disclosed in being incorporated by reference into patent application herein.Light field video is captured, it can be by making
Light field, which is focused on again at each position of virtual aperture, calculates multiple views.It can be sent out after the capture of video flowing 100
This is done in raw one or more steps.
In step 210, video flowing 100 can be received in calculating equipment, the calculating equipment can be camera or phase
A part of machine array.Alternatively, calculating equipment can be as the institute in being such as incorporated by reference into patent application herein
It is the same in disclosed individual after-treatment system to be separated with camera.This calculating equipment is hereinafter referred to as " system ".
In a step 220, the user that system can receive the reference picture of video flowing 100 specifies, and the reference picture will
It is used for initial binary segmentation.User can for example choose with prospect and back be clearly defined and/or easily recognizing
The image of scene area.In alternative embodiment, reference picture can be automatically selected by system.In some embodiments, it may be possible to
The reference picture and/or other images for being directed to video flowing 100 calculate each frame depth or disparity map and/or record it is every
The light stream image of the accurate motion of a pixel.
In step 230, system can receive specified to the user on the boundary between foreground area and background area.This is used
Family is specified can draw around one or more elements for example by user in foreground area and/or background area one or
Multiple elements such as bounded frames is made.It can be specified in step 240 using this user to calculate the initial binary segmentation of Fig. 1
130。
Optionally, boundary 132 can be jointly constituted by the element that user draws.It in step 250, can be to boundary
132 are refined to provide fine boundary 142, to provide fine binary segmentation 140.Optionally, can be used figure cutting and/
Or other computer vision techniques are defined and/or are refined to boundary 132 and/or fine boundary 142.It optionally, can be with
Future, the Depth cue of depth map of self-reference image was included in probabilistic model to refine background area and foreground area and have
The edge of similar color.
In step 260, ternary diagram 150 can be calculated based on fine binary segmentation 140.As described above, ternary diagram can be with
Including the zone of ignorance between foreground area and background area.In step 270, nomography can be scratched to the application of ternary diagram 150
To generate the alpha masking-out 170 for being used for reference picture.Colouring information from adjacent view may be used to prospect component and back
Scape component separation, to reduce the quantity of the unknown number in synthesis equation and significant improve the alpha masking-out that result obtains
170 accuracy.In some embodiments, alpha masking-out 170 can be each pixel of the image of instruction video flowing 100
The figure pixel-by-pixel of transparency.Alpha masking-out 170 can be encoded into the channel alpha for being used for video flowing 100.Alpha masking-out
170 generation can be related to using the color data 102 of video flowing 100 and ternary diagram 150 as input.
In inquiry 280, all frames and the view whether processed determination about video flowing 100 can be made.Such as
Fruit does not have, then in step 290, new images (" first can be always selected in the different frames and/or view of self-reference image
Non-reference picture ").In step 291, the ternary diagram 150 calculated in step 260 can be traveled into new images.It can phase
Step 240, step 250, step 260 and step 270 are repeated for non-reference picture, and can be selected again in step 290
Select continuous non-reference picture, and handle these non-reference picture as described above, until inquiry 280 be answered affirmatively for
Only.
In some embodiments, each non-reference picture can be automatically selected in step 290 by system, and can be with
It is processed in the case where not further user inputs in step 240, step 250, step 260 and step 270.In alternative
In embodiment, user can provide input according to one or more of step 290 selection new images.It optionally, can be with
Step 230 is repeated for each new images, so that user provides input to facilitate properly generating for initial binary segmentation 130.
In some embodiments, step 240, step 250, step 260 and step can be omitted for each non-reference picture
Rapid one or more of 270.For example, the ternary diagram 150 of reference picture can be used for according to step for the first non-reference picture
Rapid 260 calculate ternary diagram 150.This can be in the feelings for not having to execute step 240 and/or step 250 for new non-reference picture
It is completed under condition.
Non-reference picture may be selected so that they from from those of previously processed image view and/or frame
Adjacent view and/or frame, so that can be in step 260 using the ternary diagram of adjacent image 150 come for new non-reference picture
Generate ternary diagram 150.For example, light stream is applied between new non-reference picture and previously processed image by (1), and/or
(2) the extrinsic parameter with new non-reference picture and/or previously processed image is used in conjunction with depth data 104, can
According to the previous ternary diagram extrapolation each ternary diagram 150 to be generated.
In some embodiments, step 240 and/or step 250 can only partially be executed.For example, due to movement and/or
Depth estimation error, the new ternary diagram propagated in step 291 for new non-reference picture to new non-reference picture come
Say to may be inaccuracy；This inaccuracy can make stingy nomography fail in step 270.In order to solve these inaccuracy
Property, it can be only for the zone of ignorance of the ternary diagram 150 generated via the extrapolation according to the ternary diagram of previously processed image
Execute step 240 and/or step 250.Then the ternary diagram of revision can be calculated with operating procedure 260 and step 270, this can be with
Stingy nomography is caused preferably to run in step 270.
Fig. 3 is the figure that the propagation of consecutive frame and/or view of video flowing 100 is divided into according to the description of one embodiment
300.For example, from reference picture 310, segmentation can back-propagating in time to adjacent previous non-reference picture 320,
And/or it is propagated forward to adjacent subsequent non-reference picture 330 in time.Additionally or alternatively, from reference picture 310
It rises, segmentation can be transmitted to the first adjacent view non-reference picture 340 and/or the second adjacent view non-reference picture 350.
Similarly, from adjacent previous non-reference picture 320, segmentation can be transmitted to the first adjacent view non-reference
Image 360 and/or the second adjacent view non-reference picture 370.From adjacent subsequent non-reference picture 330, segmentation can be passed
It is multicast to the first adjacent view non-reference picture 380 and/or the second adjacent view non-reference picture 390.
Fig. 4 is the propagation according to the consecutive frame and/or view for being further depicted as being divided into video flowing 100 of one embodiment
Figure 40 0.It can be propagated in any order using view and/or the time propagates.As indicated, from the reference-view V in time tR
The ternary diagram 410 of the reference picture at place, view propagation can cause to be used for alternative (that is, non-reference) view VAInitial ternary diagram
420 generation.Binary segmentation 430 can be executed to generate for view VAAdaptive ternary diagram 440, can be used for generating
For view VAAlpha masking-out 170.Similarly, from ternary diagram 410, time propagation can cause at subsequent frame (in the time
T+1 view V)RInitial ternary diagram 450 generation.Binary segmentation 460 can be executed to generate in time t+1 for view VR
Adaptive ternary diagram 470.
It, can will be in step in step 292 in conjunction with Fig. 1 referring again to FIGS. 2, once all views and/or frame are processed
It is each image in rapid 270 and 170 back projection of alpha masking-out that generates is to the light for linking together the view of video flowing 100
Field or other data.Known view geometry can be used in conjunction with depth map to project zone of ignorance and calculate for institute
There is back projection's alpha masking-out of other views.This, which can be convenient, is suitably applied to video flowing 100 for alpha masking-out 170.?
In step 294, alpha masking-out 170 and/or back projection's alpha masking-out can be used by effect and be applied to video flowing 100 with life
At the modified video flowing 180 of Fig. 1.It optionally, can be by single-view alpha masking-out back projection to 4D light field so that can be real
Existing 4D synthetic work process, wherein the light field that capture and computer generates is mixed together to form final image.
In step 296, modified video flowing 180 can be displayed for a user.This can be for example by such as passing through
Show that modified video flowing 180 is complete on the display screen those of being incorporated by disclosed in patent application herein
At.Optionally, method can be a part of virtual reality or augmented reality experience.Then method can terminate.
The method of Fig. 2 only can be used for many possible methods to the video stream application effect of such as multi-frame video stream
One of.According to various alternatives, each step of Fig. 2 can be performed in a different order, omit and/or by other
Step replacement.For example, other image processing steps can be incorporated into the method for Fig. 2, at any stage of method, and
It can be performed before, during and/or after application effect relative to image.
Each of the step of method of Fig. 2 can be executed in various ways within the scope of this disclosure.It will be below
It illustrates in greater detail and describes to execute some exemplary approach in these steps.Hereafter it is only exemplary；It can
With the step of executing Fig. 2 in from hereafter significant different modes.
Binary segmentation
It can be shown in graphical user interfaces to input initiation binary segmentation via user as in step 230
Show reference-view.Then artist can define 3D bounded frame in one or more portions of image.For example, for background
Replacement, artist can will define 3D bounded frame around one or more foreground objects from background segment.This can pass through
It is completed for two opposite corners regulation minimum and maximum X, Y and Z values of frame.
Fig. 5 is the screenshot capture 500 that can specify a mode of foreground object according to the description user of one embodiment.
As indicated, image may include prospect 510 and background 520.3D bounded frame 530 can be placed as described above with restriction prospect 510,
To provide the initial boundary for being used for initial binary segmentation, the initial boundary can be the side of the initial binary segmentation 130 of Fig. 1
Boundary 132.
Then iteration diagram cutting can be used to refine this initial binary segmentation 130.First step can be using initial
Binary segmentation 130 trains four probability distribution from initial binary segmentation 130, is each represented as gauss hybrid models:
1. foreground color probability PFG,C；
2. background color probability PBG,C；
3. foreground depth probability PFG,D；And
4. background depth probability PBG,D。
Depth probability distribution (above the 3rd and No. 4) can be introduced into binary segmentation.One group of reality can be used in user
It is worth weight WCAnd WDTo assign the relative importance of depth or colouring information.User can be optionally based on him or she to depth map
And/or the confidence level of color image changes these weights.
Using this definition, given pixel p can be belonged to the definition of probability of prospect are as follows:
It can be by WCAnd WDIt is provided that
0 < wC1: 0 < w of <D< 1:wD+wC=1
Background probability can be defined in an identical manner.
Then global minimization can be executed to refer to find based on probabilistic model above for the new label of each pixel
Send (prospect or background).Smoothness term can be used for promoting the coherence in the region of similar color.Other than existing method,
The depth value of neighborhood pixels can be incorporated herein in smoothness term, so that the region of similar depths will often have consistent mark
Note.
The min-cut algorithm that generation is finally divided can be used to complete the final minimum.It can repeat this process
(probability Estimation and figure cutting) to obtain one group of new segmentation tag, usually until until the convergence of these segmentation tags.As a result may be used
Be Fig. 1 fine binary segmentation 140 generation, as described in the step 250 of Fig. 2.
User can choose to draw stroke and corrects by drawing on the known region for belonging to prospect and/or background area
This segmentation.The each pixel drawn under stroke can be correspondingly marked, so that the not optimised process change of user-defined label,
But for training probabilistic model.
Fig. 6 is the screen for specifying foreground and background material using the stroke that user draws according to the description of one embodiment
Screenshot 600.As indicated, red stroke 6 30 can be used to specify the part for the image for belonging to prospect 510 in user.Similarly, may be used
The part for the image for belonging to background 520 is specified to use blue pen stroke 640.As set forth above, it is possible to not cut the phase in iteration diagram
Between modify these titles.
Ternary diagram generates
Fine binary segmentation 140 can be used for calculating ternary diagram 150, as in the step 260 of the method for Fig. 2.Three
First Figure 150 can be each pixel and be marked as the one ternary image belonged in following three regions: prospect, background and not
Know.Mistake can appear in initial binary segmentation 130 and/or fine binary segmentation 140, especially when boundary 132 and/or essence
When low contrast regions are crossed on thin boundary 142.The thickness for being dynamically selected ternary diagram 150 can be measured based on Edge Distance, made
Obtain the actual edge that zone of ignorance correctly includes image.
Fig. 7 is according to the description prospect 510 of one embodiment, background 520 and zone of ignorance 720 (that is, zone of ignorance)
A part of ternary diagram 700.As indicated, zone of ignorance 720 can be such that prospect 510 separates with background 520.Calculate ternary diagram 700
It may include unknown to position using the binary segmentation edge 710 that can be the fine boundary 142 from fine binary segmentation 140
Region 720.
The width of zone of ignorance 720 can be adaptive, and reason is that it can change along the length of zone of ignorance 720
Become.It spread out for example, zone of ignorance 720 can have to adapt to close to such as practical soft edges 750 in binary segmentation edge 710
Feature ternary diagram edge 730, the feature can be the high contrast characteristics close to low contrast regions 740.
In some embodiments, following procedure can be used:
1. minimum (the t that zone of ignorance 720 can be arranged in user as unit of pixelmin) and maximum (tmax) thickness.
2. can detecte the high-contrast edges in image.It in form, can will be at pixel I relative to neighborhood pixels j
Contrast value is defined as:
3. threshold process can be carried out to C to form binary edge image C', so that C'(i) in high-contrast edges etc.
In 1, otherwise it is equal to zero.
4. distance map D can be calculated according to bianry image C'.For each pixel I, D (i) be may include in C' most
The distance of nearly high-contrast edges.Consider for speed, can make E fuzzy with triangle filter.This can be at each edge
Surrounding creation instruction current pixel is from edge slope section how far.Another more accurate option is to calculate distance according to C' to become
It changes.
5. then final ternary diagram image T can be calculated as indicated by exemplary pseudo-code:
T=' prospect ' or T=' background ' is initialized according to binary segmentation
For each pixel i on binary segmentation edge:
Setting T (i)=' unknown '
If D (i) is less than t max:
It is arranged r=max (D (i), t min)
For each pixel j in the disk of the radius r around i:
Setting T (j)=' unknown '
Final ternary diagram image can have zone of ignorance correctly comprising nearest high-contrast edges, wherein feature pair
Claim section in binary segmentation perimeter.It is worth noting that, other than the color edges in zone of ignorance 720, comparison diagram
The definition of picture C (i) may also be ensured that depth edge is correctly captured, while ensure zone of ignorance 720 only as required
It is thick.This, which can be convenient to execute in the step 270 of Fig. 2, scratches figure.
Parameter tminAnd tmaxIt can control the final shape of zone of ignorance 720.They can be adjusted by user with tighten or
Person extends zone of ignorance 720 to realize desired result.
Time propagates
As previously mentioned, the ternary diagram 150 calculated for reference picture can temporally be traveled to video flowing
100 different frames.It can continue this to propagate until all frames of video flowing 100 have all been divided.
In some embodiments, following procedure can be used:
1. user can divide the image of several frames from video flowing 100, such as in step 230, step 240, step 250
As in step 260.Each of these images can be referred to as " reference picture ", and the correspondence frame of video flowing 100
" key frame " can be referred to as.
2. interpolation can be carried out the ternary diagram 150 to each image in the frame between key frame as follows:
A. light stream timely propagated forward ternary diagram from each key frame can be used.
It b. can the ternary diagram of back-propagating in time.
C. it can be grouped together before to ternary diagram and backward ternary diagram.
3. user can be by editing segmentation for intermediate frame duplicate customer auxiliary partition, the intermediate frame can be defined
New key frame.The process can be repeated until finally dividing satisfactory for each image of each frame.
Fig. 8 and Fig. 9 describes communication process.Specifically, Fig. 8 is preceding to three according to being described using light stream for one embodiment
The set 800 for the image that member figure is propagated.Key frame i includes the reference picture for having calculated ternary diagram 810, to show prospect
510, background 520 and zone of ignorance 720.It can have correspondence to the key frame i+3 of first three frame in time from key frame i
Ternary diagram 840.
It can be raw to the key frame i+1 of previous frame in time from key frame i to the application light stream of ternary diagram 810
At ternary diagram 820.Similarly, can to ternary diagram 820 application light stream with for from key frame i in time to first two frames
Key frame i+2 generates ternary diagram 830.Therefore, can by propagate forward in time come for key frame i and key frame i+3 it
Between frame i+1 and frame i+2 generate ternary diagram.
More precisely, propagated forward can be executed in the following manner:
1. distorting the ternary diagram 810 from frame i to frame i+1 using light stream.As a result the ternary diagram 820 obtained is imperfect
's；Light stream possibly can not illustrate the occlusion area in the frame i disclosed in frame i+1.
2. executing binary segmentation for only zone of ignorance 720 as in step 230, step 240 and/or step 250.
This, which can be used according to prospect 510 and the probabilistic model of the calculating of background 520, estimates binary segmentation.
3. calculating ternary diagram for frame i+1 as in step 260.
4. repeating above step one to three since frame i+1, stop if frame i+1 is key frame.
Fig. 9 is the set 900 for the image propagated after being described using light stream according to another embodiment to ternary diagram.It is crucial
Frame i includes the reference picture for having calculated ternary diagram 810, shows prospect 510, background 520 and zone of ignorance 720.From key frame i
Rising can have corresponding ternary diagram 840 to the key frame i+3 of first three frame in time.
It can be used that (light stream estimated from frame i to frame i-1) comes in a manner of identical with propagated forward in inverse direction
Define back-propagating.Can to ternary diagram 840 application light stream with for from key frame i+3 in time to the key of following frame
Frame i+2 generates ternary diagram 930.Similarly, can to ternary diagram 930 application light stream with for from key frame i+3 in time to
The key frame i+1 of latter two frame generates ternary diagram 920.As propagated forward, ternary diagram the blocking in frame i that as a result obtains
Region may be faulty in the case where being revealed in frame i-1, and can be refined on demand more accurately to divide not
Know region 720.It therefore, can be by back-propagating in time come the frame i+1 and frame i between key frame i and key frame i+3
+ 2 generate ternary diagram.
It is possible inconsistent in the three-dimensional figure 150 propagated there are the position forward and backward of light stream inaccuracy.Such as above
Step 2 (c) in it is the same, the target that forward and backward ternary diagram 150 is combined can be in order to coordinate two propagation
Ternary diagram 150 to form final segmentation.This further will show and describe about Figure 10.
Figure 10 is the combined figure for propagating the ternary diagram 150 obtained via forward and backward according to the description of one embodiment
The set 1000 of picture.Specifically, the ternary diagram 810 and ternary diagram for key frame i.e. frame i and frame i+3 of Fig. 8 and 9 is depicted
840.In addition, further depict propagated forward in fig. 8 ternary diagram 820 and ternary diagram 830 and in Fig. 9 back-propagating three
First Figure 92 0 and ternary diagram 930.It can combine both for the ternary diagram 820 of frame i+1 and ternary diagram 920 in terms of for frame i+1
Calculate combination ternary diagram 1020.Similarly, it can combine both for the ternary diagram 830 of frame i+2 and ternary diagram 930 for frame
I+2 calculates combination ternary diagram 1030.
Figure 11 be using i at key frame 1110 and k at key frame 1120 describe the ternary diagram for frame j 1140
Interpolation chart 1100.Additional frame 1130 can be between the key frame 1120 at k and frame j 1140.For every at frame j
A pixel p, can be used (1) from key frame i propagated forward triple TFjAnd (2) back-propagating from key frame k
Triple TBjTo calculate a combination thereof triplet Tj(p).Whether this value can depend on frame j near frame i or frame k.It can be with
It is calculated in the case where j is near frame i according to the table 1200 of Figure 12, is otherwise calculated according to the table of Figure 13 1300.
Figure 12 is being depicted according to one embodiment | i-j |≤| k-j | in the case where combination ternary map values.Figure 13
Being depicted according to one embodiment | i-j | > | k-j | in the case where combination ternary map values table 1300.
This mode for calculating ternary map values can have following benefit:
Image border week especially can be captured in the case where forward and backward propagates incongruent situation in zone of ignorance
The uncertainty enclosed；And
If one in the ternary diagram pixel only propagated be it is known, can by nearest key frame carry out
Priority ranking inputs to abide by user.
It can be by being answered in the ternary diagram 1020 of combination ternary diagram such as Figure 10 or the zone of ignorance 720 of ternary diagram 1030
With step 230, step 240 and/or step 250 come further sub-combination ternary diagram.Fine binary segmentation 140 can be executed
Step 260 calculates final ternary diagram 150.
View is propagated
Up to the present, segmentation is calculated only for reference-view.Using at reference-view calculate depth map and
Segmentation can be traveled to the view by the relative position of another view.Following exemplary method assumes the foreground object to be divided
It is fully visible from reference-view.
Figure 14 A to Figure 14 D is according to one embodiment for adjacent (non-reference) view VA1450 top view, just
Walk ternary diagram, binary segmentation and final ternary diagram.As shown in fig. 14 a, point XA1410 may be due to background 1420 and prospect
The depth of boundary between 1430 is discontinuously and in reference-view VRIt is blocked in 1460.However, new ternary diagram will be generated
150 adjacent view VAIn 1450, point XA1410 may be visible.As indicated, background 1420 and prospect 1430 can be
Behind the plane of delineation 1440.
Although in adjacent view VAIn 1450, p is projectedA1470 in preliminary ternary diagram TAIt is arranged to unknown in 1480,
As shown in Figure 14 B, but right value can be detected during binary segmentation 1482, as shown in Figure 14 C.It can be final
Three bitmap TABoundary is eliminated in ' 1484, as shown in fig. 14d.
More specifically, given reference-view VR1460 and adjacent view VA1450, it can be by using reference-view TRPlace
Ternary diagram and depth map DR(not shown) calculates final ternary diagram TA'1484.Following procedure can be used:
1. by preliminary ternary diagram TA1480 are initialized as " unknown ".
2. being directed to reference-view VREach pixel p in 1460R:
A. it uses in pRLocate the depth map D of samplingRTo calculate its 3D view spaces position XR。
B. by XRIt is converted into adjacent view VASpace.This can provide the position 3D XA。
C. by XAProject to the pixel p of adjacent viewA。
D. by TA (pA) it is set as TR (pR) value.
3. only in TAZone of ignorance 720 in applying step 230, step 240 and/or step 250 to estimate binary segmentation
1482.Depth map DAIt is not assumed to can be used for VA.It, can be only for V if it is unavailableAIt is walked using color image
Binary segmentation 1482 in rapid 3.
4. applying step 260 is in binary segmentation 1482 to estimate final ternary diagram TA’1484。
VABlocking and in V of may discontinuously locating due to depth of some pixelsRIn it is invisible.It can be in above step 1
In preliminary ternary diagram TAIt is defaulted as these pixels in 1480 and assigns " unknown " value.If depth continuously crosses over true foreground/back
Scape boundary, then this can be right value, but if zone of ignorance 720 completely inside the prospect of image 510 or background 520, then
It may be incorrect.This is the situation described in Figure 10.
Right value can be assigned by above step 3 and step 4.Two generated in above step 3 can be made in
Member divides 1482 to ensure the consistency in adjacent domain.Work as pANearest-neighbors when being all classified as " background ", this will
" closing " boundary and in final ternary diagram TARight value is set in ' 1484.If depth discontinuously only in prospect 510 if phase
Same solution can be applicable in.
Multiple view scratches figure
According to the step 270 of the method for Fig. 2, alpha masking-out 170 and prospect picture can be estimated in zone of ignorance 720
Element value.If such as scratching figure Laplce (matting Laplacian) in the presence of the stem algorithm done so to single view.It uses
Information from adjacent view, which can permit, estimates accurate alpha value for alpha masking-out 170.Image scratches the problem of figure can
Meet the given pixel color I of following synthesis equation to be and find real value alpha (α) and color value F (prospect) and B (back
Scape):
I=α F+ (1- α) B
F, B and α can be usually estimated together, this may need to estimate seven unknown parameters.In alternative, Ke Yicong
Restore B in multiple views, this can eliminate three unknown numbers (each one of each Color Channel) in equation.
For example, adjacent view VAIt can be seen below with a part on the boundary of zone of ignorance 720.It can be by by VAAnd VR's
Relative pose relatively detects this situation compared with the orientation on the boundary of zone of ignorance 720.Figure 11 describes VAFrom left sides side
The case where boundary.
Figure 15 is according to one embodiment for adjacent (non-reference view) VA1450 top view 1500.Figure 15 is retouched
Element identical with Figure 14 A is drawn, while being added to zone of ignorance 1510 and coming from reference-view VR1460 projection pR
1520。
It can be by all colours sample that will be blocked by prospect 510 from reference-view VR1460 project to the plane of delineation
1440 restore background sample.This is considered two step process:
1. depth is propagated.Can the known relative position based on two views by depth map from VRAgain V is projected toA.In depth
It spends in figure and may exist " hole "-in VRIn be blocked but in VAIn visible region.It can be according in spatial domain and property field
Highly relevant neighborhood pixels insert depth value for this some holes in the two.
2. projecting again.It can be by VA3D point project to V againRImage pane.It can be blocked now from VR " seeing "
Object.
Depth value can be inserted in hole as described in above step 1.Bore region can be VRIn quilt
Occlusion area；Therefore, depth value here can be closer to neighbouring background pixel.It is as follows that iterative diffusion process can be used:
It A., can be by following formula come estimating depth value z for the pixel p in bore regionp:
zp=zqD if (p, q) < εs&(||Ip-Iq| | < εc)&q∈B
B. pixel p can be labeled as " filling ".If εsAnd εcIt is not small enough, then it can update εsAnd εc, and can benefit
With updated εsAnd εcRestart iterative diffusion process.
Otherwise, iterative diffusion process can be stopped.
Then the background sample restored can be utilized in the stingy legend line program of step 270.Expression formula (x, y, zB,
IB) it can be used to indicate that projected color sample again in the plane of delineation of VR, (x, y, zF, IF) it is used as primitive color sample.So
Afterwards, sample will can be projected again to compare with original sample.There may be three kinds of situations:
1: α (x of situationi, yi) > 0, if | | zB(xi, yi)-zF(xi, yi) | | >=εz。
2: α (x of situationi, yi)=0or α (xi, yi)=1,
If | | zB(xi, yi)-zF(xi, yi) | | < εzAnd | | IB(xi, yi)-IF(xi, yi) | | < εc。
Situation 3: pixel
xi, yi||zB(xi, yi)-zF(xi, yi) | | < εz and||IB(xi, yi)-IF(xi, yi) | | >=εcIt is outlier.
In situation 1, the quantity of the unknown number of pixel can be reduced from seven to four.For situation 2, it can reduce unknown
The area in region 720 and the quantity for therefore reducing the pixel to be estimated.This can produce the more accurate of prospect and alpha value
Estimation.
In some embodiments, final alpha value can be estimated in two step process.It can be by being looked for for image pixel I
Initial estimation is calculated to " best match " of prospect F and background B color card.It, can be with for each expected sample F and B
" best match " is defined in the sense that making following cost function minimization:
C (α)=βcolor||I-αF-(1-α)B||+βdistance(dist (I, F)+dist (I, B))
Wherein,Dist (X, Y) is that the Europe between two image patterns is several
Reed distance, and βcolorAnd βdistanceIt is to apply in color and between mismatch the constant punished relatively respectively.
If background sample B is successfully recovered, it can be used to reduce search space significantly.Specifically, may be used
It can need the best prospect sample F according only to cost function C.It can be globally (for the best match in entire foreground area)
Or it is locally scanned for (along prospect/zone of ignorance boundary).Then, addition makes alpha value according to local color affinity
Smooth post-processing step can be advantageous.Depending on quality level required for user, " wave filter " can be used
Algorithm or " scratching figure Laplce " algorithm.
Foreground color estimation
After estimating final alpha masking-out, it can carried on the back according to above-cited stingy figure equation I=α F+ (1- α) B
Foreground color F is readily calculated when known to scape color B.
When cannot be according to background color is restored in multiple view priori, in addition to alpha can be fixation in this case
, most " possible " the foreground and background color pair for minimizing cost function C as stated above can be found out.This
It can produce the final foreground color for the estimation of entire zone of ignorance.
Final foreground image can be for example formed by following steps:
Black is set by pixel inside known background region；
In zone of ignorance, the foreground color estimated above is used；
In known foreground area, the pixel from source images is directly used.
4D synthesis
The alpha masking-out 170 and foreground image generated as described above can be 2D, and therefore can only be used to synthesize other
2D image.Multiple views of same scene can be generated in the multiple view camera configuration of such as light-field camera.Once being adjacent view
Figure obtains alpha masking-out 170 and foreground image, so that it may by alpha masking-out 170 and foreground image back projection to single RGBA
Light field.This, which can permit, uses the light field technology for such as focusing and projecting again for any viewpoint.
Figure 16 is to scheme back projection of the description image from multiple views to light field using multiple alpha according to one embodiment
The step of screen shot image set 1600.This description is used for the general work process of back projection, passes through the general work
Process can be with back projection's foreground image and alpha figure to re-create 4 components R GBA light fields.As indicated, 4D light can be captured
1610, and 4D light field 1610 can be used calculating as a series of M × M sub-aperture images 1620 and show a series of
View.It can be that each of sub-aperture image 1620 generates figure (being in this case alpha figure), so as to cause M × M
The generation of a alpha Figure 163 0.Similar process can be executed to generate M × M foreground picture.It is also based on 4D light field 1610
Depth map 1640 is generated, and can be made in the multiple view back projection of the channel alpha or alpha Figure 163 0 to 4D light field 1610
With depth map 1640.This can lead to the generation of RGBA light field 1650, and the RGBA light field 1650 includes now can be together with color
And/or depth data is projected to the channel alpha in any sub-aperture image together.
Above description and the attached drawing of reference are relative to possible embodiment setting forth specific details.Those skilled in the art
It will be appreciated that technology described herein can be practiced in other embodiments.Firstly, the specific name of component, term
Capitalization, attribute, data structure or any other programming or configuration aspects are not enforceable or important, and are realized herein
The mechanism of the technology of description can have different titles, format or agreement.Furthermore it is possible to as description via hardware and
System is realized in the combination of software in software element completely in hardware element or completely.In addition, described herein
Functional particular division between various system components is only exemplary, rather than enforceable；By individual system group
The function that the function that part executes can alternatively be executed by multiple components, and be executed by multiple components can be alternatively by single
Component executes.
Mean in the description to " one embodiment " or to the reference of " embodiment " together with these embodiments
The a particular feature, structure, or characteristic of description is included at least one embodiment.The phrase " in one embodiment " is in this theory
The appearance in each place in bright book is not necessarily all referring to the same embodiment.
Some embodiments may include the system or method for individually or in any combination executing above-mentioned technology.Its
Its embodiment may include computer program product, and the computer program product includes being encoded on medium for making to calculate
Processor in equipment or other electronic equipments executes the non-transitory computer-readable storage media and computer of above-mentioned technology
Program code.
It is presented in terms of the algorithm of the operation to the data bit in the memory for calculating equipment and symbol expression above-mentioned
Some parts.These algorithm descriptions and indicate be data processing field technical staff using carrying out the most effectively work by them
Essence be communicated to the means of others skilled in the art.Algorithm is envisioned for being that expectation is caused to be tied herein and generally
Self concensus sequence of the step of fruit (instruction).These steps are to need those of physical manipulation of physical quantity step.In general, to the greatest extent
Pipe not necessarily, however this tittle take can by storage, the electricity for transmission, combining, comparing and manipulating in other ways, magnetically or optically believe
Number form.Sometimes primarily for general reason, these signals are known as bit, value, element, symbol, character, term, numbers
Etc. being convenient.In addition, certain cloth of the step of sometimes in the case where without loss of generality by the physical manipulation for needing physical quantity
It sets referred to as module or code devices is also convenient.
It should be borne in mind, however, that all these and similar terms will be associated with appropriate physical quantity and only
It is the facilitate label applied to this tittle.Unless specifically stating otherwise as apparent from the following discussions, it should be understood that
It is throughout the specification, to utilize the term of " processing " or " calculating " or " calculating " or " display " or " determination " etc.
Movement and process with reference to computer system or similar electronics computing module and/or equipment be discussed, the computer system or
Similar electronics computing module and/or device manipulation and convert be represented as computer system memory or register or it is other this
The data of physics (electronics) amount in the storage of kind information, transmission or display equipment.
Some aspects are included herein the process steps and instruction described in the form of algorithm.It should be noted that this
The process steps of described in the text and instruction can be implemented with software, firmware and/or hardware, and work as and had with software
When body is realized, it can be downloaded to reside on the different platforms used by various operating systems and by different flat from these
Platform operation.
Some embodiments are related to the device for executing operation described herein.This device can be as required mesh
Technically construct or it may include by be stored in calculate equipment in computer program selectively activate or
The universal computing device reconfigured.This computer program can be stored in computer readable storage medium, the meter
Calculation machine readable storage medium storing program for executing such as, but not limited to include floppy disk, CD, CD-ROM, magneto-optic disk any kind of disk, read-only deposit
Reservoir (ROM), EPROM, EEPROM, flash memory, solid state drive, magnetically or optically blocks, specially random access memory (RAM)
With integrated circuit (ASIC) and/or it is suitable for storing e-command and is respectively coupled to any type of computer system bus
Medium.In addition, the calculating equipment referred to herein may include single processor or can be calculating energy for raising
The framework of power and use multiple processors design.
Algorithm presented herein and display not inherently with any specific computing device, virtualization system or other devices
It is related.Various general-purpose systems can also be used together with according to the program instructed herein, or the more dedicated device of construction is to execute
Required method and step may be proved to be convenient.Structure required for these various systems will be retouched according to provided herein
It states apparent.In addition, describing technology described herein without reference to any certain programmed language.It is to be understood that can make
Technology described herein is realized with various programming languages, and any reference of concrete syntax has been merely above and has been said
Bright property purpose and provide.
Therefore, in various embodiments, technology described herein can be used as controlling computer system, calculating equipment
Or other electronic equipments or any combination thereof or multiple software, hardware and/or other elements are implemented.According to many in this field
Well known technology, this electronic equipment may include such as processor, input equipment (such as keyboard, mouse, touch tablet, touch-control
Plate, control stick, trace ball, microphone and/or any combination thereof), output equipment (such as screen, loudspeaker and/or analog),
Memory, long-term storage apparatus (such as magnetic memory apparatus, optical storage and/or analog) and/or network connection.It is this
Electronic equipment can be portable or non-portable.It can be used to realize the electronic equipment of technology described herein
Example includes: mobile phone, personal digital assistant, smart phone, information kiosk, server computer, corporate computing devices, desktop
Type computer, laptop computer, tablet computer, consumer-elcetronics devices, TV, set-top box etc..For realizing retouching herein
Any operating system, such as Linux can be used in the electronic equipment for the technology stated；Microsoft Windows, can be from China
The Microsoft for containing a state redmond obtains；Mac OS X can be obtained from the Apple Inc. of California cupertino；
IOS can be obtained from the Apple Inc. of California cupertino；Android, can be from the paddy in California mountain scene city
Song company obtains；And/or it is adapted to any other operation system used in equipment.
It in various embodiments, can be real in distributed processing environment, networked computing environment or calculating environment based on web
Existing technology described herein.It can be in client computing device, server, router and/or other networks or non-network component
Upper realization element.In some embodiments, technology described herein is realized using user terminal/server framework, wherein one
A little components are implemented in one or more client computing devices and other components are implemented on one or more servers.
In one embodiment, during realizing the technology of the disclosure, client is from server request content, and server is rung
Should in request and returned content.Browser can be installed at client computing device for so that being able to achieve this request
And response, and can be used to initiate and control this interaction for providing user and observe user circle of presented content
Face.
It can be used in some embodiments for realizing any or all in the networking component of described technology
Any suitable electric network, either wired or wireless or any combination thereof, and use is led to for realizing this
Any suitable agreement of letter, it is communicatively coupled with one another.This network another example is internet, but it is described herein
Technology can also be used other networks to realize.
Although the embodiment of limited quantity has been described herein, the technology of this field described above is benefited from
Personnel are it is to be understood that can be designed that the other embodiments for not departing from the scope of the claims.It is further noted that
Language used in this specification is selected primarily for readable and indicative purpose, and may not yet be selected to delineation or
Person limits present subject matter.Therefore, the disclosure is it is intended that illustrative and not restrictive.
Claims (41)
1. a kind of method for video stream application effect, which comprises
In data storage, video flowing is received；
In input equipment, the foreground area for specifying the reference picture of the video flowing and depth different from the foreground area are received
First user on the boundary between background area at degree inputs；
In the processor, it inputs to generate based on first user and indicates that the reference of the foreground area and the background area is covered
Mould；
In the processor, the first non-reference mask is generated using the reference mask, the first non-reference mask refers to
Show foreground area and the background area of first non-reference picture different from the reference picture of the video flowing；And
In the processor, effect is applied to described in the reference picture and use by using the reference mask
The effect is applied to first non-reference picture and generates modified video flowing by the first non-reference mask.
2. according to the method described in claim 1, further including in the display device, showing the modified video flowing.
3. according to the method described in claim 1, further including, before receiving the video flowing in data storage, in phase
The video flowing is captured in machine.
4. according to the method described in claim 1, wherein:
Reference frame of the reference picture from the video flowing；And
From the reference frame different first frames of first non-reference picture from the video flowing.
5. according to the method described in claim 4, further including in the processor, being come using the first non-reference mask
Generate the second non-reference mask, the second non-reference mask instruction from the video flowing with the reference frame and described the
The foreground area of second non-reference picture of the second different frame of one frame and background area；
Wherein, generating the modified video flowing further includes being applied to the effect using the second non-reference mask
Second non-reference picture.
6. according to the method described in claim 1, wherein, the video flowing includes multi-view video stream.
7. according to the method described in claim 6, further including that in the display device, the modified video flowing is shown as
Virtual reality or augmented reality experience.
8. according to the method described in claim 6, wherein:
The reference-view of reference frame of the reference picture from the video flowing；And
First views different from the reference-view of the reference frame of first non-reference picture from the video flowing
Figure.
9. according to the method described in claim 8, further including in the processor, being come using the first non-reference mask
Generate the second non-reference mask, the reference frame of the second non-reference mask instruction from the video flowing with the ginseng
Examine foreground area and the background area of the second non-reference picture of view second view different with the first view；
Wherein, generating the modified video flowing further includes being applied to the effect using the second non-reference mask
Second non-reference picture.
10. according to the method described in claim 1, further including in the input equipment, receiving the first user input
Before, receive specifies the second user of the reference picture to input in the multiple images of the video flowing.
11. according to the method described in claim 1, wherein, generating the reference mask includes:
The initial binary of the foreground area and the background area of specifying the reference picture is calculated using the boundary
Segmentation；
The boundary is refined to delineate the fine boundary for more accurately specifying the foreground area and the background area；And
The reference mask is generated using the fine boundary.
12. according to the method for claim 11, wherein generating the reference mask using the fine boundary includes making
The ternary for further indicating the zone of ignorance between the foreground area and the background area is generated with the fine boundary
Figure, the zone of ignorance include the pixel that may belong to the foreground area or the background area.
13. according to the method for claim 12, further including in the processor, scratching graphic calculation to ternary diagram application
Method is to obtain including the alpha value for the pixel in the foreground area, the background area and the zone of ignorance
Alpha masking-out.
14. according to the method for claim 13, wherein the video flowing includes being captured by one or more light-field cameras
Light field video.
15. further including according to the method for claim 14, by the foreground image of the alpha masking-out and the foreground area
Back projection is to the light field video.
16. according to the method for claim 12, wherein generate the first non-reference mask using the reference mask
Include:
The first non-reference of first non-reference picture is specified to be segmented the first non-reference side between the segmentation of the second non-reference
Boundary；
Calculated using first non-reference boundary specify first non-reference picture first non-reference be segmented and
The initial non-reference binary segmentation of the second non-reference segmentation；
First non-reference boundary is refined to delineate and more accurately specify the first non-reference segmentation and the described second non-ginseng
The fine non-reference boundary of examination mark section；And
It is generated using the fine non-reference boundary and further indicates the first non-reference segmentation and second non-reference
The non-reference ternary diagram of unknown non-reference segmentation between segmentation, the unknown non-reference segmentation is comprising may belong to described first
The non-reference pixel of non-reference segmentation or second non-reference segmentation.
17. according to the method for claim 16, further including, in the processor, to the non-reference ternary diagram application
It scratches nomography and generates the non-reference foreground image of the foreground area, the non-reference to obtain non-reference alpha masking-out
Alpha masking-out includes the non-reference alpha value of the non-reference pixel in the unknown non-reference segmentation.
18. a kind of non-transitory computer-readable medium for video stream application effect, including the instruction being stored thereon,
Described instruction when executed by the processor, execute the following terms the step of:
So that data storage receives video flowing；
So that input equipment receives the foreground area for specifying the reference picture of the video flowing and depth different from the foreground area
First user on the boundary between background area at degree inputs；
It is inputted based on first user and generates the reference mask for indicating the foreground area and the background area；
Generate the first non-reference mask using the reference mask, the first non-reference mask indicate the video flowing with
The foreground area of the first different non-reference picture of the reference picture and background area；And
Effect is applied to by the reference picture by using the reference mask and uses the first non-reference mask
The effect is applied to first non-reference picture and generates modified video flowing.
19. non-transitory computer-readable medium according to claim 18 further includes the instruction being stored thereon, described
It instructs when executed by the processor, so that display equipment shows the modified video flowing.
20. non-transitory computer-readable medium according to claim 18 further includes the instruction being stored thereon, described
It instructs when executed by the processor, so that camera captures the video before receiving the video flowing in data storage
Stream.
21. non-transitory computer-readable medium according to claim 18, in which:
Reference frame of the reference picture from the video flowing；
From the reference frame different first frames of first non-reference picture from the video flowing；
The non-transitory computer-readable medium further includes the instruction being stored thereon, and described instruction is executed by processor
When, the second non-reference mask is generated using the first non-reference mask, the second non-reference mask instruction is from described
The foreground area and background of second non-reference picture of second frames different from the reference frame and the first frame of video flowing
Region；And
Generate the modified video flowing further include the effect is applied to using the second non-reference mask it is described
Second non-reference picture.
22. non-transitory computer-readable medium according to claim 18, in which:
The video flowing includes multi-view video stream；And
The non-transitory computer-readable medium further includes the instruction being stored thereon, and described instruction is executed by processor
When, so that the modified video flowing is shown as virtual reality to display equipment or augmented reality is experienced.
23. non-transitory computer-readable medium according to claim 18, in which:
The video flowing includes multi-view video stream；
The reference-view of reference frame of the reference picture from the video flowing；
First views different from the reference-view of the reference frame of first non-reference picture from the video flowing
Figure；
The non-transitory computer-readable medium further includes the instruction being stored thereon, and described instruction is executed by processor
When, the second non-reference mask is generated using the first non-reference mask, the second non-reference mask instruction is from described
Second non-reference figure of second views different from the reference-view and the first view of the reference frame of video flowing
The foreground area of picture and background area；And
Generate the modified video flowing further include the effect is applied to using the second non-reference mask it is described
Second non-reference picture.
24. non-transitory computer-readable medium according to claim 18 further includes the instruction being stored thereon, described
It instructs when executed by the processor, so that the input equipment receives before receiving the first user input from the video
The second user of the reference picture is specified to input in the multiple images of stream.
25. non-transitory computer-readable medium according to claim 18, wherein generating the reference mask includes:
The initial binary of the foreground area and the background area of specifying the reference picture is calculated using the boundary
Segmentation；
The boundary is refined to delineate the fine boundary for more accurately specifying the foreground area and the background area；And
The reference mask is generated using the fine boundary；
Wherein, it includes that further finger is generated using the fine boundary that the reference mask is generated using the fine boundary
Show that the ternary diagram of the zone of ignorance between the foreground area and the background area, the zone of ignorance include that may belong to institute
State the pixel of foreground area or the background area.
26. non-transitory computer-readable medium according to claim 25 further includes the instruction being stored thereon, described
Instruction scratches nomography when executed by the processor, to ternary diagram application to obtain including for the foreground area, described
The alpha masking-out of the alpha value of the pixel in background area and the zone of ignorance.
27. non-transitory computer-readable medium according to claim 26, in which:
The video flowing includes the light field video captured by one or more light-field cameras；And
The non-transitory computer-readable medium further includes the instruction being stored thereon, and described instruction is worked as to be executed by processor
When, by the foreground image back projection of the alpha masking-out and the foreground area to the light field video.
28. non-transitory computer-readable medium according to claim 25, wherein generated using the reference mask
The first non-reference mask includes:
The first non-reference of first non-reference picture is specified to be segmented the first non-reference side between the segmentation of the second non-reference
Boundary；
Calculated using first non-reference boundary specify first non-reference picture first non-reference be segmented and
The initial non-reference binary segmentation of the second non-reference segmentation；
First non-reference boundary is refined to delineate and more accurately specify the first non-reference segmentation and the described second non-ginseng
The fine non-reference boundary of examination mark section；And
It is generated using the fine non-reference boundary and further indicates the first non-reference segmentation and second non-reference
The non-reference ternary diagram of unknown non-reference segmentation between segmentation, the unknown non-reference segmentation is comprising may belong to described first
The non-reference pixel of non-reference segmentation or second non-reference segmentation.
29. non-transitory computer-readable medium according to claim 28 further includes the instruction being stored thereon, described
Instruction scratches nomography when executed by the processor, to non-reference ternary diagram application to obtain non-reference alpha masking-out with life
At the non-reference foreground image of the foreground area, the non-reference alpha masking-out includes in the unknown non-reference segmentation
The non-reference alpha value of the non-reference pixel.
30. a kind of system for video stream application effect, the system comprises:
Data storage, the data storage are configured to receive video flowing；
Input equipment, the input equipment be configured to receive the foreground area for specifying the reference picture of the video flowing and with institute
State the first user input on the boundary between the background area at foreground area different depth；And
Processor is coupled to the processor communication data storage and the input equipment, is configured to:
It is inputted based on first user and generates the reference mask for indicating the foreground area and the background area；
Generate the first non-reference mask using the reference mask, the first non-reference mask indicate the video flowing with
The foreground area of the first different non-reference picture of the reference picture and background area；And
Effect is applied to by the reference picture by using the reference mask and uses the first non-reference mask
The effect is applied to first non-reference picture and generates modified video flowing.
31. system according to claim 30, further includes display equipment, the display equipment is display configured to described repair
Video flowing after changing.
32. system according to claim 30 further includes camera, the camera is configured to connect in data storage
The video flowing is captured before receiving the video flowing.
33. system according to claim 30, in which:
Reference frame of the reference picture from the video flowing；
From the reference frame different first frames of first non-reference picture from the video flowing；
The processor is further configured to generate the second non-reference mask using the first non-reference mask, and described second is non-
Second non-reference of from the reference frame and the first frame different second frames of the reference mask instruction from the video flowing
The foreground area of image and background area；And
The processor is further configured to that the effect is applied to described second by using the second non-reference mask
Non-reference picture and generate the modified video flowing.
34. system according to claim 30, in which:
The video flowing includes multi-view video stream；And
The system also includes display equipment, the display equipment is configured to the modified video flowing being shown as virtual
Reality or augmented reality experience.
35. system according to claim 34, in which:
The video flowing includes multi-view video stream；
The reference-view of reference frame of the reference picture from the video flowing；
First views different from the reference-view of the reference frame of first non-reference picture from the video flowing
Figure；
The processor is further configured to generate the second non-reference mask using the first non-reference mask, and described second is non-
Reference mask indicates different from the reference-view and the first view the of the reference frame from the video flowing
The foreground area of second non-reference picture of two views and background area；And
The processor is further configured to that the effect is applied to described second by using the second non-reference mask
Non-reference picture and generate the modified video flowing.
36. system according to claim 30, wherein the input equipment is further configured to, and is used receiving described first
Before the input of family, reception specifies the second user of the reference picture to input in the multiple images of the video flowing.
37. system according to claim 30, in which:
The processor is further configured to generate the reference mask by following steps:
The initial binary of the foreground area and the background area of specifying the reference picture is calculated using the boundary
Segmentation；
The boundary is refined to delineate the fine boundary for more accurately specifying the foreground area and the background area；And
The reference mask is generated using the fine boundary；And
The processor, which is further configured to generate by using the fine boundary, further indicates the foreground area and institute
It states the ternary diagram of the zone of ignorance between background area and generates the reference mask using the fine boundary, it is described unknown
Region includes the pixel that may belong to the foreground area or the background area.
38. the system according to claim 37, wherein the processor is further configured to scratch figure to ternary diagram application
Algorithm is to obtain including the alpha for the pixel in the foreground area, the background area and the zone of ignorance
The alpha masking-out of value.
39. the system according to claim 38, in which:
The video flowing includes the light field video captured by one or more light-field cameras；And
The processor is further configured to the foreground image back projection of the alpha masking-out and the foreground area to the light
Field video.
40. the system according to claim 37, wherein the processor is further configured to lead to using the reference mask
It crosses following steps and generates the first non-reference mask:
The first non-reference of first non-reference picture is specified to be segmented the first non-reference side between the segmentation of the second non-reference
Boundary；
Calculated using first non-reference boundary specify first non-reference picture first non-reference be segmented and
The initial non-reference binary segmentation of the second non-reference segmentation；
First non-reference boundary is refined to delineate and more accurately specify the first non-reference segmentation and the described second non-ginseng
The fine non-reference boundary of examination mark section；And
It is generated using the fine non-reference boundary and further indicates the first non-reference segmentation and second non-reference
The non-reference ternary diagram of unknown non-reference segmentation between segmentation, the unknown non-reference segmentation is comprising may belong to described first
The non-reference pixel of non-reference segmentation or second non-reference segmentation.
41. system according to claim 40, wherein the processor is further configured to answer the non-reference ternary diagram
The non-reference foreground image of the foreground area, the non-reference are generated with stingy nomography to obtain non-reference alpha masking-out
Alpha masking-out includes the non-reference alpha value of the non-reference pixel in the unknown non-reference segmentation.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662347734P | 2016-06-09 | 2016-06-09 | |
US62/347,734 | 2016-06-09 | ||
US15/462,752 | 2017-03-17 | ||
US15/462,752 US10275892B2 (en) | 2016-06-09 | 2017-03-17 | Multi-view scene segmentation and propagation |
PCT/US2017/035148 WO2017213923A1 (en) | 2016-06-09 | 2017-05-31 | Multi-view scene segmentation and propagation |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109479098A true CN109479098A (en) | 2019-03-15 |
CN109479098B CN109479098B (en) | 2021-06-29 |
Family
ID=60573928
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780034730.5A Active CN109479098B (en) | 2016-06-09 | 2017-05-31 | Multi-view scene segmentation and propagation |
Country Status (6)
Country | Link |
---|---|
US (1) | US10275892B2 (en) |
EP (1) | EP3469788A4 (en) |
JP (1) | JP6655737B2 (en) |
KR (1) | KR102185179B1 (en) |
CN (1) | CN109479098B (en) |
WO (1) | WO2017213923A1 (en) |
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200074185A1 (en) * | 2019-11-08 | 2020-03-05 | Intel Corporation | Fine-grain object segmentation in video with deep features and multi-level graphical models |
CN111223106A (en) * | 2019-10-28 | 2020-06-02 | 稿定（厦门）科技有限公司 | Full-automatic portrait mask matting method and system |
CN111754528A (en) * | 2020-06-24 | 2020-10-09 | Oppo广东移动通信有限公司 | Portrait segmentation method, portrait segmentation device, electronic equipment and computer-readable storage medium |
CN112200756A (en) * | 2020-10-09 | 2021-01-08 | 电子科技大学 | Intelligent bullet special effect short video generation method |
US20220222854A1 (en) * | 2021-01-13 | 2022-07-14 | Samsung Electronics Co., Ltd. | Dynamic calibration correction in multi-frame, multi-exposure capture |
US11689693B2 (en) * | 2020-04-30 | 2023-06-27 | Boe Technology Group Co., Ltd. | Video frame interpolation method and device, computer readable storage medium |
Families Citing this family (55)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10298834B2 (en) | 2006-12-01 | 2019-05-21 | Google Llc | Video refocusing |
US9858649B2 (en) | 2015-09-30 | 2018-01-02 | Lytro, Inc. | Depth-based image blurring |
US10334151B2 (en) | 2013-04-22 | 2019-06-25 | Google Llc | Phase detection autofocus using subaperture images |
US10341632B2 (en) | 2015-04-15 | 2019-07-02 | Google Llc. | Spatial random access enabled video system with a three-dimensional viewing volume |
US10546424B2 (en) | 2015-04-15 | 2020-01-28 | Google Llc | Layered content delivery for virtual and augmented reality experiences |
US10565734B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US10275898B1 (en) | 2015-04-15 | 2019-04-30 | Google Llc | Wedge-based light-field video capture |
US10412373B2 (en) | 2015-04-15 | 2019-09-10 | Google Llc | Image capture for virtual reality displays |
US11328446B2 (en) | 2015-04-15 | 2022-05-10 | Google Llc | Combining light-field data with active depth data for depth map generation |
US10440407B2 (en) | 2017-05-09 | 2019-10-08 | Google Llc | Adaptive control for immersive experience delivery |
US10419737B2 (en) | 2015-04-15 | 2019-09-17 | Google Llc | Data structures and delivery methods for expediting virtual reality playback |
US10444931B2 (en) | 2017-05-09 | 2019-10-15 | Google Llc | Vantage generation and interactive playback |
US10540818B2 (en) | 2015-04-15 | 2020-01-21 | Google Llc | Stereo image generation and interactive playback |
US10567464B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video compression with adaptive view-dependent lighting removal |
US10469873B2 (en) | 2015-04-15 | 2019-11-05 | Google Llc | Encoding and decoding virtual reality video |
US9979909B2 (en) | 2015-07-24 | 2018-05-22 | Lytro, Inc. | Automatic lens flare detection and correction for light-field images |
US10475186B2 (en) * | 2016-06-23 | 2019-11-12 | Intel Corportation | Segmentation of objects in videos using color and depth information |
US10679361B2 (en) | 2016-12-05 | 2020-06-09 | Google Llc | Multi-view rotoscope contour propagation |
US10594945B2 (en) | 2017-04-03 | 2020-03-17 | Google Llc | Generating dolly zoom effect using light field image data |
US10474227B2 (en) | 2017-05-09 | 2019-11-12 | Google Llc | Generation of virtual reality with 6 degrees of freedom from limited viewer data |
US10354399B2 (en) | 2017-05-25 | 2019-07-16 | Google Llc | Multi-view back-projection to a light-field |
US10545215B2 (en) * | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US11143865B1 (en) * | 2017-12-05 | 2021-10-12 | Apple Inc. | Lens array for shifting perspective of an imaging system |
US10122969B1 (en) | 2017-12-07 | 2018-11-06 | Microsoft Technology Licensing, Llc | Video capture systems and methods |
WO2019129923A1 (en) * | 2017-12-29 | 2019-07-04 | Nokia Technologies Oy | An apparatus, a method and a computer program for volumetric video |
US10965862B2 (en) | 2018-01-18 | 2021-03-30 | Google Llc | Multi-camera navigation interface |
KR102450948B1 (en) * | 2018-02-23 | 2022-10-05 | 삼성전자주식회사 | Electronic device and method for providing augmented reality object thereof |
JP6965439B2 (en) * | 2018-04-04 | 2021-11-10 | 株式会社ソニー・インタラクティブエンタテインメント | Reference image generator, display image generator, reference image generation method, and display image generation method |
US10515463B2 (en) * | 2018-04-20 | 2019-12-24 | Sony Corporation | Object segmentation in a sequence of color image frames by background image and background depth correction |
US10706556B2 (en) * | 2018-05-09 | 2020-07-07 | Microsoft Technology Licensing, Llc | Skeleton-based supplementation for foreground image segmentation |
US10921596B2 (en) * | 2018-07-24 | 2021-02-16 | Disney Enterprises, Inc. | Adaptive luminance/color correction for displays |
US20200036880A1 (en) * | 2018-07-25 | 2020-01-30 | Microsoft Technology Licensing, Llc | Detecting fluid on a surface |
CN109151489B (en) * | 2018-08-14 | 2019-05-31 | 广州虎牙信息科技有限公司 | Live video image processing method, device, storage medium and computer equipment |
WO2020053482A1 (en) * | 2018-09-13 | 2020-03-19 | Nokia Technologies Oy | A method, an apparatus and a computer program product for volumetric video |
CN110378867A (en) * | 2018-09-26 | 2019-10-25 | 惠州学院 | By prospect background pixel to and grayscale information obtain transparency mask method |
CN109816611B (en) * | 2019-01-31 | 2021-02-12 | 北京市商汤科技开发有限公司 | Video repair method and device, electronic equipment and storage medium |
US11055852B2 (en) * | 2019-02-15 | 2021-07-06 | Nokia Technologies Oy | Fast automatic trimap generation and optimization for segmentation refinement |
US11004208B2 (en) * | 2019-03-26 | 2021-05-11 | Adobe Inc. | Interactive image matting using neural networks |
US10902265B2 (en) * | 2019-03-27 | 2021-01-26 | Lenovo (Singapore) Pte. Ltd. | Imaging effect based on object depth information |
CN110335277A (en) * | 2019-05-07 | 2019-10-15 | 腾讯科技（深圳）有限公司 | Image processing method, device, computer readable storage medium and computer equipment |
CN112153483B (en) * | 2019-06-28 | 2022-05-13 | 腾讯科技（深圳）有限公司 | Information implantation area detection method and device and electronic equipment |
CN110381369B (en) | 2019-07-19 | 2022-02-22 | 腾讯科技（深圳）有限公司 | Method, device and equipment for determining recommended information implantation position and storage medium |
AU2019477545B2 (en) | 2019-12-13 | 2023-10-19 | Telefonaktiebolaget Lm Ericsson (Publ) | Methods for handling occlusion in augmented reality applications using memory and device tracking and related apparatus |
CN111127486B (en) * | 2019-12-25 | 2023-09-08 | Oppo广东移动通信有限公司 | Image segmentation method, device, terminal and storage medium |
CN111274902B (en) * | 2020-01-15 | 2023-05-02 | 浙江大学 | Gesture depth image continuous detection method using multi-head mask equalizing fusion unit |
US11427193B2 (en) | 2020-01-22 | 2022-08-30 | Nodar Inc. | Methods and systems for providing depth maps with confidence estimates |
CN111524082B (en) * | 2020-04-26 | 2023-04-25 | 上海航天电子通讯设备研究所 | Target ghost eliminating method |
US11263750B1 (en) * | 2020-10-06 | 2022-03-01 | Adobe Inc. | Cyclic scheme for object segmentation networks |
CN112560684B (en) * | 2020-12-16 | 2023-10-24 | 阿波罗智联(北京)科技有限公司 | Lane line detection method, lane line detection device, electronic equipment, storage medium and vehicle |
KR20230142640A (en) * | 2021-01-06 | 2023-10-11 | 노다르 인크. | Methods and systems for providing depth maps with confidence estimates |
KR102461478B1 (en) * | 2021-05-03 | 2022-11-03 | (주)이머시브캐스트 | Clould vr device for reducing object jittering of image |
US20220379475A1 (en) * | 2021-05-25 | 2022-12-01 | Fanuc Corporation | Transparent object bin picking |
CN113516672B (en) * | 2021-09-07 | 2022-02-25 | 北京美摄网络科技有限公司 | Image segmentation method and device, electronic equipment and readable storage medium |
US11577748B1 (en) | 2021-10-08 | 2023-02-14 | Nodar Inc. | Real-time perception system for small objects at long range for autonomous vehicles |
WO2023244252A1 (en) | 2022-06-14 | 2023-12-21 | Nodar Inc. | 3d vision system with automatically calibrated stereo vision sensors and lidar sensor |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120057040A1 (en) * | 2010-05-11 | 2012-03-08 | Byung Kwan Park | Apparatus and method for processing light field data using a mask with an attenuation pattern |
CN102388391A (en) * | 2009-02-10 | 2012-03-21 | 汤姆森特许公司 | Video matting based on foreground-background constraint propagation |
US20130121577A1 (en) * | 2009-10-30 | 2013-05-16 | Jue Wang | Methods and Apparatus for Chatter Reduction in Video Object Segmentation Using Optical Flow Assisted Gaussholding |
US20130321574A1 (en) * | 2012-06-04 | 2013-12-05 | City University Of Hong Kong | View synthesis distortion model for multiview depth video coding |
US20140003719A1 (en) * | 2012-06-29 | 2014-01-02 | Xue Bai | Adaptive Trimap Propagation for Video Matting |
US20140347540A1 (en) * | 2013-05-23 | 2014-11-27 | Samsung Electronics Co., Ltd | Image display method, image display apparatus, and recording medium |
Family Cites Families (451)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US725567A (en) | 1902-09-25 | 1903-04-14 | Frederic E Ives | Parallax stereogram and process of making same. |
JPS5672575A (en) | 1979-11-19 | 1981-06-16 | Toshiba Corp | Picture input unit |
GB8317407D0 (en) | 1983-06-27 | 1983-07-27 | Rca Corp | Image transform techniques |
US4694185A (en) | 1986-04-18 | 1987-09-15 | Eastman Kodak Company | Light sensing devices with lenticular pixels |
US4920419A (en) | 1988-05-31 | 1990-04-24 | Eastman Kodak Company | Zoom lens focus control device for film video player |
JP3429755B2 (en) | 1990-04-27 | 2003-07-22 | 株式会社日立製作所 | Depth of field control device for imaging device |
US5077810A (en) | 1990-07-19 | 1991-12-31 | Eastman Kodak Company | Distributed digital signal processing system using standard resolution processors for a high resolution sensor |
US5076687A (en) | 1990-08-28 | 1991-12-31 | Massachusetts Institute Of Technology | Optical ranging apparatus |
US5251019A (en) | 1991-01-25 | 1993-10-05 | Eastman Kodak Company | Solid state color image sensor using a field-staggered color filter pattern |
US5838239A (en) | 1992-10-20 | 1998-11-17 | Robotic Vision Systems, Inc. | System for detecting ice or snow on surface which specularly reflects light |
US5757423A (en) | 1993-10-22 | 1998-05-26 | Canon Kabushiki Kaisha | Image taking apparatus |
US5499069A (en) | 1994-05-24 | 1996-03-12 | Eastman Kodak Company | Camera system and an optical adapter to reduce image format size |
US5572034A (en) | 1994-08-08 | 1996-11-05 | University Of Massachusetts Medical Center | Fiber optic plates for generating seamless images |
JPH08107194A (en) | 1994-10-03 | 1996-04-23 | Fuji Photo Optical Co Ltd | Solid state image sensor |
WO1996024085A1 (en) | 1995-02-03 | 1996-08-08 | The Regents Of The University Of Colorado | Extended depth of field optical systems |
DE19624421B4 (en) | 1995-06-30 | 2008-07-10 | Carl Zeiss Ag | Device and method for the spatially resolved measurement of wavefront deformations |
US6009188A (en) | 1996-02-16 | 1999-12-28 | Microsoft Corporation | Method and system for digital plenoptic imaging |
US6315445B1 (en) | 1996-02-21 | 2001-11-13 | Lunar Corporation | Densitometry adapter for compact x-ray fluoroscopy machine |
GB9607541D0 (en) | 1996-04-11 | 1996-06-12 | Discreet Logic Inc | Processing image data |
JPH09289655A (en) | 1996-04-22 | 1997-11-04 | Fujitsu Ltd | Stereoscopic image display method, multi-view image input method, multi-view image processing method, stereoscopic image display device, multi-view image input device and multi-view image processor |
US5818525A (en) | 1996-06-17 | 1998-10-06 | Loral Fairchild Corp. | RGB image correction using compressed flat illuminated files and a simple one or two point correction algorithm |
GB9616262D0 (en) | 1996-08-02 | 1996-09-11 | Philips Electronics Nv | Post-processing generation of focus/defocus effects for computer graphics images |
US6028606A (en) | 1996-08-02 | 2000-02-22 | The Board Of Trustees Of The Leland Stanford Junior University | Camera simulation system |
JP3154325B2 (en) | 1996-11-28 | 2001-04-09 | 日本アイ・ビー・エム株式会社 | System for hiding authentication information in images and image authentication system |
US5907619A (en) | 1996-12-20 | 1999-05-25 | Intel Corporation | Secure compressed imaging |
US7304670B1 (en) | 1997-03-28 | 2007-12-04 | Hand Held Products, Inc. | Method and apparatus for compensating for fixed pattern noise in an imaging system |
US6351570B1 (en) | 1997-04-01 | 2002-02-26 | Matsushita Electric Industrial Co., Ltd. | Image coding and decoding apparatus, method of image coding and decoding, and recording medium for recording program for image coding and decoding |
US6115556A (en) | 1997-04-10 | 2000-09-05 | Reddington; Terrence P. | Digital camera back accessory and methods of manufacture |
DE69810919T2 (en) | 1997-04-14 | 2003-05-15 | Dicon As Lystrup | DEVICE AND METHOD FOR ILLUMINATING A LIGHT-SENSITIVE MEDIUM |
US6097394A (en) | 1997-04-28 | 2000-08-01 | Board Of Trustees, Leland Stanford, Jr. University | Method and system for light field rendering |
US5835267A (en) | 1997-07-15 | 1998-11-10 | Eastman Kodak Company | Radiometric calibration device and method |
US6091860A (en) | 1997-11-12 | 2000-07-18 | Pagemasters, Inc. | System and method for processing pixels for displaying and storing |
US6061400A (en) | 1997-11-20 | 2000-05-09 | Hitachi America Ltd. | Methods and apparatus for detecting scene conditions likely to cause prediction errors in reduced resolution video decoders and for using the detected information |
US6466207B1 (en) | 1998-03-18 | 2002-10-15 | Microsoft Corporation | Real-time image rendering with layered depth images |
US5974215A (en) | 1998-05-20 | 1999-10-26 | North Carolina State University | Compound image sensor array having staggered array of tapered optical fiber bundles |
US6448544B1 (en) | 1998-06-08 | 2002-09-10 | Brandeis University | Low noise, high resolution image detection system and method |
US6137100A (en) | 1998-06-08 | 2000-10-24 | Photobit Corporation | CMOS image sensor with different pixel sizes for different colors |
US6075889A (en) | 1998-06-12 | 2000-06-13 | Eastman Kodak Company | Computing color specification (luminance and chrominance) values for images |
US6674430B1 (en) | 1998-07-16 | 2004-01-06 | The Research Foundation Of State University Of New York | Apparatus and method for real-time volume processing and universal 3D rendering |
US6021241A (en) | 1998-07-17 | 2000-02-01 | North Carolina State University | Systems and methods for using diffraction patterns to determine radiation intensity values for areas between and along adjacent sensors of compound sensor arrays |
US7057647B1 (en) | 2000-06-14 | 2006-06-06 | E-Watch, Inc. | Dual-mode camera system for day/night or variable zoom operation |
US6833865B1 (en) | 1998-09-01 | 2004-12-21 | Virage, Inc. | Embedded metadata engines in digital capture devices |
US6577342B1 (en) | 1998-09-25 | 2003-06-10 | Intel Corporation | Image sensor with microlens material structure |
US6320979B1 (en) | 1998-10-06 | 2001-11-20 | Canon Kabushiki Kaisha | Depth of field enhancement |
US6201899B1 (en) | 1998-10-09 | 2001-03-13 | Sarnoff Corporation | Method and apparatus for extended depth of field imaging |
US6169285B1 (en) | 1998-10-23 | 2001-01-02 | Adac Laboratories | Radiation-based imaging system employing virtual light-responsive elements |
EP1008956A1 (en) | 1998-12-08 | 2000-06-14 | Synoptics Limited | Automatic image montage system |
JP2000207549A (en) | 1999-01-11 | 2000-07-28 | Olympus Optical Co Ltd | Image processor |
US7469381B2 (en) | 2007-01-07 | 2008-12-23 | Apple Inc. | List scrolling and document translation, scaling, and rotation on a touch-screen display |
US6137634A (en) | 1999-02-01 | 2000-10-24 | Intel Corporation | Microlens array |
US6424351B1 (en) | 1999-04-21 | 2002-07-23 | The University Of North Carolina At Chapel Hill | Methods and systems for producing three-dimensional images using relief textures |
KR100346259B1 (en) | 1999-05-29 | 2002-07-26 | 엘지전자주식회사 | Image retrieval method using multiple features per image sub region |
JP2000350071A (en) | 1999-06-02 | 2000-12-15 | Nikon Corp | Electronic still camera |
JP3595759B2 (en) | 1999-07-02 | 2004-12-02 | キヤノン株式会社 | Imaging apparatus and imaging system |
US7015954B1 (en) | 1999-08-09 | 2006-03-21 | Fuji Xerox Co., Ltd. | Automatic video system using multiple cameras |
US6768980B1 (en) | 1999-09-03 | 2004-07-27 | Thomas W. Meyer | Method of and apparatus for high-bandwidth steganographic embedding of data in a series of digital signals or measurements such as taken from analog data streams or subsampled and/or transformed digital data |
US6597859B1 (en) | 1999-12-16 | 2003-07-22 | Intel Corporation | Method and apparatus for abstracting video data |
US6483535B1 (en) | 1999-12-23 | 2002-11-19 | Welch Allyn, Inc. | Wide angle lens system for electronic imagers having long exit pupil distances |
US6221687B1 (en) | 1999-12-23 | 2001-04-24 | Tower Semiconductor Ltd. | Color image sensor with embedded microlens array |
US6476805B1 (en) | 1999-12-23 | 2002-11-05 | Microsoft Corporation | Techniques for spatial displacement estimation and multi-resolution operations on light fields |
WO2001052178A1 (en) | 2000-01-13 | 2001-07-19 | Digimarc Corporation | Authenticating metadata and embedding metadata in watermarks of media signals |
US20010045965A1 (en) | 2000-02-14 | 2001-11-29 | Julian Orbanes | Method and system for receiving user input |
US20010048968A1 (en) | 2000-02-16 | 2001-12-06 | Cox W. Royall | Ink-jet printing of gradient-index microlenses |
US6681195B1 (en) | 2000-03-22 | 2004-01-20 | Laser Technology, Inc. | Compact speed measurement system with onsite digital image capture, processing, and portable display |
US6460997B1 (en) | 2000-05-08 | 2002-10-08 | Alcon Universal Ltd. | Apparatus and method for objective measurements of optical systems using wavefront analysis |
EP1285304B1 (en) | 2000-05-19 | 2004-08-18 | BALOGH, Tibor | Method and apparatus for displaying 3d images |
JP3748786B2 (en) | 2000-06-19 | 2006-02-22 | アルプス電気株式会社 | Display device and image signal processing method |
US7092014B1 (en) | 2000-06-28 | 2006-08-15 | Microsoft Corporation | Scene capturing and view rendering based on a longitudinally aligned camera array |
US6868191B2 (en) | 2000-06-28 | 2005-03-15 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method for median fusion of depth maps |
US9189069B2 (en) | 2000-07-17 | 2015-11-17 | Microsoft Technology Licensing, Llc | Throwing gestures for mobile devices |
US7085409B2 (en) | 2000-10-18 | 2006-08-01 | Sarnoff Corporation | Method and apparatus for synthesizing new video and/or still imagery from a collection of real video and/or still imagery |
US7034866B1 (en) | 2000-11-22 | 2006-04-25 | Koninklijke Philips Electronics N.V. | Combined display-camera for an image processing system |
IL139995A (en) | 2000-11-29 | 2007-07-24 | Rvc Llc | System and method for spherical stereoscopic photographing |
US7003061B2 (en) * | 2000-12-21 | 2006-02-21 | Adobe Systems Incorporated | Image extraction from complex scenes in digital video |
EP1231767B1 (en) | 2001-02-09 | 2011-04-13 | Canon Kabushiki Kaisha | Information processing apparatus and its control method, computer program, and storage medium |
WO2002065761A2 (en) | 2001-02-12 | 2002-08-22 | Carnegie Mellon University | System and method for stabilizing rotational images |
US6924841B2 (en) | 2001-05-02 | 2005-08-02 | Agilent Technologies, Inc. | System and method for capturing color images that extends the dynamic range of an image sensor using first and second groups of pixels |
US7480796B2 (en) | 2001-06-07 | 2009-01-20 | Kwan Sofware Engineering, Inc. | System and method for authenticating data using incompatible digest functions |
US6842297B2 (en) | 2001-08-31 | 2005-01-11 | Cdm Optics, Inc. | Wavefront coding optics |
US7239345B1 (en) | 2001-10-12 | 2007-07-03 | Worldscape, Inc. | Camera arrangements with backlighting detection and methods of using same |
JP2005521123A (en) | 2001-10-22 | 2005-07-14 | ライカ ミクロジュステムス ヴェツラー ゲーエムベーハー | Method and apparatus for generating three-dimensional image detected by optical microscope |
US7929808B2 (en) | 2001-10-30 | 2011-04-19 | Hewlett-Packard Development Company, L.P. | Systems and methods for generating digital images having image meta-data combined with the image data |
US20030210329A1 (en) | 2001-11-08 | 2003-11-13 | Aagaard Kenneth Joseph | Video system and methods for operating a video system |
US7120293B2 (en) | 2001-11-30 | 2006-10-10 | Microsoft Corporation | Interactive images |
EP1468314A4 (en) | 2001-12-18 | 2006-12-13 | Univ Rochester | Imaging using a multifocal aspheric lens to obtain extended depth of field |
US7053953B2 (en) | 2001-12-21 | 2006-05-30 | Eastman Kodak Company | Method and camera system for blurring portions of a verification image to show out of focus areas in a captured archival image |
JP3997085B2 (en) | 2001-12-28 | 2007-10-24 | キヤノン株式会社 | Image generation device |
US7046292B2 (en) | 2002-01-16 | 2006-05-16 | Hewlett-Packard Development Company, L.P. | System for near-simultaneous capture of multiple camera images |
ITTO20020103A1 (en) | 2002-02-06 | 2003-08-06 | Fioravanti Srl | SYSTEM FOR THE FRONT LIGHTING OF A VEHICLE. |
US7017144B2 (en) | 2002-06-17 | 2006-03-21 | Microsoft Corporation | Combined image views and method of creating images |
US6744109B2 (en) | 2002-06-26 | 2004-06-01 | Agilent Technologies, Inc. | Glass attachment over micro-lens arrays |
US20040012688A1 (en) | 2002-07-16 | 2004-01-22 | Fairchild Imaging | Large area charge coupled device camera |
US20040012689A1 (en) | 2002-07-16 | 2004-01-22 | Fairchild Imaging | Charge coupled devices in tiled arrays |
US7639838B2 (en) | 2002-08-30 | 2009-12-29 | Jerry C Nims | Multi-dimensional images system for digital image input and output |
US7778438B2 (en) | 2002-09-30 | 2010-08-17 | Myport Technologies, Inc. | Method for multi-media recognition, data conversion, creation of metatags, storage and search retrieval |
US7373432B2 (en) | 2002-10-31 | 2008-05-13 | Lockheed Martin | Programmable circuit and related computing machine and method |
US7206022B2 (en) | 2002-11-25 | 2007-04-17 | Eastman Kodak Company | Camera system with eye monitoring |
US20040114176A1 (en) | 2002-12-17 | 2004-06-17 | International Business Machines Corporation | Editing and browsing images for virtual cameras |
EP1584067A2 (en) | 2003-01-16 | 2005-10-12 | D-blur Technologies LTD. C/o Yossi Haimov CPA | Camera with image enhancement functions |
JP4324404B2 (en) | 2003-04-22 | 2009-09-02 | 富士フイルム株式会社 | Solid-state imaging device and digital camera |
US7164807B2 (en) | 2003-04-24 | 2007-01-16 | Eastman Kodak Company | Method and system for automatically reducing aliasing artifacts |
US7025515B2 (en) | 2003-05-20 | 2006-04-11 | Software 2000 Ltd. | Bit mask generation system |
WO2004106857A1 (en) | 2003-05-29 | 2004-12-09 | Olympus Corporation | Stereo optical module and stereo camera |
JP3801616B2 (en) | 2003-05-29 | 2006-07-26 | 松下電器産業株式会社 | Imaging device |
US7620218B2 (en) | 2006-08-11 | 2009-11-17 | Fotonation Ireland Limited | Real-time face tracking with reference images |
JP4148041B2 (en) | 2003-06-27 | 2008-09-10 | ソニー株式会社 | Signal processing apparatus, signal processing method, program, and recording medium |
JP2005094740A (en) | 2003-08-08 | 2005-04-07 | Ricoh Co Ltd | Image processing apparatus, image forming apparatus and image processing method |
US7044912B2 (en) | 2003-08-28 | 2006-05-16 | Siemens Medical Solutions Usa Inc. | Diagnostic medical ultrasound system having method and apparatus for storing and retrieving 3D and 4D data sets |
US7262771B2 (en) | 2003-10-10 | 2007-08-28 | Microsoft Corporation | Systems and methods for all-frequency relighting using spherical harmonics and point light distributions |
JP2005143094A (en) | 2003-10-14 | 2005-06-02 | Canon Inc | Image verification system, management method, and storage medium readable by computer |
US7078260B2 (en) | 2003-12-31 | 2006-07-18 | Dongbu Electronics Co., Ltd. | CMOS image sensors and methods for fabricating the same |
JP4525089B2 (en) | 2004-01-27 | 2010-08-18 | フジノン株式会社 | Auto focus system |
US20050212918A1 (en) | 2004-03-25 | 2005-09-29 | Bill Serra | Monitoring system and method |
ES2217986B1 (en) | 2004-04-27 | 2005-08-16 | Fico Mirrors, S.A. | OPENING MECHANISM FOR EXTERIOR REAR VIEW MACHINES OF MOTOR VEHICLES. |
US7436403B2 (en) | 2004-06-12 | 2008-10-14 | University Of Southern California | Performance relighting and reflectance transformation with time-multiplexed illumination |
JP4642757B2 (en) | 2004-07-23 | 2011-03-02 | パナソニック株式会社 | Image processing apparatus and image processing method |
US20060023066A1 (en) | 2004-07-27 | 2006-02-02 | Microsoft Corporation | System and Method for Client Services for Interactive Multi-View Video |
WO2006012678A1 (en) | 2004-08-03 | 2006-02-09 | Silverbrook Research Pty Ltd | Walk-up printing |
US7329856B2 (en) | 2004-08-24 | 2008-02-12 | Micron Technology, Inc. | Image sensor having integrated infrared-filtering optical device and related method |
US7477304B2 (en) | 2004-08-26 | 2009-01-13 | Micron Technology, Inc. | Two narrow band and one wide band color filter for increasing color image sensor sensitivity |
TWI242368B (en) | 2004-09-02 | 2005-10-21 | Asia Optical Co Inc | Image capturing system of image formation on curved surface |
US7336430B2 (en) | 2004-09-03 | 2008-02-26 | Micron Technology, Inc. | Extended depth of field using a multi-focal length lens with a controlled range of spherical aberration and a centrally obscured aperture |
JP2006078738A (en) | 2004-09-09 | 2006-03-23 | Fuji Photo Film Co Ltd | Camera system, camera body, and camera head |
JP4587166B2 (en) | 2004-09-14 | 2010-11-24 | キヤノン株式会社 | Moving body tracking system, photographing apparatus, and photographing method |
US20060056604A1 (en) | 2004-09-15 | 2006-03-16 | Research In Motion Limited | Method for scaling images for usage on a mobile communication device |
US8750509B2 (en) | 2004-09-23 | 2014-06-10 | Smartvue Corporation | Wireless surveillance system releasably mountable to track lighting |
WO2007092545A2 (en) | 2006-02-07 | 2007-08-16 | The Board Of Trustees Of The Leland Stanford Junior University | Variable imaging arrangements and methods therefor |
EP2398224B1 (en) | 2004-10-01 | 2016-01-13 | The Board of Trustees of The Leland Stanford Junior University | Imaging arrangements and methods therefor |
JP2006107213A (en) | 2004-10-06 | 2006-04-20 | Canon Inc | Stereoscopic image printing system |
US7417670B1 (en) | 2005-01-12 | 2008-08-26 | Ambarella, Inc. | Digital video camera with binning or skipping correction |
US7532214B2 (en) | 2005-05-25 | 2009-05-12 | Spectra Ab | Automated medical image visualization using volume rendering with local histograms |
EP1887399A4 (en) | 2005-05-30 | 2011-03-23 | Nikon Corp | Image formation state detection device |
KR101120092B1 (en) | 2005-06-04 | 2012-03-23 | 삼성전자주식회사 | Method for improving quality of composite video signal and the apparatus therefore and method for decoding composite video signal and the apparatus therefore |
US7577309B2 (en) | 2005-06-18 | 2009-08-18 | Muralidhara Subbarao | Direct vision sensor for 3D computer vision, digital imaging, and digital video |
JP4826152B2 (en) | 2005-06-23 | 2011-11-30 | 株式会社ニコン | Image composition method and imaging apparatus |
US20070019883A1 (en) | 2005-07-19 | 2007-01-25 | Wong Earl Q | Method for creating a depth map for auto focus using an all-in-focus picture and two-dimensional scale space matching |
US8601475B2 (en) | 2005-08-02 | 2013-12-03 | Aol Inc. | Download and upload of email messages using control commands in a client/server web application |
US8237801B2 (en) | 2005-08-05 | 2012-08-07 | The Innovation Science Fund I, LLC | Image processing system and communication method |
JP3930898B2 (en) | 2005-08-08 | 2007-06-13 | 松下電器産業株式会社 | Image composition apparatus and image composition method |
US8310554B2 (en) | 2005-09-20 | 2012-11-13 | Sri International | Method and apparatus for performing coordinated multi-PTZ camera tracking |
JP4718952B2 (en) | 2005-09-27 | 2011-07-06 | 富士フイルム株式会社 | Image correction method and image correction system |
EP1941314A4 (en) | 2005-10-07 | 2010-04-14 | Univ Leland Stanford Junior | Microscopy arrangements and approaches |
US20070081081A1 (en) | 2005-10-07 | 2007-04-12 | Cheng Brett A | Automated multi-frame image capture for panorama stitching using motion sensor |
US9270976B2 (en) | 2005-11-02 | 2016-02-23 | Exelis Inc. | Multi-user stereoscopic 3-D panoramic vision system and method |
US7409149B2 (en) | 2005-11-03 | 2008-08-05 | International Business Machines Corporation | Methods for improved autofocus in digital imaging systems |
US20070103558A1 (en) | 2005-11-04 | 2007-05-10 | Microsoft Corporation | Multi-view video delivery |
US7523405B2 (en) | 2005-11-16 | 2009-04-21 | Microsoft Corporation | Displaying 2D graphic content using depth wells |
US7623726B1 (en) | 2005-11-30 | 2009-11-24 | Adobe Systems, Incorporated | Method and apparatus for using a virtual camera to dynamically refocus a digital image |
US7286295B1 (en) | 2005-11-30 | 2007-10-23 | Sandia Corporation | Microoptical compound lens |
US7945653B2 (en) | 2006-10-11 | 2011-05-17 | Facebook, Inc. | Tagging digital media |
JP4874641B2 (en) | 2005-12-16 | 2012-02-15 | Ｈｏｙａ株式会社 | Camera with autofocus device |
US8358354B2 (en) | 2009-01-26 | 2013-01-22 | The Board Of Trustees Of The Leland Stanford Junior University | Correction of optical abberations |
US7614018B1 (en) | 2006-02-13 | 2009-11-03 | Google Inc. | Web based user interface for selecting options |
US7748022B1 (en) | 2006-02-21 | 2010-06-29 | L-3 Communications Sonoma Eo, Inc. | Real-time data characterization with token generation for fast data retrieval |
US7590344B2 (en) | 2006-02-28 | 2009-09-15 | Microsoft Corp. | Adaptive processing for images captured with flash |
JP5186727B2 (en) | 2006-03-31 | 2013-04-24 | 株式会社ニコン | Electronic camera with projector, projection apparatus and electronic apparatus |
US7620309B2 (en) | 2006-04-04 | 2009-11-17 | Adobe Systems, Incorporated | Plenoptic camera |
US7609906B2 (en) | 2006-04-04 | 2009-10-27 | Mitsubishi Electric Research Laboratories, Inc. | Method and system for acquiring and displaying 3D light fields |
IES20060564A2 (en) | 2006-05-03 | 2006-11-01 | Fotonation Vision Ltd | Improved foreground / background separation |
US7724952B2 (en) | 2006-05-15 | 2010-05-25 | Microsoft Corporation | Object matting using flash and no-flash images |
CN101449586B (en) | 2006-05-25 | 2012-08-29 | 汤姆逊许可证公司 | Method and system for weighted coding |
US8213734B2 (en) | 2006-07-07 | 2012-07-03 | Sony Ericsson Mobile Communications Ab | Active autofocus window |
JP2008022372A (en) | 2006-07-13 | 2008-01-31 | Canon Inc | Alteration detection information generating apparatus, imaging apparatus, alteration detection information generating method, program, and storage medium |
EP2052552A4 (en) | 2006-07-16 | 2011-12-28 | Seambi Ltd | System and method for virtual content placement |
JP5041757B2 (en) | 2006-08-02 | 2012-10-03 | パナソニック株式会社 | Camera control device and camera control system |
US7813586B2 (en) | 2006-08-07 | 2010-10-12 | Mela Sciences, Inc. | Reducing noise in digital images |
US7953277B2 (en) | 2006-09-05 | 2011-05-31 | Williams Robert C | Background separated images for print and on-line use |
US8106856B2 (en) | 2006-09-06 | 2012-01-31 | Apple Inc. | Portable electronic device for photo management |
US8155478B2 (en) | 2006-10-26 | 2012-04-10 | Broadcom Corporation | Image creation with software controllable depth of field |
JP2008129554A (en) | 2006-11-27 | 2008-06-05 | Sanyo Electric Co Ltd | Imaging device and automatic focusing control method |
AU2006246497B2 (en) | 2006-11-30 | 2010-02-11 | Canon Kabushiki Kaisha | Method and apparatus for hybrid image compression |
US8559705B2 (en) | 2006-12-01 | 2013-10-15 | Lytro, Inc. | Interactive refocusing of electronic images |
JP4569561B2 (en) | 2006-12-01 | 2010-10-27 | 富士フイルム株式会社 | Image file creation device |
US10298834B2 (en) | 2006-12-01 | 2019-05-21 | Google Llc | Video refocusing |
US20130113981A1 (en) | 2006-12-01 | 2013-05-09 | Lytro, Inc. | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
EP1927949A1 (en) | 2006-12-01 | 2008-06-04 | Thomson Licensing | Array of processing elements with local registers |
US20100265385A1 (en) | 2009-04-18 | 2010-10-21 | Knight Timothy J | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
US8103111B2 (en) | 2006-12-26 | 2012-01-24 | Olympus Imaging Corp. | Coding method, electronic camera, recording medium storing coded program, and decoding method |
KR101373890B1 (en) | 2006-12-28 | 2014-03-12 | 톰슨 라이센싱 | Method and apparatus for automatic visual artifact analysis and artifact reduction |
JP4947639B2 (en) | 2007-01-19 | 2012-06-06 | 浜松ホトニクス株式会社 | Reflection type phase change device and setting method of reflection type phase modulation device |
US7872796B2 (en) | 2007-01-25 | 2011-01-18 | Adobe Systems Incorporated | Light field microscope with lenslet array |
US8538210B2 (en) | 2007-02-01 | 2013-09-17 | Alliance Fiber Optic Products, Inc. | Micro free-space WDM device |
US7792423B2 (en) | 2007-02-06 | 2010-09-07 | Mitsubishi Electric Research Laboratories, Inc. | 4D light field cameras |
CN100585453C (en) | 2007-02-09 | 2010-01-27 | 奥林巴斯映像株式会社 | Decoding method and decoding apparatus |
JP5034556B2 (en) | 2007-02-27 | 2012-09-26 | 株式会社ニコン | Focus detection apparatus and imaging apparatus |
US8077964B2 (en) | 2007-03-19 | 2011-12-13 | Sony Corporation | Two dimensional/three dimensional digital information acquisition and display device |
US20080253652A1 (en) | 2007-04-10 | 2008-10-16 | Aricent Inc. | Method of demosaicing a digital mosaiced image |
WO2008128205A1 (en) | 2007-04-13 | 2008-10-23 | Presler Ari M | Digital cinema camera system for recording, editing and visualizing images |
US20080260291A1 (en) | 2007-04-17 | 2008-10-23 | Nokia Corporation | Image downscaling by binning |
US7936377B2 (en) | 2007-04-30 | 2011-05-03 | Tandent Vision Science, Inc. | Method and system for optimizing an image for improved analysis of material and illumination image features |
US8494304B2 (en) | 2007-05-11 | 2013-07-23 | Xerox Corporation | Punched hole detection and removal |
JP5109803B2 (en) | 2007-06-06 | 2012-12-26 | ソニー株式会社 | Image processing apparatus, image processing method, and image processing program |
JP2008312080A (en) | 2007-06-18 | 2008-12-25 | Sony Corp | Imaging apparatus and imaging method |
US8290358B1 (en) | 2007-06-25 | 2012-10-16 | Adobe Systems Incorporated | Methods and apparatus for light-field imaging |
US7982776B2 (en) | 2007-07-13 | 2011-07-19 | Ethicon Endo-Surgery, Inc. | SBI motion artifact removal apparatus and method |
JP4967873B2 (en) | 2007-07-13 | 2012-07-04 | ソニー株式会社 | Imaging device |
US8085391B2 (en) | 2007-08-02 | 2011-12-27 | Aptina Imaging Corporation | Integrated optical characteristic measurements in a CMOS image sensor |
US8559756B2 (en) | 2007-08-06 | 2013-10-15 | Adobe Systems Incorporated | Radiance processing by demultiplexing in the frequency domain |
JP5203655B2 (en) | 2007-09-07 | 2013-06-05 | キヤノン株式会社 | Content display device and display method thereof |
US8253824B2 (en) | 2007-10-12 | 2012-08-28 | Microsoft Corporation | Multi-spectral imaging |
US7956924B2 (en) | 2007-10-18 | 2011-06-07 | Adobe Systems Incorporated | Fast computational camera based on two arrays of lenses |
JP4905326B2 (en) | 2007-11-12 | 2012-03-28 | ソニー株式会社 | Imaging device |
US8488834B2 (en) | 2007-11-15 | 2013-07-16 | Certifi-Media Inc. | Method for making an assured image |
US8229294B2 (en) | 2007-12-10 | 2012-07-24 | Mitsubishi Electric Research Laboratories, Inc. | Cameras with varying spatio-angular-temporal resolutions |
JP5018504B2 (en) | 2008-01-21 | 2012-09-05 | 株式会社ニコン | Data processing device, imaging device, data processing program |
US7962033B2 (en) | 2008-01-23 | 2011-06-14 | Adobe Systems Incorporated | Methods and apparatus for full-resolution light-field capture and rendering |
US8189065B2 (en) | 2008-01-23 | 2012-05-29 | Adobe Systems Incorporated | Methods and apparatus for full-resolution light-field capture and rendering |
JP4941332B2 (en) | 2008-01-28 | 2012-05-30 | ソニー株式会社 | Imaging device |
JP4483951B2 (en) | 2008-01-28 | 2010-06-16 | ソニー株式会社 | Imaging device |
KR101445606B1 (en) | 2008-02-05 | 2014-09-29 | 삼성전자주식회사 | Digital photographing apparatus, method for controlling the same, and recording medium storing program to implement the method |
US8577216B2 (en) | 2008-02-13 | 2013-11-05 | Qualcomm Incorporated | Auto-focus calibration for image capture device |
US9094675B2 (en) | 2008-02-29 | 2015-07-28 | Disney Enterprises Inc. | Processing image data from multiple cameras for motion pictures |
JP2009246745A (en) | 2008-03-31 | 2009-10-22 | Panasonic Corp | Image pickup apparatus, image pickup module, electronic still camera and electronic movie camera |
EP2107446A1 (en) | 2008-04-04 | 2009-10-07 | ETH Zurich | System and a method for tracking input devices on LC-displays |
US8155456B2 (en) | 2008-04-29 | 2012-04-10 | Adobe Systems Incorporated | Method and apparatus for block-based compression of light-field images |
US7780364B2 (en) | 2008-05-01 | 2010-08-24 | Mitsubishi Electric Research Laboratories, Inc. | Apparatus and method for reducing glare in images |
US20100097444A1 (en) | 2008-10-16 | 2010-04-22 | Peter Lablans | Camera System for Creating an Image From a Plurality of Images |
JP4524717B2 (en) | 2008-06-13 | 2010-08-18 | 富士フイルム株式会社 | Image processing apparatus, imaging apparatus, image processing method, and program |
KR101483714B1 (en) | 2008-06-18 | 2015-01-16 | 삼성전자 주식회사 | Apparatus and method for capturing digital image |
CN101309359B (en) | 2008-06-20 | 2013-02-06 | 埃派克森微电子(上海)股份有限公司 | System and method for eliminating fixed mode noise by dummy pixels |
US20090321861A1 (en) | 2008-06-26 | 2009-12-31 | Micron Technology, Inc. | Microelectronic imagers with stacked lens assemblies and processes for wafer-level packaging of microelectronic imagers |
JP5525703B2 (en) | 2008-07-10 | 2014-06-18 | オリンパス株式会社 | Image playback display device |
US8736751B2 (en) | 2008-08-26 | 2014-05-27 | Empire Technology Development Llc | Digital presenter for displaying image captured by camera with illumination system |
US7587109B1 (en) | 2008-09-02 | 2009-09-08 | Spectral Imaging Laboratory | Hybrid fiber coupled artificial compound eye |
US8086275B2 (en) | 2008-10-23 | 2011-12-27 | Microsoft Corporation | Alternative inputs of a mobile communications device |
AU2008246243B2 (en) | 2008-11-19 | 2011-12-22 | Canon Kabushiki Kaisha | DVC as generic file format for plenoptic camera |
US8201951B2 (en) | 2008-11-19 | 2012-06-19 | Seiko Epson Corporation | Catadioptric projectors |
WO2010065344A1 (en) | 2008-11-25 | 2010-06-10 | Refocus Imaging, Inc. | System of and method for video refocusing |
GB2465793A (en) | 2008-11-28 | 2010-06-02 | Sony Corp | Estimating camera angle using extrapolated corner locations from a calibration pattern |
DK200801722A (en) | 2008-12-05 | 2010-06-06 | Unisensor As | Optical sectioning of a sample and detection of particles in a sample |
US8289440B2 (en) | 2008-12-08 | 2012-10-16 | Lytro, Inc. | Light field data acquisition devices, and methods of using and manufacturing same |
US8013904B2 (en) | 2008-12-09 | 2011-09-06 | Seiko Epson Corporation | View projection matrix based high performance low latency display pipeline |
US7949252B1 (en) | 2008-12-11 | 2011-05-24 | Adobe Systems Incorporated | Plenoptic camera with large depth of field |
US9635253B2 (en) | 2009-01-05 | 2017-04-25 | Duke University | Multiscale telescopic imaging system |
JP5355208B2 (en) | 2009-05-01 | 2013-11-27 | 富士フイルム株式会社 | Three-dimensional display device and digital zoom correction method |
CN102282840B (en) | 2009-01-19 | 2016-01-06 | 杜比实验室特许公司 | Multiplexed imaging |
US8315476B1 (en) | 2009-01-20 | 2012-11-20 | Adobe Systems Incorporated | Super-resolution with the focused plenoptic camera |
US8189089B1 (en) | 2009-01-20 | 2012-05-29 | Adobe Systems Incorporated | Methods and apparatus for reducing plenoptic camera artifacts |
US7687757B1 (en) | 2009-01-29 | 2010-03-30 | Visera Technologies Company Limited | Design of microlens on pixel array |
US8195040B2 (en) | 2009-02-10 | 2012-06-05 | Canon Kabushiki Kaisha | Imaging apparatus, flash device, and control method thereof |
US8589374B2 (en) | 2009-03-16 | 2013-11-19 | Apple Inc. | Multifunction device with integrated search and application selection |
US8797321B1 (en) | 2009-04-01 | 2014-08-05 | Microsoft Corporation | Augmented lighting environments |
US20100253782A1 (en) | 2009-04-07 | 2010-10-07 | Latent Image Technology Ltd. | Device and method for automated verification of polarization-variant images |
US20120249550A1 (en) | 2009-04-18 | 2012-10-04 | Lytro, Inc. | Selective Transmission of Image Data Based on Device Attributes |
US8908058B2 (en) | 2009-04-18 | 2014-12-09 | Lytro, Inc. | Storage and transmission of pictures including multiple frames |
ATE551841T1 (en) | 2009-04-22 | 2012-04-15 | Raytrix Gmbh | DIGITAL IMAGING METHOD FOR SYNTHESIZING AN IMAGE USING DATA RECORDED BY A PLENOPTIC CAMERA |
US8358365B2 (en) | 2009-05-01 | 2013-01-22 | Samsung Electronics Co., Ltd. | Photo detecting device and image pickup device and method thereon |
US8345144B1 (en) | 2009-07-15 | 2013-01-01 | Adobe Systems Incorporated | Methods and apparatus for rich image capture with focused plenoptic cameras |
US8228417B1 (en) | 2009-07-15 | 2012-07-24 | Adobe Systems Incorporated | Focused plenoptic camera employing different apertures or filtering at different microlenses |
US8097894B2 (en) | 2009-07-23 | 2012-01-17 | Koninklijke Philips Electronics N.V. | LED with molded reflective sidewall coating |
US8654234B2 (en) | 2009-07-26 | 2014-02-18 | Massachusetts Institute Of Technology | Bi-directional screen |
US9582889B2 (en) | 2009-07-30 | 2017-02-28 | Apple Inc. | Depth mapping based on pattern matching and stereoscopic information |
JP5350123B2 (en) | 2009-08-10 | 2013-11-27 | 株式会社日立ハイテクノロジーズ | Charged particle beam apparatus and image display method |
US8497914B2 (en) | 2009-08-10 | 2013-07-30 | Wisconsin Alumni Research Foundation | Vision system and method for motion adaptive integration of image frames |
WO2011028837A2 (en) | 2009-09-01 | 2011-03-10 | Prime Focus Vfx Services Ii Inc. | System and process for transforming two-dimensional images into three-dimensional images |
US8228413B2 (en) | 2009-09-01 | 2012-07-24 | Geovector Corp. | Photographer's guidance systems |
WO2011029209A2 (en) | 2009-09-10 | 2011-03-17 | Liberovision Ag | Method and apparatus for generating and processing depth-enhanced images |
KR101600010B1 (en) | 2009-09-22 | 2016-03-04 | 삼성전자주식회사 | Modulator apparatus for obtaining light field data using modulator apparatus and method for processing light field data using modulator |
JP2013507084A (en) | 2009-10-05 | 2013-02-28 | アイ．シー．ヴイ．ティー リミテッド | Method and system for image processing |
DE102009049387B4 (en) | 2009-10-14 | 2016-05-25 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V. | Apparatus, image processing apparatus and method for optical imaging |
US8624925B2 (en) | 2009-10-16 | 2014-01-07 | Qualcomm Incorporated | Content boundary signaling techniques |
US8502909B2 (en) | 2009-10-19 | 2013-08-06 | Pixar | Super light-field lens |
US8259198B2 (en) | 2009-10-20 | 2012-09-04 | Apple Inc. | System and method for detecting and correcting defective pixels in an image sensor |
CN102550041A (en) | 2009-10-30 | 2012-07-04 | 索尼计算机娱乐公司 | Data processing device, tuner and data processing method |
WO2011060809A1 (en) | 2009-11-17 | 2011-05-26 | Telefonaktiebolaget L M Ericsson (Publ) | Synchronization of cameras for multi-view session capturing |
US8102460B2 (en) | 2009-11-20 | 2012-01-24 | Fujifilm Corporation | Solid-state imaging device |
KR101608970B1 (en) | 2009-11-27 | 2016-04-05 | 삼성전자주식회사 | Apparatus and method for processing image using light field data |
US8400555B1 (en) | 2009-12-01 | 2013-03-19 | Adobe Systems Incorporated | Focused plenoptic camera employing microlenses with different focal lengths |
AU2009243486B2 (en) | 2009-12-02 | 2012-12-13 | Canon Kabushiki Kaisha | Processing captured images having geolocations |
US8716953B2 (en) | 2009-12-07 | 2014-05-06 | At&T Intellectual Property I, L.P. | Mechanisms for light management |
KR101367820B1 (en) | 2009-12-21 | 2014-02-27 | 한국전자통신연구원 | Portable multi view image acquisition system and method |
US8330720B2 (en) | 2009-12-18 | 2012-12-11 | Avago Technologies Ecbu Ip (Singapore) Pte. Ltd. | Optical navigation system and method for performing self-calibration on the system using a calibration cover |
JP5490514B2 (en) | 2009-12-22 | 2014-05-14 | 三星電子株式会社 | Imaging apparatus and imaging method |
WO2011081187A1 (en) | 2009-12-28 | 2011-07-07 | 株式会社ニコン | Image capture element, and image capture device |
RU2557084C2 (en) | 2010-01-29 | 2015-07-20 | Конинклейке Филипс Электроникс Н.В. | System and method for interactive illumination control |
AU2011217862B9 (en) | 2010-02-19 | 2014-07-10 | Pacific Biosciences Of California, Inc. | Integrated analytical system and method |
US8749620B1 (en) | 2010-02-20 | 2014-06-10 | Lytro, Inc. | 3D light field cameras, images and files, and methods of using, operating, processing and viewing same |
US8400533B1 (en) | 2010-02-23 | 2013-03-19 | Xilinx, Inc. | Methods of reducing aberrations in a digital image |
US20110205384A1 (en) | 2010-02-24 | 2011-08-25 | Panavision Imaging, Llc | Variable active image area image sensor |
US8817015B2 (en) | 2010-03-03 | 2014-08-26 | Adobe Systems Incorporated | Methods, apparatus, and computer-readable storage media for depth-based rendering of focused plenoptic camera data |
US8411948B2 (en) * | 2010-03-05 | 2013-04-02 | Microsoft Corporation | Up-sampling binary images for segmentation |
US8989436B2 (en) | 2010-03-30 | 2015-03-24 | Nikon Corporation | Image processing method, computer-readable storage medium, image processing apparatus, and imaging apparatus |
JP2011217062A (en) | 2010-03-31 | 2011-10-27 | Sony Corp | Camera system, signal delay amount adjusting method and program |
US9100581B2 (en) | 2010-04-02 | 2015-08-04 | Microsoft Technology Licensing, Llc | Time interleaved exposures and multiplexed illumination |
JP4888579B2 (en) | 2010-04-21 | 2012-02-29 | パナソニック電工株式会社 | Visual function inspection device |
TW201138466A (en) | 2010-04-23 | 2011-11-01 | Hon Hai Prec Ind Co Ltd | Video camera and method for monitoring videos of a person or an object |
CN102860019B (en) | 2010-04-28 | 2015-07-29 | 富士胶片株式会社 | Stereo-picture regenerating unit and method, stereo photographic device, stereoscopic display device |
US9053573B2 (en) * | 2010-04-29 | 2015-06-09 | Personify, Inc. | Systems and methods for generating a virtual camera viewpoint for an image |
US20110273466A1 (en) | 2010-05-10 | 2011-11-10 | Canon Kabushiki Kaisha | View-dependent rendering system with intuitive mixed reality |
US20110292258A1 (en) | 2010-05-28 | 2011-12-01 | C2Cure, Inc. | Two sensor imaging systems |
EP2390772A1 (en) | 2010-05-31 | 2011-11-30 | Sony Ericsson Mobile Communications AB | User interface with three dimensional user input |
US9008457B2 (en) * | 2010-05-31 | 2015-04-14 | Pesonify, Inc. | Systems and methods for illumination correction of an image |
US8406556B2 (en) | 2010-06-10 | 2013-03-26 | Microsoft Corporation | Light transport reconstruction from sparsely captured images |
JP2012003679A (en) | 2010-06-21 | 2012-01-05 | Kyocera Mita Corp | Method for ensuring security of additional application for image forming apparatus, image forming system, and image forming apparatus |
US8493432B2 (en) | 2010-06-29 | 2013-07-23 | Mitsubishi Electric Research Laboratories, Inc. | Digital refocusing for wide-angle images using axial-cone cameras |
JP2012013730A (en) | 2010-06-29 | 2012-01-19 | Kowa Co | Telephoto lens unit |
US8724000B2 (en) | 2010-08-27 | 2014-05-13 | Adobe Systems Incorporated | Methods and apparatus for super-resolution in integral photography |
US8803918B2 (en) | 2010-08-27 | 2014-08-12 | Adobe Systems Incorporated | Methods and apparatus for calibrating focused plenoptic camera data |
US8866822B2 (en) | 2010-09-07 | 2014-10-21 | Microsoft Corporation | Alternate source for controlling an animation |
EP2619761B1 (en) | 2010-09-22 | 2018-06-20 | NDS Limited | Enriching digital photographs |
WO2012047216A1 (en) | 2010-10-06 | 2012-04-12 | Hewlett-Packard Development Company, L.P. | Systems and methods for acquiring and processing image data produced by camera arrays |
EP2633361A1 (en) | 2010-10-26 | 2013-09-04 | BAE Systems Plc. | Display assembly, in particular a head-mounted display |
WO2012068675A1 (en) | 2010-11-25 | 2012-05-31 | Lester Kirkland | An imaging robot |
JP5547854B2 (en) | 2010-11-29 | 2014-07-16 | デジタルオプティックス・コーポレイション・ヨーロッパ・リミテッド | Portrait image synthesis from multiple images captured by a portable device |
JP5858381B2 (en) * | 2010-12-03 | 2016-02-10 | 国立大学法人名古屋大学 | Multi-viewpoint image composition method and multi-viewpoint image composition system |
US8878950B2 (en) | 2010-12-14 | 2014-11-04 | Pelican Imaging Corporation | Systems and methods for synthesizing high resolution images using super-resolution processes |
JP5906062B2 (en) | 2010-12-17 | 2016-04-20 | キヤノン株式会社 | Imaging apparatus and control method thereof |
US9179134B2 (en) | 2011-01-18 | 2015-11-03 | Disney Enterprises, Inc. | Multi-layer plenoptic displays that combine multiple emissive and light modulating planes |
US8908013B2 (en) | 2011-01-20 | 2014-12-09 | Canon Kabushiki Kaisha | Systems and methods for collaborative image capturing |
US8768102B1 (en) | 2011-02-09 | 2014-07-01 | Lytro, Inc. | Downsampling light field images |
US8665440B1 (en) | 2011-02-10 | 2014-03-04 | Physical Optics Corporation | Pseudo-apposition eye spectral imaging system |
US20140176592A1 (en) | 2011-02-15 | 2014-06-26 | Lytro, Inc. | Configuring two-dimensional image processing based on light-field parameters |
JP5689707B2 (en) | 2011-02-15 | 2015-03-25 | 任天堂株式会社 | Display control program, display control device, display control system, and display control method |
US8666191B2 (en) | 2011-03-02 | 2014-03-04 | Canon Kabushiki Kaisha | Systems and methods for image capturing |
JP5623313B2 (en) | 2011-03-10 | 2014-11-12 | キヤノン株式会社 | Imaging apparatus and imaging optical system |
US8619179B2 (en) | 2011-03-28 | 2013-12-31 | Canon Kabushiki Kaisha | Multi-modal image capture apparatus with a tunable spectral response |
US8849132B2 (en) | 2011-03-31 | 2014-09-30 | Eastman Kodak Company | Compensating for periodic nonuniformity in electrophotographic printer |
JP5325255B2 (en) | 2011-03-31 | 2013-10-23 | 富士フイルム株式会社 | Stereoscopic image display device, stereoscopic image display method, and stereoscopic image display program |
JP5786412B2 (en) | 2011-03-31 | 2015-09-30 | ソニー株式会社 | Image processing apparatus, image processing method, and image processing program |
KR101824005B1 (en) | 2011-04-08 | 2018-01-31 | 엘지전자 주식회사 | Mobile terminal and image depth control method thereof |
US9313390B2 (en) | 2011-04-08 | 2016-04-12 | Qualcomm Incorporated | Systems and methods to calibrate a multi camera device |
WO2012149971A1 (en) | 2011-05-04 | 2012-11-08 | Sony Ericsson Mobile Communications Ab | Method, graphical user interface, and computer program product for processing of a light field image |
JP5956808B2 (en) | 2011-05-09 | 2016-07-27 | キヤノン株式会社 | Image processing apparatus and method |
JP2012252321A (en) | 2011-05-10 | 2012-12-20 | Canon Inc | Imaging system, and method of controlling the same |
JP5917017B2 (en) | 2011-05-11 | 2016-05-11 | キヤノン株式会社 | Image processing apparatus, control method therefor, and program |
US8531581B2 (en) | 2011-05-23 | 2013-09-10 | Ricoh Co., Ltd. | Focusing and focus metrics for a plenoptic imaging system |
US8605199B2 (en) | 2011-06-28 | 2013-12-10 | Canon Kabushiki Kaisha | Adjustment of imaging properties for an imaging assembly having light-field optics |
US20130002936A1 (en) | 2011-06-30 | 2013-01-03 | Nikon Corporation | Image pickup apparatus, image processing apparatus, and storage medium storing image processing program |
US8908062B2 (en) | 2011-06-30 | 2014-12-09 | Nikon Corporation | Flare determination apparatus, image processing apparatus, and storage medium storing flare determination program |
US9401039B2 (en) * | 2011-07-01 | 2016-07-26 | Panasonic Intellectual Property Management Co., Ltd. | Image processing device, image processing method, program, and integrated circuit |
US9100587B2 (en) | 2011-07-22 | 2015-08-04 | Naturalpoint, Inc. | Hosted camera remote control |
US9184199B2 (en) | 2011-08-01 | 2015-11-10 | Lytro, Inc. | Optical assembly including plenoptic microlens array |
US8432435B2 (en) | 2011-08-10 | 2013-04-30 | Seiko Epson Corporation | Ray image modeling for fast catadioptric light field rendering |
US8764633B2 (en) | 2011-08-12 | 2014-07-01 | Intuitive Surgical Operations, Inc. | Feature differentiation image capture unit and method in a surgical instrument |
JP5824297B2 (en) | 2011-08-30 | 2015-11-25 | キヤノン株式会社 | Image processing apparatus and method, and imaging apparatus |
US9014470B2 (en) * | 2011-08-31 | 2015-04-21 | Adobe Systems Incorporated | Non-rigid dense correspondence |
JP5206853B2 (en) | 2011-09-08 | 2013-06-12 | カシオ計算機株式会社 | Interpolated image generating device, reconstructed image generating device, interpolated image generating method, and program |
US8879901B2 (en) | 2011-09-13 | 2014-11-04 | Caldwell Photographic, Inc. | Optical attachment for reducing the focal length of an objective lens |
WO2013043751A1 (en) | 2011-09-19 | 2013-03-28 | Pelican Imaging Corporation | Systems and methods for controlling aliasing in images captured by an array camera for use in super resolution processing using pixel apertures |
US8593564B2 (en) | 2011-09-22 | 2013-11-26 | Apple Inc. | Digital camera including refocusable imaging mode adaptor |
CN104081414B (en) | 2011-09-28 | 2017-08-01 | Fotonation开曼有限公司 | System and method for coding and decoding light field image file |
US20130088616A1 (en) | 2011-10-10 | 2013-04-11 | Apple Inc. | Image Metadata Control Based on Privacy Rules |
JP5389139B2 (en) | 2011-10-14 | 2014-01-15 | 株式会社東芝 | Electronic device and display control method |
US8953094B2 (en) | 2011-11-10 | 2015-02-10 | Apple Inc. | Illumination system |
JP6019568B2 (en) | 2011-11-28 | 2016-11-02 | ソニー株式会社 | Image processing apparatus and method, recording medium, and program |
US8854724B2 (en) | 2012-03-27 | 2014-10-07 | Ostendo Technologies, Inc. | Spatio-temporal directional light modulator |
US9264627B2 (en) | 2012-01-09 | 2016-02-16 | Lifetouch Inc. | Video photography system |
EP2817955B1 (en) | 2012-02-21 | 2018-04-11 | FotoNation Cayman Limited | Systems and methods for the manipulation of captured light field image data |
US8948545B2 (en) | 2012-02-28 | 2015-02-03 | Lytro, Inc. | Compensating for sensor saturation and microlens modulation during light-field image processing |
US8811769B1 (en) | 2012-02-28 | 2014-08-19 | Lytro, Inc. | Extended depth of field and variable center of perspective in light-field processing |
US9420276B2 (en) | 2012-02-28 | 2016-08-16 | Lytro, Inc. | Calibration of light-field camera geometry via robust fitting |
US8995785B2 (en) | 2012-02-28 | 2015-03-31 | Lytro, Inc. | Light-field processing and analysis, camera control, and user interfaces and interaction on light-field capture devices |
US8831377B2 (en) | 2012-02-28 | 2014-09-09 | Lytro, Inc. | Compensating for variation in microlens position during light-field image processing |
JP5924978B2 (en) | 2012-02-28 | 2016-05-25 | キヤノン株式会社 | Image processing apparatus and image processing method |
EP2635022A1 (en) | 2012-02-29 | 2013-09-04 | Flir Systems AB | A method and system for performing alignment of a projection image to detected infrared (IR) radiation information |
US8416240B1 (en) | 2012-04-02 | 2013-04-09 | Google Inc. | Determining 3D model information from stored images |
US8994845B2 (en) | 2012-04-27 | 2015-03-31 | Blackberry Limited | System and method of adjusting a camera based on image data |
CN104303493A (en) | 2012-05-09 | 2015-01-21 | 莱特洛公司 | Optimization of optical systems for improved light field capture and manipulation |
CN104285435B (en) | 2012-05-10 | 2016-09-28 | 富士胶片株式会社 | Camera head and signal calibration method |
US8736710B2 (en) | 2012-05-24 | 2014-05-27 | International Business Machines Corporation | Automatic exposure control for flash photography |
US9031319B2 (en) | 2012-05-31 | 2015-05-12 | Apple Inc. | Systems and methods for luma sharpening |
US8872946B2 (en) | 2012-05-31 | 2014-10-28 | Apple Inc. | Systems and methods for raw image processing |
US8953882B2 (en) | 2012-05-31 | 2015-02-10 | Apple Inc. | Systems and methods for determining noise statistics of image data |
US9179126B2 (en) | 2012-06-01 | 2015-11-03 | Ostendo Technologies, Inc. | Spatio-temporal light field cameras |
US9253373B2 (en) | 2012-06-06 | 2016-02-02 | Apple Inc. | Flare detection and mitigation in panoramic images |
US9053582B2 (en) | 2012-06-11 | 2015-06-09 | Disney Enterprises, Inc. | Streaming light propagation |
US9083935B2 (en) | 2012-06-15 | 2015-07-14 | Microsoft Technology Licensing, Llc | Combining multiple images in bracketed photography |
US20130342700A1 (en) | 2012-06-26 | 2013-12-26 | Aharon Kass | System and method for using pattern matching to determine the presence of designated objects in digital images |
US9607424B2 (en) | 2012-06-26 | 2017-03-28 | Lytro, Inc. | Depth-assigned content for depth-enhanced pictures |
US9858649B2 (en) | 2015-09-30 | 2018-01-02 | Lytro, Inc. | Depth-based image blurring |
US10129524B2 (en) | 2012-06-26 | 2018-11-13 | Google Llc | Depth-assigned content for depth-enhanced virtual reality images |
US8978981B2 (en) | 2012-06-27 | 2015-03-17 | Honeywell International Inc. | Imaging apparatus having imaging lens |
GB2503654B (en) | 2012-06-27 | 2015-10-28 | Samsung Electronics Co Ltd | A method and apparatus for outputting graphics to a display |
US8754829B2 (en) | 2012-08-04 | 2014-06-17 | Paul Lapstun | Scanning light field camera and display |
WO2014074202A2 (en) | 2012-08-20 | 2014-05-15 | The Regents Of The University Of California | Monocentric lens designs and associated imaging systems having wide field of view and high resolution |
EP3869797B1 (en) | 2012-08-21 | 2023-07-19 | Adeia Imaging LLC | Method for depth detection in images captured using array cameras |
US9214013B2 (en) | 2012-09-14 | 2015-12-15 | Pelican Imaging Corporation | Systems and methods for correcting user identified artifacts in light field images |
US9940901B2 (en) | 2012-09-21 | 2018-04-10 | Nvidia Corporation | See-through optical image processing |
US8799829B2 (en) | 2012-09-28 | 2014-08-05 | Interactive Memories, Inc. | Methods and systems for background uploading of media files for improved user experience in production of media-based products |
US8799756B2 (en) | 2012-09-28 | 2014-08-05 | Interactive Memories, Inc. | Systems and methods for generating autoflow of content based on image and user analysis as well as use case data for a media-based printable product |
US20140096018A1 (en) | 2012-09-28 | 2014-04-03 | Interactive Memories, Inc. | Methods for Recognizing Digital Images of Persons known to a Customer Creating an Image-Based Project through an Electronic Interface |
US9237263B2 (en) | 2012-10-05 | 2016-01-12 | Vidinoti Sa | Annotation method and apparatus |
US9595553B2 (en) | 2012-11-02 | 2017-03-14 | Heptagon Micro Optics Pte. Ltd. | Optical modules including focal length adjustment and fabrication of the optical modules |
US8997021B2 (en) | 2012-11-06 | 2015-03-31 | Lytro, Inc. | Parallax and/or three-dimensional effects for thumbnail image displays |
US20140139538A1 (en) | 2012-11-19 | 2014-05-22 | Datacolor Holding Ag | Method and apparatus for optimizing image quality based on measurement of image processing artifacts |
US9001226B1 (en) | 2012-12-04 | 2015-04-07 | Lytro, Inc. | Capturing and relighting images using multiple devices |
WO2014087807A1 (en) | 2012-12-05 | 2014-06-12 | 富士フイルム株式会社 | Image capture device, anomalous oblique entry light detection method and program, and recording medium |
US9070050B2 (en) | 2012-12-20 | 2015-06-30 | Rovi Guides, Inc. | Methods and systems for customizing a plenoptic media asset |
US9092890B2 (en) | 2012-12-20 | 2015-07-28 | Ricoh Company, Ltd. | Occlusion-aware reconstruction of three-dimensional scenes from light field images |
JP6220125B2 (en) | 2012-12-28 | 2017-10-25 | キヤノン株式会社 | Imaging apparatus and control method thereof |
WO2014110191A1 (en) | 2013-01-08 | 2014-07-17 | Peripheral Vision, Inc. | Lighting system characterization |
KR101701790B1 (en) | 2013-01-18 | 2017-02-02 | 인텔 코포레이션 | Variance estimation light field reconstruction for defocus blur |
US9686537B2 (en) | 2013-02-05 | 2017-06-20 | Google Inc. | Noise models for image processing |
JP6055332B2 (en) | 2013-02-12 | 2016-12-27 | キヤノン株式会社 | Image processing apparatus, imaging apparatus, control method, and program |
US9497380B1 (en) | 2013-02-15 | 2016-11-15 | Red.Com, Inc. | Dense field imaging |
US9201193B1 (en) | 2013-02-18 | 2015-12-01 | Exelis, Inc. | Textured fiber optic coupled image intensified camera |
US9456141B2 (en) | 2013-02-22 | 2016-09-27 | Lytro, Inc. | Light-field based autofocus |
US9519972B2 (en) | 2013-03-13 | 2016-12-13 | Kip Peli P1 Lp | Systems and methods for synthesizing images from image data captured by an array camera using restricted depth of field depth maps in which depth estimation precision varies |
JP2014199393A (en) | 2013-03-13 | 2014-10-23 | 株式会社東芝 | Image display device |
US20140300753A1 (en) | 2013-04-04 | 2014-10-09 | Apple Inc. | Imaging pipeline for spectro-colorimeters |
US9224782B2 (en) | 2013-04-19 | 2015-12-29 | Semiconductor Components Industries, Llc | Imaging systems with reference pixels for image flare mitigation |
US10334151B2 (en) | 2013-04-22 | 2019-06-25 | Google Llc | Phase detection autofocus using subaperture images |
US9519144B2 (en) | 2013-05-17 | 2016-12-13 | Nvidia Corporation | System, method, and computer program product to produce images for a near-eye light field display having a defect |
JP6480919B2 (en) | 2013-05-21 | 2019-03-13 | クラレト，ホルヘ ヴィセンテ ブラスコ | Plenoptic sensor, manufacturing method thereof, and arrangement having plenoptic sensor |
KR102088401B1 (en) | 2013-05-31 | 2020-03-12 | 삼성전자 주식회사 | Image sensor and imaging device including the same |
US9418400B2 (en) | 2013-06-18 | 2016-08-16 | Nvidia Corporation | Method and system for rendering simulated depth-of-field visual effect |
US8903232B1 (en) | 2013-08-05 | 2014-12-02 | Caldwell Photographic, Inc. | Optical attachment for reducing the focal length of an objective lens |
US20150062178A1 (en) | 2013-09-05 | 2015-03-05 | Facebook, Inc. | Tilting to scroll |
US9013611B1 (en) | 2013-09-06 | 2015-04-21 | Xilinx, Inc. | Method and device for generating a digital image based upon a selected set of chrominance groups |
JP6476658B2 (en) | 2013-09-11 | 2019-03-06 | ソニー株式会社 | Image processing apparatus and method |
US9030580B2 (en) | 2013-09-28 | 2015-05-12 | Ricoh Company, Ltd. | Color filter modules for plenoptic XYZ imaging systems |
US20150104101A1 (en) | 2013-10-14 | 2015-04-16 | Apple Inc. | Method and ui for z depth image segmentation |
US20150103200A1 (en) | 2013-10-16 | 2015-04-16 | Broadcom Corporation | Heterogeneous mix of sensors and calibration thereof |
US9390505B2 (en) | 2013-12-12 | 2016-07-12 | Qualcomm Incorporated | Method and apparatus for generating plenoptic depth maps |
US9392153B2 (en) | 2013-12-24 | 2016-07-12 | Lytro, Inc. | Plenoptic camera resolution |
US9483869B2 (en) | 2014-01-17 | 2016-11-01 | Intel Corporation | Layered reconstruction for defocus and motion blur |
US9305375B2 (en) | 2014-03-25 | 2016-04-05 | Lytro, Inc. | High-quality post-rendering depth blur |
US9414087B2 (en) | 2014-04-24 | 2016-08-09 | Lytro, Inc. | Compression of light field images |
JP6327922B2 (en) | 2014-04-25 | 2018-05-23 | キヤノン株式会社 | Image processing apparatus, image processing method, and program |
US8988317B1 (en) | 2014-06-12 | 2015-03-24 | Lytro, Inc. | Depth determination for light field images |
JP6338467B2 (en) | 2014-06-18 | 2018-06-06 | キヤノン株式会社 | Imaging device |
JP2016005198A (en) | 2014-06-18 | 2016-01-12 | キヤノン株式会社 | Imaging apparatus |
US9210391B1 (en) | 2014-07-31 | 2015-12-08 | Apple Inc. | Sensor data rescaler with chroma reduction |
US9635332B2 (en) | 2014-09-08 | 2017-04-25 | Lytro, Inc. | Saturated pixel recovery in light-field images |
US9444991B2 (en) | 2014-11-13 | 2016-09-13 | Lytro, Inc. | Robust layered light-field rendering |
JP6473608B2 (en) | 2014-11-27 | 2019-02-20 | 三星ディスプレイ株式會社Ｓａｍｓｕｎｇ Ｄｉｓｐｌａｙ Ｃｏ．，Ｌｔｄ． | Image processing apparatus, image processing method, and program |
TWI510086B (en) | 2014-12-03 | 2015-11-21 | Nat Univ Tsing Hua | Digital refocusing method |
US20160253837A1 (en) | 2015-02-26 | 2016-09-01 | Lytro, Inc. | Parallax bounce |
US10085005B2 (en) | 2015-04-15 | 2018-09-25 | Lytro, Inc. | Capturing light-field volume image and video data using tiled light-field cameras |
US10419737B2 (en) | 2015-04-15 | 2019-09-17 | Google Llc | Data structures and delivery methods for expediting virtual reality playback |
US10341632B2 (en) | 2015-04-15 | 2019-07-02 | Google Llc. | Spatial random access enabled video system with a three-dimensional viewing volume |
WO2016168415A1 (en) | 2015-04-15 | 2016-10-20 | Lytro, Inc. | Light guided image plane tiled arrays with dense fiber optic bundles for light-field and high resolution image acquisition |
US10565734B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US10412373B2 (en) | 2015-04-15 | 2019-09-10 | Google Llc | Image capture for virtual reality displays |
US10540818B2 (en) | 2015-04-15 | 2020-01-21 | Google Llc | Stereo image generation and interactive playback |
US20170059305A1 (en) | 2015-08-25 | 2017-03-02 | Lytro, Inc. | Active illumination for enhanced depth map generation |
US10567464B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video compression with adaptive view-dependent lighting removal |
US10546424B2 (en) | 2015-04-15 | 2020-01-28 | Google Llc | Layered content delivery for virtual and augmented reality experiences |
US11328446B2 (en) | 2015-04-15 | 2022-05-10 | Google Llc | Combining light-field data with active depth data for depth map generation |
US20160307368A1 (en) | 2015-04-17 | 2016-10-20 | Lytro, Inc. | Compression and interactive playback of light field pictures |
EP3099055A1 (en) | 2015-05-29 | 2016-11-30 | Thomson Licensing | Method and apparatus for displaying a light field based image on a user's device, and corresponding computer program product |
US9979909B2 (en) | 2015-07-24 | 2018-05-22 | Lytro, Inc. | Automatic lens flare detection and correction for light-field images |
US9639945B2 (en) | 2015-08-27 | 2017-05-02 | Lytro, Inc. | Depth-based application of image effects |
US11293873B2 (en) | 2015-09-08 | 2022-04-05 | Xerox Corporation | Methods and devices for improved accuracy of test results |
US10028437B2 (en) | 2015-10-06 | 2018-07-24 | Deere & Company | System for clearing a feeder house and belt pickup |
US20170256036A1 (en) | 2016-03-03 | 2017-09-07 | Lytro, Inc. | Automatic microlens array artifact correction for light-field images |
EP3220351A1 (en) | 2016-03-14 | 2017-09-20 | Thomson Licensing | Method and device for processing lightfield data |
US10127712B2 (en) | 2016-07-08 | 2018-11-13 | Google Llc | Immersive content framing |
US10186756B2 (en) | 2016-08-01 | 2019-01-22 | Intel IP Corporation | Antennas in electronic devices |
US10679361B2 (en) | 2016-12-05 | 2020-06-09 | Google Llc | Multi-view rotoscope contour propagation |
US9900510B1 (en) | 2016-12-08 | 2018-02-20 | Lytro, Inc. | Motion blur for light-field images |
-
2017
- 2017-03-17 US US15/462,752 patent/US10275892B2/en active Active
- 2017-05-31 CN CN201780034730.5A patent/CN109479098B/en active Active
- 2017-05-31 JP JP2018562978A patent/JP6655737B2/en not_active Expired - Fee Related
- 2017-05-31 EP EP17810729.8A patent/EP3469788A4/en not_active Withdrawn
- 2017-05-31 WO PCT/US2017/035148 patent/WO2017213923A1/en unknown
- 2017-05-31 KR KR1020187034558A patent/KR102185179B1/en active IP Right Grant
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102388391A (en) * | 2009-02-10 | 2012-03-21 | 汤姆森特许公司 | Video matting based on foreground-background constraint propagation |
US20130121577A1 (en) * | 2009-10-30 | 2013-05-16 | Jue Wang | Methods and Apparatus for Chatter Reduction in Video Object Segmentation Using Optical Flow Assisted Gaussholding |
US20120057040A1 (en) * | 2010-05-11 | 2012-03-08 | Byung Kwan Park | Apparatus and method for processing light field data using a mask with an attenuation pattern |
US20130321574A1 (en) * | 2012-06-04 | 2013-12-05 | City University Of Hong Kong | View synthesis distortion model for multiview depth video coding |
US20140003719A1 (en) * | 2012-06-29 | 2014-01-02 | Xue Bai | Adaptive Trimap Propagation for Video Matting |
US20140347540A1 (en) * | 2013-05-23 | 2014-11-27 | Samsung Electronics Co., Ltd | Image display method, image display apparatus, and recording medium |
Non-Patent Citations (1)
Title |
---|
CHUANG,YUNGYU: "《Video matting of complex scenes》", 《ACM TRANSACTIONS ON GRAPHICS》 * |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111223106A (en) * | 2019-10-28 | 2020-06-02 | 稿定（厦门）科技有限公司 | Full-automatic portrait mask matting method and system |
CN111223106B (en) * | 2019-10-28 | 2022-08-09 | 稿定（厦门）科技有限公司 | Full-automatic portrait mask matting method and system |
US20200074185A1 (en) * | 2019-11-08 | 2020-03-05 | Intel Corporation | Fine-grain object segmentation in video with deep features and multi-level graphical models |
US11763565B2 (en) * | 2019-11-08 | 2023-09-19 | Intel Corporation | Fine-grain object segmentation in video with deep features and multi-level graphical models |
US11689693B2 (en) * | 2020-04-30 | 2023-06-27 | Boe Technology Group Co., Ltd. | Video frame interpolation method and device, computer readable storage medium |
CN111754528A (en) * | 2020-06-24 | 2020-10-09 | Oppo广东移动通信有限公司 | Portrait segmentation method, portrait segmentation device, electronic equipment and computer-readable storage medium |
CN112200756A (en) * | 2020-10-09 | 2021-01-08 | 电子科技大学 | Intelligent bullet special effect short video generation method |
US20220222854A1 (en) * | 2021-01-13 | 2022-07-14 | Samsung Electronics Co., Ltd. | Dynamic calibration correction in multi-frame, multi-exposure capture |
Also Published As
Publication number | Publication date |
---|---|
JP2019525515A (en) | 2019-09-05 |
US20170358092A1 (en) | 2017-12-14 |
KR102185179B1 (en) | 2020-12-01 |
CN109479098B (en) | 2021-06-29 |
WO2017213923A1 (en) | 2017-12-14 |
US10275892B2 (en) | 2019-04-30 |
KR20180132946A (en) | 2018-12-12 |
EP3469788A1 (en) | 2019-04-17 |
EP3469788A4 (en) | 2020-03-11 |
JP6655737B2 (en) | 2020-02-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109479098A (en) | Multiple view scene cut and propagation | |
CN110188760B (en) | Image processing model training method, image processing method and electronic equipment | |
Karsch et al. | Depth transfer: Depth extraction from video using non-parametric sampling | |
Zhou et al. | Color map optimization for 3d reconstruction with consumer depth cameras | |
Fried et al. | Perspective-aware manipulation of portrait photos | |
US11348267B2 (en) | Method and apparatus for generating a three-dimensional model | |
CN102196292B (en) | Human-computer-interaction-based video depth map sequence generation method and system | |
Ward et al. | Depth director: A system for adding depth to movies | |
Kong et al. | Intrinsic depth: Improving depth transfer with intrinsic images | |
CN110322468A (en) | A kind of automatic edit methods of image | |
Zhi et al. | Toward dynamic image mosaic generation with robustness to parallax | |
CN110827312B (en) | Learning method based on cooperative visual attention neural network | |
Tong et al. | Stereopasting: interactive composition in stereoscopic images | |
Zhang et al. | Refilming with depth-inferred videos | |
Wei et al. | Simulating shadow interactions for outdoor augmented reality with RGBD data | |
Xue et al. | 3-d modeling from a single view of a symmetric object | |
Lin et al. | Extracting depth and radiance from a defocused video pair | |
Englert et al. | Enhancing the ar experience with machine learning services | |
Shen et al. | Re-texturing by intrinsic video | |
Liu et al. | Fog effect for photography using stereo vision | |
Zhang et al. | Coherent video generation for multiple hand-held cameras with dynamic foreground | |
EP4150560B1 (en) | Single image 3d photography with soft-layering and depth-aware inpainting | |
Calagari et al. | Data driven 2-D-to-3-D video conversion for soccer | |
Comino Trinidad et al. | Easy authoring of image-supported short stories for 3d scanned cultural heritage | |
Yan et al. | Re-texturing by intrinsic video |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |