US8285536B1 - Optimizing parameters for machine translation - Google Patents
Optimizing parameters for machine translation Download PDFInfo
- Publication number
- US8285536B1 US8285536B1 US12/533,519 US53351909A US8285536B1 US 8285536 B1 US8285536 B1 US 8285536B1 US 53351909 A US53351909 A US 53351909A US 8285536 B1 US8285536 B1 US 8285536B1
- Authority
- US
- United States
- Prior art keywords
- translation
- hypergraph
- language
- gram
- lattice
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
Definitions
- This specification relates to statistical machine translation.
- Machine translation attempts to identify a most probable translation in a target language given a particular input in a source language. For example, when translating a sentence from French to English, statistical machine translation identifies the most probable English sentence given the French sentence. This maximum likelihood translation can be expressed as:
- the most likely e i.e., the most likely English translation
- f i.e., the probability that a given English sentence would be translated into the French sentence
- a first decoder can generate a list of possible translations, e.g., an N-best list.
- a second decoder e.g., a Minimum Bayes-Risk (MBR) decoder, can then be applied to the list to ideally identify which of the possible translations are the most accurate, as measured by minimizing a loss function that is part of the identification.
- MBR Minimum Bayes-Risk
- an N-best list contains between 100 and 10,000 candidate translations (or hypotheses). Increasing the number of candidate translations and efficiency in which the candidate translations are encoded improves the translation performance of an MBR decoder.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of accessing a translation hypergraph that represents a plurality of candidate translations, the translation hypergraph including a plurality of paths including nodes connected by edges; calculating first posterior probabilities for each edge in the translation hypergraph; calculating second posterior probabilities for each n-gram represented in the translation hypergraph based on the first posterior probabilities; and performing decoding on the translation hypergraph using the second posterior probabilities to convert a sample text from a first language to a second language.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- the method further includes generating the translation hypergraph using Synchronous Context Free Grammars (SCFG).
- SCFG Synchronous Context Free Grammars
- Performing decoding on the translation hypergraph includes: for each node, determining a score for each n-gram that is represented by a path from a source node of the translation hypergraph to the node, where the score is a highest second posterior probability for edges on paths that include the n-gram.
- Calculating second posterior probabilities includes calculating:
- ⁇ ) ⁇ E ⁇ ⁇ ⁇ ⁇ e ⁇ E ⁇ f ⁇ ( e , w , E ) ⁇ P ⁇ ( E
- ⁇ ) is the posterior probability of the n-gram w in the translation hypergraph
- E is a candidate translation
- F is the sample text in the first language
- e is an edge
- f(e,w,E) 1 when wee, P(e
- ⁇ ), and e′ precedes e on E; otherwise, f(e,w,E) 0.
- another aspect of the subject matter described in this specification can be embodied in methods that include the actions of accessing a translation lattice that represents a plurality of candidate translations, the translation lattice including a plurality of paths including nodes connected by edges; calculating posterior probabilities for each n-gram represented in the translation lattice including: determining, for each node, a score for each n-gram that is represented by a path from a source node of the translation lattice to the node, where the score is a highest posterior probability for edges on paths that include the n-gram; and performing decoding on the translation lattice using the scores to convert a sample text from a first language to a second language.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- Calculating posterior probabilities includes calculating:
- ⁇ ) ⁇ E ⁇ ⁇ ⁇ ⁇ e ⁇ E ⁇ f ⁇ ( e , w , E ) ⁇ P ⁇ ( E
- ⁇ ) is the posterior probability of the n-gram w in the translation lattice
- E is a candidate translation
- F is the sample text in the first language
- e is an edge
- f(e,w,E) 1 when wee, P(e
- ⁇ ), and e′ precedes e on E; otherwise, f(e,w,E) 0.
- another aspect of the subject matter described in this specification can be embodied in methods that include the actions of determining, for a plurality of feature functions in a translation hypergraph, a corresponding plurality of error surfaces for each of one or more candidate translations represented in the translation hypergraph; combining the plurality of error surfaces to produce a combined error surface; traversing the combined error surface to select weights for the feature functions that minimize error counts for traversing the combined error surface; and applying the selected weights to reduce an error count in a decoder that converts a sample of text from a first language to a second language.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- Determining a corresponding plurality of error surfaces includes: generating a factor graph from the translation hypergraph; and generating the plurality of error surfaces from the factor graph.
- the method further includes generating the translation hypergraph using Synchronous Context Free Grammars (SCFG).
- SCFG Synchronous Context Free Grammars
- the method further includes applying SCFG rules to line segments represented in the plurality of error surfaces to expand the plurality of error surfaces, and where combining the plurality of error surfaces includes calculating a sum of one or more first error surfaces and calculating a union of one or more second error surfaces.
- the sum is a Minkowski sum.
- the selected weights are applied to select a first candidate translation using Minimum-Bayes Risk (MBR).
- MRR Minimum-Bayes Risk
- the method further includes performing a backoff and using Maximum A Posteriori (MAP) based on at least one of the selected weights to determine a second candidate translation.
- MBR Maximum A Posteriori
- MAP Maximum A Posteriori
- , g i (E′,F) ⁇ w:
- i # w (E′)P(w
- another aspect of the subject matter described in this specification can be embodied in computer readable mediums that store data including a translation hypergraph that represents a plurality of candidate translations, the translation hypergraph including a plurality of paths including nodes connected by edges; where first posterior probabilities are associated with each edge in the translation hypergraph, and second posterior probabilities calculated based on the first posterior probabilities are associated with each n-gram represented in the translation hypergraph.
- Other embodiments of this aspect include corresponding systems, apparatus, and methods.
- the second posterior probabilities are expressed as:
- ⁇ ) ⁇ E ⁇ ⁇ ⁇ ⁇ e ⁇ E ⁇ f ⁇ ( e , w , E ) ⁇ P ⁇ ( E
- ⁇ ) is the posterior probability of the n-gram w in the translation hypergraph
- E is a candidate translation
- F is the sample text in the first language
- e is an edge
- f(e,w,E) 1 when w ⁇ e, P(e
- ⁇ ), and e′ precedes e on E; otherwise, f(e,w,E) 0.
- Efficient MBR decoding of a lattice or hypergraph increases sizes of hypothesis and evidence spaces, thereby increasing a number of candidate translations available and the likelihood of obtaining an accurate translation.
- efficient MBR decoding provides a better approximation of a corpus BLEU score (as described in further detail below), thereby further improving translation performance.
- efficient MBR decoding of a lattice or hypergraph is runtime efficient, thereby increasing the flexibility of statistical machine translation since the decoding can be performed at runtime.
- Efficient lattice or hypergraph-based Minimum Error Rate Training provides exact error surfaces for all translations in a translation lattice or hypergraph, thereby further improving translation performance of a statistical machine translation system.
- the systems and techniques for efficient lattice or hypergraph-based MERT are also space and runtime efficient, thereby reducing an amount of memory used and increasing a speed of translation performance.
- Optimizing MBR parameters using efficient lattice or hypergraph-based MERT also reduces an amount of user intervention required to select MBR parameters, thereby providing a better approximation of a corpus BLEU score for use in MBR decoding and further improving translation performance.
- FIG. 1 is a conceptual diagram of an example process for translating input text from a source language to a target language.
- FIG. 2 illustrates an example translation lattice
- FIG. 3 illustrates an example translation hypergraph
- FIG. 4 shows an example process for efficient MBR decoding of a translation lattice.
- FIG. 5 shows an example process for efficient MBR decoding of a translation hypergraph.
- FIG. 6 includes an example transformation of a hypergraph into a factor graph.
- FIG. 7 shows an example process for efficient MERT on a translation hypergraph.
- FIG. 8 shows an example of a generic computer device and a generic mobile device.
- Machine translation seeks to take input text in one language and accurately convert it into text in another language. Generally, the accuracy of a translation is measured against the ways in which expert humans would translate the input.
- An automatic translation system can analyze prior translations performed by human experts to form a statistical model of translation from one language to another. No such model can be complete, however, because the meaning of words often depends on context. Consequently, a step-wise word-for-word transformation of words from one language to another may not provide acceptable results. For example, idioms such as “babe in the woods” or slang phrases, do not translate well in a literal word-for-word transformation.
- Adequate language models can help provide such context for an automatic translation process.
- the models can, for example, provide indications regarding the frequency with which two words appear next to each other in normal usage, e.g., in training data, or that other groups of multiple words or elements (n-grams) appear in a language.
- An n-gram is a sequence of n consecutive tokens, e.g., words or characters.
- An n-gram has an order or size, which is the number of tokens in the n-gram. For example, a 1-gram (or unigram) includes one token; a 2-gram (or bi-gram) includes two tokens.
- a given n-gram can be described according to different portions of the n-gram.
- An n-gram can be described as a context and a future token, (context, w), where the context has a length n ⁇ 1 and w represents the future token.
- the 3-gram “c 1 c 2 c 3 ” can be described in terms of an n-gram context and a future token, where c 1 , c 2 , and c 3 each represent a word (or a character in other examples).
- the n-gram left context includes all tokens of the n-gram preceding the last token of the n-gram.
- “c i c 2 ” is the context.
- the left most token in the context is referred to as the left token.
- the future token is the last token of the n-gram, which in the example is “c 3 ”.
- the n-gram can also be described with respect to a right context.
- the right context includes all tokens of the n-gram following the first token of the n-gram, represented as a (n ⁇ 1)-gram. In the example above, “c 2 c 3 ” is the right context.
- Each n-gram can have an associated probability estimate, e.g., a log-probability, that is calculated as a function of a count of occurrences in training data relative to a count of total occurrences in the training data.
- the probabilities of n-grams being a translation of input text is trained using the relative frequency of the n-grams represented in a target language as a reference translation of corresponding text in a source language in training data, e.g., training data including a set of text in the source language and corresponding text in the target language.
- a distributed training environment is used for large training data (e.g., terabytes of data).
- large training data e.g., terabytes of data.
- MapReduce MapReduce Details of MapReduce are described in J. Dean and S. Ghemawat, MapReduce: Simplified Data Processing on Large Clusters , Proceedings of the 6th Symposium on Operating Systems Design and Implementation, pp. 137-150 (Dec. 6, 2004).
- Past usage represented by a training set can be used to predict how samples in one language should be translated to a target language.
- the n-grams, associated probability estimates, and respective counts can be stored in a language model for use by a decoder, e.g., a Bayesian decoder to identify translations for input text.
- a score indicating the likelihood that input text can be translated to corresponding text in a target language can be calculated by mapping the n-grams included in the input text to associated probability estimates for a particular translation.
- FIG. 1 is a conceptual diagram of an example process 100 for translating input text from a source language to a target language.
- a source sample 102 is shown as a passage of Chinese text, and is provided to a first decoder 104 .
- the decoder 104 can take a variety of forms and can be used in an attempt to maximize a posterior probability for the passage, given a training set of documents 106 that has been provided to the decoder 104 during a training phase for the decoder 104 .
- the decoder 104 can select n-grams from within the document and attempt to translate the n-grams.
- the decoder 104 can be provided with a re-ordering model, alignment model, and language model, among other possible models.
- the models direct the decoder 104 in selecting n-grams from within the sample 102 for translation.
- the model can use delimiters, e.g., punctuation such as a comma or period, to identify the end of an n-gram that may represent a word.
- the decoder 104 can produce a variety of outputs, e.g., data structures that include translation hypotheses 108 .
- the decoder 104 can produce an N-best list of translations.
- the decoder 104 generates a representation of the translation hypotheses 108 that is encoded in a translation lattice or a translation hypergraph, as described in further detail below.
- a second decoder 110 then processes the translation hypotheses 108 . While the first decoder 104 is generally aimed at maximizing the posterior probability of the translation, i.e., matching the input to what the historical collection of documents 106 may indicate to be a best match to past expert manual translations of other passages, the second decoder 110 is aimed at maximizing a quality measure for the translation. As such, the second decoder 110 may re-rank the candidate translations that reside in the translation lattice so as to produce a “best” translation that may be displayed to a user of the system 100 . This translation is represented by the English sample 112 corresponding to the translation of the Chinese sample 102 .
- the second decoder 110 can use a process known as MBR decoding, which seeks the hypothesis (or candidate translation) that minimizes the expected error in classification. The process thus directly incorporates a loss function into the decision criterion for making a translation selection.
- FIG. 2 illustrates an example translation lattice 200 .
- translation lattice 200 is a translation n-gram lattice that can be considered to be a compact representation for very large N-best lists of translation hypotheses and their likelihoods.
- the lattice is an acyclic weighted finite state acceptor including states (e.g., nodes 0 through 6 ) and arcs (or edges) representing transitions between states. Each arc is associated with an n-gram (e.g., a word or phrase) and a weight.
- states e.g., nodes 0 through 6
- arcs or edges representing transitions between states.
- Each arc is associated with an n-gram (e.g., a word or phrase) and a weight.
- n-grams are represented by labels “a”, “b”, “c”, “d”, and “e”.
- State 0 is connected to a first arc that provides a path to state 1 , a second arc that provides a path to state 4 from state 1 , and a third arc that provides a path to state 5 from state 4 .
- the first arc is associated with “a” and weight 0.5
- the second arc is associated with “b” and weight 0.6
- the third arc is also associated with “d” and weight 0.3.
- Each path in the translation lattice 200 expresses a candidate translation. Aggregation of the weights along a path produces a weight of the path's candidate translation J(E, F) according to the model.
- the weight of the path's candidate translation represents the posterior probability of the translation E given the source sentence F as:
- FIG. 3 illustrates an example translation hypergraph.
- the translation hypergraph is a directed hypergraph.
- the translation hypergraph is produced by a Synchronous Context Free Grammars (SCFG)-based machine translation system.
- a hyperedge is a non-empty subset of V.
- a number of tail vertices is called the arity (
- the arity of a hypergraph is the maximum arity of its hyperedges.
- a hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph, i.e., a lattice.
- Each hyperedge can be associated with a rule r e from the SCFG.
- a number of nonterminals, e.g., phrase pairs, on the right-hand side of r e corresponds with the arity of e.
- Following a path in a translation hypergraph produces a translation hypothesis E and its associated sequence of SCFG rules, i.e., a derivation tree for E.
- Minimum Bayes-Risk (MBR) decoding aims to find a translation hypothesis, e.g., a candidate translation, that has the least expected error under the probability model.
- Statistical machine translation can be described as mapping of input text F in a source language to translated text E in a target language.
- a decoder ⁇ (F) e.g., decoder 104 , can perform the mapping.
- the reference translation E is known, the decoder performance can be measured by the loss function L(E, ⁇ (F)).
- the MBR decoder e.g., the second decoder 110 , can be represented by:
- R(E) represents the Bayes risk of candidate translation E′ under the loss function L
- ⁇ represents the space of translations.
- the space ⁇ is an N-best list produced, for example, by the first decoder 104 .
- ⁇ represents candidate translations encoded in the translation lattice.
- ⁇ represents candidate translations encoded in the translation hypergraph.
- E ⁇ arg ⁇ ⁇ max E ′ ⁇ ⁇ ⁇ ⁇ E ⁇ ⁇ ⁇ G ⁇ ( E , E ′ ) ⁇ P ⁇ ( E
- MBR decoding uses different spaces for hypothesis selection and risk computation.
- the hypothesis can be selected from a translation lattice and the risk can be computed based on a translation hypergraph.
- the MBR decoder can be rewritten as:
- E ⁇ arg ⁇ ⁇ max E ′ ⁇ ⁇ h ⁇ ⁇ E ⁇ ⁇ e ⁇ G ⁇ ( E , E ′ ) ⁇ P ⁇ ( E
- MBR decoding can be improved by using larger spaces, i.e., hypothesis and risk computation spaces.
- Lattices and hypergraphs can include more candidate translations than an N-best list.
- lattices and hypergraphs can include more than one billion candidate translations.
- representing the hypothesis and risk computation spaces using lattices or hypergraphs increases the accuracy of MBR decoding, thereby increasing the likelihood that an accurate translation is provided.
- a gain function G is expressed as a sum of local gain functions g i .
- a gain function can be considered to be a local gain function if it can be applied to all paths in a lattice using Weighted Finite State Transducers (WFSTs) composition, resulting in a o(N) increase in the number of states N in the lattice.
- WFSTs Weighted Finite State Transducers
- E ′) ⁇ w ⁇ # w ( E ′) ⁇ ( E ) where ⁇ w is a constant, # w (E′) is a number of times that w occurs in E′, and ⁇ w (E) is 1 if w ⁇ E and 0 otherwise.
- G(E, E′) can be written as a sum of local gain functions and a constant ⁇ 0 times the length of the hypothesis E′
- the overall gain function can be expressed as:
- G ⁇ ( E , E ′ ) ⁇ 0 ⁇ ⁇ E ′ ⁇ + ⁇ w ⁇ N ⁇ g w ⁇ ( E
- E ′ ) ⁇ 0 ⁇ ⁇ E ′ ⁇ + ⁇ w ⁇ N ⁇ ⁇ ⁇ w ⁇ ⁇ # w ⁇ ( E ′ ) ⁇ ⁇ w ⁇ ( E ) .
- Equation 1 Equation 1
- E ⁇ arg ⁇ ⁇ max E ′ ⁇ ⁇ ⁇ ⁇ 0 ⁇ ⁇ E ′ ⁇ + ⁇ w ⁇ N ⁇ ⁇ ⁇ w ⁇ ⁇ # w ⁇ ( E ′ ) ⁇ P ⁇ ( w
- ⁇ ) is the posterior probability of the n-gram w in the lattice, and can be expressed as:
- Equation 3 The posterior probability P(w
- ⁇ ) ⁇ E ⁇ ⁇ ⁇ ⁇ e ⁇ E ⁇ f ⁇ ( e , w , E ) ⁇ P ⁇ ( E
- f(e,w,E) is a score assigned to edge e on path E containing n-gram w.
- f(e,w,E) 1 when w ⁇ e, P(e
- ⁇ ), and e′ precedes e on E; otherwise, f(e,w,E) 0.
- the edge that contributes n-gram w and has the highest edge posterior probability relative to its predecessors on the path E is counted.
- f(e,w,E) is calculated based on the full path of E
- f(e,w,E) can be calculated based on local statistics by using an approximation f*(e,w, ⁇ ) that counts the edge e with n-gram w that has the highest arc posterior probability relative to predecessors in the entire lattice ⁇ .
- f*(e,w, ⁇ ) that counts the edge e with n-gram w that has the highest arc posterior probability relative to predecessors in the entire lattice ⁇ .
- ⁇ ) ⁇ E ⁇ ⁇ ⁇ ⁇ e ⁇ E ⁇ f * ⁇ ( e , w , ⁇ ) ⁇ P ⁇ ( E
- F ) ⁇ e ⁇ E ⁇ 1 w ⁇ e ⁇ f * ⁇ ( e , w , ⁇ ) ⁇ ⁇ E ⁇ ⁇ ⁇ ⁇ 1 E ⁇ ( e ) ⁇ P ⁇ ( E
- F ) ⁇ e ⁇ E ⁇ 1 w ⁇ e ⁇ f * ⁇ ( e , w , ⁇ ) ⁇ P ⁇ ( e
- FIG. 4 shows an example process 400 for efficient MBR decoding of a translation lattice.
- efficient MBR decoding of a lattice will be described with respect to a system that performs the decoding.
- the system accesses 410 a translation lattice that represents candidate translations.
- the translation lattice includes paths including nodes connected by edges.
- the system can sort the lattice nodes topologically. For example, the nodes in the lattice can be sorted according to the arrangement in which the nodes are connected to each other in the lattice.
- the system calculates backward probabilities of each node.
- each node represents a state in the lattice
- the backward probability of a particular node is the probability of producing a sequence of words (e.g., a sentence) given that the current location in traversing a path of the lattice is the particular node (or state).
- the system calculates 420 posterior probabilities for each n-gram represented in the translation lattice.
- the calculation includes determining, for each node, a score for each n-gram that is represented by a path from a source node of the translation lattice to the node, where the score is a highest posterior probability for edges on paths that include the n-gram.
- a score Score(w,t) is calculated for each n-gram w that lies on a path from the source node to t.
- Each n-gram w introduced by each edge e is processed to determine a final value of Score(w,t) that it is the highest posterior probability among all edges on the paths that terminate on t and contain n-gram w.
- the system then associates each edge with a score using the risk (or cost) calculated using Equation 2 above.
- the system performs 430 decoding on the translation lattice using the scores to convert a sample text from a first language to a second language. For example, the system uses the MBR decoder to identify the path in the lattice that ideally represents the most likely translation.
- the technique described in reference to FIG. 4 can be refined to rescore translation hypergraphs generated by a SCFG based machine translation system.
- FIG. 5 shows an example process 500 for efficient MBR decoding of a translation hypergraph.
- efficient MBR decoding of a hypergraph will be described with respect to a system that performs the decoding.
- the system accesses 510 a translation hypergraph that represents candidate translations.
- the translation hypergraph includes paths including nodes connected by edges.
- the system can sort the hypergraph nodes topologically. For example, the nodes in the hypergraph can be sorted according to the arrangement in which the nodes are connected to each other in the lattice.
- the system calculates inside probabilities of each node.
- the inside probability of a particular node is the probability of producing a sequence of words (e.g., a sentence) given that the current location in traversing a path of the hypergraph is the particular node, i.e., a nonterminal.
- the system calculates 520 first posterior probabilities for each edge in the translation hypergraph, e.g., P(e
- the system calculates 530 second posterior probabilities for each n-gram represented in the translation hypergraph based on the first posterior probabilities.
- Each n-gram w on the tail nodes T(e) e.g., identified by propagating n-gram prefixes and suffixes at each node, is processed to determine a final value of Score(w, T(e)) that it is the highest posterior probability among all hyperedges on the paths that contain n-gram w.
- n-grams can be represented by concatenating lower order n-grams (e.g., words) from either side of a node.
- n-grams on the tail nodes are compared for each hyperedge e in the hypergraph.
- the highest Score(w, T(e)) is used for duplicate n-grams, e.g., same n-grams, that occur from propagating multiple tail nodes.
- the system applies the rule (e.g., SCFG rule) on hyperedge e to the n-grams on T(e).
- the system associates each hyperedge with a score using the risk (or cost) calculated using Equation 2 above.
- the system performs 540 decoding on the translation hypergraph using the second posterior probabilities to convert a sample text from a first language to a second language. For example, the system uses the MBR decoder to identify the path in the hyperedge that ideally represents the most likely translation.
- Minimum error rate training measures an error metric of a decision rule for classification, e.g., MBR decision rule using a zero-one loss function.
- MERT estimates model parameters such that the decision under the zero-one loss function maximizes an end-to-end performance measure on a training corpus.
- the training procedure optimizes an unsmoothed error count.
- the translation that maximizes the a-posteriori probability can be selected based on arg e max P(e
- e represents a translation in a target language (e.g., an English translation) and f represents text in a source language (e.g., a French sentence).
- the log-linear translation model can be expressed as:
- the feature function weights are the parameters of the model, and the MERT criterion finds a parameter set ⁇ 1 M that minimizes the error count on a representative set of training sentences using the decision rule, e.g., arg e max P(e
- the decision rule e.g., arg e max P(e
- the MERT criterion can be expressed as:
- a line optimization technique can be used to train a linear model under the MERT criterion.
- the line optimization determines, for each feature function h m , and sentence f s , the exact error surface on a set of candidate translations G.
- the feature function weights are then adjusted by traversing the combined error surfaces of sentences in the training corpus and setting weights to a point where the resulting error is a minimum.
- e ⁇ ⁇ ( f s ; ⁇ ) arg ⁇ ⁇ max e ⁇ C s ⁇ ⁇ ( ⁇ 1 M + ⁇ ⁇ d 1 M ) T ⁇ h 1 M ⁇ ( e , f s ) ⁇ .
- C s defines K lines where each line may be divided into at most K line segments due to possible intersections with other K ⁇ 1 lines.
- the decoder determines a respective candidate translation that yields the highest score and therefore corresponds to a topmost line segment.
- a sequence of topmost line segments constitute an upper envelope that is a point-wise maximum over all lines defined by C s .
- the upper envelope is a convex hull and can be inscribed with a convex polygon whose edges are the segments of a piecewise linear function in ⁇ .
- the upper envelope is calculated using a sweep line technique. Details of the sweep line technique are described, for example, in W. Macherey, F. Och, I. Thayer, and J. Uzskoreit, Lattice - based Minimum Error Rate Training for Statistical Machine Translation , Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725-734, Honolulu, October 2008.
- the upper envelope provides an exhaustive representation of all possible outcomes that the decoder may yield if ⁇ is shifted along a given direction.
- Translation candidates of an upper envelope's constituent line segments can be projected onto corresponding error counts to produce an exact and unsmoothed error surface for all candidate translations defined by C s .
- the error surface can be traversed to find ⁇ under which ⁇ 1 M + ⁇ circumflex over ( ⁇ ) ⁇ d 1 M minimizes a global error.
- MERT can be applied to a hypergraph to efficiently calculate and represent upper envelopes (e.g., error surfaces) over all candidate translations represented in the hypergraph.
- envelopes are generated from the hypergraph's source nodes bottom-up to its unique root node, thereby expanding the envelopes through application of SCFG rules to partial candidate translations that are associated with the envelope's constituent line segments.
- the envelopes are combined using a sum operation and maximum operation over convex polygons, as described in further detail below.
- FIG. 6 includes an example transformation of a hypergraph into a factor graph 600 .
- FIG. 6 shows an isomorphic transformation, specifically a transformation of a hyperedge with arity 3 .
- >1, is replaced with a subgraph including a new vertex v ⁇ (e), e.g., node 640 .
- Incoming and outgoing edges of v ⁇ (e) connect the same head and tail nodes in the factor graph as were connected by e in the hypergraph. The unique outgoing edge of v ⁇ (e) is associated with rule r e , and incoming edges are not linked to rules.
- Rules associated with hyperedges specify how line segments in the envelopes of the corresponding hyperedge's tail nodes can be combined.
- X 1 and X 2 are substituted in the rule with candidate translations associated with line segments in envelopes Env(v 1 ) and Env(v 2 ), respectively.
- Calculating and propagating tail envelopes over a hyperedge e to its head node is performed by specifying how envelopes associated with tail vertices are propagated to the head vertex for the node types and .
- An envelope e.g., envelope 652
- ⁇ e.g., node 650
- the sum is calculated using a Minkowski sum.
- the Minkowski sum provides an upper bound to the number of line segments that constitute the resulting envelope, i.e., the bound is the sum over the number of line segments in the envelopes of the incoming edges, or
- Using the Minkowski sum can be advantageous because the growth rate of the calculation is linear (O(n)) with respect to the size of the individual envelopes.
- An envelope e.g., envelope 662
- An envelope is calculated from a node marked with the symbol , e.g., node 660 , by calculating the union of envelopes of the incoming edges, e.g., envelopes 664 and 668 .
- the union, or “max”, can be calculated using a sweep line technique. Details of the sweep line technique are described, for example, in W. Macherey, F. Och, I. Thayer, and J. Uzskoreit, Lattice-based Minimum Error Rate Training for Statistical Machine Translation, Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725 734, Honolulu, October 2008.
- the envelopes associated with tail nodes T(e) for each incoming hyperedge of the node are combined by summing the envelopes as described above. Then, for each incoming hyperedge e, the resulting envelope is expanded by applying a rule r e to its constituent line segments. Envelopes associated with different incoming hyperedges of node v are combined (and reduced), e.g., by calculating the union as described above.
- FIG. 7 shows an example process 700 for efficient MERT on a translation hypergraph.
- efficient MERT on a translation hypergraph will be described with respect to a system that performs the MERT.
- the system determines 710 , for feature functions in a translation hypergraph, a corresponding collection of error surfaces for each of one or more candidate translations represented in the translation hypergraph.
- the system combines 720 the error surfaces to produce a combined error surface.
- the system traverses 730 the combined error surface to select weights for the feature functions that minimize error counts for traversing the combined error surface.
- the system applies 740 the selected weights to reduce an error count in a decoder that converts a sample of text from a first language to a second language.
- the technique described with reference to FIGS. 6 and 7 is applied to a translation lattice. In some implementations, the technique described with reference to FIGS. 6 and 7 is applied to the local gain function G(E, E′) for MBR. Applying the technique is particularly advantageous when user selection (e.g., manual selection) of parameters in G(E, E′) are not accurate for unobserved test data or language pairs. As an example, the parameters in G(E, E′) can be considered accurate when a Kendal tau rank correlation coefficient between a ranking using G(E, E′) and a ranking using an exact BLEU score is greater than or equal to 0.8. A BLEU score is an indicator of translation quality of text which has been machine translated.
- BLEU BLEU: a Method for Automatic Evaluation of Machine Translation , Technical Report RC22176 (WO109-022), IBM Research Division. Additional details of the linear approximation to the BLEU score are described in R. Tromble, S. Kumar, F. Och, and W. Macherey, 2008 , Lattice Minimum Bayes - Risk Decoding for Statistical Machine Translation , in EMNLP, Honolulu, Hi.
- the decoder selects a MAP translation as a backoff.
- an additional feature function g N+1 (E,F) equal to the original decoder cost for the sentence is used.
- a weight assignment of 1.0 for this feature function and zero for other feature functions indicates that a MAP translation is selected.
- N+2 feature functions are optimized using MERT to determine the highest BLEU score on a training set.
- FIG. 8 shows an example of a generic computer device 800 and a generic mobile computer device 850 , which may be used with the techniques (e.g., processes 400 , 500 , and 700 ) described.
- Computing device 800 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 850 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the systems and techniques described and/or claimed in this document.
- Computing device 800 includes a processor 802 , memory 804 , a storage device 806 , a high-speed interface 808 connecting to memory 804 and high-speed expansion ports 810 , and a low speed interface 812 connecting to low speed bus 814 and storage device 806 .
- Each of the components 802 , 804 , 806 , 808 , 810 , and 812 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 802 can process instructions for execution within the computing device 800 , including instructions stored in the memory 804 or on the storage device 806 to display graphical information for a GUI on an external input/output device, such as display 816 coupled to high speed interface 808 .
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 800 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 804 stores information within the computing device 800 .
- the memory 804 is a volatile memory unit or units.
- the memory 804 is a non-volatile memory unit or units.
- the memory 804 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 806 is capable of providing mass storage for the computing device 800 .
- the storage device 806 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 804 , the storage device 806 , or memory on processor 802 .
- the high speed controller 808 manages bandwidth-intensive operations for the computing device 800 , while the low speed controller 812 manages lower bandwidth-intensive operations.
- the high-speed controller 808 is coupled to memory 804 , display 816 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 810 , which may accept various expansion cards (not shown).
- low-speed controller 812 is coupled to storage device 806 and low-speed expansion port 814 .
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 800 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 820 , or multiple times in a group of such servers. It may also be implemented as part of a rack server system 824 . In addition, it may be implemented in a personal computer such as a laptop computer 822 . Alternatively, components from computing device 800 may be combined with other components in a mobile device (not shown), such as device 850 . Each of such devices may contain one or more of computing device 800 , 850 , and an entire system may be made up of multiple computing devices 800 , 850 communicating with each other.
- Computing device 850 includes a processor 852 , memory 864 , an input/output device such as a display 854 , a communication interface 866 , and a transceiver 868 , among other components.
- the device 850 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 850 , 852 , 864 , 854 , 868 , and 868 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 852 can execute instructions within the computing device 850 , including instructions stored in the memory 864 .
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 850 , such as control of user interfaces, applications run by device 850 , and wireless communication by device 850 .
- Processor 852 may communicate with a user through control interface 858 and display interface 856 coupled to a display 854 .
- the display 854 may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 856 may comprise appropriate circuitry for driving the display 854 to present graphical and other information to a user.
- the control interface 858 may receive commands from a user and convert them for submission to the processor 852 .
- an external interface 862 may be provided in communication with processor 852 , so as to enable near area communication of device 850 with other devices. External interface 862 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 864 stores information within the computing device 850 .
- the memory 864 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 874 may also be provided and connected to device 850 through expansion interface 872 , which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 874 may provide extra storage space for device 850 , or may also store applications or other information for device 850 .
- expansion memory 874 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 874 may be provide as a security module for device 850 , and may be programmed with instructions that permit secure use of device 850 .
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 864 , expansion memory 874 , memory on processor 852 , or a propagated signal that may be received, for example, over transceiver 868 or external interface 862 .
- Device 850 may communicate wirelessly through communication interface 866 , which may include digital signal processing circuitry where necessary. Communication interface 866 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 868 . In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 870 may provide additional navigation- and location-related wireless data to device 850 , which may be used as appropriate by applications running on device 850 .
- GPS Global Positioning System
- Device 850 may also communicate audibly using audio codec 860 , which may receive spoken information from a user and convert it to usable digital information. Audio codec 860 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 850 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 850 .
- Audio codec 860 may receive spoken information from a user and convert it to usable digital information. Audio codec 860 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 850 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 850 .
- the computing device 850 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 880 . It may also be implemented as part of a smartphone 882 , personal digital assistant, or other similar mobile device.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
which describes the English sentence, e, out of all possible sentences, that provides the highest value for P(e|f). Additionally, Bayes Rule provides that:
Using Bayes Rule, this most likely sentence can be re-written as:
where P(w|Ψ) is the posterior probability of the n-gram w in the translation hypergraph; E is a candidate translation; F is the sample text in the first language, e is an edge; and f(e,w,E)=1 when wee, P(e|Ψ)>P(e′|Ψ), and e′ precedes e on E; otherwise, f(e,w,E)=0.
where P(w|Ψ) is the posterior probability of the n-gram w in the translation lattice; E is a candidate translation; F is the sample text in the first language, e is an edge; and f(e,w,E)=1 when wee, P(e|Ψ)>P(e′|Ψ), and e′ precedes e on E; otherwise, f(e,w,E)=0.
where P(w|Ψ) is the posterior probability of the n-gram w in the translation hypergraph; E is a candidate translation; F is the sample text in the first language, e is an edge; and f(e,w,E)=1 when wεe, P(e|Ψ)>P(e′|Ψ), and e′ precedes e on E; otherwise, f(e,w,E)=0.
where a αε(0, ∞) is a scaling factor that flattens the distribution when α<1, and sharpens the distribution when α>1.
where R(E) represents the Bayes risk of candidate translation E′ under the loss function L, and Ψ represents the space of translations. For N-best MBR, the space Ψ is an N-best list produced, for example, by the
where Ψh represents the hypothesis space and Ψe represents an evidence space used for computing Bayes risk.
g w(E|E′)=θw·#w(E′)·δ(E)
where θw is a constant, #w (E′) is a number of times that w occurs in E′, and δw(E) is 1 if wεE and 0 otherwise. Assuming that the overall gain function G(E, E′) can be written as a sum of local gain functions and a constant θ0 times the length of the hypothesis E′, the overall gain function can be expressed as:
can be rewritten such that the MBR decoder for the lattice (in Equation 1) is expressed as:
where P(w|Ψ) is the posterior probability of the n-gram w in the lattice, and can be expressed as:
where f(e,w,E) is a score assigned to edge e on path E containing n-gram w. Furthermore, f(e,w,E)=1 when wεe, P(e|Ψ)>P(e′|Ψ), and e′ precedes e on E; otherwise, f(e,w,E)=0. In other words, for each path E, the edge that contributes n-gram w and has the highest edge posterior probability relative to its predecessors on the path E is counted.
where P(e|Ψ) is the posterior probability of a lattice edge.
where
Claims (4)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/533,519 US8285536B1 (en) | 2009-07-31 | 2009-07-31 | Optimizing parameters for machine translation |
US13/528,426 US8401836B1 (en) | 2009-07-31 | 2012-06-20 | Optimizing parameters for machine translation |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/533,519 US8285536B1 (en) | 2009-07-31 | 2009-07-31 | Optimizing parameters for machine translation |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/528,426 Continuation US8401836B1 (en) | 2009-07-31 | 2012-06-20 | Optimizing parameters for machine translation |
Publications (1)
Publication Number | Publication Date |
---|---|
US8285536B1 true US8285536B1 (en) | 2012-10-09 |
Family
ID=46964299
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/533,519 Expired - Fee Related US8285536B1 (en) | 2009-07-31 | 2009-07-31 | Optimizing parameters for machine translation |
US13/528,426 Active US8401836B1 (en) | 2009-07-31 | 2012-06-20 | Optimizing parameters for machine translation |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/528,426 Active US8401836B1 (en) | 2009-07-31 | 2012-06-20 | Optimizing parameters for machine translation |
Country Status (1)
Country | Link |
---|---|
US (2) | US8285536B1 (en) |
Cited By (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8489385B2 (en) * | 2007-11-21 | 2013-07-16 | University Of Washington | Use of lexical translations for facilitating searches |
CN103646019A (en) * | 2013-12-31 | 2014-03-19 | 哈尔滨理工大学 | Method and device for fusing multiple machine translation systems |
WO2014114140A1 (en) * | 2013-01-25 | 2014-07-31 | 哈尔滨工业大学 | Parameter adjustment method used for statistical machine translation |
US20150149151A1 (en) * | 2013-11-26 | 2015-05-28 | Xerox Corporation | Procedure for building a max-arpa table in order to compute optimistic back-offs in a language model |
US20150248457A1 (en) * | 2014-02-28 | 2015-09-03 | Ebay Inc. | Automatic machine translation using user feedback |
US9158762B2 (en) | 2012-02-16 | 2015-10-13 | Flying Lizard Languages, Llc | Deconstruction and construction of words of a polysynthetic language for translation purposes |
US9798720B2 (en) | 2008-10-24 | 2017-10-24 | Ebay Inc. | Hybrid machine translation |
US9805031B2 (en) | 2014-02-28 | 2017-10-31 | Ebay Inc. | Automatic extraction of multilingual dictionary items from non-parallel, multilingual, semi-structured data |
US10009358B1 (en) * | 2014-02-11 | 2018-06-26 | DataVisor Inc. | Graph based framework for detecting malicious or compromised accounts |
US10303775B2 (en) * | 2010-08-23 | 2019-05-28 | Eleven Street Co., Ltd. | Statistical machine translation method using dependency forest |
US11232364B2 (en) | 2017-04-03 | 2022-01-25 | DataVisor, Inc. | Automated rule recommendation engine |
US11341340B2 (en) * | 2019-10-01 | 2022-05-24 | Google Llc | Neural machine translation adaptation |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8725496B2 (en) * | 2011-07-26 | 2014-05-13 | International Business Machines Corporation | Customization of a natural language processing engine |
US8874428B2 (en) * | 2012-03-05 | 2014-10-28 | International Business Machines Corporation | Method and apparatus for fast translation memory search |
Citations (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5748850A (en) * | 1994-06-08 | 1998-05-05 | Hitachi, Ltd. | Knowledge base system and recognition system |
US20030110023A1 (en) * | 2001-12-07 | 2003-06-12 | Srinivas Bangalore | Systems and methods for translating languages |
US20070150257A1 (en) * | 2005-12-22 | 2007-06-28 | Xerox Corporation | Machine translation using non-contiguous fragments of text |
US20070239432A1 (en) * | 2006-03-30 | 2007-10-11 | Microsoft Corporation | Common word graph based multimodal input |
US20070282590A1 (en) | 2006-06-02 | 2007-12-06 | Microsoft Corporation | Grammatical element generation in machine translation |
US20090248416A1 (en) * | 2003-05-29 | 2009-10-01 | At&T Corp. | System and method of spoken language understanding using word confusion networks |
US20100004920A1 (en) | 2008-07-03 | 2010-01-07 | Google Inc. | Optimizing parameters for machine translation |
US7856351B2 (en) * | 2007-01-19 | 2010-12-21 | Microsoft Corporation | Integrated speech recognition and semantic classification |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6751621B1 (en) * | 2000-01-27 | 2004-06-15 | Manning & Napier Information Services, Llc. | Construction of trainable semantic vectors and clustering, classification, and searching using trainable semantic vectors |
US7243064B2 (en) * | 2002-11-14 | 2007-07-10 | Verizon Business Global Llc | Signal processing of multi-channel data |
-
2009
- 2009-07-31 US US12/533,519 patent/US8285536B1/en not_active Expired - Fee Related
-
2012
- 2012-06-20 US US13/528,426 patent/US8401836B1/en active Active
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5748850A (en) * | 1994-06-08 | 1998-05-05 | Hitachi, Ltd. | Knowledge base system and recognition system |
US20030110023A1 (en) * | 2001-12-07 | 2003-06-12 | Srinivas Bangalore | Systems and methods for translating languages |
US20090248416A1 (en) * | 2003-05-29 | 2009-10-01 | At&T Corp. | System and method of spoken language understanding using word confusion networks |
US7957971B2 (en) * | 2003-05-29 | 2011-06-07 | At&T Intellectual Property Ii, L.P. | System and method of spoken language understanding using word confusion networks |
US20070150257A1 (en) * | 2005-12-22 | 2007-06-28 | Xerox Corporation | Machine translation using non-contiguous fragments of text |
US20070239432A1 (en) * | 2006-03-30 | 2007-10-11 | Microsoft Corporation | Common word graph based multimodal input |
US20070282590A1 (en) | 2006-06-02 | 2007-12-06 | Microsoft Corporation | Grammatical element generation in machine translation |
US7856351B2 (en) * | 2007-01-19 | 2010-12-21 | Microsoft Corporation | Integrated speech recognition and semantic classification |
US20100004920A1 (en) | 2008-07-03 | 2010-01-07 | Google Inc. | Optimizing parameters for machine translation |
US20100004919A1 (en) | 2008-07-03 | 2010-01-07 | Google Inc. | Optimizing parameters for machine translation |
Non-Patent Citations (6)
Title |
---|
Dean, J., et al., MapReduce: Simplified Data Processing on Large Clusters, Proceedings of the 6th Symposium on Operating Systems Design and Implementation, Dec. 6, 2004, 13 pages. |
International Search Report and Written Opinion for PCT Application No. PCT/US2009/049613, mailed Sep. 16, 2010, 10 pages. |
Macherey, W., et al., "Lattice-based Minimum Error Rate Training for Statistical Machine Translation", In: Proceedings of the 2008 Conference on Empirical Methods in natural Language Processing, pp. 725-734. |
Papineni, K., et al., "Bleu: a Method for Automatic Evaluation of Machine Translation," IBM Research Report, RC22176 (W0109-022), IBM Research Division, Yorktown Heights, NY, Sep. 17, 2001. |
Tromble, R., et al., "Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation", In: Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing, Oct. 25-27, 2008, 10 pages. |
Zhang, Hao et al., "Efficient Multi-pass Decoding for Synchronous Context Free Grammar," Proceedings of ACL-08:HLT, pp. 206-217. 2008. * |
Cited By (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8489385B2 (en) * | 2007-11-21 | 2013-07-16 | University Of Washington | Use of lexical translations for facilitating searches |
US9798720B2 (en) | 2008-10-24 | 2017-10-24 | Ebay Inc. | Hybrid machine translation |
US10303775B2 (en) * | 2010-08-23 | 2019-05-28 | Eleven Street Co., Ltd. | Statistical machine translation method using dependency forest |
US9158762B2 (en) | 2012-02-16 | 2015-10-13 | Flying Lizard Languages, Llc | Deconstruction and construction of words of a polysynthetic language for translation purposes |
WO2014114140A1 (en) * | 2013-01-25 | 2014-07-31 | 哈尔滨工业大学 | Parameter adjustment method used for statistical machine translation |
US9400783B2 (en) * | 2013-11-26 | 2016-07-26 | Xerox Corporation | Procedure for building a max-ARPA table in order to compute optimistic back-offs in a language model |
US20150149151A1 (en) * | 2013-11-26 | 2015-05-28 | Xerox Corporation | Procedure for building a max-arpa table in order to compute optimistic back-offs in a language model |
CN103646019A (en) * | 2013-12-31 | 2014-03-19 | 哈尔滨理工大学 | Method and device for fusing multiple machine translation systems |
US10009358B1 (en) * | 2014-02-11 | 2018-06-26 | DataVisor Inc. | Graph based framework for detecting malicious or compromised accounts |
US20150248457A1 (en) * | 2014-02-28 | 2015-09-03 | Ebay Inc. | Automatic machine translation using user feedback |
US9569526B2 (en) * | 2014-02-28 | 2017-02-14 | Ebay Inc. | Automatic machine translation using user feedback |
US9805031B2 (en) | 2014-02-28 | 2017-10-31 | Ebay Inc. | Automatic extraction of multilingual dictionary items from non-parallel, multilingual, semi-structured data |
US11232364B2 (en) | 2017-04-03 | 2022-01-25 | DataVisor, Inc. | Automated rule recommendation engine |
US11341340B2 (en) * | 2019-10-01 | 2022-05-24 | Google Llc | Neural machine translation adaptation |
Also Published As
Publication number | Publication date |
---|---|
US8401836B1 (en) | 2013-03-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8285536B1 (en) | Optimizing parameters for machine translation | |
US8744834B2 (en) | Optimizing parameters for machine translation | |
JP5901001B1 (en) | Method and device for acoustic language model training | |
US9412365B2 (en) | Enhanced maximum entropy models | |
US10372821B2 (en) | Identification of reading order text segments with a probabilistic language model | |
US8140332B2 (en) | Technique for searching out new words that should be registered in dictionary for speech processing | |
US7219051B2 (en) | Method and apparatus for improving statistical word alignment models | |
US11593571B2 (en) | Machine translation method, device, and computer-readable storage medium | |
US7983898B2 (en) | Generating a phrase translation model by iteratively estimating phrase translation probabilities | |
US8849665B2 (en) | System and method of providing machine translation from a source language to a target language | |
US20090112573A1 (en) | Word-dependent transition models in HMM based word alignment for statistical machine translation | |
US20040044530A1 (en) | Method and apparatus for aligning bilingual corpora | |
US20110282643A1 (en) | Statistical machine translation employing efficient parameter training | |
US11276394B2 (en) | Method for re-aligning corpus and improving the consistency | |
CN101271450B (en) | Method and device for cutting language model | |
WO2014036827A1 (en) | Text correcting method and user equipment | |
JPH10326275A (en) | Method and device for morpheme analysis and method and device for japanese morpheme analysis | |
JP5565827B2 (en) | A sentence separator training device for language independent word segmentation for statistical machine translation, a computer program therefor and a computer readable medium. | |
JP7096199B2 (en) | Information processing equipment, information processing methods, and programs | |
US20130110491A1 (en) | Discriminative learning of feature functions of generative type in speech translation | |
JP5500636B2 (en) | Phrase table generator and computer program therefor | |
CN115796167A (en) | Machine reading understanding method and device and computer readable storage medium | |
Silfverberg et al. | Predictive text entry for agglutinative languages using unsupervised morphological segmentation | |
Cooley | Increasing the IQ of Your Smartphone: Reducing Email Keystrokes Using Personalized Word Prediction |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KUMAR, SHANKAR;MACHEREY, WOLFGANG;DYER, CHRISTOPHER JAMES;AND OTHERS;SIGNING DATES FROM 20090822 TO 20091002;REEL/FRAME:023383/0114 |
|
REMI | Maintenance fee reminder mailed | ||
LAPS | Lapse for failure to pay maintenance fees | ||
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20161009 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044142/0357Effective date: 20170929 |