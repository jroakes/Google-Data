US9922645B2 - Recognizing speech in the presence of additional audio - Google Patents
Recognizing speech in the presence of additional audio Download PDFInfo
- Publication number
- US9922645B2 US9922645B2 US15/460,342 US201715460342A US9922645B2 US 9922645 B2 US9922645 B2 US 9922645B2 US 201715460342 A US201715460342 A US 201715460342A US 9922645 B2 US9922645 B2 US 9922645B2
- Authority
- US
- United States
- Prior art keywords
- speech
- model
- output
- audio
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/20—Speech recognition techniques specially adapted for robustness in adverse environments, e.g. in noise, of stress induced speech
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/165—Management of the audio stream, e.g. setting of volume, audio stream path
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L15/222—Barge in, i.e. overridable guidance for interrupting prompts
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
- G10L17/06—Decision making techniques; Pattern matching strategies
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0316—Speech enhancement, e.g. noise reduction or echo cancellation by changing the amplitude
- G10L21/0324—Details of processing therefor
- G10L21/034—Automatic adjustment
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L25/84—Detection of presence or absence of voice signals for discriminating voice from noise
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03G—CONTROL OF AMPLIFICATION
- H03G3/00—Gain control in amplifiers or frequency changers without distortion of the input signal
- H03G3/20—Automatic control
- H03G3/30—Automatic control in amplifiers having semiconductor devices
- H03G3/3005—Automatic control in amplifiers having semiconductor devices in amplifiers suitable for low-frequencies, e.g. audio amplifiers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G10L15/265—
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
Definitions
- This disclosure generally relates to speech recognition.
- Automatic speech recognition can be used in mobile devices and other devices. In general, automatic speech recognition attempts to provide accurate transcriptions of what a person has said.
- this document describes a computer-implemented method that includes receiving, at a processing system, a first signal including an output of a speaker device and an additional audio signal. The method also includes determining, by the processing system, based at least in part on a model trained to identify the output of the speaker device, that the additional audio signal corresponds to an utterance of a user. The method further includes initiating a reduction in an audio output level of the speaker device based on determining that the additional audio signal corresponds to the utterance of the user.
- the document describes a system that includes one or more computers and one or more storage devices storing instructions.
- the instructions are operable, when executed by the one or more computers, to cause the one or more computers to perform various operations.
- the operations include receiving a first signal comprising an output of a speaker device and an additional audio signal.
- the operations also include accessing a model trained to identify the output of the speaker device, and determining, based, at least in part on the model, that the additional audio signal corresponds to an utterance of a user.
- the operations further include initiating a reduction in an audio output level of the speaker device based on determining that the additional audio signal corresponds to the utterance of the user.
- the document describes one or more machine-readable storage devices storing instructions that, upon execution by one or more processing devices, cause the one or more processing devices to perform various operations.
- the operations include receiving a first signal comprising an output of a speaker device and an additional audio signal, and determining, based at least in part on a model trained to identify the output of the speaker device, that the additional audio signal corresponds to an utterance of a user.
- the operations also include initiating a reduction in an audio output level of the speaker device based on determining that the additional audio signal corresponds to the utterance of the user.
- Implementations of the above aspects may include one or more of the following.
- the dialog manager can receive a second signal in which an effect of the output of the speaker device is less than that in the first signal, and the second signal can be provided to a speech recognition engine.
- the output of the speaker device can be based on an output of a text-to-speech system.
- a transcription corresponding to the utterance of the user can be generated by a transcription engine.
- a first vector corresponding to the first signal can be received, for example, by the dialog manager, and the first vector can be compared to a second vector corresponding to the model.
- a presence of the additional audio signal can be determined if a result of the comparison satisfies a threshold condition.
- a presence of the additional audio signal can also be determined if the first vector satisfies a threshold condition.
- the model can be an i-vector based model.
- the model can be a neural network based model.
- the model can jointly represent a user voice and the output of the speaker device.
- a more natural human-like spoken interaction can be facilitated between a user and a computing device.
- Accurate speech recognition can be performed in the event of a “barge-in” by the user during a time when audio from a text-to-speech system, media player, or navigation system is being played back.
- FIG. 1 is a diagram of example signals and utterance during a voice-based exchange between a user and a computing device.
- FIG. 2 is a diagram of an example system that identifies speech in the event of a barge-in by the user.
- FIG. 3 is a diagram of an example process for identifying speech in the event of a barge-in by the user.
- FIG. 4 is a block diagram of an example of a computing device
- Voice-based interactions with computing devices allow for hands-free input, for example, for searching the web, launching an application, storing an appointment, asking a question, getting directions, playing media, or performing another action on the computing device.
- the computing device In response to a voice-based query from a user, the computing device often plays back synthesized audio via a text-to-speech (TTS) system.
- TTS text-to-speech
- the TTS output played back in response to a query is long, and the user may get the desired information by listening to only a portion of the TTS output. In other cases, the user may decide that the played back TTS output is not providing the desired information, and that a separate spoken query is needed.
- the user may need to “barge in” with another query or other spoken input during a time when the TTS output is still being played back by the computing device. For example, a user can barge in by speaking a predetermined word or phrase (e.g., “stop” or “new query”), or simply ask another question. If the microphone of the computing device is switched on to accept such interruptions by the user, the microphone also captures the audio output from the TTS system. Unless the TTS output is suppressed from the audio captured by the microphone, the TTS output is fed back to the speech recognizer and can affect the accuracy of the speech recognizer in recognizing the actual user input.
- a predetermined word or phrase e.g., “stop” or “new query”
- the audio that needs to be suppressed may come from sources other than a TTS system.
- a user may provide a spoken input in the presence of audio output from a media player or navigation system, and a suppression of such audio output may be required for accurately recognizing the spoken input from the user. While this document uses a TTS output as an example of audio that is suppressed for accurate speech recognition, the technology can be used for suppressing other types of audio (e.g., outputs of a media player or navigation system) without deviating from the scope of this disclosure.
- the TTS output can be suppressed from the captured audio in multiple ways.
- adaptive filtering e.g., echo cancellation techniques
- adaptive filtering is often not adequate to suppress the TTS output, and may also distort the spoken input from the user, thereby affecting recognition accuracy.
- the known signal that is subtracted using echo cancellation techniques may be misrepresented in the captured audio due to latency in the audio path, distortion added by a speaker, or echoes and distortions produced by the physical channel.
- This document describes techniques for detecting a presence of spoken input in the captured audio using, for example, a model for the synthesized voice of the TTS output.
- a deviation from the model can be attributed to the presence of additional audio such as an utterance, and accordingly, the audio output from the TTS system can be reduced to accurately capture the additional audio.
- a model jointly representing the synthesized voice and a user utterance can also be used to detect “barge-in” situations.
- FIG. 1 is a diagram of example signals and utterance 100 during a voice-based exchange between a user 102 and a computing device 104 such as a smartphone.
- the user 102 provides a voice-based input to the computing device 104 by uttering the question 106 : “Where's the nearest gas station?”
- the computing device 104 provides an audible response 108 as an output 110 from a text-to-speech (TTS) system associated with the computing device 104 .
- TTS text-to-speech
- the response 108 to the question 106 is: “There's one two miles away.
- the user 102 may not need to listen to the entire response 108 before providing a follow up instruction 112 at a time when the TTS output 110 is still being played back.
- the user 102 starts providing the instruction 112 : “Get directions,” such that the instruction 112 overlaps with the response 108 .
- a microphone of the computing device 104 captures an audio signal that includes both the TTS output 110 , as well as the instruction 112 provided by the user. This is represented as the first signal 114 in FIG. 1 .
- the event of a user providing voice-based input e.g., the instruction 112
- a time when a TTS output 110 is being played may sometimes be referred to as a “barge-in.”
- This document describes techniques for suppressing the TTS output 110 from the first signal 114 , such that a second signal 120 representing the utterance corresponding to the instruction 112 can be recognized.
- an additional audio signal e.g., the spoken instruction 112
- an audio level corresponding to the TTS output 110 can be reduced or even completely turned down to facilitate recognition of the instruction 112 .
- the signal 116 represents such suppression of the TTS output.
- FIG. 2 is a diagram of an example system 200 that identifies speech in the event of a barge-in by the user 102 .
- the system 200 includes a speaker device 202 that provides an audio output, for example, from a TTS system that generates the audio from text-based transcription data.
- the speaker device 202 can be a part of a larger system (e.g., a TTS system, a media player, or a navigational system) that provides audio outputs.
- another system such as one that renders symbolic linguistic representations (e.g., phonetic transcriptions) to speech may also be used either in conjunction with or in the place of the TTS system.
- both audio output from the speaker device 202 and utterance by the user 102 are captured by the microphone 206 , and the system 200 has to identify the utterance from the captured audio.
- the system 200 includes a dialog manager 208 that receives the signal (also referred to as the first signal) 114 captured by the microphone 206 .
- the dialog manager 208 can be configured to detect “barge-in” situations, i.e., situations in which a user provides a spoken input during which an audio signal is being output from the speaker device 202 .
- the dialog manager 208 can be configured to deactivate the speaker device 202 , or at least initiate a reduction in the level of audio output coming out of the speaker device 202 , upon detecting an additional audio signal within the first signal 114 .
- the dialog manager 208 detects the presence of the additional audio within the first signal 114 by accessing a model 210 of the audio output coming out of the speaker device 202 .
- the model 210 can be trained to represent a voice-model of the synthesized voice.
- the voice-model representing the synthesized voice can also be referred to as a “fingerprint” of the synthesized voice.
- the model 210 can be trained, for example, using samples of speech previously generated by the TTS system.
- the training of the model 210 can be performed off-line using a large number of samples.
- the model 210 can be improved by supplementing the off-line training with on-line updates.
- the model 210 can use a fingerprint of a user voice to accurately detect the user's speech in the presence of various types of other audio. For example, if the model 210 is trained to represent the voice of a particular user, the particular user's voice can be detected in the presence of, for example, audio coming out of a media player. In such cases, the user's voice can be detected, for example, by determining that a confidence in the model is above a threshold condition.
- the model 210 to train the model 210 to represent a voice coming from a media device, it can be assumed that the audio corresponding to a predetermined time (e.g., the last T seconds) comes from the same speaker. In such cases, the audio can be used to train the model 210 to represent the speaker. The operation can be repeated over and over while any voice is detected.
- a predetermined time e.g., the last T seconds
- the model 210 can represent the synthesized speech (or other audio emanating from the speaker device 202 ) using a set of vectors.
- the model 210 can be represented using a set of “i-vectors” or vectors provided by a neural network classifier.
- the dialog manager 208 can be configured to detect an additional audio in the first signal by comparing one or more vectors generated from the first signal 114 with reference vectors representing the model 210 .
- the dialog manager can compare an i-vector generated from the first signal to a reference i-vector representing the model 210 .
- the dialog manager can conclude that no additional audio is present in the first signal and therefore take no action.
- the dialog manager can conclude that an additional audio is present in the first signal 114 .
- the dialog manager can initiate a reduction in the audio output level of the speaker device 202 by sending a control signal 211 to the speaker device 202 .
- control signal 211 may cause the speaker device 202 to switch off, such that only the additional audio is captured by the microphone 206 .
- control signal 211 can cause a reduction in the audio output level from the speaker device 202 .
- the signal captured by the microphone 206 (referred to in FIG. 1 as the second signal 120 ) is passed on by the dialog manager 208 to a speech recognition engine 212 .
- a Factor Analysis (FA) paradigm can be used to account for variability (such as background noise or different communication channels) in the training data.
- An FA paradigm aims to express the main “factors” contributing to the observed variability.
- techniques such as Join Factor Analysis (JFA) or Total Variability Model (TVM) can also be used.
- JFA Join Factor Analysis
- TVM Total Variability Model
- the acoustic space can be divided into different subspaces that can are used to independently model factors associated with variability (e.g., inter-speaker variability or environmental variability across different sessions of data collection) in the training data.
- TVM the sources of variability (both speaker and session) are modeled together in a single space of multiple dimensions.
- a synthesized voice (or a voice of a human speaker, if a user voice-model is included) can be represented as a vector of a given dimension.
- the vector of latent factors for a given utterance (coming from the speaker device 202 , or the user 102 ) is referred to as an i-vector.
- i-vectors are considered sufficient to represent the differences between various utterances.
- Variability and speaker information can be separated in the i-vector domain using techniques such as Linear Discriminant Analysis (LDA) or Within Class Covariance Normalization (WCCN).
- LDA Linear Discriminant Analysis
- WCCN Within Class Covariance Normalization
- Cosine distances can be used to compare two i-vectors (e.g., a reference i-vector from the model 210 , and a second i-vector computed from the first signal 114 ).
- Probabilistic Linear Discriminant Analysis may also be used in the i-vector space under a probabilistic framework.
- PLDA Probabilistic Linear Discriminant Analysis
- the utterance is first represented in terms of a large Gaussian mixture model (GMM) (e.g., a Universal Background Model (UBM)), which can be parameterized by ⁇ .
- GMM Gaussian mixture model
- UBM Universal Background Model
- the accumulated and centered first-order Baum-Welch statistics can then be computed as:
- o, ⁇ ) is the Gaussian occupation probability for mixture m and observation o.
- the i-vector extraction procedure described above depends on utterance data and the TV model parameters ⁇ , T, and ⁇ . Other methods of determining i-vectors are also within the scope of this disclosure.
- the model 210 alternatively or in addition includes a voice-model of the user 102 .
- a voice-model can be referred to as a fingerprint of the user's voice.
- Such a voice-model of the user voice can be generated, for example, by collecting samples of the user's voice.
- the collected samples are used to train the voice-model of the user locally, for example, on a mobile device housing portions of the system 200 .
- the collected voice samples can be provided to a remote computing device (e.g., a server) that updates the voice-model corresponding to the user.
- the dialog manager can be configured to take various actions upon identifying a source of the audio in the first signal 114 . For example, if the dialog manager 208 determines that only the user 102 is speaking, the dialog manager can determine that the user 102 should not be interrupted, and accordingly delay or cancel any output about to be output from the speaker device 202 . If a combination of the user voice and the synthesized voice is detected, the dialog manager 208 may initiate a reduction of the audio output level coming out of the speaker device 202 .
- a combination of the user voice and the synthesized voice can be detected, for example, by combining scores or vectors from the individual voice-models corresponding to the synthesized voice and user voice, respectively. If only the presence of the synthesized voice is detected (possibly together with noise such as background music, or environmental noise), the dialog manager 208 may decide to take no action, and allow an output from the speaker device 202 to continue.
- the model 210 includes a voice-model for a particular user 102
- the voice-model may be used to differentiate one user from others. In such cases, speech from other users may be classified as noise, and treated accordingly by the dialog manager 208 . This way, interruption of the speaker device can be personalized for a particular user, and made secure.
- the dialog manager Upon receiving a second signal 120 in which the effect of the speaker device output is reduced, the dialog manager forwards the signal to a speech recognizing system 212 . While FIG. 2 depicts the dialog manager 208 separately from the speech recognition engine 212 , some implementations can include the dialog manager 208 as a part of the speech recognition engine 212 .
- the speech recognition engine 212 provides an output 218 obtained by recognizing the utterance of the user in the second signal 120 .
- the output 218 is provided to a language model 214 which outputs information representing words and/or phrases.
- the output 220 of the language model is then provided to a transcription engine 216 that converts the recognized words and/or phrases to text.
- the output of the transcription engine can then be provided to a system configured to handle user queries.
- the output of the transcription engine can be provided to a semantic parser that transforms text into a structured representation suitable for processing by a particular application.
- the signal (e.g., the first signal 114 , or the second signal 120 ) captured by the microphone 206 may be sampled at a particular frequency and resolution.
- the signals may be sampled at 8 kHz, 16 kHz, 44.1 kHz, or any other sample rate, and the resolution may be 16 bits, 32 bits, or any other resolution.
- the speech recognition engine 212 can be configured to extract acoustic features from a received signal and classify the extracted acoustic features separately using one or more acoustic models 213 .
- the acoustic features extracted from the signals can be represented using, for example, mel-frequency cepstral coefficients, cepstral coefficients, spectral vectors, spectrograms, filterbank energies, fast Fourier transform (FFT) frames or other time-frequency or frequency domain representations.
- FFT fast Fourier transform
- the extracted acoustic features may be represented as one or more feature vectors.
- the acoustic features can be extracted using a processor either included within, or external to the system 200 .
- the processor may generate the acoustic features based on one or more audio frames corresponding to signal received at the speech recognition engine 212 .
- the audio frames may be, for example, between ten and twenty-five milliseconds in length.
- an acoustic model 213 can be configured to establish statistical representations for the features or feature vectors extracted from the signal received at the speech recognition engine 212 .
- Various types of acoustic models 213 can be used by the speech recognition engine 212 .
- the acoustic model 213 can be a classifier such as a neural network.
- the acoustic model 213 can include a Gaussian mixture model (GMM), a Hidden Markov Model (HMM), a segmental model, a super-segmental model, a hidden dynamic model, a maximum entropy model, or a conditional random field.
- GMM Gaussian mixture model
- HMM Hidden Markov Model
- the output 218 from the speech recognition engine 212 can be a vector of ordered pairs of speech units (e.g., phonemes or tri-phones) and corresponding weights.
- the output of the speech recognition engine 212 can be a vector that includes a weight (also referred to as a score or probability) for each known phoneme in a given language.
- the output 218 can be provided to a language model 214 that determines a likelihood of word and phrase sequences from the speech units (e.g., phonemes) in the output 218 .
- the language model 214 can be used, for example, to capture properties of the language (e.g. English) being recognized, and predict the next word in a sequence.
- the language model 214 may be used to constrain a search among alternative word or phrase sequence hypotheses during recognition.
- the language model 214 can be used to determine whether the speaker likely said “let's recognize speech,” or “let's wreck a nice beach,” from similar sounding sequence of phonemes.
- the language model 214 can be, for example, a unigram model, a n-gram model, or another model such as a positional language model, factored language model, or cache language model. Even though FIG. 2 shows the language model 214 to be external to the speech recognition engine 212 , the language model 214 can also be implemented as a part of the speech recognition engine 212 .
- the language model can be implemented as a separate hardware module, or using a processor that executes appropriate machine readable instructions to perform the above-mentioned functionalities.
- the output 215 of the language model 214 can be provided to a transcription engine 216 that generates text-based strings based on the output 215 .
- the text strings generated as an output 218 of the transcription engine 216 can be provided to a system that can handle user queries.
- the output 220 of the transcription engine can be provided to a semantic parser that transforms text into a structured representation suitable for processing by a particular application that processes user queries.
- FIG. 3 is a diagram of an example process 300 for identifying speech in the event of a barge-in by the user.
- the process 300 may be performed, for example, by one or more processors used within the system 200 of FIG. 2 . In some implementations, at least a part of the process 300 can be performed by a processor included within a dialog manager 208 of the system 200 .
- the process 300 includes receiving a first signal that includes an output of a speaker device as well as an additional audio signal ( 310 ). In some implementations, the output of the speaker device can be based on an output of a TTS system. In some implementations, the output of the speaker device can be based on an output signal received from a media player or a navigation system.
- the process also includes determining that the additional audio signal corresponds to an utterance of a user ( 320 ).
- the determination can be done, for example, based at least in part on a model trained to identify the output of the speaker device.
- the model can be substantially same as the model 210 described with reference to FIG. 2 .
- the model can be an i-vector based model or a neural network based model.
- the model can represent only a user voice, or jointly represent a user voice and the output of the speaker device.
- the determination can be made, for example, based on a first model trained to identify the output of the speaker device, and a second model trained to identify an utterance of a user.
- the determination can include obtaining a vector (e.g., an i-vector) corresponding to the first signal.
- the obtained vector can then be compared to a reference vector corresponding to the model, and the presence of the additional audio signal can be determined if a result of the comparison satisfies a threshold condition. For example, if a cosine distance between the obtained vector and the reference vector is less than a threshold value, the two vectors can be determined to be substantially similar to one another. Conversely, in another example, if the cosine distance between the two vectors exceed a threshold value, the two vectors can be determined to be sufficiently different, and this can lead to a conclusion that an additional audio signal (in addition to the output of the speaker device) is present in the first signal.
- determination of a presence of the additional signal can be made if the vector obtained from the first signal satisfies a threshold condition.
- the process 300 further includes initiating a reduction in the audio output level of the speaker device ( 330 ) based on determining that the additional audio signal corresponds to the utterance of the user.
- the reduction can be initiated, for example, by providing a control signal to the speaker device.
- the control signal may cause a switch-off of the speaker device, or cause a reduction in the audio output level (that can also be referred to as the volume) of the speaker device.
- a switch-off or level reduction of the audio output of the speaker device results in a captured signal in which an effect of the speaker device output is reduced. For example, if the audio output level of the speaker device is reduced upon detection of a user voice, a dominant part of the signal captured by the microphone after such a reduction may include the spoken input from the user, thereby allowing for more accurate recognition of the spoken input.
- the captured signal is provided to a speech recognition engine, and an output of the speech recognition engine is processed using a transcription engine to generate a transcription of the spoken input or utterance.
- the output of the speech recognition engine can be provided to a language model (e.g., the language model 214 of FIG. 2 ), and the output of the language model can be provided to a transcription engine (e.g., the transcription engine 216 of FIG. 2 ) to generate the transcription.
- FIG. 4 is block diagram of an example computer system 400 that may be used in performing the processes described herein.
- the dialog manager 208 can include at least portions of the computing device 400 described below.
- Computing device 400 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 400 is further intended to represent various typically non-mobile devices, such as televisions or other electronic devices with one or more processers embedded therein or attached thereto.
- Computing device 400 also represents mobile devices, such as personal digital assistants, touchscreen tablet devices, e-readers, cellular telephones, smartphones.
- the system 400 includes a processor 410 , a memory 420 , a storage device 430 , and an input/output module 440 .
- Each of the components 410 , 420 , 430 , and 440 can be interconnected, for example, using a system bus 450 .
- the processor 410 is capable of processing instructions for execution within the system 400 .
- the processor 410 is a single-threaded processor.
- the processor 410 is a multi-threaded processor.
- the processor 410 is capable of processing instructions stored in the memory 420 or on the storage device 430 .
- the memory 420 stores information within the system 400 .
- the memory 420 is a computer-readable medium.
- the memory 420 is a volatile memory unit.
- the memory 420 is a non-volatile memory unit.
- the storage device 430 is capable of providing mass storage for the system 400 .
- the storage device 430 is a computer-readable medium.
- the storage device 430 can include, for example, a hard disk device, an optical disk device, or some other large capacity storage device.
- the input/output module 440 provides input/output operations for the system 400 .
- the input/output module 440 can include one or more of a network interface devices, e.g., an Ethernet card, a serial communication device, e.g., an RS-232 port, and/or a wireless interface device, e.g., and 802.11 card.
- the input/output device can include driver devices configured to receive input data and send output data to other input/output devices, e.g., keyboard, printer and display devices 460 .
- the web server, advertisement server, and impression allocation module can be realized by instructions that upon execution cause one or more processing devices to carry out the processes and functions described above.
- Such instructions can comprise, for example, interpreted instructions, such as script instructions, e.g., JavaScript or ECMAScript instructions, or executable code, or other instructions stored in a computer readable medium.
- the web server and advertisement server can be distributively implemented over a network, such as a server farm, or can be implemented in a single computer device.
- Example computer system 400 can include a server.
- Various servers which may act in concert to perform the processes described herein, may be at different geographic locations, as shown in the figure.
- the processes described herein may be implemented on such a server or on multiple such servers.
- the servers may be provided at a single location or located at various places throughout the globe.
- the servers may coordinate their operation in order to provide the capabilities to implement the processes.
- implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Implementations of the subject matter described in this specification can be implemented as one or more computer program products, e.g., one or more modules of computer program instructions encoded on a tangible program carrier, for example a non-transitory computer-readable medium, for execution by, or to control the operation of, a processing system.
- the non-transitory computer readable medium can be a machine readable storage device, a machine readable storage substrate, a memory device, or a combination of one or more of them.
- various implementations of the systems and techniques described herein can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which can be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be a form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in a form, including acoustic, speech, or tactile input.
- feedback provided to the user can be a form of sensory feedback (e.g., visual feedback, auditory feedback,
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or a combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by a form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- Content such as ads and GUIs, generated according to the processes described herein may be displayed on a computer peripheral (e.g., a monitor) associated with a computer.
- the display physically transforms the computer peripheral.
- the computer peripheral is an LCD display
- the orientations of liquid crystals are changed by the application of biasing voltages in a physical transformation that is visually apparent to the user.
- the computer peripheral is a cathode ray tube (CRT)
- CTR cathode ray tube
- the state of a fluorescent screen is changed by the impact of electrons in a physical transformation that is also visually apparent.
- the display of content on a computer peripheral is tied to a particular machine, namely, the computer peripheral.
- the users may be provided with an opportunity to control whether programs or features that may collect personal information (e.g., information about a user's calendar, social network, social actions or activities, a user's preferences, or a user's current location), or to control whether and/or how to receive content that may be more relevant to (or likely to be clicked on by) the user.
- personal information e.g., information about a user's calendar, social network, social actions or activities, a user's preferences, or a user's current location
- certain data may be anonym ized in one or more ways before it is stored or used, so that personally identifiable information is removed when generating monetizable parameters (e.g., monetizable demographic parameters).
- a user's identity may be anonym ized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code, or state level
- the user may have control over how information is collected (and/or used) about him or her.
Abstract
Description
Θ=(o 1 , . . . , o O); o iε
wherein D is the dimensionality of the observation vectors. The accumulated and centered first-order Baum-Welch statistics can then be computed as:
where μm is the mean vector of mixture components m (m=1, . . . , C) of the UBM, and P(m|o, λ) is the Gaussian occupation probability for mixture m and observation o. A vector containing the stacked statistics is defined as:
F=(F 1 T, . . . , F C T)T ; Fε
xε
where, according to TVM, the vector F is related to x via the rectangular low-rank matrix
Tε
known as the total variability (TV) subspace:
N −1 F=Tx
where
Nε
is a diagonal matrix with C blocks of size D×D along the diagonal. Block m=1, . . . , C, is the matrix:
NmI(D×D)
Therefore, the i-vector formulation transforms the sequence of D dimensional vectors, into a single vector per utterance, the single vector carrying information about the speaker. The dimensionality of the i-vector is denoted as d.
x=(I+T TΣ−1 NT)−1 T TΣ−1 F
Claims (20)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/460,342 US9922645B2 (en) | 2014-02-14 | 2017-03-16 | Recognizing speech in the presence of additional audio |
US15/887,034 US10431213B2 (en) | 2014-02-14 | 2018-02-02 | Recognizing speech in the presence of additional audio |
US16/548,947 US11031002B2 (en) | 2014-02-14 | 2019-08-23 | Recognizing speech in the presence of additional audio |
US17/303,139 US11942083B2 (en) | 2014-02-14 | 2021-05-21 | Recognizing speech in the presence of additional audio |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/181,345 US9318112B2 (en) | 2014-02-14 | 2014-02-14 | Recognizing speech in the presence of additional audio |
US15/093,309 US9601116B2 (en) | 2014-02-14 | 2016-04-07 | Recognizing speech in the presence of additional audio |
US15/460,342 US9922645B2 (en) | 2014-02-14 | 2017-03-16 | Recognizing speech in the presence of additional audio |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/093,309 Continuation US9601116B2 (en) | 2014-02-14 | 2016-04-07 | Recognizing speech in the presence of additional audio |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/887,034 Continuation US10431213B2 (en) | 2014-02-14 | 2018-02-02 | Recognizing speech in the presence of additional audio |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170186424A1 US20170186424A1 (en) | 2017-06-29 |
US9922645B2 true US9922645B2 (en) | 2018-03-20 |
Family
ID=53798631
Family Applications (6)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/181,345 Active 2034-05-18 US9318112B2 (en) | 2014-02-14 | 2014-02-14 | Recognizing speech in the presence of additional audio |
US15/093,309 Active US9601116B2 (en) | 2014-02-14 | 2016-04-07 | Recognizing speech in the presence of additional audio |
US15/460,342 Active US9922645B2 (en) | 2014-02-14 | 2017-03-16 | Recognizing speech in the presence of additional audio |
US15/887,034 Active US10431213B2 (en) | 2014-02-14 | 2018-02-02 | Recognizing speech in the presence of additional audio |
US16/548,947 Active US11031002B2 (en) | 2014-02-14 | 2019-08-23 | Recognizing speech in the presence of additional audio |
US17/303,139 Active 2034-10-10 US11942083B2 (en) | 2014-02-14 | 2021-05-21 | Recognizing speech in the presence of additional audio |
Family Applications Before (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/181,345 Active 2034-05-18 US9318112B2 (en) | 2014-02-14 | 2014-02-14 | Recognizing speech in the presence of additional audio |
US15/093,309 Active US9601116B2 (en) | 2014-02-14 | 2016-04-07 | Recognizing speech in the presence of additional audio |
Family Applications After (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/887,034 Active US10431213B2 (en) | 2014-02-14 | 2018-02-02 | Recognizing speech in the presence of additional audio |
US16/548,947 Active US11031002B2 (en) | 2014-02-14 | 2019-08-23 | Recognizing speech in the presence of additional audio |
US17/303,139 Active 2034-10-10 US11942083B2 (en) | 2014-02-14 | 2021-05-21 | Recognizing speech in the presence of additional audio |
Country Status (1)
Country | Link |
---|---|
US (6) | US9318112B2 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10431213B2 (en) * | 2014-02-14 | 2019-10-01 | Google Llc | Recognizing speech in the presence of additional audio |
WO2022102987A1 (en) * | 2020-11-12 | 2022-05-19 | 삼성전자 주식회사 | Electronic device and control method thereof |
US11763799B2 (en) | 2020-11-12 | 2023-09-19 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
Families Citing this family (97)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9812150B2 (en) | 2013-08-28 | 2017-11-07 | Accusonus, Inc. | Methods and systems for improved signal decomposition |
US20170078737A1 (en) * | 2014-02-27 | 2017-03-16 | Lg Electronics Inc. | Digital device and control method therefor |
US10468036B2 (en) * | 2014-04-30 | 2019-11-05 | Accusonus, Inc. | Methods and systems for processing and mixing signals using signal decomposition |
US9905224B2 (en) * | 2015-06-11 | 2018-02-27 | Nice Ltd. | System and method for automatic language model generation |
US9779759B2 (en) * | 2015-09-17 | 2017-10-03 | Sonos, Inc. | Device impairment detection |
US10142754B2 (en) | 2016-02-22 | 2018-11-27 | Sonos, Inc. | Sensor on moving component of transducer |
US10264030B2 (en) | 2016-02-22 | 2019-04-16 | Sonos, Inc. | Networked microphone device control |
US10743101B2 (en) | 2016-02-22 | 2020-08-11 | Sonos, Inc. | Content mixing |
US9947316B2 (en) | 2016-02-22 | 2018-04-17 | Sonos, Inc. | Voice control of a media playback system |
US10509626B2 (en) | 2016-02-22 | 2019-12-17 | Sonos, Inc | Handling of loss of pairing between networked devices |
US10095470B2 (en) | 2016-02-22 | 2018-10-09 | Sonos, Inc. | Audio response playback |
US9965247B2 (en) | 2016-02-22 | 2018-05-08 | Sonos, Inc. | Voice controlled media playback system based on user profile |
CN105788608B (en) * | 2016-03-03 | 2019-03-26 | 渤海大学 | Chinese phonetic mother method for visualizing neural network based |
CN105869645B (en) * | 2016-03-25 | 2019-04-12 | 腾讯科技（深圳）有限公司 | Voice data processing method and device |
US20170294138A1 (en) * | 2016-04-08 | 2017-10-12 | Patricia Kavanagh | Speech Improvement System and Method of Its Use |
US20170330565A1 (en) * | 2016-05-13 | 2017-11-16 | Bose Corporation | Handling Responses to Speech Processing |
US9978390B2 (en) | 2016-06-09 | 2018-05-22 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US10152969B2 (en) | 2016-07-15 | 2018-12-11 | Sonos, Inc. | Voice detection by multiple devices |
US10134399B2 (en) | 2016-07-15 | 2018-11-20 | Sonos, Inc. | Contextualization of voice inputs |
US10115400B2 (en) | 2016-08-05 | 2018-10-30 | Sonos, Inc. | Multiple voice services |
US9693164B1 (en) | 2016-08-05 | 2017-06-27 | Sonos, Inc. | Determining direction of networked microphone device relative to audio playback device |
US9794720B1 (en) | 2016-09-22 | 2017-10-17 | Sonos, Inc. | Acoustic position measurement |
US9942678B1 (en) | 2016-09-27 | 2018-04-10 | Sonos, Inc. | Audio playback settings for voice interaction |
US9743204B1 (en) | 2016-09-30 | 2017-08-22 | Sonos, Inc. | Multi-orientation playback device microphones |
US10181323B2 (en) | 2016-10-19 | 2019-01-15 | Sonos, Inc. | Arbitration-based voice recognition |
US10134396B2 (en) | 2016-12-07 | 2018-11-20 | Google Llc | Preventing of audio attacks |
US10242673B2 (en) * | 2016-12-07 | 2019-03-26 | Google Llc | Preventing of audio attacks using an input and an output hotword detection model |
US11100384B2 (en) | 2017-02-14 | 2021-08-24 | Microsoft Technology Licensing, Llc | Intelligent device user interactions |
US10467509B2 (en) | 2017-02-14 | 2019-11-05 | Microsoft Technology Licensing, Llc | Computationally-efficient human-identifying smart assistant computer |
US11010601B2 (en) | 2017-02-14 | 2021-05-18 | Microsoft Technology Licensing, Llc | Intelligent assistant device communicating non-verbal cues |
US11183181B2 (en) | 2017-03-27 | 2021-11-23 | Sonos, Inc. | Systems and methods of multiple voice services |
KR102068182B1 (en) * | 2017-04-21 | 2020-01-20 | 엘지전자 주식회사 | Voice recognition apparatus and home appliance system |
JP6531776B2 (en) * | 2017-04-25 | 2019-06-19 | トヨタ自動車株式会社 | Speech dialogue system and speech dialogue method |
US20190028806A1 (en) * | 2017-07-19 | 2019-01-24 | Origin Acoustics, LLC | Amplifier with Voice Activated Audio Override |
US10475449B2 (en) | 2017-08-07 | 2019-11-12 | Sonos, Inc. | Wake-word detection suppression |
JP2019032400A (en) * | 2017-08-07 | 2019-02-28 | 富士通株式会社 | Utterance determination program, utterance determination method, and utterance determination device |
US10048930B1 (en) | 2017-09-08 | 2018-08-14 | Sonos, Inc. | Dynamic computation of system response volume |
US10446165B2 (en) | 2017-09-27 | 2019-10-15 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US10051366B1 (en) | 2017-09-28 | 2018-08-14 | Sonos, Inc. | Three-dimensional beam forming with a microphone array |
US10482868B2 (en) | 2017-09-28 | 2019-11-19 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US10621981B2 (en) | 2017-09-28 | 2020-04-14 | Sonos, Inc. | Tone interference cancellation |
US10466962B2 (en) | 2017-09-29 | 2019-11-05 | Sonos, Inc. | Media playback system with voice assistance |
US10665234B2 (en) * | 2017-10-18 | 2020-05-26 | Motorola Mobility Llc | Detecting audio trigger phrases for a voice recognition session |
US10481865B2 (en) | 2017-10-23 | 2019-11-19 | International Business Machines Corporation | Automated voice enablement of applications |
US10268457B1 (en) | 2017-10-23 | 2019-04-23 | International Business Machines Corporation | Prospective voice user interface modality identification |
US10268458B1 (en) | 2017-10-23 | 2019-04-23 | International Business Mahcines Corporation | Prospective voice user interface modality identification |
US10585640B2 (en) | 2017-10-23 | 2020-03-10 | International Business Machines Corporation | Automated voice enablement of applications |
US10880650B2 (en) | 2017-12-10 | 2020-12-29 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US10818290B2 (en) | 2017-12-11 | 2020-10-27 | Sonos, Inc. | Home graph |
KR102420567B1 (en) * | 2017-12-19 | 2022-07-13 | 삼성전자주식회사 | Method and device for voice recognition |
US10923101B2 (en) * | 2017-12-26 | 2021-02-16 | International Business Machines Corporation | Pausing synthesized speech output from a voice-controlled device |
WO2019152722A1 (en) | 2018-01-31 | 2019-08-08 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
US10937438B2 (en) * | 2018-03-29 | 2021-03-02 | Ford Global Technologies, Llc | Neural network generative modeling to transform speech utterances and augment training data |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US10847178B2 (en) | 2018-05-18 | 2020-11-24 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US10959029B2 (en) | 2018-05-25 | 2021-03-23 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
US10681460B2 (en) | 2018-06-28 | 2020-06-09 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
US10461710B1 (en) | 2018-08-28 | 2019-10-29 | Sonos, Inc. | Media playback system with maximum volume setting |
US11076035B2 (en) | 2018-08-28 | 2021-07-27 | Sonos, Inc. | Do not disturb feature for audio notifications |
KR102637339B1 (en) * | 2018-08-31 | 2024-02-16 | 삼성전자주식회사 | Method and apparatus of personalizing voice recognition model |
US10878811B2 (en) | 2018-09-14 | 2020-12-29 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US10587430B1 (en) | 2018-09-14 | 2020-03-10 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US10811015B2 (en) | 2018-09-25 | 2020-10-20 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US10692518B2 (en) | 2018-09-29 | 2020-06-23 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
KR102606789B1 (en) | 2018-10-01 | 2023-11-28 | 삼성전자주식회사 | The Method for Controlling a plurality of Voice Recognizing Device and the Electronic Device supporting the same |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
EP3654249A1 (en) | 2018-11-15 | 2020-05-20 | Snips | Dilated convolutions and gating for efficient keyword spotting |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11132989B2 (en) | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US10602268B1 (en) | 2018-12-20 | 2020-03-24 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US10867604B2 (en) | 2019-02-08 | 2020-12-15 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
CN110111798B (en) * | 2019-04-29 | 2023-05-05 | 平安科技（深圳）有限公司 | Method, terminal and computer readable storage medium for identifying speaker |
US11120794B2 (en) | 2019-05-03 | 2021-09-14 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
KR20220024217A (en) * | 2019-05-30 | 2022-03-03 | 인슈어런스 서비시스 오피스, 인코포레이티드 | Systems and methods for machine learning of speech properties |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
US10586540B1 (en) | 2019-06-12 | 2020-03-10 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US10871943B1 (en) | 2019-07-31 | 2020-12-22 | Sonos, Inc. | Noise classification for event detection |
US11138975B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US11138969B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US11342895B2 (en) * | 2019-10-07 | 2022-05-24 | Bose Corporation | Systems and methods for modifying an audio playback |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
CN111048067A (en) * | 2019-11-11 | 2020-04-21 | 云知声智能科技股份有限公司 | Microphone response method and device |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
CN115699170A (en) | 2020-06-10 | 2023-02-03 | 谷歌有限责任公司 | Text echo cancellation |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
US11694692B2 (en) * | 2020-11-11 | 2023-07-04 | Bank Of America Corporation | Systems and methods for audio enhancement and conversion |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
Citations (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6314402B1 (en) | 1999-04-23 | 2001-11-06 | Nuance Communications | Method and apparatus for creating modifiable and combinable speech objects for acquiring information from a speaker in an interactive voice response system |
US20020173333A1 (en) * | 2001-05-18 | 2002-11-21 | Buchholz Dale R. | Method and apparatus for processing barge-in requests |
US7162421B1 (en) | 2002-05-06 | 2007-01-09 | Nuance Communications | Dynamic barge-in in a speech-responsive system |
US7315613B2 (en) | 2002-03-11 | 2008-01-01 | International Business Machines Corporation | Multi-modal messaging |
US7392188B2 (en) | 2003-07-31 | 2008-06-24 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method enabling acoustic barge-in |
US8073699B2 (en) | 2005-08-16 | 2011-12-06 | Nuance Communications, Inc. | Numeric weighting of error recovery prompts for transfer to a human agent from an automated speech response system |
US20120029904A1 (en) | 2010-07-30 | 2012-02-02 | Kristin Precoda | Method and apparatus for adding new vocabulary to interactive translation and dialogue systems |
US8112280B2 (en) * | 2007-11-19 | 2012-02-07 | Sensory, Inc. | Systems and methods of performing speech recognition with barge-in for use in a bluetooth system |
US20120084084A1 (en) * | 2010-10-04 | 2012-04-05 | LI Creative Technologies, Inc. | Noise cancellation device for communications in high noise environments |
US8527270B2 (en) | 2010-07-30 | 2013-09-03 | Sri International | Method and apparatus for conducting an interactive dialogue |
US20140128004A1 (en) | 2012-11-08 | 2014-05-08 | Stmicroelectronics Asia Pacific Pte Ltd. | Converting samples of a signal at a sample rate into samples of another signal at another sample rate |
US20150112671A1 (en) | 2013-10-18 | 2015-04-23 | Plantronics, Inc. | Headset Interview Mode |
US20150112684A1 (en) | 2013-10-17 | 2015-04-23 | Sri International | Content-Aware Speaker Recognition |
US20150110263A1 (en) | 2013-10-18 | 2015-04-23 | Plantronics, Inc. | Headset Dictation Mode |
US9390725B2 (en) * | 2014-08-26 | 2016-07-12 | ClearOne Inc. | Systems and methods for noise reduction using speech recognition and speech synthesis |
US20160260440A1 (en) * | 2015-03-03 | 2016-09-08 | Continental Automotive Systems, Inc. | Speech quality under heavy noise conditions in hands-free communication |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9318112B2 (en) * | 2014-02-14 | 2016-04-19 | Google Inc. | Recognizing speech in the presence of additional audio |
WO2018187234A1 (en) * | 2017-04-03 | 2018-10-11 | Ex-Iq, Inc. | Hands-free annotations of audio text |
WO2019156961A1 (en) * | 2018-02-06 | 2019-08-15 | Bose Corporation | Location-based personal audio |
US10943599B2 (en) * | 2018-10-26 | 2021-03-09 | Spotify Ab | Audio cancellation for voice recognition |
CA3193267A1 (en) * | 2020-09-14 | 2022-03-17 | Pindrop Security, Inc. | Speaker specific speech enhancement |
-
2014
- 2014-02-14 US US14/181,345 patent/US9318112B2/en active Active
-
2016
- 2016-04-07 US US15/093,309 patent/US9601116B2/en active Active
-
2017
- 2017-03-16 US US15/460,342 patent/US9922645B2/en active Active
-
2018
- 2018-02-02 US US15/887,034 patent/US10431213B2/en active Active
-
2019
- 2019-08-23 US US16/548,947 patent/US11031002B2/en active Active
-
2021
- 2021-05-21 US US17/303,139 patent/US11942083B2/en active Active
Patent Citations (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6314402B1 (en) | 1999-04-23 | 2001-11-06 | Nuance Communications | Method and apparatus for creating modifiable and combinable speech objects for acquiring information from a speaker in an interactive voice response system |
US20020173333A1 (en) * | 2001-05-18 | 2002-11-21 | Buchholz Dale R. | Method and apparatus for processing barge-in requests |
US7315613B2 (en) | 2002-03-11 | 2008-01-01 | International Business Machines Corporation | Multi-modal messaging |
US20080152095A1 (en) | 2002-03-11 | 2008-06-26 | Jan Kleindienst | Multi-modal messaging |
US7162421B1 (en) | 2002-05-06 | 2007-01-09 | Nuance Communications | Dynamic barge-in in a speech-responsive system |
US7392188B2 (en) | 2003-07-31 | 2008-06-24 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method enabling acoustic barge-in |
US8073699B2 (en) | 2005-08-16 | 2011-12-06 | Nuance Communications, Inc. | Numeric weighting of error recovery prompts for transfer to a human agent from an automated speech response system |
US8566104B2 (en) | 2005-08-16 | 2013-10-22 | Nuance Communications, Inc. | Numeric weighting of error recovery prompts for transfer to a human agent from an automated speech response system |
US8112280B2 (en) * | 2007-11-19 | 2012-02-07 | Sensory, Inc. | Systems and methods of performing speech recognition with barge-in for use in a bluetooth system |
US8527270B2 (en) | 2010-07-30 | 2013-09-03 | Sri International | Method and apparatus for conducting an interactive dialogue |
US20120029904A1 (en) | 2010-07-30 | 2012-02-02 | Kristin Precoda | Method and apparatus for adding new vocabulary to interactive translation and dialogue systems |
US20120084084A1 (en) * | 2010-10-04 | 2012-04-05 | LI Creative Technologies, Inc. | Noise cancellation device for communications in high noise environments |
US20140128004A1 (en) | 2012-11-08 | 2014-05-08 | Stmicroelectronics Asia Pacific Pte Ltd. | Converting samples of a signal at a sample rate into samples of another signal at another sample rate |
US20150112684A1 (en) | 2013-10-17 | 2015-04-23 | Sri International | Content-Aware Speaker Recognition |
US20150112671A1 (en) | 2013-10-18 | 2015-04-23 | Plantronics, Inc. | Headset Interview Mode |
US20150110263A1 (en) | 2013-10-18 | 2015-04-23 | Plantronics, Inc. | Headset Dictation Mode |
US9390725B2 (en) * | 2014-08-26 | 2016-07-12 | ClearOne Inc. | Systems and methods for noise reduction using speech recognition and speech synthesis |
US20160260440A1 (en) * | 2015-03-03 | 2016-09-08 | Continental Automotive Systems, Inc. | Speech quality under heavy noise conditions in hands-free communication |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10431213B2 (en) * | 2014-02-14 | 2019-10-01 | Google Llc | Recognizing speech in the presence of additional audio |
US20200051553A1 (en) * | 2014-02-14 | 2020-02-13 | Google Llc | Recognizing speech in the presence of additional audio |
US11031002B2 (en) * | 2014-02-14 | 2021-06-08 | Google Llc | Recognizing speech in the presence of additional audio |
US20210272562A1 (en) * | 2014-02-14 | 2021-09-02 | Google Llc | Recognizing speech in the presence of additional audio |
US11942083B2 (en) * | 2014-02-14 | 2024-03-26 | Google Llc | Recognizing speech in the presence of additional audio |
WO2022102987A1 (en) * | 2020-11-12 | 2022-05-19 | 삼성전자 주식회사 | Electronic device and control method thereof |
US11763799B2 (en) | 2020-11-12 | 2023-09-19 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
Also Published As
Publication number | Publication date |
---|---|
US20160225373A1 (en) | 2016-08-04 |
US20150235637A1 (en) | 2015-08-20 |
US10431213B2 (en) | 2019-10-01 |
US20200051553A1 (en) | 2020-02-13 |
US9601116B2 (en) | 2017-03-21 |
US20170186424A1 (en) | 2017-06-29 |
US20180211653A1 (en) | 2018-07-26 |
US9318112B2 (en) | 2016-04-19 |
US20210272562A1 (en) | 2021-09-02 |
US11031002B2 (en) | 2021-06-08 |
US11942083B2 (en) | 2024-03-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11942083B2 (en) | Recognizing speech in the presence of additional audio | |
US9240183B2 (en) | Reference signal suppression in speech recognition | |
US9754584B2 (en) | User specified keyword spotting using neural network feature extractor | |
US9466289B2 (en) | Keyword detection with international phonetic alphabet by foreground model and background model | |
US8775191B1 (en) | Efficient utterance-specific endpointer triggering for always-on hotwording | |
US8775177B1 (en) | Speech recognition process | |
US8831947B2 (en) | Method and apparatus for large vocabulary continuous speech recognition using a hybrid phoneme-word lattice | |
Torres-Carrasquillo et al. | Dialect identification using Gaussian mixture models | |
US20150279351A1 (en) | Keyword detection based on acoustic alignment | |
US20150127594A1 (en) | Transfer learning for deep neural network based hotword detection | |
CN111627424A (en) | Method, system, and medium for speech endpoint localization based on word comparison | |
CN110675866B (en) | Method, apparatus and computer readable recording medium for improving at least one semantic unit set | |
Fukuda et al. | Detecting breathing sounds in realistic Japanese telephone conversations and its application to automatic speech recognition | |
Këpuska et al. | A novel wake-up-word speech recognition system, wake-up-word recognition task, technology and evaluation | |
Costa et al. | Speech and phoneme segmentation under noisy environment through spectrogram image analysis | |
Këpuska | Wake-up-word speech recognition | |
Zehetner et al. | Wake-up-word spotting for mobile systems | |
Bhukya et al. | End point detection using speech-specific knowledge for text-dependent speaker verification | |
Hung et al. | Automatic identification of vietnamese dialects | |
JP7098587B2 (en) | Information processing device, keyword detection device, information processing method and program | |
Prukkanon et al. | F0 contour approximation model for a one-stream tonal word recognition system | |
AU2019100034A4 (en) | Improving automatic speech recognition based on user feedback | |
Li et al. | Multi-task deep neural network acoustic models with model adaptation using discriminative speaker identity for whisper recognition | |
Okomba et al. | Survey of Technical Progress in Speech Recognition by Machine over Few Years of Research | |
Kang et al. | Speaking rate control based on time-scale modification and its effects on the performance of speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MELENDO CASADO, DIEGO;LOPEZ MORENO, IGNACIO;GONZALEZ-DOMINGUEZ, JAVIER;SIGNING DATES FROM 20040214 TO 20140214;REEL/FRAME:041600/0396 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044129/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |