CN101821734B - Detection and classification of matches between time-based media - Google Patents
Detection and classification of matches between time-based media Download PDFInfo
- Publication number
- CN101821734B CN101821734B CN2008801105573A CN200880110557A CN101821734B CN 101821734 B CN101821734 B CN 101821734B CN 2008801105573 A CN2008801105573 A CN 2008801105573A CN 200880110557 A CN200880110557 A CN 200880110557A CN 101821734 B CN101821734 B CN 101821734B
- Authority
- CN
- China
- Prior art keywords
- fingerprint
- keyword
- sub
- video
- input
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/48—Matching video sequences
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
Abstract
A system and method detects matches between portions of video content. A matching module receives an input video fingerprint representing an input video and a set of reference fingerprints representing reference videos in a reference database. The matching module compares the reference fingerprints and input fingerprints to generate a list of candidate segments from the reference video set. Each candidate segment comprises a time- localized portion of a reference video that potentially matches the input video. A classifier is applied to each of the candidate segments to classify the segment as a matching segment or a non-matching segment. A result is then outputted identifying a matching portion of a reference video from the reference video set based on the segments classified as matches.
Description
Technical field
Present invention relates in general to Video processing, relate more specifically to detect the video content of coupling.
Background technology
The electric video storehouse can comprise thousands of video file, and this makes management to these storehouses become to have challenging task.Video trustship website need be used for the mechanism of sign unauthorized video.Though some files can identify by other information that filename or user provide, these identifying informations may be wrong or be not enough to correctly identify video.Use the alternative approach of manually coming manually to identify video content expensive and consuming time.
Another problem that the video sharing site faces is that this website may comprise a plurality of copies of same video content.This has wasted storage space, and becomes a big expense of main frame.The 3rd problem is: because the huge amount of file, be difficult to according to for the user easily mode organize video library.For example, Search Results may have the copy of a plurality of identical or closely similar videos, makes that the user is difficult to the result is navigated.
In view of the above problems, need a kind of technology that automatically compares and mate overlapping video content.
Summary of the invention
A kind of system and method detects the video content that repeats.Matching module receives the input video fingerprint (fingerprint) of expression input video.Matching module generates the tabulation from the candidate segment of reference video set.Each candidate segment comprises the time-localized portion of the reference video in the reference video set.To each candidate segment application class device, in order to segmentation is categorized as coupling segmentation or non-matching segmentation.Bear results based on the segmentation that is classified as coupling then, this result's sign is from the compatible portion of the reference video of reference video set.
In one embodiment, mate to determine candidate segment by the reference fingerprint that obtains the reference video in the expression reference video set and the part that identifies between input fingerprint and the reference fingerprint.Mate to determine the set of initial candidate reference video then based on the part of sign.Analyze the initial candidate reference video, with the coupling continuous in time between the segmentation of the segmentation of determining input video and reference video.Select candidate segment based on this coupling continuous in time then.
The feature of describing in the instructions and advantage are not to be intended to limit, and especially, with reference to the accompanying drawings, instructions and claim, multiple additional feature and advantage will be obvious to those skilled in the art.And, should be noted that in the instructions that employed language is in principle for readability and purpose of indicating rather than in order to define or to limit subject matter and select.
Description of drawings
Fig. 1 is an embodiment for detection of the system of the coupling between input video and the reference video set;
Fig. 2 is an embodiment for the reference database that reference video and input video are mated;
Fig. 3 is an embodiment for detection of the process of match video content;
Fig. 4 is for the embodiment of generation with the process of the tabulation of candidate's video segmentation of the potential coupling of input video;
Fig. 5 shows an embodiment that repeats the table of (duplicate-free) tabulation at the nothing of the LSH keyword of input video generation;
Fig. 6 show for according to and the part of input video mate to determine the embodiment of technology of the quality factor (figure of merit) of reference video;
Fig. 7 shows an embodiment for the technology that the input video fingerprint is arranged;
Fig. 8 shows an embodiment for the technology that the LSH keyword of the sub-fingerprint of video is arranged;
Fig. 9 shows an embodiment of the technology of searching for the LSH of the sub-fingerprint ordered set of input video;
Figure 10 shows for the sub-fingerprint of input video of determining coupling and an embodiment of the mapping between the sub-fingerprint of reference video;
Figure 11 is an embodiment for the process that the reference video segmentation is categorized as coupling segmentation or non-matching segmentation; And
Figure 12 is an embodiment of the technology of time of input video localization piece and reference video being mated be used to the time of striding.
Accompanying drawing has only been drawn each embodiment of the present invention for the purpose of illustrating.Those skilled in the art will be easy to recognize according to following description, under the situation that does not break away from principle of the present invention described herein, can adopt the structure that illustrates and the alternate embodiment of method herein.
Embodiment
A kind of system and method has been described, be used for to determine input medium file (for example video or audio frequency or both) whether with the reference set (for example, the database of video and/or audio fragment) of media file in reference media file coupling or part coupling.The matching detection process can detect the coupling between the part (for example, 20 seconds fragment) of media file, even media file does not have identical beginning and end point, and the content difference that perhaps before or after compatible portion, occurs.In addition, the enough robusts of this process reduce to bear the standard that occurs owing to the inferior quality transcoding, and are robusts for a certain amount of time-scale modification (for example, comparatively fast or playback video) more slowly.This process can correctly classify " kidney-Yang " (situation that has one or more matched media files in the database) or " Kidney-Yin " (do not have in the database corresponding coupling situation) both.Usually, this process can (be uploaded traffic rate in order to processing) and/or use limited amount storer to detect coupling under urgent time-constrain.
Fig. 1 shows an embodiment for detection of the system of the coupling between the time-base media.Though it should be noted that concrete example is to provide in the context of match video content, the media content that described system and method can be used for other types mates for example audio frequency, image etc.Picked-up (ingest) server 104 receives input video 102 from video source.For example, video source can be the client computer that communicates by network with ingest server 104.Alternatively, video source can be database or other memory device that can be coupled to ingest server 104 communicatedly.For example, video source can be video storage medium, for example DVD, CD-ROM, digital video recorder (DVR), hard disk drive, flash memory or other storeies.Ingest server 104 can also be coupled to video capture system (for example video camera) communicatedly to receive the instant video content.
In one embodiment, fingerprint generation module 106 is divided into a plurality of overlapping segmentations with the video that receives, and generates sub-fingerprint at each segmentation.The preferred length of segmentation is 0.5 second to 5.0 seconds, but also can use other length segment.Thus, the time of each sub-fingerprint representation medium localization segmentation (for example, the audio frequency of 4 seconds video or 1.5 seconds).The start time of segmentation usually with fixed frequency separately (for example, for video every 0.25 second, or for audio frequency every 0.10 second).For example, at calculate the first sub-fingerprint from 0 second to 4 seconds segmentation, at calculate the second sub-fingerprint from 0.25 second to 4.25 seconds segmentation, at calculate the 3rd sub-fingerprint from 0.50 second to 4.50 seconds segmentation, by that analogy.Each sub-fingerprint comes reference by sub-fingerprint identifier code, and this sub-fingerprint identifier code identifies the particular fragments of the represented video of this sub-fingerprint.For example, sub-fingerprint identifier code can comprise that (video, skew) is right." video " part of identifier identifies the video (for example, using 32 bit identifiers) in the reference video data storehouse uniquely." skew " part of identifier identifies the particular fragments (or corresponding sub-fingerprint) of video by for example offset index of referred fragment start time.For example, if the beginning of segmentation 0.25 second at interval, then the segmentation beginning in 0 second can have offset index 0, has offset index 1 in the segmentation of beginning in 0.25 second, can have offset index 2 in the segmentation of beginning in 0.5 second, by that analogy.Segmentation also can directly identify by its start time.The complete ordered sequence of sub-fingerprint provides the complete fingerprint of video.
In one embodiment, each sub-fingerprint comprises the vector of value, and each value is taken from the unordered value in the alphabet (for example, the alphabet of 256 sizes, each vector dimension is encoded to a byte, but order not) of limited size.For example, each value can be encoded based on the probability distribution of the value of using the entropy coding.By Hamming (Hamming) distance metric is replaced with the distance metric that is suitable for from the space of wherein obtaining sub-fingerprint, carry out the expansion to metric space.General introduction to (having generated fingerprint) sequence of nonuniform sampling will easily be seen to those skilled in the art.For clear, this instructions will be supposed uniform sampling, and will suppose to use the unordered alphabet of Hamming distance so that comparison.
In one embodiment, the fingerprint of some reference video can use the attaching metadata in the fingerprint base 122 to carry out mark, and for example reference video comprises the indication of " premium content ".The video that is marked as " premium content " is those videos that are worth increasing protection level and give special consideration during matching process, and this will be described following.Premium content is specified and can be determined according to a plurality of different factors.In one embodiment, the content owner determines which content is appointed as premium content.For example, media companies can select " top " video of some as its video of paying close attention to most from the video that it has.In another embodiment, the appointment of premium content can be determined based on previous coupling is historical.For example, before be confirmed as having with the be complementary reference video of content of the input video of uploading subsequently and can automatically be appointed as premium content.In another embodiment, the appointment of premium content is based on the time span of reference video in reference library.For example, can be regarded as high-quality with reference to be in first month in the database at it, remove the appointment of its premium content then alternatively.In addition, the appointment of premium content can be multi-level.For example, can specify the different stage of premium content corresponding to the different stage of analyzing.
In one embodiment, each media track (for example audio or video) at reference video generates independently fingerprint.In addition, in some embodiments, can generate a plurality of fingerprints at each media track.For example, if the fingerprint generative process is left and right sides mirror image sensitivity, then this process can be chosen as each track of video and generate two fingerprints (second move fingerprint generative process at the mirror image frame, wherein, the frame in the video had carried out a left side/right upset before fingerprint generates).The fingerprint number (and type of applied conversion) that generates at each track can depend on the metadata that is associated with reference video, and whether reference video is marked as " premium content ".As another example, video can have a plurality of audio tracks corresponding to different sound channels, and in this case, each audio track can have fingerprint, or alternatively have only selected audio track (for example, a stereo left side and the stereo right side) can have fingerprint.
When generating a plurality of fingerprint at single track, sub-fingerprint with identical sub-fingerprint identifier code and medium type (for example, audio or video) identifier code and different subcodes are added to fingerprint base 122, in order to clearly distinguish between a plurality of fingerprints.To describe in order simplifying hereinafter, to suppose at the fingerprint storage of each classification of track (for example, Voice ﹠ Video) in fingerprint base 122 independently.And in order to simplify, described sample situation supposes that each reference video only has a track of video and an audio track, and each track has a fingerprint.
In one embodiment, based on the content creating reverse indexing table 124 of fingerprint base 122.If storehouse 122 is updated periodically (for example, when the user uploads new video), reverse indexing table 124 can upgrade by scheduling interval, perhaps in the renewal whenever of storehouse content change.In reverse indexing table 124, the subclass of sub-fingerprint value (or title " sub-fingerprint keyword ") provides the means of the sub-fingerprint of reference in a kind of home banking table 122.Each sub-fingerprint keyword and the sub-fingerprint set of the reference that comprises this sub-fingerprint keyword storage explicitly.Constructing this reverse indexing table 124 is for approximate nearest-neighbors function is provided.
An example of the reverse indexing table 124 of the responsive hash in reference fingerprint storehouse 122 and use location (LSH) has been shown among Fig. 2.The set of the sub-fingerprint of reference is corresponding to the reference video in the reference library 122.Each sub-fingerprint is associated with a segmentation of video, and uses representation X@ (Y) to identify, and wherein X is the identifier of video, and Y is the offset index of the segmentation of sign video X.Sub-fingerprint set (X@ (Y
1) ... X@ (Y
n)) formed the fingerprint of video X.For example, in Fig. 2, the fingerprint of reference video A comprises sub-fingerprint A@ (0), A@ (1) ... etc.Each sub-fingerprint is corresponding to the segmentation of reference video A, and this segmentation is identified by offset index.For example, A@ (0) sign expression video A is at the sub-fingerprint of the segmentation at offset index 0 place, and A@ (1) sign expression video A is at the sub-fingerprint of the segmentation at offset index 1 place, by that analogy.
Each sub-fingerprint comprises the sequence (for example, each value can be a byte) of value.The sequence of value is divided into a plurality of LSH bands, and each band is corresponding to a subclass of sub-fingerprint intermediate value.For example, LSH is with 0 preceding four values that comprise sub-fingerprint, and LSH is with 1 ensuing four values that comprise sub-fingerprint, by that analogy.In one embodiment, sub-fingerprint has 25 LSH bands, and each comprises 4 byte values.The set of the LSH band intermediate value of sub-fingerprint is with keyword corresponding to the LSH of aforesaid sub-fingerprint, and is stored in explicitly in the reverse indexing table 124 with the sub-fingerprint identifier that comprises this keyword.Each sub-fingerprint band keyword is also referred to as " LSH keyword " at this.
For example, sub-fingerprint A@ (0) be with at LSH and have value in 0 (65,22, A5, B1).This value set is represented by LSH keyword (65 22 A5 B1+0) in reverse indexing LSH table 124.In this representation ,+0 this keyword of indication appears at LSH and is with in 0.LSH table 124 maps to each LSH keyword in each the sub-fingerprint that comprises this LSH keyword in the storehouse 122.For example, this table and LSH keyword 234344D2+1 store the identifier of sub-fingerprint A@ (0) and B@ (0) explicitly, because each sub-fingerprint comprises LSH with the value (2,343 44 D2) in 1.The value of it should be noted that (11 34 55 56) appear at sub-fingerprint B@ (0) with in 2 and being with in 1 of sub-fingerprint B@ (1).Yet these are considered to different LSH keywords, and index independently in reverse indexing table 124 thus, because value sequence is in different LSH bands.
In alternative embodiment, the LSH band comprises the value subclass of non-intersect (non-adjacent) rather than the successive value that illustrates.Value is grouped into the constraint that the LSH band depends on concrete application.Another kind of alternative approach is used the LSH keyword of deriving from whole sub-fingerprints.For example, can by according to at random but a plurality of projections of (random-but-memorized) dividing plane of memory, according to the short sequence of sub-fingerprint or for example the frequency histogram of the sub-fingerprint entries value in the short support window calculate the code that symbol is arranged, thereby the LSH keyword of definite sub-fingerprint.The frequency histogram method can use the point of fixity frequency counting as keyword, and perhaps it can use in this period the most frequently the value of signature value as keyword.
In another alternate embodiment, use the tree of spill and leakage for example (spill tree) or M tree to wait the technology that is different from LSH that the nearest-neighbors function is provided, it uses Hamming distance in sub-fingerprint vector rank measurement as its metric space.In the following description, reverse indexing is called the LSH table, even and do not require that approximate nearest-neighbors function is provided by this specific data structure.
In this embodiment, optionally revise reverse indexing table 124 at common LSH keyword, with employed memory space and calculated amount in the control matching process.Particularly, table is configured to be marked with the LSH keyword that the possibility that helps to distinguish sub-fingerprint reduces.By the frequency of each the LSH keyword in the reference fingerprint and/or the various tests of distribution are determined this state.
The frequency that first test occurs based on the LSH keyword.In its fingerprint, comprise given LSH keyword if surpass the reference video of the predefine number percent of sum, then this LSH keyword is designated " first order blacklist " keyword in LSH table 124, and the sub-fingerprint identifier that will comprise this keyword deletes from table 124.What replace is that special identifier symbol code and this keyword of indication first order blacklist keyword are stored in the reverse indexing table 124 explicitly.For example, in one embodiment, if the reference video above 5% comprises keyword (00 00 00 0A+1), then in reverse indexing table 124, this keyword is designated first order blacklist keyword (by storing special identifier symbol code explicitly with this keyword, as " BL1 "), and do not store the sub-fingerprint identifier that is associated with this keyword.
If do not satisfy the standard of first order blacklist, but the sub-fingerprint sum that comprises given LSH keyword is still on certain threshold value, then this LSH keyword is labeled as in a different manner " second level blacklist " keyword in reverse indexing table 124.For example, if comprise keyword (00 00 00 0B+2) more than 10,000 sub-fingerprints of reference, but this 10,000 sub-fingerprint of reference is included in only in 1% of reference video, and then the special identifier of indication " second level blacklist " symbol is stored explicitly with this keyword.In one embodiment, do not store the tabulation fully of the sub-fingerprint identifier that comprises second level blacklist keyword.What replace is that the LSH table 124 only tabulation of store video identifier is not still stored offset index (that is, not identifying concrete segmentation).In addition, this table can be stored the counting that comprises the sub-fingerprint number in each video that mates keyword.For example, in table 124, keyword (00 00 00 0B+2) is identified as second level blacklist keyword, and special identifier symbol code (as " BL2 ") is stored explicitly with this keyword.Code (G, 6) is also stored explicitly with this keyword, and the fingerprint of instruction video " G " has 26 different sub-fingerprints that comprise coupling keyword (00 00 00 0B+2).The individual sub-fingerprint identifier that comprises the sub-fingerprint that mates keyword is not stored separately.
The reference video that the additional LSH table that can use does not have blacklist to be labeled as premium content as cup provides additional index.These tables only are included in the main LSH table and are put into those LSH keywords of a certain rank blacklist, and only comprise the reference video that is marked as premium content.In addition, these tables can also comprise the coefficient index to the reference video segmentation, otherwise it will lack from reverse indexing fully owing to blacklist.In one embodiment, sparse index is guaranteed: each marginal time interval (for example, the interval to be no more than 20 seconds) exists once all reference video all to have the reverse indexing clauses and subclauses that exclude blacklist at least.To add in the matching process described below from the clauses and subclauses of these tables, come the set of autonomous LSH table as them.
Fingerprint matching
When receiving input video 102 for coupling (for example, from uploaded content or from existing video database), according to the fingerprint generative process identical with being applied to reference video input video 102 is carried out fingerprint and generate.Matching module 108 determines which part (if any) of input video 102 and the part of the reference video in the reference database 120 are complementary then.In one embodiment, matching module 108 is determined coupling according to three phase process shown in Fig. 3.In the stage 1, matching module 108 is the candidate list that generates (302) candidate matches from the input video 102 of reference set.Each clauses and subclauses indication in the candidate list: the part that is complementary with the candidate potentially in (1) input video; (2) candidate is with reference to the video identifier of coupling; And the part that is complementary with input video potentially in (3) reference video.For example, candidate list can comprise the result who shows as the following table middle finger:
In subordinate phase, further each candidate entries in the candidate list is assessed (304), be correctly or the classification of false this locality so that indication coupling to be provided.The basis that should determine is from the evidence in the compatible portion of indicating in the candidate list (evidence).
In the phase III, remaining candidate matches is made up and blocks, with determine (306) stride the time and optional extend across passage () coupling for example, Voice ﹠ Video, thus final results set is provided.Below each stage of three stage matching processs is carried out more detailed description.
In one embodiment, if input video 102 is grown (for example, being longer than 30 minutes), then alternatively input video is carried out pre-service, input video is divided into " chapters and sections (chapter) " (for example, 32 minutes) that to manage size.Usually, the start and end time of chapters and sections defines like this, makes to have certain overlapping (for example, 2 of each ending place minutes) on a small quantity between the chapters and sections.In post-processing step, will finally export result's " stitching " and return together.For the example purpose, following description hypothesis input video has the manageable duration (for example, less than 30-32 minute).
Stage 1: candidate list generates
This creates from the short tabulation of the referred fragment of reference set and further considers being used in stage.This step helps control to calculate and storage is used, and provides and matching treatment and overall data storehouse size kept apart but only have and the greatest expected number (for example, 30-60 clauses and subclauses) of the real coupling clauses and subclauses Calculation Method of increase equally soon.The instantiation procedure in stage 1 has been shown among Fig. 4.
First step is that definite (402) are present in the LSH lists of keywords in the whole sequence of the sub-fingerprint of input of representing input video 102, and is mapped to the sub-fingerprint identifier of the corresponding reference of those keywords in the reverse indexing table 124.Fig. 5 shows an example embodiment of this step 402.The sub-fingerprint 502 of each input video is divided into 25 LSH bands, and each comprises the keyword of 4 byte values.In one embodiment, as shown in the figure, remove the duplicate key word in the identical LSH band.Even it should be noted that keyword in the Different L SH band comprises identical value and also do not think repetition.Then, from the LSH table 124 corresponding to each the unique LSH keyword the sub-fingerprint set of input, obtain with reference to sub-fingerprint identifier.The output of this step be the nothing of LSH keyword repeat to tabulate 504 and be mapped to LSH keyword in the LSH table 124 each with reference to sub-fingerprint identifier.Herein, representation Z@ (X
1, X
2, X
3... X
n) be illustrated in the time migration index X among the video Z
1, X
2, X
3... X
nThe place finds keyword.If any keyword causes first order blacklist or second level blacklist, then it being repeated to tabulate in nothing marks in 504.
Next, determine that (404) initial candidate video further considers being used for.In one embodiment, mark the general time location of sub-fingerprint and the counting of safeguarding the LSH keyword number that is associated with each part of this video by repeating in above-mentioned nothing to tabulate in 504, create the tabulation of initial candidate video.Then can be at the relevant coupling counting of each reference video creation-time, it is recorded in the matching frequency between the sub-fingerprint keyword of reference video and the sub-fingerprint keyword of input video during the different time window of each reference video.For example, Fig. 6 shows the histogram (certainly, in practice, the coupling of time correlation counting only is stored in the storer, and does not need to show or otherwise present in addition) of reference video D.In the example that illustrates, the coupling tally function is rudenss quantization (for example, 5 seconds resolution).Thus, the coupling tally function is safeguarded the counting of coupling keyword number in each the 5 seconds window occur in reference video, between reference video and the input video.For example, in the time window between 0 second to 5 seconds, exist the keyword that repeats to tabulate in 504 from 5 examples of the sub-fingerprint keyword of reference video D and nothing from the keyword of input video to be complementary.In the time window of the 5-10 of reference video between second, there are 3 coupling examples, in the time window of 10-15 between second, have 1 coupling example, etc.
Obtain the quality factor of each reference video then based on the coupling counting.This process is selected a plurality of videos from the top of the tabulation of arranging by quality factor then, will only consider selected a plurality of video in the processing in future.In one embodiment, by summation obtains quality factor to the coupling counting on the length of input video, wherein begin to be selected as making the quality factor maximization with end point.For example, in reference video D, to the sum summation of the coupling of the input video length (45 seconds) between 0 second to 45 seconds, to obtain quality factor 15.This can be implemented as, and coupling distributes and the convolution of the length of input video, follows and selects by maximal value:
Another alternative approach of creating the initial candidate list of videos is: be sighted the reference of having mated with additional priority on the passage (for example, another audio track) that checks before.For example, if this process has been finished voice-grade channel coupling and a present checking video channel of taking turns at the input medium file, then because identical reference will be added to the video channel candidate list, this process can make the inspection of video sound channel comparatively easy.The Voice ﹠ Video passage often is paired (or being selected from alternative group at least), so if it is reasonably that hypothesis voice-grade channel and given reference are complementary, then this process can be configured to more closely check the possibility of the same coupling of video channel.Be used for the constraint that this logic of striding the passage promotion depends on matching process.If constraint is not too strict, then can simply all references of mating at different passages be added in the initial candidate list of videos.If constraint does not allow this straightforward procedure, then can use previous coupling to reduce for the threshold value of creating the initial candidate list of videos.The complete duration that these threshold values can formerly be mated clauses and subclauses go up to reduce, and perhaps only reduces at the sync section of current track.
All subsequent treatment will only be considered as the reference video of the part of initial candidate reference video tabulation.Next, the piece that input video is divided into (406) time localization.In one embodiment, the length of piece is half of the real length of mating of the necessary the shortest expection that detects at the most.For example, in order to detect the coupling of 20 seconds length, piece can be 10 seconds or littler.Can be by with any cutting of input video being nonoverlapping 10 seconds pieces or by being that overlapping 10 seconds pieces (for example, by striding the whole input video length 10 seconds windows that slide) form these pieces with the input video cutting.Alternatively, can definite border (for example, cutting border or high moving boundaries when too wide based on the border extension of analyzing, utilize evenly spaced border to fill) forms piece by using video or audio analysis.Simple in order to describe, given example is used 10 seconds pieces of zero lap of any cutting.
Only consider that above this restriction of initial candidate list of videos of determining can be achieved like this: utilize the initial candidate list of videos to the initial candidate video (for example, video from above-mentioned preselected process) figure (for example, hash figure) fills out in advance, and do not allow further to add other reference video to this figure.
Each piece to input video carries out processing as described below independently.Which in one embodiment, the sub-fingerprint of the current processing block of input video is sorted, to determine at first to handle sub-fingerprint.In one embodiment, the antithetical phrase fingerprint sorts, and makes sub-fingerprint tabulation in the current block be up to the difference minimum from difference and arranges (408).In this embodiment, to counting from the sum initial candidate list of videos, that have the sub-fingerprint of reference of the keyword that is complementary with the sub-fingerprint of each input video.At first list the sub-fingerprint with minimum coupling.For example, Fig. 7 shows the sub-fingerprint set from the input video piece of working as pre-treatment.At each each LSH keyword of importing in the sub-fingerprint matching number is shown.For example, LSH keyword 1C865002+0 with have coupling from 4 sub-fingerprints of reference of initial candidate list of videos.Importing sub-fingerprint at each sues for peace to matching number.Should and be used to the sub-fingerprint of input is arranged as from minimum coupling (difference is maximum) to mating (difference is minimum) at most.
If second level blacklist will be listed in one or more LSH keywords of importing in the sub-fingerprint, then with the number of the matching candidate of sub-fingerprint and (stride the video summation in the LSH lists of keywords, or alternatively, stride candidate's video summation) be used for this part of sub-fingerprint.Will list in the first order blacklist as the one or more LSH keywords in the fruit fingerprint, then that certain is bigger value is used for the related candidate of the sub-fingerprint of input with reference to the number (for example, 100x second level blacklist threshold value) of sub-fingerprint.
In case carried out importing the arrangement that sub-fingerprint is gathered at current block, then the antithetical phrase fingerprint is handled, this starts from distinguishing maximum sub-fingerprint (minimum coupling).Handle each sub-fingerprint and scheme from the candidate for sky, unless the initial candidate video preprocessor has been filled into the top layer of figure.In each sub-fingerprint, the LSH keyword also is up to minimum order according to difference and arranges (410), is similar to the arrangement of above-mentioned antithetical phrase fingerprint.For example, as shown in Figure 8, determine the sub-fingerprint candidate's of match reference number for each the LSH keyword in the sub-fingerprint.Begin to handle keyword from the keyword with minimum coupling (difference is maximum) in order then.Use this arrangement of LSH keyword, processing starts from a LSH keyword, and adds the sub-fingerprint candidate of reference with coupling keyword in candidate figure.For example, as shown in Figure 9, the sub-fingerprint of orderly input video (and in each sub-fingerprint according to the order of LSH keyword) is carried out LSH search.When each is searched in execution, the sub-fingerprint identifier that storage obtains.In one embodiment, according to following restriction candidate's (corresponding to reference to sub-fingerprint) is added among the candidate figure:
(1) candidate comes from the initial candidate list of videos.This inspection can implicitly be finished by the pre-filling step of above-mentioned figure.If video identifier is in pre-blank map, then it is in the initial candidate tabulation, and process can be proceeded.If this position is not in the drawings, then do not record candidate matches.
(2) different candidates' number is no more than predefined threshold value among the figure.In case it reaches this threshold value, LSH keyword subsequently can increase the support to existing candidate, but cannot add new candidate.Again, the premium content candidate can make an exception.
(3) for the new candidate that will allow, there is (t-1) individual residue LSH piece at least, wherein t is LSH fingerprint threshold value (below be described).But this counting of residue LSH piece comprises the LSH piece that will be put into second level blacklist be not put into first order blacklist.
(4) consider at last to list the LSH piece of second level blacklist in, and be the visible occurrence count of each video creation (seen-occurrence-count), but the piece that pipes off of crossing over each candidate's video is sued for peace.
The candidate figure that obtains has from the sub-fingerprint identifier of the reference of initial candidate list of videos and the limited tabulation of mating the skew of the sub-fingerprint of current input.It is next in conjunction with the candidate that next step is striden the sub-fingerprint of input video, supports which reference and skew candidate to determine the sub-fingerprint of input.Each coupling between the sub-fingerprint of input video and the sub-fingerprint candidate of reference is carried out " ballot " at the concrete starting position of mating between input video and the reference video.Deduct by the matched position from reference video the starting position is determined in the skew of input video.For example, as shown in figure 10, the sub-fingerprint of input video that skew (2) is located has coupling with the sub-fingerprint D@ of reference (3).This has generated " ballot " for starting position D@ (2).Similarly, input video the sub-fingerprint located of skew (3) with have a coupling with reference to sub-fingerprint D@ (4).This coupling has also generated " ballot " for starting position D@ (2).Usually, if the sub-fingerprint at skew X place and the offset Y in the reference video have coupling in the input video, the beginning of this support input video and the Y-X+1 position in the reference video this inference that is complementary.The sub-fingerprint of striding each reference video adds up to ballot.In fact, the result is the Hough conversion of unit parallax, and it has provided the input video reference time that may mate.
Handle ballot similarly for each the sub-fingerprint in the input video.In one embodiment, will have at the reference offset of t at least of starting position ballot and transfer among the new candidate figure that maintenance supports at the candidate of current input video piece.Can optionally reduce this threshold value at premium content.For example, even comprise that the candidate of premium content is lower than the support level of non-prime content, also can transfer among the new candidate figure.For generating the sub-fingerprint of input too much have t or the reference candidates that vote a go-ahead, this threshold value is with higher more.Import on the sub-fingerprint at each, to this threshold value of adjusted, up to from the candidate's of the sub-fingerprint that will pass through number less than predefined number.For example, in one embodiment, allow 400,000 candidates right, wherein each candidate is to corresponding at per 10 seconds unique match reference of input video or begin skew.Yet employed candidate's concrete maximum number height depends on calculating and the time-constrain of system.By with add previous support (from the previous sub-fingerprint of considering of input in the current input video piece) to by candidate's support, will be transferred among the new candidate figure of current input video piece by the candidate.
In case checked all sub-fingerprints of current block, then with current block, previous piece with the figure of piece is added together afterwards, this has given evidence from the figure of current block with highest weighting, but allows other two piece figure to add the candidate or increase support to existing candidate.According to this combination mapping, process uses fuzzy peak value collection (smeared peak picking) (non-maximum the inhibition) to create the candidate list (video, skew) that will consider in subordinate phase to this figure.Simple convolutional in the skew that fuzzy peak value collection is the start time in the single candidate.An example embodiment comprises by quarter window carries out convolution, and three angular breadth of this quarter window are for the twice to the greatest expected temporal extension that may change of playback speed.For example, in the video playback of the piece with 10 seconds sizes, in order to support maximum 10% acceleration or slow down that complete three angular breadth will be 2 seconds.The convolution process will increase the height at the peak that has the support that is offset at the contiguous start time.Non-maximum the inhibition is such process, and the peak that its pressure is selected as final candidate of this stage belongs to the diacritic maximum lobe (lobe) of this Fuzzy Time signal.This process starts from locating the highest maximum value and record this position (and value) in signal.This process will make zero than more approaching this peaked value of certain time interval (for example, when using 10 seconds pieces, separating 5 seconds) then.The process that makes zero continues outwards to advance in time or retreat, and continues dull decline up to initial (bluring) function.Use fuzzy peak value collection/non-maximum inhibition that more general slope and intercept Hough mapping function is provided effectively, and do not need the needed extra memory of this type of bidimensional conversion.The output in stage 1 is the limited tabulation of candidate's video segmentation (each is corresponding to the sub-fingerprint of reference), will further consider to determine the compatible portion between input video and the reference video to it.Restriction to its setting also highly depends on system, is appropriate but common per 10 seconds input blocks are lower than 1000 candidate matches.
Stage 2: candidate's assessment
Subordinate phase handle to be considered candidate's video segmentation of being found by the phase one, and determine some, all or do not have whether candidate's video segmentation is effective coupling.Because process allows in the reference set possibility of repetition (all or part of) is arranged, so comparatively general description (but not the single of each detection piece is/denys to mate) is provided.Subordinate phase is handled and can be handled various medium, from non-the descriptions part that has a false coupling with multiple different files to responsive part of time (even identical reference material, when its in time during slight shift, this time-sensitive part is also mated not good).
In order to handle this task and this material ranges, assorting process starts from creating quality of match at each candidate's video segmentation and measures.In one embodiment, definite (1102) the current input video piece of dynamic time warping (DTW) was aimed at the time optimal of striding between the reference video.The parameter of DTW is next definite by using time distortion amount in the coupling medium must support (being to accelerate about 15% time or slow down for a lot of application for example).According to the output of DTW, at the individual sub-fingerprint of input video and with reference to determining pairing between the sub-fingerprint.Set to pairing is assessed, to generate the vector description of the coupling between (1104) input video and the reference video.The example of the possible clauses and subclauses in this matching vector can be:
1) Hamming distance that adds up between the one-tenth antithetical phrase fingerprint vector.
2) number percent of the sub-fingerprint that is complementary of at least 80% paired vector.
3) number percent of the sub-fingerprint that is complementary of at least 60% paired vector.
4) number percent of the sub-fingerprint that is complementary of at least 40% paired vector.
5) number percent of the sub-fingerprint that is complementary of at least 20% paired vector.
6) mean square deviation (MSE) between decoding path and minimum variance (LSE) the fitting a straight line path.
7) slope in LSE fitting a straight line decoding path.
8) with effective sub-fingerprint pairing and at least 10% in pairs to the number of flux matched effective sub-fingerprint, wherein use a certain measurement of taking during the fingerprint generative process of first front end (for example, non-NULL or non-silence) determine effectively/invalid.
9) with the number of the invalid sub-fingerprint of invalid sub-fingerprint pairing.
10) with the number of the invalid sub-fingerprint of effective sub-fingerprint pairing.
11) have with its phase one candidate list in the number of adjacent detection piece of the same video listed and approximate same offset.
12) number of the ballot that during the phase one evidence-gathering, receives of this pairing.
13) with the previous passage that checks on existence, degree of confidence and time and the skew similarity of coupling of this reference.
14) will be with reference to the appointment as high-quality.
15) generate this detection the user upload history (for example, the user had before uploaded the content from reference set).
16) if metadata can be used the similarity between those descriptions (for example anchor text) about detection and reference content both.
According to this vector description, determine (1106) mass measurement.An example of mass measurement is used the model of real coupling and the likelihood ratio between the false model that mates.The model that is fit to can be full covariance Gauss model or diagonal line variance gauss hybrid models, and other.Alternatively, by all mass measurements simply are set to 0, can skip this mass measurement step 1106.
Alternatively, in case calculated mass measurement at all phase one candidates of current block, can use overall (population) statistics to this set, with help non-description segmentation and time-distinguish between skew-responsive segmentation.This can be different carrying out between the two from the expansion of the mass measurement of phase one candidate list by hypothesis.For example, can be that non-description content has at certain distinguishing a plurality of candidate's pairing again simultaneously of (more than the threshold value) degree coupling, the time-sensitive content only have on certain axle (linearity in the path of for example decoding) but not on other single candidate of matched well right.Can describe by utilizing overall regularization clauses and subclauses expansion coupling-pairing, the support to a certain degree to the difference of the type is provided.These can comprise:
1) mass measurement of pairing (not regularization).
2) Pei Dui rank, wherein rank is used the mass measurement of the candidate matches of surveying piece.
3) average quality of surveying the candidate matches of piece is measured.
4) standard deviation of the candidate matches of detection piece.
5) the contrary standard deviation of the candidate matches of detection piece.
6) by to the average of the candidate's that surveys piece mass measurement and the standardization that standard deviation is carried out the mass measurement of pairing.
7) pairing by the average of the candidate's that surveys piece mass measurement and standard deviation being carried out regularization mutually.
Also can be set to 0 by additional entries and omit this step.
The coupling that to expand then-pairing is described as input and is offered very/the false sorter that mates pairing.Using (1108) sorter is effective or invalid coupling with candidate classification, and the degree of confidence score that coupling is provided.Sorter can be any various form, no matter also be based on linear classifier based on neural network shape structure.If this pairing is accepted in sorter indication, then this pairing is included in the tabulation that provides to the phase III with the degree of confidence score.Otherwise it is left in the basket.The output in this stage is the tabulation of effective matching candidate.
Stage 3: candidate combinations and deleting
For some contents, there are a large amount of candidates by the subordinate phase test, much comprise overlapping content.Embodiment is reduced to candidate list succinct and more credible more, but not lists each among these candidates independently or ignore the candidate that a certain length that exceeds final candidate list limits.
This following realization: collect the reference video part that is complementary with different input video pieces, and be the coupling of a combination with these component combination.For example, as shown in figure 12, the composition coupling (piece 1, piece 2, piece 3) between input video and the reference video is combined into the coupling of a combination.In order from the stage of deleting, to remove message, can impose restriction, for example:
1) at least 2 different input video pieces are the coupling of combination provide support (wherein any signal between the part of input video piece and reference video matches and only can support a combinations matches).
2) the average quality of match of combinations matches is on certain threshold value.
3) stride the input video of the piece of supporting combinations matches and the difference between the reference section less than certain threshold value.
A method of carrying out this matched packet process is greedy algorithm, and wherein the important coupling in the boundary of special time position is placed in the support list.If this total length tabulation shows that input video and the offset differences between the reference video of striding component blocks are too big, then exceptional value (in the skew) is deleted from this coupling, and consider this set again, be 2 or littler up to set length, or pass through its test up to combinations matches.If combinations matches is by its test, then all the components match flag that will provide support for combinations matches is matching content.In addition, determine that other couplings in " shade " of combinations matches comprise matching content, wherein the shade of combinations matches is to have between input video and identical reference video that the similar time is supported and between has the compatible portion of similar time migration.With an example that shade occurs be have the time that remains unchanged for a long period of time in the video and between compatible portion, have the gap in.If combinations matches not by its test, then deletes the composition coupling of the seed that is used as the combinations matches that proposes.Repeat anabolic process in (unclaimed) of the failed call that reduces tabulation then, be sky up to this tabulation.
An embodiment of terminal stage process is configured to avoid mating the general harmonic motion sequence of similar type.An example of this problem is " head of speaking " video.This track of video is clearly, and normally clearly, but multiple traditional fingerprint generation method can not and distinguish near the facial video content of the light color at center black background, black clothes.For example, the track of video of the press conference of first theme, " George " can with the similar appearance of the every other press conference that George holds (level of detail that a plurality of fingerprint generative processes are used), and the press conference that may be easy to hold with second main body " Bill " seems extraordinary image.Thus, traditional fingerprint generates the coupling that may indicate undesirably between these videos.These erroneous matching of video channel will tend to generate a lot of shades coupling, and will be not corresponding coupling between can the accompanying audio passage.So, for fear of these erroneous matching of report, do not covering two passages and having on those couplings of certain ratio threshold value of mating more than shade-support, process can reduce the degree of confidence in this coupling, perhaps, if degree of confidence may be too low, then coupling is Removed All.By this way, create and returned a spot of coupling that polylith is supported that has according to whole process.
Matching result
Final output provides the tabulation that is defined as the reference video (or identification division of reference video) that is complementary with input video (or part of input video).Should determine to be used for multiple purpose.At first, are the copies that have been in the video in the reference video set if matching module 108 is determined the input video of uploading 102, can abandon the input video of uploading 102 in order to save storage space.Secondly, input video 102 can be used for surveying for example reference video of the video content of copyright protection.Can carry out mark or from reference video set, remove these videos then.Valuably, though described system and method under urgent time restriction and/or use limited amount storer, also can be effectively and accurately detect coupling.
Additional alternate embodiment
Some parts described above has provided feature of the present invention according to algorithm and the symbolic representation of information operating.These arthmetic statements and expression are the methods that those skilled in the art use, in order to the essence of its work is conveyed to others skilled in the art in the field most effectively.These operations though functionally or logically be described, be interpreted as by computer program and realize.In addition, proof repeatedly under situation about being without loss of generality, is suitable with the arrangement of these operations as module or code devices.
Yet, should be kept in mind that all these will be associated with the physical quantity that is fit to similar term, and only be the mark that makes things convenient for that is applied to this tittle.Unless tangible special instruction is arranged in this manual in addition, be to be understood that, in instructions full text, use the discussion of term such as " processing " or " computing " or " calculating " or " determining " or " demonstration " etc., refer to that in computer system memory or register or other such information storage, transmission or display devices computer system or similar electronic computing device operation and conversion table are shown action and the process of the data of physics (electronics) amount.
Some aspect of the present invention comprises that this sentences process steps and instruction that algorithm pattern is described.Should be noted that process steps of the present invention and instruction can realize in software, firmware or hardware, and when realizing, there is the line operate of going forward side by side in the different platform that can download to use in real-time network operating system in software.
The invention still further relates to for the device of carrying out operation herein.This device can be the special structure of needed purpose, or it can comprise the multi-purpose computer that is activated alternatively or reshuffled by the computer program that is stored in the computing machine.Such computer program can be stored in the computer-readable recording medium, dish such as but not limited to any kind, comprise floppy disk, CD, CD-ROM, magneto-optic disk, ROM (read-only memory) (ROM), random-access memory (ram), EPROM, EEPROM, magnetic or optical card, special IC (ASIC), or be suitable for the medium of any kind of store electrons instruction, and each is coupled to computer system bus.In addition, the computing machine that relates in this instructions can comprise that single processor maybe can be to adopt the framework of multiprocessor design to improve calculated performance.
The algorithm of Ti Chuing and demonstration herein do not relate to any certain computer or other devices inherently.Various general-purpose systems also can prove that maybe conveniently the more special-purpose device of structure is carried out needed method step with using according to the program of instruction herein.The needed structure of multiple such system will occur in the above description.In addition, the present invention is not described with reference to any specific program language.Be appreciated that and use multiple program language to realize instruction of the present invention described herein, and any quoting of concrete syntax is provided for disclosing of exploitativeness and optimal mode of the present invention.
At last, should be noted that the language that uses in the present disclosure selects for the purpose of readability and instruction in principle, rather than select in order to define or limit subject matter.Therefore, of the present invention openly is schematically, rather than to the restriction of scope of the present invention.
Claims (32)
1. method for detection of the video content that repeats, described method comprises:
Receive the input fingerprint of expression input video;
Obtain the reference fingerprint of the reference video in the expression reference video set;
Identify the part coupling between described input fingerprint and the described reference fingerprint;
Described part based on sign is mated the tabulation that generates from the candidate segment of described reference video set, and the tabulation of described candidate segment comprises the time localization segmentation of the reference video in the described reference video set;
Use sorter to be categorized as matching candidate segmentation or non-matching candidate segment from the candidate segment of the tabulation of described candidate segment; And
Bear results based on the classification to described candidate segment, described result's sign is from the compatible portion of the reference video of described reference video set.
2. method according to claim 1, the tabulation that wherein generates from the described candidate segment of described reference video set comprises:
Based on the described part coupling of sign, determine the set of initial candidate reference video.
3. method according to claim 2 further comprises:
Obtain the set of the sub-fingerprint of input of described input fingerprint, wherein each is imported sub-fingerprint and comprises the sequence of importing sub-fingerprint keyword;
Obtain the set of the sub-fingerprint of reference of described reference video, wherein each comprises the sequence of the sub-fingerprint keyword of reference with reference to sub-fingerprint; And
Based on the coupling between the sub-fingerprint keyword of described reference and the sub-fingerprint keyword of described input, determine the set of described initial candidate reference video.
4. method according to claim 2 further comprises:
Based on the localization coupling of the time between described reference video and the described input video, determine the quality factor of each reference video; And
Based on described quality factor, determine the set of described initial candidate reference video.
5. method according to claim 2 further comprises:
According to be up to the minimum clooating sequence of difference from difference, the sub-fingerprint of the reference of representing described reference video is sorted, the maximum sub-fingerprint of reference of difference has with each of described input fingerprint imports the minimum coupling keyword of sub-fingerprint, and the minimum sub-fingerprint of reference of difference has with each of described input fingerprint and imports the maximum coupling keyword of sub-fingerprint; And
Based on the described clooating sequence of the sub-fingerprint of described reference, determine described candidate segment at least in part.
6. method according to claim 2 further comprises:
According to be up to the minimum clooating sequence of difference from difference, the reference keyword of the sub-fingerprint of reference to the expression reference video sorts, the maximum sub-fingerprint of reference of difference has with each of described input fingerprint imports the minimum coupling keyword of sub-fingerprint, and the minimum sub-fingerprint of reference of difference has with each of described input fingerprint and imports the maximum coupling keyword of sub-fingerprint; And
Based on described described clooating sequence with reference to keyword, determine described candidate segment at least in part.
7. method according to claim 2 further comprises:
Coupling continuous in time between the sub-fingerprint of determining to represent the sub-fingerprint of input video of described input video and represent described reference video of reference video; And
Based on described coupling continuous in time, determine the tabulation of described candidate segment.
8. method according to claim 1, the described candidate segment of wherein classifying comprises:
Based on known Matching Model, determine the mass measurement of the quality of match between the described reference video of indication and the described candidate segment; And
Based on described mass measurement, described candidate segment is categorized as matching candidate segmentation or non-matching candidate segment.
9. method according to claim 1 further comprises:
Receive expression from the input keyword of the part value of described input fingerprint; And
Inquiry reverse indexing table to be obtaining one or more sub-fingerprint identifiers, described one or more sub-fingerprint identifiers signs have the keyword that is complementary with described input keyword, from the time localization segmentation of the reference video of described reference video set.
10. method according to claim 9, wherein said input keyword comprise the consecutive value from the sub-fingerprint of input.
11. method according to claim 9, wherein said input keyword comprise the non-adjacent value from the sub-fingerprint of input.
12. method according to claim 9, comprise first with reference to keyword if wherein surpass the sub-fingerprint of reference of the described reference video of expression of predetermined percentage, then described first in described reverse indexing table, be identified as first order blacklist keyword with reference to keyword, and wherein indicate the identifier code and described first of described first order blacklist keyword to be stored in explicitly in the described reverse indexing table with reference to keyword.
13. method according to claim 12, if wherein the described first sub-fingerprint of described reference that does not satisfy the standard of described first order blacklist keyword and surpass the described reference video of expression of another predefine number with reference to keyword comprises described first with reference to keyword, then described first in described reverse indexing table, be identified as second level blacklist keyword with reference to keyword, and wherein indicate the identifier code and described first of described second level keyword to be stored in explicitly in the described reverse indexing table with reference to keyword.
14. method according to claim 1, the tabulation that wherein generates described candidate segment comprises:
Determine from the set of described reference video, be noted as the high-quality reference video with premium content; And
Based on determining of described high-quality reference video, generate the tabulation of described candidate segment.
15. the equipment for detection of the repetition video content comprises:
Be used for receiving the device of the input fingerprint of representing input video;
Be used for to obtain the device of reference fingerprint of the reference video of expression reference video set;
The device that is used for the part coupling between the described input fingerprint of sign and the described reference fingerprint;
Be used for mating generation from the device of the tabulation of the candidate segment of described reference video set based on the described part of sign, the tabulation of described candidate segment comprises the time localization segmentation of the reference video in the described reference video set;
Be used for to use sorter will be categorized as the device of matching candidate segmentation or non-matching candidate segment from the candidate segment of the tabulation of described candidate segment; And
For the device that bears results based on the classification to described candidate segment, described result's indication is from the compatible portion of the reference video of described reference video set.
16. equipment according to claim 15 wherein comprises for the device of generation from the tabulation of the described candidate segment of described reference video set:
Be used for determining based on the described part coupling of sign the device of the set of initial candidate reference video.
17. equipment according to claim 16 further comprises:
Be used for to obtain the device of set of the sub-fingerprint of input of described input fingerprint, wherein each is imported sub-fingerprint and comprises the sequence of importing sub-fingerprint keyword;
Be used for to obtain the device of set of the sub-fingerprint of reference of described reference video, wherein each comprises sequence with reference to sub-fingerprint keyword with reference to sub-fingerprint; And
Be used for determining based on the coupling between the sub-fingerprint keyword of described reference and the sub-fingerprint keyword of described input the device of the set of described initial candidate reference video.
18. equipment according to claim 16 further comprises:
Be used for determining based on the localization coupling of the time between described reference video and the described input video device of the quality factor of each reference video; And
Be used for determining based on described quality factor the device of the set of described initial candidate reference video.
19. equipment according to claim 16 further comprises:
Be used for according to be up to the device that the minimum clooating sequence of difference sorts to the sub-fingerprint of the reference of representing described reference video from difference, the maximum sub-fingerprint of reference of difference has with each of described input fingerprint imports the minimum coupling keyword of sub-fingerprint, and the minimum sub-fingerprint of reference of difference has with each of described input fingerprint and imports the maximum coupling keyword of sub-fingerprint; And
Be used for determining based on the described clooating sequence of the sub-fingerprint of described reference at least in part the device of described candidate segment.
20. equipment according to claim 16 further comprises:
Be used for according to be up to the device that the reference keyword of the minimum clooating sequence of difference to the sub-fingerprint of reference of expression reference video sorts from difference, the maximum sub-fingerprint of reference of difference has with each of described input fingerprint imports the minimum coupling keyword of sub-fingerprint, and the minimum sub-fingerprint of reference of difference has with each of described input fingerprint and imports the maximum coupling keyword of sub-fingerprint; And
Be used at least in part based on the described device of determining described candidate segment with reference to the described clooating sequence of keyword.
21. equipment according to claim 16 further comprises:
The device of the coupling continuous in time between the sub-fingerprint of reference video that is used for determining the sub-fingerprint of input video of the described input video of expression and representing reference video; And
Be used for determining based on described coupling continuous in time the device of the tabulation of described candidate segment.
22. equipment according to claim 15, the device that wherein is used for the described candidate segment of classification comprises:
Be used for determining based on known Matching Model the device of the mass measurement of the quality of match between the described reference video of indication and the described candidate segment; And
Be used for based on described mass measurement described candidate segment being categorized as the device of matching candidate segmentation or non-matching candidate segment.
23. equipment according to claim 15 further comprises:
Be used for receiving expression from the device of the input keyword of the part value of described input fingerprint; And
Be used for inquiry reverse indexing table obtaining the device of one or more sub-fingerprint identifiers, described one or more sub-fingerprint identifiers signs have the keyword that is complementary with described input keyword, from the time localization segmentation of the reference video of described reference video set.
24. equipment according to claim 23, wherein said input keyword comprise the consecutive value from the sub-fingerprint of input.
25. equipment according to claim 23, wherein said input keyword comprise the non-adjacent value from the sub-fingerprint of input.
26. equipment according to claim 23, comprise first with reference to keyword if wherein surpass the sub-fingerprint of described reference of the described reference video of expression of predefine number percent, then described first in described reverse indexing table, be identified as first order blacklist keyword with reference to keyword, and wherein indicate the identifier code and described first of described first order blacklist keyword to be stored in explicitly in the described reverse indexing table with reference to keyword.
27. equipment according to claim 26, if wherein the described first sub-fingerprint of described reference that does not satisfy the standard of described first order blacklist keyword and surpass the described reference video of expression of another predefine number with reference to keyword comprises described first with reference to keyword, then described first in described reverse indexing table, be identified as second level blacklist keyword with reference to keyword, and wherein indicate the identifier code and described first of described second level keyword to be stored in explicitly in the described reverse indexing table with reference to keyword.
28. equipment according to claim 15, the device that wherein is used for the tabulation of the described candidate segment of generation comprises:
Be used for determining from device described reference video set, that be noted as the high-quality reference video with premium content; And
Be used for the device based on the tabulation of the described candidate segment of definite generation of described high-quality reference video.
29. the system for detection of the repetition video content comprises:
Ingest server is used for receiving input video;
The fingerprint generation module is used for generating the input fingerprint of representing described input video;
Reference database, the reference fingerprint of the set of storage representation reference video;
Matching module, it is coupled to described fingerprint generation module and described reference database, and described matching module is used for: identify the part coupling between described input fingerprint and the described reference fingerprint; Mate generation from the tabulation of the candidate segment of the set of described reference video based on the described part of sign; Described candidate segment is categorized as coupling segmentation or non-matching segmentation; And bear results based on the classification to described candidate segment, described result's sign is from the compatible portion of the reference video of the set of described reference video.
30. system according to claim 29, wherein said reference database comprises:
The fingerprint base unit is used for the described reference fingerprint of storage, and wherein each reference fingerprint comprises the sequence of the sub-fingerprint of reference of the time localization segmentation of representing described reference video; And
The reverse indexing table unit, it stores the identifier of described reference video and with reference to the mapping between the keyword, wherein saidly comprises localization set from the value of the sub-fingerprint of described reference with reference to keyword.
31. system according to claim 30, comprise first with reference to keyword if wherein surpass the sub-fingerprint of described reference of the described reference video of expression of predefine number percent, then described first in described reverse indexing table unit, be identified as first order blacklist keyword with reference to keyword, and wherein indicate the identifier code and described first of described first order blacklist keyword to be stored in explicitly in the described reverse indexing table unit with reference to keyword.
32. system according to claim 31, if wherein the described first sub-fingerprint of described reference that does not satisfy the standard of described first order blacklist keyword and surpass the described reference video of expression of another predefine threshold value with reference to keyword comprises described first with reference to keyword, then described first in described reverse indexing table unit, be identified as second level blacklist keyword with reference to keyword, and wherein indicate the identifier code and described first of described second level keyword to be stored in explicitly in the described reverse indexing table unit with reference to keyword.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US95744607P | 2007-08-22 | 2007-08-22 | |
US60/957,446 | 2007-08-22 | ||
US12/174,366 US8238669B2 (en) | 2007-08-22 | 2008-07-16 | Detection and classification of matches between time-based media |
US12/174,366 | 2008-07-16 | ||
PCT/US2008/074105 WO2009026564A1 (en) | 2007-08-22 | 2008-08-22 | Detection and classification of matches between time-based media |
Publications (2)
Publication Number | Publication Date |
---|---|
CN101821734A CN101821734A (en) | 2010-09-01 |
CN101821734B true CN101821734B (en) | 2013-09-25 |
Family
ID=40378706
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN2008801105573A Active CN101821734B (en) | 2007-08-22 | 2008-08-22 | Detection and classification of matches between time-based media |
Country Status (7)
Country | Link |
---|---|
US (1) | US8238669B2 (en) |
EP (1) | EP2191400B1 (en) |
JP (1) | JP5479340B2 (en) |
CN (1) | CN101821734B (en) |
AU (1) | AU2008288797B2 (en) |
CA (1) | CA2696890C (en) |
WO (1) | WO2009026564A1 (en) |
Families Citing this family (134)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8205237B2 (en) | 2000-09-14 | 2012-06-19 | Cox Ingemar J | Identifying works, using a sub-linear time search, such as an approximate nearest neighbor search, for initiating a work-based action, such as an action on the internet |
US20070162761A1 (en) * | 2005-12-23 | 2007-07-12 | Davis Bruce L | Methods and Systems to Help Detect Identity Fraud |
US8738749B2 (en) | 2006-08-29 | 2014-05-27 | Digimarc Corporation | Content monitoring and host compliance evaluation |
US8707459B2 (en) | 2007-01-19 | 2014-04-22 | Digimarc Corporation | Determination of originality of content |
US8010511B2 (en) | 2006-08-29 | 2011-08-30 | Attributor Corporation | Content monitoring and compliance enforcement |
US7707224B2 (en) | 2006-11-03 | 2010-04-27 | Google Inc. | Blocking of unlicensed audio content in video files on a video hosting website |
US9179200B2 (en) | 2007-03-14 | 2015-11-03 | Digimarc Corporation | Method and system for determining content treatment |
US10242415B2 (en) * | 2006-12-20 | 2019-03-26 | Digimarc Corporation | Method and system for determining content treatment |
US9177209B2 (en) * | 2007-12-17 | 2015-11-03 | Sinoeast Concept Limited | Temporal segment based extraction and robust matching of video fingerprints |
GB2457694B (en) | 2008-02-21 | 2012-09-26 | Snell Ltd | Method of Deriving an Audio-Visual Signature |
GB2487499B (en) * | 2008-02-21 | 2013-02-27 | Snell Ltd | Method of comparing audio data |
US8184953B1 (en) * | 2008-02-22 | 2012-05-22 | Google Inc. | Selection of hash lookup keys for efficient retrieval |
US8069176B1 (en) | 2008-09-25 | 2011-11-29 | Google Inc. | LSH-based retrieval using sub-sampling |
US10334324B2 (en) | 2008-11-26 | 2019-06-25 | Free Stream Media Corp. | Relevant advertisement generation based on a user operating a client device communicatively coupled with a networked media device |
US10880340B2 (en) | 2008-11-26 | 2020-12-29 | Free Stream Media Corp. | Relevancy improvement through targeting of information based on data gathered from a networked device associated with a security sandbox of a client device |
US10567823B2 (en) | 2008-11-26 | 2020-02-18 | Free Stream Media Corp. | Relevant advertisement generation based on a user operating a client device communicatively coupled with a networked media device |
US9519772B2 (en) | 2008-11-26 | 2016-12-13 | Free Stream Media Corp. | Relevancy improvement through targeting of information based on data gathered from a networked device associated with a security sandbox of a client device |
US9986279B2 (en) | 2008-11-26 | 2018-05-29 | Free Stream Media Corp. | Discovery, access control, and communication with networked services |
US10419541B2 (en) | 2008-11-26 | 2019-09-17 | Free Stream Media Corp. | Remotely control devices over a network without authentication or registration |
US8180891B1 (en) | 2008-11-26 | 2012-05-15 | Free Stream Media Corp. | Discovery, access control, and communication with networked services from within a security sandbox |
US9154942B2 (en) | 2008-11-26 | 2015-10-06 | Free Stream Media Corp. | Zero configuration communication between a browser and a networked media device |
US10631068B2 (en) | 2008-11-26 | 2020-04-21 | Free Stream Media Corp. | Content exposure attribution based on renderings of related content across multiple devices |
US9026668B2 (en) | 2012-05-26 | 2015-05-05 | Free Stream Media Corp. | Real-time and retargeted advertising on multiple screens of a user watching television |
US9386356B2 (en) | 2008-11-26 | 2016-07-05 | Free Stream Media Corp. | Targeting with television audience data across multiple screens |
US9961388B2 (en) | 2008-11-26 | 2018-05-01 | David Harrison | Exposure of public internet protocol addresses in an advertising exchange server to improve relevancy of advertisements |
US10977693B2 (en) | 2008-11-26 | 2021-04-13 | Free Stream Media Corp. | Association of content identifier of audio-visual data with additional data through capture infrastructure |
US20100165123A1 (en) * | 2008-12-29 | 2010-07-01 | Microsoft Corporation | Data-Driven Video Stabilization |
US9633014B2 (en) * | 2009-04-08 | 2017-04-25 | Google Inc. | Policy based video content syndication |
US20100262488A1 (en) * | 2009-04-08 | 2010-10-14 | Google Inc. | Policy-based media syndication and monetization |
US10116972B2 (en) | 2009-05-29 | 2018-10-30 | Inscape Data, Inc. | Methods for identifying video segments and displaying option to view from an alternative source and/or on an alternative device |
US10375451B2 (en) | 2009-05-29 | 2019-08-06 | Inscape Data, Inc. | Detection of common media segments |
US8595781B2 (en) | 2009-05-29 | 2013-11-26 | Cognitive Media Networks, Inc. | Methods for identifying video segments and displaying contextual targeted content on a connected television |
US9094714B2 (en) | 2009-05-29 | 2015-07-28 | Cognitive Networks, Inc. | Systems and methods for on-screen graphics detection |
US9449090B2 (en) | 2009-05-29 | 2016-09-20 | Vizio Inscape Technologies, Llc | Systems and methods for addressing a media database using distance associative hashing |
US10949458B2 (en) | 2009-05-29 | 2021-03-16 | Inscape Data, Inc. | System and method for improving work load management in ACR television monitoring system |
AU2010255498B2 (en) * | 2009-06-04 | 2014-09-18 | Bae Systems Plc | System and method of analysing transfer of data over at least one network |
US8713068B2 (en) * | 2009-06-11 | 2014-04-29 | Yahoo! Inc. | Media identification system with fingerprint database balanced according to search loads |
US8229219B1 (en) * | 2009-08-06 | 2012-07-24 | Google Inc. | Full-length video fingerprinting |
US8290918B1 (en) | 2009-09-29 | 2012-10-16 | Google Inc. | Robust hashing of digital media data |
EP2520084A4 (en) * | 2009-12-29 | 2013-11-13 | Tv Interactive Systems Inc | Method for identifying video segments and displaying contextually targeted content on a connected television |
US8886531B2 (en) * | 2010-01-13 | 2014-11-11 | Rovi Technologies Corporation | Apparatus and method for generating an audio fingerprint and using a two-stage query |
US20110173185A1 (en) * | 2010-01-13 | 2011-07-14 | Rovi Technologies Corporation | Multi-stage lookup for rolling audio recognition |
US8625033B1 (en) * | 2010-02-01 | 2014-01-07 | Google Inc. | Large-scale matching of audio and video |
US9838753B2 (en) | 2013-12-23 | 2017-12-05 | Inscape Data, Inc. | Monitoring individual viewing of television events using tracking pixels and cookies |
US10192138B2 (en) | 2010-05-27 | 2019-01-29 | Inscape Data, Inc. | Systems and methods for reducing data density in large datasets |
US8374386B2 (en) * | 2011-01-27 | 2013-02-12 | Polytechnic Institute Of New York University | Sensor fingerprint matching in large image and video databases |
US10127578B2 (en) * | 2011-05-09 | 2018-11-13 | Capital One Services, Llc | Method and system for matching purchase transaction history to real-time location information |
US8917823B1 (en) | 2011-05-26 | 2014-12-23 | Google Inc. | Transcribing and navigating a response system |
US9092520B2 (en) * | 2011-06-20 | 2015-07-28 | Microsoft Technology Licensing, Llc | Near-duplicate video retrieval |
US9026540B1 (en) * | 2012-01-31 | 2015-05-05 | Google Inc. | Systems and methods for information match scoring |
US9165124B1 (en) | 2012-02-01 | 2015-10-20 | Convertro, Inc. | Systems and methods for identifying a returning web client |
US9684715B1 (en) * | 2012-03-08 | 2017-06-20 | Google Inc. | Audio identification using ordinal transformation |
US8838609B1 (en) | 2012-10-10 | 2014-09-16 | Google Inc. | IDF weighting of LSH bands for live reference ingestion |
US8990951B1 (en) * | 2012-03-30 | 2015-03-24 | Google Inc. | Claiming delayed live reference streams |
US20140101551A1 (en) * | 2012-10-05 | 2014-04-10 | Google Inc. | Stitching videos into an aggregate video |
CN103020140B (en) * | 2012-11-21 | 2016-01-20 | 合一网络技术(北京)有限公司 | A kind of method and apparatus Internet user being commented on to content automatic fitration |
US9146990B2 (en) * | 2013-01-07 | 2015-09-29 | Gracenote, Inc. | Search and identification of video content |
US9053121B2 (en) | 2013-01-10 | 2015-06-09 | International Business Machines Corporation | Real-time identification of data candidates for classification based compression |
US9564918B2 (en) | 2013-01-10 | 2017-02-07 | International Business Machines Corporation | Real-time reduction of CPU overhead for data compression |
US9792350B2 (en) * | 2013-01-10 | 2017-10-17 | International Business Machines Corporation | Real-time classification of data into data compression domains |
KR101456926B1 (en) * | 2013-06-14 | 2014-10-31 | (주)엔써즈 | System and method for detecting advertisement based on fingerprint |
US9236056B1 (en) * | 2013-08-13 | 2016-01-12 | Google Inc. | Variable length local sensitivity hash index |
ES2879628T3 (en) | 2013-11-08 | 2021-11-22 | Friend For Media Ltd | Identification of multimedia components |
US9955192B2 (en) | 2013-12-23 | 2018-04-24 | Inscape Data, Inc. | Monitoring individual viewing of television events using tracking pixels and cookies |
US9529840B1 (en) * | 2014-01-14 | 2016-12-27 | Google Inc. | Real-time duplicate detection of videos in a massive video sharing system |
US9619854B1 (en) * | 2014-01-21 | 2017-04-11 | Google Inc. | Fingerprint matching for recommending media content within a viewing session |
US9471663B1 (en) | 2014-01-22 | 2016-10-18 | Google Inc. | Classification of media in a media sharing system |
GB2523311B (en) * | 2014-02-17 | 2021-07-14 | Grass Valley Ltd | Method and apparatus for managing audio visual, audio or visual content |
CA2939117C (en) * | 2014-03-04 | 2022-01-18 | Interactive Intelligence Group, Inc. | Optimization of audio fingerprint search |
US10318543B1 (en) | 2014-03-20 | 2019-06-11 | Google Llc | Obtaining and enhancing metadata for content items |
NL2012567B1 (en) * | 2014-04-04 | 2016-03-08 | Teletrax B V | Method and device for generating improved fingerprints. |
US9930375B2 (en) * | 2014-06-16 | 2018-03-27 | Nexidia Inc. | Media asset management |
US9881083B2 (en) | 2014-08-14 | 2018-01-30 | Yandex Europe Ag | Method of and a system for indexing audio tracks using chromaprints |
WO2016024172A1 (en) | 2014-08-14 | 2016-02-18 | Yandex Europe Ag | Method of and a system for matching audio tracks using chromaprints with a fast candidate selection routine |
KR20160044954A (en) * | 2014-10-16 | 2016-04-26 | 삼성전자주식회사 | Method for providing information and electronic device implementing the same |
US9805099B2 (en) * | 2014-10-30 | 2017-10-31 | The Johns Hopkins University | Apparatus and method for efficient identification of code similarity |
BR112017016123A2 (en) | 2015-01-30 | 2018-04-17 | Inscape Data Inc | correspondence server for identifying video content that is displayed by a television system, computer-performed method, and computer program product concretely incorporated into a permanent machine-read storage medium of a computer device |
US9392324B1 (en) | 2015-03-30 | 2016-07-12 | Rovi Guides, Inc. | Systems and methods for identifying and storing a portion of a media asset |
CN107949849B (en) | 2015-04-17 | 2021-10-08 | 构造数据有限责任公司 | System and method for reducing data density in large data sets |
EP3323245B1 (en) | 2015-07-16 | 2021-08-25 | Inscape Data, Inc. | Detection of common media segments |
US10080062B2 (en) | 2015-07-16 | 2018-09-18 | Inscape Data, Inc. | Optimizing media fingerprint retention to improve system resource utilization |
AU2016291690B2 (en) | 2015-07-16 | 2020-08-27 | Inscape Data, Inc. | Prediction of future views of video segments to optimize system resource utilization |
AU2016291674B2 (en) | 2015-07-16 | 2021-08-26 | Inscape Data, Inc. | Systems and methods for partitioning search indexes for improved efficiency in identifying media segments |
CN105279489B (en) * | 2015-10-13 | 2018-07-13 | 成都纽捷那科技有限公司 | A kind of method for extracting video fingerprints based on sparse coding |
WO2017075493A1 (en) | 2015-10-28 | 2017-05-04 | Ustudio, Inc. | Video frame difference engine |
KR102560635B1 (en) * | 2015-12-28 | 2023-07-28 | 삼성전자주식회사 | Content recognition device and method for controlling thereof |
US9723344B1 (en) * | 2015-12-29 | 2017-08-01 | Google Inc. | Early detection of policy violating media |
US20170357654A1 (en) * | 2016-06-10 | 2017-12-14 | Google Inc. | Using audio and video matching to determine age of content |
WO2017222569A1 (en) * | 2016-06-22 | 2017-12-28 | Gracenote, Inc. | Matching audio fingerprints |
US10013614B2 (en) * | 2016-06-29 | 2018-07-03 | Google Llc | Using an image matching system to improve the quality of service of a video matching system |
RU2634211C1 (en) | 2016-07-06 | 2017-10-24 | Общество с ограниченной ответственностью "Траст" | Method and system of protocols analysis of harmful programs interaction with control centers and detection of computer attacks |
RU2649793C2 (en) | 2016-08-03 | 2018-04-04 | ООО "Группа АйБи" | Method and system of detecting remote connection when working on web resource pages |
RU2634209C1 (en) | 2016-09-19 | 2017-10-24 | Общество с ограниченной ответственностью "Группа АйБи ТДС" | System and method of autogeneration of decision rules for intrusion detection systems with feedback |
US10789623B1 (en) * | 2016-09-23 | 2020-09-29 | Zeta Global Corp. | Ad collision reduction |
US10061987B2 (en) * | 2016-11-11 | 2018-08-28 | Google Llc | Differential scoring: a high-precision scoring method for video matching |
RU2637477C1 (en) | 2016-12-29 | 2017-12-04 | Общество с ограниченной ответственностью "Траст" | System and method for detecting phishing web pages |
US10909161B2 (en) | 2016-12-29 | 2021-02-02 | Arris Enterprises Llc | System to build advertisement database from unreliable sources |
RU2671991C2 (en) | 2016-12-29 | 2018-11-08 | Общество с ограниченной ответственностью "Траст" | System and method for collecting information for detecting phishing |
US10671852B1 (en) * | 2017-03-01 | 2020-06-02 | Matroid, Inc. | Machine learning in video classification |
BR112019019430A2 (en) | 2017-04-06 | 2020-04-14 | Inscape Data Inc | computer program system, method and product |
US11074290B2 (en) * | 2017-05-03 | 2021-07-27 | Rovi Guides, Inc. | Media application for correcting names of media assets |
US9936230B1 (en) * | 2017-05-10 | 2018-04-03 | Google Llc | Methods, systems, and media for transforming fingerprints to detect unauthorized media content items |
US20180359539A1 (en) * | 2017-06-08 | 2018-12-13 | Qualcomm Incorporated | Smooth transition for content type changes in streaming content |
US10972203B2 (en) * | 2017-06-12 | 2021-04-06 | Gracenote, Inc. | Detecting and responding to rendering of interactive video content |
US10440413B2 (en) * | 2017-07-31 | 2019-10-08 | The Nielsen Company (Us), Llc | Methods and apparatus to perform media device asset qualification |
RU2689816C2 (en) | 2017-11-21 | 2019-05-29 | ООО "Группа АйБи" | Method for classifying sequence of user actions (embodiments) |
RU2677361C1 (en) | 2018-01-17 | 2019-01-16 | Общество с ограниченной ответственностью "Траст" | Method and system of decentralized identification of malware programs |
RU2677368C1 (en) | 2018-01-17 | 2019-01-16 | Общество С Ограниченной Ответственностью "Группа Айби" | Method and system for automatic determination of fuzzy duplicates of video content |
RU2676247C1 (en) | 2018-01-17 | 2018-12-26 | Общество С Ограниченной Ответственностью "Группа Айби" | Web resources clustering method and computer device |
RU2680736C1 (en) | 2018-01-17 | 2019-02-26 | Общество с ограниченной ответственностью "Группа АйБи ТДС" | Malware files in network traffic detection server and method |
RU2668710C1 (en) | 2018-01-17 | 2018-10-02 | Общество с ограниченной ответственностью "Группа АйБи ТДС" | Computing device and method for detecting malicious domain names in network traffic |
RU2681699C1 (en) | 2018-02-13 | 2019-03-12 | Общество с ограниченной ответственностью "Траст" | Method and server for searching related network resources |
US10713495B2 (en) | 2018-03-13 | 2020-07-14 | Adobe Inc. | Video signatures based on image feature extraction |
CN109189991B (en) * | 2018-08-17 | 2021-06-08 | 百度在线网络技术（北京）有限公司 | Duplicate video identification method, device, terminal and computer readable storage medium |
RU2708508C1 (en) | 2018-12-17 | 2019-12-09 | Общество с ограниченной ответственностью "Траст" | Method and a computing device for detecting suspicious users in messaging systems |
RU2701040C1 (en) | 2018-12-28 | 2019-09-24 | Общество с ограниченной ответственностью "Траст" | Method and a computer for informing on malicious web resources |
EP3842968B1 (en) | 2019-02-27 | 2024-04-24 | "Group IB" Ltd. | Method and system for identifying a user according to keystroke dynamics |
US11325044B2 (en) * | 2019-03-07 | 2022-05-10 | Sony Interactive Entertainment LLC | Video game guidance system |
CN110099302B (en) | 2019-04-29 | 2020-11-24 | 北京达佳互联信息技术有限公司 | Video grading method, device, equipment and storage medium |
US10997459B2 (en) * | 2019-05-23 | 2021-05-04 | Webkontrol, Inc. | Video content indexing and searching |
RU2728498C1 (en) | 2019-12-05 | 2020-07-29 | Общество с ограниченной ответственностью "Группа АйБи ТДС" | Method and system for determining software belonging by its source code |
RU2728497C1 (en) | 2019-12-05 | 2020-07-29 | Общество с ограниченной ответственностью "Группа АйБи ТДС" | Method and system for determining belonging of software by its machine code |
RU2743974C1 (en) | 2019-12-19 | 2021-03-01 | Общество с ограниченной ответственностью "Группа АйБи ТДС" | System and method for scanning security of elements of network architecture |
CN111339356B (en) * | 2020-02-21 | 2023-07-18 | 北京字节跳动网络技术有限公司 | Misjudgment preventing method and device in video retrieval and electronic equipment |
CN111353301B (en) * | 2020-02-24 | 2023-07-21 | 成都网安科技发展有限公司 | Auxiliary secret determination method and device |
SG10202001963TA (en) | 2020-03-04 | 2021-10-28 | Group Ib Global Private Ltd | System and method for brand protection based on the search results |
US11475090B2 (en) | 2020-07-15 | 2022-10-18 | Group-Ib Global Private Limited | Method and system for identifying clusters of affiliated web resources |
RU2743619C1 (en) | 2020-08-06 | 2021-02-20 | Общество с ограниченной ответственностью "Группа АйБи ТДС" | Method and system for generating the list of compromise indicators |
US11798577B2 (en) | 2021-03-04 | 2023-10-24 | Gracenote, Inc. | Methods and apparatus to fingerprint an audio signal |
US11947572B2 (en) | 2021-03-29 | 2024-04-02 | Group IB TDS, Ltd | Method and system for clustering executable files |
US11860977B1 (en) * | 2021-05-04 | 2024-01-02 | Amazon Technologies, Inc. | Hierarchical graph neural networks for visual clustering |
CN113421241B (en) * | 2021-06-23 | 2023-08-18 | 平安国际智慧城市科技股份有限公司 | Abnormal event reporting method and device, computer equipment and storage medium |
US11417099B1 (en) * | 2021-11-08 | 2022-08-16 | 9219-1568 Quebec Inc. | System and method for digital fingerprinting of media content |
US11599856B1 (en) | 2022-01-24 | 2023-03-07 | My Job Matcher, Inc. | Apparatuses and methods for parsing and comparing video resume duplications |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1758331A (en) * | 2005-10-31 | 2006-04-12 | 浙江大学 | Quick audio-frequency separating method based on tonic frequency |
CN1842151A (en) * | 2005-03-30 | 2006-10-04 | 株式会社东芝 | Information processing apparatus and method |
Family Cites Families (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH03291752A (en) * | 1990-04-10 | 1991-12-20 | Matsushita Electric Ind Co Ltd | Data retrieving device |
US5903454A (en) * | 1991-12-23 | 1999-05-11 | Hoffberg; Linda Irene | Human-factored interface corporating adaptive pattern recognition based controller apparatus |
US5973723A (en) * | 1997-12-12 | 1999-10-26 | Deluca; Michael Joseph | Selective commercial detector and eliminator apparatus and method |
US6990453B2 (en) * | 2000-07-31 | 2006-01-24 | Landmark Digital Services Llc | System and methods for recognizing sound and music signals in high noise and distortion |
US7043473B1 (en) * | 2000-11-22 | 2006-05-09 | Widevine Technologies, Inc. | Media tracking system and method |
CN100426861C (en) * | 2002-07-01 | 2008-10-15 | 微软公司 | A system and method for providing user control over repeating objects embedded in a stream |
WO2005050620A1 (en) * | 2003-11-18 | 2005-06-02 | Koninklijke Philips Electronics N.V. | Matching data objects by matching derived fingerprints |
US7986913B2 (en) * | 2004-02-19 | 2011-07-26 | Landmark Digital Services, Llc | Method and apparatus for identificaton of broadcast source |
US20050198006A1 (en) * | 2004-02-24 | 2005-09-08 | Dna13 Inc. | System and method for real-time media searching and alerting |
CN100485574C (en) * | 2004-05-28 | 2009-05-06 | 皇家飞利浦电子股份有限公司 | Method and apparatus for content item signature matching |
US20060271947A1 (en) * | 2005-05-23 | 2006-11-30 | Lienhart Rainer W | Creating fingerprints |
JP2009524273A (en) * | 2005-11-29 | 2009-06-25 | グーグル・インコーポレーテッド | Repetitive content detection in broadcast media |
-
2008
- 2008-07-16 US US12/174,366 patent/US8238669B2/en active Active
- 2008-08-22 WO PCT/US2008/074105 patent/WO2009026564A1/en active Application Filing
- 2008-08-22 AU AU2008288797A patent/AU2008288797B2/en active Active
- 2008-08-22 EP EP08798558.6A patent/EP2191400B1/en active Active
- 2008-08-22 JP JP2010522089A patent/JP5479340B2/en active Active
- 2008-08-22 CA CA2696890A patent/CA2696890C/en active Active
- 2008-08-22 CN CN2008801105573A patent/CN101821734B/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1842151A (en) * | 2005-03-30 | 2006-10-04 | 株式会社东芝 | Information processing apparatus and method |
CN1758331A (en) * | 2005-10-31 | 2006-04-12 | 浙江大学 | Quick audio-frequency separating method based on tonic frequency |
Also Published As
Publication number | Publication date |
---|---|
CN101821734A (en) | 2010-09-01 |
AU2008288797A1 (en) | 2009-02-26 |
CA2696890A1 (en) | 2009-02-26 |
AU2008288797B2 (en) | 2013-04-18 |
EP2191400A1 (en) | 2010-06-02 |
WO2009026564A1 (en) | 2009-02-26 |
AU2008288797A2 (en) | 2010-04-01 |
US8238669B2 (en) | 2012-08-07 |
US20090052784A1 (en) | 2009-02-26 |
EP2191400A4 (en) | 2013-01-02 |
JP2010537585A (en) | 2010-12-02 |
EP2191400B1 (en) | 2018-10-03 |
JP5479340B2 (en) | 2014-04-23 |
CA2696890C (en) | 2016-05-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN101821734B (en) | Detection and classification of matches between time-based media | |
Wei et al. | Frame fusion for video copy detection | |
US8719884B2 (en) | Video identification and search | |
CN102799605B (en) | A kind of advertisement detecting method and system | |
Saba et al. | Analysis of vision based systems to detect real time goal events in soccer videos | |
US20100182401A1 (en) | System and method for managing digital videos using video features | |
CN103617234A (en) | Device and method for active video concentration | |
CN102682024B (en) | Method for recombining incomplete JPEG file fragmentation | |
US8175392B2 (en) | Time segment representative feature vector generation device | |
Awad et al. | Content-based video copy detection benchmarking at TRECVID | |
Nandzik et al. | CONTENTUS—technologies for next generation multimedia libraries: Automatic multimedia processing for semantic search | |
Kannao et al. | Segmenting with style: detecting program and story boundaries in TV news broadcast videos | |
CN111738042A (en) | Identification method, device and storage medium | |
Manson et al. | Automatic TV broadcast structuring | |
Ibrahim et al. | Tv stream structuring | |
Li et al. | A confidence based recognition system for TV commercial extraction | |
Yang et al. | Multiscale video sequence matching for near-duplicate detection and retrieval | |
Chenot et al. | A large-scale audio and video fingerprints-generated database of tv repeated contents | |
CN116340551A (en) | Similar content determining method and device | |
Belkhatir et al. | Near-duplicate video detection featuring coupled temporal and perceptual visual structures and logical inference based matching | |
Herley | Accurate repeat finding and object skipping using fingerprints | |
Bailer et al. | A distance measure for repeated takes of one scene | |
Kundu et al. | A survey on video segmentation the future roadmap | |
Khoenkaw et al. | Video similarity measurement using spectrogram | |
Min et al. | Near-duplicate video detection using temporal patterns of semantic concepts |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
C14 | Grant of patent or utility model | ||
GR01 | Patent grant | ||
CP01 | Change in the name or title of a patent holder | ||
CP01 | Change in the name or title of a patent holder |
Address after: American CaliforniaPatentee after: Google limited liability companyAddress before: American CaliforniaPatentee before: Google Inc. |