CN113811893A - Connection weight learning for guided architecture evolution - Google Patents
Connection weight learning for guided architecture evolution Download PDFInfo
- Publication number
- CN113811893A CN113811893A CN202080034659.2A CN202080034659A CN113811893A CN 113811893 A CN113811893 A CN 113811893A CN 202080034659 A CN202080034659 A CN 202080034659A CN 113811893 A CN113811893 A CN 113811893A
- Authority
- CN
- China
- Prior art keywords
- neural network
- block
- network architecture
- architecture
- blocks
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/086—Learning methods using evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/20—Processor architectures; Processor configuration, e.g. pipelining
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T3/00—Geometric image transformation in the plane of the image
- G06T3/40—Scaling the whole image or part thereof
- G06T3/4046—Scaling the whole image or part thereof using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/207—Analysis of motion for motion estimation over a hierarchy of resolutions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/776—Validation; Performance evaluation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for determining one or more neural network architectures for neural networks used to perform video processing neural network tasks. In one aspect, a method comprises: at each of a plurality of iterations: selecting a parent neural network architecture from the set of neural network architectures; training a neural network having a parent neural network architecture to perform a video processing neural network task, including determining training values for connection weight parameters of the parent neural network architecture; generating a new neural network architecture based at least in part on the training values of the connection weight parameters of the parent neural network architecture; and adding the new neural network architecture to the set of neural network architectures.
Description
Cross Reference to Related Applications
This application is an international application and claims the benefit of U.S. application No.62/852,217 filed on 23/5/2019. The disclosures of the above applications are incorporated herein by reference in their entirety.
Background
This specification relates to processing data using machine learning models.
The machine learning model receives input and generates output, e.g., predicted output, based on the received input. Some machine learning models are parametric models, and generate an output based on the received inputs and values of the model parameters.
Some machine learning models are depth models that employ a multi-layer model to generate output for received input. For example, a deep neural network is a deep machine learning model that includes an output layer and one or more hidden layers, each applying a non-linear transformation to a received input to generate an output.
Disclosure of Invention
This specification describes a system for determining a neural network architecture for performing a machine learning task, the system being implemented as a computer program on one or more computers in one or more locations.
According to a first aspect, there is provided a method performed by one or more data processing apparatus for determining a neural network architecture of a neural network for performing a video processing neural network task, the method comprising: maintaining data defining a set of neural network architectures, wherein for each neural network architecture: the neural network architecture includes a plurality of blocks, wherein each block is a space-time convolution block that includes one or more neural network layers configured to process a block input to generate a block output; and for each of one or more given blocks: (i) the block input of a given block comprises a block output from each of a plurality of other blocks, (ii) the given block has a respective connection weight parameter corresponding to each of the plurality of other blocks, (iii) processing the block input comprises combining the other block outputs using the connection weight parameters corresponding to the other blocks; at each of a plurality of iterations: selecting a parent neural network architecture from the set of neural network architectures; training a neural network having the parent neural network architecture to perform a video processing neural network task, including determining training values for connection weight parameters of the parent neural network architecture; generating a new neural network architecture based at least in part on the training values of the connection weight parameters of the parent neural network architecture; and adding the new neural network architecture to the set of neural network architectures; and selecting, after a final iteration of the plurality of iterations, a final neural network architecture from the set of neural network architectures based on performance metrics of the final neural network architecture on the video processing neural network task.
In some embodiments, each neural network architecture is configured to process an input comprising (i) a plurality of video frames, and/or (ii) a plurality of optical flow frames corresponding to the plurality of video frames.
In some embodiments, each block processes the block input at a respective time resolution to generate a block output having a respective number of channels.
In some embodiments, each block includes one or more expanded temporal convolutional layers having a temporal expansion rate corresponding to the temporal resolution of the block.
In some embodiments, each neural network architecture includes blocks with different time resolutions.
In some implementations, each block includes one or more residual modules.
In some embodiments, combining the other block outputs using the connection weight parameters corresponding to the other blocks comprises: for each other block output, scaling the other block output by the connection weight parameter corresponding to the other block; and generating a combined input by summing the scaled other block outputs.
In some embodiments, processing the block input further comprises processing the combined input according to a plurality of block parameters to generate the block output.
In some embodiments, generating the new neural network architecture based at least in part on the training values of the connection weight parameters of the parent neural network architecture comprises: determining which blocks in the new neural network architecture should receive block outputs from which other blocks in the new neural network architecture based at least in part on training values of connection weight parameters of the parent neural network architecture.
In some embodiments, determining which blocks in a new neural network architecture should receive block outputs from which other blocks in the new neural network architecture based at least in part on training values of connection weight parameters of a parent neural network architecture comprises: for each given block in the parent neural network architecture that (i) receives block outputs from other blocks and (ii) has connection weight parameters corresponding to other blocks having training values that exceed a threshold, it is determined that the block in the new neural network architecture that corresponds to the given block should receive block outputs from the blocks in the new neural network architecture that correspond to the other blocks.
In some embodiments, the threshold is a predetermined threshold.
In some embodiments, the threshold is sampled according to a predetermined probability distribution.
In some embodiments, the method further includes, for each of one or more pairs of blocks in the new neural network architecture that includes the first block and the second block, randomly determining whether the second block should receive a block output from the first block.
In some embodiments, wherein generating the new neural network architecture comprises: for each of one or more blocks in the new neural network architecture that correspond to a respective block in the parent neural network architecture, applying one or more mutation operations to the block, wherein the mutation operations comprise: splitting blocks, merging blocks with different blocks, and adjusting the temporal resolution of the blocks.
In some embodiments, selecting a parent neural network architecture from the set of neural network architectures comprises: for each of a plurality of particular neural network architectures from a set of neural network architectures, determining a performance metric for a neural network having the particular neural network architecture trained to perform video processing neural network tasks; and selecting a parent neural network architecture from the plurality of particular neural network architectures based on the performance metric.
In some embodiments, selecting a parent neural network architecture from a plurality of specific neural network architectures based on the performance metrics comprises: the parent neural network architecture is selected as the particular neural network architecture with the highest performance metric on the video processing neural network task.
In some embodiments, the method further comprises removing a particular neural network architecture from the set of neural network architectures that has the lowest performance metric on the video processing neural network task.
In some embodiments, selecting the final neural network architecture from the set of neural network architectures comprises: for each neural network architecture from the set of neural network architectures, determining a performance metric for a neural network having the neural network architecture trained to perform the video processing neural network task; and selecting the final neural network architecture as the neural network architecture having the highest performance metric over the video processing neural network task.
In some embodiments, the method further includes providing a neural network that (i) has a final neural network architecture, and (ii) has been trained to perform a video processing neural network task.
In some embodiments, for each neural network architecture: each block in the neural network architecture is associated with a respective level in the sequence of levels; and for each given block associated with a given level subsequent to the first level in the sequence of levels, the given block receives block outputs only from other blocks associated with levels prior to the given level.
According to another aspect, a system is provided that includes one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the operations of the aforementioned method.
According to another aspect, one or more computer storage media are provided that store instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the aforementioned methods.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
The system described in this specification is capable of automatically selecting a neural network architecture that may enable a neural network having the architecture to efficiently perform machine learning tasks, such as video processing tasks. The system may select an architecture from a space of possible architectures, each architecture including a plurality of neural network layer blocks (e.g., spatio-temporal convolutional layers) that process block inputs that may be derived at respective temporal resolutions from different input streams (e.g., streams of video frames or optical streams). As part of the selection architecture, the system selects how the blocks in the architecture are connected to each other, i.e., which blocks receive input from which other blocks, thereby enabling control of how data flows through the architecture, and when and where features encoding different information at various levels of abstraction are combined together. In contrast, some conventional architecture selection systems select an architecture from a less flexible space of possible architectures, e.g., each architecture includes a space of the same neural network layer module that is sequentially repeated multiple times. By searching for a complex and flexible space of possible architectures, the system described in this specification can select an architecture that enables a neural network with that architecture to perform machine learning tasks more efficiently, e.g., with higher prediction accuracy.
To select an architecture, the system evolves (i.e., updates) a population (i.e., set) of neural network architectures in multiple evolutionary iterations. In each evolutionary iteration, the system can generate a new neural network architecture from an existing "parent" neural network architecture based on training values for connection weight parameters that define the strength of connections between blocks in the parent neural network architecture. For example, the system can determine that a strongly connected block in the parent architecture should still be connected in the new architecture, while connections between other blocks may be randomly reconfigured in the new architecture. Directing the evolution of the population of neural network architectures based on the training values of the connection weight parameters may enable the system to select an architecture that achieves acceptable performance with fewer iterations of evolution on the machine learning task, thereby reducing the consumption of computing resources, e.g., memory and computing power.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 shows an example of a possible neural network architecture for performing video processing tasks generated by the architecture selection system.
FIG. 2 illustrates an example architecture selection system.
FIG. 3 is a flow diagram of an example process for determining a neural network architecture for a neural network used to perform a video processing neural network task.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The present specification describes an architecture selection system 200 for selecting an architecture for a neural network configured to perform a machine learning task.
The machine learning task may be a video processing task, i.e., a neural network processes video (or features derived from video) to generate an output that characterizes the video. "video" may refer to a sequence of video frames, where each video frame may be represented as an array of values and corresponds to a respective point in time. For example, a video frame may be represented as an array of RBG or CIELAB values.
The input to the neural network may include, for example, a sequence of video frames, a sequence of optical flow frames corresponding to the video frames, or both. Each optical flow frame may characterize motion between a respective pair of video frames, i.e., between a first video frame and a subsequent video frame. For example, each optical-flow frame may specify a respective displacement vector corresponding to each pixel in the first video frame, where the displacement vector estimates the displacement of the pixel between the first video frame and the subsequent video frame. The optical flow frames may be derived from the video frames using any of a variety of techniques (e.g., the Lucas-Kanade method).
The neural network may generate any of a variety of outputs that characterize the input video. For example, the neural network may generate a classification output that includes a respective score for each of a plurality of categories, where the score for a category defines a likelihood that the video is included in the category. In a particular example, the neural network may generate a classification output that classifies the action being performed by the person depicted in the video, and the classification output may specify a respective score for each action in the set of possible actions.
The architecture of the architecture selection system 200 for selecting a neural network from a space of possible architectures will be described in more detail with reference to fig. 2. An example architecture 100 for neural network selection by an architecture selection system 200 configured to perform video processing tasks is illustrated in fig. 1. Each possible architecture contains multiple "blocks," including one or more input blocks and one or more intermediate blocks. As used throughout this specification, a block refers to a set of neural network layers, which may be arranged in order, for example.
Each input block (e.g., input block 102) may be configured to process a sequence of video frames (e.g., video frame 104), a sequence of optical stream frames (e.g., optical stream frame 106), or both. Each intermediate block may be configured to process inputs including outputs of one or more other blocks (e.g., block 108 may be configured to receive inputs from blocks 102 and 110). The neural network may generate an output (e.g., output 112) by processing the output of one or more blocks (e.g., block 114) of the neural network by one or more neural network layers, e.g., a sequence of layers including a pooling layer, a fully-connected layer, and a soft-max layer.
Each block may be a spatio-temporal volume block, i.e., a block that includes one or more convolutional neural network layers and is configured to process a spatio-temporal input to generate a spatio-temporal output. An ordered set of spatio-temporal data index values, e.g., a tensor of values, comprising a plurality of spatial dimensions, temporal dimensions, and optionally, channel dimensions. Each block may generate an output having a corresponding number of channels, and in the neural network illustrated in fig. 1, the number of channels in the output of each block is denoted by C, e.g., where block 102 generates a block output having C ═ 32 channels.
Each block may include, for example, a spatial convolution layer (i.e., having convolution kernels defined in a spatial dimension), a spatio-temporal convolution layer (i.e., having convolution kernels defined across spatial and temporal dimensions), and a temporal convolution layer (i.e., having convolution kernels defined in a temporal dimension).
The time convolutional layer may be a "scaled" time convolutional layer, i.e., the layer output is generated by convolving the layer input with a kernel defined in the time dimension, where the convolution skips the input (i.e., along the time dimension) according to a step size called the expansion rate. The dilated temporal convolution layer can be said to process the spatio-temporal input with a "temporal resolution" defined by the dilation rate, e.g., such that a higher dilation rate may correspond to a lower temporal resolution. In some cases, each temporal convolution layer in a block may be configured to process spatio-temporal input at the same temporal resolution, which may be referred to as the temporal resolution of the block, and different blocks may have different temporal resolutions. In the neural network illustrated with reference to fig. 1, the expansion rate of each block is denoted by r, for example, where the expansion rate r of the block 102 is 4. The temporal resolution of a block may affect the nature of the information encoded in the output generated by that block, and blocks with different temporal resolutions may generate block outputs encoding complementary information, which may improve the performance of the neural network.
The spatial convolution layer may be a "strided" convolution layer, i.e., a layer output generated by convolving a layer input with a kernel defined in a spatial dimension, where the convolution window traverses the input (i.e., along the spatial dimension) according to a step size called a stride.
In some embodiments, the neural network layers in each block may be arranged into "residual modules". Each residual module includes one or more neural network layers, and the output of the residual module may be the sum of: (i) an input of a first layer of the residual module, and (ii) an output of a last layer of the residual module. The use of the residual module may stabilize the training of the neural network and thereby improve the performance of the trained neural network.
A given block configured to receive input from other blocks may maintain a respective "connection weight" parameter corresponding to each of these other blocks. A given block may combine inputs from other blocks in accordance with the connection weight parameters to generate a combined input, which is then processed (e.g., by one or more convolutional neural network layers) to generate a block output. A given block may generate a combined input FinFor example, the following:
where i indexes n other blocks, w, providing input to a given blockiRepresents a connection weight parameter of the corresponding block i, and
In some implementations, each block in the neural network can be associated with a respective level, e.g., level 116 and 124, and each block in a given level receives input only from blocks of lower levels. For example, each block in level 3(120) may receive input from blocks of level 1(116) and level 2(118), but may not receive input from blocks of level 3(120), level 4(122), or level 5 (124).
As part of selecting a neural network architecture, the architecture selection system may select: the number of blocks in each level of the architecture, the temporal resolution of each block, the number of channels in the output generated by each block, and which blocks receive input from which other blocks. To this end, the architecture selection system performs an automatic search through a space of possible architectures to select an architecture that enables the neural network to efficiently perform machine learning tasks, as will be described in more detail with reference to fig. 2.
While the present specification primarily describes the architecture selection system as selecting a neural network architecture for performing a video processing task, more generally, the architecture selection system can be used to select a neural network architecture for performing any of a variety of neural network tasks. Examples of other neural network tasks include, for example: the system comprises a semantic segmentation task, an image classification task, an object detection task, an action selection task, a natural language processing task and a voice recognition task. Further, while this specification primarily describes the blocks as spatio-temporal volume blocks, more generally, the blocks may comprise any suitable type of neural network layer. Furthermore, although this specification primarily describes selecting a neural network architecture configured to process video frames and/or optical flow frames, the architecture selection system can be used to select a neural network architecture that processes any suitable type of data, for example, data comprising a sequence of point clouds (e.g., generated by a lidar sensor) and/or data comprising a sequence of hyperspectral images (e.g., generated by a hyperspectral sensor).
FIG. 2 illustrates an example architecture selection system 200. Architecture selection system 200 is an example of a system implemented as a computer program on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented.
The system 200 is configured to select a "final" neural network architecture 202 that enables a neural network having the architecture to efficiently perform machine learning tasks, e.g., video processing tasks, as described above. To this end, the system 200 maintains a population (i.e., set) of possible neural network architectures 204, and updates the population 204 at each of one or more iterations, which will be referred to herein as "evolution" iterations. In particular, the system 200 updates the population of architectures 204 to increase the likelihood that the population 204 includes architectures with superior (e.g., higher) performance metrics. The performance metric of an architecture may characterize the performance (e.g., prediction accuracy) of a neural network having the architecture on a machine learning task. After the final evolution iteration, the system 200 may select an architecture from the population 204 as a final architecture 202, as will be described in more detail below.
The system 200 includes a population of possible architectures 204, a selection engine 206, and an architecture generation engine 208.
The population of possible architectures 204 includes a plurality of possible architectures and is maintained by the system 200 in evolutionary iterations. Each possible architecture may be, for example, a video processing architecture comprising a plurality of space-time volume blocks, as described with reference to fig. 1.
Prior to the first evolutionary iteration, the system 200 may initialize the population 204 of architectures by randomly generating a predefined number (e.g., 20 or any other suitable number) of architectures and adding the generated architectures to the population 204.
To randomly generate the possible architectures, the system 200 may initialize the architecture with a predefined number of blocks at each of a predefined number of levels (e.g., 5 levels, or any other suitable number of levels) in the architecture. The architecture of each block (e.g., the number, type, and configuration of neural network layers within the block) may be randomly selected from a predefined set of possible block architectures. Each possible block architecture may include an expanded time convolutional neural network layer with an expansion rate from a set of possible expansion rates, e.g., r ∈ {1,2,4,8 }. Each possible block architecture may include layers of a strided convolutional neural network with strides from a set of possible strides, e.g., s e {1,2,4,8 }. Each possible block architecture may be configured to generate a block output having multiple channels from a set of possible channel dimensions, e.g., C ∈ {32,64,128,256 }.
After initializing the blocks of each layer in the randomly generated neural network architecture, the system 200 may apply a random number of "splitting" or "merging" operations to the randomly selected blocks in the architecture. Applying a split operation to a block may refer to replacing the block with two new blocks, for example, where each new block may be configured to generate an output that is only half of the channel of the original block, but has the same temporal resolution as the original block. Applying a merge operation to a pair of blocks may refer to replacing the pair of blocks with a single new block. The new block may be configured to generate an output having a number of channels equal to the sum of the respective numbers of channels in the output of each of the original blocks in the merged pair of blocks. The temporal resolution of the new block may be randomly selected from among the temporal resolutions of each of the original blocks.
As part of randomly generating the possible architectures, the system 200 also determines which blocks should provide input to which other blocks, i.e., the system determines which blocks should be "connected". As used throughout this specification, a first block may be said to be connected to a second block if the second block is configured to receive input from the first block. The system 200 may determine which blocks should be connected in a randomly generated fabric, for example, by adding each possible connection to the fabric with a predefined probability, e.g., p 0.5. Connections between blocks are said to be "possible" only if it is specified that a higher level block should receive input from a lower level block.
In each evolutionary iteration, the selection engine 206 selects a "parent" neural network architecture from the population of architectures 204. To select the parent architecture 210, the selection engine 206 may randomly sample a predefined number of "candidate" architectures from the population of architectures 204 and determine a respective performance metric for each candidate architecture.
To determine performance metrics for the candidate architectures, the selection engine 206 may train the neural network with the candidate architecture to perform a machine learning task by training the neural network on a set of training data to determine training values for neural network parameters. In particular, the selection engine trains the neural network to determine training values for connection weight parameters corresponding to connections between blocks in the candidate architecture, and to determine training values for other neural network parameters, e.g., parameters of the neural network layer within each block.
The training data set may include a plurality of training examples, where each training example specifies: (i) a training input to the neural network, and (ii) a target output that should be generated by the neural network by processing the training input. For example, each training example may include a training input specifying a sequence of video frames and/or a corresponding sequence of optical flow frames, and a target classification output indicating, for example, an action being performed by a person depicted in the video frames. The selection engine 206 may train the neural network using any suitable machine learning training technique, such as stochastic gradient descent, where the gradient of the objective function is propagated back through the neural network at each of one or more training iterations. The objective function may be, for example, a cross-entropy objective function or any other suitable objective function.
It will be appreciated that by appropriate selection of training data and/or loss functions, the neural network can be trained for video processing tasks other than classification tasks. For example, using a training set comprising a downsampled video and a corresponding higher resolution real video, the neural network can be trained for super-resolution (in the spatial and/or temporal domain) using a loss function, e.g., L1 or L2, that compares the output of the neural network with the higher resolution real video corresponding to the downsampled video input to the neural network. As a further example, the neural network can be trained to remove one or more types of image/video artifacts from the video, such as block artifacts that may be introduced during video encoding. In this example, the training data set may comprise a set of real videos, each having one or more respective "degraded" videos (i.e., having one or more types of artifacts introduced), and a loss function that relates the output of the neural network to the real videos corresponding to the degraded video input of the neural network, e.g., L1 or L2.
After training the neural network with the candidate architecture on the training data set, the selection engine 206 may determine performance metrics for the candidate architecture by evaluating the performance of the trained neural network on the validation data set. The validation data may also include a plurality of training examples, as described above, but is typically separate from the training data, i.e., such that the neural network is not trained on the validation data. The selection engine 206 may use an objective function, e.g., a cross-entropy objective function, that measures the prediction accuracy of the trained neural network to evaluate the performance of the trained neural network on the validation data set. The selection engine 206 may determine a performance metric for the candidate architecture, e.g., as an average of objective function values across the validation data set when training inputs of the validation data are processed using the trained neural network with the candidate architecture.
The selection engine 206 may identify a parent architecture 210 for the current evolutionary iteration based on performance metrics of candidate architectures selected from the population of architectures 204 in the current evolutionary iteration. For example, the selection engine 206 may identify the candidate architecture having the "best" (e.g., highest) performance metric as the parent architecture 210 for the current evolutionary iteration. In addition, the selection engine 206 may also remove one or more candidate schemas from the population of schemas 204. For example, the selection engine 206 may remove the candidate architecture having the "worst" (e.g., lowest) performance metric from the population of architectures 204.
In some cases, some or all of the candidate architectures may have been trained to perform machine learning tasks at previous evolutionary iterations, and the architecture selection system 200 may have stored previously generated performance metrics for these architectures. In these cases, the selection engine 206 may reuse the performance metrics previously generated for these architectures, rather than generating them again at each iteration of the evolution.
In each evolutionary iteration, architecture generation engine 208 is configured to generate a "new" neural network architecture based on both: (i) parent architecture 210, and (ii) a training value for connection weight parameter 212 of parent architecture 210. In particular, architecture generation engine 208 determines which blocks in the new architecture should be connected (i.e., which blocks should receive input from which other blocks) based at least in part on the training values of connection weight parameters 212 of parent architecture 210.
For example, the schema generation engine 208 may initialize the new schema 214 by generating a copy of the parent schema 210 and setting the new schema 214 equal to the copy of the parent schema 210. Architecture generation engine 208 may then modify the connections between blocks of new architecture 214 based on connection weight parameter values 212 of parent architecture 210. For example, the fabric generation engine 208 may prune (remove) each connection from the new fabric 214 corresponding to a connection weight parameter value 212 below a threshold. That is, the fabric generation engine 208 may only maintain connections in the new fabric 214 that correspond to connection weight parameter values 212 that exceed the threshold while removing other connections. The threshold may be a predefined threshold (e.g., 0.5) or a threshold dynamically determined for each connection by sampling from a predefined probability distribution, e.g., a uniform probability distribution over the interval [0,1 ].
The architecture generation engine 208 may also add new connections to the new architecture 214. For example, for each possible connection that is not included in parent architecture 210, architecture generation engine 208 may add the possible connection to the new architecture with a predefined probability. The predefined probability may be a ratio of: (i) the number of connections pruned from the new architecture, and (ii) the number of possible edges not included in parent architecture 210. Adding new connections to new architecture 214 with such probabilities may result in new architecture 214 having, on average, the same total number of connections as parent architecture 210.
After modifying the connections between the blocks of the new architecture 214, the architecture generation engine 208 may apply one or more "mutation" operations to the randomly selected blocks of the new architecture 214. The mutation operation may include, for example: block splitting operations, block merging operations, and operations that change the expansion rate of the time convolution layer in a block. When a split operation is applied to a chunk to generate two new chunks, architecture generation engine 208 may determine that each new chunk has the same connection as the original chunk. That is, each new block may be configured to receive input from the same source as the original block and provide its output to the same destination as the original block. When applying a merge operation on a pair of blocks, architecture generation engine 208 may determine that the new block has all connections of the two original blocks. That is, the new block may be configured to receive input from the same source as each original block and provide its output to the same destination as each original block. For example, the architecture generation engine 208 may change the expansion rate of the time convolution layers in the block by randomly sampling the new expansion rate of the time convolution layers in the block from a set of possible time expansion rates.
The system 200 may add the new architecture 214 generated by the architecture generation engine 208 to the population of architectures 204 and then proceed to the next iteration of the evolution.
Generating a new architecture 214 based on the learned connection weight parameter values 212 of the parent architecture 210 may enable the system 200 to generate the new architecture 214, the new architecture 214 maintaining some properties that make the parent architecture 210 effective in performing machine learning tasks. At the same time, new connections are randomly added to the new architecture 214, enabling the system 200 to explore changes to the parent architecture 210 that may be more effective in performing machine learning tasks. Directing the evolution of the architecture population based on the training values of the connection weight parameters may increase the likelihood that the system generates new architectures 214 with better (e.g., higher) performance metrics.
After the final evolutionary iteration, the system 200 may select one or more architectures from the population of architectures 204 as the final architecture 202. For example, the system 200 may identify the final architecture 202 as the architecture having the best (e.g., highest) performance metric among the population of architectures 204. As another example, the system 200 may identify a predefined number of architectures of the population of architectures 204 having the best performance metrics as final architectures.
The final architecture(s) 202 can be used in any of a variety of ways. For example, a neural network with the final architecture 202 may be deployed to perform machine learning tasks. As another example, the final architecture(s) 202 may be used to initialize subsequent architecture refinement processes, e.g., modify the final architecture(s) 202 to determine other architectures for performing machine learning tasks. As another example, multiple final architectures may be combined to form an integrated model for performing a machine learning task, e.g., such that the output of each final architecture is combined (e.g., averaged) to determine an integrated output.
FIG. 3 is a flow diagram of an example process 300 for determining a neural network architecture for a neural network used to perform a video processing neural network task. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, the architecture selection system 200 of FIG. 2, appropriately programmed according to the present description, is capable of performing the process 300.
The system maintains data defining a set of neural network architectures (302). Each neural network architecture includes a plurality of blocks, where each block is a spatio-temporal volume block that includes one or more neural network layers configured to process block inputs to generate block outputs. For each of one or more given blocks: (i) the block input of a given block comprises a block output from each of one or more other blocks, (ii) the given block has a respective connection weight parameter corresponding to each of the one or more other blocks, and (iii) processing the block input comprises combining the other block outputs using the connection weight parameters corresponding to the other blocks.
The system performs step 304 in each of one or more evolution iterations 310.
The system selects a parent neural network architecture from a set of neural network architectures (304).
The system trains a neural network having a parent neural network architecture to perform a video processing neural network task, including determining training values for connection weight parameters of the parent neural network architecture (306).
The system generates a new neural network architecture based at least in part on the training values of the connection weight parameters of the parent neural network architecture (308).
The system adds the new neural network architecture to the set of neural network architectures (310).
If the current evolution iteration is not the final evolution iteration, the system returns to step 304. If the current evolutionary iteration is a final evolutionary iteration, the system selects a final neural network architecture from the set of neural network architectures based on performance metrics of the final neural network architecture on the video processing neural network task (312).
The term "configured" is used herein in connection with system and computer program components. A system configured to perform a particular operation or action with respect to one or more computers means that the system has installed thereon software, firmware, hardware, or a combination thereof that, when executed, causes the system to perform the operation or action. By one or more computer programs configured to perform certain operations or actions is meant that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, tangibly embodied computer software or firmware, computer hardware, structures comprising the structures disclosed in this specification and their structural equivalents, or combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and includes various apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further comprise, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software, a software application, an application, a module, a software module, a script, or code, can be written in any form of programmed language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other suitable unit for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that contains other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC.
A computer suitable for executing a computer program can be based on a general purpose or special purpose microprocessor or both, or any other type of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such a device. Moreover, the computer can be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game player, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a Universal Serial Bus (USB) flash drive, to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; as well as CDROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices can be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, the computer is able to interact with the user by sending and receiving documents to and from the device used by the user; for example, a Web page is sent to a Web browser on a user device in response to a request received from the Web browser. In addition, computers can interact with users by sending text messages or other forms of messages to personal devices, such as smartphones running messaging applications, and then receiving response messages from the users.
The data processing apparatus for implementing the machine learning model can also include, for example, a dedicated hardware accelerator unit for handling general and computationally intensive parts of machine learning training or production, i.e., reasoning, workload.
The machine learning model can be implemented and deployed using a machine learning framework, such as a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer, a Web browser, or an application having a graphical user interface through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), e.g., the internet.
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, e.g., HTML pages, to the user device, e.g., for the purpose of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., results of the user interaction, can be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings and described in the claims below in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated into a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (22)
1. A method performed by one or more data processing apparatus for determining a neural network architecture for a neural network used to perform a video processing neural network task, the method comprising:
maintaining data defining a set of neural network architectures, wherein for each neural network architecture:
the neural network architecture comprises a plurality of blocks, wherein each block is a space-time convolution block comprising one or more neural network layers, the space-time convolution block configured to process a block input to generate a block output; and
for each of one or more given blocks: (i) the block input of the given block comprises a block output from each of a plurality of other blocks, (ii) the given block has a respective connection weight parameter corresponding to each of the plurality of other blocks, and (iii) processing the block input comprises combining the other block outputs using the connection weight parameters corresponding to the other blocks;
at each of a plurality of iterations:
selecting a parent neural network architecture from the set of neural network architectures;
training a neural network having the parent neural network architecture to perform a video processing neural network task, including determining training values for connection weight parameters of the parent neural network architecture;
generating a new neural network architecture based at least in part on training values for connection weight parameters of the parent neural network architecture; and
adding the new neural network architecture to the set of neural network architectures; and
after a final iteration of the plurality of iterations, selecting a final neural network architecture from the set of neural network architectures based on performance metrics of the final neural network architecture on the video processing neural network task.
2. The method of claim 1, wherein each neural network architecture is configured to process inputs comprising: (i) a plurality of video frames, and/or (ii) a plurality of optical flow frames corresponding to the plurality of video frames.
3. A method according to any preceding claim, wherein each block processes block inputs at a respective time resolution to generate block outputs having a respective number of channels.
4. The method of claim 3, wherein each block includes one or more expanded temporal convolution layers having a temporal expansion rate corresponding to a temporal resolution of the block.
5. The method of any of claims 3-4, wherein each neural network architecture comprises blocks with different temporal resolutions.
6. The method of any preceding claim, wherein each block comprises one or more residual modules.
7. The method of any preceding claim, wherein combining the other block outputs using the connection weight parameters corresponding to the other blocks comprises:
for each other block output, scaling the other block output by the connection weight parameter corresponding to the other block; and
the combined input is generated by summing the scaled other block outputs.
8. The method of claim 7, wherein processing the block input further comprises:
processing the combined input according to a plurality of block parameters to generate the block output.
9. The method of any preceding claim, wherein generating a new neural network architecture based at least in part on the training values of the connection weight parameters of the parent neural network architecture comprises:
determining which blocks in the new neural network architecture should receive block outputs from which other blocks in the new neural network architecture based at least in part on training values of connection weight parameters of the parent neural network architecture.
10. The method of claim 9, wherein determining which blocks in the new neural network architecture should receive block outputs from which other blocks in the new neural network architecture based at least in part on training values of connection weight parameters of the parent neural network architecture comprises:
for each given block in the parent neural network architecture that (i) receives block outputs from other blocks and (ii) has connection weight parameters corresponding to the other blocks with training values that exceed a threshold, determining that a block in the new neural network architecture that corresponds to the given block should receive block outputs from blocks in the new neural network architecture that correspond to the other blocks.
11. The method of claim 10, wherein the threshold is a predetermined threshold.
12. The method of claim 10, wherein the threshold is sampled according to a predetermined probability distribution.
13. The method according to any one of claims 9-12, further including:
for each pair of one or more pairs of blocks in the new neural network architecture that includes a first block and a second block, randomly determining whether the second block should receive a block output from the first block.
14. The method of any preceding claim, wherein generating the new neural network architecture comprises:
for each of one or more blocks of the new neural network architecture that correspond to a respective block of the parent neural network architecture, applying one or more mutation operations to the block, wherein the mutation operations comprise: splitting the chunk, merging the chunk with a different chunk, and adjusting a temporal resolution of the chunk.
15. The method of any preceding claim, wherein selecting a parent neural network architecture from the set of neural network architectures comprises:
for each of a plurality of particular neural network architectures from the set of neural network architectures, determining a performance metric for a neural network having the particular neural network architecture trained to perform the video processing neural network task; and
selecting the parent neural network architecture from the plurality of particular neural network architectures based on the performance metrics.
16. The method of claim 15, wherein selecting the parent neural network architecture from the plurality of particular neural network architectures based on the performance metrics comprises:
selecting the parent neural network architecture as the particular neural network architecture having the highest performance metric on the video processing neural network task.
17. The method according to any one of claims 15-16, further comprising:
removing a particular neural network architecture from the set of neural network architectures that has a lowest performance metric on the video processing neural network task.
18. The method of any preceding claim, wherein selecting the final neural network architecture from the set of neural network architectures comprises:
for each neural network architecture from the set of neural network architectures, determining a performance metric for a neural network having the neural network architecture trained to perform the video processing neural network task; and
selecting the final neural network architecture as the neural network architecture having the highest performance metric over the video processing neural network task.
19. The method of any preceding claim, further comprising:
providing a neural network that (i) has the final neural network architecture, and (ii) has been trained to perform the video processing neural network task.
20. The method of any preceding claim, wherein, for each neural network architecture:
each block in the neural network architecture is associated with a respective level in a sequence of levels; and
for each given block associated with a given level following a first level in the sequence of levels, the given block receives block outputs only from other blocks associated with levels preceding the given level.
21. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations of the respective methods of any of claims 1-20.
22. One or more computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations of the respective methods of any of claims 1-20.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962852217P | 2019-05-23 | 2019-05-23 | |
US62/852,217 | 2019-05-23 | ||
PCT/US2020/034267 WO2020237168A1 (en) | 2019-05-23 | 2020-05-22 | Connection weight learning for guided architecture evolution |
Publications (1)
Publication Number | Publication Date |
---|---|
CN113811893A true CN113811893A (en) | 2021-12-17 |
Family
ID=71094819
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080034659.2A Pending CN113811893A (en) | 2019-05-23 | 2020-05-22 | Connection weight learning for guided architecture evolution |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220189154A1 (en) |
EP (1) | EP3948682A1 (en) |
CN (1) | CN113811893A (en) |
WO (1) | WO2020237168A1 (en) |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10931702B2 (en) * | 2018-04-24 | 2021-02-23 | Cyberfortress, Inc. | Vulnerability profiling based on time series analysis of data streams |
US10984245B1 (en) * | 2018-06-11 | 2021-04-20 | Facebook, Inc. | Convolutional neural network based on groupwise convolution for efficient video analysis |
US11797843B2 (en) * | 2019-03-06 | 2023-10-24 | Samsung Electronics Co., Ltd. | Hashing-based effective user modeling |
EP3924933A1 (en) * | 2019-03-21 | 2021-12-22 | Huawei Technologies Co., Ltd. | Image processor |
US11790213B2 (en) * | 2019-06-12 | 2023-10-17 | Sri International | Identifying complex events from hierarchical representation of data set features |
EP3923183A1 (en) * | 2020-06-11 | 2021-12-15 | Tata Consultancy Services Limited | Method and system for video analysis |
EP4094199A1 (en) * | 2020-07-14 | 2022-11-30 | Google LLC | Neural network models using peer-attention |
US11816987B2 (en) * | 2020-11-18 | 2023-11-14 | Nvidia Corporation | Emergency response vehicle detection for autonomous driving applications |
WO2023123380A1 (en) * | 2021-12-31 | 2023-07-06 | 深圳先进技术研究院 | Dynamic addiction neural circuit generation method and system based on weakly supervised contrastive learning |
-
2020
- 2020-05-22 WO PCT/US2020/034267 patent/WO2020237168A1/en unknown
- 2020-05-22 CN CN202080034659.2A patent/CN113811893A/en active Pending
- 2020-05-22 US US17/605,783 patent/US20220189154A1/en active Pending
- 2020-05-22 EP EP20733096.0A patent/EP3948682A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP3948682A1 (en) | 2022-02-09 |
WO2020237168A1 (en) | 2020-11-26 |
US20220189154A1 (en) | 2022-06-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11829880B2 (en) | Generating trained neural networks with increased robustness against adversarial attacks | |
US10671889B2 (en) | Committed information rate variational autoencoders | |
US20220014807A1 (en) | Method, apparatus, device and medium for generating captioning information of multimedia data | |
US11934956B2 (en) | Regularizing machine learning models | |
CN111602148B (en) | Regularized neural network architecture search | |
US20230281445A1 (en) | Population based training of neural networks | |
US20210004677A1 (en) | Data compression using jointly trained encoder, decoder, and prior neural networks | |
CN111758105A (en) | Learning data enhancement strategy | |
US11514694B2 (en) | Teaching GAN (generative adversarial networks) to generate per-pixel annotation | |
CN111279362A (en) | Capsule neural network | |
CN110427899B (en) | Video prediction method and device based on face segmentation, medium and electronic equipment | |
US20220292329A1 (en) | Neural architecture search with weight sharing | |
CN116686017A (en) | Time bottleneck attention architecture for video action recognition | |
US20230306258A1 (en) | Training video data generation neural networks using video frame embeddings | |
US11908103B2 (en) | Multi-scale-factor image super resolution with micro-structured masks | |
Huai et al. | Zerobn: Learning compact neural networks for latency-critical edge systems | |
US11948090B2 (en) | Method and apparatus for video coding | |
CN113811893A (en) | Connection weight learning for guided architecture evolution | |
CN116157804A (en) | Neural network model using peer-to-peer attention | |
CN114730380A (en) | Deep parallel training of neural networks | |
US20240005663A1 (en) | Per-clip video object segmentation using machine learning | |
US20230325658A1 (en) | Conditional output generation through data density gradient estimation | |
WO2023225340A1 (en) | Performing computer vision tasks using guiding code sequences | |
WO2023059737A1 (en) | Self-attention based neural networks for processing network inputs from multiple modalities | |
CN118043818A (en) | Self-attention-based neural network for processing network metrics from multiple modalities |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |