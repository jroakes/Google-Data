CN112789480A - Method and apparatus for navigating two or more users to meeting location - Google Patents
Method and apparatus for navigating two or more users to meeting location Download PDFInfo
- Publication number
- CN112789480A CN112789480A CN201980064401.4A CN201980064401A CN112789480A CN 112789480 A CN112789480 A CN 112789480A CN 201980064401 A CN201980064401 A CN 201980064401A CN 112789480 A CN112789480 A CN 112789480A
- Authority
- CN
- China
- Prior art keywords
- user
- landmark
- location
- visual
- landmarks
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/3407—Route searching; Route guidance specially adapted for specific applications
- G01C21/3438—Rendez-vous, i.e. searching a destination where several users can meet, and the routes to this destination for these users; Ride sharing, i.e. searching a route such that at least two users can share a vehicle for at least part of the route
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/28—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network with correlation of data from several navigational instruments
- G01C21/30—Map- or contour-matching
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/3453—Special cost functions, i.e. other than distance or default speed limit of road segments
- G01C21/3476—Special cost functions, i.e. other than distance or default speed limit of road segments using point of interest [POI] information, e.g. a route passing visible POIs
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3602—Input other than that of destination using image analysis, e.g. detection of road signs, lanes, buildings, real preceding vehicles using a camera
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3605—Destination input or retrieval
- G01C21/3614—Destination input or retrieval through interaction with a road map, e.g. selecting a POI icon on a road map
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
- G01C21/3644—Landmark guidance, e.g. using POIs or conspicuous other objects
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3667—Display of a road map
- G01C21/3673—Labelling using text of road map data items, e.g. road names, POI names
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/38—Electronic maps specially adapted for navigation; Updating thereof
- G01C21/3804—Creation or updating of map data
- G01C21/3807—Creation or updating of map data characterised by the type of data
- G01C21/3811—Point data, e.g. Point of Interest [POI]
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S5/00—Position-fixing by co-ordinating two or more direction or position line determinations; Position-fixing by co-ordinating two or more distance determinations
- G01S5/16—Position-fixing by co-ordinating two or more direction or position line determinations; Position-fixing by co-ordinating two or more distance determinations using electromagnetic waves other than radio waves
-
- G—PHYSICS
- G08—SIGNALLING
- G08G—TRAFFIC CONTROL SYSTEMS
- G08G1/00—Traffic control systems for road vehicles
- G08G1/20—Monitoring the location of vehicles belonging to a group, e.g. fleet of vehicles, countable or determined number of vehicles
- G08G1/202—Dispatching vehicles on the basis of a location, e.g. taxi dispatching
Abstract
An interactive landmark locating system includes a database of visual landmarks corresponding to salient entity objects within a geographic area. The visual landmark database includes geographic information for each landmark, including the position and height of the landmark, the orientation the landmark is facing, the size of the landmark, the appearance of the landmark, the name of the landmark, a field of view of the landmark indicating the set of positions from which the landmark can be seen, and the like. When a user requests map data or navigation directions to a destination, the interactive landmark positioning system uses the visual landmarks and their corresponding geographic information to verify the user's location and orientation. For example, when the user is at a particular location and orientation, the interactive landmark positioning system provides an indication of landmarks in the vicinity of the particular location to verify the user's location and orientation.
Description
Technical Field
The present disclosure relates to navigation directions, and in particular, to directing a user with landmarks.
Background
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Systems that automatically route drivers between different geographic locations typically utilize distance indications, street names, building numbers to generate navigation directions based on the route. For example, these systems may provide instructions to the driver such as: "go forward one quarter mile, then turn right to enter maple street. However, it is difficult for the driver to accurately judge the distance, and it is not always easy to see the road sign. Furthermore, there are geographical areas where streets and roads are poorly identified.
To provide guidance to the driver that is more similar to the interpretation that others may have of the driver, navigation directions may be enhanced by referencing salient objects along the way, such as visually prominent buildings or billboards. These salient objects may be referred to as "visual landmarks". Thus, the system may generate such navigation directions as "at quarter mile, you will see McDonald's work on your right
However, not every landmark is visible at all times. For example, some billboards may be brightly illuminated at night, but are generally not noticed during the day. On the other hand, a complex facade of a building may be easily noticed during the day, but may be under-illuminated at night and therefore not easily noticed.
Further, at the beginning of navigation, when the user is stationary and has not moved directions, the navigation system typically provides absolute directions, such as "walk north in state street". Since users do not always know their orientation, they may feel it difficult to use absolute directions. However, it is difficult for a client device to determine its operation or heading while the client device is at rest.
In addition, navigation systems typically use positioning sensors, such as a Global Positioning Service (GPS) module that receives signals from several GPS satellites, to determine the position of the user. However, the position determined by GPS is often not very accurate, especially in areas with tall buildings, bridges, tunnels, etc.
Disclosure of Invention
To determine the precise location and orientation of the user, for example, when the user's client device is at rest, the interactive landmark positioning system obtains the current location of the user's client device and identifies landmarks that are visible from the user's current location and/or the estimated orientation of the user. The interactive landmark positioning system then transmits the recognized landmarks to the user's client device and requests that the user confirm that the recognized landmarks are within her field of view. If the recognized landmark is not in her field of view, the interactive landmark positioning system may send other landmarks to the user's client device. For example, the interactive landmark positioning system may estimate that the user is facing north, and may identify landmarks that are located north of the user's current location. If the user does not see these landmarks, the interactive landmark positioning system may estimate that the user is looking south and may identify landmarks that are south of the user's current location.
In some implementations, when the identified landmark is not in the user's field of view, the client device may switch to an Augmented Reality (AR) mode, present a camera view, and provide a real-world image included in the camera view to the interactive landmark positioning system. The interactive landmark positioning system may then identify landmarks included in the camera view, for example using object detection, to determine the precise location and orientation of the user. The precise location of the user may be a location with a higher accuracy than the location determined from a positioning device, such as GPS. More specifically, the interactive landmark positioning system may retrieve the location, orientation, size, etc. of landmarks from a landmark database. The precise location and orientation of the user may be based on the location, orientation, size, etc. of the identified landmarks included in the camera view. For example, if Walgling in the camera view
In any case, the interactive landmark positioning system may also use landmarks to navigate two or more users to a meeting location, such as a person-to-person location. In some cases, the interactive landmark positioning system may use shared landmarks to direct the user to a meeting location. For example, when a first user requests to travel to a destination and a second user accepts a request to transport the first user to the destination, the interactive landmark positioning system may direct the first user to a pickup location based on a particular landmark within the first user's field of view. More specifically, the interactive landmark positioning system may direct a first user to walk to a particular landmark and then stand under the particular landmark while waiting for a second user to arrive. The interactive landmark positioning system may also direct the second user (e.g., in the final navigation instruction) to proceed toward a particular landmark and may indicate that the first user is standing under the particular landmark.
An example embodiment of these techniques is a method, performed by one or more processors, for navigating two or more users to a meeting location. The method includes receiving a request for a first user to meet a second user at a meeting location, the request including a first location of the first user and a second location of the second user, the first location and the second location being within a geographic area. The method also includes retrieving a plurality of visual landmarks corresponding to physical objects within a geographic area from a database and selecting a shared visual landmark from the plurality of visual landmarks for directing the first user and the second user to a meeting location within the geographic area, the shared visual landmark selected based on visibility from the first location and the second location. Further, the method includes providing directions to the meeting location with reference to the shared visual landmark to the first user and the second user.
Another example embodiment of these techniques is a server device for navigating two or more users to a meeting location. The server device includes one or more processors and non-transitory computer-readable memory coupled to the one or more processors and storing instructions thereon. When executed by one or more processors, the instructions cause a server device to receive a request for a first user to meet a second user at a meeting location, the request including a first location of the first user and a second location of the second user, the first location and the second location being within a geographic area. The instructions also cause the server device to retrieve a plurality of visual landmarks corresponding to physical objects within a geographic area from a database and select a shared visual landmark from the plurality of visual landmarks used to direct the first user and the second user to meeting locations within the geographic area, the shared visual landmark selected based on visibility from the first location and the second location. Further, the instructions cause the server device to provide directions to the meeting location with reference to the shared visual landmark to the first user and the second user.
Yet another example embodiment of these techniques is a method performed by one or more processors of using a landmark to verify a current location or orientation of a user. The method includes receiving location and orientation information of a user of a client device from the client device, retrieving a plurality of visual landmarks from a database corresponding to entity objects within a geographic area corresponding to the location information, selecting one or more of the plurality of visual landmarks within a field of view of the user based on the location and orientation information of the user, and providing an indication of the one or more visual landmarks to the client device and a request to confirm the one or more visual landmarks within the field of view of the user. In response to receiving an indication from the client device that one or more visual landmarks are not within the user's field of view, the method includes receiving a real-world image including the visual landmarks from the field of view of the client device and determining a position and orientation of the user based on the real-world image.
Drawings
Fig. 1 is a block diagram of an example communication system in which a client device and a server device are operable to implement an interactive landmark positioning system;
fig. 2 is an example landmark data table included in a visual landmark database that may be implemented in the system of fig. 1;
FIG. 3 is an example map display depicting a field of view of a particular landmark indicating a location from which the particular landmark may be seen;
FIGS. 4A and 4B are example navigation displays presented on client devices of two users that direct the users to meeting locations via a shared landmark;
FIG. 5 is an example location verification display presented on a client device to verify a location and/or orientation of the client device from landmarks within a user's field of view;
FIG. 6 is an example display of an area within a user's field of view in AR mode presented on a client device;
FIG. 7 is an example navigation display presented on a client device in response to identifying a precise location and/or orientation of the client device;
FIG. 8 is a flow diagram of an example method that may be implemented in a server device for navigating two or more users to meeting locations; while
Fig. 9 is a flow diagram of an example method that may be implemented in a server device to verify a current location or orientation of a user with a landmark.
Detailed Description
SUMMARY
Aspects of the disclosed subject matter provide an improved navigation system by which multiple users are directed to a particular location based on one or more determined landmarks that are visible to each other. That is, navigation is provided in a deterministic manner, where navigation for each user depends on visible real world objects for the user and each other user.
Further, aspects of the disclosed subject matter provide improved methods of determining a user's location or orientation based on real world objects. For example, if a response is received from a client device that one or more visual landmarks are not within the user's field of view, then deterministic data in the form of real-world images may be obtained from the client device's field of view to determine the user's position and orientation in a deterministic manner.
Example hardware and software Components
Fig. 1 illustrates at least some environments 10 in which techniques for providing interactive visual landmarks may be implemented. The environment 10 includes a client device 12 and a server device 14 interconnected via a communication network 16. The network 16 may be a public network, such as the internet, or a private network, such as an intranet. The server device 14, in turn, may communicate with various databases and other server devices, such as navigation servers, map data servers, car pool servers, real-time traffic servers, weather servers, and the like.
Further, the client device 12 includes a camera (not shown), a user interface 28, and a network interface 30. In either case, the user interface 28 may include one or more input components (such as a touch screen, microphone, keyboard, etc.), and one or more output components, such as a screen or speaker.
The network interface 30 may support short-range and/or long-range communications. For example, the network interface 30 may support cellular communications, personal area network protocols, such as IEEE 802.11 (e.g., Wi-Fi) or 802.15 (Bluetooth). In some embodiments, client device 12 includes a plurality of network interface modules to interconnect multiple devices within client device 12 and to connect client device 12 to network 16.
Further, the network interface 30 may support geolocation in some cases. For example, the network interface 30 may support Wi-Fi trilateration. In other cases, the client device 12 may include a dedicated positioning module 32, such as a GPS module. The client device 12 may also include other sensors such as accelerometers, gyroscopes, magnetometers such as compasses, and the like.
With continued reference to fig. 1, the client device 12 may communicate with the server device 14 via a network 16, which may be a wide area network such as the internet. Server device 14 may be implemented in one or more server devices including devices distributed across multiple geographic locations. Server device 14 may implement navigation instruction generator 42 and interactive landmark selection module 44. The components 42-44 may be implemented using any suitable combination of hardware, firmware, and software. The databases, such as the map database 50, the visual landmark database 52, and the real-world image database 54, may be accessed by server devices that may be implemented using any suitable data storage and access technology.
In operation, the interactive landmark selection module 44 may receive a request for map data or navigation directions from the client device 12. The request may include the user's current location and/or estimated orientation. The interactive landmark selection module 44 may retrieve visual landmarks from the visual landmark database 52 that are within a threshold distance of the current location and visible from the current location and/or estimated orientation of the user. More specifically, interactive landmark selection module 44 may identify visual landmarks with geographic information indicating that the visual landmarks are within a threshold distance of the current location and may be seen from the current location and/or estimated orientation of the user. In any case, the interactive landmark selection module 44 may provide an indication of the recognized visual landmarks to the client device 12 and request the user to confirm that the recognized visual landmarks are within the user's field of view or request the user to turn around until she is facing the recognized visual landmarks. For example, the interactive landmark selection module 44 may provide a request that is displayed on the client device 12 via the user interface 28 for the user to confirm that she can see mcdonald's account directly in front of her
If the user confirms, via, for example, user controls presented on the user interface 28, that the recognized visual landmark is within the user's field of view or that she has turned to face the recognized visual landmark, the interactive landmark selection module 44 verifies the user's current position and orientation and determines the user's precise position and orientation based on the user's current position and orientation.For example, if Ma Dang Laolandmark selection module 44 determines that the user is facing north. The precise location of the user may be a location with a higher accuracy than the location determined from a positioning device, such as GPS. The server device 14 may then use the precise location and orientation of the user to provide map data, navigation data, ride share data, etc. to the client device 12. For example, if the user requests navigation directions to a destination location, the navigation instruction generator 42 may retrieve road geometry data, road and intersection restrictions (e.g., one-way, no left turn), road type data (e.g., highway, local road), speed limit data, etc. from the map database 50 to generate a route from the user's current location to the destination. In addition to road data, the map database 50 may store geometric descriptions and location indications of various natural geographic features, such as rivers, mountains, and forests, as well as artificial geographic features, such as buildings and parks. The map data may include vector graphics data, raster image data, and text data, among other data. In an example embodiment, the map database 50 organizes map data into map tiles, which generally corresponds to two-dimensional or three-dimensional organization of geospatial data into traversable data structures, such as octrees.
In any case, the navigation instruction generator 42 may use one or more routes to generate a sequence of navigation instructions. Examples of navigation instructions include "turn right in elm street within 500 feet" and "continue four miles straight". The navigation instruction generator 42 may implement natural language generation techniques to construct these instructions and similar phrases in the user's language. For initial navigation instructions, rather than providing absolute directions, such as "northing in state street," the navigation instruction generator 42 may direct the user based on the user's precise location and orientation as determined by the interactive landmark selection module 44. For example, the navigation instruction generator 42 may generate an initial navigation instruction "continue straight on state street" because the user is facing north. If it is initialThe instruction requires the user to proceed southward, the navigation instruction generator 42 may generate an initial navigation instruction "turn around and leave McDonald
If the user does not see the recognized visual landmark in her field of view and an indication is provided that she does not see the recognized visual landmark, for example, via a user control presented on user interface 28, interactive landmark selection module 44 requests that client device 12 switch to an Augmented Reality (AR) mode presenting a camera view of a real-world image in front of client device 12. The interactive landmark selection module 44 receives the real-world image and uses object recognition to identify landmarks within the real-world image. More specifically, the interactive landmark selection module 44 may obtain several template landmarks from a database. Template landmarks may include logos or other salient physical objects. The interactive landmark selection module 44 may identify visual features of each of the template landmarks by detecting stable regions within the template landmarks that are detectable regardless of camera perspective blur, motion, distortion, orientation, illumination, zoom, and/or other changes. The stable region may be extracted from the template landmark using scale-invariant feature transform (SIFT), accelerated robust features (SURF), fast retinal keypoints (FREAK), binary robust scalable keypoints (BRISK), or any other suitable computer vision technique. In some implementations, the keypoints may be located in high contrast regions of the template landmarks, such as edges within the template landmarks. A bounding box may be formed around the keypoint, and the portion of the template landmark created by the bounding box may be a feature.
The interactive landmark selection module 44 may create a digital representation of the feature to generate a template feature vector, such as the width and height of the feature, the RGB pixel values of the feature, the pixel location of the center point of the feature within the object, and so on. Further, the interactive landmark selection module 44 may identify visual features (also referred to herein as "objects") of the camera view or portions of the camera view from the camera of the client device 12 to generate feature vectors. The interactive landmark selection module 44 may then compare the feature vectors of the camera view or of objects within the camera view to the template feature vectors to identify one or more template objects having template feature vectors that are closest to the feature vectors of the camera view of the camera of the client device 12. For example, the interactive landmark selection module 44 may identify the first three template objects having template feature vectors closest to the feature vectors of the camera view. If the majority of the one or more identified template objects are Wallacelandmark selection module 44 may identify the object in the camera view as wolgrelin
In any case, the interactive landmark selection module 44 may retrieve, for example, human visual geocoding or geographic information of the landmarks identified within the camera view from the visual landmark database 52 to determine the location, orientation, size, etc. of each identified landmark. When recognized landmarks (such as walgland)landmark selection module 44 retrieves geographic information for a location closest to the user's approximate current location. In other implementations, when more than one landmark is identified within the camera view, the interactive landmark selection module 44 determines the location of each identified landmark based on the geographic area that includes each of the identified landmarks. In any case, the interactive landmark selection module 44 determines the current position and orientation of the user based on the position and orientation of the landmarks identified within the camera view. For example, if Wallacelandmark selection module 44 determines that the user is facing west and is located east of the corners of main and state streets. Further, the interactive landmark selection module 44 may determine the current location of the user based on the size of each recognized landmark within the camera view. For example, Walgelin in Camera View
In another exemplary scenario, the interactive landmark selection module 44 receives a request to navigate two or more users to a meeting location via their respective client devices 12. Interactive landmark selection module 44 may then identify landmarks for directing each of the users to the meeting location based on the orientation and location from which the user is approaching the meeting location. In some implementations, the same landmark may be used by the interactive landmark selection module 44 to identify a shared landmark for directing each of the users to the meeting location. In other embodiments, interactive landmark selection module 44 may identify different landmarks for directing each of the users to the meeting location based on the landmarks that are most visible to each user according to their location and orientation proximate to the meeting location.
For example, in a ride share environment, a rider may request traffic services to a destination via the ride share application 27 in his rider client device 12. The driver may receive and accept the request via the ride share application 27 in his driver client device 12, and the server device 14 or another server may identify a pickup location for picking up the rider. The interactive landmark selection module 44 may then obtain the pickup and current locations of the driver and the occupant. For example, the current location of the driver and the occupant may be determined using the techniques described above. In some embodiments, the navigation instruction generator 42 may generate routes to pickup locations for the driver and the rider, and the interactive landmark selection module 44 may obtain locations to perform final guidance of the respective routes. For example, navigation instruction generator 42 may generate a set of instructions for directing the driver to the pickup location, where the last instruction is to turn left from state street to main street after driving north from state street, and continue straight to the pickup location. In this example, the interactive landmark selection module 44 may obtain the intersection of the state street and the main street as the location of the driver, and obtain the orientation of the driver as the direction the driver will face after turning left to the north in the state street. In this manner, the interactive landmark selection module 44 identifies thresholds from the driver's position and toward the visible catch positionLandmarks within range of values. The interactive landmark selection module 44 may then provide the identified landmarks to the navigation instruction generator 42 to combine the identified landmarks into a set of navigation instructions. For example, after directing the driver to turn left from state street into main street, navigation instruction generator 42 may generate additional navigation instructions to point towards Wallacenavigation instruction generator 42 may generate additional navigation instructions for directing the rider toward walgland
The visual landmark database 52 may store information about significant geographic entities that are visible while driving (or riding a bicycle, walking, or moving along a navigation route), and thus are used as visual landmarks. For example, the visual landmark database 52 may store human visual geocodes, such as PGE park 70, Jinmen bridge 72, Walgling, Denver
For each visual landmark, the visual landmark database 52 may store one or more photographs, two-or three-dimensional geographic coordinates, textual descriptions, heights of landmarks, orientations that the landmarks are facing, sizes of landmarks, appearances of landmarks, names of landmarks, views of landmarks indicating sets of locations that are visible to the landmark under various conditions and/or at various times of the day, and the like. The field of view may vary depending on the time of day, weather conditions, season, etc. Thus, the visual landmark database 52 may store several views of landmarks at different times of day, seasons, weather conditions, lighting conditions, traffic conditions, and the like.
To populate the visual landmark database 52, the server device 14 may receive satellite images, photographs and videos submitted by various users, street level images collected by cars equipped with dedicated panoramic cameras, street and sidewalk images collected by pedestrians and riders, and so forth. Similarly, the visual landmark database 52 may receive descriptions of landmarks from various sources, such as the operator of the server device 14 and the person submitting the user-generated content.
A data table 200 including an example index of human visual geocoding or geographic information for each landmark included in the visual landmark database 52 is shown in fig. 2. For each landmark, the data table 200 includes, among other things, a landmark identifier 202, a location 204 of the landmark including two-dimensional or three-dimensional geo-coordinates and a height of the landmark, an orientation 206 that the landmark is facing, a size or dimension 208 of the landmark, a textual description 210 of an appearance of the landmark, a name 212 of the landmark, and a field of view 214 of the landmark indicating a set of locations that are visible to the landmark under various conditions and/or at various times of the day. The data table 200 may also include a photograph of a landmark.
For example, the first entry of data table 200 includes Wallace
An example field of view 302 of a particular landmark 304 (e.g., a PGE park) is shown in the map display 300 of fig. 3. The field of view 302 includes a set of points in space from which landmarks 304 are visible. The shaded area represents a point from which at least a portion of the landmark 304 is visible. The field of view 302 may be determined based on test points along streets and other traversable paths proximate to the landmark 304. As shown, the field of view 302 has a complex shape because the landmark 304 is occluded by other objects, such as other buildings near the landmark 304. In the example, the landmark is visible from each direction, but the landmark 304 is seen to be farther away from the position of the northwest direction, the position of the southern direction, and the position of the southwest direction than the landmark is seen from the position of the north, east, or west side of the landmark. For each landmark, the visual landmark database 52 may store an indication of a field of view similar to that shown in fig. 3 and including an example field of view 302 from which the set of locations of landmarks may be seen.
The field of view of the landmark 304 may be determined, for example, by accessing a three-dimensional model associated with the geographic area in which the landmark is located. The model may include a three-dimensional representation of a landmark. In one embodiment, the three-dimensional model may be a stereoscopic mesh model of a geographic area including a plurality of mesh triangles associated with a landmark. The models may also include or have access to synthetic models of geographic features or other landmarks within the geographic region, such as those provided in virtual earth applications. The model may also include textures mapped to surfaces in the three-dimensional model. The texture may include a photograph or other image associated with the geographic area, and may depict geographic objects such as trees (with or without leaves), bridge supports, and other structures that may obscure visibility of landmarks within the geographic area.
The visibility of the three-dimensional representation of landmark 304 may be evaluated from a number of different geographic points in three-dimensional space within the three-dimensional model. For example, for each geographic point, it may be determined whether a three-dimensional representation of the landmark 304 is visible from the geographic point. If so, the geographic point is included in the field of view 302 of the landmark 304.
In one embodiment, a geo-point can only be included in the field of view 302 if a three-dimensional representation of the landmark 304 is visible from the geo-point that is greater than a predefined threshold. The threshold may be based on any suitable metric associated with the visibility of the landmark 304. For example, the threshold may be based on a visible surface area of the landmark 304, such that if a visible surface area of the landmark 304 that is larger than a predefined surface area is visible from the geo-point, the geo-point may be included in the field of view 302. As another example, the threshold may be based on the bearing subtended by the features of a given view of the landmark 304 from the geo-point, such that the geo-point is included in the field of view 302 if a bearing greater than a predefined bearing is visible from the geo-point.
As described above, server device 14 may receive requests from client devices 12 to navigate multiple users to meeting locations. In some embodiments, server device 14 may receive the meeting location and current location of each user. In other implementations, server device 14 may determine the face location as a location within a threshold distance of one of the users or a location between the current locations of each user. For example, when a rider requests traffic service to a destination location via the ride share application 26, and the driver accepts the request, the server device 14 or another server (such as a ride share server) may identify pickup locations that are within a threshold distance of the rider's current location. In another example, two users may request a meeting, and server device 14 may identify a meeting location that is approximately equidistant from both users' current locations.
In any case, server device 14 may generate a route to the meeting location for each user via navigation instruction generator 42. In some embodiments, server device 14 may identify a final guidance or a guidance prior to a final guidance for each route and determine the user's location and orientation after completing the final guidance or the guidance prior to the final guidance. In other embodiments, server device 14 does not generate a route to the meeting location when the user's current location is less than a threshold distance (e.g., 100 meters) from the meeting location. Rather, server device 14 identifies landmarks that are visible from the user's current location, which may be used to direct the user to a meeting location.
Using the determined location and orientation of the user, server device 14 may then identify, via interactive landmark selection module 44, candidate landmarks from visual landmark database 52 that are visible from the determined location and orientation of the user and/or within a geographic area that includes the meeting location. The interactive landmark selection module 44 may identify candidate landmarks using the field of view, orientation, position, height, size, etc. of each landmark in the visual landmark database 52. The interactive landmark selection module 44 may then assign a visibility score or ranking to the candidate landmarks based on several metrics including, for example, the location of the candidate landmarks including proximity to meeting locations, visibility based on the field of view corresponding to time of day, weather conditions, season, etc., size of the landmarks, whether the landmarks are facing directly toward the user according to the orientation of the user and the orientation of the landmarks, etc. Interactive landmark selection module 44 may assign individual scores to each metric, such as a proximity score to a meeting location, a visibility score, a landmark size score, a landmark orientation score, a landmark height score, and the like, and aggregate or combine the individual scores in any suitable manner to generate a visibility score for the candidate landmark based on the user's determined location and orientation. In some implementations, the interactive landmark selection module 44 may select the highest scoring or highest ranking candidate landmark as the landmark. Server device 14 may then provide navigation instructions to the user's client device 12 to direct the user to the meeting location via the selected landmark. Also in some embodiments, server device 14 may move the meeting location to the landmark location when the landmark is very close to the meeting location (e.g., within a threshold distance of 10 m).
The interactive landmark selection module 44 may select different landmarks for directing each user to the meeting location based on the highest scoring or highest ranking candidate landmark based on the location and heading from which the user is near the meeting location. In other embodiments, the interactive landmark selection module 44 may select a shared landmark for directing each user to a meeting location using the same landmark. For example, the interactive landmark selection module 44 may generate a score for each candidate landmark that is specific to one of the users. The interactive landmark selection module 44 may then combine the user-specific scores to generate an overall score for each candidate landmark. The interactive landmark selection module 44 may then select the candidate landmark with the highest overall score as the shared landmark. In some implementations, for each user-specific score or ranking, the shared landmarks must have a user-specific score above a threshold score or a user-specific ranking above a threshold ranking, such that the interactive landmark selection module 44 does not select shared landmarks that are highly visible from one user's location and difficult to see from another user's location. In this case, if the candidate landmark has the highest overall score but has a user-specific score below the threshold score, the interactive landmark selection module 44 may select the candidate landmark having the next highest overall score until the interactive landmark selection module 44 identifies the candidate landmark having the user-specific score above the threshold score.
Example user interface
Fig. 4A-4B illustrate an example navigation display 400, an example navigation display 450 presented on client devices 12 of two users for directing the users to a meeting location via a shared landmark. As shown in FIGS. 4A-4B, the shared landmark is Volgrelinexample navigation display 400, the client device 12 presents the rider with a map display 402 indicating walking directions to the meeting location. Navigation display 400 also includes navigation instructions 404 indicating a trend toward Wallacenavigation display 400 includes Volgrelinexample navigation display 450, the client device 12 presents a map display 452 to the driver indicating driving directions to a meeting location or directions for any suitable travel mode. The navigation display 450 also includes navigation instructions 454 to indicate that a right turn at 500 meters into the main street will stand at Walglinnavigation display 450 also includes Volterglinglabel 456 is such that the user knows what to look for. In an example, the user may meet for a ride share arrangement, where the navigation display 400 is presented on the rider's client device 12 and the navigation display 450 is presented on the driver's client device 12.
Also as described above, the server device 14 may receive requests for map data or navigation directions from the client device 12. The request may include, for example, the user's current location and/or estimated orientation from the GPS module 32 and/or compass. The interactive landmark selection module 44 may retrieve visual landmarks from the visual landmark database 52 that are within a threshold distance of the current location and visible from the current location and/or estimated orientation of the user. More specifically, interactive landmark selection module 44 may identify visual landmarks having geographic information indicating that the visual landmark is within a threshold distance of the current location and visible from the current location and/or estimated orientation of the user. In any case, the interactive landmark selection module 44 may provide an indication of the recognized visual landmarks to the client device 12 and for the user to confirm that the recognized visual landmarks are within the user's field of view or to request the user to turn around until she is faced with a request for the recognized visual landmarks.
FIG. 5 illustrates an example location verification display 500 presented on the user's client device 12. For example, the location verification display 500 may be presented when a user requests navigation directions from her current location to a destination via the geographic application 26. The interactive landmark selection module 44 may then identify the precise location and orientation of the user based on the visual landmarks in the user's field of view to generate initial navigation instructions for directing the user to a destination. In another example, the location verification display 500 may be presented when a user requests a transportation service via the ride share application 27. The interactive landmark selection module 44 may identify the precise location and orientation of the user based on visual landmarks within the user's field of view to generate initial navigation instructions for directing the user to a pickup location or for identifying the pickup location as the precise location of the user. The interactive landmark selection module 44 may then provide the driver's client device 12 with the user's precise location as the pickup location. In yet another example, the interactive landmark selection module 44 may identify the precise location and orientation of the driver based on visual landmarks within the driver's field of view. The interactive landmark selection module 44 may then provide the driver's precise location to the rider's client device 12 to direct the rider to the driver's precise location when the rider has difficulty finding the driver.
In any case, the location verification display 500 may include a request 502 for the user to confirm that she can see the recognized landmark (e.g., the hamburger king)Location verification display 500 may also include a description of the landmark 504, a description of the landmark 506, so that the user knows what to look for. Further, the location verification display 500 includes user controls 508, 510 for confirming or denying the identified landmark in the user's field of view. In other embodiments, the location verification display 500 may prompt the user to shoot identificationTo confirm that the recognized landmark is within the user's field of view. As described above, if the user does not see landmark 504, landmark 506 in front of her, she may turn around or look around for landmark 504, landmark 506, and if she finds landmark 504, landmark 506 after a search, confirm that the recognized landmark 504, landmark 506 is within her field of view. However, if the user does not see recognized landmarks 504, 506 even after a search, she may select user control 510 to deny her seeing recognized landmarks 504, 506. Client device 12 may then send an indication of the user's response to server device 14.
If server device 14 receives an indication confirming that recognized landmarks 504, 506 are within the user's field of view, server device 14 may determine the precise location and orientation of the user based on the current location and/or the estimated orientation. Server device 14 may then generate navigation directions to the destination based on the user's precise location and orientation, may present an indication of the user's precise location and orientation in a map display, may provide the user's precise location and orientation to another client device 12 for a second user of another client device 12 to meet the user at the precise location, may instruct the user to turn to a particular direction to view the other user, the meeting location, or a landmark for directing the user to the meeting location, or may use the precise location and orientation for any other suitable purpose.
On the other hand, if server device 14 receives an indication to deny that recognized landmark 504, landmark 506 is within the user's field of view, server device 14 may provide client device 12 with a request to switch to an AR mode that presents a camera view of a real-world image in front of client device 12. In other implementations, client device 12 automatically switches to the AR mode in response to receiving a selection by user control 510 to deny that recognized landmark 504, landmark 506 is within the user's field of view.
Fig. 6 shows an example camera view 600 of a real-world image within a user's field of view from an AR mode presented on the client device 12. The camera view 600 includes a vehicle, a building, a pole, a street, etc. The camera view 600 also includes an identification 602 of joe's restaurant as a landmark included in the visual landmark database 52, the visual landmark database 52 storing geographic information for joe's restaurant 602. Client device 12 provides camera view 600 to server device 14, and server device 14 may, in turn, identify landmarks within camera view 600, such as an identification 602 of joe's restaurant.
More specifically, the interactive landmark selection module 44 may obtain several template landmarks from the database, such as photographs of template landmarks from the visual landmark database 52. The interactive landmark selection module 44 may identify visual features of each of the template landmarks by detecting stable regions within the template landmarks that are detectable despite blurring, dynamics, distortion, orientation, illumination, zooming, and/or other changes in the camera's perspective. In some implementations, the keypoints may be located in high contrast regions of the template landmarks, such as edges within the template landmarks. A bounding box may be formed around the keypoint, and the portion of the template landmark created by the bounding box may be a feature. The interactive landmark selection module 44 may create a digital representation of the feature to generate a template feature vector, such as the width and height of the feature, the RGB pixel values of the feature, the pixel location of the center point of the feature within the object, and so on.
The interactive landmark selection module 44 may identify visual features within the camera view 600 or visual features within objects in the camera view, such as visual features of an area in the camera view 600 that includes an identification 602 of a restaurant of joe.
The interactive landmark selection module 44 may then compare the feature vectors of the camera view 600 or of objects within the camera view 600 to the template feature vectors to identify one or more template landmarks having template feature vectors that are closest to the feature vectors of the camera view 600.
In response to determining that camera view 600 includes joe's restaurant identification 602, interactive landmark selection module 44 may obtain geographic information for joe's restaurant identification 602 from visual landmark database 52, such as joe's restaurant identification 602 location, joe's restaurant identification 602 orientation, joe's restaurant identificationHeight of 602, size of joe's restaurant identification 602, etc. If there are multiple instances of joe's restaurant identification 602 in the visual landmark database 52, the interactive landmark selection module 44 may select the instance that is located closest to the received user's current location. In other implementations, when more than one landmark is identified within the camera view 600, the interactive landmark selection module 44 determines the location of each identified landmark based on the geographic area that includes each of the identified landmarks. For example, Joe's restaurant may be located in five different cities, but Joe's restaurant may only be located in McDonald's place in one of the five citieslandmark selection module 44 may select from having a neighboring mcdonald's works
In any case, the interactive landmark selection module 44 determines the precise location and orientation of the user based on the location, orientation, height, and/or size of the landmarks identified within the camera view 600. More specifically, the interactive landmark selection module 44 may determine a distance between the user and the landmark based on the size of the landmark relative to the size of the landmark in the camera view 600. The distance from the user to the landmark may be proportional to the size of the landmark within the camera view 600. A landmark that appears larger in the camera view 600 relative to the dimensions of the landmark may indicate that the user is near the landmark. For example, if the entire golden gate bridge is included in the camera view 600, the interactive landmark selection module 44 may determine that the user is at least 100 meters from the golden gate bridge. On the other hand, if only one bridle of the golden portal bridge is included in the camera view 600, the interactive landmark selection module 44 may determine that the user is standing on or within a few meters of the golden portal bridge. The interactive landmark selection module 44 may also determine a distance between the user and the landmark based on whether the landmark is in the foreground or the background of the camera view 600. If the landmark is in the foreground of the camera view 600, the landmark may be closer to the user than if the landmark is in the background of the camera view 600.
Based on the distance from the user to the landmark, the interactive landmark selection module 44 may identify a geographic area in which the user is located, and may determine the precise location and/or orientation of the user as a location within the geographic area. To determine the precise location and/or orientation of the user as a location within the geographic area, the interactive landmark selection module 44 may determine the orientation of landmarks within the camera view 600. If the landmark is facing directly toward the point of view of the camera view 600, the interactive landmark selection module 44 may determine that the user is located in the direction in which the landmark is facing and that the user is facing in a direction substantially opposite to the direction in which the landmark is facing. For example, if a landmark is directly towards the point of view of the camera view 600 and the landmark is facing south, the interactive landmark selection module 44 may determine that the user is located directly south of the landmark and facing directly north. The interactive landmark selection module 44 may then determine the precise location of the user based on the distance from the landmark, the location of the landmark, and the orientation of the user relative to the landmark. For example, if the landmark is on a corner of main and state streets and the user is 50 meters south of the landmark, interactive landmark selection module 44 may determine that the user's precise location is the location of the corner of main and state streets 50 meters south and that the user's orientation is north.
If the landmark is not directly facing the point of view of the camera view 600, such as the restaurant identification 602 of joe in the camera view 600, the interactive landmark selection module 44 determines the orientation of the landmark within the camera view 600. The interactive landmark selection module 44 may then determine the orientation of the user relative to the landmarks based on the orientation of the landmarks within the camera view 600 and the direction in which the landmarks are facing. Additionally, the interactive landmark selection module 44 may determine the orientation of the user as being substantially opposite to the direction of the user relative to the landmarks. For example, the interactive landmark selection module 44 may determine that joe's restaurant identification 602 is south-facing based on geographic information for joe's restaurant identification 602 included in the visual landmark database 52. The interactive landmark selection module 44 may determine that the restaurant identification 602 of joe is at a 45 ° angle relative to the viewpoint of the camera view 600. Accordingly, the interactive landmark selection module 44 may determine that the user is in the southwest direction of joe's restaurant identification 602, and facing in the northeast direction.
Fig. 7 illustrates an example navigation display 700 presented on client device 12 in response to identifying a precise location and/or orientation of client device 12. In the example navigation display 700, the client device 12 presents a map display 702 indicating walking directions from the user's precise location and/or heading to a destination or meeting location. The navigation display 700 also includes navigation instructions 704 to continue traveling straight 100 meters on the Columbus street. In this example, the user is facing the direction she needs to travel to reach the destination according to her precise location and orientation. In other examples, when the user is not facing the direction she needs to travel to reach her destination, the initial navigation instructions may provide the relative direction the user needs to turn to start to her destination. More specifically, the initial navigation instruction may be to turn in a particular direction relative to the direction the user is currently facing without using an absolute or cardinal direction. The initial navigation instruction may be "turn around," "turn left," "turn right," etc. before proceeding.
Example method of providing an interactive landmark in a geographic or carpool application
FIG. 8 illustrates a flow chart of an example method 800 for navigating two or more users to a meeting location. The method may be implemented in a set of instructions stored on a computer readable memory and executable on one or more processors of server device 14. For example, the method may be implemented by the navigation instruction generator 42 and/or the interactive landmark selection module 44.
At block 802, a request for a first user to meet a second user at a meeting location is received. In some implementations, the request can include current locations of the first user and the second user within the geographic area and a meeting location. In other embodiments, the request may include current locations of the first user and the second user, and server device 14 may determine the meeting location based on at least one of the current locations, for example. For example, the request may be a request for a transportation service from a current location of the first user to a drop-off location. The server device 14 may then broadcast the request to the driver and may receive an indication that the second user accepted the request and an indication of the second user's current location. Server device 14 may then identify a meeting location that is proximate to the first user's pick-up location. In any case, server device 14 may identify the meeting location as a location within a threshold distance of one of the users or a location between the current locations of each of the users.
In response to receiving the request, server device 14 may generate a route to the meeting location for each user via navigation instruction generator 42. In some embodiments, server device 14 may identify a final guidance or a guidance prior to a final guidance for each route and determine the user's location and orientation after completing the final guidance or the guidance prior to the final guidance. In other embodiments, server device 14 does not generate a route to the meeting location when the user's current location is less than a threshold distance (e.g., 100 meters) from the meeting location. Rather, server device 14 identifies landmarks that are visible from the user's current location, which may be used to direct the user to a meeting location. Also in some embodiments, server device 14 may move the meeting location to the landmark location when the landmark is very close to the meeting location (e.g., within a threshold distance of 10 m).
Then, using the determined location and orientation of the first user, for example, after completing the final guidance along the route, server device 14 may identify, via interactive landmark selection module 44, candidate landmarks that are visible from the determined location and orientation of the first user and/or within a geographic area that includes the meeting location from visual landmark database 52 (block 804). The interactive landmark selection module 44 may identify candidate landmarks using the field of view, orientation, position, altitude, dimensions, etc., of each landmark included in the visual landmark database 52. At block 806, the interactive landmark selection module 44 may then assign a first visibility score or ranking to the candidate landmarks based on several metrics including, for example, the location of the candidate landmarks including proximity to meeting locations, visibility based on the field of view corresponding to time of day, weather conditions, season, etc., the size of the landmarks, whether the landmarks are facing directly toward the user according to the orientation of the user and the orientation of the landmarks, etc. Interactive landmark selection module 44 may assign individual scores to each metric, such as a proximity score to a meeting location, a field of view score, a landmark size score, a landmark orientation score, a landmark height score, and the like, and aggregate or combine the individual scores in any suitable manner to generate a first visibility score for the candidate landmark based on the first user's determined location and orientation.
The process is then repeated for the second user. Server device 14 may identify candidate landmarks that are visible from the determined location and orientation of the second user and/or that are within a geographic area that includes the meeting location from visual landmark database 52 (block 804). At block 806, the interactive landmark selection module 44 may then assign a second visibility score or ranking to the candidate landmarks based on several metrics including, for example, proximity to the meeting location, visibility based on a field of view corresponding to time of day, weather conditions, season, etc., size of the landmark, whether the landmark is facing directly toward the second user according to the orientation of the second user and the orientation of the landmark, etc.
At block 808, the interactive landmark selection module 44 may select a shared landmark for directing each user to the meeting location using the same landmark based on the visibility score and/or ranking assigned to each candidate landmark. As described above, the interactive landmark selection module 44 may generate a first score for each candidate landmark specific to the first user and a second score for each candidate landmark specific to the first user. The interactive landmark selection module 44 may then aggregate or combine the user-specific scores to generate a total score for each candidate landmark. The interactive landmark selection module 44 may rank the candidate landmarks according to their respective user-specific scores. For example, interactive landmark selection module 44 may generate a first set of rankings for the candidate landmarks based on the first visibility score and a second set of rankings for the candidate landmarks based on the second visibility score.
The interactive landmark selection module 44 may then select the candidate landmark with the highest overall score as the shared landmark. In some implementations, for each user-specific score or ranking, the shared landmarks must have a user-specific score above a threshold score or a user-specific ranking above a threshold ranking, such that the interactive landmark selection module 44 does not select shared landmarks that are highly visible from one user's location and difficult to see from another user's location. In this case, if the candidate landmark has the highest overall score, but has a user-specific score below the threshold score or a user-specific ranking below the threshold ranking, the interactive landmark selection module 44 may select the candidate landmark having the next highest overall score until the interactive landmark selection module 44 identifies the candidate landmark having a user-specific score above the threshold score or a user-specific ranking above the threshold ranking.
At block 810, the server device 14 generates and provides a first set of navigation directions to the first user that reference the shared landmark. In the example above, where the first user is a car rider requesting a ride share service, the first set of navigation directions may be walking directions to a pickup location. The first set of navigation directions may include navigation instructions (e.g., final navigation instructions) that include a shared landmark, such as "continue straight through to the hamburger king
At block 812, the server device 14 generates and provides a second set of navigation directions to the second user that reference the shared landmark. In the example above, where the second user is a driver of a pickup vehicle, the second set of navigation directions may be driving directions to a pickup location. The second set of navigation directions may include navigation instructions (e.g., final navigation instructions) that include a shared landmark, such as "reach hamburger king
Fig. 9 illustrates a flow chart of an example method 900 for verifying a current location or orientation of a user using a landmark. The method may be implemented in a set of instructions stored on a computer readable memory and executable on one or more processors of server device 14. For example, the method may be implemented by the navigation instruction generator 42 and/or the interactive landmark selection module 44.
At block 902, a current location and/or estimated orientation is received from the user's client device 12. In some implementations, a current location and/or estimated orientation and a request for map data, navigation directions, ride share data, and the like can be received. The current location and/or the estimated orientation may be obtained from sensors (such as the GPS module 32, compass, etc.) within the user's client device 12.
In some implementations, the interactive landmark selection module 44 may assign a visibility score or ranking to each candidate visual landmark based on several metrics including, for example, the location of the candidate landmark including proximity to the user's current location, visibility from the user's current location and/or estimated orientation based on a field of view corresponding to time of day, weather conditions, season, etc., the size of the landmark, whether the landmark is facing the user directly according to the user's orientation and the orientation of the landmark, and the like.
The interactive landmark selection module 44 may then select one or more candidate visual landmarks having a visibility score above a threshold score. For example, when the visibility scores of three landmarks are above a threshold score, indicating that each of the three landmarks are within the user's field of view based on proximity to the user's current location, visibility from the user's current location and/or estimated orientation, whether the landmark is facing the user according to the user's estimated orientation and landmark orientation, etc., then interactive landmark selection module 44 may select the three landmarks to provide to the user's client device. On the other hand, when the visibility score of only one landmark is above the threshold score, the interactive landmark selection module 44 may select the one landmark to provide to the user's client device 12.
The interactive landmark selection module 44 may then provide an indication of the recognized visual landmarks to the client device 12 and request the user to confirm that the recognized visual landmarks are within the user's field of view or request the user to turn around until she is facing the recognized visual landmarks at block 908. For example, the interactive landmark selection module 44 may provide a request that is displayed on the client device 12 via the user interface 28 for the user to confirm that she can see mcdonald's duty directly
If at block 910, the userConfirming that the identified visual landmark is within the user's field of view, or that she has turned to face the identified visual landmark, for example, via a user control presented on the user interface 28 of the client device 12, the interactive landmark selection module 44 may determine that the user's precise position and orientation is the current position and/or estimated orientation (block 918). For example, if Ma Dang Laolandmark selection module 44 determines that the user is facing north. The server device 14 may then provide the map data, navigation data, carpool data, etc. to the client device 12 using the user's precise location and orientation. For example, if the user requests navigation directions to a destination location, the navigation instruction generator 42 may retrieve road geometry data, road and intersection restrictions (e.g., one-way, no left turn), road type data (e.g., highway, local road), speed limit data, etc. from the map database 50 to generate a route from the user's precise location to the destination (block 916).
If at block 910, the user does not see the recognized visual landmark within her field of view, and provides an indication that she does not see the recognized visual landmark, for example, via a user control presented on the user interface 28 of the client device 12, the interactive landmark selection module 44 requests the client device 12 to switch to an AR mode presenting a camera view of a real-world image in front of the client device 12. In other implementations, the client device 12 automatically switches to the AR mode in response to receiving a selection of a user control that denies the recognized landmark within the user's field of view.
The interactive landmark selection module 44 then receives a real-world image from the camera view of the client device 12 and identifies landmarks within the real-world image using object recognition (block 912). More specifically, the interactive landmark selection module 44 may obtain several template landmarks from the database, such as photographs of template landmarks from the visual landmark database 52. The interactive landmark selection module 44 may identify visual features of each of the template landmarks. In some implementations, the keypoints may be located in high contrast regions of the template landmarks, such as edges within the template landmarks. A bounding box may be formed around the keypoint, and the portion of the template landmark created by the bounding box may be a feature. The interactive landmark selection module 44 may create a digital representation of the feature to generate a template feature vector, such as the width and height of the feature, the RGB pixel values of the feature, the pixel location of the center point of the feature within the object, and so on.
Further, the interactive landmark selection module 44 may identify visual features of the camera view or visual features within the object of the camera view. The interactive landmark selection module 44 may then compare the feature vector of the camera view or the feature vector of the object within the camera view to the template feature vectors to identify one or more template landmarks having template feature vectors that are closest to the feature vector of the camera view. For each identified object in the camera view, the interactive landmark selection module 44 may identify the template landmark having the closest template feature vector to the feature vector of the object. Each of the template landmarks may be identified as a landmark within the camera view.
The interactive landmark selection module 44 may then retrieve geographic information for the recognized landmarks within the camera view from, for example, the visual landmark database 52 to determine the position, heading, and/or location of each recognized landmarkOrientation, size, etc. When recognized landmarks (such as walgland)landmark selection module 44 retrieves geographic information for a location closest to the user's approximate current location. In other implementations, when more than one landmark is identified within the camera view, the interactive landmark selection module 44 determines the location of each identified landmark based on the geographic area that includes each of the identified landmarks. For example, Joe's restaurant may be located in five different cities, but Joe's restaurant may only be located in McDonald's place in one of the five citieslandmark selection module 44 may choose from having a neighboring mcdonald's works
In any case, the interactive landmark selection module 44 determines the precise position and orientation of the user based on the position, orientation, height, and/or size of the landmark identified within the camera view (block 914). More specifically, for each recognized landmark, the interactive landmark selection module 44 may determine a distance between the user and the landmark based on the size of the landmark relative to the size of the landmark in the camera view. The distance from the user to the landmark may be proportional to the size of the landmark within the camera view. A landmark that appears larger in the camera view relative to the dimensions of the landmark may indicate that the user is near the landmark. The interactive landmark selection module 44 may also determine a distance between the user and the landmark based on whether the landmark is in the foreground or the background of the camera view. If the landmark is in the foreground of the camera view, the landmark may be closer to the user than if the landmark is in the background of the camera view.
Based on the distance from the user to the landmark, the interactive landmark selection module 44 may identify a geographic area in which the user is located, and may determine the precise location and/or orientation of the user as a location within the geographic area. To determine the precise location and/or orientation of the user as a location within the geographic area, the interactive landmark selection module 44 may determine the orientation of landmarks within the camera view. If the landmark is directly facing the point of view of the camera view, the interactive landmark selection module 44 may determine that the user is located in a direction that the landmark is facing and that the user is facing in a direction substantially opposite to the direction that the landmark is facing. For example, if the landmark is directly facing the point of view of the camera view and the landmark is facing south, the interactive landmark selection module 44 may determine that the user is located directly south and facing north of the landmark. The interactive landmark selection module 44 may then determine the precise location of the user based on the distance from the landmark, the location of the landmark, and the orientation of the user relative to the landmark. For example, if the landmark is on a corner of main and state streets and the user is looking 50 meters south of the landmark, interactive landmark selection module 44 may determine that the user's precise location is the location of the corner of main and state streets that is 50 meters south and that the user's orientation is north.
If the landmark is not directly facing the viewpoint of the camera view, the interactive landmark selection module 44 determines the orientation of the landmark within the camera view. The interactive landmark selection module 44 may then determine the orientation of the user relative to the landmark based on the orientation of the landmark within the camera view and the direction in which the landmark is facing. Additionally, the interactive landmark selection module 44 may determine the orientation of the user as being in a direction substantially opposite to the direction of the user relative to the landmarks.
As described above, the server device 14 may then use the user's precise location and orientation to provide map data, navigation data, ride share data, and the like to the client device 12. For example, if the user requests navigation directions to a destination location, the navigation instruction generator 42 may retrieve road geometry data, road and intersection restrictions (e.g., one-way, no left turn), road type data (e.g., highway, local road), speed limit data, etc. from the map database 50 to generate a route from the user's precise location to the destination (block 916).
Additional notes
Various operations of the example methods described herein may be performed, at least in part, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily configured or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions. In some example embodiments, the modules referred to herein may comprise processor-implemented modules.
Similarly, the methods or routines described herein may be implemented at least in part by a processor. For example, at least some of the operations of the method may be performed by one or more processors or processor-implemented hardware modules. Execution of some of the operations may be distributed among one or more processors, and not just resident on a single machine, but deployed across multiple machines. In some example embodiments, one or more processors may be located at a single location (e.g., within a home environment, an office environment, or as a server farm), while in other embodiments, processors may be distributed across multiple locations.
The one or more processors are also operative to support performance of related operations in a cloud computing environment, or as software as a service (SaaS). For example, at least some of the operations may be performed by a set of computers (as an example of machines including processors), which may be accessed via a network (e.g., the internet) and via one or more appropriate interfaces (e.g., Application Program Interfaces (APIs)).
Upon reading this disclosure, those of ordinary skill in the art will appreciate additional alternative structural and functional designs for systems that use interactive landmarks in navigation. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. It will be apparent to those skilled in the art that various adjustments, changes, and modifications in the arrangement, operation, and details of the methods and apparatus disclosed herein may be made without departing from the spirit and scope of the invention as defined in the appended claims.
Claims (20)
1. A method for navigating two or more users to a meeting location, the method comprising:
receiving, at one or more processors, a request for a first user to meet a second user at a meeting location, the request comprising a first location of the first user and a second location of the second user, the first location and the second location being within a geographic area;
retrieving, by the one or more processors, a plurality of visual landmarks from a database corresponding to physical objects within the geographic area;
selecting, by the one or more processors, a shared visual landmark from the plurality of visual landmarks used to direct the first user and the second user to meeting locations within the geographic area, the shared visual landmark selected based on visibility from both the first location and the second location; and
providing, by the one or more processors, directions to the meeting location with reference to the shared visual landmark to the first user and the second user.
2. The method of claim 1, wherein selecting a shared visual landmark for directing the first user and the second user to the meeting location comprises:
identifying, by the one or more processors, candidate visual landmarks within a threshold distance of the meeting location;
assigning, by the one or more processors, a first score to the candidate visual landmark based on visibility from the first location;
assigning, by the one or more processors, a second score to the candidate visual landmark based on visibility from the second location;
combining, by the one or more processors, the first score and the second score to generate a total score for the candidate visual landmark; and
selecting, by the one or more processors, the candidate visual landmark with the highest overall score as the shared visual landmark.
3. The method of claim 2, further comprising:
generating, by the one or more processors, a first set of rankings for the candidate visual landmarks according to the first score;
generating, by the one or more processors, a second set of rankings for the candidate visual landmarks according to the second score;
wherein selecting the candidate visual landmark with the highest total score as the shared visual landmark comprises: selecting, by the one or more processors, the candidate visual landmark with the highest overall score as the shared visual landmark when the candidate visual landmark with the highest overall score is ranked above the threshold ranking in both the first set of rankings and the second set of rankings.
4. The method of claim 3, wherein the candidate visual landmark having the next highest overall score is selected by the one or more processors as the shared visual landmark in response to determining that the candidate visual landmark having the highest overall score is not ranked above the threshold ranking in both the first and second group rankings and when the candidate visual landmark having the next highest overall score is ranked above the threshold ranking in both the first and second group rankings.
5. The method of claim 2, wherein retrieving a plurality of visual landmarks corresponding to physical objects within the geographic area comprises:
retrieving, by the one or more processors, the plurality of visual landmarks from the database, each visual landmark comprising one or more of:
i) an indication of a three-dimensional position of the visual landmark within the geographic area,
ii) an indication of an orientation of the visual landmark,
iii) an indication of a size of the visual landmark,
iv) an indication of a name of the visual landmark,
v) an indication of the appearance of the visual landmark, or
vi) an indication of a geographic area from which the visual landmark is visible.
6. The method of claim 5, wherein assigning a first score to the candidate visual landmark based on visibility from the first location comprises:
determining, by the one or more processors, visibility from the first location based on one or more of: an indication of a geographic area from which the visual landmark is visible, an indication of a three-dimensional position of the visual landmark, an indication of an orientation of the visual landmark, or an indication of a size of the visual landmark.
7. The method of claim 1, wherein receiving a request for a first user to meet a second user at a meeting location comprises:
receiving, by the one or more processors, a request from a first client device of the first user to transport the first user to an exit location;
sending, by the one or more processors, the request to the second user;
receiving, by the one or more processors, an indication from a second client device of the second user to accept the request; and
identifying, by the one or more processors, a pick-up location of the first user as the meeting location.
8. The method of claim 7, wherein providing directions to the meeting location comprises:
providing, by the one or more processors, walking directions to the first user that reference the shared visual landmark; and
providing, by the one or more processors, driving directions referencing the shared visual landmark to the second user.
9. A server device for navigating two or more users to a meeting location, the server device comprising:
one or more processors; and
a non-transitory computer-readable memory coupled to the one or more processors and storing instructions thereon that, when executed by the one or more processors, cause the server device to:
receiving a request for a first user to meet a second user at a meeting location, the request comprising a first location of the first user and a second location of the second user, the first location and the second location being within a geographic area;
retrieving a plurality of visual landmarks corresponding to physical objects within the geographic area from a database;
selecting a shared visual landmark from the plurality of visual landmarks used to direct the first user and the second user to meeting locations within the geographic area, the shared visual landmark selected based on visibility from both the first location and the second location; and
providing directions to the meeting location with reference to the shared visual landmark to the first user and the second user.
10. The server device of claim 9, wherein to select a shared visual landmark for directing the first user and the second user to the meeting location, the instructions cause the server device to:
identifying candidate visual landmarks within a threshold distance of the meeting location;
assigning a first score to the candidate visual landmark based on visibility from the first location;
assigning a second score to the candidate visual landmark based on visibility from the second location;
combining the first score and the second score to generate a total score for the candidate visual landmark; and
selecting the candidate visual landmark with the highest overall score as the shared visual landmark.
11. The server device of claim 10, wherein the instructions further cause the server device to:
generating a first set of rankings for the candidate visual landmarks according to the first score;
generating a second set of rankings for the candidate visual landmarks according to the second score;
wherein to select the candidate visual landmark with the highest overall score as the shared visual landmark, the instructions cause the server device to select the candidate visual landmark with the highest overall score as the shared visual landmark when the candidate visual landmark with the highest overall score is ranked above a threshold ranking in both the first set of rankings and the second set of rankings.
12. The server device of claim 11, wherein, in response to determining that the candidate visual landmark with the highest overall score is not ranked above the threshold ranking in both the first and second sets of ranks, the instructions cause the server device to select the candidate visual landmark with the next highest overall score as the shared visual landmark when the candidate visual landmark with the next highest overall score is ranked above the threshold ranking in both the first and second sets of ranks.
13. The server device of claim 10, wherein to retrieve a plurality of visual landmarks corresponding to physical objects within the geographic area, the instructions cause the server device to:
retrieving the plurality of visual landmarks from the database, each visual landmark including one or more of:
i) an indication of a three-dimensional position of the visual landmark within the geographic area,
ii) an indication of an orientation of the visual landmark,
iii) an indication of a size of the visual landmark,
iv) an indication of a name of the visual landmark,
v) an indication of the appearance of the visual landmark, or
vi) an indication of a geographic area from which the visual landmark is visible.
14. The server device of claim 13, wherein to assign a first score to the candidate visual landmark based on visibility from the first location, the instructions cause the server device to:
determining visibility from the first location based on one or more of: an indication of a geographic area from which the visual landmark is visible, an indication of a three-dimensional position of the visual landmark, an indication of an orientation of the visual landmark, or an indication of a size of the visual landmark.
15. The server device of claim 9, wherein to receive a request for a first user to meet a second user at a meeting location, the instructions cause the server device to:
receiving, from a first client device of the first user, a request to transport the first user to an exit location;
sending the request to the second user;
receiving, from a second client device of the second user, an indication to accept the request; and
identifying the meeting location as the pick-up location of the first user.
16. The server device of claim 15, wherein to provide directions to the meeting location, the instructions cause the server device to:
providing walking directions to the first user that reference the shared visual landmark; and
providing driving directions referencing the shared visual landmark to the second user.
17. A method for verifying a current location or orientation of a user using a landmark, the method comprising:
receiving, at one or more processors, location information and orientation information of a user of a client device from the client device;
retrieving, by the one or more processors, from a database, a plurality of visual landmarks corresponding to physical objects within a geographic area, the physical objects within the geographic area corresponding to the location information;
selecting, by the one or more processors, one or more of the plurality of visual landmarks within the field of view of the user based on the position information and orientation information of the user;
providing, by the one or more processors to the client device, an indication of the one or more visual landmarks and a request to confirm that the one or more visual landmarks are within the field of view of the user; and
in response to receiving an indication from the client device that the one or more visual landmarks are not within the user field of view:
receiving, at the one or more processors, a real-world image comprising a visual landmark from a field of view of the client device; and
determining, by the one or more processors, a location and an orientation of the user based on the real-world image.
18. The method of claim 17, wherein determining the user's location and orientation based on the real-world image comprises:
comparing, by the one or more processors, a visual landmark included in the real-world image to a plurality of visual landmarks from the database;
identifying, by the one or more processors, one or more of the plurality of visual landmarks from the database in the real-world image; and
determining, by the one or more processors, a position and an orientation of the user based on the one or more positions and one or more orientations of the one or more identified visual landmarks.
19. The method of claim 17, wherein determining the position and orientation of the user based on the one or more positions and one or more orientations of the one or more identified visual landmarks comprises:
determining, by the one or more processors, one or more locations of the one or more identified visual landmarks based on the one or more locations and based on one or more dimensions of the one or more identified visual landmarks in the real-world image; and
determining, by the one or more processors, the orientation as a direction substantially opposite the one or more directions of the one or more identified visual landmarks.
20. The method of claim 17, further comprising:
determining, by the one or more processors, a route from the user's location and orientation to a destination; and
providing, by the one or more processors, navigation instructions that guide the user along a route to the destination.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202310107990.8A CN116124173A (en) | 2019-05-24 | 2019-05-24 | Method and apparatus for navigating two or more users to meeting locations |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/033890 WO2020242434A1 (en) | 2019-05-24 | 2019-05-24 | Method and device for navigating two or more users to a meeting location |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310107990.8A Division CN116124173A (en) | 2019-05-24 | 2019-05-24 | Method and apparatus for navigating two or more users to meeting locations |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112789480A true CN112789480A (en) | 2021-05-11 |
CN112789480B CN112789480B (en) | 2023-01-03 |
Family
ID=66858022
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980064401.4A Active CN112789480B (en) | 2019-05-24 | 2019-05-24 | Method and apparatus for navigating two or more users to meeting location |
CN202310107990.8A Pending CN116124173A (en) | 2019-05-24 | 2019-05-24 | Method and apparatus for navigating two or more users to meeting locations |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310107990.8A Pending CN116124173A (en) | 2019-05-24 | 2019-05-24 | Method and apparatus for navigating two or more users to meeting locations |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220349719A1 (en) |
EP (3) | EP3827222B1 (en) |
JP (1) | JP7267409B2 (en) |
KR (1) | KR20220012212A (en) |
CN (2) | CN112789480B (en) |
WO (1) | WO2020242434A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2022116154A1 (en) * | 2020-12-04 | 2022-06-09 | 深圳市优必选科技股份有限公司 | Map library establishment method, computer device, and storage medium |
US20220316906A1 (en) * | 2021-04-03 | 2022-10-06 | Naver Corporation | Apparatus and Method for Generating Navigational Plans |
US20230202674A1 (en) * | 2021-12-23 | 2023-06-29 | OneSky Flight LLC | Dynamic aircraft-specific graphical user interfaces |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110191019A1 (en) * | 2010-01-29 | 2011-08-04 | Holsinger David J | Method of Operating a Navigation System |
CN103913174A (en) * | 2012-12-31 | 2014-07-09 | 深圳先进技术研究院 | Navigation information generation method and system, mobile client and server |
CN106403971A (en) * | 2016-08-25 | 2017-02-15 | 北京小米移动软件有限公司 | Information interaction method and device |
US20170059347A1 (en) * | 2015-08-28 | 2017-03-02 | Google Inc. | Determining Improved Pick-Up Locations |
US20170314954A1 (en) * | 2016-05-02 | 2017-11-02 | Google Inc. | Systems and Methods for Using Real-Time Imagery in Navigation |
US20180266845A1 (en) * | 2016-06-06 | 2018-09-20 | Uber Technologies, Inc. | User-specific landmarks for navigation systems |
US20180356239A1 (en) * | 2017-06-13 | 2018-12-13 | Gt Gettaxi Limited | System and method for navigating drivers to dynamically selected drop-off locations for shared rides |
CN109040960A (en) * | 2018-08-27 | 2018-12-18 | 优视科技新加坡有限公司 | A kind of method and apparatus for realizing location-based service |
CN109357673A (en) * | 2018-10-30 | 2019-02-19 | 上海仝物云计算有限公司 | Vision navigation method and device based on image |
Family Cites Families (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5144685A (en) * | 1989-03-31 | 1992-09-01 | Honeywell Inc. | Landmark recognition for autonomous mobile robots |
JP2986100B1 (en) * | 1999-01-11 | 1999-12-06 | 大倉電気株式会社 | Voice guidance system for PHS terminal position |
JP3749821B2 (en) * | 1999-09-30 | 2006-03-01 | 株式会社東芝 | Pedestrian road guidance system and pedestrian road guidance method |
JP4037866B2 (en) * | 2002-07-25 | 2008-01-23 | 富士通株式会社 | POSITION ESTIMATION DEVICE FOR MOBILE BODY, POSITION ESTIMATION METHOD, AND POSITION ESTIMATION PROGRAM |
JP2007192707A (en) | 2006-01-20 | 2007-08-02 | Fujitsu Ten Ltd | Information processor and computer-executable program |
JP4717650B2 (en) * | 2006-02-02 | 2011-07-06 | 株式会社ゼンリン | Current position estimation device |
TWI387728B (en) * | 2009-10-13 | 2013-03-01 | Chunghwa Telecom Co Ltd | Instantly get the method and system of team location |
US8463543B2 (en) * | 2010-02-05 | 2013-06-11 | Apple Inc. | Schematic maps |
KR101667715B1 (en) * | 2010-06-08 | 2016-10-19 | 엘지전자 주식회사 | Method for providing route guide using augmented reality and mobile terminal using this method |
US8880333B2 (en) * | 2010-11-02 | 2014-11-04 | Here Global B.V. | Effective slope for fuel consumption calculation |
WO2013059730A1 (en) * | 2011-10-21 | 2013-04-25 | Qualcomm Incorporated | Methods for generating visibility maps |
US10068157B2 (en) * | 2012-05-10 | 2018-09-04 | Apple Inc. | Automatic detection of noteworthy locations |
US8848983B1 (en) * | 2012-05-31 | 2014-09-30 | Google Inc. | System and method for ranking geographic features using viewshed analysis |
US9030499B2 (en) * | 2012-08-20 | 2015-05-12 | Google Inc. | Custom labeling of a map based on content |
US20140172570A1 (en) * | 2012-12-14 | 2014-06-19 | Blaise Aguera y Arcas | Mobile and augmented-reality advertisements using device imaging |
US9529907B2 (en) * | 2012-12-31 | 2016-12-27 | Google Inc. | Hold back and real time ranking of results in a streaming matching system |
US9625612B2 (en) * | 2013-09-09 | 2017-04-18 | Google Inc. | Landmark identification from point cloud generated from geographic imagery data |
US11026046B2 (en) * | 2014-04-11 | 2021-06-01 | Flaregun Inc. | Apparatus, systems and methods for visually connecting people |
US9551586B2 (en) * | 2015-06-12 | 2017-01-24 | Uber Technologies, Inc. | System and method for providing contextual information for a location |
US9762601B2 (en) * | 2015-06-17 | 2017-09-12 | Uber Technologies, Inc. | Trip anomaly detection system |
US10156452B2 (en) * | 2016-11-14 | 2018-12-18 | Conduent Business Service, Llc | Method and system for ridesharing management |
US10818188B2 (en) * | 2016-12-13 | 2020-10-27 | Direct Current Capital LLC | Method for dispatching a vehicle to a user's location |
US10262464B2 (en) * | 2016-12-30 | 2019-04-16 | Intel Corporation | Dynamic, local augmented reality landmarks |
EP3364644A1 (en) * | 2017-02-20 | 2018-08-22 | Koninklijke Philips N.V. | Image capturing |
US10222221B2 (en) * | 2017-02-21 | 2019-03-05 | Conduent Business Services, Llc | System and method for optimizing passenger pick-up |
US11379502B2 (en) * | 2018-11-09 | 2022-07-05 | Uber Technologies, Inc. | Place visibility scoring system |
US11527060B2 (en) * | 2019-01-23 | 2022-12-13 | Uber Technologies, Inc. | Location determination service based on user-sourced image updates |
-
2019
- 2019-05-24 KR KR1020217007735A patent/KR20220012212A/en unknown
- 2019-05-24 CN CN201980064401.4A patent/CN112789480B/en active Active
- 2019-05-24 US US16/609,037 patent/US20220349719A1/en active Pending
- 2019-05-24 EP EP19730632.7A patent/EP3827222B1/en active Active
- 2019-05-24 EP EP22168896.3A patent/EP4089370B1/en active Active
- 2019-05-24 JP JP2021517803A patent/JP7267409B2/en active Active
- 2019-05-24 WO PCT/US2019/033890 patent/WO2020242434A1/en unknown
- 2019-05-24 CN CN202310107990.8A patent/CN116124173A/en active Pending
- 2019-05-24 EP EP23215918.6A patent/EP4332506A3/en active Pending
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110191019A1 (en) * | 2010-01-29 | 2011-08-04 | Holsinger David J | Method of Operating a Navigation System |
CN103913174A (en) * | 2012-12-31 | 2014-07-09 | 深圳先进技术研究院 | Navigation information generation method and system, mobile client and server |
US20170059347A1 (en) * | 2015-08-28 | 2017-03-02 | Google Inc. | Determining Improved Pick-Up Locations |
US20170314954A1 (en) * | 2016-05-02 | 2017-11-02 | Google Inc. | Systems and Methods for Using Real-Time Imagery in Navigation |
CN109073404A (en) * | 2016-05-02 | 2018-12-21 | 谷歌有限责任公司 | For the system and method based on terrestrial reference and real time image generation navigation direction |
US20180266845A1 (en) * | 2016-06-06 | 2018-09-20 | Uber Technologies, Inc. | User-specific landmarks for navigation systems |
CN106403971A (en) * | 2016-08-25 | 2017-02-15 | 北京小米移动软件有限公司 | Information interaction method and device |
US20180356239A1 (en) * | 2017-06-13 | 2018-12-13 | Gt Gettaxi Limited | System and method for navigating drivers to dynamically selected drop-off locations for shared rides |
CN109040960A (en) * | 2018-08-27 | 2018-12-18 | 优视科技新加坡有限公司 | A kind of method and apparatus for realizing location-based service |
CN109357673A (en) * | 2018-10-30 | 2019-02-19 | 上海仝物云计算有限公司 | Vision navigation method and device based on image |
Also Published As
Publication number | Publication date |
---|---|
EP3827222A1 (en) | 2021-06-02 |
US20220349719A1 (en) | 2022-11-03 |
EP3827222B1 (en) | 2022-07-06 |
JP7267409B2 (en) | 2023-05-01 |
EP4332506A2 (en) | 2024-03-06 |
KR20220012212A (en) | 2022-02-03 |
EP4332506A3 (en) | 2024-03-20 |
WO2020242434A1 (en) | 2020-12-03 |
EP4089370A1 (en) | 2022-11-16 |
JP2023106379A (en) | 2023-08-01 |
CN112789480B (en) | 2023-01-03 |
CN116124173A (en) | 2023-05-16 |
EP4089370B1 (en) | 2023-12-27 |
JP2022533869A (en) | 2022-07-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11692842B2 (en) | Augmented reality maps | |
US11604077B2 (en) | Systems and method for using visual landmarks in initial navigation | |
CN112789480B (en) | Method and apparatus for navigating two or more users to meeting location | |
JP7485824B2 (en) | Method, computer device, and computer readable memory for verifying a user's current location or orientation using landmarks - Patents.com | |
US20230175854A1 (en) | Explicit Signage Visibility Cues in Driving Navigation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |