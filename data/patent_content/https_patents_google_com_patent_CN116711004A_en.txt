CN116711004A - Automatic assistant execution of non-assistant application operations in response to user input capable of limiting parameters - Google Patents
Automatic assistant execution of non-assistant application operations in response to user input capable of limiting parameters Download PDFInfo
- Publication number
- CN116711004A CN116711004A CN202180088331.3A CN202180088331A CN116711004A CN 116711004 A CN116711004 A CN 116711004A CN 202180088331 A CN202180088331 A CN 202180088331A CN 116711004 A CN116711004 A CN 116711004A
- Authority
- CN
- China
- Prior art keywords
- application
- assistant
- user
- automated assistant
- computing device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000004044 response Effects 0.000 title claims abstract description 32
- 230000000670 limiting effect Effects 0.000 title description 2
- 238000000034 method Methods 0.000 claims description 82
- 230000003993 interaction Effects 0.000 claims description 14
- 238000009877 rendering Methods 0.000 claims description 9
- 238000004590 computer program Methods 0.000 claims 1
- 230000009471 action Effects 0.000 abstract description 24
- 230000000875 corresponding effect Effects 0.000 abstract 1
- 230000008569 process Effects 0.000 description 18
- 238000012545 processing Methods 0.000 description 16
- 238000010801 machine learning Methods 0.000 description 10
- 230000004927 fusion Effects 0.000 description 8
- 238000001514 detection method Methods 0.000 description 6
- 230000006870 function Effects 0.000 description 6
- 238000012549 training Methods 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 238000012552 review Methods 0.000 description 5
- 239000003795 chemical substances by application Substances 0.000 description 4
- 230000015654 memory Effects 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 230000002093 peripheral effect Effects 0.000 description 3
- 238000010079 rubber tapping Methods 0.000 description 3
- 230000001629 suppression Effects 0.000 description 3
- 238000010586 diagram Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000002452 interceptive effect Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 239000000203 mixture Substances 0.000 description 2
- 230000036961 partial effect Effects 0.000 description 2
- 230000002829 reductive effect Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 238000004378 air conditioning Methods 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 230000001747 exhibiting effect Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000010438 heat treatment Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 238000009423 ventilation Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
Abstract
Implementations described herein relate to an automated assistant capable of providing selectable action intent suggestions when a user is accessing a third party application controllable via the automated assistant. The user can initialize the action intent without explicitly invoking an automated assistant using, for example, a call phrase (e.g., "Assistant."). Instead, the user is able to initiate execution of the corresponding action by identifying one or more action parameters. In some implementations, the selectable suggestion can indicate that the microphone is active so that the user provides a spoken utterance of the recognition parameters. When initializing an action intent in response to a spoken utterance from a user, the automated assistant can control a third party application in accordance with the action intent and any identified parameters.
Description
Background
Humans may participate in human-machine conversations with interactive software applications referred to herein as "automated assistants" (also referred to as "digital agents," "chat robots," "interactive personal assistants," "intelligent personal assistants," "conversation agents," etc.). For example, humans (which may be referred to as "users" when they interact with an automated assistant) may provide commands and/or requests using spoken natural language input (i.e., utterances) and/or by providing text (i.e., typed) natural language input, which may in some cases be converted to text and then processed.
In some instances, the automated assistant can provide various features that can be initialized even when a user is interacting with a separate application in the foreground of his computing device. For example, a user can utilize an automated assistant to perform searches within a separate foreground application. For example, in response to a spoken utterance of "Search for Asian fusion (search for Asian fusion)" to the automated assistant when the separate restaurant review application is in the foreground, the automated assistant can interact (e.g., directly and/or via simulated user input) with the foreground application to cause a search for "Asian fusion" to be submitted using the search interface of the restaurant review application. Also, for example, in response to a spoken utterance of "Add acalendar entry of patent meeting for 2:00tomorrow (adding calendar entry for patent meeting of tomorrow 2:00)" for an automated assistant when a separate calendar application is in the foreground, the automated assistant can interact with the foreground application so that a calendar entry for "tomorrow" 2:00 and titled "patent meeting" is created.
However, when utilizing an automated assistant to interact with a foreground application, the user must specify both an intent (e.g., "search for" or "add a calendar entry (add calendar entry)" in the previous example) and a parameter of intent (e.g., "Asian fusion" or "patent meeting for 2:00tomorrow (patent conference of tomorrow 2:00)") in the spoken utterance. Also, in some cases, the user may have to provide a spoken phrase for the automated assistant, or other automated assistant invocation input before providing the spoken utterance.
Furthermore, in some cases, the user may not be aware of the ability of the automated assistant to interact with the foreground application as desired by the user and in response to the user's spoken utterance. Thus, the user may instead utilize a greater amount of input when interacting directly with the application and/or utilize a longer duration input for the application when interacting directly with the application. For example, assume that a user does not know that speaking "add a calendar entry of patent meeting for 2:00tomorrow (adding calendar entries for patent meetings of tomorrow 2:00)" to an automated assistant would cause the automated assistant to interact with a calendar application as described above. In such a case, to add a corresponding calendar entry at the calendar application, the user may instead have to locate and tap the "add calendar entry" interface element of the calendar application, which causes the entry interface of the calendar application to be presented—then click and populate (e.g., using a virtual keyboard and/or selection menu) the date, time, and title fields of the entry interface.
Furthermore, in some cases, the automated assistant may not be able to correctly ascertain whether the spoken utterance seeks to control the foreground application, or alternatively, seek a general automated assistant response that is independent of the foreground application and that is generated without any control over the foreground application. For example, assume again that when a separate restaurant review is applied in the foreground with a spoken utterance for "Search for Asian fusion (search asian fusion dish)" of the automated assistant. In such examples, it may not be clear whether the user is seeking to assist in searching for "Asian fusion" restaurants within the restaurant review application or alternatively wants the automated assistant to perform a general search (independent of the restaurant review application) and return a general description of what constitutes "Asian fusion" dishes.
Disclosure of Invention
Implementations set forth herein relate to an automated assistant that provides selectable GUI elements when a user is interacting with an application that can be controlled via the automated assistant. The selectable GUI elements can be rendered when the automated assistant determines that the application interface identifies an operation (e.g., search function) that can be initialized or otherwise controlled via the automated assistant. The selectable GUI elements can include content, such as text content and/or graphical content, that identifies the operation and/or solicits the user to provide one or more parameters for the operation. When the selectable GUI elements are rendered, the microphone and/or camera can be activated with prior permission from the user to enable the user to identify one or more operating parameters-without the user explicitly identifying an automated assistant or intent/operation. When the user has provided one or more operating parameters, the automated assistant can control the application to perform an operation (e.g., a search function) using the one or more operating parameters (e.g., search terms).
In these and other ways, interactions between the application and the automated assistant can be performed with reduced input and/or more concise user input. For example, the user's spoken utterance can specify only parameters of intent or operation, and not intent or operation. This results in a more concise spoken utterance and correspondingly reduces the processing of the spoken utterance by the automatic speech recognition component and/or other components. Also, the user need not provide an explicit invocation phrase (e.g., "Assistant. (Assistant.)") to further reduce the duration of the spoken utterance and the overall duration of the human/assistant interaction. Further, by the user's selection of GUI elements, the user's intent is clear, preventing the automated assistant from misinterpreting the spoken utterance as a general assistant request, rather than a request to the assistant to control a foreground application. Still further, through presentation of the GUI elements, the user will be aware of the ability to control the foreground application via spoken utterances for the automated assistant rather than more complex direct interactions with the foreground application, and/or the user will control the foreground application more frequently through spoken utterances (e.g., provided after selection of the GUI elements).
In some implementations, the automated assistant can determine whether the application interface includes features corresponding to respective operations compatible with the automated assistant and/or assistant operations. In some instances, a plurality of different compatible operations can be identified for the application interface, causing the automated assistant to render one or more selectable GUI elements to achieve the respective operations. The type of selectable GUI element rendered by the automated assistant may depend on the corresponding operation identified by the automated assistant. For example, when a user is accessing a home control application that includes dial GUI elements for controlling the temperature of a home, the automated assistant can render selectable GUI elements that identify command phrases for adjusting the temperature. In some implementations, the selectable GUI element can include text such as "set temperature to _____," which can indicate that the selectable GUI element corresponds to an operation for setting the temperature of the home.
The blank or placeholder region (e.g., "____") of the selectable GUI element can solicit the user and/or otherwise provide an indication that the user can provide a spoken utterance or other input to identify parameters for completing a command phrase and/or initializing performance of a corresponding operation. For example, the user can tap on the selectable GUI element and/or subsequently provide a spoken utterance such as "65 degrees" to complete a command phrase set forth in the text of the selectable GUI element. In response to receiving the spoken utterance, the automated assistant can control the application to cause the temperature setting of the application to be adjusted to "65" degrees. In some implementations, the automated assistant can also cause an audio interface (e.g., one or more microphones) of the computing device to become active when the selectable GUI element is rendered by the automated assistant. Thus, rather than the user tapping on a selectable GUI element, the user is able to provide a spoken utterance of a recognition parameter value (e.g., "65 degres") without recognizing an operation to be performed (e.g., "change the temperature (change temperature)") and without recognizing an Assistant (e.g., "Assistant)").
In some implementations, the selectable GUI element can be rendered by the automated assistant in a foreground of a display interface of the computing device for a threshold duration. The duration can be selected according to one or more characteristics associated with interactions between the user and the application. For example, when the home screen of the application is rendered at the display interface and the user has not otherwise provided input to the application, the selectable GUI element can be rendered for a static duration (e.g., 3 seconds). However, while rendering the selectable GUI element above the application interface, when the user is interacting with the application (e.g., scrolling the application interface), the selectable GUI element can be rendered for a duration based on how often the user provides input to the application. Alternatively or additionally, the duration that the selectable GUI element is rendered may be based on the amount of time that the corresponding application interface element is being rendered at or expected to be presented at the application interface. For example, if the user typically provides an application input that transitions the application from the home screen to the login screen within a time t of viewing the home screen, then selectable GUI elements can be rendered over the home screen for a duration based on time t.
In some implementations, the selection of the type of selectable GUI element to render may be based on a heuristic process and/or one or more trained machine learning models. For example, an automated assistant and/or operating system of a computing device can process data associated with an application interface of an application to identify one or more operations that can be initialized via user interaction with the application interface. In the case of prior permissions from the user, the data can include a screenshot of the application interface, a link corresponding to a graphical element of the interface, library data and/or other functional data associated with the application and/or interface, and/or any other information that can indicate an operation that can be initialized via the application interface.
Depending on the one or more operations identified for the application interface, the automated assistant can select and/or generate selectable GUI elements corresponding to the respective operations. The selectable GUI elements can be selected to provide an indication that the corresponding operation can be controlled via the automated assistant. For example, the automated assistant can determine that a magnifier icon (e.g., a search icon) disposed over or adjacent to a blank text field (e.g., a search field) of the application interface can indicate that the application interface can control a search operation of the application. Based on this determination, the automated assistant can render a selectable GUI element (e.g., "search ______") that includes the same or different magnifying glass icons and/or that includes one or more natural language terms that are synonymous with the term "search". In some implementations, when the selectable GUI element, the user can select the selectable GUI element by providing a spoken utterance (e.g., "Nearby restaurant (nearby restaurant)) that identifies the search parameter or by tapping the selectable GUI element and then providing a spoken utterance that identifies the search parameter.
In some implementations, in the event of a prior permission from the user, a microphone of the computing device that is rendering the selectable GUI element can remain active after the user selects the selectable GUI element. Alternatively or additionally, the automated assistant can select a different selectable GUI element to render when the application interface changes in response to selection of the selectable GUI element and/or the spoken utterance. The automated assistant can select another selectable GUI element to render based on the next application interface to which the application transitions. For example, when a user issues search parameters for selectable GUI elements, the application can render a list of search results. Search results from the list of search results can be selectable by a user to cause the application to perform a particular operation. The automated assistant can determine that a particular operation is compatible with the assistant operation (e.g., an operation that can be performed by the automated assistant) and cause another selectable GUI element (e.g., a hand with an index finger extending toward the corresponding search result) to be rendered over or adjacent to the corresponding search result. Alternatively or additionally, other optional GUI elements can include, for example, a text string (e.g., "Time Four Thai Restaurant (Time fourier tay)") that identifies a term corresponding to the search result. When the user provides a spoken utterance (e.g., "Thai resultaant") that includes one or more terms that identify the corresponding search result, the automated assistant can cause the corresponding search result to be selected without the user having to explicitly identify a "select" operation or automated assistant. In this way, as the automated assistant continues to identify compatible operations at each interface of the application, the user is able to navigate the interface by providing parameter values (e.g., "central reservants..thai reservants..menu..)". In some implementations, the user can initially begin interaction by automatically assisting in opening a particular application (e.g., "Assistant, open my recipe application.)") via a first oral command. Subsequently, when the automated assistant recognizes compatible application operations, the user can provide another command via the second spoken utterance so that the automated assistant controls the particular application in accordance with the parameters (e.g., the user can recite "Pad Thai" so as to have the automated assistant search for "Pad Thai" in their recipe application). Thereafter, the user can continue to use these brief spoken utterances to navigate the particular application at least when the automated assistant recognizes one or more application operations as compatible and/or controllable via one or more corresponding automated assistant operations.
The above description is provided as an overview of some implementations of the present disclosure. Further description of these and other implementations are described in more detail below.
Other implementations may include a non-transitory computer-readable storage medium storing instructions executable by one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU), and/or a Tensor Processing Unit (TPU)) to perform a method, such as one or more methods described above and/or elsewhere herein. Other implementations may include a system of one or more computers including one or more processors operable to execute stored instructions to perform methods such as one or more of the methods described above and/or elsewhere herein.
It should be understood that all combinations of the above concepts and other concepts described in more detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
Drawings
1A, 1B, and 1C illustrate views of a user interacting with an automated assistant that recognizes and suggests assistant-compatible operations at a third party application interface.
Fig. 2A, 2B, and 2C illustrate views of a user interacting with an application providing one or more operations that can be controlled by an automated assistant.
FIG. 3 illustrates a system with an automated assistant that can provide selectable action intent suggestions including an indication that a user should provide one or more parameters to control an operation when the user is accessing a third party application controllable via the automated assistant.
FIG. 4 illustrates a method for providing selectable GUI elements at an interface of a computing device via which an application operation compatible with an automated assistant is executable.
FIG. 5 is a block diagram of an example computer system.
Detailed Description
1A, 1B, and 1C illustrate views 100, 120, and 140, respectively, of a user 102 interacting with an automated assistant that recognizes and suggests assistant-compatible operations at a third party application interface. When an assistant compatible operation is detected at the application interface, the automated assistant can provide an indication that the user can provide one or more parameters to initiate execution of the compatible operation. In this manner, the user is able to bypass providing assistant invocation phrases and/or other inputs that might otherwise be necessary to control an automated assistant and/or third party application.
For example, the user 102 may be interacting with a device supporting an assistant, such as the computing device 104, in order to change the settings of the home control application 108. Initially, the user 102 is able to initialize the home control application 108 in response to the user input 106, which may be a touch input provided by the hand 116 of the user 102. When the home control application 108 is launched, the user 102 is able to select to control certain devices within the user's 102 home, such as the home's heating, ventilation, and air conditioning (HVAC) system. To control the HVAC system, the home control application 108 can render an application interface 110 that can include a thermostat GUI 112.
While the application interface 110 is being rendered at the display interface 118 of the computing device 104, the automated assistant is able to identify one or more assistant-compatible operations 114. When the assistant compatible operation is identified as being associated with a selectable GUI element, the automated assistant can cause one or more graphical elements to be rendered at the display interface 118. For example, and as provided in view 120 of fig. 1B, the automated assistant can cause selectable GUI elements 122 and/or suggestion elements 126 to be rendered at display interface 118. In some implementations, the selectable GUI elements 122 can be rendered in the foreground and/or above the home control application 108. The selectable GUI element 122 can provide an indication to the user that the automated assistant is currently initialized and that a phrase need not be invoked while the selectable GUI element 122 is being rendered. Alternatively or additionally, the suggestion elements 126 can provide natural language content that characterizes spoken utterances that can be provided to an automated assistant for controlling the home control application 108. Further, the presence of the suggestion element 126 can indicate that the automated assistant is initialized and does not necessarily call a phrase before confirming spoken input. Alternatively or additionally, the natural language content of the suggestion element 126 can include at least a portion of the suggested spoken utterance and also include white space, which may be a placeholder for one or more parameters of the assistant-compatible operation.
For example, and as provided in fig. 1B, the automated assistant can cause the please selectable GUI element 122 to be rendered over the home control application 108 in order to indicate that the automated assistant can receive temperature input. In some implementations, an icon corresponding to an assistant compatible operation can be selected for rendering with the selectable GUI element 122. Selection of the icon may be based on parameters that can be provided by the user 102 to control the thermostat GUI 112. For example, a thermometer icon can be selected to indicate that a user can specify a value for temperature in order to control the application interface 110 and adjust the thermostat GUI 112. The user 102 can then provide a spoken utterance 124, such as "65 degrees", indicating to the automated assistant that the user wishes the automated assistant to modify the temperature setting of the home control application 108 from 72 degrees to 65 degrees.
In response to the spoken utterance 124, the automated assistant can initiate an assistant compatible operation 144. For example, the automated assistant can generate a request to the home control application 108 to modify the current setting of the thermostat from 72 degrees to 65 degrees. In some implementations, an Application Programming Interface (API) can be used to interface between the automated assistant and the home control application 108. The home control application 108 is able to process requests from the automated assistant and modify the settings of the thermostat accordingly. Additionally, the updated thermostat GUI 142 can be rendered as an update 146 at the application interface 110 to indicate to the user 102 that the automated assistant and home control application 108 successfully performed the operation. In some implementations, the selectable GUI element 122 and/or the suggested element 126 can be removed from the display interface 118 after the threshold duration has occurred and/or regardless of whether the user 102 interacted with the selectable GUI element 122 or the suggested element 126. For example, the selectable GUI element 122 can be rendered for a threshold duration from when the home control application 108 is initially rendered at the display interface 118. If the user 102 does not interact with the selectable GUI elements 122 for a threshold duration, the automated assistant can cause the selectable GUI elements 122 to no longer be rendered at the display interface 118 and/or provide notification that the selectable GUI elements 122 will be removed after a period of time.
Fig. 2A, 2B, and 2C illustrate views 200, 220, and 240, respectively, of a user 202 interacting with an application providing one or more operations that can be controlled by an automated assistant. For example, the user 202 can access the messaging application 208 using the computing device 204. In response to user 202 launching messaging application 208, an automated assistant accessible via computing device 204 can determine whether features of messaging application 208 can be controlled by the automated assistant. In some implementations, the automated assistant can determine that one or more operations can be performed using the particular parameters identified by the user 202. For example, the automated assistant can determine that in order to reply to a message, the check box 212 must be selected for the particular message, and then select the reply icon 218. In some implementations, the automated assistant can make this determination based on a heuristic process and/or one or more trained machine learning models. For example, training data for one or more trained machine learning models can characterize an instance of selecting reply icon 218 if check box 212 is not selected, as well as other instances of selecting reply icon 218 if check box 212 is selected. In some implementations, one or more trained machine learning models can be used to process screenshots and/or other images of an application interface in the event of a prior permission from a user in order to cause certain selectable GUI elements and/or selectable suggestions to be rendered at computing device 204.
When the computing device 204 has initialized the messaging application 208 in response to the user input 206, the automated assistant can identify assistant-compatible operations available via the application interface 210 of the messaging application 208. When one or more assistant-compatible operations are identified, the automated assistant can cause one or more selectable GUI elements 222 and/or one or more selectable suggestions 224 to be rendered 214 at the computing device 204. The selectable GUI element 222 can be rendered to indicate that the automated assistant and/or audio interface has been initialized and the user 202 can identify a parameter to cause the automated assistant to use to control the messaging application 208.
For example, the selectable GUI element 222 can include a graphical representation of a person or contact, indicating that the user 202 should identify the name of the person to whom the user 202 wants to send a message. Alternatively or additionally, the selectable suggestion 224 can include a text identifier and/or graphical content that identifies a command that can be issued to the automated assistant but that also omits one or more parameters. For example, the selectable suggestion 224 can include a phrase "reply to a message from _____," which can indicate that if the user 202 identifies a contact associated with a particular message, the automated assistant can reply to the message identified in the application interface 210. The user 202 can then provide a spoken utterance 226 that identifies parameters of the assistant-compatible operation. In response to the spoken utterance 226, the automated assistant can make a selection of a check box 212 corresponding to a parameter (e.g., "Linus") identified by the user 202. Additionally, in response to the spoken utterance 226, the automated assistant can select the reply icon 218 to cause the messaging application 208 to reply to a message from the contact identified by the user 202. Alternatively or additionally, the automated assistant can pass the API call to the messaging application 208 as a back-end process to initiate a reply to a message from the contact identified by the user 202.
In response to the spoken utterance 226 from the user 202, the automated assistant can cause the messaging application 208 to process a request to reply to a message from a contact (e.g., linus) identified by the user 202. When the messaging application 208 receives a request from an automated assistant, the messaging application 208 can render the updated application interface 248. Application interface 248 can correspond to a draft reply message that can be modified by user 202. The automated assistant can process the content of the application interface 248 and/or other data stored in association with the application interface 248 to determine whether to provide additional suggestions to the user 202. For example, the automated assistant can cause one or more additional selectable GUI elements 242 to be rendered in the foreground of the application interface 248. Selectable GUI elements 242 can be rendered at operation 244 to indicate to user 202 that the automated assistant is active and that user 202 can provide a spoken utterance detailing the composition of the reply message. For example, when the selectable GUI element 242 is being rendered, the user 202 can provide another spoken utterance 246, such as "Yeah, see there," for the composition of the message without explicitly identifying the operation and/or identifying the automated assistant.
In response, the automated assistant can pass another request to the messaging application 208 in order for the messaging application 208 to perform one or more operations for entering the text "Yeah, see" into the body of the message. Thereafter, the user 202 can provide another spoken utterance (e.g., "Send") for the automated assistant and the separate selectable GUI element 250. In this way, user 202 is able to cause messaging application 208 to send a message without explicitly identifying an automated assistant and without providing touch input to computing device 204 and messaging application 208. This can reduce the number of inputs that need to be provided directly from the user 202 to the third party application. Furthermore, user 202 will be able to rely on an automated assistant when interacting with most other applications that may not employ a trained machine learning model based on actual interaction training with user 202.
Fig. 3 illustrates a system 300 having an automated assistant 304 that can provide selectable action intent suggestions including an indication that a user should provide one or more parameters in order to initiate an action when the user is accessing a third party application controllable via the automated assistant. In the event that the user does not necessarily identify the automated assistant 304 or the third party application, one or more operations associated with the action intent can be initiated in response to the user providing input (e.g., a spoken utterance) identifying one or more parameters. The automated assistant 304 can operate as part of an assistant application provided at one or more computing devices, such as the computing device 302 and/or a server device. The user can interact with the automated assistant 304 via an assistant interface 320, which can be a microphone, a camera, a touch screen display, a user interface, and/or any other device that can provide an interface between the user and an application. For example, the user can initialize the automated assistant 304 by providing verbal input, textual input, and/or graphical input to the assistant interface 320 to cause the automated assistant 304 to initialize one or more actions (e.g., providing data, controlling peripherals, accessing agents, generating inputs and/or outputs, etc.). Alternatively, the automated assistant 304 can be initialized based on processing of the context data 336 using one or more trained machine learning models. The context data 336 can characterize one or more features of an environment accessible to the automated assistant 304 and/or one or more features of a user predicted to be intending to interact with the automated assistant 304. The computing device 302 can include a display device, which can be a display panel that includes a touch interface for receiving touch inputs and/or gestures to allow a user to control an application 334 of the computing device 302 via the touch interface. In some implementations, the computing device 302 may lack a display device, providing audible user interface output, rather than graphical user interface output. In addition, the computing device 302 can provide a user interface, such as a microphone, for receiving spoken natural language input from a user. In some implementations, the computing device 302 can include a touch interface, possibly lacking a camera, but can optionally include one or more other sensors.
The computing device 302 and/or other third party client devices are capable of communicating with the server device over a network such as the internet. Additionally, the computing device 302 and any other computing devices can communicate with each other over a Local Area Network (LAN), such as a Wi-Fi network. Computing device 302 is capable of offloading computing tasks to a server device in order to conserve computing resources at computing device 302. For example, the server device can host the automated assistant 304, and/or the computing device 302 can transmit input received at the one or more assistant interfaces 320 to the server device. However, in some implementations, the automated assistant 304 can be hosted at the computing device 302 and can execute various processes at the computing device 302 that can be associated with automated assistant operations.
In various implementations, all or less than all aspects of the automated assistant 304 can be implemented on the computing device 302. In some of those implementations, aspects of the automated assistant 304 are implemented via the computing device 302 and are capable of interfacing with a server device that is capable of implementing other aspects of the automated assistant 304. The server device can optionally serve multiple users and their associated assistant applications via multiple threads. In implementations where all or less than all aspects of the automated assistant 304 are implemented via the computing device 302, the automated assistant 304 may be an application separate from (e.g., installed "on top of") the operating system of the computing device 302—or alternatively, can be implemented directly by (e.g., considered to be an application of, but integrated with) the operating system of the computing device 302.
In some implementations, the automated assistant 304 can include an input processing engine 306 that can employ a plurality of different modules for processing inputs and/or outputs for the computing device 302 and/or server device. For example, the input processing engine 306 can include a speech processing engine 308 that can process audio data received at the assistant interface 320 to perform speech recognition and/or to identify text embodied in the audio data. The audio data can be transmitted from, for example, the computing device 302 to a server device in order to conserve computing resources at the computing device 302. Additionally or alternatively, the audio data can be processed exclusively at the computing device 302.
The process for converting audio data to text can include a speech recognition algorithm, which can employ a neural network, and/or a statistical model for identifying the set of audio data corresponding to the word or phrase. Text converted from the audio data can be parsed by the data parsing engine 310 and provided as text data to the automated assistant 304, which can be used to generate and/or identify command phrases, intents, actions, slot values, and/or any other content specified by a user. In some implementations, the output data provided by the data parsing engine 310 can be provided to the parameter engine 312 to determine whether the user provides input corresponding to a particular intent, action, and/or routine that can be performed by the automated assistant 304 and/or an application or agent that can be accessed via the automated assistant 304. For example, the assistant data 338 can be stored at the server device and/or the computing device 302 and can include data defining one or more actions that can be performed by the automated assistant 304, as well as parameters necessary to perform the actions. The parameter engine 312 can generate one or more parameters for the intent, action, and/or slot values and provide the one or more parameters to the output generation engine 314. The output generation engine 314 can use one or more parameters to communicate with the assistant interface 320 to provide output to a user and/or with one or more applications 334 to provide output to one or more applications 334.
In some implementations, the automated assistant 304 may be an application that can be installed "on top of" the operating system of the computing device 302, and/or itself can form part (or all) of the operating system of the computing device 302. The automated assistant application includes and/or has access to on-device speech recognition, on-device natural language understanding, and on-device execution. For example, on-device speech recognition can be performed using an on-device speech recognition module that processes audio data (detected by a microphone) using an end-to-end speech recognition machine learning model stored locally at the computing device 302. On-device speech recognition generates recognition text for spoken utterances (if any) present in the audio data. Also, for example, on-device Natural Language Understanding (NLU) can be performed using an on-device NLU module that processes recognition text and optionally contextual data generated using on-device speech recognition to generate NLU data.
The NLU data can include parameters (e.g., slot values) corresponding to the intent, and optionally the intent, of the spoken utterance. The on-device execution can be performed using an on-device execution module that utilizes NLU data (from the on-device NLU) and optionally other local data to determine actions to be taken to resolve the intent (and optionally parameters of the intent) of the spoken utterance. This can include determining a local response and/or a remote response (e.g., answer) to the spoken utterance, interactions with locally installed applications to be performed based on the spoken utterance, commands to be transmitted to an internet of things (IoT) device (directly or via a corresponding remote system) based on the spoken utterance, and/or other resolution actions to be performed based on the spoken utterance. The execution on the device can then initiate local execution/execution and/or remote execution/execution of the determined action to resolve the spoken utterance.
In various implementations, at least remote speech processing, remote NLU, and/or remote implementation can be selectively utilized. For example, the recognition text can be at least selectively transferred to a remote automated assistant component for remote NLU and/or remote enforcement. For example, the recognition text can optionally be transmitted for remote execution in parallel with on-device execution or in response to failure of on-device NLU and/or on-device execution. However, on-device speech processing, on-device NLU, on-device execution, and/or on-device execution may be prioritized at least because of the reduced latency they provide in resolving spoken utterances (due to the lack of client-server roundtrips to resolve spoken utterances). Furthermore, on-device functions may be the only functions available without or with limited network connections.
In some implementations, the computing device 302 can include one or more applications 334 that can be provided by a third party entity that is different from the entity that provided the computing device 302 and/or the automated assistant 304. The application state engine of the automated assistant 304 and/or the computing device 302 can access the application data 330 to determine one or more actions that can be performed by the one or more applications 334, as well as a state of each of the one or more applications 334 and/or a state of a respective device associated with the computing device 302. The device state engine of the automated assistant 304 and/or the computing device 302 can access the device data 332 to determine one or more actions that can be performed by the computing device 302 and/or one or more devices associated with the computing device 302. In addition, the application data 330 and/or any other data (e.g., the device data 332) can be accessed by the automated assistant 304 to generate context data 336 that can characterize the context in which the particular application 334 and/or device is running, and/or the context in which the particular user is accessing the computing device 302, accessing the application 334, and/or any other device or module.
While one or more applications 334 are running at the computing device 302, the device data 332 can characterize the current running state of each application 334 running at the computing device 302. Further, the application data 330 can characterize one or more features of the running application 334, such as content of one or more graphical user interfaces being rendered under the direction of the one or more applications 334. Alternatively or additionally, the application data 330 can characterize an action pattern that can be updated by the respective application and/or by the automated assistant 304 based on the current running state of the respective application. Alternatively or additionally, one or more action patterns of one or more applications 334 can remain static, but can be accessed by the application state engine to determine appropriate actions to initialize via the automated assistant 304.
Computing device 302 can further include an assistant invocation engine 322 that can process application data 330, device data 332, context data 336, and/or any other data that can be utilized by computing device 302 using one or more trained machine learning models. The assistant invocation engine 322 can process this data to determine whether to wait for the user to explicitly speak the invocation phrase to invoke the automated assistant 304, or to consider the data to indicate the user's intent to invoke the automated assistant-instead of requiring the user to explicitly speak the invocation phrase. For example, one or more trained machine learning models can be trained using instances of training data based on a scene of a user located in an environment in which multiple devices and/or applications are exhibiting various operating states. Instances of training data can be generated to capture training data characterizing a user's context of invoking an automated assistant and other contexts in which the user does not invoke an automated assistant.
When one or more trained machine learning models are trained from these instances of training data, the assistant invocation engine 322 can cause the automated assistant 304 to detect or limit the detection of spoken invocation phrases from the user based on characteristics of the context and/or environment. Additionally or alternatively, the assistant invocation engine 322 can cause the automated assistant 304 to detect or limit detection of one or more assistant commands from the user based on characteristics of the context and/or environment. In some implementations, the assistant invocation engine 322 can be disabled or restricted based on the computing device 302 detecting an assistant suppression output from another computing device. In this way, when the computing device 302 is detecting an assistant suppression output, the automatic assistant 304 will not be invoked based on the context data 336—if no assistant suppression output is detected, this would otherwise cause the automatic assistant 304 to be invoked.
In some implementations, the system 300 can include an operation detection engine 316 that can identify one or more operations that can be executed by the application 334 and controlled via the automated assistant 304. For example, the operation detection engine 316 can process the application data 330 and/or the device data 332 to determine whether the application is running at the computing device 302. The automated assistant 304 can determine whether an application can be controlled via the automated assistant 304 and can identify one or more application operations that can be controlled via the automated assistant 304. For example, the application data 330 identifies one or more application GUI elements being rendered at an interface of the computing device 302, and the application data 330 can be processed to identify one or more operations that can be controlled via the application GUI elements. When an operation is identified as compatible with the automated assistant 304, the operation detection engine 316 can communicate with the GUI element content engine 318 to generate a selectable GUI element corresponding to the operation.
The GUI element content engine 318 can identify one or more operations that the automated assistant 304 has determined to be compatible with the automated assistant 304 and generate one or more corresponding selectable GUI elements based on the one or more operations. For example, when the search icon and/or search text field is determined to be available by the application and the application search operation is compatible with the automated assistant 304, the GUI element content engine 318 can generate content for rendering at the display interface of the computing device 302. The content can include text content (e.g., natural language content) and/or graphical content that can be based on compatible operations (e.g., application search operations). In some implementations, command phrases can be generated and rendered that instruct the automated assistant 304 to initialize operations in order to make the user aware of compatible operations that have been identified. Alternatively or additionally, the command phrase may be a partial command phrase that omits one or more parameters of the operation, thereby indicating to the user that the user is able to provide the one or more parameters to the automated assistant 304 for initializing the operation. Text content and/or graphical content can be rendered at the display interface of the computing device 302 concurrently with the application rendering of one or more additional GUI elements. The user can initiate performance of the compatible operation by tapping the display interface to select a selectable GUI element and/or providing a spoken utterance specifying one or more parameters to the automated assistant 304.
In some implementations, the system 300 can include a GUI element duration engine 326 that can control the duration of rendering of selectable GUI elements at the display interface by the automated assistant 304. In some implementations, the amount of time that the selectable GUI element is rendered may be based on an amount of interaction between the user and the automated assistant 304, and/or an amount of interaction between the user and an application associated with the selectable GUI element. For example, the GUI element duration engine 326 can establish a longer duration of display time for the selectable GUI element when the user has not yet provided input to the application since the selectable GUI element was rendered. When the user has provided input to an application (not the automated assistant 304), this longer duration may be longer relative to the duration of the display time of the selectable GUI element being rendered. Alternatively or additionally, the duration of the display time for the selectable GUI element may be longer for the selectable GUI element with which the user has previously interacted in the past. This longer duration may be relative to the duration of other selectable GUI elements that have been previously presented to the user but with which the user has not previously interacted or otherwise expressed interest.
In some implementations, the system 300 can include an operations execution engine 324 that can initialize one or more operations of the identified application in response to one or more parameters of the user identification operation. For example, while the selectable GUI element is being rendered over the application interface by the automated assistant 304, the user can select the selectable GUI element and/or provide a spoken utterance that identifies a parameter. The operation execution engine 324 is then able to process the selection and/or the spoken utterance and generate one or more requests for the application based on one or more parameters recognized by the user. For example, the spoken utterance processed by the input processing engine 306 can produce recognition of one or more particular parameter values. The parameter values can be used by the operation execution engine 324 to generate one or more requests for applications corresponding to user-identified selectable GUI elements. For example, the request generated by the automated assistant 304 can identify an operation to perform, one or more parameters identified by the automated assistant 304, and/or one or more parameters identified by a user. In some implementations, the automated assistant 304 can select one or more parameters for the operation, and the user can identify one or more additional parameters to have the operation initialized. For example, when the application is a travel reservation application, the automated assistant 304 can assume a date parameter (e.g., month "one month") and the user can specify the destination city via a spoken utterance (e.g., "Nairobi"). Based on this data and the corresponding selectable GUI element being rendered at the display interface, the operation execution engine 324 can generate a request (e.g., application. Travel. Com [ search. Setcity ("Nairobi"), search. Settime ("January") ]) for execution of the operation to initialize the travel reservation application. This request can be received by the travel reservation application from the automated assistant 304, and in response, the travel reservation application can render a different application interface that includes the results of the operation (e.g., a list of results of available hotels that are Roman in one month).
Fig. 4 illustrates a method 400 for providing selectable GUI elements at an interface of a computing device at which an application operation compatible with an automated assistant is executable. The method 400 can be performed by one or more applications, devices, and/or any other apparatus or module capable of interacting with an automated assistant. In some implementations, the method 400 can include an operation 402 of determining whether a non-assistant application is running at an interface of a computing device. The computing device can provide access to an automated assistant that can respond to natural language input from a user in order to control a plurality of different applications and/or devices. The automated assistant is able to process data indicating whether certain applications are running at the computing device. For example, data based on content being rendered at the interface can be processed by the automated assistant to identify application operations that can be initialized via the interface. Upon determining that the non-assistant application is running at an interface, such as a display interface of a computing device, the method 400 can proceed from operation 402 to operation 404. Otherwise, the automated assistant can continue to determine whether the application is running at the interface of the computing device.
Operation 404 can include determining whether the application operation is compatible with the automated assistant. In other words, the automated assistant can determine whether an operation that can be performed by the application can be controlled or otherwise initialized by the automated assistant. For example, when the application is a home control application and the application interface includes a control dial GUI, the automated assistant can determine that the operation controlled by the control dial GUI is compatible with one or more functions of the automated assistant. Thus, the automated assistant can operate to control the dial GUI and/or corresponding application operations. When the application operation is determined to be compatible with the automated assistant, the method 400 can proceed from operation 404 to operation 406. Otherwise, the automated assistant can continue to determine whether any other application operations are compatible with the automated assistant, or whether any other non-assistant applications are running at the computing device or a separate computing device.
Operation 406 can include causing selectable GUI elements to be rendered at the interface and also causing the audio interface to be active at the computing device. The selectable GUI element can provide an indication that the automated assistant is active for receiving one or more input parameters. In some implementations, the selectable GUI elements can include text content and/or graphical content based on the application operations identified at operation 404. In this way, at least while the selectable GUI is being rendered at the interface, the user can notice that the automated assistant can receive input identifying one or more parameters of the particular application operation. In some implementations, graphical content and/or text content of the selectable GUI element can indicate that the microphone is active for receiving user input from a user. For example, the selectable GUI element can have a dynamic attribute that indicates one or more sensor activities associated with the computing device. Alternatively or additionally, the text content of the selectable GUI element can identify one or more partial assistant command phrases that lack one or more respective parameters that should be identified for one or more respective application operations to be performed.
When the selectable GUI element is rendered at the interface, the method 400 can proceed from operation 406 to selectable operation 408, which includes determining whether the user provided a touch input or another input for the selectable GUI element. Upon determining that the user has provided input for the selectable GUI element, the method 400 can proceed from operation 408 to selectable operation 410. Operation 410 can include initializing detection of audio data corresponding to parameters of an application operation. For example, the automated assistant can identify one or more speech processing models for identifying one or more parameters associated with the application operation. In some instances, when the application operation includes one or more digits as potential parameters, a speech processing model for identifying digits of various sizes can be employed. Alternatively or additionally, a speech processing model for recognizing proper nouns in speech can be employed when the application operation includes one or more proper nouns as possible parameters.
From either operation 410 or operation 408, the method 400 can proceed to operation 412, which can include determining whether the user provided input parameters associated with the application operation to the automated assistant. For example, a user can provide input associated with an application operation by identifying a value of a control dial GUI. Alternatively or additionally, the user can provide input associated with the application operation by identifying one or more other values of one or more parameters that can be used as the application operation. For example, when the application operation is controllable via the control dial GUI of the application, the user can provide a spoken utterance such as "10percent (10%)" to the automated assistant. This spoken utterance can indicate that the user is specifying "10percent (10%)" as a parameter for the application operation, and the automated assistant should initialize the application operation based on this identified parameter. When an application operation corresponds to, for example, the brightness of a lamp in a user's home, the user specifying a value for a parameter can cause an automatic assistant to adjust the brightness of the lamp via an application (e.g., an IoT application that controls Wi-Fi enabled bulbs).
Upon determining that the user has provided input identifying one or more parameters of the application operation, the method 400 can proceed from operation 412 to operation 414. Operation 414 can include causing the automated assistant to control the non-assistant application in accordance with the input parameters identified by the user. For example, when a user provides a spoken utterance such as "10percent (10%)", the automated assistant can control the non-assistant application such that one or more lights associated with the non-assistant application are adjusted to a 10% brightness level. This can be performed without the user explicitly identifying assistant or non-assistant applications in the spoken utterance. This can conserve computing resources and limit the likelihood that certain disturbances (e.g., background noise) will affect the audio data captured by the automated assistant. When the user does not provide input identifying parameters for a threshold duration, the method 400 can proceed from operation 412 to operation 416, which can include causing the selectable GUI element to be removed from the interface after the threshold duration. From operation 414, the method 400 can proceed to operation 416, after which the method 400 can return to operation 402 or another operation.
Fig. 5 is a block diagram 500 of an example computer system 510. Computer system 510 typically includes at least one processor 514 that communicates with a number of peripheral devices via a bus subsystem 512. These peripheral devices may include a storage subsystem 524 (e.g., including a memory subsystem 525 and a file storage subsystem 526), a user interface output device 520, a user interface input device 522, and a network interface subsystem 516. Input devices and output devices allow users to interact with computer system 510. Network interface subsystem 516 provides an interface to external networks and couples to corresponding interface devices in other computer systems.
The user interface input device 522 may include a keyboard, a pointing device such as a mouse, trackball, touch pad, or graphics tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computer system 510 or onto a communication network.
The user interface output device 520 may include a display subsystem, printer, facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a viewable image. The display subsystem may also provide for non-visual display, such as via an audio output device. In general, the term "output device" is intended to include all possible types of devices and ways to output information from computer system 510 to a user, or to another machine or computer system.
Storage subsystem 524 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, the storage subsystem 524 may include logic for performing selected aspects of the method 400 and/or implementing one or more of the system 300, the computing device 104, the computing device 204, the automated assistant, and/or any other application, device, apparatus, and/or module discussed herein.
These software modules are typically executed by processor 514 alone or in combination with other processors. The memory 525 used in the storage subsystem 524 can include a number of memories including a main Random Access Memory (RAM) 530 for storing instructions and data during program execution and a Read Only Memory (ROM) 532 with fixed instructions stored. File storage subsystem 526 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical drive, or removable media cartridge. Modules implementing the functionality of certain implementations may be stored in storage subsystem 524 by file storage subsystem 526, or in other machines accessible to processor 514.
Bus subsystem 512 provides a mechanism for letting the various components and subsystems of computer system 510 communicate with each other as intended. Although bus subsystem 512 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
Computer system 510 may be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computer system 510 depicted in FIG. 5 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computer system 510 may have more or fewer components than the computer system depicted in FIG. 5.
In situations where the system described herein gathers personal information about a user (or "participant" as often referred to herein) or may utilize personal information, the user may be provided with an opportunity to control whether programs or features gather user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current geographic location) or whether and/or how to receive content from a content server that may be more relevant to the user. Also, certain data may be processed in one or more ways before being stored or used so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information cannot be determined for the user, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where geographic location information is obtained such that a particular geographic location of the user cannot be determined. Thus, the user may control how information is collected and/or used with respect to the user.
Although several implementations have been described and illustrated herein, various other means and/or structures for performing a function and/or obtaining a result and/or one or more advantages described herein may be utilized and each of such changes and/or modifications are considered to be within the scope of implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, without such features, systems, articles, materials, kits, and/or methods being inconsistent with each other, is included within the scope of the present disclosure.
In some implementations, a method implemented by one or more processors is set forth as including operations such as determining that an assistant operation is compatible with an application running at a computing device, where the application is separate from an automated assistant accessible via the computing device. The method can further include the operations of: selectable Graphical User Interface (GUI) elements are rendered at a display interface of the computing device based on the assistant operations being compatible with the application, wherein the selectable GUI elements identify the assistant operations and are rendered in a foreground of the display interface of the computing device. The method can further include the operations of: the selection of the selectable GUI element by the user via the display interface of the computing device is detected by the automated assistant. The method can further include the operations of: after selection of the selectable GUI element, speech recognition is performed on audio data that captures a spoken utterance provided by a user and received at an audio interface of the computing device, wherein the spoken utterance specifies a particular value for a parameter of an assistant operation without explicitly identifying the assistant operation. The method can further include the operations of: in response to a spoken utterance from a user, the automated assistant is caused to control an application based on the assistant operation and the particular value of the parameter.
In some implementations, causing the selectable GUI element to be rendered at a display interface of the computing device includes: generating content rendered with selectable GUI elements, wherein the content comprises: a text identifier or graphical representation of the assistant's operation, and a placeholder region indicating that the user is able to specify a value for the parameter. In some implementations, causing the selectable GUI element to be rendered at a display interface of the computing device includes: causing the selectable GUI element to be rendered over an application interface of the application for a threshold duration, wherein the threshold duration is based on an amount of interaction between the user and the application. In some implementations, the automated assistant does not respond to the spoken utterance when the user provides the spoken utterance after a threshold duration and no longer renders the selectable GUI element at the display interface of the computing device.
In some implementations, determining that the assistant operation is compatible with the application running at the computing device includes: it is determined that the additional selectable GUI elements being rendered at the application interface of the application correspond to application operations that can be executed in response to initializing the assistant operation. In some implementations, wherein determining that the assistant operation is compatible with the application running at the computing device includes: the determination that the additional selectable GUI element includes a search icon or search field and the application operation corresponds to a search operation. In some implementations, wherein causing the automated assistant to control the application based on the assistant operation and the particular value of the parameter includes: the application is caused by the automated assistant to provide search results based on the particular values of the parameters as specified in the spoken utterance from the user to the automated assistant.
In other implementations, a method implemented by one or more processors is set forth that includes, such as determining that a user has provided a first spoken utterance to an automated assistant accessible via a computing device, wherein the first spoken utterance includes a request to initialize an application separate from the automated assistant. The method can further include the operations of: in response to the first spoken utterance, the application is initialized and an application interface is rendered in a foreground of a display interface of the computing device, wherein the application interface includes content that identifies operations that are controllable via the automated assistant. The method can further include the operations of: the selectable GUI element is rendered over an application interface of the application based on the operation being controllable via the automated assistant, wherein the selectable GUI element includes a text identifier or a graphical representation of the operation capable of being controlled by the automated assistant. The method can further include the operations of: determining that the user has provided a second spoken utterance to the automated assistant, wherein the second spoken utterance identifies parameters that can be utilized by the application during operation of the operation, and wherein the second spoken utterance does not explicitly identify the operation. The method can further include the operations of: in response to the second spoken utterance, the automated assistant is caused to initiate execution of the operation via the application using parameters recognized in the second spoken utterance.
In some implementations, causing the selectable GUI element to be rendered over an application interface of the application includes: the text identifier is rendered with a command phrase that includes terms that identify the operation and white space that indicates that the user-identifiable parameter was omitted from the command phrase. In some implementations, the method can further include the operations of: the base operation is controllable via the automated assistant such that an audio interface of the computing device is initialized for receiving the specific spoken utterance from the user, wherein, when the audio interface is initialized, the user is able to provide the specific spoken utterance for controlling the automated assistant without explicitly recognizing the automated assistant. In some implementations, causing the selectable GUI element to be rendered over an application interface of the application includes: content presented with the selectable GUI elements is generated, wherein the content includes a graphical representation of assistant operations selectable via touch input to a display interface of the computing device.
In some implementations, causing the selectable GUI element to be rendered at an application interface of the application includes: causing the selectable GUI element to be rendered over the application interface of the application for a threshold duration, wherein the threshold duration is based on an amount of interaction between the user and the automated assistant since the selectable GUI element was rendered over the application interface. In some implementations, the automated assistant does not respond to the additional spoken utterance when the user provides the additional spoken utterance after rendering the selectable GUI element over the application interface no longer. In some implementations, the method can further include the operations of: the audio interface of the computing device is caused to be initialized for detecting another spoken utterance that identifies one or more parameters of the operation based on the operation being controllable via the automated assistant.
In still other implementations, a method implemented by one or more processors is set forth that includes operations such as determining that an assistant operation is compatible with an application running at a computing device, where the application is separate from an automated assistant accessible via the computing device. The method can further include the operations of: selectable Graphical User Interface (GUI) elements are rendered at a display interface of the computing device based on the assistant operations being compatible with the application, wherein the selectable GUI elements identify the assistant operations and are rendered in a foreground of the display interface of the computing device. The method can further include the operations of: while the selectable GUI element is being rendered at the display interface of the computing device, it is determined that the user has provided a spoken utterance for the automated assistant, wherein the spoken utterance specifies a particular value for a parameter of the assistant operation without explicitly identifying the assistant operation. The method can further include the operations of: in response to a spoken utterance from a user, the automated assistant is caused to control an application based on the assistant operation and the particular value of the parameter.
In some implementations, causing the selectable GUI element to be rendered at a display interface of the computing device includes: content rendered with selectable GUI elements is generated, wherein the content includes icons selected based on the assistant operation and selectable via touch input to a display interface of the computing device. In some implementations, causing the selectable GUI element to be rendered at a display interface of the computing device includes: content rendered with selectable GUI elements is generated, wherein the content includes natural language content characterizing a portion of the command phrase omitting one or more parameter values of the assistant operation. In some implementations, determining that the assistant operation is compatible with the application running at the computing device includes: it is determined that additional selectable GUI elements being rendered by the application control application operations that can be initialized by the automated assistant. In some implementations, causing the automated assistant to control the application based on the assistant operation and the particular value of the parameter includes: the application is caused to render another application interface generated by the application based on the particular value of the parameter. In some implementations, causing the selectable GUI element to be rendered at a display interface of the computing device includes: the selectable GUI elements are rendered simultaneously with one or more application GUI elements of the application rendering application.
Claims (22)
1. A method implemented by one or more processors, the method comprising:
determining that the assistant operation is compatible with the application being run at the computing device,
wherein the application is separate from an automated assistant accessible via the computing device;
causing selectable graphical user interface GUI elements to be rendered at a display interface of the computing device based on the assistant operation being compatible with the application,
wherein the selectable GUI element identifies the assistant operation and is rendered in a foreground of the display interface of the computing device;
detecting, by the automated assistant, a user selection of the selectable GUI element via the display interface of the computing device;
after selection of the selectable GUI element, performing speech recognition on audio data capturing a spoken utterance provided by the user and received at an audio interface of the computing device,
wherein the spoken utterance specifies a particular value of a parameter of the assistant operation without explicitly identifying the assistant operation; and
in response to the spoken utterance from the user, causing the automated assistant to control the application based on the assistant operation and a particular value of the parameter.
2. The method of claim 1, wherein causing the selectable GUI element to be rendered at the display interface of the computing device comprises:
generating content rendered with the selectable GUI element,
wherein the content comprises: a text identifier or graphical representation of the assistant operation indicates a placeholder region in which the user is able to specify a value for the parameter.
3. The method of claim 1 or claim 2, wherein causing the selectable GUI element to be rendered at the display interface of the computing device comprises:
causing the selectable GUI element to be rendered over an application interface of the application for a threshold duration,
wherein the threshold duration is based on an amount of interaction between the user and the application.
4. A method according to claim 3, wherein the automated assistant is not responsive to the spoken utterance when the user provides the spoken utterance after the threshold duration and no longer renders the selectable GUI element at the display interface of the computing device.
5. The method of any of the preceding claims, wherein determining that the assistant operation is compatible with the application being run at the computing device comprises:
It is determined that the additional selectable GUI elements being rendered at the application interface of the application correspond to application operations that are capable of being run in response to initializing the assistant operation.
6. The method according to claim 5,
wherein determining that the assistant operation is compatible with the application being run at the computing device comprises:
it is determined that the additional selectable GUI element includes a search icon or search field and the application operation corresponds to a search operation.
7. The method according to claim 6, wherein the method comprises,
wherein causing the automated assistant to control the application based on the assistant operation and the particular value of the parameter comprises:
the method further includes causing, by the automated assistant, the application to provide search results based on particular values of the parameters as specified from the spoken utterance of the user to the automated assistant.
8. A method implemented by one or more processors, the method comprising:
it is determined that the user has provided a first spoken utterance to an automated assistant accessible via the computing device,
wherein the first spoken utterance includes a request to initialize an application separate from the automated assistant;
in response to the first spoken utterance, cause the application to initialize and render an application interface in a foreground of a display interface of the computing device,
Wherein the application interface includes content identifying operations controllable via the automated assistant;
based on the operation being controllable via the automated assistant, causing selectable GUI elements to be rendered over the application interface of the application,
wherein the selectable GUI element includes a text identifier or a graphical representation of the operation controllable by the automated assistant;
determining that the user has provided a second spoken utterance to the automated assistant,
wherein the second spoken word recognition is capable of utilizing by the application parameters during the operation of the operation, and
wherein the second spoken utterance does not explicitly recognize the operation; and responsive to the second spoken utterance, causing the automated assistant to initiate, via the application, performance of the operation using the parameters recognized in the second spoken utterance.
9. The method of claim 8, wherein causing the selectable GUI element to be rendered over the application interface of the application comprises:
causing the text identifier to be rendered with a command phrase that includes a term that identifies the operation and a white space that indicates that a user-identifiable parameter was omitted from the command phrase.
10. The method of claim 8 or claim 9, further comprising:
based on the operations being controllable via the automated assistant, causing an audio interface of the computing device to be initialized for receiving a particular spoken utterance from the user,
wherein, when the audio interface is initialized, the user is able to provide the particular spoken utterance for controlling the automatic assistant without explicitly identifying the automatic assistant.
11. The method of any of claims 8 to 10, wherein causing the selectable GUI element to be rendered over the application interface of the application comprises:
generating content rendered with the selectable GUI element,
wherein the content includes the graphical representation of the assistant operation selectable via touch input to the display interface of the computing device.
12. The method of any of claims 8 to 11, wherein causing the selectable GUI element to be rendered over an application interface of the application comprises:
causing the selectable GUI element to be rendered over the application interface of the application for a threshold duration,
wherein the threshold duration is based on an amount of interaction between the user and the automated assistant since the selectable GUI element was rendered over the application interface.
13. The method of claim 12, wherein the automated assistant is not responsive to an additional spoken utterance when the user provides the additional spoken utterance after the selectable GUI element is no longer rendered over the application interface.
14. The method of claim 8, further comprising:
an audio interface of the computing device is caused to be initialized for detecting another spoken utterance that identifies one or more parameters for the operation based on the operation being controllable via the automated assistant.
15. A method implemented by one or more processors, the method comprising:
determining that the assistant operation is compatible with the application being run at the computing device,
wherein the application is separate from an automated assistant accessible via the computing device;
causing selectable graphical user interface GUI elements to be rendered at a display interface of the computing device based on the assistant operation being compatible with the application,
wherein the selectable GUI element identifies the assistant operation and is rendered in a foreground of the display interface of the computing device;
while the selectable GUI element is being rendered at the display interface of the computing device, determining that a user has provided a spoken utterance for the automated assistant,
Wherein the spoken utterance specifies a particular value of a parameter of the assistant operation without explicitly identifying the assistant operation; and
in response to the spoken utterance from the user, causing the automated assistant to control the application based on the assistant operation and a particular value of the parameter.
16. The method of claim 15, wherein causing the selectable GUI element to be rendered at the display interface of the computing device comprises:
generating content rendered with the selectable GUI element,
wherein the content includes icons selected based on the assistant operation and selectable via touch input to the display interface of the computing device.
17. The method of claim 15, wherein causing the selectable GUI element to be rendered at the display interface of the computing device comprises:
generating content rendered with the selectable GUI element,
wherein the content includes natural language content that characterizes a portion of a command phrase omitting one or more parameter values of the assistant operation.
18. The method of any of claims 15-17, wherein determining that the assistant operation is compatible with the application being run at the computing device comprises:
It is determined that additional selectable GUI element controls being rendered by the application are operable by the application initialized by the automated assistant.
19. The method of any of claims 15-18, wherein causing the automated assistant to control the application based on the assistant operation and a particular value of the parameter comprises:
causing the application to render another application interface generated by the application based on the particular value of the parameter.
20. The method of any of claims 15-19, wherein causing the selectable GUI element to be rendered at a display interface of the computing device comprises:
causing the selectable GUI element to be rendered simultaneously with the application rendering of one or more application GUI elements of the application.
21. A computer program comprising instructions which, when executed by one or more processors of a computing system, cause the computing system to perform the method of any preceding claim.
22. A computing device configured to perform the method of any one of claims 1 to 20.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/233,223 US11720325B2 (en) | 2021-04-16 | 2021-04-16 | Automated assistant performance of a non-assistant application operation(s) in response to a user input that can be limited to a parameter(s) |
US17/233,223 | 2021-04-16 | ||
PCT/US2021/063633 WO2022220882A1 (en) | 2021-04-16 | 2021-12-15 | Automated assistant performance of a non-assistant application operation(s) in response to a user input that can be limited to a parameter(s) |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116711004A true CN116711004A (en) | 2023-09-05 |
Family
ID=80445803
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180088331.3A Pending CN116711004A (en) | 2021-04-16 | 2021-12-15 | Automatic assistant execution of non-assistant application operations in response to user input capable of limiting parameters |
Country Status (6)
Country | Link |
---|---|
US (2) | US11720325B2 (en) |
EP (1) | EP4143684A1 (en) |
JP (1) | JP2024509660A (en) |
KR (1) | KR20230121150A (en) |
CN (1) | CN116711004A (en) |
WO (1) | WO2022220882A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220383869A1 (en) * | 2021-05-27 | 2022-12-01 | Soundhound, Inc. | Enabling natural language interactions with user interfaces for users of a software application |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10056077B2 (en) | 2007-03-07 | 2018-08-21 | Nuance Communications, Inc. | Using speech recognition results based on an unstructured language model with a music system |
US8924219B1 (en) | 2011-09-30 | 2014-12-30 | Google Inc. | Multi hotword robust continuous voice command detection in mobile devices |
US10237209B2 (en) * | 2017-05-08 | 2019-03-19 | Google Llc | Initializing a conversation with an automated agent via selectable graphical element |
CN113826158A (en) | 2019-04-26 | 2021-12-21 | 谷歌有限责任公司 | Dynamic delay of automated auxiliary action execution and/or background application requests |
US11238868B2 (en) * | 2019-05-06 | 2022-02-01 | Google Llc | Initializing non-assistant background actions, via an automated assistant, while accessing a non-assistant application |
US11972307B2 (en) | 2019-05-06 | 2024-04-30 | Google Llc | Automated assistant for generating, in response to a request from a user, application input content using application data from other sources |
-
2021
- 2021-04-16 US US17/233,223 patent/US11720325B2/en active Active
- 2021-12-15 WO PCT/US2021/063633 patent/WO2022220882A1/en active Application Filing
- 2021-12-15 CN CN202180088331.3A patent/CN116711004A/en active Pending
- 2021-12-15 JP JP2023537163A patent/JP2024509660A/en active Pending
- 2021-12-15 KR KR1020237025796A patent/KR20230121150A/en unknown
- 2021-12-15 EP EP21854902.0A patent/EP4143684A1/en active Pending
-
2023
- 2023-08-07 US US18/366,172 patent/US20230385022A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20220334794A1 (en) | 2022-10-20 |
JP2024509660A (en) | 2024-03-05 |
US20230385022A1 (en) | 2023-11-30 |
EP4143684A1 (en) | 2023-03-08 |
KR20230121150A (en) | 2023-08-17 |
US11720325B2 (en) | 2023-08-08 |
WO2022220882A1 (en) | 2022-10-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN111033492A (en) | Providing command bundle suggestions to automated assistants | |
US11900944B2 (en) | Initializing non-assistant background actions, via an automated assistant, while accessing a non-assistant application | |
JP2024506778A (en) | Passive disambiguation of assistant commands | |
US20230298583A1 (en) | Suggesting an alternative interface when environmental interference is expected to inhibit certain automated assistant interactions | |
CN113261056A (en) | Speaker perception using speaker-dependent speech models | |
US20230385022A1 (en) | Automated assistant performance of a non-assistant application operation(s) in response to a user input that can be limited to a parameter(s) | |
KR20230047188A (en) | Undo application action(s) via user interaction(s) with automated assistant | |
KR20230005351A (en) | Error Detection and Handling in Automated Voice Assistants | |
US20230237312A1 (en) | Reinforcement learning techniques for selecting a software policy network and autonomously controlling a corresponding software client based on selected policy network | |
KR20230007478A (en) | Automated assistant control from external applications that do not have automated assistant API capabilities | |
CN114981772A (en) | Selectively invoking automatic assistant based on detected environmental conditions without requiring voice-based invocation of automatic assistant | |
US20240038246A1 (en) | Non-wake word invocation of an automated assistant from certain utterances related to display content | |
US20240087564A1 (en) | Restricting third party application access to audio data content | |
US11481686B1 (en) | Selectively rendering a keyboard interface in response to an assistant invocation in certain circumstances | |
US20240062757A1 (en) | Generating and/or causing rendering of video playback-based assistant suggestion(s) that link to other application(s) | |
US20240029728A1 (en) | System(s) and method(s) to enable modification of an automatically arranged transcription in smart dictation | |
US20220147775A1 (en) | Generating a selectable suggestion using a provisional machine learning model when use of a default suggestion model is inconsequential | |
JP2024505794A (en) | Dynamic adaptation of graphical user interface elements by an automated assistant as the user repeatedly provides an audio utterance or sequence of audio utterances | |
CN117157614A (en) | Providing related queries to an auxiliary automation assistant based on past interactions | |
CN114787917A (en) | Processing utterances received simultaneously from multiple users |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |