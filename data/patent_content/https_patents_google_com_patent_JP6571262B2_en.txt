JP6571262B2 - Display objects based on multiple models - Google Patents
Display objects based on multiple models Download PDFInfo
- Publication number
- JP6571262B2 JP6571262B2 JP2018500421A JP2018500421A JP6571262B2 JP 6571262 B2 JP6571262 B2 JP 6571262B2 JP 2018500421 A JP2018500421 A JP 2018500421A JP 2018500421 A JP2018500421 A JP 2018500421A JP 6571262 B2 JP6571262 B2 JP 6571262B2
- Authority
- JP
- Japan
- Prior art keywords
- point
- determining
- requested
- image
- texel
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5838—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using colour
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/04—Texture mapping
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/20—Perspective computation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/20—Perspective computation
- G06T15/205—Image-based rendering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/50—Lighting effects
- G06T15/503—Blending, e.g. for anti-aliasing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/30—Scenes; Scene-specific elements in albums, collections or shared content, e.g. social network photos or video
Description
関連出願の相互参照
本願は、2015年10月7日に出願された米国特許出願第14/877,368号の継続出願であり、その開示は、参照によって本明細書に組み込まれる。
CROSS REFERENCE TO RELATED APPLICATIONS This application is a continuation of US patent application Ser. No. 14 / 877,368, filed Oct. 7, 2015, the disclosure of which is incorporated herein by reference.
オブジェクトのあるパノラマ画像は、画像がキャプチャされた地理的な場所および方位に関する情報に関連付けられる。たとえば、画像の各ピクセルは、画像がキャプチャされた地理的場所から(もしあれば)外観がピクセルの視覚的特性によって表されるオブジェクトの表面の一部への角度を識別するデータに関連付けられてよい。各ピクセルはまた、キャプチャ場所からピクセルによって表される表面の一部への距離を識別するデプスデータに関連付けられてよい。 A panoramic image with an object is associated with information about the geographical location and orientation from which the image was captured. For example, each pixel of the image is associated with data that identifies the angle from the geographic location where the image was captured to the part of the surface of the object whose appearance (if any) is represented by the visual characteristics of the pixel. Good. Each pixel may also be associated with depth data that identifies the distance from the capture location to the portion of the surface represented by the pixel.
画像に現れる表面の場所の3次元モデルはデプスデータに基づいて生成されてよい。このモデルは、頂点が表面場所に一致するポリゴンを含んでよい。ポリゴンは、パノラマ画像の視覚的特性を投影することによって、レイトレーシングを使用するモデルへ織り込まれてよい。ユーザは、モデルがユーザへ表示されてよい有利な地点を選択してよい。 A three-dimensional model of the surface location appearing in the image may be generated based on the depth data. This model may include polygons whose vertices coincide with surface locations. Polygons may be woven into a model that uses ray tracing by projecting the visual characteristics of a panoramic image. The user may select an advantageous point where the model may be displayed to the user.
本開示の態様は、1つまたは複数のプロセッサと、有利な地点に対するオブジェクトの表面の方位および視覚的特性のモデルを記憶するメモリと、1つまたは複数のプロセッサによって実行可能な命令とを含むシステムを提供する。視覚的特性は、第1の有利な地点からの表面の外観を表す第1の視覚的特性のセットと、第2の有利な地点からの表面の外観を表す第2の視覚的特性のセットとを含んでよい。これらの命令は、第1の有利な地点および第2の有利な地点とは異なる要求された有利な地点からオブジェクトの画像に対する要求を受信することと、第1の視覚的特性のセットからの第1の視覚的特性と、第2の視覚的特性のセットからの第2の視覚的特性とを識別することと、要求された有利な地点と第1の有利な地点とに対する表面の方位に基づいて、第1の視覚的特性のための第1の重み値を決定することと、要求された有利な地点と第2の有利な地点とに対する表面の方位に基づいて、第2の視覚的特性のための第2の重み値を決定することと、第1および第2の視覚的特性と、第1および第2の重み値とに基づいて、要求された画像の視覚的特性を決定することと、要求された画像を提供することとを含んでよい。 Aspects of the present disclosure include a system that includes one or more processors, a memory that stores a model of an object's surface orientation and visual characteristics relative to a point of interest, and instructions executable by the one or more processors. I will provide a. The visual characteristics are a first set of visual characteristics representing the appearance of the surface from the first advantageous point, and a second set of visual characteristics representing the appearance of the surface from the second advantageous point. May be included. These instructions receive a request for an image of an object from a requested advantageous point that is different from the first advantageous point and the second advantageous point, and a first from a first set of visual characteristics. Identifying a visual characteristic of 1 and a second visual characteristic from a second set of visual characteristics and based on the orientation of the surface with respect to the requested advantageous point and the first advantageous point Determining a first weight value for the first visual characteristic and a second visual characteristic based on the orientation of the surface with respect to the requested advantageous point and the second advantageous point. Determining a second weight value for the image and determining a visual characteristic of the requested image based on the first and second visual characteristics and the first and second weight values And providing the requested image.
本開示の態様はまた、表示のため画像を提供する方法を提供する。この方法は、要求された有利な地点からオブジェクトの画像に対する要求を受信するステップと、有利な地点に対するオブジェクトの表面の方位および視覚的特性のモデルにアクセスするステップであって、視覚的特性は、第1の有利な地点からの表面の外観を表す第1の視覚的特性のセットと、第2の有利な地点からの表面の外観を表す第2の視覚的特性のセットとを含み、第1および第2の有利な地点は、要求された有利な地点とは異なる、ステップと、第1の視覚的特性のセットからの第1の視覚的特性と、第2の視覚的特性のセットからの第2の視覚的特性とを識別するステップと、要求された有利な地点と第1の有利な地点とに対する表面の方位に基づいて、第1の視覚的特性のための第1の重み値を決定するステップと、要求された有利な地点と第2の有利な地点とに対する表面の方位に基づいて、第2の視覚的特性のための第2の重み値を決定するステップと、第1および第2の視覚的特性と、第1および第2の重み値とに基づいて、要求された画像の視覚的特性を決定するステップと、表示のために要求された画像を提供するステップとを含んでよい。 Aspects of the present disclosure also provide a method for providing an image for display. The method includes receiving a request for an image of an object from a requested advantageous point, and accessing a model of the object's surface orientation and visual characteristic relative to the advantageous point, wherein the visual characteristic is: A first set of visual characteristics representing the appearance of the surface from the first advantageous point, and a second set of visual characteristics representing the appearance of the surface from the second advantageous point, the first And the second advantageous point is different from the requested advantageous point from the step, the first visual characteristic from the first set of visual characteristics, and the second visual characteristic set. A first weight value for the first visual characteristic is determined based on identifying the second visual characteristic and the orientation of the surface relative to the requested advantageous point and the first advantageous point; The steps to determine, the advantageous points requested and the Determining a second weight value for the second visual characteristic based on the orientation of the surface relative to the two advantageous points; the first and second visual characteristics; and the first and second Determining a visual characteristic of the requested image based on the weight value and providing the requested image for display.
本開示の態様は、プログラムのコンピューティングデバイス読取可能な命令が記憶された非一時的なコンピューティングデバイス読取可能な記憶媒体をさらに提供する。これらの命令は、1つまたは複数のコンピューティングデバイスによって実行された場合、1つまたは複数のコンピューティングデバイスに対して、要求された有利な地点からオブジェクトの画像に対する要求を受信することと、有利な地点に対するオブジェクトの表面の方位および視覚的特性のモデルにアクセスすることであって、視覚的特性は、第1の有利な地点からの表面の外観を表す第1の視覚的特性のセットと、第2の有利な地点からの表面の外観を表す第2の視覚的特性のセットとを備え、第1および第2の有利な地点は、要求された有利な地点とは異なる、アクセスすることと、第1の視覚的特性のセットからの第1の視覚的特性と、第2の視覚的特性のセットからの第2の視覚的特性とを識別することと、要求された有利な地点と第1の有利な地点とに対する表面の方位に基づいて、第1の視覚的特性のための第1の重み値を決定することと、要求された有利な地点と第2の有利な地点とに対する表面の方位に基づいて、第2の視覚的特性のための第2の重み値を決定することと、第1および第2の視覚的特性と、第1および第2の重み値とに基づいて、要求された画像の視覚的特性を決定することと、表示のために要求された画像を提供することとを含む方法を実行させてよい。 Aspects of the present disclosure further provide a non-transitory computing device readable storage medium having stored thereon the computer device readable instructions of the program. These instructions, when executed by one or more computing devices, advantageously receive a request for an image of an object from a requested advantageous point for one or more computing devices; Accessing a model of an object's surface orientation and visual characteristics with respect to a particular point, the visual characteristic comprising a first set of visual characteristics representing the appearance of the surface from a first advantageous point; A second set of visual characteristics representing the appearance of the surface from the second advantageous point, wherein the first and second advantageous points are different from the requested advantageous point; Identifying a first visual characteristic from the first set of visual characteristics and a second visual characteristic from the second set of visual characteristics; and 1, advantageous Determining a first weight value for the first visual characteristic based on the orientation of the surface relative to the point and based on the orientation of the surface relative to the requested advantageous point and the second advantageous point Determining a second weight value for the second visual characteristic, and based on the first and second visual characteristic and the first and second weight value, the requested image A method may be performed that includes determining a visual characteristic of and providing a requested image for display.
概観
本技術は、オブジェクトのイメージがキャプチャされた有利な地点とは異なる有利な地点からオブジェクトを表示することに関する。たとえば、2つ以上のパノラマ画像が、2つの異なる有利な地点からオブジェクトをキャプチャしてよく、ユーザは、2つのキャプチャ地点間の場所からオブジェクトの画像を要求してよい。本システムは、オブジェクトの対応する表面の視覚的に正確な表現であるフラグメントの尤度に比例する画像の対応するフラグメントをともに混合することによって、ユーザによって要求された画像を生成してよい。例として、ユーザによって要求された画像を生成する場合、本システムは、表示されるべきオブジェクトの表面の方位に対するキャプチャ場所およびユーザによって要求された地点の関係に基づく品質値を計算してよい。フラグメントが混合された場合、他のフラグメントよりも良好な品質値を有するフラグメントへ重みがより適用されてよい。
Overview The present technology relates to displaying an object from an advantageous point different from the advantageous point from which the image of the object was captured. For example, two or more panoramic images may capture an object from two different advantageous points, and a user may request an image of the object from a location between the two capture points. The system may generate the image requested by the user by mixing together corresponding fragments of the image that are proportional to the likelihood of the fragment, which is a visually accurate representation of the corresponding surface of the object. As an example, when generating an image requested by a user, the system may calculate a quality value based on the relationship of the capture location and the point requested by the user to the orientation of the surface of the object to be displayed. When fragments are mixed, more weight may be applied to fragments that have better quality values than other fragments.
例示として、図2は、車の2つのソース画像がキャプチャされた2つの異なる有利な地点を図示する。この例では、車の正面がキャプチャされた角度は、第1のソース画像において比較的直角であり、第2のソース画像において比較的鋭角である。逆に、車の側面がキャプチャされた角度は、第1の画像において比較的鋭角であり、第2の画像において比較的直角である。図はまた、車を見るためにユーザによって選択された有利な地点を図示する。 By way of example, FIG. 2 illustrates two different advantageous points where two source images of a car were captured. In this example, the angle at which the front of the car was captured is relatively normal in the first source image and relatively acute in the second source image. Conversely, the angle at which the side of the car was captured is relatively acute in the first image and relatively perpendicular in the second image. The figure also illustrates the advantageous points selected by the user to view the car.
ユーザによって選択された有利な地点からオブジェクトを表示するために、システムは、各ソース画像においてキャプチャされた表面のすべての3次元(3D)モデルを生成してよい。たとえば、レーザ照準機は、デプスマップを準備するために使用されてよい。デプスマップは、頂点がオブジェクトの表面に沿った地点の場所に一致するポリゴンのメッシュを含むソースモデルを準備するために使用された。 In order to display an object from an advantageous point selected by the user, the system may generate all three-dimensional (3D) models of the surface captured in each source image. For example, a laser sighting machine may be used to prepare the depth map. The depth map was used to prepare a source model that contained a mesh of polygons whose vertices coincided with the location of points along the surface of the object.
各ソース画像に関連付けられたソースモデルはまた、モデルに対するキャプチャ地点の場所を識別してもよく、システムは、ソース画像においてキャプチャされた視覚的情報をモデルへ投影するために、その場所を使用してよい。1つのソース画像に関連付けられた3Dモデルは、表面場所に関して別のソース画像に関連付けられた3Dモデルに実質的に同一であってよいが、モデルへ投影されたテクスチャの視覚的特性は、表面がキャプチャされた角度に依存して異なってよい。 The source model associated with each source image may also identify the location of the capture point relative to the model, and the system uses that location to project the visual information captured in the source image onto the model. It's okay. The 3D model associated with one source image may be substantially identical to the 3D model associated with another source image with respect to the surface location, but the visual characteristics of the texture projected onto the model are It may be different depending on the angle captured.
ユーザへ表示されるために、画像のピクセルの視覚的特性を決定する場合、システムは、表示された各ピクセルを通って延びる光線が、たとえばテクセルのような、モデルのテクスチャと交差する場所を識別するために、レイトレーシング、およびユーザによって要求された有利な地点の場所を使用してよい。システムは、表示されたピクセルの視覚的特性(たとえば、色相、飽和、および輝度)を決定するために、異なるソース画像からのテクセルをともに混合してよい。 When determining the visual characteristics of the pixels of the image to be displayed to the user, the system identifies where the rays that extend through each displayed pixel intersect the texture of the model, such as a texel. To do so, ray tracing and advantageous point locations requested by the user may be used. The system may mix texels from different source images together to determine the visual characteristics (eg, hue, saturation, and brightness) of the displayed pixels.
ソース画像のモデルからのテクセルがともに混合される場合、別のソース画像からのテクセルよりも、1つのソース画像からのテクセルへより大きな重みが適用されてよい。この重みが、表示されるべきオブジェクトの視覚的特性の正確な表現であるテクセルの尤度を反映する品質値に基づいてよい。 If texels from a model of a source image are mixed together, a greater weight may be applied to texels from one source image than from texels from another source image. This weight may be based on a quality value that reflects the likelihood of the texel, which is an accurate representation of the visual characteristics of the object to be displayed.
少なくとも1つの態様では、品質値は、テクセルの解像度に対する表示されるピクセルの解像度に依存してよい。たとえば、表示された各ピクセルのために単一のテクセルが存在する場合に生じるように、最適な品質が定義されてよい。逆に、(表面のテクスチャが一直線にキャプチャされたがグレージング角度で見られる場合に生じてもよい)単一のピクセルに関連付けられた多くのテクセルが存在する、または(表面のテクスチャがグレージング角度でキャプチャされたが一直線に見られるのであれば生じてもよい)単一のテクセルに関連付けられた多くのピクセルが存在する場合に生じるように、低品質が定義されてよい。 In at least one aspect, the quality value may depend on the resolution of the displayed pixel relative to the resolution of the texel. For example, an optimal quality may be defined to occur when there is a single texel for each displayed pixel. Conversely, there may be many texels associated with a single pixel (which may occur when the surface texture is captured in a straight line but seen at a glazing angle), or (the surface texture is at a glazing angle). Low quality may be defined to occur when there are many pixels associated with a single texel (which may occur if captured but seen in a straight line).
テクセルの品質値は、表示されるべき表面の方位に対するユーザによって定義されたキャプチャされた有利な地点の場所に基づいて計算されてよい。例示として、図3は、2つの地点の場所、すなわち有利な地点と、ユーザへ表示されるべきモデルの表面における地点とを図示する。図はまた、円錐および平面の交差を表す楕円を図示する。平面は、有利な地点、たとえばテクセルを含むソースモデルポリゴンの頂点によって定義される平面、に対する表面の方位を反映する。円錐は、有利な地点から表面の地点へ延びる線(「有利/表面線」)を中心とされる。楕円が延長される範囲は、表面の方位と、有利な地点から表面が見られる角度とに関する。有利/表面線が、表面の方位に完全に直角ならば、楕円は円になるだろう。有利/表面線の立体角が、表面の方位に対してより鋭角になると、楕円は、より延長されるようになり、楕円の短軸に対する長軸の比が増加するであろう。 The texel quality value may be calculated based on the location of the captured advantageous point defined by the user relative to the orientation of the surface to be displayed. By way of example, FIG. 3 illustrates the location of two points, an advantageous point and a point on the surface of the model to be displayed to the user. The figure also illustrates an ellipse representing the intersection of a cone and a plane. The plane reflects the orientation of the surface with respect to an advantageous point, for example the plane defined by the vertices of the source model polygon containing the texel. The cone is centered on a line ("Advantage / Surface Line") that extends from an advantageous point to a surface point. The extent to which the ellipse is extended relates to the orientation of the surface and the angle at which the surface is viewed from an advantageous point. If the advantage / surface line is completely perpendicular to the surface orientation, the ellipse will be a circle. As the solid angle of the advantage / surface line becomes more acute with respect to the surface orientation, the ellipse will become more elongated and the ratio of the major axis to the minor axis of the ellipse will increase.
テクセルの品質値は、キャプチャ地点に関連付けられた楕円(「テクセル楕円」)と、ユーザによって要求された有利な地点に関連付けられた楕円(「ピクセル楕円」)との間の差分に基づいて決定されてよい。図4は、テクセル楕円およびピクセル楕円の例を提供する。品質値は、ピクセル楕円の半径と、テクセル楕円の半径との間の長さにおける最大差を生成する角度における、テクセル楕円の半径に対するピクセル楕円の半径の比から計算されてよい。 The texel quality value is determined based on the difference between the ellipse associated with the capture point (the `` texel ellipse '') and the ellipse associated with the advantageous point requested by the user (the `` pixel ellipse ''). It's okay. FIG. 4 provides examples of texel ellipses and pixel ellipses. The quality value may be calculated from the ratio of the pixel ellipse radius to the texel ellipse radius at an angle that produces the maximum difference in length between the pixel ellipse radius and the texel ellipse radius.
各テクセルについて、品質値が計算されると、品質値は、混合中の重みとして適用されてよい。たとえば、3つのテクセルT1、T2、およびT3を識別するために、3つのソース画像が使用されたのであれば、出力は、(w1T1+w2T2+w3T3)/(w1+w2+w3)として計算されてよい。wnは、テクセルの品質値に等しい。重みはまた、他の手法で適用されてよい。 For each texel, once the quality value is calculated, the quality value may be applied as a weight during mixing. For example, if three source images were used to identify three texels T 1 , T 2 , and T 3 , the output would be (w 1 T 1 + w 2 T 2 + w 3 T 3 ) / (w 1 + w 2 + w 3 ). w n is equal to the texel quality value. The weight may also be applied in other ways.
システムはまた、閉塞(occlusion)へ対処するために重み値を使用してよい。図5に図示されるように、表示されるピクセルの特性を決定するために使用される光線が、ユーザによって要求された有利な地点における視点から妨害されるであろうソース画像においてキャプチャされたオブジェクトの表面を通じて延びてよい。システムは、表面に面している各背面のための重みがゼロに設定されるように、表面に面している正面と背面との両方をレンダリングしてよい。システムはまた、各ソース画像内のビューワに最も近い表面が表示のために選択されるように画像を生成してよい。これによってシステムは、デプスを計算することなく、両モデルからのテクスチャをともに混合することが可能となる。 The system may also use weight values to deal with occlusion. As illustrated in FIG. 5, an object captured in a source image where the rays used to determine the characteristics of the displayed pixel will be disturbed from the viewpoint at the advantageous point requested by the user May extend through the surface. The system may render both the front and back facing the surface so that the weight for each back facing the surface is set to zero. The system may also generate images so that the surface closest to the viewer in each source image is selected for display. This allows the system to mix textures from both models together without calculating the depth.
アーティファクトもまた、重み値を用いて対処されてよい。たとえば、図6に図示されるように、デプスデータにおける不連続性は、表面におけるギャップがオブジェクトの表面として不正確にモデル化されるようにしてよい。そのような存在しない表面をモデルから除去するのではなく、システムは、存在しない表面のテクセルの品質値を決定してよい。光線の角度が存在しない表面の方位に比較的直角であれば、テクセルの品質値は、異なる有利な地点からキャプチャされた別の表面におけるテクセルの品質値と比較して非常に低くてよい。しかしながら、存在しない表面が見られる角度が存在しない表面の方位に対して比較的平行であれば、その表面におけるテクセルの品質値は比較的高くてよい。 Artifacts may also be addressed using weight values. For example, as illustrated in FIG. 6, a discontinuity in the depth data may cause a gap in the surface to be incorrectly modeled as the surface of the object. Rather than removing such nonexistent surfaces from the model, the system may determine the quality value of the nonexistent surface texels. If the ray angle is relatively perpendicular to the non-existing surface orientation, the texel quality value may be very low compared to the texel quality value at another surface captured from a different advantageous point. However, if the angle at which a non-existing surface is seen is relatively parallel to the orientation of the non-existing surface, the texel quality value on that surface may be relatively high.
システムは、ユーザによって要求される有利な地点からユーザへオブジェクトを表示するために使用されてよい。その点に関して、図7に図示されるように、ユーザは、ソースイメージがキャプチャされた有利な地点以外の有利な地点からオブジェクトを見ることが可能であってよい。 The system may be used to display objects to the user from the advantageous points required by the user. In that regard, as illustrated in FIG. 7, the user may be able to view the object from an advantageous point other than the advantageous point where the source image was captured.
例示的なシステム
図1は、本明細書で開示された態様が実施されてよい1つの可能なシステム100を例示する。この例では、システム100は、コンピューティングデバイス110および120を含んでよい。コンピューティングデバイス110は、1つまたは複数のプロセッサ112、メモリ114、および汎用コンピューティングデバイスに典型的に存在する他のコンポーネントを含んでよい。図1は、単一のブロックとしても表されるデバイス110内の単一のブロックとしてプロセッサ112およびメモリ114の各々を機能的に表すが、本明細書で説明されたシステムおよび方法は、同じ物理的なハウジングに格納されてよい、または格納されなくてよい多数のプロセッサ、メモリ、およびデバイスを含んでよい。たとえば、単一のコンポーネント(たとえば、プロセッサ112)を含むとして以下に説明される様々な方法は、複数のコンポーネント(たとえば、負荷平準サーバファームにおける多数のプロセッサ)を含んでよい。同様に、異なるコンポーネント(たとえば、デバイス110およびデバイス120)を含むとして以下に説明される様々な方法は、単一のコンポーネントを含んでよい(たとえば、デバイス120が、以下に説明される決定を実行するのではなく、デバイス120は、処理のために関連するデータをデバイス110へ送信し、さらなる処理または表示のための決定の結果を受信してよい)。
Exemplary System FIG. 1 illustrates one
コンピューティングデバイス110のメモリ114は、プロセッサによって実行されてよい命令116を含む、プロセッサ112によってアクセス可能な情報を記憶してよい。メモリ114はまた、プロセッサ112によって検索、操作、または記憶されてよいデータ118を含んでよい。メモリ114は、非一時的なデータを記憶することができる媒体のように、適切なプロセッサによってアクセス可能な情報を記憶することができる任意のタイプのストレージであってよい。例として、メモリ114は、ハードディスクドライブ、ソリッドステートドライブ、メモリカード、RAM、DVD、書込可能なメモリ、または読取専用メモリであってよい。それに加えて、メモリは分散された記憶システムを含んでよく、データ150のようなデータは、同じまたは異なる地理的場所において物理的に配置されてよい複数の異なる記憶デバイスに記憶される。
The
命令116は、プロセッサ112または他のコンピューティングデバイスによって実行されるべき命令の任意のセットであってよい。その点に関し、「命令」、「アプリケーション」、「ステップ」、および「プログラム」という用語は、本明細書において置換可能に使用されてよい。これらの命令は、プロセッサによる即時処理のためのオブジェクトコードフォーマットで、またはオンデマンドで翻訳される、または先行してコンパイルされる独立したソースコードモジュールのスクリプトまたは集合を含む別のコンピューティングデバイス言語で記憶されてよい。命令の機能、方法、およびルーチンは、以下により詳細に説明される。プロセッサ112は、市販のCPUのような任意の従来のプロセッサであってよい。あるいは、プロセッサは、ASICまたは他のハードウェアベースのプロセッサのような専用コンポーネントであってよい。
The
データ118は、命令116に従ってコンピューティングデバイス110によって検索、記憶、または修正されてよい。たとえば、本明細書で説明された主題は、どの特定のデータ構造によっても限定されないが、データは、コンピュータレジスタに、多くの異なるフィールドおよびレコードまたはXMLドキュメントを有するテーブルとしてリレーショナルデータベースに記憶されてよい。データはまた、限定されないが、バイナリ値、ASCIIまたはUnicodeのような任意のコンピューティングデバイス読取可能なフォーマットでフォーマットされてよい。さらに、データは、数字、説明文、所有者コード、ポインタ、他のネットワーク場所におけるような他のメモリに記憶されたデータに対する参照、または関連データを計算するための関数によって使用される情報のような関連情報を識別するために十分な任意の情報を備えてよい。
コンピューティングデバイス110は、ネットワーク160の1つのノードにおいて存在してよく、ネットワーク160の他のノードとダイレクトおよび非ダイレクトに通信することが可能であってよい。少数のコンピューティングデバイスしか図1に図示されていないが、典型的なシステムは、多数の接続されたコンピューティングデバイスを含んでよく、異なる各コンピューティングデバイスは、ネットワーク160の異なるコードにある。本明細書で説明されるネットワーク160および介在するノードは、様々なプロトコルおよびシステムを使用して相互接続されてよく、これによってネットワークは、インターネット、ワールドワイドウェブ、特定のイントラネット、広域ネットワーク、またはローカルネットワークの一部であってよい。ネットワークは、イーサネット(登録商標)、Wi-FiおよびHTTP、1つまたは複数の会社へ専用であるプロトコル、および先述したものの様々な組合せのような標準的な通信プロトコルを利用してよい。例として、コンピューティングデバイス110は、ネットワーク160を経由してコンピューティングデバイス120と通信することが可能なウェブサーバであってよい。コンピューティングデバイス120は、クライアントコンピューティングデバイスであってよく、サーバ110は、ディスプレイ122を経由してデバイス120のユーザ135へ情報を伝送し提示するために、ネットワーク160を使用することによって情報を表示してよい。上述したように情報が伝送または受信された場合にある利点が取得されるが、本明細書で説明された主題の他の態様は、情報の伝送のいずれの特定の方式にも限定されない。
The
コンピューティングデバイス120は、上記で説明されたようなプロセッサ、メモリ、および命令を備えたサーバ110に類似して構成されてよい。コンピューティングデバイス120は、ユーザによる使用のために意図されたパーソナルコンピューティングデバイスであってよく、中央処理装置(CPU)、データおよび命令を記憶するメモリ、ディスプレイ122(たとえば、スクリーンのあるモニター、タッチスクリーン、プロジェクタ、テレビ、または情報を表示するために動作可能な他のデバイス)のようなディスプレイ、ユーザ入力デバイス162(たとえば、マウス、キーボード、タッチスクリーン、マイクロホン等)、またカメラ163のように、パーソナルコンピューティングデバイスと接続して通常使用されるコンポーネントのすべてを有してよい。
コンピューティングデバイス120はまた、インターネットのようなネットワークを介してサーバとワイヤレスにデータを交換することができるモバイルコンピューティングデバイスであってよい。単なる例として、デバイス120は、モバイル電話、もしくはワイヤレス対応PDA、タブレットPC、ウェアラブルコンピューティングデバイス、またはインターネットを経由して情報を取得することが可能なネットブックのようなデバイスであってよい。デバイスは、GoogleのAndroidオペレーティングシステム、Microsoft Windows、またはApple iOSのようなオペレーティングシステムで動作するように構成されてよい。その点に関し、本明細書で説明された動作中に実行される命令のいくつかは、オペレーティングシステムによって提供されてよい一方、他の命令は、デバイスにインストールされたアプリケーションによって提供されてよい。本明細書において説明されたシステムおよび方法に従うコンピューティングデバイスは、命令を処理し、人間へ/からデータを伝送することが可能な他のデバイス、および/またはローカル記憶能力を欠くネットワークコンピュータ、およびテレビのためのセットトップボックスを含む他のコンピュータを含んでよい。
The
コンピューティングデバイス120は、デバイスの地理的場所および方位を決定するために、回路のようなコンポーネント130を含んでよい。たとえば、クライアントデバイス120は、デバイスの緯度、経度、および高度位置を決定するために、GPSレシーバ131を含んでよい。このコンポーネントはまた、クライアントデバイスがセル電話であれば、1つまたは複数のセル電話塔からセル電話のアンテナにおいて受信した信号のような、クライアントデバイス120において受信された他の信号に基づいて、デバイスの位置を決定するためのソフトウェアを備えてよい。それはまた、デバイスが向けられる方位を決定するために、磁気コンパス132、加速度計133、およびジャイロスコープ134を含んでよい。単なる例として、デバイスは、重力の方向、またはそれに垂直な面に対するピッチ、ヨー、またはロール(またはその変化)を決定してもよい。コンポーネント130はまた、デバイスとオブジェクトの表面との間の距離を決定するためにレーザ照準機または類似のデバイスを含んでよい。
The
サーバ110は、その少なくとも一部がクライアントデバイスへ伝送されてよい地図関連情報を記憶してよい。地図情報は、どの特定のフォーマットにも限定されない。たとえば、地図データは、衛星または航空機によってキャプチャされた写真のような、地理的場所のビットマップ画像を含んでよい。
サーバ110はまた、単なる例として平面写真、光球、または背景の動画のようなイメージを記憶してよい。このイメージは、後のアクセスのために、または特徴に関連する情報を探索する人へ、写真をアクセス可能にする目的のために、エンドユーザによってキャプチャされアップロードされてよい。カメラによってキャプチャされた画像データに加えて、イメージの個々のアイテムが、キャプチャの日付、キャプチャの日時、キャプチャの地理的方位(たとえば、カメラ角度または方向)および場所(たとえば、緯度、経度、および高度)のような追加のデータに関連付けられてよい。
イメージの一部は、イメージ内に現れる特徴の地理的場所のモデルを含む追加情報に関連付けられてよい。たとえばモデルは、パノラマ画像においてキャプチャされたオブジェクトの表面の場所を識別してよい。表面の場所はたとえば、その場所が固定位置からの立体角および距離(たとえば、イメージがキャプチャされたポイントからの方位および距離)として定義される地点のコンステレーション、または頂点が緯度/経度/高度座標で表現される地理的に位置するポリゴンのように、異なる手法でメモリに記憶されてよい。システムおよび方法はさらに、その場所が、レーザ照準機を使用してダイレクトにキャプチャされた、または立体画法の三角測量を使用することによって画像から生成された地点のコンステレーションから地理的に位置されたポリゴンのモデルを生成することのように、1つの基準系から別の基準系へ場所を変換してよい。場所は、他の手法でも同様に表現されてよく、アプリケーションおよび必要とされる精度の性質に依存してよい。単なる例として、地理的場所は、道路住所、(道路地図のエッジに対するピクセル位置のような)地図のエッジに対するx-y座標、または地理的場所を識別することが可能な他の基準系(たとえば、サーベイ地図上のロットおよびブロック番号)によって識別されてよい。場所はまた範囲によって説明されてよく、たとえば地理的場所は、緯度/経度/高度座標の範囲、離散的な系列によって説明されてよい。 A portion of the image may be associated with additional information including a model of the geographic location of features that appear in the image. For example, the model may identify the location of the surface of the captured object in the panoramic image. A surface location is, for example, a constellation of points defined as solid angles and distances from a fixed location (for example, azimuth and distance from the point where the image was captured), or vertices with latitude / longitude / altitude coordinates May be stored in memory in different ways, such as a geographically located polygon represented by. The system and method are further geographically located from a constellation of points where the location was captured directly using a laser sighting machine or generated from an image using stereoscopic triangulation. The location may be converted from one reference system to another, like generating a model of a polygon. The location may be expressed in other ways as well, and may depend on the nature of the application and the accuracy required. Merely by way of example, a geographic location is a street address, an xy coordinate for a map edge (such as a pixel location for a road map edge), or other reference system that can identify a geographic location (e.g., a survey). Lots and block numbers on the map). A location may also be described by a range, for example, a geographic location may be described by a range of latitude / longitude / altitude coordinates, a discrete series.
例示的な方法
本発明の様々な態様に従う動作が説明されるであろう。以下の動作は、以下に説明される正確な順序で実行される必要はないことが理解されるべきである。むしろ、様々なステップは、異なる順序で、または同時に取り扱われることができる。
Exemplary Methods Operations according to various aspects of the invention will be described. It should be understood that the following operations need not be performed in the exact order described below. Rather, the various steps can be handled in different orders or simultaneously.
地理的なオブジェクトは、多数の有利な地点からイメージによってキャプチャされてよい。例として、図2は、それぞれ、2つの異なる場所210および220から2つの個別の画像215および225においてキャプチャされた車240を図示する。有利な地点210から車の正面によって一般に定義される平面202に対するカメラ角度211は、比較的直角であり、車の側面によって一般に定義される平面201に対するカメラ角度212は、比較的鋭角である。対照的に、有利な地点220から正面の平面202に対する視角221は、比較的鋭角であり、側面201に対する視角222は、比較的直角である。(参照番号201および202は、これら表面によって一般に定義される平面のみならず、車240の正面および側面を称するために、相互置換可能に使用される。)その点に関し、場所210および220からキャプチャされた画像は、異なる角度から車の異なる表面をキャプチャし、1つの表面は、比較的一直線にキャプチャされ、他の表面は、鋭い角度からキャプチャされる。
Geographic objects may be captured by images from a number of advantageous points. As an example, FIG. 2 illustrates a
イメージにおいてキャプチャされたオブジェクト表面の場所のモデルが、生成され、かつイメージに関連付けられてよい。例として、ソースイメージは、パノラマ画像を含んでよい。ソース画像がキャプチャされた時において、パノラマ画像の各ピクセルをキャプチャ場所からその視覚的特性がピクセルによって表される表面の一部への距離へ関連付けるために、レーザ照準機135または別のデプス決定技術が使用されてよい。そのような情報と、画像がキャプチャされた場所(たとえば、GPSレシーバ131によって提供された緯度/経度/高度情報)とに基づいて、システムは、その頂点がオブジェクトの表面に沿った地点の場所(たとえば、緯度/経度/高度)に一致するポリゴン(たとえば、三角形)のメッシュを含むソースモデルを生成してよい。
A model of the location of the object surface captured in the image may be generated and associated with the image. As an example, the source image may include a panoramic image.
ソース画像においてキャプチャされた視覚的特性は、そのソース画像に関連付けられたモデルを織り込むために使用されてよい。たとえば、イメージ215および216の各ピクセルはまた、たとえばカメラからピクセルに関連付けられた表面の一部へ延びる光線を定義するデータのようなカメラ角度に関連付けられてよい。カメラ角度データは、コンパス132によって提供される基本方向と、ジャイロスコープ134によって提供される方位データとのような、画像がキャプチャされた時に地理的コンポーネント130によって提供される情報に基づいてよい。光線との交差の地点におけるポリゴンの視覚的特性が、関連付けられたピクセルの視覚的特性と一致するように、画像215の各ピクセルの視覚的特性をモデル216のポリゴンへ投影するために、レイトレーシングが使用されてよい。
The visual characteristics captured in the source image may be used to interweave the model associated with that source image. For example, each pixel of
ポリゴンの視覚的情報は、テクセルから構成されたテクスチャとして記憶されてよい。単なる例として、テクスチャのテクセルは、画像のピクセルが配置されてよい手法、たとえば1つまたは複数の視覚的特性を定義するグリッドまたは個々のユニットの他の集合と類似の方式で配置されてよい。以下の説明の一部は、説明の容易のために、色のようなピクセルまたはテクセルの単一の視覚的特性のみを称してよい。しかしながら、ピクセルおよびテクセルは、色相、飽和、および輝度を含む多くの異なる視覚的特性を定義するデータに関連付けられてよい。 Polygon visual information may be stored as a texture composed of texels. Merely by way of example, texture texels may be arranged in a manner similar to the manner in which the pixels of the image may be arranged, eg, a grid or other set of individual units that define one or more visual characteristics. Some of the description below may refer to only a single visual characteristic of a pixel or texel, such as color, for ease of explanation. However, pixels and texels may be associated with data that defines many different visual characteristics including hue, saturation, and luminance.
2つの異なるソースモデルが表面の場所の同一の表現を含んでいても、各モデルにおける表面の視覚的特性は、大きく異なってよい。たとえば、イメージ215は鋭い角度212で車240の側面201をキャプチャするので、車の側面の長さ全体はほんの少数の画像215においてのみ水平に表れてよい。その結果、ピクセル情報が車の側面を表すポリゴンへ投影された場合、単一のピクセルからの情報は、水平方向において、多くのテクセルを介して延びてよい。モデル216が有利な地点230から表示されたのであれば、車の側面は、結果として色の長い水平線を有するように見えてよい。しかしながら、モデル216が有利な地点210(テクスチャが投影された同じ場所)から表示されるのであれば、車240の側面201は少ないアーティファクト乃至アーティファクトなしで表示されてよい。
Even if two different source models contain the same representation of the surface location, the visual properties of the surface in each model can vary greatly. For example, the
システムは、未だに別の有利な地点から地理的オブジェクトを表示するために、多数の有利な地点からキャプチャされた視覚的情報を組み合わせてよい。たとえば、ユーザは、有利な地点230から車240の画像235を要求してよい。ユーザへ表示されるべき画像のピクセル(「表示されるピクセル」)の視覚的特性を決定する場合、システムは、表示された各ピクセルを有利な地点からおよびピクセルを含む画像面を介して延びる光線に関連付けてよい。システムは、各ピクセルの関連付けられた光線がモデルによって定義された表面と交差する地点およびテクセルを決定してよい。たとえば、画像235におけるピクセルの視覚的特性を決定する場合、システムは、ピクセルの光線がモデル216のポリゴンと交差するテクセル(T1)と、ピクセルの光線が、モデル226のポリゴンと交差するテクセル(T2)とを決定してよい。システムは、T1の色とT2の色とをアルファ混合することによってピクセルの色を決定してよい。
The system may combine visual information captured from multiple advantageous points to still display the geographic object from another advantageous point. For example, the user may request an
交差したテクセルの視覚的特性がともに混合される場合、別のソース画像から導出されたテクセルよりも1つのソース画像から導出されたテクセルへより大きな重みが適用されてよい。この重みは、表示されるべきオブジェクトの視覚的特性の正確な表現であるテクセルの尤度に基づいてよい。 If the visual characteristics of intersecting texels are mixed together, a greater weight may be applied to texels derived from one source image than to texels derived from another source image. This weight may be based on texel likelihood, which is an accurate representation of the visual characteristics of the object to be displayed.
少なくとも1つの態様では、品質値は、テクセルの解像度に対する表示されたピクセルの解像度に依存してよく、最適な品質は、表示された各ピクセルのために単一のテクセルが存在する場合に生じるように定義されてよい。逆に、低品質は、単一のピクセルに関連付けられた多くのテクセルが存在する場合、または単一のテクセルに関連付けられた多くのピクセルが存在する場合に生じるように定義されてよい。例として、第1のポリゴンが生成されてよい。第1のポリゴンへ投影される各ピクセルのために単一のテクセルが存在する。ピクセルが、比較的一直線な角度から第1のポリゴンへ投影されているのであれば、テクスチャは、一杯に詰め込まれた多くのテクセルを含んでよい。このポリゴンが一直線に表示されたのであれば、表示された各ピクセルのためにおおよそ1つのテクセルが存在してよく、したがってテクスチャは、比較的高い品質を有すると考慮されるであろう。しかしながら、ポリゴンが鋭角から表示されたのであれば、表示された各ピクセルのために多くのテクセルが存在してよく、テクスチャの比較的高い解像度に関わらず、テクスチャは、比較的低い品質を有すると考慮されるであろう。 In at least one aspect, the quality value may depend on the resolution of the displayed pixel relative to the resolution of the texel, with the optimal quality occurring when there is a single texel for each displayed pixel. May be defined. Conversely, low quality may be defined to occur when there are many texels associated with a single pixel, or when there are many pixels associated with a single texel. As an example, a first polygon may be generated. There is a single texel for each pixel that is projected onto the first polygon. If the pixel is projected from a relatively straight angle onto the first polygon, the texture may include many stuffed texels. If this polygon is displayed in a straight line, there may be approximately one texel for each displayed pixel, and therefore the texture will be considered to have a relatively high quality. However, if the polygon is displayed from an acute angle, there may be many texels for each displayed pixel, and the texture has a relatively low quality, regardless of the relatively high resolution of the texture. Will be considered.
さらなる例として、第2のポリゴンが生成されてよい。第2のポリゴンへ投影される各ピクセルのためにも単一のテクセルが存在する。しかしながら、ピクセルが比較的鋭角から第2のポリゴンへ投影されたのであれば、テクスチャは、長く薄いテクセルを含んでよい。このポリゴンがその後、一直線に表示されるのであれば、表示された多くのピクセルは、それらの特性を単一のテクセルのみから導出するであろう。このケースでは、テクスチャは、比較的低品質を有すると考慮されるであろう。しかしながら、ポリゴンが比較的鋭角から表示されるのであれば、表示される各ピクセルのために1つのみのテクセルが存在してよい。このケースでは、テクスチャの比較的低い解像度にも関わらず、テクスチャは比較的高い品質を有すると考慮されるであろう。 As a further example, a second polygon may be generated. There is also a single texel for each pixel that is projected onto the second polygon. However, if the pixel is projected from a relatively acute angle onto the second polygon, the texture may include long and thin texels. If this polygon is then displayed in a straight line, the many pixels displayed will derive their properties from only a single texel. In this case, the texture will be considered to have a relatively low quality. However, if the polygon is displayed from a relatively acute angle, there may be only one texel for each pixel displayed. In this case, the texture will be considered to have a relatively high quality despite the relatively low resolution of the texture.
テクセルの品質値は、表示されるべき表面の方位に対するユーザによって定義されたキャプチャされた有利な地点の場所に基づいて計算されてもよい。例示として、図3の地点320は、モデルの地理的に配置されたポリゴンが見られるであろう地理的場所であってよい。地点330は、表示されたピクセルに関連付けられたカメラ角度がポリゴンと交差する地点である。線"s"は、有利な地点320から交差地点330へ延びる。円錐350は、鋭い立体角("α")で地点320から外側へ延びる。αの尺度は、任意に選択されてよい。αの尺度はまた、テクスチャがキャプチャされた場所から見られるように、円錐がテクセルと類似した立体角を提供するように、たとえばテクスチャの解像度が高いほど、より小さな円錐となるように選択されてよい。楕円310は、円錐350と平面(図示せず)との交差を表す。平面、たとえば交差地点330を含むポリゴンの頂点によって定義される平面は、交差地点における表面の方位を反映する。
The texel quality value may be calculated based on the location of the captured advantageous point defined by the user relative to the orientation of the surface to be displayed. By way of example, the
楕円が延ばされる範囲は、表面の方位と有利な地点に関連する。たとえば、線"s"が、平面の方位に完全に直交している(それは、表面を一直線に見ることに関連付けられるであろう)のであれば、楕円310は、完全な円になるであろう。線"s"の立体角が平面の方位に対してより鋭角になると、楕円310はより延ばされるようになり、楕円の短軸("a")に対する長軸("b")の比は、"n"を面法線として増加するであろう。軸"a"および"b"は、式a=αs×nおよびb=(n・s/|s|)(a×n)から決定されてよい。
The extent to which the ellipse is extended is related to the orientation of the surface and the advantageous points. For example, if line “s” is completely orthogonal to the plane orientation (which would be associated with viewing the surface in a straight line),
テクセルの品質値は、キャプチャ場所から交差地点へ延びる円錐に関連付けられた楕円(「テクセル楕円」)と、ユーザによって選択された場所から交差地点へ延びる円錐に関連付けられた楕円(「ピクセル楕円」)との差分に基づいて決定されてよい。図4では、テクセル楕円425がキャプチャ場所420に関連付けられ、ピクセル楕円435がユーザによって要求された有利な地点430に関連付けられる。交差地点450におけるテクセルの品質値は、特定の角度θにおけるテクセル楕円の半径に対するピクセル楕円の半径の比、たとえばquality(θ)=radiust(θ)/radiusp(θ)から計算されてよい。1つの態様では、角度θは、最小比をもたらす角度または角度の推定値である。たとえば、品質値は、異なるθの値において計算されてよく、テクセルの品質値は、たとえばqualitymin=quality(argminθ{quality(θ)})のように、計算された最低値に等しくてよい。最小は、両楕円を行列形式で表現し、かつテクセル楕円が単位円となるであろう座標系へピクセル楕円をマップするテクセル楕円の反転をピクセル楕円へ乗じることによって決定されてよい。この座標系内では、比は再マップされたピクセル楕円の半径に等しく、最小比は再マップされたピクセル楕円の短軸長に等しい。
The texel quality values are the ellipse associated with the cone extending from the capture location to the intersection (the `` texel ellipse '') and the ellipse associated with the cone extending from the location selected by the user to the intersection (the `` pixel ellipse ''). It may be determined based on the difference between. In FIG. 4, a
品質値は、テクセル楕円軸の各々をピクセル楕円軸の各々へ投影し、最長比を与えるピクセル楕円軸を選択することによって推定されてよい。例として、シェーダは、4つの可能な方向をサンプルし、最低品質値に関連付けられた角度を選択する式qualitymin〜1/max((at・ap)/(ap・ap),(bt・ap)/(ap・ap),(at・bp)/(bp・bp),(bt・bp)/(bp・bp))に従って値を計算することによって最小品質値を概算してよい。他の方法も品質を計算するために使用されてよい。 The quality value may be estimated by projecting each texel ellipse axis onto each pixel ellipse axis and selecting the pixel ellipse axis that gives the longest ratio. As an example, the shader samples the four possible directions and selects the angle associated with the lowest quality value quality min ~ 1 / max ((a t・ a p ) / (a p・ a p ), (b t・ a p ) / (a p・ a p ), (a t・ b p ) / (b p・ b p ), (b t・ b p ) / (b p・ b p )) The minimum quality value may be approximated by calculating Other methods may also be used to calculate quality.
テクセルの品質が一旦計算されると、品質は、表示されるピクセルの色がテクセルの色にどの程度類似しているのかを決定するために使用されてよい。たとえば、3つのテクセルT1、T2、およびT3を識別するために3つのソース画像が使用されるのであれば、各テクセルのための混合重みが、フラグメントシェーダにおいて各入力テクセルおよび出力(w1T1+w2T2+w3T3)/(w1+w2+w3)について計算されてよく、wnはテクセルの品質値に等しい。重みはまた、品質値を累乗へ上げることのように他の手法でも適用されてよい。たとえば、重みを大きな指数まで上げることによって、最大重みを有する表面が他の重みを支配してよい。したがって、より低い重みを有するテクセルのインパクトを低減することによってゴーストを低減する。 Once the texel quality is calculated, the quality may be used to determine how similar the color of the displayed pixel is to the texel color. For example, if three source images are used to identify the three texels T 1 , T 2 , and T 3 , then the blend weight for each texel is calculated for each input texel and output (w 1 T 1 + w 2 T 2 + w 3 T 3 ) / (w 1 + w 2 + w 3 ), where w n is equal to the texel quality value. The weight may also be applied in other ways, such as raising the quality value to a power. For example, by raising the weight to a large index, the surface with the largest weight may dominate other weights. Thus, ghosting is reduced by reducing the impact of texels with lower weights.
システムはまた、図5に例として図示されるように、閉塞へ対処するために重み値を使用してよい。表面502および503は、画像520において有利な地点Aからキャプチャされ、表面501および503は、画像530において有利な地点Bからキャプチャされた。モデル521および531はそれぞれ、各画像520および530のために生成されてよい。モデルが生成される場合、モデルは、テクスチャをポリゴンの特定の側面に関連付けてよい。たとえば、表面502は単一の三角形によってモデル521において表されてよく、1つの側面は有利な地点Aに面し、他の側面は有利な地点Aから離れている。画像データがモデルへ投影された場合、モデルは、テクスチャが有利な地点に面している三角形の側面上にあるかどうかを示してよい。
The system may also use weight values to address the blockage, as illustrated by way of example in FIG.
要求された画像を生成する場合、その関連付けられたテクスチャが要求された有利な地点に面しているか離れているかを決定することによって、およびテクスチャが同じモデル内の他のテクスチャよりも有利な地点に近いかどうかを決定することによって、閉塞された表面が隠されてよい。たとえば、表示されたピクセルの色を有利な地点540から決定した場合、システムは、ピクセルの関連付けられた光線550が各表面と交差したポイントのすべて、すなわちポイント511〜513を決定してよい。システムはまた、各モデルについて、有利な地点に最も近い交差したテクセルと、交差したテクスチャのテクスチャが、有利な地点に面している(「正面向き」)ポリゴンの側面にあるか、または有利な地点から離れている(「背面向き」)ポリゴンの側面にあるかを決定してよい。したがって、システムは、T511Bが、モデル531における最も近いテクセルであり、正面向きであると決定してよい("Tpppm"は、交差地点pppにおけるテクセルであり、有利な地点mに関連付けられたモデルに記憶されたテクセルを称する)。システムはまた、T512Aが、モデル521における最も近いテクセルであり、かつ背面向きであることを決定してよい。T512Aが背面向きであるので、システムはその重みをゼロへ自動的に設定してよい。その結果、光線550に関連付けられたピクセルの色は、(w511BT511B+w512AT512A)/(w511B+w512A)によって決定されてよく、w511BはT511Bについて決定された品質値であり、w512Aはゼロに設定される。その結果、表示されたピクセルの色はT511Bと同じになるであろう。
When generating the requested image, by determining whether its associated texture faces or is away from the requested advantageous point, and where the texture is more advantageous than other textures in the same model By deciding whether it is close to the occluded surface may be hidden. For example, if the color of the displayed pixel is determined from an
あるいは、あるテクスチャを無視することでも、またはそれらの重みをゼロへ設定することでもなく、それらの相対的な重みが低減されてよい。たとえば、w512Aをゼロへ設定することでも、T513AおよびT513Bをともに無視することでもなく、背面向きのテクセルおよび他のテクセルがまた、低減された重みで混合のために使用されてよい。 Alternatively, rather than ignoring certain textures or setting their weights to zero, their relative weights may be reduced. For example, neither setting w 512A to zero nor ignoring both T 513A and T 513B , back-facing texels and other texels may also be used for mixing with reduced weights.
デプスデータにおける不連続性によって引き起こされるアーティファクトを含むアーティファクトはまた、重み値を用いて対処されてよい。たとえば、図6に図示されるように、モデル621は、場所620からキャプチャされた画像に関連付けられてよく、モデル631は、場所630からキャプチャされた画像に関連付けられてよい。表面601、602間のギャップ603は、モデル621において正確に表されてよい。しかしながら、キャプチャ時において検索されたデプスデータから生じた不正確さ、またはモデル631の生成中に生じた他のいくつかの誤差の結果、表面635がギャップ603を介して延びることをモデル631が不正確に示すことになってよい。モデル631が有利な地点640から見られたのであれば、無関係な表面635は、ギャップの1つのエッジから他のエッジへ延びたゴムシートの外観を有してよい。いくつかの態様では、システムは、モデル621をモデル631と比較することによって無関係なポリゴンをチェックし、かつ除去してよい。
Artifacts including artifacts caused by discontinuities in depth data may also be addressed using weight values. For example, as illustrated in FIG. 6,
他の態様では、システムは、表示するための画像を生成する場合、無関係な表面を使用してよい。たとえば、ユーザは、有利な地点640から表面を表示する画像を要求してよい。光線641に関連付けられたピクセルの色に対するモデル621の寄与を決定する場合、システムは、第1の命令地点として表面601における地点Aを識別してよい。テクセルは、テクセルがキャプチャされた角度(場所620)に類似した角度において見られるため、システムはまた、地点Aにおけるテクセルが比較的高い品質値を有すると決定してよい。ピクセルの色に対するモデル631の寄与を決定する場合、システムは、第1の交差地点として無関係な表面635における地点Bを識別してよい。テクセルは、テクセルがキャプチャされた角度(場所630)に比較的直角である角度において見られるため、システムは、地点Bにおけるテクセルが比較的低い品質値を有すると決定してよい。その結果、ポイントAおよびBにおいてテクセルがともに混合される場合、比較的少ない重みが地点Bにおいてテクセルへ適用されるであろう。また、ピクセルの色は、表面601における地点Aの色にほとんど完全に基づくであろう。
In other aspects, the system may use an irrelevant surface when generating an image for display. For example, the user may request an image that displays the surface from an
ある角度から、無関係な表面は、表示されるべき画像の視覚的特性に対する顕著な寄与を有してよい。たとえば、ユーザは、有利な地点650からの画像を要求してよい。これは、モデル631のキャプチャ場所630に比較的近い。光線641に関連付けられたピクセルの色へのモデル631の寄与を決定する場合、システムは、第1の交差地点として無関係な表面635における地点Cを識別してよく、さらに、モデル631の地点Cにおけるテクセルは、比較的高い品質値を有していることを決定してよい。同じピクセルの色に対するモデル621の寄与を決定する場合、表面602がモデルによって表されないのであれば、システムは交差地点を識別しなくてよい。あるいは、表面602がモデルによって表されるのであれば、交差地点から有利な地点650への角度は交差地点からキャプチャ場所620への角度に比較的直角であるため、モデル621における交差地点におけるテクセルの品質値は比較的低くてよい。いずれのケースであれ、結果として表示されたピクセルの色は無関係な表面のテクセルと実質的に同じでよい。これは、無関係な表面635を表示することは、何も表示しないこと(たとえば、表面の不在を示す色)よりも好適であってよいため、モデル621が表面602の表現を有していないのであれば、特に有利である。
From an angle, an irrelevant surface may have a significant contribution to the visual characteristics of the image to be displayed. For example, the user may request an image from an
ユーザは、システムを使用し、かつモデルとインタラクトしてよい。単なる例として、図1を参照して、ユーザ135は、多数の有利な地点から画像データおよびデプスデータをキャプチャするために、クライアントデバイス120のカメラ163および地理的コンポーネント130を使用してよい。ユーザ135は、画像およびデプスデータをサーバ110へアップロードしてよい。プロセッサ112は、ユーザによって提供されたデータに基づいて、各画像のテクスチャモデルを生成し、かつそのデータをメモリ114に記憶してよい。サーバ110は、その後、具体的な有利な地点からオブジェクトの画像に対する要求を受信してよい。たとえば、ユーザ135(または、クライアントデバイス121を使用する異なるユーザ)は、ユーザ入力162を用いて地理的場所を選択し、かつ要求をサーバ110へ送信することによって、具体的な地理的場所におけるパノラマ画像を要求してよい。要求を受信すると、サーバ110は、たとえば要求された地理的場所のしきい距離内のキャプチャ場所または表面場所を有するすべてまたは限定された数のモデルを選択することによって、要求された地理的場所に基づいて2つ以上のモデルを検索してよい。サーバはその後、ネットワーク160を経由してクライアントデバイス120へモデルを送信してよい。モデルを受信すると、クライアントデバイス120のプロセッサは、モデルに基づいてかつ有利な地点として要求された地理的場所を使用して、パノラマ画像を生成してよい。要求された地理的場所が高度を含んでいなかったのであれば、有利な地点の高度はモデルのキャプチャ場所の高度に基づいてよい。
The user may use the system and interact with the model. By way of example only and with reference to FIG. 1, a
パノラマ画像はディスプレイ122上に表示されてよい。たとえば、図2および図7を参照して示すように、車240のモデル216および226がクライアントデバイスへ送信されたのであれば、車240の画像710は、有利な地点230に基づいて生成され、かつユーザへ表示されてよい。ユーザは、クライアントデバイスのユーザインターフェースを経由してコマンドを提供すること、たとえば画像をピックアップするためにボタンを押すことによって、キャプチャ場所とは異なる他の有利な地点を選択してよい。
The panoramic image may be displayed on the
図8は、上記で説明された態様のいくつかに従うフローチャートである。ブロック801では、第1の有利な地点と第2の有利な地点とに対するオブジェクトの表面の方位と、第1の有利な地点からの表面の外観を表す第1の視覚的特性のセットと、第2の有利な地点からの表面の外観を表す第2の視覚的特性のセットとを含む、オブジェクトの表面のモデルがアクセスされる。ブロック802では、第1の有利な地点および第2の有利な地点とは異なる、要求された有利な地点からオブジェクトの画像に対する要求が受信される。ブロック803では、第1の視覚的特性が、第1の視覚的特性のセットから識別され、第2の視覚的特性が、第2の視覚的特性のセットから識別される。ブロック804では、要求された有利な地点および第1の有利な地点に対する表面の方位に基づいて、第1の視覚的特性のための第1の重み値が決定される。ブロック805では、要求された有利な地点および第2の有利な地点に対する表面の方位に基づいて、第2の視覚的特性のための第2の重み値が決定される。ブロック806では、第1および第2の視覚的特性と、第1および第2の重み値とに基づいて、要求された画像の視覚的特性が決定される。ブロック807では、要求された画像が表示のためにユーザへ提供される。
FIG. 8 is a flowchart according to some of the aspects described above. In
特許請求の範囲によって定義されたような本発明から逸脱することなく、上記で議論された特徴のこれらおよび他のバリエーションおよび組合せを利用することができるので、実施形態の先述した説明は、特許請求の範囲によって定義されたような本発明の限定によってではなく、例示として採用されるべきである。本発明の例の提供(のみならず、「〜のような」、「たとえば」、「〜を含む」等のようにフレーズされた句)は、具体的な例に本発明を限定するものとして解釈されるべきではなく、むしろ、これら例は、多くの可能な態様のうちのいくつかのみを例示することが意図されていると理解されるであろう。 Since these and other variations and combinations of the features discussed above can be utilized without departing from the invention as defined by the claims, the foregoing description of the embodiments provides Should be taken as an illustration, not as a limitation of the present invention as defined by the scope. Providing examples of the present invention (as well as phrases such as “such as”, “for example”, “including”, etc.) is intended to limit the present invention to specific examples It should not be construed, but rather it will be understood that these examples are intended to illustrate only some of the many possible aspects.
100 システム
110 サーバ
112 プロセッサ
114 メモリ
116 命令
118 データ
120 クライアントデバイス
121 クライアントデバイス
122 ディスプレイ
130 地理的コンポーネント
131 GPSレシーバ
132 コンパス
133 加速度計
134 ジャイロスコープ
135 照準機
135 ユーザ
160 ネットワーク
162 ユーザ入力
163 カメラ
201 側面
202 正面
210 有利な地点
211 カメラ角度
212 カメラ角度
215 画像
216 モデル
220 有利な地点
221 視角
222 視角
225 画像
226 モデル
230 有利な地点
235 画像
240 車
310 楕円
320 有利な地点
330 交差地点
350 円錐
420 キャプチャ場所
425 テクセル楕円
430 有利な地点
435 ピクセル楕円
450 交差地点
501 表面
502 表面
503 表面
511 ポイント
512 ポイント
513 ポイント
520 画像
521 モデル
530 画像
531 モデル
540 有利な地点
550 光線
601 表面
602 表面
603 ギャップ
620 キャプチャ場所
621 モデル
630 キャプチャ場所
631 モデル
635 表面
640 有利な地点
641 光線
650 有利な地点
710 画像
100 system
110 servers
112 processor
114 memory
116 instructions
118 data
120 client devices
121 client devices
122 display
130 Geographic components
131 GPS receiver
132 Compass
133 Accelerometer
134 Gyroscope
135 sighting machine
135 users
160 network
162 User input
163 Camera
201 side
202 front
210 Advantageous points
211 Camera angle
212 Camera angle
215 images
216 models
220 Advantageous points
221 viewing angle
222 viewing angle
225 images
226 models
230 Advantageous points
235 images
240 cars
310 ellipse
320 Advantageous points
330 Intersection
350 cone
420 capture location
425 texel ellipse
430 Advantageous point
435 pixel ellipse
450 intersection
501 surface
502 surface
503 surface
511 points
512 points
513 points
520 images
521 models
530 images
531 models
540 Advantageous point
550 rays
601 surface
602 surface
603 gap
620 capture location
621 models
630 capture location
631 models
635 surface
640 Advantageous points
641 rays
650 Advantageous point
710 images
Claims (13)
1つまたは複数のプロセッサと、
有利な地点に対するオブジェクトの表面の方位および視覚的特性の第1のモデルおよび第2のモデルを記憶するメモリであって、前記第1のモデルの前記視覚的特性は、第1の有利な地点からキャプチャされた画像データを前記第1のモデルへ投影することによって生成される第1のテクセルであり、前記第2のモデルの前記視覚的特性は、第2の有利な地点からキャプチャされた画像データを前記第2のモデルへ投影することによって生成される第2のテクセルである、メモリと、
前記1つまたは複数のプロセッサによって実行可能な命令であって、
前記第1の有利な地点および前記第2の有利な地点とは異なる要求された有利な地点から前記オブジェクトの画像に対する要求を受信することと、
ピクセルを含む画像面を通って前記要求された有利な地点から延びる光線に関連付けられる、ユーザに表示されるべき前記画像のピクセルについて、前記光線が前記第1のモデルのポリゴンと交差する、第1のテクセルのセットからの第1のテクセルと、前記光線が前記第2のモデルのポリゴンと交差する、第2のテクセルのセットからの第2のテクセルとを識別することと、
前記要求された有利な地点と前記第1の有利な地点とに対する前記表面の前記方位に基づいて、前記第1のテクセルのための第1の重み値を決定することと、
前記要求された有利な地点と前記第2の有利な地点とに対する前記表面の前記方位に基づいて、前記第2のテクセルのための第2の重み値を決定することと、
前記第2のテクセルの視覚的特性よりも前記第1のテクセルの視覚的特性により類似した視覚的特性を決定することによって、前記第1のテクセルおよび第2のテクセルと、前記第1の重み値および第2の重み値とに基づいて、前記要求された画像のピクセルの視覚的特性を決定することと、
前記要求された画像を提供することと
を備える命令と
を備える、システム。 A system,
One or more processors;
A memory storing a first model and a second model of an object's surface orientation and visual characteristics relative to an advantageous point, wherein the visual characteristic of the first model is determined from the first advantageous point; A first texel generated by projecting captured image data onto the first model, wherein the visual characteristic of the second model is image data captured from a second advantageous point A second texel generated by projecting to the second model , memory,
Instructions executable by the one or more processors,
Receiving a request for an image of the object from a requested advantageous point different from the first advantageous point and the second advantageous point;
For a pixel of the image to be displayed to a user associated with a ray that extends from the requested advantageous point through an image plane that includes the pixel, the first ray intersects a polygon of the first model . a first texel from the set of texels, and that the ray intersects the polygon of the second model, identifying a second texels from the second set of texels,
Determining a first weight value for the first texel based on the orientation of the surface with respect to the requested advantageous point and the first advantageous point;
Determining a second weight value for the second texel based on the orientation of the surface with respect to the requested advantageous point and the second advantageous point;
By determining a visual characteristic that is more similar to the visual characteristic of the first texel than the visual characteristic of the second texel , the first texel and the second texel, and the first weight value Determining a visual characteristic of a pixel of the requested image based on and a second weight value;
A system comprising: providing the requested image.
前記命令は、前記平面上の地点を選択することをさらに備え、
第1の重み値を決定することは、前記平面と前記第1の有利な地点から前記選択された地点へ延びる線との間の立体角と、前記平面と前記要求された有利な地点から前記選択された地点へ延びる線との間の立体角との関係に基づいて値を決定することを備え、
第2の重み値を決定することは、前記平面と前記第2の有利な地点から前記選択された地点へ延びる線との間の立体角と、前記平面と前記要求された有利な地点から前記選択された地点へ延びる線との間の立体角との関係に基づいて値を決定することを備える、
請求項1に記載のシステム。 The orientation of the surface defines a plane;
The instructions further comprise selecting a point on the plane;
Determining a first weight value includes: a solid angle between the plane and a line extending from the first advantageous point to the selected point; and from the plane and the requested advantageous point. Determining a value based on a relationship with a solid angle between a line extending to a selected point,
Determining a second weight value includes: a solid angle between the plane and a line extending from the second advantageous point to the selected point; and from the plane and the requested advantageous point. Determining a value based on a relationship with a solid angle between a line extending to a selected point,
The system according to claim 1.
前記平面における地点を選択することは、前記ピクセルに関連付けられた前記方向が、前記平面と交差する前記地点を選択することを備え、
前記要求された画像の視覚的特性を決定することは、前記選択されたピクセルの視覚的特性を決定することを備える、
請求項4に記載のシステム。 The instructions further comprise selecting a pixel of the requested image and determining a direction associated with the selected pixel;
Selecting a point in the plane comprises selecting the point where the direction associated with the pixel intersects the plane;
Determining the visual characteristics of the requested image comprises determining visual characteristics of the selected pixels.
The system according to claim 4 .
要求された有利な地点からオブジェクトの画像に対する要求を受信するステップと、
有利な地点に対する前記オブジェクトの表面の方位および視覚的特性の第1のモデルおよび第2のモデルにアクセスするステップであって、前記第1のモデルの前記視覚的特性は、第1の有利な地点からキャプチャされた画像データを前記第1のモデルへ投影することによって生成される第1のテクセルであり、前記第2のモデルの前記視覚的特性は、第2の有利な地点からキャプチャされた画像データを前記第2のモデルへ投影することによって生成される第2のテクセルであり、前記第1の有利な地点および第2の有利な地点は、前記要求された有利な地点とは異なる、ステップと、
ピクセルを含む画像面を通って前記要求された有利な地点から延びる光線に関連付けられる、ユーザに表示されるべき前記画像のピクセルについて、前記光線が前記第1のモデルのポリゴンと交差する、第1のテクセルのセットからの第1のテクセルと、前記光線が前記第2のモデルのポリゴンと交差する、第2のテクセルのセットからの第2のテクセルとを識別するステップと、
前記要求された有利な地点と前記第1の有利な地点とに対する前記表面の前記方位に基づいて、前記第1のテクセルのための第1の重み値を決定するステップと、
前記要求された有利な地点と前記第2の有利な地点とに対する前記表面の前記方位に基づいて、前記第2のテクセルのための第2の重み値を決定するステップと、
前記第2のテクセルの視覚的特性よりも前記第1のテクセルの視覚的特性により類似した視覚的特性を決定することによって、前記第1のテクセルおよび第2のテクセルと、前記第1の重み値および第2の重み値とに基づいて、前記要求された画像のピクセルの視覚的特性を決定するステップと、
表示のために前記要求された画像を提供するステップと
を備える、方法。 A method for providing an image for display comprising:
Receiving a request for an image of an object from a requested advantageous point;
Comprising the steps of: accessing a first model and a second model of the orientation and visual properties of the surface of said object with respect vantage point, wherein the visual characteristics of the first model, a first vantage point A first texel generated by projecting image data captured from the first model onto the first model, wherein the visual characteristic of the second model is an image captured from a second advantageous point. A second texel generated by projecting data onto the second model , wherein the first advantageous point and the second advantageous point are different from the requested advantageous point; When,
For a pixel of the image to be displayed to a user associated with a ray that extends from the requested advantageous point through an image plane that includes the pixel, the first ray intersects a polygon of the first model . a first texel from the set of texels, the ray intersects the polygon of the second model, and identifying a second texels from the second set of texels,
Determining a first weight value for the first texel based on the orientation of the surface with respect to the requested advantageous point and the first advantageous point;
Determining a second weight value for the second texel based on the orientation of the surface with respect to the requested advantageous point and the second advantageous point;
By determining a visual characteristic that is more similar to the visual characteristic of the first texel than the visual characteristic of the second texel , the first texel and the second texel, and the first weight value And determining a visual characteristic of a pixel of the requested image based on and a second weight value;
Providing the requested image for display.
前記平面における地点を選択するステップをさらに備え、
第1の重み値を決定するステップは、前記平面と前記第1の有利な地点から前記選択された地点へ延びる線との間の立体角と、前記平面と前記要求された有利な地点から前記選択された地点へ延びる線との間の立体角との関係に基づいて値を決定するステップを備え、
第2の重み値を決定するステップは、前記平面と前記第2の有利な地点から前記選択された地点へ延びる線との間の立体角と、前記平面と前記要求された有利な地点から前記選択された地点へ延びる線との間の立体角との関係に基づいて値を決定するステップを備える、
請求項8に記載の方法。 The orientation of the surface defines a plane;
Further comprising selecting a point in the plane;
Determining a first weight value includes: a solid angle between the plane and a line extending from the first advantageous point to the selected point; and from the plane and the requested advantageous point. Determining a value based on a relationship with a solid angle between a line extending to a selected point;
Determining a second weight value includes: a solid angle between the plane and a line extending from the second advantageous point to the selected point; and from the plane and the requested advantageous point. Determining a value based on a relationship with a solid angle between a line extending to the selected point;
The method according to claim 8 .
前記平面における地点を選択するステップは、前記ピクセルに関連付けられた前記方向が、前記平面と交差する前記地点を選択するステップを備え、
前記要求された画像の視覚的特性を決定するステップは、前記選択されたピクセルの視覚的特性を決定するステップを備える、
請求項11に記載の方法。 Selecting a pixel of the requested image and determining a direction associated with the selected pixel;
Selecting a point in the plane comprises selecting the point where the direction associated with the pixel intersects the plane;
Determining a visual characteristic of the requested image comprises determining a visual characteristic of the selected pixel;
The method of claim 11 .
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
JP2019143176A JP6915001B2 (en) | 2015-10-07 | 2019-08-02 | Displaying objects based on multiple models |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/877,368 US9773022B2 (en) | 2015-10-07 | 2015-10-07 | Displaying objects based on a plurality of models |
US14/877,368 | 2015-10-07 | ||
PCT/US2016/055323 WO2017062357A1 (en) | 2015-10-07 | 2016-10-04 | Displaying objects based on a plurality of models |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019143176A Division JP6915001B2 (en) | 2015-10-07 | 2019-08-02 | Displaying objects based on multiple models |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2018526724A JP2018526724A (en) | 2018-09-13 |
JP6571262B2 true JP6571262B2 (en) | 2019-09-04 |
Family
ID=57218984
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2018500421A Active JP6571262B2 (en) | 2015-10-07 | 2016-10-04 | Display objects based on multiple models |
JP2019143176A Active JP6915001B2 (en) | 2015-10-07 | 2019-08-02 | Displaying objects based on multiple models |
JP2021115686A Active JP7247276B2 (en) | 2015-10-07 | 2021-07-13 | Viewing Objects Based on Multiple Models |
JP2023041182A Pending JP2023076498A (en) | 2015-10-07 | 2023-03-15 | Display of object based on multiple models |
Family Applications After (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2019143176A Active JP6915001B2 (en) | 2015-10-07 | 2019-08-02 | Displaying objects based on multiple models |
JP2021115686A Active JP7247276B2 (en) | 2015-10-07 | 2021-07-13 | Viewing Objects Based on Multiple Models |
JP2023041182A Pending JP2023076498A (en) | 2015-10-07 | 2023-03-15 | Display of object based on multiple models |
Country Status (8)
Country | Link |
---|---|
US (5) | US9773022B2 (en) |
EP (1) | EP3304497A1 (en) |
JP (4) | JP6571262B2 (en) |
KR (3) | KR102232724B1 (en) |
CN (2) | CN107851329B (en) |
DE (1) | DE112016003134T5 (en) |
GB (2) | GB2556511B (en) |
WO (1) | WO2017062357A1 (en) |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9691241B1 (en) * | 2012-03-14 | 2017-06-27 | Google Inc. | Orientation of video based on the orientation of a display |
US9773022B2 (en) * | 2015-10-07 | 2017-09-26 | Google Inc. | Displaying objects based on a plurality of models |
CN108921191B (en) * | 2018-05-25 | 2021-10-26 | 北方工业大学 | Multi-biological-feature fusion recognition method based on image quality evaluation |
US11284054B1 (en) * | 2018-08-30 | 2022-03-22 | Largo Technology Group, Llc | Systems and method for capturing, processing and displaying a 360° video |
CN112583900B (en) * | 2020-12-02 | 2023-04-07 | 深圳市互盟科技股份有限公司 | Data processing method for cloud computing and related product |
US11756261B2 (en) * | 2021-11-10 | 2023-09-12 | Ford Global Technologies, Llc | Single-perspective image relighting |
US11776200B2 (en) | 2021-11-10 | 2023-10-03 | Ford Global Technologies, Llc | Image relighting |
Family Cites Families (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6697062B1 (en) | 1999-08-06 | 2004-02-24 | Microsoft Corporation | Reflection space image based rendering |
US7085409B2 (en) | 2000-10-18 | 2006-08-01 | Sarnoff Corporation | Method and apparatus for synthesizing new video and/or still imagery from a collection of real video and/or still imagery |
AU2002303082A1 (en) | 2001-01-26 | 2002-09-12 | Zaxel Systems, Inc. | Real-time virtual viewpoint in simulated reality environment |
KR100433625B1 (en) * | 2001-11-17 | 2004-06-02 | 학교법인 포항공과대학교 | Apparatus for reconstructing multiview image using stereo image and depth map |
GB2390792B (en) * | 2002-07-08 | 2005-08-31 | Vision Rt Ltd | Image processing system for use with a patient positioning device |
RU2358319C2 (en) * | 2003-08-29 | 2009-06-10 | Самсунг Электроникс Ко., Лтд. | Method and device for photorealistic three dimensional simulation of face based on image |
JP4521568B2 (en) * | 2005-06-14 | 2010-08-11 | 国立大学法人京都大学 | Corresponding point search method, relative orientation method, three-dimensional image measurement method, corresponding point search device, relative orientation device, three-dimensional image measurement device, corresponding point search program, and computer-readable recording medium recording the corresponding point search program |
US8244025B2 (en) * | 2006-03-20 | 2012-08-14 | Siemens Energy, Inc. | Method of coalescing information about inspected objects |
CN101321299B (en) * | 2007-06-04 | 2011-06-01 | 华为技术有限公司 | Parallax generation method, generation cell and three-dimensional video generation method and device |
JP5011224B2 (en) * | 2008-07-09 | 2012-08-29 | 日本放送協会 | Arbitrary viewpoint video generation apparatus and arbitrary viewpoint video generation program |
US8525871B2 (en) * | 2008-08-08 | 2013-09-03 | Adobe Systems Incorporated | Content-aware wide-angle images |
CN101398936A (en) * | 2008-11-07 | 2009-04-01 | 北京航空航天大学 | Bidirectional texture function compressing and synthesizing method |
EP2489195A1 (en) * | 2009-10-14 | 2012-08-22 | Nokia Corp. | Autostereoscopic rendering and display apparatus |
US9445072B2 (en) | 2009-11-11 | 2016-09-13 | Disney Enterprises, Inc. | Synthesizing views based on image domain warping |
WO2013032955A1 (en) * | 2011-08-26 | 2013-03-07 | Reincloud Corporation | Equipment, systems and methods for navigating through multiple reality models |
US20130127988A1 (en) | 2011-11-17 | 2013-05-23 | Sen Wang | Modifying the viewpoint of a digital image |
US9626798B2 (en) * | 2011-12-05 | 2017-04-18 | At&T Intellectual Property I, L.P. | System and method to digitally replace objects in images or video |
WO2013133648A1 (en) | 2012-03-07 | 2013-09-12 | 엘지전자 주식회사 | Method and device for processing video signal |
US8842162B2 (en) * | 2012-08-16 | 2014-09-23 | Nice-Systems Ltd | Method and system for improving surveillance of PTZ cameras |
US9836875B2 (en) | 2013-04-26 | 2017-12-05 | Flipboard, Inc. | Viewing angle image manipulation based on device rotation |
US9269012B2 (en) * | 2013-08-22 | 2016-02-23 | Amazon Technologies, Inc. | Multi-tracker object tracking |
US20150134651A1 (en) * | 2013-11-12 | 2015-05-14 | Fyusion, Inc. | Multi-dimensional surround view based search |
US20160062571A1 (en) * | 2014-09-02 | 2016-03-03 | Apple Inc. | Reduced size user interface |
JP2016162392A (en) * | 2015-03-05 | 2016-09-05 | セイコーエプソン株式会社 | Three-dimensional image processing apparatus and three-dimensional image processing system |
US11783864B2 (en) * | 2015-09-22 | 2023-10-10 | Fyusion, Inc. | Integration of audio into a multi-view interactive digital media representation |
US9773022B2 (en) * | 2015-10-07 | 2017-09-26 | Google Inc. | Displaying objects based on a plurality of models |
-
2015
- 2015-10-07 US US14/877,368 patent/US9773022B2/en active Active
-
2016
- 2016-10-04 JP JP2018500421A patent/JP6571262B2/en active Active
- 2016-10-04 EP EP16788840.3A patent/EP3304497A1/en active Pending
- 2016-10-04 GB GB1800102.4A patent/GB2556511B/en active Active
- 2016-10-04 WO PCT/US2016/055323 patent/WO2017062357A1/en active Application Filing
- 2016-10-04 DE DE112016003134.3T patent/DE112016003134T5/en active Pending
- 2016-10-04 CN CN201680039889.1A patent/CN107851329B/en active Active
- 2016-10-04 CN CN202210415621.0A patent/CN114898025A/en active Pending
- 2016-10-04 KR KR1020207013013A patent/KR102232724B1/en active IP Right Grant
- 2016-10-04 KR KR1020217008457A patent/KR102340678B1/en active IP Right Grant
- 2016-10-04 KR KR1020187000631A patent/KR102111079B1/en active IP Right Grant
- 2016-10-04 GB GB2112089.4A patent/GB2596662B/en active Active
-
2017
- 2017-08-31 US US15/692,548 patent/US10474712B2/en active Active
-
2019
- 2019-08-02 JP JP2019143176A patent/JP6915001B2/en active Active
- 2019-09-19 US US16/575,941 patent/US11086927B2/en active Active
-
2021
- 2021-05-14 US US17/320,648 patent/US11809487B2/en active Active
- 2021-07-13 JP JP2021115686A patent/JP7247276B2/en active Active
-
2023
- 2023-03-15 JP JP2023041182A patent/JP2023076498A/en active Pending
- 2023-10-09 US US18/377,989 patent/US20240054155A1/en active Pending
Also Published As
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6571262B2 (en) | Display objects based on multiple models | |
JP2019194924A5 (en) | ||
US9898857B2 (en) | Blending between street view and earth view | |
US10235800B2 (en) | Smoothing 3D models of objects to mitigate artifacts | |
US11086926B2 (en) | Thumbnail generation from panoramic images | |
US9551579B1 (en) | Automatic connection of images using visual features | |
CN112714266A (en) | Method and device for displaying label information, electronic equipment and storage medium | |
JP2023021469A (en) | Positioning method, positioning apparatus, method of generating visual map, and apparatus thereof | |
US8982120B1 (en) | Blurring while loading map data | |
WO2023164084A1 (en) | Systems and methods for generating dimensionally coherent training data | |
CN113379914A (en) | Generation method and device of visual corridor analysis chart and computer equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20180205 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20190115 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20190121 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20190412 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20190708 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20190807 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6571262Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |