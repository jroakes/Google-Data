US11790550B2 - Learnable cost volume for determining pixel correspondence - Google Patents
Learnable cost volume for determining pixel correspondence Download PDFInfo
- Publication number
- US11790550B2 US11790550B2 US17/292,647 US202017292647A US11790550B2 US 11790550 B2 US11790550 B2 US 11790550B2 US 202017292647 A US202017292647 A US 202017292647A US 11790550 B2 US11790550 B2 US 11790550B2
- Authority
- US
- United States
- Prior art keywords
- image
- matrix
- feature vectors
- feature vector
- pixel
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
- G06T7/579—Depth or shape recovery from multiple images from motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/16—Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication, matrix factorization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
- G06T7/593—Depth or shape recovery from multiple images from stereo images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/215—Motion-based segmentation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10004—Still image; Photographic image
- G06T2207/10012—Stereo images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20016—Hierarchical, coarse-to-fine, multiscale or multiresolution image processing; Pyramid transform
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20081—Training; Learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20084—Artificial neural networks [ANN]
Definitions
- a pixel correspondence between two images may indicate how pixels in a first image relate to pixels in a second image.
- the first image and the second image may be captured at different times and/or from different perspectives, and may thus provide different representations of an environment.
- a pair of pixels mapped to one another may represent the same portion of the environment, thus indicating an apparent displacement of the portion of the environment relative to the camera over time and/or due to the different perspectives of the images.
- Pixel correspondences may be determined for and used in a plurality of different applications.
- a pixel correspondence between two images captured at different times may be determined as part of an optical flow calculation.
- a pixel correspondence between two simultaneously-captured images may be determined as part of a stereo disparity calculation.
- a pixel correspondence between two images may be determined based on a comparison of feature vectors associated with pixels of a first image to feature vectors associated with pixels of a second image.
- This comparison of the feature vectors may generate a cost volume that indicates, for each pair of compared pixels, a similarity between pixels of the pair.
- the similarity between two pixels may be quantified by an inner product between the feature vectors associated with the two pixels.
- the cost volume may be generated using a learned/trained elliptical inner product.
- a first feature vector of the pair of pixels may be transformed by a learned/trained kernel matrix, and the transformed first feature vector may then be multiplied by a second feature vector of the pair of pixels.
- the kernel matrix may be learned/trained to define the elliptical inner product space.
- the cost volume generated based on the learned/trained elliptical inner product may then be used to determine the pixel correspondence between the two images.
- a computer-implemented method includes obtaining (i) a first plurality of feature vectors associated with a first image and (ii) a second plurality of feature vectors associated with a second image.
- the computer-implemented method also includes generating a plurality of transformed feature vectors by transforming each respective feature vector of the first plurality of feature vectors by a kernel matrix trained to define an elliptical inner product space.
- the computer-implemented method additionally includes generating a cost volume by determining, for each respective transformed feature vector of the plurality of transformed feature vectors, a plurality of inner products.
- Each respective inner product of the plurality of inner products may be between the respective transformed feature vector and a corresponding candidate feature vector of a corresponding subset of the second plurality of feature vectors.
- the computer-implemented method further includes determining, based on the cost volume, a pixel correspondence between the first image and the second image.
- a system in a second example, includes a processor and a non-transitory computer-readable storage medium having stored thereon instructions that, when executed by the processor, cause the processor to perform operations.
- the operations include obtaining (i) a first plurality of feature vectors associated with a first image and (ii) a second plurality of feature vectors associated with a second image.
- the operations also include generating a plurality of transformed feature vectors by transforming each respective feature vector of the first plurality of feature vectors by a kernel matrix trained to define an elliptical inner product space.
- the operations additionally include generating a cost volume by determining, for each respective transformed feature vector of the plurality of transformed feature vectors, a plurality of inner products.
- Each respective inner product of the plurality of inner products may be between the respective transformed feature vector and a corresponding candidate feature vector of a corresponding subset of the second plurality of feature vectors.
- the operations further include determining, based on the cost volume, a pixel correspondence between the first image and the second image.
- a non-transitory computer-readable storage medium having stored thereon instructions that, when executed by a computing system, cause the computing system to perform operations.
- the operations include obtaining (i) a first plurality of feature vectors associated with a first image and (ii) a second plurality of feature vectors associated with a second image.
- the operations also include generating a plurality of transformed feature vectors by transforming each respective feature vector of the first plurality of feature vectors by a kernel matrix trained to define an elliptical inner product space.
- the operations additionally include generating a cost volume by determining, for each respective transformed feature vector of the plurality of transformed feature vectors, a plurality of inner products.
- Each respective inner product of the plurality of inner products may be between the respective transformed feature vector and a corresponding candidate feature vector of a corresponding subset of the second plurality of feature vectors.
- the operations further include determining, based on the cost volume, a pixel correspondence between the first image and the second image.
- a system in a fourth example, includes means for obtaining (i) a first plurality of feature vectors associated with a first image and (ii) a second plurality of feature vectors associated with a second image.
- the system also includes means for generating a plurality of transformed feature vectors by transforming each respective feature vector of the first plurality of feature vectors by a kernel matrix trained to define an elliptical inner product space.
- the system additionally includes means for generating a cost volume by determining, for each respective transformed feature vector of the plurality of transformed feature vectors, a plurality of inner products. Each respective inner product of the plurality of inner products may be between the respective transformed feature vector and a corresponding candidate feature vector of a corresponding subset of the second plurality of feature vectors.
- the system further includes means for determining, based on the cost volume, a pixel correspondence between the first image and the second image.
- the kernel matrix may be symmetric and positive definite.
- the kernel matrix may be defined based on a product of (i) a diagonal matrix that includes a plurality of positive diagonal values and (ii) an orthogonal matrix.
- the kernel matrix may be defined via eigendecomposition by determining (i) a first product between a transpose of the diagonal matrix and the orthogonal matrix, and (ii) a second product between the first product and the diagonal matrix.
- Each respective positive diagonal value of the plurality of positive diagonal values of the diagonal matrix may include a respective weight applied to a corresponding dimension of each respective feature vector of the first plurality of feature vectors.
- the orthogonal matrix may be configured to apply a rotation to each respective feature vector of the first plurality of feature vectors.
- Each respective positive diagonal value of the plurality of positive diagonal values may be expressed as a function of a corresponding training parameter.
- the function may map (i) real number values assigned to the corresponding training parameter to (ii) positive real number values.
- the kernel matrix may be trained to define the elliptical inner product space by iteratively adjusting, for each respective positive diagonal value, a real number value assigned to the corresponding training parameter.
- the orthogonal matrix may be an element of a subset of a special orthogonal group of matrices.
- the subset may be selected such that eigenvalues of matrices of the subset exclude negative one.
- the orthogonal matrix may be expressed using a Cayley representation as a product of: (i) a difference between an identity matrix and a training matrix, and (ii) an inverse of a sum of the identity matrix and the training matrix.
- the training matrix may be skew-symmetric.
- the kernel matrix may be trained to define the elliptical inner product space by initializing the training matrix as the identity matrix and iteratively adjusting values of the training matrix.
- the orthogonal matrix may be an element of a Stiefel matrix manifold that includes a plurality of square orthonormal matrices.
- the kernel matrix may be trained to define the elliptical inner product space by a Riemann gradient descent that includes: determining a projection of a gradient from the Stiefel matrix manifold to a tangent space at the orthogonal matrix, adjusting values of the orthogonal matrix based on the projection of the gradient, and determining a retraction of the orthogonal matrix as adjusted into the Stiefel matrix.
- Each respective feature vector of the first plurality of feature vectors may be associated with a corresponding pixel of the first image.
- Each respective feature vector of the second plurality of feature vectors may be associated with a corresponding pixel of the second image.
- the corresponding subset of the second plurality of feature vectors may be associated with a plurality of pixels located within a search window that includes the particular pixel position within the second image.
- the first image and the second image may each represent an environment.
- Determining the pixel correspondence between the first image and the second image may include determining, based on the cost volume and for each respective pixel of a plurality of pixels in the first image, a corresponding pixel in the second image determined to represent a same portion of the environment as the respective pixel.
- the first image may represent the environment at a first time.
- Obtaining the first plurality of feature vectors and the second plurality of feature vectors may include generating the first plurality of feature vectors by processing the first image by one or more machine learning models, and generating the second plurality of feature vectors by processing the second image by the one or more machine learning models.
- the first plurality of feature vectors and the second plurality of feature vectors may correspond to a first size scale of the first image and the second image.
- a third plurality of feature vectors associated with the first image and a fourth plurality of feature vectors associated with the second image may be obtained.
- the third plurality of feature vectors and the fourth plurality of feature vectors may correspond to a second size scale of the first image and the second image.
- the second size scale may be larger than the first size scale.
- the fourth plurality of feature vectors may be warped based on the pixel correspondence between the first image and the second image at the first size scale.
- a second plurality of transformed feature vectors may be generated by transforming each respective feature vector of the third plurality of feature vectors by the kernel matrix.
- a second cost volume may be generated by determining, for each respective transformed feature vector of the second plurality of transformed feature vectors, a second plurality of inner products. Each respective inner product of the second plurality of inner products may be between the respective transformed feature vector of the second plurality of transformed feature vectors and a corresponding candidate feature vector of a corresponding subset of the fourth plurality of feature vectors as warped. Based on the second cost volume, a second pixel correspondence may be determined between the first image and the second image at the second size scale.
- FIG. 1 illustrates a computing system, in accordance with examples described herein.
- FIG. 2 illustrates a computing device, in accordance with examples described herein.
- FIG. 3 A illustrates a feature vector, in accordance with examples described herein.
- FIG. 3 B illustrates an image search window, in accordance with examples described herein.
- FIG. 3 C illustrates a cost volume, in accordance with examples described herein.
- FIG. 4 illustrates a trainable cost volume model, in accordance with examples described herein.
- FIG. 5 A illustrates aspects of a Euclidean inner product space, in accordance with examples described herein.
- FIG. 5 B illustrates aspects of an elliptical inner product space, in accordance with examples described herein.
- FIG. 6 illustrates a system for determining an optical flow, in accordance with examples described herein.
- FIG. 7 illustrates a flow diagram, in accordance with examples described herein.
- FIGS. 8 A and 8 B illustrate testing results, in accordance with examples described herein.
- Example methods, devices, and systems are described herein. It should be understood that the words “example” and “exemplary” are used herein to mean “serving as an example, instance, or illustration.” Any example or feature described herein as being an “example,” “exemplary,” and/or “illustrative” is not necessarily to be construed as preferred or advantageous over other examples or features unless stated as such. Thus, other examples can be utilized and other changes can be made without departing from the scope of the subject matter presented herein.
- any enumeration of elements, blocks, or steps in this specification or the claims is for purposes of clarity. Thus, such enumeration should not be interpreted to require or imply that these elements, blocks, or steps adhere to a particular arrangement or are carried out in a particular order. Unless otherwise noted, figures are not drawn to scale.
- a pixel correspondence between two images may indicate pairs of pixels where each pixel of the pair provides a different representation of a same portion of an environment.
- the pixel correspondence may be determined based on a cost volume.
- the cost volume may indicate a degree of similarity between a plurality of potentially-matching pixel pairs in a first image and a second image.
- the cost volume may indicate, for each respective pixel in the first image, a plurality of similarity metrics.
- Each respective similarity metric of the plurality of similarity metrics may be associated with a corresponding candidate pixel of a plurality of candidate pixels located within a search window in the second image.
- the search window (e.g., square, rectangle, etc.) may be used to select the plurality candidate pixels from the second image, and it's position within the second image may be based on (e.g., centered on) the coordinates of the respective pixel in the first image.
- the search window may indicate the candidate pixels in the second image that are most likely to correspond to the respective pixel in the first image.
- a similarity metric between a first pixel and a second pixel may be based on an inner product between a first feature vector associated with the first pixel and a second feature vector associated with the second pixel.
- a feature vector associated with a pixel may be determined by processing the pixel and/or one or more neighboring pixels by machine learning algorithms (e.g., convolutional neural networks), rule-based image processing algorithms, and/or combinations thereof.
- the feature vector may represent the informational content of the pixel and/or the one or more neighboring pixels.
- the inner product between the first feature vector and the second feature vector may thus represent an extent of similarity between these two feature vectors.
- some dimensions of the feature vectors may be correlated with one another, and may thus at least partly represent duplicative information. Further, some dimensions of the feature vectors may contain information that is useful in identifying pixel correspondences, while other dimensions may contain noise or information that is not as relevant. These problems may result from the combination of algorithms used to generate the feature vectors sometimes generating correlated and/or noisy vector values. Constructing the cost volume using a Euclidean inner product space does not provide a way to account for the correlations among different dimensions of the feature vectors and/or a way to weigh discriminative dimensions of the feature vectors differently than dimensions that represent noise and/or less relevant information.
- the cost volume may instead be defined based on an elliptical inner product.
- the elliptical inner product may be determined by transforming the first feature vector by a kernel matrix, and subsequently multiplying the transformed first feature vector by the second feature vector.
- the kernel matrix may be configured to define the elliptical inner product space via a training process.
- a cost volume generated using this kernel matrix may be referred to as a learned cost volume and/or a trained cost volume.
- the learned cost volume may allow for more accurate determinations of pixel correspondences when images include large pixel displacements, textureless regions, motion blur, occlusions, illumination changes, and non-Lambertian reflection, among others.
- a particular trained kernel matrix may be specific to feature vectors generated using a particular set of algorithms (e.g., convolutional neural networks).
- algorithms e.g., convolutional neural networks.
- the kernel matrix may be retrained to appropriately adjust the elliptical inner product space to handle the correlations and noise resulting from the modified algorithms.
- the kernel matrix may be symmetric and positive-definite. Explicitly imposing the constraints of symmetry and positive definiteness on the kernel matrix during training may complicate the training process, since each update to the kernel matrix made during training may need to both follow the gradient and result in a symmetric and positive definite matrix. Thus, potential updates that follow the gradient but do not result in a symmetric and positive definite matrix may be discarded until an update is determined that does result in a symmetric and positive definite matrix. This iterative process may be time consuming and computationally wasteful. Accordingly, rather than imposing the constraints of symmetry and positive definiteness on the kernel matrix directly, the kernel matrix may instead be decomposed into a diagonal matrix and an orthogonal matrix via eigenvalue decomposition.
- the diagonal matrix may include, along its diagonal, the eigenvalues of the kernel matrix. In order to satisfy the requirement of positive definiteness, the eigenvalues may be positive.
- the diagonal matrix may operate to re-weigh different dimensions of the feature vectors, which may be viewed geometrically as a stretching of the coordinate axes. For example, the diagonal matrix may amplify discriminative dimensions of the feature vectors while suppressing the dimensions that represent noise and/or indiscriminate signals.
- the eigenvalues represented by the diagonal matrix may be parametrized by a function that maps real values of a training parameter to positive real values of the eigenvalues. Thus, the training process may be carried out by modifying the training parameter, which is unconstrained, rather than the eigenvalue itself, which is to be positive.
- the orthogonal matrix may, as the name implies, be an orthogonal matrix such that a transpose of the orthogonal matrix is equal to an inverse of the orthogonal matrix.
- the orthogonal matrix may operate to linearly transform the feature vector into a new feature space which accounts for correlation among different dimensions thereof. This transformation may be viewed geometrically as a rotation of the coordinate axes.
- the orthogonal matrix may be trained either by parameterizing the orthogonal matrix in terms of a skew symmetric training matrix using the Cayley representation/transform, or by Riemann gradient descent on a Stiefel matrix manifold. In either case, the orthogonality of the orthogonal matrix may be achieved as a result of the training process without needing to impose thereon any explicit constraints, thus allowing the training process to freely follow the gradient determined with respect to the training parameters.
- the orthogonal matrix may be selected from a subset of the special orthogonal group of matrices, where matrices of the subset are connected and do not include eigenvalues of negative one in their spectra.
- Matrices of the subset may be expressed using the Cayley transform as a product of (i) a difference between an identity matrix and the skew symmetric training matrix and (ii) an inverse of a sum of the identity matrix and the skew symmetric training matrix. Since matrices of the subset are connected and include the identity matrix, the skew symmetric training matrix may be initiated to the identity matrix and its values may be iteratively refined until a desired orthogonal matrix is reached. The desired orthogonal matrix may be reached when the cost volume generated based thereon allows for determination of a pixel correspondence having at least a threshold accuracy.
- using Riemann gradient descent may involve selecting an initial orthogonal matrix from the Stiefel matrix manifold and determining a standard gradient at the point on the Stiefel matrix manifold corresponding to the initial orthogonal matrix.
- the standard gradient may then be projected to a tangent space that is tangent to the Stiefel matrix manifold at the point thereon corresponding to the initial orthogonal matrix, resulting in a natural gradient.
- the natural gradient may be used to update the values of the projected initial orthogonal matrix, resulting in an updated matrix that likely is no longer orthogonal since it resides in the tangent space, rather than the Stiefel matrix manifold.
- the updated matrix may be retracted back into the Stiefel matrix manifold, resulting in an updated matrix that is orthogonal.
- FIG. 1 illustrates an example form factor of computing system 100 .
- Computing system 100 may be, for example, a mobile phone, a tablet computer, or a wearable computing device. However, other examples are possible.
- Computing system 100 may include various elements, such as body 102 , display 106 , and buttons 108 and 110 .
- Computing system 100 may further include front-facing camera 104 , rear-facing cameras 112 and 113 , front-facing infrared camera 114 , and infrared pattern projector 116 .
- Front-facing camera 104 may be positioned on a side of body 102 typically facing a user while in operation (e.g., on the same side as display 106 ).
- Rear-facing cameras 112 and 113 may be positioned on a side of body 102 opposite front-facing camera 104 . Referring to the cameras as front and rear facing is arbitrary, and computing system 100 may include multiple cameras positioned on various sides of body 102 .
- Front-facing camera 104 and rear-facing cameras 112 and 113 may each be configured to capture images in the visible light spectrum.
- Display 106 could represent a cathode ray tube (CRT) display, a light emitting diode (LED) display, a liquid crystal (LCD) display, a plasma display, an organic light emitting diode (OLED) display, or any other type of display known in the art.
- display 106 may display a digital representation of the current image being captured by front-facing camera 104 , rear-facing camera 112 , rear-facing camera 113 , and/or infrared camera 114 , and/or an image that could be captured or was recently captured by one or more of these cameras.
- display 106 may serve as a viewfinder for the cameras.
- Display 106 may also support touchscreen functions that may be able to adjust the settings and/or configuration of any aspect of computing system 100 .
- Front-facing camera 104 may include an image sensor and associated optical elements such as lenses. Front-facing camera 104 may offer zoom capabilities or could have a fixed focal length. In other examples, interchangeable lenses could be used with front-facing camera 104 . Front-facing camera 104 may have a variable mechanical aperture and a mechanical and/or electronic shutter. Front-facing camera 104 also could be configured to capture still images, video images, or both. Further, front-facing camera 104 could represent, for example, a monoscopic camera. Rear-facing camera 112 , rear-facing camera 113 , and/or infrared camera 114 may be similarly or differently arranged.
- front-facing camera 104 may be an array of one or more cameras.
- rear-facing camera 112 and rear-facing camera 113 may form part of a stereoscopic camera.
- One or more of front-facing camera 104 , rear-facing camera 112 , and/or rear-facing camera 113 may include or be associated with an illumination component that provides a light field in the visible light spectrum to illuminate a target object.
- an illumination component could provide flash or constant illumination of the target object.
- An illumination component could also be configured to provide a light field that includes one or more of structured light, polarized light, and light with specific spectral content. Other types of light fields known and used to recover three-dimensional (3D) models from an object are possible within the context of the examples herein.
- Infrared pattern projector 116 may be configured to project an infrared structured light pattern onto the target object.
- infrared projector 116 may be configured to project a dot pattern and/or a flood pattern.
- infrared projector 116 may be used in combination with infrared camera 114 to determine a plurality of depth values corresponding to different physical features of the target object.
- infrared projector 116 may project a known and/or predetermined dot pattern onto the target object, and infrared camera 114 may capture an infrared image of the target object that includes the projected dot pattern.
- Computing system 100 may then determine a correspondence between a region in the captured infrared image and a particular part of the projected dot pattern. Given a position of infrared projector 116 , a position of infrared camera 114 , and the location of the region corresponding to the particular part of the projected dot pattern within the captured infrared image, computing system 100 may then use triangulation to estimate a depth to a surface of the target object. By repeating this for different regions corresponding to different parts of the projected dot pattern, computing system 100 may estimate the depth of various physical features or portions of the target object. In this way, computing system 100 may be used to generate a three-dimensional (3D) model of the target object.
- 3D three-dimensional
- Computing system 100 may also include an ambient light sensor that may continuously or from time to time determine the ambient brightness of a scene (e.g., in terms of visible and/or infrared light) that cameras 104 , 112 , 113 , and/or 114 can capture.
- the ambient light sensor can be used to adjust the display brightness of display 106 .
- the ambient light sensor may be used to determine an exposure length of one or more of cameras 104 , 112 , 113 , or 114 , or to help in this determination.
- Computing system 100 could be configured to use display 106 and front-facing camera 104 , rear-facing camera 112 , rear-facing camera 113 , and/or front-facing infrared camera 114 to capture images of a target object.
- the captured images could be a plurality of still images or a video stream.
- the image capture could be triggered by activating button 108 , pressing a softkey on display 106 , or by some other mechanism.
- the images could be captured automatically at a specific time interval, for example, upon pressing button 108 , upon appropriate lighting conditions of the target object, upon moving digital camera device 100 a predetermined distance, or according to a predetermined capture schedule.
- FIG. 2 is a simplified block diagram showing some of the components of an example computing device 200 that may include camera components 224 .
- computing device 200 may be a cellular mobile telephone (e.g., a smartphone), a still camera, a video camera, a computer (such as a desktop, notebook, tablet, or handheld computer), personal digital assistant (PDA), a home automation component, a digital video recorder (DVR), a digital television, a remote control, a wearable computing device, a gaming console, a robotic device, or some other type of device.
- Computing device 200 may be equipped with at least some image capture and/or image processing capabilities. It should be understood that computing device 200 may represent a physical image processing system, a particular physical hardware platform on which an image sensing and processing application operates in software, or other combinations of hardware and software that are configured to carry out image capture and/or processing functions.
- computing device 200 may include communication interface 202 , user interface 204 , processor 206 , data storage 208 , and camera components 224 , all of which may be communicatively linked together by a system bus, network, or other connection mechanism 210 .
- Communication interface 202 may allow computing device 200 to communicate, using analog or digital modulation, with other devices, access networks, and/or transport networks.
- communication interface 202 may facilitate circuit-switched and/or packet-switched communication, such as plain old telephone service (POTS) communication and/or Internet protocol (IP) or other packetized communication.
- POTS plain old telephone service
- IP Internet protocol
- communication interface 202 may include a chipset and antenna arranged for wireless communication with a radio access network or an access point.
- communication interface 202 may take the form of or include a wireline interface, such as an Ethernet, Universal Serial Bus (USB), or High-Definition Multimedia Interface (HDMI) port.
- USB Universal Serial Bus
- HDMI High-Definition Multimedia Interface
- Communication interface 202 may also take the form of or include a wireless interface, such as a Wi-Fi, BLUETOOTH®, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or 3GPP Long-Term Evolution (LTE)).
- a wireless interface such as a Wi-Fi, BLUETOOTH®, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or 3GPP Long-Term Evolution (LTE)).
- GPS global positioning system
- LTE 3GPP Long-Term Evolution
- communication interface 202 may comprise multiple physical communication interfaces (e.g., a Wi-Fi interface, a BLUETOOTH® interface, and a wide-area wireless interface).
- User interface 204 may function to allow computing device 200 to interact with a human or non-human user, such as to receive input from a user and to provide output to the user.
- user interface 204 may include input components such as a keypad, keyboard, touch-sensitive panel, computer mouse, trackball, joystick, microphone, and so on.
- User interface 204 may also include one or more output components such as a display screen which, for example, may be combined with a touch-sensitive panel. The display screen may be based on CRT, LCD, and/or LED technologies, or other technologies now known or later developed.
- User interface 204 may also be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices.
- User interface 204 may also be configured to receive and/or capture audible utterance(s), noise(s), and/or signal(s) by way of a microphone and/or other similar devices.
- user interface 204 may include a display that serves as a viewfinder for still camera and/or video camera functions supported by computing device 200 (e.g., in both the visible and infrared spectrum). Additionally, user interface 204 may include one or more buttons, switches, knobs, and/or dials that facilitate the configuration and focusing of a camera function and the capturing of images. It may be possible that some or all of these buttons, switches, knobs, and/or dials are implemented by way of a touch-sensitive panel.
- Processor 206 may comprise one or more general purpose processors—e.g., microprocessors—and/or one or more special purpose processors—e.g., digital signal processors (DSPs), graphics processing units (GPUs), floating point units (FPUs), network processors, or application-specific integrated circuits (ASICs).
- DSPs digital signal processors
- GPUs graphics processing units
- FPUs floating point units
- ASICs application-specific integrated circuits
- special purpose processors may be capable of image processing, image alignment, and merging images, among other possibilities.
- Data storage 208 may include one or more volatile and/or non-volatile storage components, such as magnetic, optical, flash, or organic storage, and may be integrated in whole or in part with processor 206 .
- Data storage 208 may include removable and/or non-removable components.
- Processor 206 may be capable of executing program instructions 218 (e.g., compiled or non-compiled program logic and/or machine code) stored in data storage 208 to carry out the various functions described herein. Therefore, data storage 208 may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by computing device 200 , cause computing device 200 to carry out any of the methods, processes, or operations disclosed in this specification and/or the accompanying drawings. The execution of program instructions 218 by processor 206 may result in processor 206 using data 212 .
- program instructions 218 e.g., compiled or non-compiled program logic and/or machine code
- program instructions 218 may include an operating system 222 (e.g., an operating system kernel, device driver(s), and/or other modules) and one or more application programs 220 (e.g., camera functions, address book, email, web browsing, social networking, audio-to-text functions, text translation functions, and/or gaming applications) installed on computing device 200 .
- data 212 may include operating system data 216 and application data 214 .
- Operating system data 216 may be accessible primarily to operating system 222
- application data 214 may be accessible primarily to one or more of application programs 220 .
- Application data 214 may be arranged in a file system that is visible to or hidden from a user of computing device 200 .
- Application programs 220 may communicate with operating system 222 through one or more application programming interfaces (APIs). These APIs may facilitate, for instance, application programs 220 reading and/or writing application data 214 , transmitting or receiving information via communication interface 202 , receiving and/or displaying information on user interface 204 , and so on.
- APIs application programming interfaces
- application programs 220 may be referred to as “apps” for short. Additionally, application programs 220 may be downloadable to computing device 200 through one or more online application stores or application markets. However, application programs can also be installed on computing device 200 in other ways, such as via a web browser or through a physical interface (e.g., a USB port) on computing device 200 .
- Camera components 224 may include, but are not limited to, an aperture, shutter, recording surface (e.g., photographic film and/or an image sensor), lens, shutter button, infrared projectors, and/or visible-light projectors.
- Camera components 224 may include components configured for capturing of images in the visible-light spectrum (e.g., electromagnetic radiation having a wavelength of 400-700 nanometers) and components configured for capturing of images in the infrared light spectrum (e.g., electromagnetic radiation having a wavelength of 701 nanometers-1 millimeter).
- Camera components 224 may be controlled at least in part by software executed by processor 206 .
- FIG. 3 A illustrates an example feature vector associated with a pixel of an image.
- FIG. 3 A illustrates image 300 having a width W and a height H, each measured in pixels.
- image 300 includes H ⁇ W pixels.
- Each pixel of image 300 may be associated with a corresponding feature vector.
- pixel 302 may be associated with feature vector 304 , which may include a plurality of values F 1 ⁇ F C (i.e., F 1 , F 2 , F 3 , F 4 , F 5 , F 6 , F 7 , and F 8 through F C ) each associated with a corresponding dimension of feature vector 304 .
- F 1 ⁇ F C i.e., F 1 , F 2 , F 3 , F 4 , F 5 , F 6 , F 7 , and F 8 through F C
- Feature vector 304 may be generated by processing pixel 302 and/or one or more other pixels adjacent to pixel 302 by way of one or more machine learning models.
- feature vector 304 may represent convolutional features of pixel 302 and/or its neighboring pixels determined by way of one or more convolutional neural networks.
- feature vector 304 may represent the output of processing pixel 302 and/or its neighboring pixels by way of one or more rule-based image processing algorithms.
- the values F 1 -F C of feature vector 304 may be determined in other ways.
- feature vector 304 may represent the informational content of pixel 302 and/or the one or more other pixels adjacent thereto (e.g., the presence of certain colors, geometric features, and/or other patterns therein).
- the respective feature vectors of two different pixels may be used to determine a similarity metric between the two different pixels.
- the similarity metric may be used to find a correspondence between pixels of the two images.
- the pixel correspondence may, in some cases, be referred to as a pixel disparity.
- FIG. 3 B illustrates part of a process for finding a pixel correspondence between two images.
- FIG. 3 B includes image 300 and image 306 , each of which may provide different representations of a common environment.
- Images 300 and/or 306 may be generated using cameras 104 , 112 , 113 , and/or 114 of computing system 100 and/or camera components 224 of computing device 200 .
- images 300 and 306 may be captured substantially simultaneously using a stereoscopic camera, and may thus each represent the environment from different perspectives.
- finding a pixel correspondence between images 300 and 306 may serve as a basis for determining a depth image based on images 300 and 306 .
- image 300 may be captured at a first time and image 306 may be captured at a second time later than the first time, and may thus each represent the environment at different points in time.
- finding a pixel correspondence between images 300 and 306 may serve as a basis for determining an optical flow associated with images 300 and 306 .
- a correspondence between a particular pixel in image 300 and another pixel in image 306 may be determined by comparing the particular pixel to a plurality of candidate pixels of image 306 that are located within a search window.
- the position of the search window within image 306 may be based on coordinates of the particular pixel in image 300 .
- search window 310 shown in image 306 may be used to find a match for pixel 308 A in image 300 .
- Search window 310 may have a width U and a height V and may be centered on pixel 308 B in image 306 .
- Pixel 308 A and 308 B may share the same coordinates in images 300 and 306 , respectively. That is, pixel 308 A may be located in row 5, column 5 of image 300 , and pixel 308 B may be located in row 5, column 5 of image 306 .
- the respective feature vector of each pixel in image 300 may be compared to U ⁇ V candidate feature vectors of corresponding pixels in image 306 .
- Search window 310 may indicate a maximum expected displacement of the environmental feature represented by pixel 308 A between images 300 and 306 .
- search windows 310 may include, but might not be centered on, pixel 308 B.
- Each respective pixel in image 300 may be associated with a corresponding search window in image 306 that includes the position of the respective pixel (e.g., by being centered on this position).
- cost volume 312 of FIG. 3 C includes an H ⁇ W ⁇ U*V array of similarity metrics resulting from comparisons of the feature vectors of images 300 and 306 .
- image 300 includes H ⁇ W pixels, each associated with a corresponding feature vector. Each of these H ⁇ W feature vectors is compared to U*V candidate feature vectors in image 306 that represent a potential pixel correspondence (i.e., are associated with pixels within a corresponding search window).
- cost volume 312 may instead be represented as a 4-dimensional H ⁇ W ⁇ U ⁇ V tensor, rather than the 3-dimensional H ⁇ W ⁇ U*V tensor illustrated in FIG. 3 C , but may nevertheless represent the same informational content.
- Cost volume 312 may represent the cost associated with a plurality of possible pixel correspondences between images 300 and 306 .
- Cost volume 312 may be processed by one or more machine learning models to select, for each pixel of a plurality of pixels in image 300 , a corresponding pixel in image 306 .
- the pixel in image 300 and the corresponding pixel in image 306 matched thereto should represent the same and/or substantially the same portion of the environment, albeit from different perspectives and/or at different times.
- the one or more machine learning model may be configured to select individual pixel pairings that improve or optimize the global cost of such pairings, rather than optimizing a local cost of the pairings. That is, the one or more machine learning models may select pixel correspondences that work well and/or best across images 300 and 306 as a whole, rather than only for some of the pixels therein.
- the similarity metric between two feature vectors of two corresponding pixels may be represented by a Euclidean inner product between the two feature vectors.
- the Euclidean inner product may limit the representational capacity of cost volume 312 for at least two reasons. First, the Euclidean inner product might not account for correlations among different dimensions of the feature vectors. Second, each dimension of the feature vectors may contribute equally to the Euclidean inner product. Thus, dimensions that represent noise may be given the same weight by the Euclidean inner product as dimensions that represent discriminative signals that are useful in determining accurate pixel correspondences. Accordingly, rather than determining cost volume 312 using a fixed calculation on the basis of the Euclidean inner product, cost volume 312 may instead be constructed using a trained cost volume model.
- FIG. 4 illustrates cost volume model 400 which may be trained to account for the correlation among different dimensions of feature vectors and to re-weigh the contribution of each feature vector dimension to the cost volume.
- Cost volume model 400 may be configured to compare feature vectors 404 - 406 of image 402 to feature vectors 410 - 412 of image 408 to determine cost volume 434 .
- Images 402 and 408 of FIG. 4 may represent images 300 and 306 of FIG. 3 B
- cost volume 434 may represent cost volume 312 of FIG. 3 C .
- Pixel correspondence matching model 436 may be configured to determine a pixel correspondence between the respective pixels of images 402 and 408 based on cost volume 434 .
- Cost volume model 400 may include vector comparison selector 414 , which may be configured to iterate through candidate pixel pairings for images 402 and 408 .
- vector comparison selector 414 may be configured to compare each respective feature vector of feature vectors 404 - 406 to each respective candidate feature vector of a corresponding plurality of candidate feature vectors selected from feature vectors 410 - 412 .
- the corresponding plurality of candidate feature vectors may be selected based on a search window corresponding to the respective feature vector from image 402 , as illustrated in and discussed with respect to FIG. 3 B .
- the corresponding plurality of candidate feature vectors may be associated with pixels that are within a threshold distance (e.g., as defined by a search window) of a pixel coordinate associated with the respective feature vector from image 402 .
- vector comparison selector 414 may iterate (i, j) through every pixel in image 402 and, for each respective pixel in image 402 , iterate (k′, l′) through every pixel in image 408 that is within the search window corresponding to the respective pixel of image 402 .
- Cost volume model 400 may be configured to apply transpose operator 416 to the respective feature vector of feature vectors 404 - 406 selected for comparison by vector comparison selector 414 .
- Cost volume model 400 may also be configured to apply inner product operator 430 to the transposed respective feature vector and kernel matrix 428 .
- Cost volume model 400 may additionally be configured to apply inner product operator 432 to the output of inner product operator 430 and a candidate feature vector selected from a corresponding subset of feature vectors 410 - 412 by vector comparison selector 414 .
- F 1 is a W ⁇ H ⁇ C tensor representing feature vectors 404 - 406 (with C representing the number of dimensions of each feature vector)
- F 2 is a W ⁇ H ⁇ C tensor representing feature vectors 410 - 412
- F i,j 1 is a vector having C values and representing
- W W T
- positive definite matrix i.e., having only positive eigenvalues
- Using a kernel matrix that is not symmetric and/or positive definite, and thus does not define a valid inner product space, may result in a cost volume that is not as useful for finding pixel correspondences as a cost volume resulting from a valid inner product space.
- kernel matrix 428 may be computed based on orthogonal matrix 418 and diagonal matrix 422 , stored by cost volume model 400 , and reused multiple times. That is, in order to reduce computational overhead, kernel matrix 428 might not be recomputed by way of operators 420 , 424 , and 426 for each computation carried out by operators 416 , 430 , and 432 based on feature vectors 404 - 406 and 410 - 412 . Kernel matrix 428 may, however, be recomputed each time orthogonal matrix 418 and/or diagonal matrix 422 is updated due to training.
- FIGS. 5 A and 5 B visually illustrate the effect of multiplying feature vectors by kernel matrix 428 .
- FIG. 5 A visually illustrates a Euclidean inner product space defined with respect to Euclidean reference frame 500
- FIG. 5 B visually illustrates an elliptical inner product space defined with respect to elliptical reference frame 510 .
- Feature vector f 1 plotted along Euclidean reference frame 500 represents an example feature vector selected from F 1
- feature vector f 2 represents an example feature vector selected from F 2 .
- feature vector f 1 may be correlated with feature vector f 2 .
- feature vector f 1 includes an x-component directed along the same direction as the feature vector f 1 .
- Elliptical reference frame 510 may be trained to adjust the extent of correlation between feature vector f 1 and f 2 .
- elliptical reference frame 510 may be transformed relative to Euclidean reference frame 500 such that feature vectors f 1 and f 2 are orthogonal to one another, thus illustrating that kernel matrix 428 may be trained to decorrelate at least some dimensions of the feature vectors.
- Transformed feature vector f 1 W may be rotated and/or scaled relative to feature vector f 1 by the transformation via kernel matrix 428 . Due to this rotation and/or scaling, transformed feature vector f 1 W may be correlated with feature vector f 2 in elliptical reference frame 510 to a different extent than feature vector f 1 is correlated with feature vector f 2 in Euclidean reference frame 500 .
- kernel matrix 428 may be trained to adjust the scalar value resulting from taking the inner product between two feature vectors such that the scalar value associated with similar feature vectors is increased and the scalar value associated with dissimilar feature vectors is decreased.
- this may result in the transformed feature vector being orthogonal to or approximately orthogonal to (i.e., uncorrelated to) some candidate feature vectors, thus indicating a poor pixel correspondence match, and being coincident with or approximately coincident with (i.e., highly correlated with) one or more other candidate feature vectors, thus indicating a good pixel correspondence match.
- kernel matrix 428 directly may be difficult because, in order to define a valid inner product space, values of kernel matrix 428 may be selected such that it is symmetric and positive definite. Thus, with each training iteration, kernel matrix 428 might be updated, based on a gradient descent calculation, with values that (i) follow the gradient and (ii) result in kernel matrix 428 being symmetric and positive definite. Directly finding values for kernel matrix 428 that satisfy both criteria may be difficult and time-consuming. Thus, kernel matrix 428 may instead be expressed based on orthogonal matrix 418 and diagonal matrix 422 , each of which may provide a corresponding set of adjustable training parameters that facilitate training/learning of kernel matrix 428 .
- diagonal matrix 422 may comprise positive values (i.e., the eigenvalues of kernel matrix 428 ) along its diagonal.
- the entries ⁇ i may instead be parametrized using a function that maps real values to positive real values.
- t i is a training parameter modifiable during training.
- training parameter t i may be modifiable based on a gradient descent process without imposing any additional constraints on the range of values that training parameter t i may have.
- Orthogonal matrix 418 may also be parametrized in order to simplify the training process.
- the training of orthogonal matrix 418 may be facilitated by using the Cayley transform to express orthogonal matrix 418 in terms of a training matrix S and an identity matrix I.
- the set of matrices that defines special orthogonal group SO*(C) may be a connected set, and may include a C ⁇ C identity matrix I as an element thereof. Accordingly, in training, orthogonal matrix P may be initialized as identity matrix I. Assuming P* to be the optimal orthogonal matrix, P* may be reached via training after initializing orthogonal matrix P as the identity matrix I because matrices of special orthogonal group SO*(C), including P* and I, are connected. Thus, there exists a continuous path joining identity matrix I and any orthogonal matrix P that is an element of special orthogonal group SO*(C), including optimal orthogonal matrix P*.
- training matrix S may be parametrized such that they can be freely adjusted during training without any additional constraints imposed thereupon.
- kernel matrix 428 may be trained by adjusting training matrix parameters s 1 -s C (defining orthogonal matrix 418 ) and/or diagonal matrix parameters t 1 -t C (defining diagonal matrix 422 ) based on a gradient computed based on backpropagation through the cost volume model.
- training matrix parameters s 1 -s C and/or diagonal matrix parameters t 1 -t C may be adjusted freely, without any explicit constraints imposed thereon, because the symmetry and positive definiteness of kernel matrix 428 results from how the values of these parameters are mapped to the values of the entries of orthogonal matrix 418 and diagonal matrix 422 .
- the training of orthogonal matrix 418 may be performed using Riemann gradient descent on a Stiefel matrix manifold.
- a T A I k ⁇ , where R n ⁇ k represents the space of n ⁇ k orthogonal matrices.
- a T A I C , which may contain therein all C ⁇ C (square) orthogonal matrices, thus satisfying the orthogonality condition of orthogonal matrix P.
- initial values may be selected for orthogonal matrix P such that initial orthogonal matrix P lies in the Stiefel matrix manifold V C (R C ).
- a gradient matrix Z at the point on the Stiefel matrix manifold corresponding to initial orthogonal matrix P may be determined.
- Gradient matrix Z may represent a standard gradient associated with initial orthogonal matrix P.
- the projection ⁇ P (Z) of gradient matrix Z at initial orthogonal matrix P may represent a natural gradient on the Stiefel matrix manifold.
- Gradient descent may be performed based on projection H(Z) to determine an updated matrix P′.
- the projection of the updated orthogonal matrix, adjustment of the values of the projected orthogonal matrix, and retraction of the adjusted orthogonal matrix may be repeated one or more times until a final orthogonal matrix P is achieved.
- Final orthogonal matrix P may, along with diagonal matrix A, provide a target level of accuracy in matching pixels with respect to a set of test data that includes a plurality of image pairs.
- the values of orthogonal matrix P and diagonal matrix A may be determined by training these matrices based on a training dataset that includes a plurality of pairs of images. Each of the two images within each pair may provide a different representation of a common environment, such that there exists a correspondence between at least some of the pixels in the two images. Orthogonal matrix P and diagonal matrix A may be assigned initial values based on which initial values for kernel matrix W may be determined.
- the plurality of pairs of images of the training dataset may be processed by cost volume model 400 using kernel matrix W assigned the initial values to determine, for each of the plurality of pairs, a training cost volume, which may be represented by cost volume 434 and may be designated a “training” cost volume because it is determined as part of the training process.
- the training cost volume may be used by pixel correspondence matching model 436 to determine a training pixel correspondence.
- the training pixel correspondence may be compared to a ground-truth pixel correspondence and/or an approximation thereof by way of a loss function.
- the loss function may explicitly compare the training pixel correspondence to a known ground-truth pixel correspondence.
- the training dataset may include pairs of synthetic (e.g., computer generated) images with known pixel correspondences.
- the loss function may measure a photometric loss (e.g., how visually similar two paired pixels actually are to one another), a smoothness loss (e.g., how abruptly the apparent displacement changes between neighboring pixels), and/or a local contrast normalization loss, among other possible loss functions.
- the loss functions in unsupervised learning applications may provide an approximation of a ground-truth correspondence, since an explicit ground-truth correspondence might not be known.
- a gradient of the loss function may be determined with respect to the training parameters that define orthogonal matrix P and diagonal matrix ⁇ , and the gradient may be used to determine one or more updates to the training parameters.
- kernel matrix W may be updated based on updated orthogonal matrix P and updated diagonal matrix ⁇ , and another iteration of the training process may be performed with respect to the training dataset.
- kernel matrix W may be iteratively refined.
- the training may be considered complete when cost volume model 400 generates cost volumes that allow pixel correspondence matching model 436 to determine pixel correspondences for the training dataset (or another dataset reserved for testing) with at least a threshold level of accuracy (e.g., 75%, 90%, 99%, etc.), as measured by way of one or more loss functions.
- a threshold level of accuracy e.g., 75%, 90%, 99%, etc.
- FIG. 6 illustrates cost volume model 400 implemented as part of optical flow system 640 configured to determine an optical flow between pairs of images.
- optical flow system 640 may be configured to determine an optical flow between image 600 and image 608 .
- Optical flow system 640 may include cost volume model 400 , feature warping model 622 , optical flow model 624 , and optical flow up-sampling model 628 .
- Each of these components of optical flow system 640 may be implemented as hardware components, software components, or a combination of both. Further, each of these components may represent machine learning models, rule-based algorithms, or combinations of both.
- cost volume model 400 may be jointly trained, allowing cost volume model 400 to learn to generate cost volume 434 such that it is useful to the other components in determining an optical flow.
- Images 600 and 608 may each be associated with one or more sets of feature vectors corresponding to one or more size scales.
- images 600 and 608 may be associated with feature vectors 606 and 614 , respectively, corresponding to size scale 620 , feature vectors 604 and 612 , respectively, corresponding to size scale 618 , and feature vectors 602 and 610 , respectively, corresponding to size scale 616 .
- Size scale 620 may correspond to full-resolution versions of images 600 and 608 (e.g., 1,879,200 pixels per image), size scale 618 may correspond to half-resolution versions of images 600 and 608 (e.g., 939,600 pixels per image), and size scale 616 may correspond to quarter-resolution versions of images 600 and 608 (e.g., 469,800 pixels per image).
- feature vectors 604 may include half the number of feature vectors 606
- feature vectors 602 may include a quarter of the number of feature vectors 606 , as indicated by the relative sizing of the corresponding blocks in FIG. 6 .
- Other size scales are possible.
- Optical flow system 640 may be configured to process the feature vectors of images 600 and 608 in a pyramidal manner, starting at the lowest resolution size scale 616 and working up to full-resolution size scale 620 .
- cost volume 400 may be configured to generate cost volume 434 based on feature vectors 602 and 610 .
- the feature vectors of image 608 may be warped by feature warping model 622 based on a previously-determined optical flow.
- feature vectors 610 might not be warped because a previously-determined optical flow might not be available.
- Optical flow model 624 may be configured to determine optical flow 626 based on cost volume 434 .
- Optical flow 626 may indicate, for each respective pixel of a plurality of pixels of image 600 , a direction and magnitude in which the respective pixel was displaced relative to image 608 .
- optical flow 626 may comprise a flow field indicative of the apparent movement of the pixels of image 600 between image 600 and image 608 .
- Optical flow model 624 may represent an example implementation of pixel correspondence matching model 436 , and/or may include pixel correspondence matching model 436 as a component thereof.
- Optical flow up-sampling model 628 may be configured to generate up-sampled optical flow 630 based on optical flow 626 .
- up-sampled optical flow 630 may correspond to the next higher size scale, that is, to size scale 618 .
- Feature warping model 622 may be configured to warp feature vectors 612 , corresponding to size scale 618 , based on up-sampled optical flow 630 , which was determined based on feature vectors 602 and 610 at size scale 616 .
- large-scale flows determined at size scale 616 may be accounted for at size scale 618 by feature warping model 622 , allowing the processing at size scale 618 to focus on smaller-scale flows.
- cost volume model 400 may be configured to generate cost volume 434 based on feature vectors 604 and warped feature vectors 612 .
- Optical flow model 624 may be configured to determine optical flow 626 based on cost volume 434 and up-sampled optical flow 630 determined based on size scale 616 , thus accounting for both large-scale flows from size scale 616 and smaller-scale flows from size scale 618 .
- Optical flow up-sampling model 628 may be configured to generate up-sampled optical flow 630 based on optical flow 626 at size scale 618 .
- Feature warping model 622 may be configured to warp feature vectors 614 , corresponding to size scale 620 , based on up-sampled optical flow 630 corresponding to size scale 618 .
- Warped feature vectors 614 and feature vectors 606 may be processed by cost volume model 400 to generate cost volume 434 for size scale 620
- optical flow model 624 may generate optical flow 626 based on cost volume 434 corresponding to size scale 620 and up-sampled optical flow 630 from size scale 618 . Since optical flow 626 at size scale 620 may correspond to a full resolution of images 600 and 608 , optical flow 626 might not be upsampled any further.
- cost volume model 400 that includes trainable kernel matrix 428 , rather than a cost volume model that relies on the Euclidean inner product, the accuracy of optical flow fields generated by optical flow system 640 may be improved.
- cost volume model 400 may also improve the quality of the outputs of other down-stream tasks, such as video interpolation, video prediction, video segmentation, and/or action recognition, among others, that utilize the determined optical flow field.
- Cost volume model 400 may additionally or alternatively be used as part of other systems that determine a pixel correspondence between two or more images, such as for the purpose of determining a depth image based on stereoscopic images. In some cases, these other systems may use a pyramidal architecture similar to that of optical flow system 640 .
- cost volume model 400 may include a corresponding kernel matrix for each size scale.
- cost volume model 400 may include a first kernel matrix corresponding to size scale 616 , a second kernel matrix corresponding to size scale 618 , and a third kernel matrix corresponding to size scale 620 .
- Each of the first, second, and third kernel matrices may be trainable, as discussed above, using corresponding orthogonal and diagonal matrices.
- An example implementation of optical flow system 640 without cost volume model 400 may include around 41 million parameters, meaning that the addition of cost volume model 400 increases the size of optical flow system 640 by less than 2%.
- FIG. 7 illustrates a flow chart of operations related to determining a pixel correspondence between two images based on a learnable/trainable cost volume.
- the operations may be carried out by one or more of computing system 100 , computing device 200 , cost volume model 400 , pixel correspondence matching model 436 , and/or optical flow system 640 , among other possible types of devices or device subsystems.
- the examples of FIG. 7 may be simplified by the removal of any one or more of the features shown therein. Further, these examples may be combined with features, aspects, and/or implementations of any of the previous figures or otherwise described herein.
- Block 700 may involve obtaining (i) a first plurality of feature vectors associated with a first image and (ii) a second plurality of feature vectors associated with a second image.
- Block 702 may involve generating a plurality of transformed feature vectors by transforming each respective feature vector of the first plurality of feature vectors by a kernel matrix trained to define an elliptical inner product space.
- Block 704 may involve generating a cost volume by determining, for each respective transformed feature vector of the plurality of transformed feature vectors, a plurality of inner products. Each respective inner product of the plurality of inner products may be between the respective transformed feature vector and a corresponding candidate feature vector of a corresponding subset of the second plurality of feature vectors.
- Block 706 may involve determining, based on the cost volume, a pixel correspondence between the first image and the second image.
- the kernel matrix may be symmetric and positive definite.
- the kernel matrix may be defined based on a product of (i) a diagonal matrix that includes a plurality of positive diagonal values and (ii) an orthogonal matrix.
- the kernel matrix may be defined via eigendecomposition by determining (i) a first product between a transpose of the diagonal matrix and the orthogonal matrix, and (ii) a second product between the first product and the diagonal matrix.
- each respective positive diagonal value of the plurality of positive diagonal values of the diagonal matrix may include a respective weight applied to a corresponding dimension of each respective feature vector of the first plurality of feature vectors.
- the orthogonal matrix may be configured to apply a rotation to each respective feature vector of the first plurality of feature vectors.
- each respective positive diagonal value of the plurality of positive diagonal values may be expressed as a function of a corresponding training parameter.
- the function may map (i) real number values assigned to the corresponding training parameter to (ii) positive real number values.
- the kernel matrix may be trained to define the elliptical inner product space by iteratively adjusting, for each respective positive diagonal value, a real number value assigned to the corresponding training parameter.
- the orthogonal matrix may be an element of a subset of a special orthogonal group of matrices.
- the subset may be selected such that eigenvalues of matrices of the subset exclude negative one.
- matrices of the subset may be connected.
- the orthogonal matrix may be expressed using a Cayley representation as a product of: (i) a difference between an identity matrix and a training matrix, and (ii) an inverse of a sum of the identity matrix and the training matrix.
- the training matrix may be skew-symmetric.
- the kernel matrix may be trained to define the elliptical inner product space by initializing the training matrix as the identity matrix and iteratively adjusting values of the training matrix.
- the orthogonal matrix may be an element of a Stiefel matrix manifold that includes a plurality of square orthonormal matrices.
- the kernel matrix may be trained to define the elliptical inner product space by a Riemann gradient descent that includes determining a projection of a gradient from the Stiefel matrix manifold to a tangent space at the orthogonal matrix, adjusting values of the orthogonal matrix based on the projection of the gradient, and determining a retraction of the orthogonal matrix as adjusted into the Stiefel matrix.
- each respective feature vector of the first plurality of feature vectors may be associated with a corresponding pixel of the first image.
- Each respective feature vector of the second plurality of feature vectors may be associated with a corresponding pixel of the second image.
- the corresponding subset of the second plurality of feature vectors may be associated with a plurality of pixels located within a search window that includes the particular pixel position within the second image.
- obtaining the first plurality of feature vectors and the second plurality of feature vectors may include generating the first plurality of feature vectors by processing the first image by one or more machine learning models and generating the second plurality of feature vectors by processing the second image by the one or more machine learning models.
- the first plurality of feature vectors and the second plurality of feature vectors may correspond to a first size scale of the first image and the second image.
- a third plurality of feature vectors associated with the first image and a fourth plurality of feature vectors associated with the second image may be obtained.
- the third plurality of feature vectors and the fourth plurality of feature vectors may correspond to a second size scale of the first image and the second image.
- the second size scale may be larger than the first size scale.
- the fourth plurality of feature vectors may be warped based on the pixel correspondence between the first image and the second image at the first size scale.
- a second plurality of transformed feature vectors may be generated by transforming each respective feature vector of the third plurality of feature vectors by the kernel matrix.
- a second cost volume may be generated by determining, for each respective transformed feature vector of the second plurality of transformed feature vectors, a second plurality of inner products. Each respective inner product of the second plurality of inner products may be between the respective transformed feature vector of the second plurality of transformed feature vectors and a corresponding candidate feature vector of a corresponding subset of the fourth plurality of feature vectors as warped. Based on the second cost volume, a second pixel correspondence may be determined between the first image and the second image at the second size scale.
- FIGS. 8 A and 8 B illustrate test results of various optical flow models/methods executed against commonly-used benchmark datasets.
- FIG. 8 A illustrates test results for various supervised models/methods executed against the Sintel benchmark dataset, which includes a Clean pass portion and a Final pass portion, and KITTI 2015 benchmark dataset. Additional details of the Sintel benchmark dataset are provided in the paper titled “A Naturalistic Open Source Movie for Optical Flow Evaluation,” authored by Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and Michael J. Black, and published by the European Conference on Computer Vision 2012. Additional details of the KITTI 2015 benchmark dataset are provided in the paper titled “Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite,” authored by Andreas Geiger, Philip Lenz, and Raquel Urtasun, and published by the Conference on Computer Vision and Pattern Recognition 2012.
- FIG. 8 B illustrates test results for various unsupervised models executed against the KITTI 2015 benchmark dataset. Both benchmark datasets include a training subset, which may be reserved for model training, and a testing subset, which may be reserved for model testing. In each of FIGS. 8 A and 8 B , results of the best-performing model with respect to each metric are indicated by the darkened pattern of the corresponding table cell.
- the supervised models illustrated in FIG. 8 A include FlowNet2, DCFlow, MirrorFlow, SpyNet, LiteFlowNet, PWC-Net, PWC-Net+, IRR-PWC, HD 3 , and VCN, each of which is detailed in a corresponding published paper.
- VCN Volumetric Correspondence Networks for Optical Flow
- the addition of “-ft” after the name of a model indicates a version of the model that has undergone additional “fine tuning” training on the Sintel and/or KITTI 2015 benchmark dataset. Unless otherwise indicated, the reported numbers in FIG.
- AEPE average endpoint error
- Each of the models reported in FIG. 8 A utilizes the Euclidean cost volume model or another predetermined (e.g., non-learnable or non-trainable) cost volume model.
- the cost volume model in each of these supervised optical flow models may be replaced by trainable/learnable cost volume model 400 to improve the performance of the supervised optical flow models.
- the bottommost row of FIG. 8 A corresponds to the VCN model paired with cost volume model 400 (i.e., VCN+LCV).
- VCN+LCV cost volume model paired with cost volume model 400
- the addition of cost volume model 400 to the VCN optical flow model improves the performance of the VCN optical flow model with respect to all but the Sintel clean test.
- the unsupervised models illustrated in FIG. 8 B include DSTFlow, GeoNet, UnFLow, DF-Net, OccAwareFlow, Back2FutureFlow, SelFlow, and DDFlow, each of which is detailed in a corresponding published paper.
- the AEPE and Fl-all metrics are the same as defined above.
- “Fl-bg” indicates the percentage of outliers (e.g., >5% end point error) averaged over background ground-truth pixels.
- “Fl-fg” indicates the percentage of outliers (e.g., >5% end point error) averaged over foreground ground-truth pixels.
- Each of the models reported in FIG. 8 B utilizes the Euclidean cost volume model or another predetermined (e.g., non-learnable or non-trainable) cost volume model.
- the cost volume model in each of these unsupervised optical flow models may be replaced by trainable/learnable cost volume model 400 to improve the performance of the unsupervised optical flow models.
- the bottommost row of FIG. 8 B corresponds to the DDFlow model paired with cost volume model 400 (i.e., DDFlow+LCV).
- DDFlow+LCV scores higher than all the other reported unsupervised optical flow models, including SelFlow, which is an improved version of DDFlow.
- the addition of cost volume model 400 to the DDFlow optical flow model improves the performance of the DDFlow optical flow model with respect to all metrics.
- cost volume model 400 may allow the optical flow models to operate more robustly under varied illumination conditions, in the presence of noise (e.g., natural or artificially-added noise), and in the presence of artificially-added adversarial patches.
- an untrained version of cost volume model 400 e.g., where the kernel matrix W is initialized as the identity matrix
- Cost volume model 400 and the pre-trained optical flow model may then undergo further training to fine-tune cost volume model 400 to cooperate with the optical flow model. Such further training may result in modifications to cost volume model 400 and/or the pre-trained optical flow model, resulting in a system that determines more accurate optical flow fields than the unmodified pre-trained optical flow model operating without cost volume model 400 .
- each step, block, and/or communication can represent a processing of information and/or a transmission of information in accordance with examples.
- Alternative examples are included within the scope of these examples.
- operations described as steps, blocks, transmissions, communications, requests, responses, and/or messages can be executed out of order from that shown or discussed, including substantially concurrently or in reverse order, depending on the functionality involved.
- blocks and/or operations can be used with any of the message flow diagrams, scenarios, and flow charts discussed herein, and these message flow diagrams, scenarios, and flow charts can be combined with one another, in part or in whole.
- a step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique.
- a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data).
- the program code may include one or more instructions executable by a processor for implementing specific logical operations or actions in the method or technique.
- the program code and/or related data may be stored on any type of computer readable medium such as a storage device including random access memory (RAM), a disk drive, a solid state drive, or another storage medium.
- the computer readable medium may also include non-transitory computer readable media such as computer readable media that store data for short periods of time like register memory, processor cache, and RAM.
- the computer readable media may also include non-transitory computer readable media that store program code and/or data for longer periods of time.
- the computer readable media may include secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, solid state drives, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- a computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- a step or block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
Abstract
Description
Claims (20)
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/041258 WO2022010476A1 (en) | 2020-07-08 | 2020-07-08 | Learnable cost volume for determining pixel correspondence |
Publications (2)
Publication Number | Publication Date |
---|---|
US20220189051A1 US20220189051A1 (en) | 2022-06-16 |
US11790550B2 true US11790550B2 (en) | 2023-10-17 |
Family
ID=71895214
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/292,647 Active 2041-05-09 US11790550B2 (en) | 2020-07-08 | 2020-07-08 | Learnable cost volume for determining pixel correspondence |
Country Status (4)
Country | Link |
---|---|
US (1) | US11790550B2 (en) |
EP (1) | EP3963546A1 (en) |
CN (1) | CN114158280A (en) |
WO (1) | WO2022010476A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN115661174A (en) * | 2022-11-14 | 2023-01-31 | 北京矩视智能科技有限公司 | Surface defect region segmentation method and device based on flow distortion and electronic equipment |
CN117409205B (en) * | 2023-12-13 | 2024-04-05 | 国网山东省电力公司济南供电公司 | Foreign matter hooking detection and segmentation method and system for power equipment |
-
2020
- 2020-07-08 US US17/292,647 patent/US11790550B2/en active Active
- 2020-07-08 WO PCT/US2020/041258 patent/WO2022010476A1/en unknown
- 2020-07-08 EP EP20750043.0A patent/EP3963546A1/en active Pending
- 2020-07-08 CN CN202080013041.8A patent/CN114158280A/en active Pending
Non-Patent Citations (47)
Title |
---|
{hacek over (Z)}bontar et al., "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches," Journal of Machine Learning Research (JMLR), 2016, pp. 1-32, vol. 17. |
Absil et al., "Optimization Algorithms on Matrix Manifolds," Princeton University Press, 2007, 241 pages. |
Bao et al., "Depth-aware Video Frame Interpolation," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1904.00830V1, Apr. 1, 2019, 11 pages. |
Butler et al., "A Naturalistic Open Source Movie for Optical Flow Evaluation," European Conference on Computer Vision (ECCV), Part VI, LNCS 7577, 2012, pp. 611-625. |
Cheng et al., "Segflow: Joint Learning for Video Object Segmentation and Optical Flow," IEEE International Conference on Computer Vision (ICCV), ARxIV:1709.06750V1, 2017, 10 pages. |
Dickscheid, Timo. Robust Wide-Baseline Stereo Matching for Sparsely Textured Scenes. Diss. Bonn, Univ., Diss., 2011, 2013. (Year: 2011). * |
Dosovitskiy et al., "FlowNet: Learning Optical Flow with Convolutional Networks," 2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2758-2766. |
Gatys et al., "Image Style Transfer Using Convolutional Neural Networks," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2414-2423. |
Gunn, Charles, "On the Homogeneous Model of Euclidean Geometry," Section 3.1 The Cayley-Klein Construction, Cornell University Library, aiXiv:1101.4542c3 [math.MG] Mar. 12, 2011, 55 pages. |
Hafner et al., "Why is the Census Transform Good for Robust Optic Flow Computation?" International Conference an Scale Space and Variational Methods in Computer Vision (SSVM), Lecture Notes in Computer Science, 2013, pp. 210-221, vol. 7893. |
Horn et al., "Determining Optical Flow," Artificial Intelligence, 1981, pp. 185-203, vol. 17, No. 1-3. |
Hui et al., LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1805.07046v1, May 18, 2018, 11 pages. |
Hur et al., Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1904.05290v1, 2019, 15 pages. |
Hur et al., MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion Estimation, IEEE International Conference on Computer Vision (ICCV), 2017, pp. 312-321. |
Ilg et al., FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1612.01925v1, 2017, 16 pages. |
International Searching Authority, International Search Report and Written Opinion dated Mar. 12, 2021, issued in connection with International Patent Application No. PCT/US2020/041258, filed on Jul. 8, 2020, 14 pages. |
Janai et al., "Unsupervised Learning of Multi-Frame Optical Flow with Occlusions," European Conference on Computer Vision (ECCV), 2018, 17 pages. |
Jiang et al., "Static-Map and Dynamic Object Reconstruction in Outdoor Scenes Using 3-D Motion Segmentation," IEEE Robotics and Automation Letters, Jan. 2016, pp. 324-331, vol. 1, No. 1. |
Kendall et al., "End-to-End Learning of Geometry and Context for Deep Stereo Regression," IEEE International Conference on Computer Vision (ICCV), arXiv:1703.04309v1, Mar. 13, 2017, 10 pages. |
Li et al., "Flow-Grounded Spatial-Temporal Video Prediction from Still Images," European Conference on Computer Vision (ECCV), arXiv:1807.09755V2, 2018, 18 pages. |
Li et al., "Universal Style Transfer Via Feature Transforms," Neural Information Processing Systems (NeurIPS), arXiv:1705.080862, Nov. 17, 2017, 11 pages. |
Lim, Jaeseung, and Sankeun Lee. "Patchmatch-based robust stereo matching under radiometric changes." IEEE transactions on pattern analysis and machine intelligence 41.5 (2018): 1203-1212. (Year: 2018). * |
Lin et al., "TSM: Temporal Shift Module for Efficient Video Understanding," IEEE International Conference on Computer Vision (ICCV), arXiv:1811.08383v3, 2019, 13 pages. |
Liu et al., "DDFlow: Learning Optical Flow with Unlabeled Data Distillation," Association for the Advancement of Artificial Intelligence (AAAI), arXiv:1902.09145, 2019, 8 pages. |
Liu et al., "SelFlow: Self-Supervised Learning of Optical Flow," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1904.09117v1, 2019, 14 pages. |
Liu, Hongmin, and Zhiheng Wang. "Robust irregular region matching based on inner product and exterior product." The 2010 IEEE International Conference on Information and Automation. IEEE, 2010. (Year: 2010). * |
Mayer et al., "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1512.02134v1, 2015, 14 pages. |
Menze et al., "Object Scene Flow," ISPRS Journal of Photogrammetry and Remote Sensing (JPRS), 2018, 47 pages. |
Ranjan et al., "Attacking Optical Flow," IEEE International Conference on Computer Vision (ICCV), arXiv:1910.10053.v1, 2019, 21 pages. |
Ranjan et al., "Optical Flow Estimation using a Spatial Pyramid Network," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1611.00850v2, 2017, 10 pages. |
Ren et al., "Unsupervised Deep Learning for Optical Flow Estimation," Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), 2017, pp. 1495-1501. |
Scharstein et al., "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms," International Journal on Computer Vision (IJCV), 2002, 35 pages, vol. 47, No. 1-3. |
Sun et al., "Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation," IEEE Transactions on Pattern Recognition and Machine Intelligence (PAMI), arXiv:1809.05571v1, 2019, 15 pages. |
Sun et al., "PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1709.02371v3, 2018, 18 pages. |
Szeliski, Richard, Computer Vision: Algorithms and Applications, Springer Science & Business Media, 2010, 979 pages. |
Truong, Prune, et al. "GOCor: Bringing globally optimized correspondence volumes into your neural network." Advances in Neural Information Processing Systems 33 (2020): 14278-14290. (Year: 2020). * |
Tsai et al., "Video Segmentation via Object Flow," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 3899-3908. |
Wang et al., "Occlusion Aware Unsupervised Learning of Optical Flow," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1711.05890v2, 2017, 10 pages. |
Weister et al., "Unflow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss," Association for the Advancement of Artificial Intelligence (AAAI), arXiv:1711.07837v1, 2017, 9 pages. |
Wenze et al., "Joint 3D Estimation of Vehicles and Scene Flow," ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2015, 8 pages, vol. II-3/W5. |
Xiao et al., "Learnable Cost Volume Using the Cayley Representation," Computer Vision and Pattern Recognition, arXiv:2007.11431v1, Jul. 21, 2020, 24 pages. |
Xu et al., "Accurate Optical Flow via Direct Cost Volume Processing," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1704.07325v1, 2017, 9 pages. |
Yang et al., "Volumetric Correspondence Networks for Optical Flow," 33rd Conference on Neural Information Processing Systems (NeurIPS), 2019, 12 pages. |
Yin et al., "GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1803.02276v2, 2018, 10 pages. |
Yin et al., "Hierarchical Discrete Distribution Decomposition for Match Density Estimation," IEEE Conference on Computer Vision and Pattern Recognition (CVPR), arXiv:1812.062643v3, 2019, 10 pages. |
Yu, Jason J. et al., "Back to Basics: Unsupervised Learning of Optical Flow via Brightness Constancy and Motion Smoothness," European Conference on Computer Vision (ECCV), arXiv:1608.05842v1, 2016, 4 pages. |
Zou et al., DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency, European Conference on Computer Vision (ECCV), arXiv:1809.01649v1, 2018, 18 pages. |
Also Published As
Publication number | Publication date |
---|---|
CN114158280A (en) | 2022-03-08 |
US20220189051A1 (en) | 2022-06-16 |
WO2022010476A1 (en) | 2022-01-13 |
EP3963546A1 (en) | 2022-03-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11100664B2 (en) | Depth-aware photo editing | |
US11978225B2 (en) | Depth determination for images captured with a moving camera and representing moving features | |
US10368062B2 (en) | Panoramic camera systems | |
US20220222776A1 (en) | Multi-Stage Multi-Reference Bootstrapping for Video Super-Resolution | |
US20150097827A1 (en) | Target Region Fill Utilizing Transformations | |
US20210383199A1 (en) | Object-Centric Learning with Slot Attention | |
CN109919971B (en) | Image processing method, image processing device, electronic equipment and computer readable storage medium | |
US11698529B2 (en) | Systems and methods for distributing a neural network across multiple computing devices | |
US9342873B1 (en) | Tile-based optical flow | |
US11790550B2 (en) | Learnable cost volume for determining pixel correspondence | |
US9860441B1 (en) | Tile-based digital image correspondence | |
JP6121302B2 (en) | Attitude parameter estimation device, attitude parameter estimation system, attitude parameter estimation method, and program | |
US20220191542A1 (en) | Object Pose Estimation and Tracking Using Machine Learning | |
US20220383628A1 (en) | Conditional Object-Centric Learning with Slot Attention for Video and Other Sequential Data | |
US20240153256A1 (en) | Locked-Model Multimodal Contrastive Tuning | |
US20230326044A1 (en) | Splatting-based Digital Image Synthesis | |
US20240031550A1 (en) | System and method of image rendering quality prediction and path planning for large-scale scenes, and computer device | |
JP7448693B2 (en) | Generate machine learning predictions using multiple domain datasets | |
EP4154211A1 (en) | Model for determining consistent depth of moving objects in video | |
WO2023233575A1 (en) | Estimation device, learning device, estimation method, learning method, and program | |
WO2023136822A1 (en) | Machine learning models for example-guided image inpainting | |
KR20230066752A (en) | Method and apparatus for collecting video information | |
US9420201B2 (en) | Image processing apparatus, image processing method, and program | |
CN117671147A (en) | Object reconstruction method, device, equipment and storage medium | |
WO2023191888A1 (en) | Correlation-based object anti-spoofing for dual-pixel cameras |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:XIAO, TAIHONG;SUN, DEQING;YANG, MING-HSUAN;AND OTHERS;REEL/FRAME:056261/0082Effective date: 20200708 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |