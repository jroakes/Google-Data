CN115917509A - Reduction server for fast distributed training - Google Patents
Reduction server for fast distributed training Download PDFInfo
- Publication number
- CN115917509A CN115917509A CN202180044539.5A CN202180044539A CN115917509A CN 115917509 A CN115917509 A CN 115917509A CN 202180044539 A CN202180044539 A CN 202180044539A CN 115917509 A CN115917509 A CN 115917509A
- Authority
- CN
- China
- Prior art keywords
- gradient
- worker
- virtual machines
- reducer
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000009467 reduction Effects 0.000 title claims description 137
- 238000012549 training Methods 0.000 title claims description 36
- 238000000034 method Methods 0.000 claims abstract description 245
- 230000008569 process Effects 0.000 claims abstract description 187
- 239000003638 chemical reducing agent Substances 0.000 claims abstract description 180
- 238000012545 processing Methods 0.000 claims abstract description 111
- 238000004891 communication Methods 0.000 claims description 44
- 238000013528 artificial neural network Methods 0.000 claims description 19
- 238000003672 processing method Methods 0.000 claims description 13
- 230000006870 function Effects 0.000 claims description 12
- 238000011946 reduction process Methods 0.000 claims description 12
- 238000005192 partition Methods 0.000 claims description 9
- 230000001360 synchronised effect Effects 0.000 claims description 9
- 238000012546 transfer Methods 0.000 claims description 9
- 238000005516 engineering process Methods 0.000 description 31
- 238000010586 diagram Methods 0.000 description 18
- 238000003860 storage Methods 0.000 description 18
- 238000010801 machine learning Methods 0.000 description 17
- 230000015654 memory Effects 0.000 description 16
- 239000003795 chemical substances by application Substances 0.000 description 7
- 238000013515 script Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 4
- 238000007726 management method Methods 0.000 description 4
- 238000006116 polymerization reaction Methods 0.000 description 4
- 238000013135 deep learning Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 230000002776 aggregation Effects 0.000 description 2
- 238000004220 aggregation Methods 0.000 description 2
- 230000002457 bidirectional effect Effects 0.000 description 2
- 238000004422 calculation algorithm Methods 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 229910003460 diamond Inorganic materials 0.000 description 2
- 239000010432 diamond Substances 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000000638 solvent extraction Methods 0.000 description 2
- 101000892301 Phomopsis amygdali Geranylgeranyl diphosphate synthase Proteins 0.000 description 1
- 230000002547 anomalous effect Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 238000007667 floating Methods 0.000 description 1
- 238000011478 gradient descent method Methods 0.000 description 1
- 238000000265 homogenisation Methods 0.000 description 1
- 230000008676 import Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000012804 iterative process Methods 0.000 description 1
- 230000000116 mitigating effect Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 238000005549 size reduction Methods 0.000 description 1
- 239000013598 vector Substances 0.000 description 1
- 238000013316 zoning Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5061—Partitioning or combining of resources
- G06F9/5066—Algorithms for mapping a plurality of inter-dependent sub-tasks onto a plurality of physical CPUs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/455—Emulation; Interpretation; Software simulation, e.g. virtualisation or emulation of application or operating system execution engines
- G06F9/45533—Hypervisors; Virtual machine monitors
- G06F9/45558—Hypervisor-specific management and integration aspects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/38—Concurrent instruction execution, e.g. pipeline, look ahead
- G06F9/3877—Concurrent instruction execution, e.g. pipeline, look ahead using a slave processor, e.g. coprocessor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
- G06F9/541—Interprogram communication via adapters, e.g. between incompatible applications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/455—Emulation; Interpretation; Software simulation, e.g. virtualisation or emulation of application or operating system execution engines
- G06F9/45533—Hypervisors; Virtual machine monitors
- G06F9/45558—Hypervisor-specific management and integration aspects
- G06F2009/45562—Creating, deleting, cloning virtual machine instances
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/455—Emulation; Interpretation; Software simulation, e.g. virtualisation or emulation of application or operating system execution engines
- G06F9/45533—Hypervisors; Virtual machine monitors
- G06F9/45558—Hypervisor-specific management and integration aspects
- G06F2009/4557—Distribution of virtual machine instances; Migration and load balancing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/098—Distributed learning, e.g. federated learning
Abstract
A data processing system, the data processing system comprising: one or more host processing devices that can be configured to support instantiation of a plurality of virtual machines such that a first set of virtual machines runs one or more worker processes, each worker process operating on a respective set of data to produce a respective gradient. The host processing device can be configured to support instantiation of a second set of virtual machines running one or more reducer processes that operate on each respective gradient produced by each worker process to produce an aggregate gradient. One or more reducer processes may cause an aggregate gradient to be broadcast to each worker process.
Description
Cross Reference to Related Applications
This application is a continuation of U.S. application No.17/076,393, filed on 21/10/2020, the disclosure of which is incorporated herein by reference.
Background
In general, machine learning relies on statistical techniques to build a model based on sample data. Machine learning provides a computer system with the ability to learn based on sample data in order to improve performance associated with a given task (e.g., make accurate predictions or decisions without being explicitly programmed to do so). Machine learning can be characterized based on the following three parameters: experience ("E"), task ("T"), and performance metrics ("P"). The criteria for machine learning using those three parameters are defined as follows: a computer program or system is considered to learn from experience E with respect to task T if the performance of the program or system as measured by P improves with experience E. Experience E includes sample data, which is more commonly referred to as training data. Deep learning is a form of machine learning that relies on neural network concepts when processing data, such as training data.
The internet has become a source of vast data from a variety of sources around the world, such as internet search engines, e-commerce platforms, and the like. This large amount of data is often referred to as big data. It provides a source of data for training various applications (e.g., fraud detection, speech recognition, object recognition, decision making, etc.). Processing large amounts of data available as big data typically requires the use of cloud computing resources, where the data is split or decomposed across multiple machines for distributed processing.
Disclosure of Invention
Distributing data processing associated with training data for machine learning across multiple machines provides the possibility for fast distributed training, but involves consideration and management of parameters such as costs, latency, efficiency, and bottlenecks associated with processing both data processing and communication among the machines. One aspect of the disclosed technology is to provide a system, architectural system or technique that provides a collective communication primitive for synchronization training that can improve throughput as much as two (2) times the throughput of an allreduce (full reduction).
One aspect of the disclosed technology may be in one form a data processing system. The system can include one or more host processing devices that support instantiation of a plurality of virtual machines. The system may further comprise: a first set of virtual machines comprising one or more of a plurality of virtual machines, the first set of virtual machines running one or more worker processes, each worker process operating on a respective set of data to produce a respective gradient; and a second set of virtual machines comprising one or more of the plurality of virtual machines, the second set of virtual machines running one or more reducer (reducer) processes that operate on at least a portion of each respective gradient produced by each worker process to produce an aggregate gradient. The one or more reducer processes cause an aggregate gradient to be broadcast to each worker process.
Other aspects of the disclosed system techniques may include, for example, each respective data set including equal-sized data blocks. The equal-sized data blocks may include data blocks generated by splitting an input data stream. Equal-sized data blocks may have a size proportional to the number of reducer processes running on one or more virtual machines.
Other aspects of the disclosed technology of the system may include, for example, the one or more host processing devices including at least one Graphics Processing Unit (GPU) for hosting the first set of virtual machines. The one or more host processing devices may, for example, also include at least one Central Processing Unit (CPU) for hosting the second set of virtual machines.
Other aspects of the disclosed system's techniques may include, for example, a first total bandwidth associated with usage of the first set of virtual machines being equal to a second total bandwidth associated with usage of the second set of virtual machines.
Other aspects of the disclosed system's techniques may include, for example, the one or more reducer processes accepting the gradients within a time limit defined by receipt of a first one of the gradients by a first reducer process of the one or more reducer processes.
Other aspects of the disclosed system's techniques may include, for example, one or more worker processes instantiated as communication primitives of an Application Programming Interface (API) library or separate from the API library. When one or more worker processes are instantiated as communication primitives of the API library, the instantiated worker processes push and pull gradients from the one or more reducer processes. One or more worker processes may also or alternatively be instantiated separate from the API library, such that connections between reducer process functions manage connections and data transfers between the one or more worker processors and the one or more reducer processes.
In still other aspects of the disclosed technology, each of the one or more worker processes may, for example, partition the gradient produced by each worker process into respective gradient chunks, and the given worker process pulls a first gradient chunk from a first reduction process while it pushes a second gradient chunk to the first reduction process. The worker process may adjust the weight coefficients of the neural network based on small batches of random gradient descent, again, for example, in the context of training the neural network.
Other aspects of the disclosed system's techniques may include, for example, computing, by one or more reducer processes, a reduction only on a subset of gradient batches received from a subset of one or more worker processes that ignore the remainder of the gradient batches, the subset having a predetermined size m or the size of the subset being determined by the gradient batches received up to a predetermined deadline T after receipt of the first gradient batch.
One aspect of the disclosed technology may be, in one form, a data processing system. The system may include: one or more host processing devices supporting instantiation of a plurality of virtual machines; a first set of virtual machines comprising one or more of a plurality of virtual machines, the first set of virtual machines running one or more worker processes, each worker process operating on a respective block of data to each produce a respective gradient; a reduction server comprising one or more reducers, each reducer comprising a virtual machine running a reducer process that operates on at least a portion of each respective gradient to generate an aggregate gradient; wherein the reduction server broadcasts the aggregate gradient to one or more virtual machines running one or more worker processes.
Other aspects of the disclosed system's techniques may include, for example, computing, by the reduction server, a reduction only on a subset of gradient batches received from a subset of workers ignoring the remainder of the gradient batches, the subset having a predetermined size m or the size of the subset being determined by the gradient batches received up to a predetermined deadline T after receipt of the first gradient batch. Each respective data block may comprise, for example, equal-sized data blocks. Equal-sized data blocks may have a size proportional to the number of reducer processes running on one or more virtual machines.
Other aspects of the disclosed technology of the system may include, for example, that the one or more host processing devices may further include at least one Graphics Processing Unit (GPU) for hosting the first set of virtual machines. The one or more host processing devices include at least one Central Processing Unit (CPU) for hosting a reducer of the reduction server. In additional aspects, a first total bandwidth associated with usage of the first set of virtual machines may be equal to a second total bandwidth associated with usage of the reduction server.
Other aspects of the disclosed system's techniques may include, for example, each reducer accepting the gradients within a time period defined by receipt of a first one of the gradients by a first reducer of the one or more reducers.
Other aspects of the disclosed system's techniques may include, for example, one or more worker processes instantiated as communication primitives of an Application Programming Interface (API) library or separate from the API library. One or more worker processes may alternatively be instantiated as communication primitives of an API library, the instantiated worker processes pushing and pulling gradients from one or more reducer processes. When the one or more worker processes are instantiated separate from the API library, the connection between the reducer process functions manages the connection and data transfer between the one or more worker processors and the one or more reducer processes.
Other aspects of the disclosed system's techniques may include, for example, one or more worker processes partitioning the gradient produced by each worker process into respective gradient chunks, and a given worker process pulling a first gradient chunk from a first reducer while it pushes a second gradient chunk to the reducer. The worker process may also or alternatively adjust the weight coefficients of the neural network based on small batches of random gradient descent in the context of training the neural network.
One aspect of the disclosed technology can be in one form a method for synchronized distributed training, comprising: instantiating a first set of virtual machines running one or more worker processes, each worker process operating on a respective block of data to each produce a respective gradient; instantiating a second set of virtual machines comprising a reduction server, the second set of virtual machines running a reducer process that operates on at least a portion of each respective gradient to generate an aggregate gradient; and broadcasting, by the reduction server, the aggregate gradient to the first set of virtual machines.
Other aspects of the techniques of the disclosed methods may include, for example, processing, by the first set of virtual machines, the aggregate gradient to produce an output data file. Additional aspects of the disclosed method may include calculating, by the reduction server, a reduction on only a subset of gradient batches received from a subset of workers ignoring the remainder of the gradient batches, the subset having a predetermined size m or the size of the subset being determined by the gradient batches received up to a predetermined deadline T after receipt of the first gradient batch.
Other aspects of the disclosed technique may include, for example, computing the reduction only on a subset of gradient batches received from a subset of workers ignoring the remainder of the gradient batches, the subset having a predetermined size m or the size of the subset being determined by the gradient batches received up to a predetermined deadline T after receipt of the first gradient batch.
Other aspects of the disclosed techniques may include, for example, instantiating a first set of virtual machines running one or more worker processes includes instantiating the first set of virtual machines on at least one Graphics Processing Unit (GPU).
Other aspects of the techniques of the disclosed methods can include, for example, instantiating a second set of virtual machines including a reduction server includes instantiating one or more Central Processing Units (CPUs) for hosting one or more reducers of the reduction server.
Other aspects of the techniques of the disclosed methods may include, for example, a first total bandwidth associated with usage of the first set of virtual machines being equal to a second total bandwidth associated with usage of the reduction server.
Other aspects of the disclosed techniques may include, for example, each of the one or more reducers accepting the gradients over a time period defined by receipt of a first one of the gradients by a first reducer of the one or more reducers.
One aspect of the disclosed technology may be in the form of a non-transitory computer readable medium having stored thereon instructions for operating one or more processing devices, the instructions causing the one or more processing devices to implement a process or method. The process comprises the following steps: instantiating a first set of virtual machines running one or more worker processes, each worker process operating on a respective data block to each produce a respective gradient; instantiating a second set of virtual machines comprising a reduction server, the second set of virtual machines running a reducer process that operates on at least a portion of each respective gradient to generate an aggregate gradient; and broadcasting, by the reduction server, the aggregate gradient to the first set of virtual machines. Other aspects of the disclosed technology of non-transitory computer readable media may include, for example, storing other instructions that cause one or more processing devices to perform other method steps or processes of the present disclosure.
Drawings
Fig. 1 is a block diagram of an example cloud system in accordance with aspects of the disclosed technology.
FIG. 2 is a block diagram of an example distributed data processing system in accordance with aspects of the disclosed technology.
FIG. 3 is a block diagram illustrating an example of a data processing system and data flow in a data processing system in accordance with aspects of the disclosed technology.
FIG. 4 is a block diagram of an example of a data processing system in accordance with aspects of the disclosed technology.
FIG. 5 is a block diagram of a data processing system model in accordance with aspects of the disclosed technology.
FIG. 6 is a block diagram of a data processing system model in accordance with aspects of the disclosed technology.
Fig. 7 is a block diagram of an example of a computer processing system in accordance with aspects of the disclosed technology.
FIG. 8 is a block diagram of a method in accordance with aspects of the disclosed technology.
FIG. 9 is a block diagram of a method in accordance with aspects of the disclosed technology.
FIG. 10 is a block diagram of a method in accordance with aspects of the disclosed technology.
FIG. 11 is an example data processing flow in accordance with aspects of the disclosed technology.
FIG. 12 illustrates a comparison of allreduce and a reduction server in accordance with aspects of the disclosed technology.
Detailed Description
SUMMARY
Aspects of the disclosed technology include techniques and systems for managing the processing of data, particularly large amounts of data or large data, as part of a synchronous training process, e.g., for machine learning-type applications. The techniques may be implemented as a reduction service or reduction server in a distributed computing environment. The distributed environment includes data processing systems that include one or more host processing devices that support instantiation of multiple Virtual Machines (VMs). For example, the techniques may be implemented as a service in which a virtual machine having a relatively small virtual central processing unit (vCPU) (i.e., small footprint in terms of required resources) provides reduction operations on gradients received from a worker running a worker process, and returns one or more aggregated gradients to the worker. In this way, the reduction process or operation is provided as a service by the vCPU acting as a reducer node in the data processing network. In the system, the collection of vcpus assigned to perform the reduction operation can be considered a reduction server. Alternatively, the worker and the reduction process may be implemented to run on physical hardware.
Processing data as part of synchronous training typically involves using collective communication primitives such as allreduce and implementations thereof, which typically include communication operations used in distributed learning systems, as part of the invida (NVIDIA) collective communications library (NCCL). The communication primitives include high-level constructs or abstractions that programs (e.g., accessible via an application programming interface ("API")) use to use an underlying communication network that interconnects processing resources (e.g., vcpus, vGPU (virtual graphics processing unit), storage, etc.). The use of allreduce has become widespread enough where it may be considered an industry standard for gradient aggregation in synchronous distributed GPU training. However, the allreduce operation is becoming a bottleneck in distributed training, especially if the operation is associated with a large model. For example, the time to perform the operation associated with the allreduce request is proportional to the number of nodes n, related to bandwidth B, as follows: t = (S/B) × (2 × (n-1)/n), where S is the number of elements to be processed. The ratio of how much network traffic is induced for each bit of input data by allreduce can be inferred from the above equation as a minimum of 2 (n-1)/n. One aspect of the disclosed technique includes a more bandwidth efficient technique that reduces the ratio 2 (n-1)/n. For example, reduction services are added that assist the worker process in computing the reductions necessary in training the neural network according to the gradient descent method. While the provision of a reduction service requires additional resources and may require hardware, the techniques and architecture presented herein are advantageous because, in one aspect, it reduces the communication complexity of conventional approaches to computing reductions among the workers themselves. The reduction in communication complexity results in higher throughput, e.g., more efficient use of bandwidth in computing reductions.
From the perspective of the NCCL environment, the reduction service or reducer role may not be part of the NCCL. Specifically, in one aspect of the technique, when a worker or worker process runs a collective operation such as "all-reduce" with reducer role or reduction server enabled, the worker pushes and pulls gradients from reducers outside the NCCL. Thus, the reducer role or reduction server includes collective communication primitives for synchronization training that may be implemented as part of the NCCL-enabled API but run outside of the NCCL. Alternatively, the reduction server concept can be implemented as a communication primitive within the NCCL.
In an example method, data in one or more input files is partitioned into data blocks, which may not be required to have equal-sized but equal-sized data blocks, and provided to a worker agent or process ("worker"). The worker operates to generate intermediate data blocks or gradients. The term "gradient" relates to machine learning, in particular a training phase of a neural network, wherein weights of the neural network are adjusted. The gradient or gradient value is the rate of change of the measured error between the neural network output and the desired output relative to the value of the adjusted weight. The gradient and optionally the learning rate over-parameter value specify the degree of adjustment to a particular weight of the network and are represented as numerical values, e.g., as integer values, floating point values, or fixed point values depending on the implementation. Although the techniques presented herein as reduction servers generally relate to the computation of reductions, the techniques are particularly useful and applicable in the context of reduction operations computed during a training phase of a neural network. Thus, the value aggregated in such a reduction is referred to herein as a "gradient". For example, the worker may perform a back-propagation (or other comparable technique for supervised learning using an artificial network of gradient descent) operation to compute gradients for the model parameters or weights based on the input data for the current iteration. Thus, the gradient, which can be considered as an intermediate data block, is mapped from the model parameters. The worker then pushes the intermediate data block or gradient to the reducer. In particular, assuming n workers and m reducers, each worker pushes its ith data block to the ith reducer, so that each reducer receives a total of n data blocks (or more generally n/m of input data). This ensures that the computation and communication for effectuating the reduction is evenly distributed across the reducers, avoiding bottlenecks, and supports high resource utilization by balancing the workload across the reducers. Each reducer then reduces the n data blocks it receives into a single data block or aggregate gradient and broadcasts the aggregate gradient or single data block to all n workers. For example, the reduction operation is a numerical aggregation operation and may include calculating the sum or average of the received gradients, i.e., numerical values. After computing the reduction, the reducer passes the results of the reduction operation back to the worker that provided the gradient. The reduction server thus pushes and pulls gradients from the worker. Typically, the workers apply the received gradients to update their parameters. After all parameters are updated, the next iteration of training begins. The worker and reducer may thus exchange gradients as part of a gradient descent process using a push/pull operation in an iterative process to arrive at a solution within an error bound (e.g., the error between the model and its parameters) or a given minimum associated with the cost function.
From an architectural perspective, the system is implemented in a distributed computing system, such as, for example, in a cloud computing environment. The worker is implemented as a worker process running on a virtual GPU that receives data blocks partitioned from an input data file or stream. The worker then operates on the data blocks as described above to produce intermediate data blocks that are provided to the reducer. In the context of gradient descent learning, the intermediate data blocks produced by the worker may comprise a set of gradients produced by training a small batch in a random gradient descent. The reducer is implemented as a reducer process running on the vCPU. Each reducer performs the reduction operation and returns the aggregate gradient or single data block as described above to the worker for further processing. With respect to the operation of the system, system performance is enhanced, wherein the total bandwidth of the reducers is equal to the total bandwidth of the workers, wherein the total bandwidth refers to the sum of the bandwidth available to each individual reducer or worker, respectively. The system also includes one or more master processes that assign tasks to the workers and reducer.
As an advantage, the reduction server or provision of a reduction service provides an improvement in the bandwidth requirements of an allreduce operation in a cloud environment by providing more bandwidth efficient operation from a communication perspective. For example, by using a reduction server, each worker would typically only need to send and receive one byte of gradient for each byte of gradient being processed. In contrast, each worker in a conventional allreduce process that does not explicitly utilize a reducer or a reduction service sends and receives each gradient 2 x (n-1)/n times. The use of a reducer or reduction service thus significantly reduces the number of messages exchanged (sent and received) by the individual workers to obtain the results of the reduction operation. In addition, the computation of the reduce operation is offloaded from the worker to a reducer or a reduce service.
Further, the reduction server process reduces latency and achieves reduced cost by using vcpus for at least some iterations of the reduction operations. In this regard, the reduction server does not require specialized hardware and structural support, which contributes to cost reduction. Furthermore, in terms of bandwidth, the reduction server can match the network bandwidth processing capabilities of the GPU at a fraction of the cost of such GPUs.
Example System and API model
Fig. 1 is an example system 100 in accordance with aspects of the present disclosure. The system 100 includes: one or more computing devices 110, which may include computing device 110 1 To 110 k Storage 138, network 140, and one or more cloud computing systems 150, which may include cloud computing system 150 1 To 150 l . Computing devices 110 may include computing devices located at customer locations that facilitateServices are computed using a cloud such as infrastructure as a service (IaaS), platform as a service (PaaS), and/or software as a service (SaaS). For example, if the computing device 110 is located at a business enterprise, the computing device 110 may use the cloud system 150 as a service to provide software applications (e.g., accounting, word processing, inventory tracking, etc. applications) to the computing device 110 used in operating the enterprise system. Additionally, computing device 110 may access cloud computing system 150 as part of its operation employing machine learning, deep learning, or more generally artificial intelligence techniques to train applications that support its business enterprise. For example, the computing device 110 may include a client computer or server in a bank or credit card issuer that accumulates data relating to the use of credit cards by its cardholder and supplies that data to a cloud platform provider, which then processes the data to detect usage patterns that may be used to update a fraud detection model or system, which may then be used to notify the cardholder of suspicious or anomalous activity in the cardholder's credit card account. Other customers may include social media platform providers, government agencies, or any other enterprise that uses machine learning as part of its operations. A machine or deep learning process (e.g., gradient descent) provided via the system 150 may provide model parameters that the customer uses to update the machine learning model used in operating their business.
As shown in fig. 1, each computing device 110 may include one or more processors 112, memory 116 storing data (D) and instructions (I), a display 120, a communication interface 124, and an input system 128, shown interconnected via a network 130. The computing device 110 may also be coupled or connected to storage 136, which storage 136 may include local or remote storage, for example on a Storage Area Network (SAN), that stores data accumulated as part of the customer's operations. The computing device 110 may include a stand-alone computer (e.g., a desktop or laptop) or a server associated with a client. A given client may also implement multiple computing devices as servers as part of its business. If a stand-alone computer, the network 130 may include a data bus or the like internal to the computer; if a server, the network 130 may include one or more of the following: a local area network, a virtual private network, a wide area network, or other type of network described below with respect to network 140. The memory 116 stores information accessible to the one or more processors 112, including instructions 132 and data 134 that may be executed or otherwise used by the processors 112. The memory 116 may be of any type capable of storing information accessible by a processor, including a computing device readable medium, or other medium that stores data that may be read by an electronic device, such as a hard disk, memory card, ROM, RAM, DVD, or other optical disk, as well as other writable and read-only memories. The systems and methods may include different combinations of the above, whereby different portions of the instructions and data are stored on different types of media.
The instructions 132 may be any set of instructions (such as machine code) to be executed directly by the processor or any set of instructions (such as script) to be executed indirectly by the processor. For example, the instructions may be stored as computing device code on a computing device readable medium. In that regard, the terms "instructions" and "programs" may be used interchangeably herein. The instructions may be stored in an object code format for direct processing by a processor, or in any other computing device language including scripts, or a collection of independent source code modules that are interpreted or compiled in advance as needed. The procedures, functions, methods and routines of the instructions are described in more detail below.
The data 132 may be retrieved, stored, or modified by the processor 112 in accordance with the instructions 132. By way of example, the data 132 associated with the memory 116 can include data used in supporting services for one or more client devices, applications, and the like. Such data may include data to support hosting web-based applications, file sharing services, communication services, games, sharing video or audio files, or any other network-based service.
The one or more processors 112 may be any conventional processor, such as a commercially available CPU. Alternatively, one or more of the processors may be a dedicated device, such as an ASIC or other hardware-based processor. Although fig. 1 functionally illustrates the processors, memory, and other elements of the computing device 110 as being within the same block, one of ordinary skill in the art will appreciate that a processor, computing device, or memory may actually comprise multiple processors, computing devices, or memories that may or may not be located or stored within the same physical housing. In one example, the one or more computing devices 110 may include one or more server computing devices having multiple computing devices, e.g., a load balancing server farm, that exchange information with different nodes of the network for the purpose of receiving, processing, and transmitting data to and from other computing devices as part of a business operation of a customer.
Computing device 110 may also include a display 120 (e.g., a monitor having a screen, a touch screen, a projector, a television, or other device operable to display information), the display 120 providing a user interface that allows control of the computing device 110. Such control may include, for example, using the computing device to cause data to be uploaded through the input system 128 to the cloud system 150 for processing, to cause data to be accumulated on the memory 136, or more generally, to manage different aspects of the customer's computing system. While the input system 128 (e.g., a USB port) may be used to upload data, the computing system may also include a mouse, keyboard, touch screen, or microphone that can be used to receive commands and/or data.
As shown in FIG. 1, computing system 150 may be illustrated as including infrastructure 152, storage 154, and computer system 158. Infrastructure 152, storage 154, and computer system 158 may comprise data centers within cloud computing system 150. Infrastructure 152 may include servers, switches, physical links (e.g., fiber optics), and other equipment used to interconnect servers within a data center with storage 154 and computer systems 158. The storage 154 may comprise a disk or other storage device that is partitionable to provide physical or virtual storage to virtual machines running on processing devices within the data center. The storage 154 may be provided as a SAN within a data center hosting the virtual machines supported by the storage 154 or disposed in a different data center that does not share physical locations with the virtual machines it supports. The computer system 158 acts as a manager or management agent for jobs being processed by a given data center. Generally, the computer system 158 will contain instructions necessary to, for example, manage operations that are part of the request for a synchronous training operation on customer data. The computer system 158 may receive jobs, for example, as a result of receiving input from a customer via an Application Programming Interface (API).
Fig. 2 is a block diagram of an example of a cloud-based distributed data processing system 200 in accordance with aspects of the disclosed technology. Fig. 2 illustrates an example of a cloud computing system 150 in terms of interconnecting data centers DC1 through DC 4. The data centers may be distributed among the cloud computing systems 150, and more than one data center may comprise a given cloud. Those skilled in the art will appreciate that the system layout is only one example configuration and that the system may be illustrated in other layouts or forms.
The system 200 receives, stores, processes, and outputs data. The system 200 may include optical links or other communication channels and other hardware such as modems, routers, switches, bridges, wireless antennas, and towers to facilitate relatively high-speed transport of data between processing devices (e.g., CPUs or GPUs) within a data center and between data centers. Within data centers, network communications may occur over a Local Area Network (LAN), while between data centers, network communications involve a Wide Area Network (WAN).
Data centers may be located near each other or, conversely, remote from each other. As shown, DC1 205 includes a number of equipment racks 209. Each rack is designed to hold or support one or more servers, switches, storage devices, etc., typically on a tray. In fig. 2, the server is not shown for simplicity. Instead, data processors 215 (e.g., CPUs and GPUs) that may make up the servers are shown supported within the racks. The data processor can include a processor associated with a network attached storage device and other computer controlled devices. At least one processor may be operated as a master processor (e.g., as system 158 in fig. 1) that controls the scheduling of computing tasks and data distribution tasks performed by the networked system 200. As shown, the processors within each rack may be interconnected through a rack switch 226. The racks within a data center can be interconnected via a data center switch.
FIG. 3 is a block diagram illustrating a logical view of an example of data flow in a data processing system and a large scale data processing system in accordance with aspects of the disclosed technology. The system and process may be implemented in the system shown in fig. 1 or fig. 2. For example, they may be implemented in a single data center, or alternatively, may be implemented as part of a cloud or distributed computing environment or spread over different data centers.
As shown in fig. 3, the jobs or operations required by the customer may be provided in the form of scripts or binaries 310. An application programmer may develop operators such as worker (), reduce (), partition (), and the like. These operators are computer programs that process input data, e.g. received via an input data file, into intermediate data or gradients. Reduction operations typically combine or otherwise combine intermediate data values, for example, combining gradients produced by workers. The partitioning operation generally specifies how the intermediate data is to be partitioned over the set of intermediate data files. These operations (e.g., reduction) invoke library functions that automatically handle reduction operations, I/O scheduling, delivery, compute parallelization, and the like. The job request or script will typically be created by an application programmer via an API. The API may then create a binary payload file 310 that is received at the computer system 158. The computer system 158 then processes the binary-loaded file to create a set of processing tasks that are controlled or managed by one or more master processes 314. The master process 314 may, for example, determine how many processes or tasks, such as reduction processes or tasks, to use and which processors should be used to perform those tasks, and where to store gradients or other forms of intermediate data and final output files (including, for example, one or more files with updated models or updated model parameters).
As discussed in further detail below, the concept of a reduction server may not be part of the NCCL. In that case, the reducer role may be a distinct binary file from the binary loaded file 310 as indicated in the file 310 by the separation between the worker and the reduction operation. Whether the reduction server is implemented within a NCCL or without a NCCL, it provides different architectural coverage in a cloud environment. The collection of reducers, reduction agents, or reduction processes, which may include reduction servers, not only provide advantages with respect to efficient bandwidth management and latency mitigation, but also allow for more dynamic allocation of computing resources, whether at the host level or the virtual machine level. In particular, the reduction server improves distributed computing architectures, particularly those for machine learning, including deep machine learning or neural networks.
Master 314 may assign tasks to one or more processors within a data center where input files are stored. In accordance with one aspect of the disclosed technique, master 314 may also determine the appropriate processors for performing a given task, e.g., a vGPU for tasks performed by worker processes or agents and a vCPU for tasks performed by reducer agents or processes. The master process 314 is coupled to one or more virtual machines 322 and 326. The virtual machines 322 may include a first set of virtual machines that each run a worker process or act as worker agents W1, W2, W3. The first set of virtual machines may include vGPU or vppu (virtual tensor processing unit), each of which may run one or more worker processes. Virtual machine 326 may include another or second set of virtual machines that each run a reduction process or act as reducers or reducer agents R1, R2, R3. Virtual machine 326 may include vcpus, where each vCPU may run one or more reduction processes.
As shown in FIG. 3, block 332 includes input data for processing. The input data may include weight values or parameters associated with the neural network model. The input data is divided into data blocks denoted as I1, I2, I3, \8230in. Although not necessary, the data blocks may be partitioned into equal-sized data blocks, e.g., n equal-sized data blocks. For example, each data block may comprise a 64MB data block. Each worker W1, W2, etc. receives a data block I1, I2, etc. for processing. Each worker then pushes its ith chunk to the ith reducer in chunk or reducer server 326. As illustrated, the worker and reducer communicate over a network 337, which network 337 may be a LAN within a data center or a LAN/WAN connection to another data center, as previously described with respect to fig. 1 and 2. As the ith block pushed by each worker is processed, it may be considered intermediate data or dataA value or a gradient. As shown, each of W1, W2, W3,. Ann Wn pushes its first data block (i.e., i = 1) to reducer R1 such that R1 receives data value G 11 、G 21 、G 31 、...G n1 Wherein G is 11 Representing a first gradient or more generally an intermediate data value, G, from a first operator 21 Representing a first gradient, G, from the second working unit 31 Representing the first gradient from the third operator and so on. Reducer R2 receives data reception value G 12 、G 22 、G 32 、...G n3 (ii) a R3 receive data value G 13 、G 23 、G 33 、...G n3 (ii) a And so on, so that Rm receives the data value G 1n 、G 2n 、G 3n 、...G nm 。
As part of the reduction operation, each reducer reduces the n gradients or data blocks it receives into a single or aggregate gradient or single data block. In particular, reducer R1 may generate a single data block or aggregate/single gradientG1R2 generating a single data block or gradientG2R3 generating a single data block or gradientG3Rm generating a single block or gradient of dataGmAnd so on. Note that the polymerization gradients G1, G2,. Gm are shown in fig. 3 as a bar above each polymerization gradient, however in this detailed description they are bolded and underlined, but the written description and drawings discuss and show the same polymerization gradient. Each reducer then broadcasts its corresponding gradient to all workers so that each worker receives each gradient. As shown in FIG. 3, each worker receives data includingG1、G2、...GmGradient of polymerization of (a). At each iteration of data processing, the reducer aggregates the gradients or data received from the worker and the worker pulls the aggregated gradients from the reducer. Upon receipt of the gradient vectors, each worker may perform further processing operations to generate output data values or files represented as O1, O2,. On, which may then be returned to the customer to, for example, update the machine learning model.
FIG. 4 is a block diagram illustrating a worker in accordance with aspects of the disclosed technologyA block diagram of an example of a data processing system 400 of a reduction server 420 in communication with a collection 440. In this example, reduction server 420 includes four workers 440 communicatively coupled to it 1 -440 4 Four reducers 420 1 -420 4 . As shown, the reduction server introduces an additional type of role to the big data distributed processing system: a reducer. The reducer may comprise an inexpensive CPU VM with a relatively small amount of vcpus. As shown in fig. 4, the reducer role allows computation and communication to be evenly distributed across the reducer, which may result in improved performance, including bottleneck avoidance. As also indicated via the example shown in fig. 4, the overall architecture provided via the reduction server may be considered similar to the architecture for the parameter server. As one skilled in the art may recognize, the parameter server is a key-value store used to train the machine learning model on the cluster. The values are parameters of a machine learning model (e.g., a neural network). The keys index the model parameters. However, unlike the parameter server where the server maintains the model parameters, the reducer is stateless and is responsible for or only responsible for receiving gradients or intermediate data blocks from the worker, performing the reduction operation, and sending gradients back to the worker. The simplicity of the reducer enables performance improvements and allows general application or homogenization across different training applications. The reduction server can be partitioned into shards, e.g., each reducer included in the reduction server processes a particular shard of data provided by the worker. In the first step of the all-reduce operation, the worker provides to each reduction server a respective slice of gradient data to be included in the reduction computed by the reducer. Thus, each reducer receives from each worker a batch of gradients to be reduced, or a subset thereof, as will be explained later in the case of a random gradient descent. In a second step, the reducer uses a reduction operation to aggregate the inputs received from the workers. In a third step, each reducer makes the results of the reduction of the slice it is handling available to all workers.
The provision of a reduction server also allows the worker to slice its gradient among all reducers. Thus, in principle, the reducer is not expected to require a relatively high bandwidth as long as the total aggregated bandwidth used by the reducer matches the total aggregated bandwidth operated. If the bandwidth of each reducer is less than the bandwidth of each worker, the number of reducers will be greater than the number of reducers. By way of example, to achieve peak network performance for 2 worker VMs with 100Gbps, either 2 reducer VMs with 100Gbps or possibly 8 reducer VMs with 25Gbps are used. This allows additional flexibility in provisioning the reducer with VM shapes in a dynamic cloud environment. At least one benefit associated with the disclosed techniques is provided after the total aggregated bandwidth of the reducer is made equal to the total aggregated bandwidth of the workers.
Synchronous training with a relatively large number of reducers may potentially be susceptible to laggard effects. More specifically, a reduction server with n workers and m reducers will have O (m x n) network links. As the number of links increases, the probability of falling behind (e.g., a worker becoming slower than the other workers) increases. However, the effect of tail-delay is not transferred because the reduction of each term tensor is independent of each other. The reduction server should ensure that the reduction is load balanced adaptively across the reducers. In addition, additional methods may be used to mitigate the latter. These additional methods may affect the deterministic nature of the training (although end-to-end performance may improve), so they should be considered optional and used only if the tail-delay impact is really problematic (i.e., as a last resort); these additional methods are particularly suitable for training based on small batch stochastic gradient descent.
1. And a backup working device. Reduction takes the first m of the n gradients generated. This makes use of the fact that: in some cases, it may be appropriate to ignore a particular batch in SGD (random gradient descent). In other words, the reduction is computed by the reduction server only on a subset of gradient batches received from a subset of workers ignoring the remaining portion of the gradient batches, the subset of workers having a predetermined size m that is at least smaller than the total number of workers. The minimum size of the subset may be determined empirically, such as by a user, for example. A trade-off between training accuracy and speed may need to be considered: the smaller the magnitude, the more noise will be present in the gradient, but the faster the iteration speed is typically.
2. There is a bounded delay. The reduction may take the gradient received within a time period T after the first gradient is received. This method can also be combined with the first method such that at least m gradients have to be received before skipping the remaining gradients. In other words, the reduction is computed by the reduction server only on a subset of the gradient batches received from the subset of workers, the size of the subset or workers being determined by the gradient batches received up to the predetermined deadline T after the first gradient batch is received.
The reduction server may also reduce latency. Compared to the O (n) latency of a conventional ring allreduce and the O (log n) latency of a conventional tree allreduce, where n is the number of worker nodes, the reduction server architecture disclosed herein only takes the O (1) latency, since each worker performs the allreduce operation in a single exchange with the help of reducers, i.e., by passing gradient data to each reducer and then obtaining the results from each reducer. Because the reduction between n workers and k reducers in each iteration is simultaneous, rather than sequential, the latency is independent of the number of reducers:
ring allreduce | Tree allreduce | Reduction server | |
Algorithmic bandwidth | Busbw*n/2(n-1) | Busbw*n/2(n-1) | Busbw |
Time delay | O(n) | O(logn) | O(1) |
In the above, busbw refers to bus bandwidth, which refers to hardware bandwidth, as compared to algbw, which refers to algorithm bandwidth. The performance of an allreduce can be derived based on its algorithmic bandwidth and bus bandwidth as follows:
algbw = input _ size/time
busbw = transfer _ size/time
algbw = = busbw/(transfer _ size/input _ size)
The ratio (transfer _ size/import _ size) represents how much network traffic is incurred for each byte of input data through the all-reduce. As previously discussed, for the allreduce, the minimum ratio is 2 (n-1)/n.
FIG. 5 is a block diagram of a data processing system model 500 in accordance with aspects of the disclosed technology. The system model 500 involves implementing the worker role as a conveyance in the NCCL, however the reducer role is not part of the NCCL. In accordance with this aspect of the present technique, when a worker performs a collective operation such as, for example, an allreduce with a reduction server mode enabled, the conveyance will then push and then pull gradients from the external worker. In this regard, the reducer role will include different binary files that run separately outside the NCCL environment as depicted in fig. 5. This implementation may be transparent to the user code, except that the user (or platform) is likely required to propose a reducer in addition to the worker. This implementation is also compatible with frameworks such as TensorFlow, pyTorch, and MXnet.
FIG. 6 is a block diagram of a data processing system model 600 in accordance with aspects of the disclosed technology. In this implementation, the worker role may be implemented as tf.distribution.strategy and the reducer role may be implemented as part of the NCCL as a redactionserversstrategy. The ReductionServerStrategy handles the connection and transfer between the worker and the reducer. This implementation allows the reduction server to control the communication topology, memory management, data copying, etc.
FIG. 7 is a computer system 700 that may be used in conjunction with data processing systems 200 and 300 shown in FIGS. 1, 2, and 3. Computer system 700 may include computer system 158 of fig. 1 and the systems discussed with respect to fig. 2 and 3 for managing workflows or jobs and scheduling, instantiating, and managing virtual machines as discussed above. The computer system 700 typically includes one or more processing units (CPU, GPU, TPU) 702, one or more network or other communication interfaces 710, memory 712, and one or more communication buses 714 for interconnecting these components. The system 700 may optionally include a user interface 704, such as a display 706 and a keyboard 708. The memory 712 may include high-speed random access memory and may also include non-volatile memory, such as one or more magnetic disk storage devices. The memory 712 may include a mass storage device located remotely from the central processing unit 702.
One or more state tables 744 are also included to track tasks and processes, as described with respect to FIG. 3. In some embodiments, computer system 700 includes worker process 738, intermediate file 740, one or more master processes 742, and a reducer process. The interaction of the worker process 438, reducer process 750, and master process 442 is described above.
As previously discussed with respect to fig. 3, an application programmer can create a script or program using application software 722, which application software 722 includes one or more operators 724, 726, and 728. The script or program is processed into a binary file 310 and provided to the work queue master 314.
Example method
Fig. 8 is a block diagram of a method 800 in accordance with aspects of the disclosed technology. At block 820, the method instantiates a first set of virtual machines running one or more worker processes. Each worker process associated with a virtual machine operates on the data blocks it receives to produce a corresponding gradient as discussed above with respect to FIG. 3.
At block 840, the method instantiates a second set of virtual machines that collectively form, or operate as, a reduction server. The reduction server operates on the gradients produced by the first set of virtual machines to produce an aggregate gradient or a single data block. The reduction server may, for example, include a number of reducers or reducer processes, each of which runs on a respective one of the second set of virtual machines and produces a gradient as discussed above with respect to fig. 3.
At block 860, the reduction server broadcasts a single data block or gradient to the first set of virtual machines. The first set of virtual machines may thereafter further process the single gradient or data block to produce an output file.
FIG. 9 illustrates a method or process flow 900 in accordance with aspects of the disclosed technology. Method 900 may include processing sub-steps performed in block 820 of fig. 8. As shown in FIG. 9, the process begins at block 910 with an initial model weight or set of parameters associated with a machine learning model. At decision diamond 920, a determination is made as to whether the model weights satisfy a termination condition. The termination condition can include, for example, an amount of error between the model and the relevant data (e.g., parameters). If the termination condition is not met, processing proceeds to block 930.
At block 930, a worker reads the next small batch of data. At block 940, the worker performs an SGD operation on the read small lot to generate one or more gradients. At block 940, the worker pushes the shard gradient to the reducer for reduction or reduction processing. In block 960, the worker pulls the aggregate gradient from the reducer, and in block 970, the worker updates the model weights or parameters using the aggregate gradient. The push and pull processes may proceed as previously discussed.
The updated model weights or parameters are returned to decision 920 to determine whether the termination condition is satisfied. If the termination condition is not satisfied, the worker performs another iteration of the process depicted as blocks 930-970. If the termination condition is satisfied, processing proceeds to block 980, where the model is saved. The saved model may then be provided to the customer as an output file. The process depicted in fig. 9 may be performed by a worker running on a VM using a vGPU or vppu.
FIG. 10 depicts a method or process flow 1000 in accordance with aspects of the disclosed technology. As mentioned above and shown in fig. 9, the worker pushes and pulls the gradient from the reducer. Fig. 10 depicts the processing performed by the reducer. At block 1010, processing begins with initializing a network connection with a worker by a reducer. At block 1020, the reducer receives gradients from all workers participating in the job. At block 1030, the reducer performs a reduction operation on each gradient slice it receives from each worker. At block 1040, the reducer broadcasts its results to all workers. At decision diamond 1050, the reducer determines whether the process is terminated. If the process is not terminated, the reducer returns to block 1020 for another iteration of the process. Otherwise, processing ends at block 1060. The process depicted in fig. 10 may be performed on a VM using a vCPU.
FIG. 11 is a block diagram of an example process flow 1100 in accordance with aspects of the disclosed technology. FIG. 11 shows an example of two workers associated with two reducers for processing iterations. As shown, the first gradient of each worker (shown as sub-blocks 1101A and 1102A) is pushed to reducer 1. Similarly, gradient sub-blocks 1101B and 1102B are pushed to reducer 2. Reducer 1 then aggregates the gradients it receives into gradients 1110, as depicted via reduction processing block 1120 for reducer 1 receiving gradients 1101A and 1102A. Reducer 2 will perform a similar process on gradients 1101B and 1102B. As shown, reducers 1 and 2 thereafter broadcast their respective gradients to workers 1 and 2, as depicted in the portion of fig. 11 below block 1120.
Example use case
As discussed above, the reduction server can reuse the same all-reduce API, so it can be used as a facile replacement for existing all-reduce primitives. During each "all-reduce" operation, the worker pushes and pulls the gradient from the reducer. Thus, the reduction server will be compatible with most frameworks that use collective operations.
Push and pull operations are not duplex, but the reduction server is able to fully utilize bidirectional network bandwidth by doing tensor zoning and pipelining. The gradient tensor can be partitioned into small chunks before the push and pull operations are performed. After pushing the first chunk to the reducer, the worker pulls the first chunk in real-time, e.g., as part of the same iteration or during the same process loop. At the same time, the worker starts pushing the second tile. Thus, the bidirectional bandwidth is fully utilized except for the first and last partitions.
By way of example, assuming that the reduction server is running outside of the NCCL (such as CAIP training) supported by the AI platform, this may be implemented as an option to allow the user to enable the reduction server when submitting a job. For the reducer support side, if the reduction server is enabled, CAIP training adds additional CPU VMs to the job as reducers. These reducer VMs will run an internal container image containing the reducer binaries.
As shown in fig. 12, approximately 2 times the algorithm bandwidth is achieved for a large message size reduction server. Algorithmic bandwidth thus refers to the amount of data to calculate a reduction divided by the time it takes to calculate a reduction.
In the context of data parallel training of neural networks, where in each iteration each worker takes a small batch of input data, computes the gradient of each weight (or parameter) relative to the small batch, averages across workers for each gradient, and then updates the model weights using the averaged gradients. allreduce is used to solve the "average across workers per gradient" step, and the reduction server is a trivial alternative to allreduce.
Non-limiting aspects of the disclosed technology can include the following features:
one or more host processing devices that support instantiation of a plurality of virtual machines;
a first set of virtual machines comprising one or more of the plurality of virtual machines, the first set of virtual machines running one or more worker processes, each worker process operating on a respective set of data to produce a respective gradient;
a second set of virtual machines comprising one or more of the plurality of virtual machines, the second set of virtual machines running one or more reducer processes that operate on at least a portion of each respective gradient produced by each worker process to produce an aggregate gradient;
wherein the one or more reducer processes cause the aggregate gradient to be broadcast to each worker process.
one or more host processing devices that support instantiation of a plurality of virtual machines;
a first set of virtual machines comprising one or more of the plurality of virtual machines, the first set of virtual machines running one or more worker processes, each worker process operating on a respective block of data to each produce a respective gradient;
a reduction server comprising one or more reducers, each reducer comprising a virtual machine running a reducer process that operates on at least a portion of each respective gradient to generate an aggregate gradient;
wherein the reduction server broadcasts the aggregate gradient to the one or more virtual machines running the one or more worker processes.
instantiating a first set of virtual machines running one or more worker processes, each worker process operating on a respective block of data to each produce a respective gradient;
instantiating a second set of virtual machines comprising a reduction server, the second set of virtual machines running a reducer process that operates on at least a portion of each respective gradient to generate an aggregate gradient; and
broadcasting, by the reduction server, the aggregate gradient to the first set of virtual machines.
one or more host processing devices that support instantiation of a plurality of virtual machines;
a first set of virtual machines comprising one or more of the plurality of virtual machines, the first set of virtual machines running one or more worker processes, each worker process operating on a respective set of data to produce a respective numerical value;
a second set of virtual machines comprising one or more of the plurality of virtual machines, the second set of virtual machines running one or more reducer processes that operate on at least a portion of each respective value produced by each worker process to produce an aggregated value;
wherein the one or more reducer processes cause the aggregate value to be broadcast to each worker process.
one or more host processing devices that support instantiation of a plurality of virtual machines;
a first set of virtual machines comprising one or more of the plurality of virtual machines, the first set of virtual machines running one or more worker processes, each worker process operating on a respective block of data to each produce a respective numerical value;
a reduction server comprising one or more reducers, each reducer comprising a virtual machine running a reducer process that operates on at least a portion of each respective value to generate an aggregated value;
wherein the reduction server broadcasts the aggregated value to the one or more virtual machines running the one or more worker processes.
Unless otherwise stated, the above-described alternative examples are not mutually exclusive, but can be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. Additionally, the provision of examples described herein, as well as terms such as "such as," "including," and the like, should not be construed to limit claimed subject matter to particular examples; rather, the examples are intended to illustrate only one of many possible embodiments. Moreover, the same reference numbers in different drawings can identify the same or similar elements.
Claims (20)
1. A data processing system, comprising:
one or more host processing devices supporting instantiation of a plurality of virtual machines;
a first set of virtual machines comprising one or more of the plurality of virtual machines, the first set of virtual machines running one or more worker processes, each worker process operating on a respective set of data to produce a respective gradient; and
a second set of virtual machines comprising one or more of the plurality of virtual machines, the second set of virtual machines running one or more reducer processes that operate on at least a portion of each respective gradient produced by each worker process to produce an aggregate gradient;
wherein the one or more reducer processes cause the aggregate gradient to be broadcast to each worker process.
2. The system of claim 1, wherein each of the respective data sets comprises equal-sized data blocks.
3. The system of claim 2, wherein the equal-sized data blocks comprise data blocks generated by splitting an input data stream.
4. The system of claim 2, wherein the equal-sized data blocks have a size proportional to a number of reducer processes running on the one or more virtual machines.
5. The system of claim 1, wherein the one or more host processing devices comprise at least one Graphics Processing Unit (GPU) to host the first set of virtual machines.
6. The system of claim 1, wherein the one or more host processing devices comprise at least one Central Processing Unit (CPU) to host the second set of virtual machines.
7. The system of claim 1, wherein a first total bandwidth associated with usage of the first set of virtual machines is equal to a second total bandwidth associated with usage of the second set of virtual machines.
8. The system of claim 1, wherein the one or more reducer processes accept the gradients within a time deadline defined by receipt of a first one of the gradients by a first reducer process of the one or more reducer processes.
9. The system of claim 1, wherein the one or more worker processes are instantiated as communication primitives of an Application Programming Interface (API) library or separate from the API library.
10. The system of claim 9, wherein when the one or more worker processes are instantiated as communication primitives of an API library, the instantiated worker processes push and pull gradients from the one or more reducer processes.
11. The system of claim 9 wherein the connection between reducer process functions manages the connection and data transfer between the one or more worker processors and the one or more reducer processes when the one or more worker processes are instantiated separate from the API library.
12. The system of claim 1, wherein each worker process of the one or more worker processes partitions the gradient produced by each worker process into a respective gradient chunk, and a given worker process pulls a first gradient chunk from a first reduction process while it pushes a second gradient chunk to the first reduction process.
13. The system of claim 1, wherein the worker process adjusts weight coefficients of a neural network according to small batch random gradient descent in the context of training the neural network.
14. The system of claim 1, wherein the reduction is calculated by the one or more reducer processes only on a subset of gradient batches received from a subset of the one or more worker processes that ignore a remainder of the gradient batches, the subset having a predetermined size m or the size of the subset being determined by the gradient batches received up to a predetermined deadline T after receipt of the first gradient batch.
15. A data processing method for synchronous distributed training, comprising:
instantiating a first set of virtual machines running one or more worker processes, each worker process operating on a respective data block to each produce a respective gradient;
instantiating a second set of virtual machines comprising a reduction server, the second set of virtual machines running a reducer process that operates on at least a portion of each respective gradient to generate an aggregate gradient; and
broadcasting, by the reduction server, the aggregate gradient to the first set of virtual machines.
16. The data processing method of claim 15, comprising calculating the reduction only on a subset of gradient batches received from a subset of workers ignoring the remainder of the gradient batches, the subset having a predetermined size m or the size of the subset being determined by the gradient batches received up to a predetermined time limit T after receiving the first gradient batch.
17. The data processing method of claim 15, wherein instantiating the first set of virtual machines running one or more worker processes comprises instantiating the first set of virtual machines on at least one Graphics Processing Unit (GPU).
18. The data processing method of claim 15, wherein instantiating the second set of virtual machines that includes the reduction server comprises instantiating one or more Central Processing Unit (CPU) s for hosting one or more reducers of the reduction server.
19. The data processing method of claim 18, comprising accepting, by each of the one or more reducers, a first one of the gradients within a time deadline defined by receipt of the gradient by a first reducer of the one or more reducers.
20. The data processing method of claim 15, wherein a first total bandwidth associated with usage of the first set of virtual machines is equal to a second total bandwidth associated with usage of the reduction server.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/076,393 US11966766B2 (en) | 2020-10-21 | 2020-10-21 | Reduction server for fast distributed training |
US17/076,393 | 2020-10-21 | ||
PCT/US2021/054973 WO2022086780A1 (en) | 2020-10-21 | 2021-10-14 | Reduction server for fast distributed training |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115917509A true CN115917509A (en) | 2023-04-04 |
Family
ID=78536619
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180044539.5A Pending CN115917509A (en) | 2020-10-21 | 2021-10-14 | Reduction server for fast distributed training |
Country Status (5)
Country | Link |
---|---|
US (1) | US11966766B2 (en) |
EP (1) | EP4232898A1 (en) |
KR (1) | KR20230024418A (en) |
CN (1) | CN115917509A (en) |
WO (1) | WO2022086780A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117725019A (en) * | 2024-02-07 | 2024-03-19 | 北京壁仞科技开发有限公司 | Method and computing system for GPU set communication |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10698766B2 (en) | 2018-04-18 | 2020-06-30 | EMC IP Holding Company LLC | Optimization of checkpoint operations for deep learning computing |
US11315013B2 (en) * | 2018-04-23 | 2022-04-26 | EMC IP Holding Company LLC | Implementing parameter server in networking infrastructure for high-performance computing |
-
2020
- 2020-10-21 US US17/076,393 patent/US11966766B2/en active Active
-
2021
- 2021-10-14 WO PCT/US2021/054973 patent/WO2022086780A1/en unknown
- 2021-10-14 CN CN202180044539.5A patent/CN115917509A/en active Pending
- 2021-10-14 EP EP21805749.5A patent/EP4232898A1/en active Pending
- 2021-10-14 KR KR1020237002240A patent/KR20230024418A/en unknown
Also Published As
Publication number | Publication date |
---|---|
US11966766B2 (en) | 2024-04-23 |
KR20230024418A (en) | 2023-02-20 |
US20220121465A1 (en) | 2022-04-21 |
EP4232898A1 (en) | 2023-08-30 |
WO2022086780A1 (en) | 2022-04-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10728091B2 (en) | Topology-aware provisioning of hardware accelerator resources in a distributed environment | |
US10698766B2 (en) | Optimization of checkpoint operations for deep learning computing | |
Saxena et al. | A proactive autoscaling and energy-efficient VM allocation framework using online multi-resource neural network for cloud data center | |
US7647590B2 (en) | Parallel computing system using coordinator and master nodes for load balancing and distributing work | |
CN102307133B (en) | Virtual machine scheduling method for public cloud platform | |
US9785472B2 (en) | Computing cluster performance simulation using a genetic algorithm solution | |
US20120016816A1 (en) | Distributed computing system for parallel machine learning | |
US20170091668A1 (en) | System and method for network bandwidth aware distributed learning | |
Naik et al. | Multiobjective virtual machine selection for task scheduling in cloud computing | |
CN111666158A (en) | Kubernetes-based container scheduling method and device, storage medium and electronic equipment | |
US20210390405A1 (en) | Microservice-based training systems in heterogeneous graphic processor unit (gpu) cluster and operating method thereof | |
CN115917509A (en) | Reduction server for fast distributed training | |
US20140101669A1 (en) | Apparatus and method for processing task | |
Hung et al. | A dynamic scheduling method for collaborated cloud with thick clients. | |
Nylander et al. | Modeling of request cloning in cloud server systems using processor sharing | |
Zhang et al. | Learning driven parallelization for large-scale video workload in hybrid CPU-GPU cluster | |
Makarov et al. | Supercomputer simulation of social processes: New technologies | |
da Rosa Righi et al. | Designing Cloud-Friendly HPC Applications | |
Ramu | Soft Computing Techniques to Analyze the Load Balancing in Cloud Environment | |
Kamboj et al. | A novel approach of optimizing performance using K-means clustering in cloud computing | |
Fiaidhi et al. | Empowering extreme automation via zero-touch operations and GPU parallelization | |
CN115037620B (en) | Resource allocation method and equipment for edge intelligent gateway | |
US10630957B2 (en) | Scalable distributed computation framework for data-intensive computer vision workloads | |
Sunder et al. | Load balancing optimization based on enhanced genetic algorithm in cloud computing | |
Salamy | Network requirements for distributed machine learning training in the cloud |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |