CN112204575A - Multi-modal image classifier using text and visual embedding - Google Patents
Multi-modal image classifier using text and visual embedding Download PDFInfo
- Publication number
- CN112204575A CN112204575A CN201980036217.9A CN201980036217A CN112204575A CN 112204575 A CN112204575 A CN 112204575A CN 201980036217 A CN201980036217 A CN 201980036217A CN 112204575 A CN112204575 A CN 112204575A
- Authority
- CN
- China
- Prior art keywords
- image
- embedding
- text
- model
- processing
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000000007 visual effect Effects 0.000 title claims description 15
- 238000012545 processing Methods 0.000 claims abstract description 43
- 238000000034 method Methods 0.000 claims abstract description 38
- 238000003860 storage Methods 0.000 claims abstract description 12
- 239000013598 vector Substances 0.000 claims description 30
- 238000012549 training Methods 0.000 claims description 20
- 230000008569 process Effects 0.000 claims description 16
- 238000004891 communication Methods 0.000 claims description 6
- 238000009826 distribution Methods 0.000 claims description 5
- 238000004590 computer program Methods 0.000 abstract description 16
- 238000013528 artificial neural network Methods 0.000 description 18
- 230000006870 function Effects 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 4
- 230000006872 improvement Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000004044 response Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 238000013145 classification model Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 241001465754 Metazoa Species 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 238000012512 characterization method Methods 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- PCHJSUWPFVWCPO-UHFFFAOYSA-N gold Chemical compound [Au] PCHJSUWPFVWCPO-UHFFFAOYSA-N 0.000 description 1
- 239000010931 gold Substances 0.000 description 1
- 229910052737 gold Inorganic materials 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
- G06F18/24147—Distances to closest patterns, e.g. nearest neighbour classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/774—Generating sets of training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for implementing a multi-modal image classifier. In one aspect, a method includes, for each image of a plurality of images: processing an image by a text generator model to obtain a set of phrases describing content of the image, wherein each phrase is one or more terms, processing the set of phrases by a text embedding model to obtain an embedding of predictive text for the image, and processing the image using an image embedding model to obtain an embedding of image pixels of the image. Then, a multi-modal image classifier is trained on the embedding of the predictive text of the image and the embedding of the image pixels of the image to produce as output a label of an output taxonomy for classifying the image based on the image as input.
Description
PRIORITY CLAIM
This application claims priority from U.S. patent application serial No. 62/768,701, filed on 2018, 11, 16, incorporated herein by reference in its entirety.
Background
This description relates to image processing, and more particularly to a multimodal image classifier (multimodality).
The machine learning image classification model takes image pixels as input and generates labels in a predefined classification method. Image classification is typically a supervised learning problem that defines a set of target classes (objects to be identified in an image) and trains a model using labeled training images to recognize them. Such a system can have high prediction accuracy and universality on unseen data. These models may rely on pixel data and features derived from the pixel data, such as color histograms, textures, and shapes, as training features.
Disclosure of Invention
This specification describes a system, implemented as a computer program on one or more computers in one or more locations, that predicts image classification using a model that captures features from embedding of predicted text of an image and embedding of image pixels of the image. The specification also describes a method for predictive image classification performed by one or more data processing apparatuses, and one or more non-transitory computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations for predictive image classification.
According to one aspect, there is provided a method comprising, for each image of a plurality of images: processing the image by a text generator model to obtain a set of phrases describing content of the image, wherein each phrase is one or more terms, processing the set of phrases by a text embedding model to obtain an embedding of predictive text of the image, and processing the image using an image embedding model to obtain an embedding of image pixels of the image; a multi-modal image classifier is trained on the embedding of the predictive text of the image and the embedding of the image pixels of the image to produce as output a label of an output taxonomy to classify the image based on the image as input. Additional aspects may be provided in the apparatus, systems, and computer program products.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. While textual information (e.g., text surrounding an image) can provide valuable orthogonal signals, machine learning models typically rely on pixel data as input. Further, for image sets, text data may not be available. Novel features of the present disclosure include text embedding of images generated from phrases generated by a text generator without requiring surrounding text as input. The multi-modal image classifier is then trained on the text embedding and pixel embedding of the images. Training using text embedding results in overall performance that is superior to that of a trained model without text embedding, resulting in significant improvements in the image classification domain.
Further, the systems and methods described herein may provide language agnostic and cross-language text embedding. This may be achieved, for example, due to training on the query-image pair.
Additionally or alternatively, the systems and methods described herein may allow for the characterization of both individual local characteristics and overall visual structure. This may be accomplished, for example, by training network weights on the input dataset, identifying the bottleneck layer, and extracting the output of that layer for any image.
The present disclosure may also provide a system framework (and associated methods) that only requires input images to generate predictions based in part on textual features. For example, the systems and methods described herein allow singular feature vectors (singular feature vectors) representing two concepts by stitching (concatenate) N-dimensional textual feature vectors with M-dimensional visual feature vectors of an image.
The systems and methods described herein may also allow for the generation of a probability distribution across each possible predicted phrase. For example, a soft maximum (softmax) layer may be used to generate a probability distribution across a large number of queries.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a diagram of a multi-modal image classification system.
FIG. 2 is a flow diagram of an example process for training a multi-modal image classifier.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This patent document describes a frame comprising: a text generator that generates text labels, called phrases, from the image data; and a multi-modal image classifier that obtains text features from the text generator and visual features from image pixels and generates labels according to an output classification method. In some implementations, the text generator is trained using web-based query/image pairs to incorporate contextual information associated with each image. In some implementations, the output of the text generator can exceed the output taxonomy of the image classifier, e.g., the text generator can generate phrases that are not included in the output taxonomy of the multimodal image classifier.
These and other features are described in more detail below.
Fig. 1 is a diagram of a multi-modal image classification system 100. The multimodal image classification system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented.
The multimodal image classification system 100 is configured to process images to generate text embedding (embedding) and image pixel embedding for training the multimodal image classification model 110. The system 100 includes a text generator 120 and a text-embedding neural network 122. The text generator 120 generates, for each of the images 102, a phrase describing one or more terms of the content of the image. The text embedding neural network 122 processes phrases generated by the text generator for the image and generates text embedding.
In some implementations, the text generator 100 is a neural network trained on web-based query-image pairs. Query-image pairs are images that are paired with a respective query for which the user selects an image.
In some implementations, long-tailed queries are included in the query-image pairs. This results in fine grained and descriptive queries, such as [2016 lamborghinghini avatar white ], [ equality state of bartolomeo Colleoni ], and the like. Query-image pair space also encompasses a number of visual concepts including landmarks, animals, artifacts, characters, artwork, and the like. This results in a technical improvement in accuracy relative to systems trained on image datasets with much smaller vocabulary size and/or less descriptive phrases (e.g., [ gold retriever ] such as "dog.
In some implementations, the text generator 120 is a convolutional neural network with classification loss. An example architecture is based on ResNet-101, followed by an embedding layer that projects ResNet responses to low-dimensional vectors, with a multi-way softmax on top. In one example implementation, the vector has M dimensions, and is followed by K million ways softmax. In some implementations, M is 64 and K is 4 million. However, other values may be used.
In this particular example, a feature of the text generator network 120 is its ability to predict a large number of queries (e.g., 4 million) from the input images. To accommodate the larger output space, the Resnet output is reduced (reduce) to a 64-dimensional bottleneck layer. Network 120 can therefore be conceptualized as comprising three phases: the main training mechanism for learned weights and biases represents the 64-dimensional feature layer of image embedding and the softmax layer that produces a probability distribution across 4 million queries.
In this example architecture, at inference, a 64-dimensional embedding is generated for each input image, and the top N most likely queries and their associated similarity scores are extracted from the query embedding index. In some implementations, N is 3, but other values of N may be used. Since the query represents contextual information, the process identifies relevant text for arbitrary images without web dependencies. Furthermore, embedding additionally creates a way to measure image-to-image similarity in the query space.
For large scale training, AI accelerator specific circuits such as Tensor Processing units (Tensor Processing units) and sampled softmax loss algorithms are used. Other suitable training pipelines (pipelines) may also be used, however, the particular pipeline used may depend on the scale of the training corpus.
In some implementations, after training, the query q computed by the softmax functioncThe predicted probability of (a) is:
where x is image embedding (e.g., from the bottleneck layer), wcQuery q from K million queries that is from a fully connected layer after the bottleneck layercCorresponding weights, and d (-) is a distance function, such as a cosine distance function. The use of this function is optional, but may also be usedOther ways of predicting the probability for a query are used.
To further increase the vocabulary of predicted phrases from the phrase generation model, in some implementations, a prototype network approach (prototypical network approach) may be employed to construct prototypes for significantly larger numbers of queries. Increasing the vocabulary through this process is optional. Prototype network by embedding functions with a learnable parameter phi
wherein S iscIs the set of images associated with query c. Prototypes can also be obtained using other methods. Can pass through phicWeight w instead of equation (1)cTo increase the output size from K to a larger output space of the phrase generation model. In some implementations, K is 4 million. Other ways of increasing the output space may also be used.
In some implementations, the prediction of the most relevant queries (phrases) for a given image can be done more efficiently by using a nearest neighbor process. Since many queries in output space may be independent of a given image, there is no need to compute their softmax probabilities to retrieve the top query. Thus, in some implementations, the system uses the top k (top-k) nearest neighbor search technique for phrase generation:
The text-embedding neural network 122 is an artificial neural network that is trained to map discrete inputs (e.g., feature vectors of phrases generated by the text generator 120) to outputs (e.g., vectors or matrices) of continuous values. The output of the text-embedded neural network 122 has the following properties: similar inputs are mapped to outputs that are close to each other in the multi-dimensional space. Thus, the output of the text-embedded neural network 122 can be described as a potential representation of the data input to the embedded neural network. In this specification, a potential representation of a collection of data refers to a digital representation of the data generated internally by the network (e.g., as a vector or matrix).
In an example implementation, the text embedding network 122 is a twin network (siernese network) pre-trained on a data set of query-query associations. A twin network is an artificial neural network that uses the same weights when working together on two different input vectors to compute comparable output vectors. To improve the embedding quality, an attention mechanism may optionally be used. The resulting embedding has the property that related queries are placed close together in the output embedding space.
The embedding model is then used to embed the top N text results obtained from the text generator 120. In some implementations, text embedding is obtained via a bag-of-words model (bag-of-words model) of unary (unigram) and bigram. The embedding (or text prediction) of the top N queries is then averaged to obtain an M-dimensional text feature vector. In some implementations, N is 3 and M is 200. However, other values may be used.
At this stage of processing, the system generates an embedding from a network trained on text signals (query-query associations), which represents the image for which the system was initially given only pixels as input. Thus, the independent text embedding of the generated text query bridges the gap between the visual and text modes.
The pixel embedding neural network 124 generates embeddings having similar properties to those of the text embedding neural network 122, i.e., the outputs of the pixel embedding neural network 124 have similar properties in which inputs are mapped to outputs that are close to each other in a multidimensional space. In some implementations, the embedding of the image features is obtained from a bottleneck layer of a pre-trained convolutional network. This involves training network weights on the input dataset, identifying the bottleneck layer (usually the layer before the output) and extracting the output of that layer for any image. In this way, features characterizing the local characteristics and the overall visual structure of the individual are obtained.
The multimodal image classifier 110 receives text embedding from a text embedding neural network 122 and pixel embedding from a pixel embedding neural network 124. On these embeddings, classifiers are trained to produce as output labels of an output taxonomy to classify the image based on the image as input. Pixel and text embeddings for arbitrary input images have been obtained and then these embeddings are combined to generate a representation of the image incorporating both types of features. In some implementations, the process for combining these features involves stitching a 200-dimensional text feature vector with a 1024-dimensional visual feature vector into a singular feature vector that represents both concepts. The resulting vector is then input into a two-stage fully-connected neural network, where the output softmax layer predicts the probability associated with a class according to a specified taxonomy. However, other suitable training procedures may be used.
Advantageously, the system 100 only requires the image to generate label-level (label-wise) results, although orthogonal text features are extracted to supplement the pixel information and generate a context-aware prediction. Since generating text with the text generator 120 enables identification of relevant text for any input image, the system framework only needs the input image to produce a prediction based in part on the text features. This is yet another technical improvement in the field of image classification.
FIG. 2 is a flow diagram of an example process 200 for training a multi-modal image classifier. Process 200 may be implemented in one or more computers at one or more locations.
For each image in the set of images (202), the process 200 obtains a text embedding for the image (204), and obtains a pixel embedding for the image (206). For example, as described above, text embedding for each image may be obtained by processing the image by the text generator 120 and the output phrase (text or query) by the text embedding neural network 122. Pixel embedding for each image may be obtained by processing the image by a pixel embedding neural network 124.
The process 200 then trains a multi-modal image classifier using text embedding and pixel embedding (208). As described above, classifiers are trained on these embeddings to produce as output the labels of the output taxonomy to classify the image based on the image as input. Once both visual and text embedding are generated, the multimodal image classifier 110 fuses the two signals to produce a final image classification. As described above, in some implementations, the input is a concatenation of features from 1024-dimensional visual embedding and 200-dimensional text embedding. The resulting vector is input to a multi-stage fully connected neural network, where the output softmax layer predicts the probability associated with a class according to the target classification.
The term "configured" is used herein in connection with system and computer program components. For a system of one or more computers configured to perform particular operations or actions, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that, when operated, causes the system to perform the operations or actions. For one or more computer programs configured to perform particular operations or actions, it is meant that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of digital electronic circuitry, tangibly embodied computer software or firmware, and computer hardware. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software application, app, module, software module, script, or code, may be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages; and a computer program can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or multiple computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC.
A computer suitable for executing a computer program may be based on a general purpose or special purpose microprocessor or both, or on any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game player, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device; a user may provide input to the computer through a keyboard and a pointing device, such as a mouse or trackball. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user's device in response to a request received from the web browser. Moreover, the computer may interact with the user by sending text messages or other forms of messages to a personal device (e.g., a smartphone running a messaging application) and in turn receiving response messages from the user.
The data processing apparatus for implementing the machine learning model may further comprise, for example, a dedicated hardware accelerator unit for processing general-purpose and compute-intensive parts, i.e. reasoning, workload, of the machine learning training or production.
The machine learning model may be implemented and deployed using a machine learning framework (e.g., a TensorFlow framework, a Microsoft cognitive toolkit framework, an Apache Singa framework, or an Apache MXNet framework).
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data, such as an HTML page, to the user device, for example, for the purpose of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., a result of the user interaction, may be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings and described in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (15)
1. A method performed by one or more data processing apparatus, the method comprising:
for each image of the plurality of images:
processing the image through a text generator model to obtain a set of phrases describing content of the image, wherein each phrase is one or more terms;
processing the set of phrases by a text embedding model to obtain embedding of a predicted text of the image; and
processing the image using an image embedding model to obtain an embedding of image pixels of the image;
a multi-modal image classifier is trained on the embedding of the predictive text of the image and the embedding of the image pixels of the image to produce as output a label of an output taxonomy to classify the image based on the image as input.
2. The method of claim 1, wherein the text generator model is a text query-based model trained on text query-image pairs.
3. The method of claim 1 or claim 2, wherein processing the image using the image embedding model to obtain embedding of image pixels of the image comprises obtaining image features from a final fully-connected layer of a pre-trained convolutional network.
4. The method according to any one of claims 1-3, wherein training a multi-modal image classifier comprises:
splicing the N-dimensional text feature vector and the M-dimensional visual feature vector into a singular feature vector; and
the singular feature vectors are provided as input to a multi-modal classifier.
5. The method of any of claims 1-4, wherein the text generator model includes a softmax layer that produces a probability distribution across each possible predicted phrase.
6. The method of claim 1, wherein the text generator model obtains a set of phrases for a given image using a nearest neighbor process.
7. A system, comprising:
a data processing device;
a memory in data communication with the data processing apparatus and storing instructions that cause the data processing apparatus to perform operations comprising:
for each image of the plurality of images:
processing the image through a text generator model to obtain a set of phrases describing content of the image, wherein each phrase is one or more terms;
processing the set of phrases by a text embedding model to obtain embedding of a predicted text of the image; and
processing the image using an image embedding model to obtain an embedding of image pixels of the image;
a multi-modal image classifier is trained on the embedding of the predictive text of the image and the embedding of the image pixels of the image to produce as output a label of an output taxonomy to classify the image based on the image as input.
8. The system of claim 7, wherein the text generator model is a text query-based model trained on text query-image pairs.
9. The system of claim 7 or 8, wherein processing the image using the image embedding model to obtain the embedding of the image pixels of the image comprises obtaining image features from a final fully-connected layer of a pre-trained convolutional network.
10. The system according to any one of claims 7-9, wherein training a multi-modal image classifier comprises:
splicing the N-dimensional text feature vector and the M-dimensional visual feature vector into a singular feature vector; and
the singular feature vectors are provided as input to a multi-modal classifier.
11. The system of any of claims 7-10, wherein the text generator model includes a softmax layer that produces a probability distribution across each possible predicted phrase.
12. One or more non-transitory computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:
for each image of the plurality of images:
processing the image through a text generator model to obtain a set of phrases describing content of the image, wherein each phrase is one or more terms;
processing the set of phrases by a text embedding model to obtain embedding of a predicted text of the image; and
processing the image using an image embedding model to obtain an embedding of image pixels of the image;
a multi-modal image classifier is trained on the embedding of the predictive text of the image and the embedding of the image pixels of the image to produce as output a label of an output taxonomy to classify the image based on the image as input.
13. The one or more non-transitory computer storage media of claim 12, wherein the text generator model is a text query-based model trained on text query-image pairs.
14. The one or more non-transitory computer storage media of claim 12 or claim 13, wherein processing the image using the image embedding model to obtain the embedding of the image pixels of the image comprises: image features are obtained from the final fully connected layer of the pre-trained convolutional network.
15. The one or more non-transitory computer storage media of any of claims 12-14, wherein training a multi-modal image classifier comprises:
splicing the N-dimensional text feature vector and the M-dimensional visual feature vector into a singular feature vector; and
the singular feature vectors are provided as input to a multi-modal classifier.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862768701P | 2018-11-16 | 2018-11-16 | |
US62/768,701 | 2018-11-16 | ||
PCT/US2019/061950 WO2020102797A1 (en) | 2018-11-16 | 2019-11-18 | Multimodal image classifier using textual and visual embeddings |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112204575A true CN112204575A (en) | 2021-01-08 |
Family
ID=68848444
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980036217.9A Pending CN112204575A (en) | 2018-11-16 | 2019-11-18 | Multi-modal image classifier using text and visual embedding |
Country Status (4)
Country | Link |
---|---|
US (2) | US11907337B2 (en) |
EP (1) | EP3791322A1 (en) |
CN (1) | CN112204575A (en) |
WO (1) | WO2020102797A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112766284A (en) * | 2021-01-26 | 2021-05-07 | 北京有竹居网络技术有限公司 | Image recognition method and device, storage medium and electronic equipment |
CN113469197A (en) * | 2021-06-29 | 2021-10-01 | 北京达佳互联信息技术有限公司 | Image-text matching method, device, equipment and storage medium |
CN116912629A (en) * | 2023-09-04 | 2023-10-20 | 小舟科技有限公司 | General image text description generation method and related device based on multi-task learning |
Families Citing this family (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11587139B2 (en) * | 2020-01-31 | 2023-02-21 | Walmart Apollo, Llc | Gender attribute assignment using a multimodal neural graph |
US11615567B2 (en) * | 2020-11-18 | 2023-03-28 | Adobe Inc. | Image segmentation using text embedding |
US20230068502A1 (en) * | 2021-08-30 | 2023-03-02 | Disney Enterprises, Inc. | Multi-Modal Content Based Automated Feature Recognition |
CN114298121A (en) * | 2021-10-09 | 2022-04-08 | 腾讯科技（深圳）有限公司 | Multi-mode-based text generation method, model training method and device |
CN116028621A (en) * | 2021-10-26 | 2023-04-28 | 北京三星通信技术研究有限公司 | Text classification method, apparatus, electronic device, storage medium and program product |
CN114548067B (en) * | 2022-01-14 | 2023-04-18 | 哈尔滨工业大学（深圳） | Template-based multi-modal named entity recognition method and related equipment |
CN115294150A (en) * | 2022-06-22 | 2022-11-04 | 华为技术有限公司 | Image processing method and terminal equipment |
CN117197737A (en) * | 2023-09-08 | 2023-12-08 | 数字广东网络建设有限公司 | Land use detection method, device, equipment and storage medium |
CN117235605B (en) * | 2023-11-10 | 2024-02-02 | 湖南马栏山视频先进技术研究院有限公司 | Sensitive information classification method and device based on multi-mode attention fusion |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10026020B2 (en) * | 2016-01-15 | 2018-07-17 | Adobe Systems Incorporated | Embedding space for images with multiple text labels |
-
2019
- 2019-11-18 WO PCT/US2019/061950 patent/WO2020102797A1/en unknown
- 2019-11-18 CN CN201980036217.9A patent/CN112204575A/en active Pending
- 2019-11-18 US US17/046,313 patent/US11907337B2/en active Active
- 2019-11-18 EP EP19818391.5A patent/EP3791322A1/en active Pending
-
2024
- 2024-01-10 US US18/409,411 patent/US20240143700A1/en active Pending
Cited By (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112766284A (en) * | 2021-01-26 | 2021-05-07 | 北京有竹居网络技术有限公司 | Image recognition method and device, storage medium and electronic equipment |
CN112766284B (en) * | 2021-01-26 | 2023-11-21 | 北京有竹居网络技术有限公司 | Image recognition method and device, storage medium and electronic equipment |
CN113469197A (en) * | 2021-06-29 | 2021-10-01 | 北京达佳互联信息技术有限公司 | Image-text matching method, device, equipment and storage medium |
CN113469197B (en) * | 2021-06-29 | 2024-03-22 | 北京达佳互联信息技术有限公司 | Image-text matching method, device, equipment and storage medium |
CN116912629A (en) * | 2023-09-04 | 2023-10-20 | 小舟科技有限公司 | General image text description generation method and related device based on multi-task learning |
CN116912629B (en) * | 2023-09-04 | 2023-12-29 | 小舟科技有限公司 | General image text description generation method and related device based on multi-task learning |
Also Published As
Publication number | Publication date |
---|---|
US11907337B2 (en) | 2024-02-20 |
US20210264203A1 (en) | 2021-08-26 |
WO2020102797A1 (en) | 2020-05-22 |
EP3791322A1 (en) | 2021-03-17 |
US20240143700A1 (en) | 2024-05-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN112204575A (en) | Multi-modal image classifier using text and visual embedding | |
US11507800B2 (en) | Semantic class localization digital environment | |
US10853726B2 (en) | Neural architecture search for dense image prediction tasks | |
CN107066464B (en) | Semantic natural language vector space | |
WO2021164772A1 (en) | Method for training cross-modal retrieval model, cross-modal retrieval method, and related device | |
CN112106081A (en) | Application development platform and software development suite for providing comprehensive machine learning service | |
KR101754473B1 (en) | Method and system for automatically summarizing documents to images and providing the image-based contents | |
US20150310862A1 (en) | Deep learning for semantic parsing including semantic utterance classification | |
Mohamed et al. | Content-based image retrieval using convolutional neural networks | |
US20210056127A1 (en) | Method for multi-modal retrieval and clustering using deep cca and active pairwise queries | |
WO2023020005A1 (en) | Neural network model training method, image retrieval method, device, and medium | |
CN112384909A (en) | Method and system for improving text-to-content suggestions using unsupervised learning | |
Liu et al. | Unsupervised deep domain adaptation for pedestrian detection | |
EP4302234A1 (en) | Cross-modal processing for vision and language | |
CN112400165A (en) | Method and system for improving text-to-content suggestions using unsupervised learning | |
CN108268629B (en) | Image description method and device based on keywords, equipment and medium | |
WO2022222854A1 (en) | Data processing method and related device | |
JP2010009517A (en) | Learning equipment, learning method and program for pattern detection device | |
Yang et al. | Joint graph regularized extreme learning machine for multi-label image classification | |
CN116304135B (en) | Cross-modal retrieval method, device and medium based on discriminant hidden space learning | |
US20240126993A1 (en) | Transformer-based text encoder for passage retrieval | |
US20240062560A1 (en) | Unified scene text detection and layout analysis | |
US20240119077A1 (en) | Apparatus and method for sharing and pruning weights for vision and language models | |
Shen | Image understanding via learning weakly-supervised cross-modal semantic translation | |
Ye et al. | Hierarchical abstract semantic model for image classification |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |