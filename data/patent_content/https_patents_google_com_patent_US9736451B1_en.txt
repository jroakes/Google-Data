US9736451B1 - Efficient dense stereo computation - Google Patents
Efficient dense stereo computation Download PDFInfo
- Publication number
- US9736451B1 US9736451B1 US15/394,314 US201615394314A US9736451B1 US 9736451 B1 US9736451 B1 US 9736451B1 US 201615394314 A US201615394314 A US 201615394314A US 9736451 B1 US9736451 B1 US 9736451B1
- Authority
- US
- United States
- Prior art keywords
- image
- disparity
- pixel
- vertex
- loss function
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
- H04N13/239—Image signal generators using stereoscopic image cameras using two 2D image sensors having a relative position equal to or related to the interocular distance
-
- H04N13/0022—
-
- H04N13/0239—
-
- H04N13/0296—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/128—Adjusting depth or disparity
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/296—Synchronisation thereof; Control thereof
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N2013/0074—Stereoscopic image analysis
- H04N2013/0077—Colour aspects
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N2013/0074—Stereoscopic image analysis
- H04N2013/0081—Depth or disparity estimation from stereoscopic image signals
Definitions
- Stereoscopic imaging typically involves capturing at least two images of a scene from two or more different perspectives (e.g., with parallel optical axes that are offset from each other by some distance), such that the images provide depth information about the scene.
- the images could be taken using two cameras having optical axes that are substantially parallel, which are separated horizontally (relative to a horizon of the scene) by a distance that may be referred to as the “baseline.”
- An individual image generated by stereo imaging could include image data related to the contents of the scene (e.g., the color, shape, location perpendicular to the direction a corresponding perspective, or other properties of contents of the scene) while differences between two images generated by stereo imaging could be related to the location of the contents of the scene relative to the corresponding two perspectives.
- differences between two images generated by stereo imaging can be related to the depth (i.e., the distance between contents of the scene and the corresponding perspective in a direction parallel to the orientation of the perspectives) of contents of the scene.
- depth i.e., the distance between contents of the scene and the corresponding perspective in a direction parallel to the orientation of the perspectives
- an object that is deeper in the scene i.e., that is further away from the cameras
- an object that is closer to the camera will appear at locations in the two images that are closer to one another within the image frame, than an object that is closer to the camera.
- an object that is very close to the two cameras e.g., a few inches away
- an object that is far away e.g. 10 feet or more.
- the ability to quickly determine depth information for a scene may be beneficial in various applications.
- a robot or other autonomous object could use determined depth information about obstacles in an environment in order to navigate the environment while avoiding the obstacles.
- determined depth and/or location information could be used to map the environment and/or to map the shape of one or more objects in the environment.
- Further applications include but are not limited to image processing, augmented reality, human interfaces, logistics, and biometrics.
- Stereoscopic imaging could be employed to capture two or more images of the environment to produce one or more stereo image pairs of the environment.
- a relative distance i.e., a disparity
- the disparity of a stereo image pair, or of subsections or elements of one or both images of a stereo image pair could be determined by a variety of methods.
- Methods described herein could allow for the determination of disparity values for a plurality of pixel locations of an image of a stereo image pair (e.g., for every pixel of the image) an order of magnitude faster than other methods currently in use for determining such disparity values.
- a method includes a computing system: (a) receiving a first image of a scene and a second image of the scene; (b) determining a disparity loss function for each of a plurality of pixel locations in the first image, wherein the disparity loss function for a given pixel location in the first image indicates variation in color-space similarity between: (i) the pixel location in the first image and (ii) a corresponding range of pixel locations in the second image; (c) associating each of the plurality of pixel locations with one or more of a plurality of vertices in a vertex space, wherein the vertex space comprises two spatial dimensions and at least one color-space dimension; (d) for each vertex in the vertex space, determining a vertex-disparity loss function based on the disparity loss functions corresponding to the associated pixel locations from the first image; (e) applying the determined vertex-disparity loss functions to determine a respective disparity value for each of the plurality of vertices
- a non-transitory computer readable medium has stored therein instructions executable by a computing system to cause the computing system to perform functions including: (a) receiving a first image of a scene and a second image of the scene; (b) determining a disparity loss function for each of a plurality of pixel locations in the first image, wherein the disparity loss function for a given pixel location indicates variation in color-space similarity between: (i) the pixel location in the first image and (ii) a corresponding range of pixel locations in the second image; (c) associating each of the plurality of pixel locations with one or more of a plurality of vertices in a vertex space, wherein the vertex space comprises two spatial dimensions and at least one color-space dimension; (d) for each vertex in the vertex space, determining a vertex-disparity loss function based on the disparity loss functions corresponding to the associated pixel locations from the first image; (e) applying the determined vertex-disparity loss
- a system in another aspect, includes: (a) two or more image capture systems that are oriented in substantially the same direction; and (b) a control system configured to: (i) operate a first image capture system of the two or more image capture systems to capture a first image of a scene; (ii) operate a second image capture system of the two or more image capture systems to capture a second image of a scene; (iii) determine a disparity loss function for each of a plurality of pixel locations in the first image, wherein the disparity loss function for a given pixel location in the first image indicates variation in color-space similarity between: (1) the pixel location in the first image and (2) a corresponding range of pixel locations in the second image; (iv) associate each of the plurality of pixel locations with one or more of a plurality of vertices in a vertex space, wherein the vertex space comprises two spatial dimensions and at least one color-space dimension; (v) for each vertex in the vertex space, determine a vertex-dispar
- FIG. 1A depicts contents of an example scene being imaged by a camera device.
- FIG. 1B depicts the camera device of FIG. 1A
- FIG. 2A is a first example image of the scene depicted in FIG. 1A as captured by a first camera of the camera device of FIGS. 1A and 1B .
- FIG. 2B is a second example image of the scene depicted in FIG. 1A as captured by a second camera of the camera device of FIGS. 1A and 1B .
- FIG. 3 depicts an example disparity map that could be determined from the example images of FIGS. 2A and 2B .
- FIG. 4A depicts pixel locations of first and second example images.
- FIG. 4B depicts an example disparity loss function related to a similarity between a given pixel location of the first image of FIG. 4A and pixel locations of the second image of FIG. 4A .
- FIG. 5A depicts pixel locations of an example one-dimensional image.
- FIG. 5B depicts an example vertex space containing example vertices into which the pixel locations of FIG. 5A have been embedded.
- FIG. 6A depicts pixel locations of an example one-dimensional image.
- FIG. 6B depicts an example vertex space containing example vertices into which the pixel locations of FIG. 6A have been embedded.
- FIG. 7A depicts pixel locations of an example one-dimensional image.
- FIG. 7B depicts an example vertex space containing example vertices into which the pixel locations of FIG. 7A have been embedded.
- FIG. 7C graphically depicts a filtering process performed on the vertices illustrated in FIG. 7B .
- FIG. 7D depicts an example vertex space containing filtered output vertices of the filtering process illustrated in FIG. 7C and related embedded pixel locations.
- FIG. 7E depicts pixel locations of a filtered one-dimensional image generated based on the filtered output vertices of FIG. 7D .
- FIG. 8 illustrates an example method
- FIG. 9A depicts front, right side, and rear views of a digital camera device, in accordance with an example embodiment.
- FIG. 9B shows a digital camera device with an arrangement of four cameras oriented in the same direction, in accordance with an example embodiment.
- FIG. 9C shows another digital camera device with an arrangement of four cameras oriented in the same direction, in accordance with an example embodiment.
- FIG. 9D shows a digital camera device with an arrangement of six cameras oriented in the same direction, in accordance with an example embodiment.
- FIG. 9E shows another digital camera device with a multi-camera arrangement, in accordance with an example embodiment.
- FIG. 10 is a simplified block diagram showing some of the components of an example computing device that may include camera components.
- Example embodiments may help to determine depth and/or other location information for a scene in a more efficient manner.
- depth, disparity, and/or other location information may be determined for the contents of a scene (e.g., objects, obstacles, surfaces) based on two or more images of the scene.
- a robot or other autonomous object e.g., a drone, or a self-driving car
- determined location information could be used to map the environment and/or to map the shape of one or more objects in the environment.
- Further applications include but are not limited to image processing (e.g., performing image processing selectively on regions of an image corresponding to objects within a specified depth range), augmented reality (e.g., providing depth cues, assisting in navigation), human interfaces (e.g., determining the location of parts of a user's body and using the determined locations to control the operation of a computer or other system), logistics (e.g., determining the location and size of boxes, packages, or other objects in a transport vehicle, container, loading zone, warehouse or other environment), biometrics (e.g., determining a depth-based mapping of a person's face or other body part and comparing the determined mapping to a database of stored mappings corresponding to identified users), and determining the location of a device (e.g., matching an image-determined mapping to a region of a known environment).
- image processing e.g., performing image processing selectively on regions of an image corresponding to objects within a specified depth range
- augmented reality e.g., providing
- stereoscopic imaging may involve capturing two or more images of the environment from two or more perspectives that are generally oriented in substantially the same direction and separated by a specified distance. A pair of images captured in this manner may be referred to as a “stereo image pair.”
- the distance between the image-frame location of an object in the first image and the image-frame location of the same object in the second image is related to the distance between the object and the stereo imaging device used to capture the stereo image pair.
- This distance is referred to herein as the disparity of the object and/or of the location (e.g., a pixel location) of the object within the image-frame of the first or second images.
- an object far from a stereo imaging device could appear in areas of the stereo image pair that are substantially overlapping (e.g., that have a displacement and/or disparity of approximately zero).
- an object near the stereo imaging device will typically have image-frame locations in a stereo image pair that are offset from each other by a distance (i.e., that have a positive disparity) that is inversely proportional to the distance from the stereo imaging device.
- the disparity of a stereo image pair, or of subsection or elements of one or both images of a stereo image pair could be determined by a variety of methods.
- a disparity value may be determined for each of a plurality of points and/or regions in a first image using the first image and a second image. This determination could include minimizing a cost function that takes as an input a plurality of disparity values corresponding to the plurality of points, pixels, regions, or other aspects of the first image.
- Such a cost function could include terms relating to the smoothness of the determined plurality of disparity values (i.e., to an overall degree of similarity between determined disparity values corresponding to respective points, pixels, regions, or other aspects of the first image that are proximate or in some way related (e.g., that are nearby in an X-Y space of the first image, that are similar in color and/or brightness, or some other metric or combination of metrics)).
- Such a cost function could additionally include terms relating to the degree of correspondence (e.g., a similarity of color and/or brightness) between an aspect of the first image (i.e., of a reference image) and an aspect of the second image of the stereo image pair (i.e., of a matching image) having a location in the second image corresponding to the location of the aspect in the first image offset by a corresponding determined disparity.
- degree of correspondence e.g., a similarity of color and/or brightness
- the cost function could include a sum of disparity loss functions for each pixel in the first image, where the disparity loss function for a given pixel indicates the variation in color-space similarity (e.g., a similarity of the color, brightness, etc.) between the given pixel in the first image and pixels of the second image located over a range of distances (e.g., a range of disparities) relative to the location of the given pixel in the first image.
- the disparity loss function for a given pixel indicates the variation in color-space similarity (e.g., a similarity of the color, brightness, etc.) between the given pixel in the first image and pixels of the second image located over a range of distances (e.g., a range of disparities) relative to the location of the given pixel in the first image.
- the disparity value for each point (e.g., for each pixel location) in a first image relative to a second image may be determined in a variety of ways.
- the process of determining disparity values for pixels of the first image could be simplified by associating pixels of the first image into groups (e.g., by determining the extent of objects in the first image (e.g., using edge detection or other image processing techniques)), with determined or specified features (e.g., specified blocks of the image, vertices in a space into which the pixels could be embedded), or according to some other method.
- each pixel in the first image could be associated with one or more vertices in a vertex space.
- the vertex space could have two spatial dimensions (e.g., spatial dimensions corresponding to the two spatial dimensions of pixel locations within the frame of the first image) and at least one color-space dimension (corresponding to the color, brightness, color-specific brightness, or properties of the first image represented by the pixels of the first image).
- the first and second images could be black-and-white images
- the vertex space could have two spatial dimensions corresponding to the location of pixels within the frame of the first image and a third color-space dimension corresponding to the brightness of the pixels in the first image.
- each pixel of the first image could have a location in the vertex space based on the location of the pixel within the frame of the first image and the brightness of the pixel.
- Individual pixels could then be associated with one or more vertices based on proximity to the vertices in the three-dimensional vertex space.
- a vertex-disparity loss function could be determined for each of the vertices based on the disparity loss functions of the zero or more pixels associated with each of the vertices.
- the vertex-disparity loss functions could then be applied to determine respective vertex disparity values for each of the vertices.
- a cost function that receives the set of determined vertex disparity values and that includes a sum of the vertex-disparity loss functions could be minimized (e.g., by gradient descent).
- Such a cost function could additionally include a smoothness term related to the similarity between individual determined disparity values.
- two example vertices in a vertex space are proximate in the vertex space or related in some other way.
- two sets of pixels corresponding respectively to the two example vertices could be close within the frame of the first image and/or have a similar brightness or color.
- a smoothness term could return a higher cost value when first and second vertex disparity values corresponding to the two example vertices are dissimilar (i.e., have different values) than when the first and second vertex disparity values are similar.
- the number of dimensions in the vertex space could vary, depending upon the particular application.
- disparity values could be determined for black-and-white images, or for images that have been processed to be black-and-white (e.g., by determining a brightness value for pixels of the image(s) based on color information of the pixels, e.g., by summing the red, green and blue light intensities of the pixel to determine an overall brightness value for the pixel).
- the vertex space could have two spatial dimensions corresponding to the location of pixels within the frame of the image(s) and a third color-space dimension corresponding to the brightness of the pixels.
- disparity values could be determined for color images (e.g., for images whose pixels define colors in a color-space, e.g., a red-green-blue (RGB) color space).
- a vertex space corresponding to such an example could have two spatial dimensions corresponding to the location of pixels within the frame of the image(s).
- the vertex space could have a further three color-space dimensions corresponding to the location, in the color-space, of colors defined by the pixels of the image(s).
- higher- or lower-dimensional vertex spaces could be used for images whose pixels define more or less color information.
- a vertex space having one fewer color-space dimension i.e., two color-space dimensions
- could be used for image(s) whose pixels only define colors in a two-dimensional color-space e.g., a red-green color-space, a red-near-infrared color-space.
- a vertex space could have more than three color-space dimensions for use with image(s) whose pixels define colors in a higher-dimensional color-space (e.g., a color-space defined to represent the output of detectors of a hyperspectral imager)) or that define other information (e.g., the direction and/or degree of polarization of light in the image(s)).
- a higher-dimensional color-space e.g., a color-space defined to represent the output of detectors of a hyperspectral imager
- other information e.g., the direction and/or degree of polarization of light in the image(s)
- the locations of the vertices in the vertex space could span a volume in the vertex space that is spanned by the pixels of a first (i.e., reference) image.
- the vertex space could be three-dimensional (e.g., the first image could be black-and-white) and the vertices could span a volume in the two spatial dimensions of the vertex space corresponding to the extent of the frame of the first image and in the one-dimensional color-space dimension corresponding to the range of brightness values represented by the pixels of the first image.
- the locations of the vertices in the vertex space could be regularly spaced (e.g., to provide a uniform ‘sampling’ of the volume of the vertex space corresponding to the first image).
- the vertices could be located in the vertex space at the vertices of a tessellated and/or regularly repeating array of three- or more-dimensional shapes or according to some other regular pattern.
- the vertices could be located in a rectangular array (i.e., regularly spaced along orthogonal directions in the multidimensional space, at the vertices of a tessellated array of hyper-rectangles).
- the vertices could be located in a triangular array (i.e., at the vertices of triangles, tetrahedra, or higher-dimensional simplexes that are regularly tessellated in the vertex space).
- imaging may refer to capturing and storing the color and brightness characteristics of a real-world environment or scene in a digital, chemical, or other format (e.g., in photographs and/or motion video).
- image capture devices e.g. CCDs, photodetector arrays, active pixel sensors
- CCDs compact discs
- photodetector arrays e.g., digital cameras
- active pixel sensors e.g., digital cameras, chemical, or other formats
- image capture devices e.g. CCDs, photodetector arrays, active pixel sensors
- Stereo imaging includes capturing two or more images of the same scene, from two or more perspectives that are oriented in substantially the same direction, to produce a stereo image pair.
- Producing a stereo image pair could include operating a single image capture device to capture multiple images at respective multiple points in time and from respective multiple perspectives. Additionally or alternatively, producing a stereo image pair could include operating multiple image capture devices separated by a fixed or a variable distance and configured to capture respective multiple images simultaneously or at respective points in time.
- a pair of images of a scene corresponding to different perspectives of the scene may be referred to as stereo image data or as a stereo image pair.
- Stereo image data may be used to produce one or more human-viewable stereo images, which may be referred to herein as a viewable stereoscopic image or 3D image. Additionally or alternatively, stereo image data may be analyzed to extract depth or other information about the scene. In particular, objects or points in the scene can be mapped in a three-dimensional (3D) coordinate system based on the stereo image data. For example, such a mapping could be based on a determined disparity between objects or points in two images of the scene.
- mapping information determined from stereo image pairs could be used for a variety of different applications.
- Applications could include performing various image processing effects based on the determined mapping information (e.g., increasing or decreasing the apparent depth of objects in the image, applying a filter (e.g., a blur) to objects and/or regions of an image according to a determined depth or other location data about the objects and/or regions of the image).
- Mapping data could be used for navigation (e.g., to allow for navigation of robots and/or vehicles through an environment), augmented reality (e.g., mapping a scene to increase depth perception and accuracy), or other applications.
- Depth information for a scene may be determined based on a stereo image pair of the scene. Such depth information may indicate the distance to various aspects in the scene from the image capture device that captured the stereo image pair. Such depth information could be determined for a plurality of pixels or groups of pixels in an image (e.g., for all objects and/or regions in an image of a scene generated by an image capture device) by assigning a depth value for each pixel of one or both of the images in a stereo image pair (i.e., by constructing a depth map).
- the depth of an object and/or region of a first image could be determined based on a determined disparity of the object and/or region of the first image (herein referred to as the reference image) relative to a corresponding object and/or region of a second image (herein referred to as the matching image), where the reference image and the matching image form the stereo image pair.
- the disparity is related to the relative displacement of two points corresponding to the same object and/or region in respective two or more images of a scene containing the object and/or region.
- the disparity between objects and/or regions in a scene could be determined based on the individual images in a stereo image pair in a variety of ways.
- the disparity could be determined for a number of different objects and/or regions of the reference or matching image (e.g., for each of a plurality of pixel locations (i.e., the locations of the pixels) of an image or for the locations of a subset of pixels of the image, for one or more specified subregions (e.g., pixel clusters, specified and/or determine bounded regions (e.g., bounded regions corresponding to individual discrete object in the image/scene) of the image, predetermined sectors or segments of an image) of an image, for a an image as a whole).
- the determination of a depth based on a disparity could include a variety of methods and/or algorithms (e.g., look-up tables, functions based on geometry (e.g., based on a separation between two or more cameras)).
- FIG. 1A illustrates scene 100 containing near object 103 and distant object 105 .
- FIG. 1A additionally illustrates a device 110 disposed proximate to the near object 103 and configured to generate images of the scene 100 (e.g., to generate a stereo image pair of the scene 100 ) among other functions.
- device 110 may be a cellular mobile telephone (e.g., a smartphone), a still camera, a video camera, a computer (such as a desktop, notebook, tablet, or handheld computer), a personal digital assistant (PDA), a home automation component, a digital video recorder (DVR), a digital television, a remote control, a wearable computing device, or some other type of device equipped with at least two image capture and/or image processing devices.
- PDA personal digital assistant
- DVR digital video recorder
- device 110 may represent a discrete physical device such as a digital camera, a particular physical hardware platform on which a camera application operates in software, or other combinations of hardware and software that are configured to carry out camera functions.
- Device 110 includes two image capture devices, reference imager 115 a and matching imager 115 b , which could be operated independently and/or collaboratively (i.e., by one or more controllers of the device 110 (not shown)).
- FIG. 1B provides a more detailed view of device 100 , illustrating the disposition of the reference imager 115 a and the matching imager 115 b in the device 110 .
- Reference imager 115 a and matching imager 115 b are separated by a specified distance such that the imagers 115 a , 115 b could be operated to capture respective images of the scene from respective perspectives.
- the imagers 115 a , 115 b are oriented such that the respective perspectives have orientations that are substantially in the same direction.
- FIG. 2A illustrates an example reference image 200 a of the scene 100 , including a reference near object image 210 a and a reference distant object image 220 a .
- the example reference image 200 a could be generated by the reference imager 115 a .
- FIG. 2B illustrates a matching image 200 b of the scene 100 , including a matching near object image 210 b and matching distant object image 220 b .
- the example matching image 200 b could be generated by the matching imager 115 b.
- Images 200 a and 200 b could be used to determine a depth map based on determined disparity information relating to the images 200 a , 200 b .
- disparity values for pixel locations of the reference image 200 a could be determined based on the reference 200 a and matching 200 b images.
- a depth map could be determined based on such determined disparity values.
- 215 a and 215 b illustrate the location of pixel locations within the reference 200 a and matching 200 b images, respectively, corresponding to images (i.e., aspects of 210 a , 210 b , respectively) of the same portion of the near object 103 .
- the location within the frame of the matching image 200 b corresponding to the location of 215 a within the reference image 200 a is illustrated by 217 b .
- 225 a and 225 b illustrate the pixel locations within the reference 200 a and matching 200 b images, respectively, corresponding to images (i.e., aspects of 220 a , 220 b , respectively) of the same portion of the distant object 105 .
- the location within the frame of the matching image 200 b corresponding to the location of 225 a within the reference image 200 a is illustrated by 227 b.
- distant object images 220 a , 220 b are substantially coincident (i.e., have a disparity of substantially zero, as illustrated by the overlapping of 225 b and 227 b ) due to the distance of the distant object 105 relative to the device 110 . Due to the proximity (e.g., relative to a baseline length of cameras of the device 100 or other properties of the device 100 ) of the near object 103 relative to the device 110 , near object images 210 a , 210 b are displaced relative to each other by a disparity. The disparity between the near object images 210 a , 210 b is related to the distance illustrated by the difference the locations of 215 b and 217 b.
- Disparity could be specified in a variety of ways, e.g., a number of pixel widths, a fraction of the width of one or both of the frames of the images 200 a , 200 b , an angular difference, or by some other method. Disparity could be measured or determined as a discrete value (e.g., a number of pixel-widths of displacement between a first pixel location in the first image corresponding to an object in the scene and a second pixel location in the second image corresponding to the object in the scene) or as a continuous value.
- a discrete value e.g., a number of pixel-widths of displacement between a first pixel location in the first image corresponding to an object in the scene and a second pixel location in the second image corresponding to the object in the scene
- a disparity value could be determined for a plurality of points in one or both of the images 200 a , 200 b .
- a disparity could be determined for every pixel location of the reference image 200 a to generate a disparity map.
- FIG. 3 illustrates an example disparity map 300 corresponding to the reference image 200 a .
- image 300 there are simplified depth map representations of objects in the images 200 a , 200 b , with a disparity between image 210 a and image 210 b being represented by near object disparity image 310 (e.g., the illustrated portion 315 of the near object disparity image 310 could be related to the disparity between 215 a and 215 b , e.g., to a determined distance between 215 b and 217 b ).
- near object disparity image 310 e.g., the illustrated portion 315 of the near object disparity image 310 could be related to the disparity between 215 a and 215 b , e.g., to a determined distance between 215 b and 217 b ).
- a disparity between image 220 a and image 220 b is represented by distance object disparity image 320 (e.g., the illustrated portion 325 of the distant object disparity image 320 could be related to the disparity between 225 a and 225 b , e.g., to a determined distance (e.g., a coincidence or distance of zero) between 225 b and 227 b ).
- distance object disparity image 320 e.g., the illustrated portion 325 of the distant object disparity image 320 could be related to the disparity between 225 a and 225 b , e.g., to a determined distance (e.g., a coincidence or distance of zero) between 225 b and 227 b ).
- the location of images (e.g., the extent and positioning of near object disparity image 310 ) within the frame of the disparity map 300 corresponds to the location of images within the frame of the reference image 200 a (e.g., to the extent and positioning of near object image 210 a ).
- the grayscale color of a given aspect of the disparity map 300 corresponds to a determined and/or measured disparity of an aspect of the reference image 200 a that is at the same location within the frame of the reference image 200 a as the location of the given aspect of the disparity map 300 within the frame of the disparity map 300 .
- the grayscale color of the disparity map 300 at 315 corresponds to the determined and/or measured disparity between 215 a and 215 b (e.g., to a determined distance between 215 b and 217 b ).
- regions that are darker (e.g., black) indicate greater disparities and regions that are lighter (e.g., white) indicate lesser disparities (e.g., a white region could indicate a disparity of zero).
- Depth information may be determined from disparity information (e.g., from the disparity map 300 ). As such, the disparity information may be utilized to generate a depth map of the scene. The depth map may then be used help determine focus settings for the scene, to determine the location of objects in the scene, to map the scene, to navigate the scene (e.g., to operate a robot or other autonomous device to move through the scene) and/or may be used for other purposes.
- a distance from the device 100 e.g., a depth
- a distance from the device 100 e.g., a depth
- such a calculation could include using a lookup table to determine a depth value based on a determined and/or measured disparity value.
- a model of the device 100 e.g., a model of the imagers 115 a , 115 b
- a disparity and information about the device 100 e.g., a separation distance between the imagers 115 a , 115 b .
- Such a model-based approach could allow depth values to be determined for a variety of different configurations of the device 100 .
- a separation distance between the imagers 115 a , 115 b could be variable, and the separation distance could be controlled to allow for some application (e.g., to allow for high-accuracy determination of the depth of certain objects in a scene by controlling the separation distance between the imagers 115 a , 115 b ).
- Other applications of determined disparity information for a scene and/or one or more pairs of images of a scene are anticipated.
- image processing could be performed on aspects of an image based on determined disparity and/or depth information corresponding to the aspects of the image. For example, regions of an image having determined disparity and/or depth values outside of a specified range could be blurred or otherwise filtered or processed (e.g., to simulate an image that could be created by a camera having a more limited depth of focus than the imagers used to create a stereo image pair (e.g., 115 a , 115 b )). Additionally or alternatively, the size, location, color balance, contrast, or other properties of aspects of an image could be altered based on a determined disparity and/or depth value.
- the distance to an object in a scene could be determined based on a determined disparity of image(s) of the object, and a focus (e.g., a configuration of optics, a focal length of one or more lenses) of an imager (e.g., an imager used to create part of a stereo image pair used to determine the determined disparity) could be adjusted based on the determined distance such that the object can be imaged in-focus.
- a focus e.g., a configuration of optics, a focal length of one or more lenses
- an imager e.g., an imager used to create part of a stereo image pair used to determine the determined disparity
- Other applications of disparity values determined based on stereo image pairs are anticipated.
- Calculating disparities based on stereo image pairs could be a computationally expensive process.
- the process of determining disparities based on digital images could include performing calculations to determine a disparity value for each point (e.g., each pixel location) within a first image of a stereo image pair. Such a determination could include comparing properties of a given point (e.g., color, location) to a set of corresponding points in a second image of the stereo image pair.
- the set of corresponding points of the second image could be points across a range of distances from a location within the second image corresponding to the location of the given point within the first image, e.g., points in the second image having displacements within a specified range relative to the location of the given point.
- Determination of a plurality of disparity values for a corresponding plurality of points in an image could include determining a disparity loss function for each of the plurality of points.
- a disparity loss function for a given pixel location of a first image could take as an input a disparity value and could be related to a degree of similarity between the given pixel location (e.g. a similarity of color, brightness, or some other pixel property) and a pixel location of a second image that is displaced from the given pixel location, within the second image, by a distance equal to the input disparity value.
- the disparity loss function for a given pixel location could be defined for a range of pixel locations in the second image; for example, the disparity loss function for a given pixel location could be defined for a range of distances between zero (i.e., comparison between the given pixel location and a pixel location of the second image having the same location as the given pixel location, within the frame of the second image) and some defined upper bound (e.g., a distance between the given pixel location and the edge of the second image, such that the disparity loss function describes the similarity between the given pixel location and a range of pixel locations of the second image at or to one direction (e.g., the right) of the given pixel location, in the second image)).
- zero i.e., comparison between the given pixel location and a pixel location of the second image having the same location as the given pixel location, within the frame of the second image
- some defined upper bound e.g., a distance between the given pixel location and the edge of the second image,
- FIG. 4A illustrates reference 400 a and matching 400 b example images that contain first 401 a and second 401 b images, respectively, of an object in an imaged scene.
- the imaged scene includes a light-colored ball against a dark-colored background.
- the image of the ball is displaced between the reference 400 a and matching 100 b images due to the ball being close to an imaging device (not shown) used to image the scene and to generate the images 400 a , 400 b .
- a given pixel location 410 of the reference image 400 a within the frame of the reference image 400 a is illustrated in FIG. 4A .
- a corresponding matching set of pixel locations 420 of the matching image 400 b is also illustrated.
- the matching set of pixel locations 420 includes all of the pixel locations of the matching image 400 b at and/or to the right of the given pixel location 410 , within the frame of the matching image 400 b.
- a variety of disparity loss functions could be determined and/or defined for the given pixel location 410 of the reference image 400 a based on a similarity between the given pixel location 410 and the matching set of pixel locations 420 of the matching image 400 b .
- a disparity loss function could be related to a similarity between the color, brightness, or other property of the given pixel location 410 and the corresponding property of individual pixel locations of the matching set of pixel locations 420 .
- a disparity loss function could be related to a similarity between the color, brightness, or other property of the given pixel location 410 and subsets of the matching set of pixel locations 420 (e.g., input disparities to the disparity loss function corresponding to a contiguous string of ‘green’ pixel locations in the set of matching pixel locations 420 could be result in identical outputs of the disparity loss function).
- the output of the disparity loss function could be continuous-valued or discrete-valued (e.g., output value ‘0’ for pixel locations of the matching set of pixel locations 420 that are more similar to the given pixel location 410 than a specified threshold, output value ‘1’ otherwise).
- FIG. 4B illustrates an example disparity loss function 430 for the given pixel location 410 relative to the matching set of pixel locations 420 .
- the example disparity loss function 430 is plotted such that the input displacement of the example disparity loss function 430 is vertically aligned below a copy of the matching set of pixel locations 420 .
- a section of the copy of the matching set of pixel locations 420 indicating the pixel locations corresponding to the image of the ball is illustrated as similar pixel locations 425 .
- Similar pixel locations 425 are a string of contiguous pixel locations of the matching set of pixel locations 420 having a brightness (or other property, e.g., color) similar to the brightness (or corresponding other property) of the given pixel location 410 .
- Other types of disparity loss functions for pixel locations of a first (e.g., reference) image of a stereo image pair relative to a second (e.g., matching) image of the stereo image pair are anticipated.
- the example disparity loss function 430 is a hinge loss function; that is, the example disparity loss function 430 has a minimum value (e.g., zero) for input disparities corresponding to the location of the similar pixel locations 425 within the frame of the matching image 400 b relative to the location of the given pixel location 410 within the frame of the reference image 400 a .
- the extent of the input disparity values that result in this minimum output of the disparity loss function are specified by upper 440 b and lower 440 a hinge disparities.
- the output of the example disparity loss function 430 increases linearly for input disparities outside the range defined by the hinge disparities 440 a , 400 b .
- f i (d i ) is the disparity loss function for the i th pixel location of a set of pixel locations, given an input disparity of d i .
- the upper 440 b and lower 440 a hinge disparities are represented by u i and l i , respectively, and max(x,y) indicates an operation to return the maximum of input arguments x and y.
- the example disparity loss function 430 could be parameterized by additional factors. For example, the slope of the increasing portions of the example disparity loss function 430 could be specified. Hinge disparities and other parameters of a disparity loss function could be determined for each pixel location of the reference image 400 a . Such a determination could be performed relative to one or more specified thresholds or other specified parameters.
- the extent of the hinge disparities for a particular pixel location of the reference image 400 a could be determined based on the extent of a contiguous string of pixel locations in the matching image 400 b that has a brightness, color, or other property that is within a specified threshold of the corresponding property of the particular pixel location.
- thresholds or other parameters could be specified and/or determined for the particular pixel location based, e.g., on the properties of corresponding pixel locations of the matching image.
- a specified brightness, color, or other property similarity threshold could be determined based on a variance or function of one or more properties of the pixel locations of the matching image.
- Disparity values for individual pixel locations of a first (e.g., reference) image of a stereo image pair could be determined based on respective disparity loss functions of the individual pixel locations in a variety of ways.
- the disparity values of individual pixel locations of the first image could be determined by performing an optimization using the determined disparity loss functions, e.g., by performing a gradient descent using a cost function that is based on the disparity loss functions (e.g., that comprises a weighted sum of the disparity loss functions of the pixel locations of the first image).
- the disparity values of individual pixel locations of the first image could be determined independently of the determination of disparity values for other pixel locations of the first image.
- the disparity values of individual pixel locations of the first image could be determined in consideration of the disparity values of disparity values for other pixel locations of the first image.
- a cost function that includes a loss term related to the determined disparity loss functions of the pixel locations and that includes a smoothness term related to the similarity between individual determined disparity values could be minimized (e.g., by gradient descent, dynamic programming, or some other method) to determine disparity values for pixel locations of the first image.
- f i (d i ) is the disparity loss function for the i th pixel location of a set of pixel locations of a first image, given an input disparity of d i .
- ⁇ i,j is an affinity factor related to a specified smoothness cost for similarity between determined disparity values for the i th and j th pixel locations of the set of pixel locations of the first image. That is, a higher value for ⁇ i,j will result in a higher cost for a given difference between a determined disparity d i of the i th pixel location and a determined disparity d i of the j th pixel location.
- ⁇ is a weighting factor that can be specified to set a relative importance of the smoothness term and the loss term in determining the overall cost of a given set of determined disparity values d (wherein the i th element of d is d i , the determined disparity value for the i th pixel location of the first image).
- the affinity factors ⁇ i,j could be chosen such that the smoothness cost for similarity between determined disparity values for the i th and j th pixel locations of the set of pixel locations of the first image is related to proximity between the i th and j th pixel locations within the frame of the first image.
- the smoothness cost for similarity could be related to an exponential or other function of the distance, within the frame of the first image, between the i th and j th pixel locations.
- the smoothness cost for similarity could be zero for pairs of i th and j th pixel locations that are more than a specified distance away from each other or according to some other criterion.
- the smoothness cost for similarity could be zero for pairs of i th and j th pixel locations that are not adjacent.
- ⁇ could be a sparse matrix, as the ⁇ i,j corresponding to pairs of i th and j th pixel locations that are not adjacent would be zero.
- the affinity factors ⁇ i,j could be chosen such that the smoothness cost for similarity between determined disparity values for the i th and j th pixel locations of the set of pixel locations of the first image is related to some other property or properties of the pixel locations (e.g., a similarity between a brightness, a color, or some other property or properties of the pixel locations).
- the affinity factors ⁇ i,j could be chosen such that the smoothness cost for similarity between determined disparity values for the i th and j th pixel locations of the set of pixel locations of the first image is related to a similarity in space (e.g., to proximity between pixel locations in the frame of the first image) and similarity in color-space (e.g., to a proximity in red-green-blue (RGB) or some other color-space representation of the color of the pixel locations) of the pixel locations.
- a similarity in space e.g., to proximity between pixel locations in the frame of the first image
- color-space e.g., to a proximity in red-green-blue (RGB) or some other color-space representation of the color of the pixel locations
- the affinity factors ⁇ i,j could be Gaussian potentials between all pairs of pixel locations defined in a five-dimensional XYRGB (e.g., location (XY) within the frame of the first image and color-space representation (RGB)) space (related, e.g., to bilateral filters):
- a i , j ( ( [ x i , y i ] - [ x j , y j ] ) 2 2 ⁇ ⁇ xy 2 + ( [ r i , g i , b i ] - [ r j , g j , b j ] ) 2 2 ⁇ ⁇ rgb 2 )
- x i , x j represent the horizontal location of the i th and j th pixel locations, respectively, within the frame of the first image and y i , y j represent the vertical location of the i th and j th pixel locations, respectively, within the frame of the first image.
- r i , g j , b i represent the red, green, and blue intensities, respectively, of the i th pixel location and r j , g j , b j represent the red, green, and blue intensities, respectively, of the j th pixel location.
- ⁇ xy 2 is a scaling factor for location within the frame of the first image and ⁇ rgb 2 is a scaling factor for location within the RGB color-space within which the colors of the pixel locations are defined.
- affinity factors ⁇ i,j could be determined, specified, or otherwise used to determine a cost function or to otherwise determine a set of determined disparity values for pixel locations of a first image of a stereo image pair (or of some other pair of images for which disparity values can be determined for one or more pixel locations).
- the affinity factors ⁇ i,j could be determined based on Gaussian potentials defined in the XY space within the frame of the first image (e.g., using an equation similar to the equation above, but omitting the right term within the exponential).
- Other example functions, classes of functions, functionals, or other methods to determine and/or specify the affinity factors ⁇ i,j are anticipated.
- a matrix of the affinity factors ⁇ i.e., a matrix whose [i,j] th element is ⁇ i,j
- I is the identity matrix and the other symbols and factors are defined as above.
- the diagonal matrix D could be determined by a variety of methods (e.g., the Sinkhorn method, or by some other method or methods).
- the use of symmetric and bistochastic affinity factors ⁇ could allow for more efficient computation of the smoothness term (e.g., by allowing for some simplification of one or more matrix multiplication or other procedures involved in, e.g., performing gradient descent using the cost function to determine a set of disparity values for pixel locations of an image).
- Determination of a disparity value for each pixel location of a high-resolution image could be computationally expensive (due, e.g., to the total number of pixels in an image scaling roughly with the square of the image resolution).
- the computational effort e.g., processor resources (e.g., MIPS), processor time
- processor resources e.g., MIPS
- processor time e.g., processor time
- determining such sets of disparity values (and/or determining corresponding depth maps based on such) based larger and/or more detailed images can be intractable given finite computational resources and/or finite available time for computation (e.g., the limited computational resources of a mobile device).
- disparity values could be determined in a first step for a subset of pixel locations of an image (e.g., for a regularly spaced subset of pixel locations within an image) and the determined disparity values could be interpolated for pixel locations of the image that are not in the subset of pixel locations for which disparity values were determined in the first step.
- a first image could be blocked or otherwise partitioned (e.g., in regular blocks, or according to areas corresponding to determined objects within the image, e.g., by some image processing) and individual disparity values could be determined for individual blocks or other partitions.
- vertex-disparity loss functions could be determined for a plurality of vertices in a vertex space, based on a plurality of disparity loss functions of pixel locations in the first image.
- a determination of vertex disparity values could include minimizing a cost function that includes a smoothness term related to a similarity (e.g., a proximity in the vertex space) between pairs of vertices and/or a similarity (e.g., a proximity in the vertex space and/or within the frame of the first image, a similarity in a color-space) between pixel locations associated with pairs of vertices.
- Pixel locations of an image could be associated with vertices in a vertex space.
- the association could be based on a proximity or other relationship within the vertex space between the pixel locations and the vertices (e.g., a given pixel location could be associated with the nearest vertex within the vertex space, or with a set of vertices (e.g., a set of vertices defining the vertices of a shape in the vertex space that encloses the given pixel location)).
- the vertex space could have spatial dimensions (e.g., dimensions corresponding to horizontal and vertical dimensions of the location of pixel locations within the image) and one or more color-space dimensions (e.g., dimensions corresponding to the brightness, color, red, green, or blue intensity, or other properties of the pixel locations within a color-space).
- spatial dimensions e.g., dimensions corresponding to horizontal and vertical dimensions of the location of pixel locations within the image
- color-space dimensions e.g., dimensions corresponding to the brightness, color, red, green, or blue intensity, or other properties of the pixel locations within a color-space.
- a process related to the pixel locations e.g., the determination of a set of disparity values of the pixel locations
- a process related to the pixel locations could be simplified (e.g., performed in less time and/or by using less computational resources) by translating the process such that the process can be performed for the vertices (which could number substantially fewer than the pixel locations).
- results of the process performed on the vertices could then be applied to the pixel locations (e.g., by interpolating or otherwise calculating disparity values for a particular pixel location of the image based on determined disparity values of vertices associated with the particular pixel location).
- FIG. 5A depicts an example one-dimensional image 500 a (i.e., a one-dimensional strip of pixel locations).
- Pixel locations of the image 500 a (including given pixel location 510 a ) define respective brightnesses (i.e., individual pixel locations of the image 500 a define colors within a one-dimensional (e.g., black-and-white) color-space).
- FIG. 5B depicts an example two-dimensional vertex space 500 b .
- the vertex space 500 b includes a spatial dimension (labeled ‘Pixel Location,’ illustrated as the horizontal axis) corresponding to the location of the pixel locations of the image 500 a within the one-dimensional ‘frame’ of the image 500 a .
- the vertex space 500 b additionally includes a color-space dimension (labeled ‘Color,’ illustrated as the vertical axis; brighter pixel locations are located in higher locations within the vertex space 500 b ).
- the location of the pixel locations of the image 500 a within the vertex space 500 b are indicated by crosses.
- a plurality of vertices are located in the vertex space 500 b .
- the locations of the vertices are regularly spaced in the vertex space 500 b in a triangular pattern and indicated by dots.
- the vertices within the vertex space 500 b could be described as being at the location of vertices of a regularly-stacked (or tessellated) pattern of regular 2-simplexes (i.e., triangles).
- the pixel locations of the image 500 a could be associated with vertices within the vertex space 500 b according to proximity or some other property of the relative location of the pixel locations and vertices within the vertex space 500 b .
- a given pixel location 510 b in the vertex space 500 b (corresponding to the given pixel location 510 a of the image 500 a ) could be associated with three vertices 520 that define an enclosing triangle 525 that encloses an area (or more generally, a space of dimension equal to the number of dimensions of the vertex space) of the vertex space that contains the given pixel location 510 b .
- the given pixel location 510 b could be equally associated with each of the three vertices 520 or could be associated in a weighted fashion according to some function or other consideration.
- the given pixel location 510 b could be associated with individual vertices of the three vertices 520 according to respective distances in the vertex space 500 b between the given pixel location 510 b and the individual vertices (e.g., according to some barycentric partition or other assignment of the given pixel location 510 b between the vertices of the three vertices 520 ).
- pixel locations within the enclosing triangle 525 could be associated with the three vertices 520 (though the weighting of the association between a first pixel location within 525 and the three vertices 520 could be different from the weighting of the association between a second pixel location within 525 and the three vertices 520 according, e.g., to a difference in the location within the vertex space 500 b of the first and second pixel locations).
- an example vertex 550 could be associated with pixel locations within a first example space 575 .
- the first example space 575 could be defined as the set of enclosing triangles that include the first example vertex 550 as a vertex.
- the given pixel location 510 b could be associated with a closest vertex 530 of the plurality of vertices that is closest to the given pixel location.
- pixel locations in the vertex space that are within an enclosing space 535 could also be associated with the first vertex.
- the enclosing space 535 could be defined as the subset of space within the vertex space 500 b within which the closest vertex 530 is the closest vertex of the plurality of vertices.
- the example vertex 550 could be associated with pixel locations 560 within a second example space 565 .
- the second example space 565 could be defined as the subset of space within the vertex space 500 b within which the example vertex 550 is the closest vertex of the plurality of vertices.
- the location of the plurality of vertices in the vertex space 500 b is meant as a non-limiting example of a pattern of location of vertices in a vertex space.
- the vertices are located in the vertex space 500 b at the vertices of regular, regularly-tiled triangles (i.e., regular 2-dimensional simplexes).
- the vertices could instead be located in some other regular or irregular pattern in the vertex space 500 b according to an application.
- the vertices could be located in the vertex space 500 b at the vertices of regularly-tiled irregular triangles (e.g., isosceles triangles, scalene triangles).
- the angle of the major axis of the tiling of the triangles could be parallel to one of the axes of one or more of the dimensions of the vertex space 500 b (e.g., parallel to the ‘Pixel Location’ Axis, as illustrated in FIG. 5B ) or could have some other orientation.
- the vertices could have a first ‘effective resolution’ (i.e., a period of pattern repetition) in a first direction or directions in the vertex space 500 b (e.g., a first period of repetition in spatial dimension of a vertex space) and a second ‘effective resolution’ in a second direction or directions (e.g., a second period of repetition in a color-space dimension of the vertex space).
- a first ‘effective resolution’ i.e., a period of pattern repetition
- a second ‘effective resolution’ in a second direction or directions
- Other patterns, scales, and schemes of association with pixel locations of a plurality of vertices in a vertex space are anticipated.
- FIG. 6A depicts an example one-dimensional image 600 a (i.e., a one-dimensional strip of pixel locations). Pixel locations of the image 600 a (including given pixel location 610 a ) define respective brightnesses (i.e., individual pixel locations of the image 600 a define colors within a one-dimensional (e.g., black-and-white) color-space).
- FIG. 6B depicts an example two-dimensional vertex space 600 b .
- the vertex space 600 b includes a spatial dimension (labeled ‘Pixel Location,’ illustrated as the horizontal axis) corresponding to the location of the pixel locations of the image 600 a within the one-dimensional ‘frame’ of the image 600 a .
- the vertex space 600 b additionally includes a color-space dimension (labeled ‘Color,’ illustrated as the vertical axis; brighter pixel locations are located in higher locations within the vertex space 600 b ).
- the locations of the pixel locations of the image 600 a within the vertex space 600 b are indicated by crosses.
- a plurality of vertices are located in the vertex space 600 b .
- the locations of the vertices are regularly spaced in the vertex space 600 b in a square pattern and indicated by dots.
- the vertices within the vertex space 600 b could be described as being at the location of vertices of a regularly-stacked (or tessellated) pattern of regular squares (i.e., 2-dimensional hypercubes).
- the pixel locations of the image 600 a could be associated with vertices within the vertex space 600 b according to proximity or some other property of the relative location of the pixel locations and vertices within the vertex space 600 b .
- a given pixel location 610 b in the vertex space 600 b (corresponding to the given pixel location 610 a of the image 600 a ) could be associated with four vertices 620 that define an enclosing square 625 that encloses an area (or more generally, a space of dimension equal to the number of dimensions of the vertex space) of the vertex space that contains the given pixel location 610 b .
- the given pixel location 610 b could be equally associated with each of the four vertices 620 or could be associated in a weighted fashion according to some function or other consideration.
- the given pixel location 610 b could be associated with individual vertices of the four vertices 620 according to respective distances in the vertex space 600 b between the given pixel location 610 b and the individual vertices (e.g., according to some barycentric partition or other assignment of the given pixel location 610 b between the vertices of the four vertices 620 ).
- pixel locations within the enclosing square 625 could be associated with the four vertices 620 (though the weighting of the association between a first pixel location within 625 and the four vertices 620 could be different from the weighting of the association between a second pixel location within 625 and the four vertices 620 according, e.g., to a difference in the location within the vertex space 600 b of the first and second pixel locations).
- an example vertex 650 could be associated with pixel locations within a first example space 675 .
- the first example space 675 could be defined as the set of enclosing squares that include the first example vertex 650 as a vertex.
- the given pixel location 610 b could be associated with a closest vertex 630 of the plurality of vertices that is closest to the given pixel location.
- pixel locations in the vertex space that are within an enclosing space 635 could also be associated with the first vertex.
- the enclosing space 635 could be defined as the subset of space within the vertex space 600 b within which the closest vertex 630 is the closest vertex of the plurality of vertices.
- the example vertex 650 could be associated with pixel locations 660 within a second example space 665 .
- the second example space 665 could be defined as the subset of space within the vertex space 600 b within which the example vertex 650 is the closest vertex of the plurality of vertices.
- the location of the plurality of vertices in the vertex space 600 b is meant as a non-limiting example of a pattern of location of vertices in a vertex space.
- the vertices are located in the vertex space 600 b at the vertices of regular, regularly-tiled rectangles (i.e., squares or regular 2-dimensional hypercubes).
- the vertices could instead be located in some other regular or irregular pattern in the vertex space 600 b according to an application.
- the vertices could be located in the vertex space 600 b at the vertices of regularly-tiled irregular hyper-rectangles.
- the angle of the major axis of the tiling of the squares, rectangles, hypercubes, hyper-rectangles, or other stacked, tiles, or otherwise tessellated shapes could be parallel to one of the axes of one or more of the dimensions of the vertex space 600 b (e.g., parallel to both the ‘Pixel Location’ axis and the ‘Color’ axis, as illustrated in FIG. 6B ) or could have some other orientation.
- the vertices could have a first ‘effective resolution’ (i.e., a period of pattern repetition) in a first direction or directions in the vertex space 600 b (e.g., a first period of repetition in spatial dimension of a vertex space) and a second ‘effective resolution’ in a second direction or directions (e.g., a second period of repetition in a color-space dimension of the vertex space).
- a first ‘effective resolution’ i.e., a period of pattern repetition
- a second ‘effective resolution’ in a second direction or directions
- Other patterns, scales, and schemes of association with pixel locations of a plurality of vertices in a vertex space are anticipated.
- the one-dimensional images 500 a , 600 a and corresponding two-dimensional vertex spaces 500 b , 600 b as illustrated in FIGS. 5A, 6A, 5B, and 6B , respectively, are intended as illustrative examples of a method for associating pixel locations of an image (e.g., a two-dimensional image comprising a two-dimensional array of pixel locations) with vertices in a vertex space (e.g., a vertex space having a number of spatial dimensions equal to a number of dimensions of a corresponding image and one or more color-space dimensions corresponding to color-space information of pixel locations of the image).
- an image e.g., a two-dimensional image comprising a two-dimensional array of pixel locations
- vertices in a vertex space e.g., a vertex space having a number of spatial dimensions equal to a number of dimensions of a corresponding image and one or more color-space dimensions corresponding to color-space information of pixel
- pixel locations of a two-dimensional black-and-white (or greyscale) image could be associated with vertices in a three-dimensional vertex space having two spatial dimensions corresponding to the two dimensions (e.g., the horizontal and vertical directions) of the image and a third color-space dimension corresponding to the brightness or other greyscale information defined by the pixel locations of the image.
- pixel locations of a two-dimensional color (e.g., RGB, YUV, YP B P R , YC B C R ) image could be associated with vertices in a five-dimensional vertex space having two spatial dimensions corresponding to the two dimensions (e.g., the horizontal and vertical directions) of the image and three color-space dimensions corresponding to the color-space information (e.g., red-, green-, and blue-color intensities) defined by the pixel locations of the image.
- a two-dimensional color (e.g., RGB, YUV, YP B P R , YC B C R ) image could be associated with vertices in a five-dimensional vertex space having two spatial dimensions corresponding to the two dimensions (e.g., the horizontal and vertical directions) of the image and three color-space dimensions corresponding to the color-space information (e.g., red-, green-, and blue-color intensities) defined by the pixel locations of the image.
- color-space information e
- Additional or fewer spatial (e.g., corresponding to 1-dimensional line images or 3-dimensional volumetric or holographic images) and/or color-space (e.g., corresponding to hyperspectral color channels, corresponding to polarization information channels) dimensions of a vertex space are anticipated.
- the location of vertices within a vertex need not be regular, but could be specified according to any regular or irregular scheme according to an application.
- the vertices could be located according to the centroids of a Gaussian or other KD-tree, a set of vertices resulting from greedy consolidation of an initial regularly-spaced set of vertices, or according to some other spacing within the vertex space and/or partitioning of the vertex space.
- FIGS. 7A to 7E illustrate the association of pixel locations of an image with vertices in a vertex space to allow for filtering of the pixel locations of the image.
- FIG. 7A depicts an example one-dimensional image 700 a (i.e., a one-dimensional strip of pixel locations). Pixel locations of the image 700 a (including given pixel location 710 a ) define respective brightnesses (i.e., individual pixel locations of the image 700 a define colors within a one-dimensional (e.g., black-and-white) color-space).
- FIG. 7B depicts an example two-dimensional vertex space 700 b .
- the vertex space 700 b includes a spatial dimension (illustrated as the horizontal axis) corresponding to the location of the pixel locations of the image 700 a within the one-dimensional ‘frame’ of the image 700 a .
- the vertex space 700 b additionally includes a color-space dimension (illustrated as the vertical axis).
- the locations of the pixel locations of the image 700 a within the vertex space 700 b are indicated by crosses.
- a plurality of vertices are located in the vertex space 700 b .
- the locations of the vertices are regularly spaced in the vertex space 700 b in a triangular pattern and indicated by dots.
- Given pixel location 710 b in the vertex space 700 b (corresponding to the given pixel location 710 a of the image 700 a ) could be associated with three vertices 720 b that define an enclosing triangle that encloses an area of the vertex space 700 b that contains the given pixel location 710 b .
- the given pixel location 710 b could be associated with the three vertices 720 b in a weighted fashion according to some function or other consideration.
- individual vertices of the plurality of vertices in the vertex space 700 b are associated with zero or more pixel locations of the image 700 a such that information about an individual vertex is related to corresponding information about the zero or more pixel locations associated with the individual vertex.
- a vertex brightness of an individual vertex could be a weighted average of the individual brightnesses of pixel locations associated with the individual vertex.
- Information about vertices that are not associated with any pixel locations i.e., that are associated with zero pixel locations) could be set to zero or not calculated, considered, or otherwise used in subsequent calculations, or handled in some other way according to an application.
- FIG. 7C illustrates the filtering 700 c of the vertex information of the vertices illustrated in FIG. 7B by a full blurring kernel 750 .
- the application of the full blurring kernel 750 could be mathematically equivalent or nearly equivalent in some way to the sequential application of three sparse blurring kernels 741 , 743 , 745 .
- sequential application of the three sparse blurring kernels 741 , 743 , 745 could allow for faster filtering 700 c of the vertex information by a computing system (e.g., by a DSP, GPU, or other specialized processor) than direct application of the full blurring kernel 750 .
- FIG. 7D depicts a two-dimensional output vertex space 700 d .
- the output vertex space is configured similarly to the vertex space 700 b .
- the location of the pixel locations of an output image 700 e (illustrated in FIG. 7E ) within the output vertex space 700 d are indicated by crosses.
- the locations of output vertices within the output vertex space 700 d are indicated by dots and are located in substantially the same locations in the vertex space 700 d as the location of the vertices in the vertex space 700 b .
- associations between pixel locations of the output image 700 e and output vertices in the output vertex space 700 d are substantially the same as between pixel locations of the input image 700 a and vertices in the vertex space 700 b .
- the brightness of a given output pixel location 710 d in the output vertex space (corresponding to the determined brightness of given output pixel location 710 e of the output image 700 e ) could be determined as a weighted combination of determined brightnesses of three output vertices 720 d associated with the given output pixel location 710 d.
- pixel locations of an image can be associated with a small set of one or more vertices of a plurality of vertices in a vertex space.
- Such an association could be according to a variety of schemes (e.g., nearest vertex, nearest n vertices, vertices defining an enclosing space, a weighted association with a determined set of vertices) according to a variety of metrics (e.g., a distance within a vertex space wherein the dimensions of the vertex space are scaled according to some application, e.g., a first scaling of spatial dimensions of the vertex space and a second scaling of color-space dimensions of the vertex space).
- metrics e.g., a distance within a vertex space wherein the dimensions of the vertex space are scaled according to some application, e.g., a first scaling of spatial dimensions of the vertex space and a second scaling of color-space dimensions of the vertex space.
- Such associations could be as described herein
- pixel locations of an image and/or information thereof could be based on a weighted combination of corresponding associated vertices and/or information thereof (e.g., vertex brightnesses, determined vertex disparity values).
- d is a vector of determined pixel location disparity values
- ⁇ is a vector of determined vertex disparity values
- S T is a matrix whose elements describe the weighted association between individual vertices and individual pixel locations.
- a particular row of S T corresponding to a respective particular pixel location is zero for all columns corresponding to vertices that are not associated with the particular pixel location, and a nonzero value related to the weight of a corresponding vertex in determining the disparity value of the particular pixel location relative to the determined vertex disparity value of other vertices (i.e., nonzero values in other corresponding columns).
- S T could be a full matrix (i.e., could have substantially no nonzero entries) if individual pixel locations of an image are, to some nonzero degree, related to all of the vertices in a vertex space.
- nonzero elements of a particular row of S T could correspond to vertices associated with the pixel location corresponding to the particular row, and the relative value of the nonzero elements could correspond to the relative weighting of the relationship of the particular pixel location with the associated vertices (e.g., according to a relative distance between the particular pixel location and the vertices, e.g., according to some barycentric weighting).
- Determination of a plurality of disparity values for respective pixel locations of a first image based on information in the first image and a second image could include determining respective disparity loss functions (as described elsewhere herein, or according to some other method and/or application) of the pixel locations.
- the determined disparity loss functions for a given pixel location of the first image could indicate variation in color-space similarity (e.g., a similarity of brightness, a similarity of color, a similarity of a red-, green-, or blue-color intensity) between the given pixel location of the first image and a corresponding range of pixel locations of the second image spanning a range of distances (e.g., spanning a range of disparities) from the given pixel location in the first image within the frame of the second image.
- the determined disparity loss functions could include polynomial functions, parabolic functions, piecewise-defined functions, piecewise linear functions, hinge functions, or some other functions or combinations of functions.
- Individual pixel locations of a first image could be associated with one or more vertices in a vertex space according to one or more schemes or methods described herein and/or according to some other method and/or application.
- an individual pixel location could be associated with a nearest vertex, a set of nearest vertices, a set of vertices defining an enclosing shape, or according to some other partition criterion.
- configuration of the vertex space e.g., a number of spatial and/or color-space dimensions, and correspondence thereof with spatial, color-space, or other properties of pixel locations of the first image
- arrangement of the vertices within the vertex space e.g., at the vertices of a regular tessellation or other tiling of the vertex space by a plurality of regular or irregular multidimensional shapes (e.g., squares, rectangles, cubes, hypercubes, hyper-rectangles, triangles, tetrahedral, simplexes)
- regular or irregular multidimensional shapes e.g., squares, rectangles, cubes, hypercubes, hyper-rectangles, triangles, tetrahedral, simplexes
- a vertex-disparity loss function For each vertex in a vertex space, a vertex-disparity loss function could be determined based on a combination of zero or more disparity loss functions corresponding to respective zero or more pixel locations associated with the vertex.
- the determined vertex-disparity loss function could be a sum, an average, a weighted average, or some other function or combination of the zero or more disparity loss functions of the associated respective pixel locations.
- the vertex-disparity loss function for vertices in a vertex space could be mathematically defined as:
- g j (d) is the vertex-disparity loss function for the j th vertex of a set of vertices in a vertex space
- f i (d) is the disparity loss function of the i th pixel location of a set of pixel locations of a first image
- d is a given input disparity to the vertex/pixel location disparity loss functions
- S j is a row of an association matrix describing the association between the j th vertex and pixel locations of the first image.
- i represents the indices of S j that are nonzero
- w represents the values of those nonzero elements of S j .
- output values of f i be calculated for a range of individual values of d and the resulting outputs be individually used to determine corresponding output values of g i for respective individual values of d
- output values of g j for respective individual values of d could be determined implicitly, based on the structure of the function(s) used to determine the f i .
- the f i could be hinge loss functions (e.g., functions similar to 430 as illustrated in FIG. 4B and parameterized individually by respective upper and a lower hinge disparity values) and the g j could be efficiently determined based on the upper and lower hinge disparity values of hinge loss functions corresponding to pixel locations associated with individual vertices.
- Such determined vertex-disparity loss functions could be applied in a variety of ways to determine respective vertex disparity values for each of the plurality of vertices in the vertex space.
- the disparity values of individual vertices could be determined by performing an optimization using the determined vertex-disparity loss functions, e.g., by performing a gradient descent using a cost function that is based on the vertex-disparity loss functions (e.g., that comprises a weighted sum of the determined vertex-disparity loss functions of the vertices in the vertex space).
- the vertex disparity values of individual vertices in the vertex space could be determined independently of the determination of disparity values for other vertices in the vertex space.
- the vertex disparity values of individual vertices in the vertex space could be determined in consideration of determined vertex disparity values of other vertices in the vertex space. For example, a cost function that includes a vertex loss term related to the determined vertex-disparity loss functions of the vertices and that includes a vertex smoothness term related to the similarity between individual determined vertex disparity values (e.g., determined vertex disparity values of neighboring vertices within the vertex space) could be minimized (e.g., by gradient descent, dynamic programming, or some other method) to determine vertex disparity values for vertices in the vertex space.
- a cost function that includes a vertex loss term related to the determined vertex-disparity loss functions of the vertices and that includes a vertex smoothness term related to the similarity between individual determined vertex disparity values (e.g., determined vertex disparity values of neighboring vertices within the vertex space) could be minimized (e.g., by gradient
- f j ( ⁇ j ) is the vertex-disparity loss function for the j th vertex of the plurality of vertices in the vertex space, given an input vertex disparity of ⁇ j (i.e., the j th element of ⁇ ).
- C i ⁇ C d B C d is an affinity matrix whose elements (e.g., whose [i,j] th element) are related to a specified smoothness cost for similarity between determined vertex disparity values pairs of vertices in the plurality of vertices (e.g., for the i th and j th vertices of the plurality of vertices in the vertex space).
- ⁇ is a weighting factor that can be specified to set a relative importance of the smoothness term and the loss term in determining the overall cost of a given set of determined vertex disparity values ⁇ (wherein the j th element of ⁇ is ⁇ j , the determined vertex disparity value for the j th vertex of the plurality of vertices).
- the elements of (C i ⁇ C d B C d ) could be chosen such that the smoothness cost for similarity between determined vertex disparity values are related to a specified smoothness cost between determined disparity values of corresponding pixel locations of a first image.
- the smoothness cost for similarity between determined vertex disparity values could be related to an exponential or other function of the distance, within the frame of the first image, between the i th and j th pixel locations of the first image, as described above.
- the diagonal matrix D could be determined by a variety of methods (e.g., the Sinkhorn method, or by some other method or methods).
- Alternate smoothness terms could be used to determine vertex disparity values for the plurality of vertices in the vertex space.
- the alternate smoothness terms could be related to the location of the vertices in the vertex space, a number of pixel locations associated with individual vertices in the vertex space (e.g., to a ‘mass’ of the vertices that is related to the number of pixel locations that an individual vertex is associated with), an aggregation, distance to other pixel locations, or other properties of the location within the vertex space of pixel locations associated with individual vertices, or some other factor or combination of factors.
- the number of vertices in the plurality of vertices in the vertex space could be based on one or more properties of an image (e.g., a first image of a stereo image pair). Further, the location of the plurality of vertices within the vertex space could be specified and/or determined based on the image. The locations of the vertices in the vertex space could span a volume in the vertex space that is spanned by the pixel locations of a first (i.e., reference) image.
- the vertex space could be three-dimensional (e.g., the first image could be black-and-white) and the vertices could span a volume in the two spatial dimensions of the vertex space corresponding to the extent of the frame of the first image and in the one-dimensional color-space dimension corresponding to the range of brightness values represented by the pixel locations of the first image.
- the locations of the vertices in the vertex space could span a volume in the vertex space that is spanned by the pixel locations of the image (e.g., three-dimensional vertex space could include vertices that span a volume in two spatial dimensions of the vertex space corresponding to the extent of the frame of the image and in a one-dimensional color-space dimension corresponding to the range of brightness values represented by the pixel locations of the image).
- the number of vertices in the plurality of vertices could be determined independently of an image or images for which disparity information is determined as described herein. For example, a number of vertices could be specified for the performance of the methods described herein for images and/or image pairs having a range of resolutions and/or numbers of pixel locations. The specified number of vertices could be small (e.g., less than 1%) of the number of pixel locations of images used to perform the methods described herein.
- the time required to perform the methods described herein to determine disparity information for pixel locations of an image could be related to a vertex-disparity-value-determination step that is slow (relative, e.g., to steps to determine disparity loss functions for pixel locations of the image, to associate individual pixel locations with one or more vertices, and/or to determine pixel location disparity values based on determined vertex disparity values) and that has a time to perform that is further related to the specified number of vertices.
- first image having a first resolution e.g., a first number of pixels
- second resolution e.g., a second number of pixels
- the methods described herein could be performed only for vertices of a plurality of vertices in a vertex space that are associated with at least one pixel location of an image (e.g., of a first image of a stereo image pair). That is, a set of vertices within a plurality of vertices could be associated with no pixel locations of the image, and thus methods and/or steps described herein (e.g., the determination of a vertex-disparity loss function, the determination of a vertex disparity value) could be performed only for vertices that are not members of that set (i.e., the methods and/or steps could be performed only for vertices that are associated with one or more pixel locations of the image).
- This distinction could include using sparse methods for matrix multiplication or for other matrix and/or vector operations, or could include the use of additional or alternate methods according to an application.
- FIG. 8 is a flowchart illustrating a method 800 , according to an example embodiment.
- Method 800 may be implemented by a device that includes two or more cameras oriented in the same direction, where each camera is separated from the other cameras by some distance, such as the devices shown in FIGS. 1A and 1B and FIGS. 9A to 9E or some other device (including by not limited to camera devices, mobile devices, portable devices, cellphones, laptops, tablet computers, biometric identification systems, human interface devices, robots, drones, self-driving cars, and autonomous vehicles).
- method 800 may be implemented by a device or system that is separate from and configured to communicate with such a multi-camera device and/or to access images (e.g., stereo image pairs) that are stored in a memory (e.g., a memory card, a compact disc, a hard drive, a cloud storage service, a memory of a remote server (e.g., a server in communication with the internet)).
- a memory e.g., a memory card, a compact disc, a hard drive, a cloud storage service, a memory of a remote server (e.g., a server in communication with the internet)
- method 800 may be implemented to determine disparity information for pixel locations of a first image based on information in the first image and in a second image.
- the method 800 includes receiving a first image of a scene and a second image of the scene ( 810 ). This can include operating first and second cameras to capture respective first and second images of the scene. Additionally or alternatively, the first and second images could be captured by a single camera at two different points in time, where the location of the camera is moved from a first perspective to a second perspective between the first and second periods of time.
- Receiving first and second images of a scene ( 810 ) can include accessing first and second images from a memory, e.g., from a memory card, a hard drive, a compact disk, or from a remote system (e.g., from a remote server that is accessible over the internet).
- receiving first and second images of a scene ( 810 ) can include accessing and/or generating images that are not captured images of a real-world scene. For example, it could include accessing first and second images that were generated by a simulation, computer graphics, or other computational process. Other methods or combinations of methods for generating, capturing, filtering, modifying, or otherwise receiving first and second images of a scene ( 810 ) are anticipated.
- the method 800 additionally includes determining a disparity loss function for each of a plurality of pixel locations in the first image 820 .
- a determined disparity loss function for a given pixel location of the first image could take as an input a disparity value and could be related to a degree of similarity between the given pixel location (e.g. a similarity of color, brightness, or some other pixel and/or pixel location property) and a pixel location of a second image that is displaced from the first pixel location, within the second image, by a distance equal to the input disparity value.
- the disparity loss function for a given pixel location could be defined for a range of pixels locations in the second image; for example, the disparity loss function for a given pixel location could be defined for a range of distances between zero (i.e., comparison between the given pixel location and a pixel location of the second image having the same location as the given pixel location, within the frame of the second image) and some defined upper bound (e.g., a distance between the given pixel location and the edge of the second image, such that the disparity loss function describes the similarity between the given pixel location and a set of pixels locations of the second image at or to one direction (e.g., the right) of the given pixel location, in the second image)).
- zero i.e., comparison between the given pixel location and a pixel location of the second image having the same location as the given pixel location, within the frame of the second image
- some defined upper bound e.g., a distance between the given pixel location and the edge of the second image, such that
- the determined disparity loss functions for each pixel location in the first image could include hinge loss functions. Determining a disparity loss function for each pixel location in the first image 830 could include additional or alternate processes, functions, functionals, determinations, or other aspects as described herein.
- the method 800 additionally includes associating each of the plurality of pixel locations with one or more of a plurality of vertices in a vertex space that comprises two spatial dimensions and at least one color-space dimension 830 .
- Associating each pixel location with one or more vertices 830 could be based on a proximity or some other relationship within the vertex space between the pixel locations and the vertices (e.g., a given pixel location could be associated with the nearest vertex within the vertex space, or with a set of vertices (e.g., a set of vertices defining the vertices of a shape in the vertex space that encloses the given pixel location)).
- the vertex space could have spatial dimensions (e.g., dimensions corresponding to horizontal and vertical dimensions of pixel locations within the first image) and one or more color-space dimensions (e.g., dimensions corresponding to the brightness, color, red, green, or blue intensity, or other properties of the pixel locations within a color-space).
- spatial dimensions e.g., dimensions corresponding to horizontal and vertical dimensions of pixel locations within the first image
- color-space dimensions e.g., dimensions corresponding to the brightness, color, red, green, or blue intensity, or other properties of the pixel locations within a color-space.
- the first image could be a color image (e.g., could be an image whose pixels and/or pixel locations define colors in a color-space, e.g., a red-green-blue (RGB color space)) and the vertex space could have two spatial dimensions corresponding to the pixel locations within the frame of the image(s) and a further three color-space dimensions corresponding to the location, in the color-space, of colors defined by the plurality of pixel locations of the first image.
- a color image e.g., could be an image whose pixels and/or pixel locations define colors in a color-space, e.g., a red-green-blue (RGB color space)
- RGB color space red-green-blue
- Higher- or lower-dimensional vertex spaces could be used for images whose pixels and/or pixel locations define more or less color information (e.g., whose pixels define colors in a two-dimensional color-space (e.g., a red-green color-space) or a more than three-dimensional color-space (e.g., a color-space defined to represent the output of detectors of a hyperspectral imager)) or other information (e.g., the direction and/or degree of polarization of light in the image).
- color information e.g., whose pixels define colors in a two-dimensional color-space (e.g., a red-green color-space) or a more than three-dimensional color-space (e.g., a color-space defined to represent the output of detectors of a hyperspectral imager)
- other information e.g., the direction and/or degree of polarization of light in the image.
- the locations of the vertices in the vertex space could span a volume in the vertex space that is spanned by the pixel locations of the first image.
- the vertex space could be three-dimensional (e.g., the first image could be black-and-white) and the vertices could span a volume in the two spatial dimensions of the vertex space corresponding to the extent of the frame of the first image and in the one-dimensional color-space dimension corresponding to the range of brightness values represented by the pixels and/or pixel locations of the first image.
- the locations of the vertices in the vertex space could be regularly spaced (e.g., to provide a uniform ‘sampling’ of the volume of the vertex space corresponding to the first image).
- the vertices could be located in the vertex space at the vertices of a tessellated and/or regularly repeating array of three- or more-dimensional shapes or according to some other regular pattern.
- the vertices could be located in a rectangular array (i.e., regularly spaced along orthogonal directions in the multidimensional space, at the vertices of a tessellated array of hyper-rectangles).
- the vertices could be located in a triangular array (i.e., at the vertices of triangles, tetrahedra, or higher-dimensional simplexes that are regularly tessellated in the vertex space).
- the method 800 additionally includes determining a vertex-disparity loss function for each vertex in the vertex space based on the disparity loss functions corresponding to the associated pixel locations from the first image 840 .
- a vertex-disparity loss function could be determined based on a combination of zero or more disparity loss functions corresponding to respective zero or more pixel locations associated with the vertex.
- the determined vertex-disparity loss function could be a sum, an average, a weighted average, or some other function or combination of the zero or more disparity loss functions of the associated respective pixel locations.
- vertex-disparity loss functions for vertices that are not associated with any pixel locations could be set to zero or not calculated, not considered, or otherwise not used in subsequent calculations, or handled in some other way according to an application.
- the method 800 additionally includes applying the determined vertex-disparity loss functions to determine a respective disparity value for each of the plurality of vertices 850 .
- applying the determined vertex-disparity loss functions to determine respective disparity values 850 could include performing an optimization using the determined vertex-disparity loss functions, e.g., by performing a gradient descent using a cost function that is based on the vertex-disparity loss functions (e.g., that comprises a weighted sum of the determined vertex-disparity loss functions of the vertices in the vertex space).
- the vertex disparity values of individual vertices in the vertex space could be determined independently of the determination of disparity values for other vertices in the vertex space.
- the vertex disparity values of individual vertices in the vertex space could be determined in consideration of determined vertex disparity values of other vertices in the vertex space. For example, a cost function that includes a loss term related to the determined vertex-disparity loss functions of the vertices and that includes a smoothness term related to the similarity between individual determined vertex disparity values (e.g., determined vertex disparity values of neighboring vertices within the vertex space) could be minimized (e.g., by gradient descent, dynamic programming, or some other method) to determine vertex disparity values for vertices in the vertex space.
- a cost function that includes a loss term related to the determined vertex-disparity loss functions of the vertices and that includes a smoothness term related to the similarity between individual determined vertex disparity values (e.g., determined vertex disparity values of neighboring vertices within the vertex space) could be minimized (e.g., by gradient descent, dynamic programming, or some other
- the method 800 additionally includes using the determined disparity value of each of one or more vertices that are associated with each of the pixel locations in the first image as a basis to determine a disparity value for the pixel location in the first image 860 .
- a disparity value for a given pixel location in the first image could be determined based on a weighted combination of determined vertex disparity values of one or more vertices that are associated with the given pixel location.
- the method 800 could include additional steps or elements in addition to those listed above (e.g., 810 , 820 , 830 , 840 , 850 , 860 ).
- the method 800 could include determining depth information (e.g., pixel locations depth values for each pixel location in the first image) from the determined disparity values for the pixel locations in the first image.
- the disparity information may be utilized to generate a depth map of the scene.
- the depth map may then be used help determine focus settings for the scene, to determine the location of objects in the scene, to map the scene, to navigate the scene (e.g., to operate a robot or other autonomous device to move through the scene) and/or may be used for other purposes.
- the method 800 could include performing image processing on the first image based on determined disparity values for pixel locations of the first image. For example, an image processing process (e.g., a blur, a filtering, an increase or decrease in contrast, a change in a color balance, an application of a color filter) could be performed selectively on regions of the first image corresponding to objects (i.e., regions of pixel locations within the first image) that have determined disparity values within a specified range of values.
- an image processing process e.g., a blur, a filtering, an increase or decrease in contrast, a change in a color balance, an application of a color filter
- the method 800 could include other applications or processes based on determined pixel location disparity information.
- Applications of information e.g., determined pixel location and/or vertex disparity values
- Applications of information include but are not limited to augmented reality (e.g., providing depth cues, assisting in navigation), human interfaces (e.g., determining the location of parts of a user's body and using the determined locations to control the operation of a computer or other system), logistics (e.g., determining the location and size of boxes, packages, or other objects in a transport vehicle, container, loading zone, warehouse or other environment), biometrics (e.g., determining a depth-based mapping of a person's face or other body part and comparing the determined mapping to a database of stored mappings corresponding to identified users), and determining the location of a device (e.g., matching an image-determined mapping to a region of a known environment).
- Other additional and/or alternative elements and/or applications of method 800 are anticipated
- the scene described in relation to the method 800 above could include an outdoor environment (e.g., a path, a roadway, an airspace, a waterway, a field) and a plurality of objects (e.g., rocks, traffic control systems, human beings, animals, plants, obstacles, autonomous vehicles).
- the scene could include a human being, and determined disparity, depth, or other information determined from at least two images (e.g., for first and second images of a stereo image pair) of the scene could be used to receive input from the human being (e.g., to provide a human-computer interface), to determine and/or confirm the identity of the human being (e.g., to provide a biometric functionality), or to allow for some other application(s).
- Other scenes, environments, scene contents, and applications of determined disparity or other information determined from two or more images of such scenes are anticipated.
- the physical components of an image capture device or other embodiments that might utilize image data may include an aperture through which light enters and a recording surface for capturing the image represented by the light.
- image capture device could also include a lens positioned in front of the aperture to focus at least part of the image on the recording surface.
- the aperture may be fixed size or adjustable.
- the recording surface may be photographic film.
- the recording surface may include an electronic image sensor (e.g., a charge coupled device (CCD), an array of active pixel sensors (APS), an array of complementary metal-oxide-semiconductor (CMOS) sensors, or some other array or other arrangement of electronic photodetectors) to capture and/or store captured images in a data storage unit (e.g., memory).
- an electronic image sensor e.g., a charge coupled device (CCD), an array of active pixel sensors (APS), an array of complementary metal-oxide-semiconductor (CMOS) sensors, or some other array or other arrangement of electronic photodetectors
- a data storage unit e.g., memory
- a shutter may be coupled to or nearby the aperture, lens, or recording surface.
- the shutter may either be in a closed position, in which it blocks light from reaching the recording surface, or an open position, in which light is allowed to reach to recording surface.
- the position of the shutter may be controlled by a shutter button. For instance, the shutter may be in the closed position by default.
- the shutter button is triggered (e.g., pressed)
- the shutter may change from the closed position to the open position for a period of time, known as the shutter cycle.
- an image may be captured on the recording surface.
- the shutter may change back to the closed position.
- the shuttering process may be electronic.
- the sensor may be reset to remove any residual signal in its light-sensitive elements. While the electronic shutter remains “open,” the light-sensitive elements may accumulate charge. When or after the shutter “closes,” these charges may be transferred to longer-term data storage.
- An electronic shuttering process could include operating one or more active pixel sensors or other photodetectors to measure a property of image light received by the photodetectors during a specified period of time (i.e., the specified period of time comprises the “opening” of the electronic shutter). Combinations of mechanical and electronic shuttering may also be possible.
- a shutter may be activated and/or controlled by something other than a shutter button.
- the shutter may be activated by a softkey, a timer, or some other trigger.
- a shutter could be operated a plurality of times at a specified rate (e.g., a frame rate) in order to capture a series of images that could comprise a video recording.
- image capture may refer to any mechanical and/or electronic shuttering process that results in one or more images being captured, regardless of how the shuttering process is triggered or controlled.
- the lens could be mechanically or otherwise actuated to enable control of a focus (e.g., an effective focal length) of a camera.
- control of the focus could be manual (e.g., by manipulation by a user of mechanisms configured to actuate the lens).
- an electronic or other system could be configured to control the focus of the camera automatically.
- an autofocus (AF) system may include a sensor of some kind, a control system that automatically determines focus settings, and an actuator to adjust the mechanical components of the camera (e.g., the lens) according to the focus settings.
- the data provided by the sensor may be used to evaluate the manner in which the environment is or will be recorded by an image sensor, and to responsively control an electro-mechanical system that can change the focus of the camera (e.g., by using an actuator to move components of the lens and/or changing the size of the aperture).
- an image capture device e.g., by digital camera devices 900 , 920 , 940 , 960 , 980 ).
- Typical passive autofocus techniques include “phase detection” autofocus (PD-AF) and “contrast detection” autofocus (CD-AF), which may also be referred to as “contrast measurement” autofocus.
- Passive autofocus processes typically involve a computing system (e.g., a processor) operating a mechanical lens system to adjust the focus settings of the lens (e.g., to change the focusing distance or depth of field), and then analyzing whether or not the resulting image (or subsection of such an image) is in focus. If the resulting image is not satisfactorily in focus, then the computing system again adjusts the focus settings and evaluates the focusing characteristics in the resulting image.
- each adjustment to the focus settings may be determined based on some measure of how out of focus the image is (or how out of focus a particular portion of the image is). In other implementations, the adjustments may be predetermined. In either case, this process may be repeated until the resulting image is deemed to be satisfactorily in focus.
- the device may utilize a camera system that does not include a separate autofocus system.
- a camera system that does not include a separate autofocus system.
- many mobile phones, tablet computers, and/or other mobile devices which often include camera systems that use the same image sensor for both autofocus and image capture.
- cameras in portable devices such as mobile phones, tablet computers, and other mobile devices use CD-AF for purposes of focusing.
- CD-AF systems can use a separate sensor that is dedicated to autofocus, most CD-AF systems use the same image sensor for both image capture and autofocus sensing.
- CD-AF systems determine whether or not a subject is in focus by measuring the contrast in the image (of a subsection of such an image) that is detected on the sensor. To do so, a CD-AF system may evaluate the change in contrast at various points in the image, with higher contrast being interpreted as an indication of a sharper image.
- the difference in intensity between adjacent pixels of a sensor is generally greater when the subject captured in the adjacent pixels is in focus, as compared to when image subject is out of focus.
- a CD-AF system may measure the contrast at specific pixels, or determine the average over certain groups of pixels. In either case, a CD-AF system may then adjust focus settings until a threshold contrast is detected (and possibly until a maximum contrast is detected). For example, an illustrative CD-AF system may pass image data through a high pass filter, and adjusts the focus of the lens until the output from the filter exceeds a threshold (and possibly until the output of the filter is at its highest level).
- FIG. 9A illustrates the form factor of a mobile device 900 .
- Mobile device 900 may be, for example, a mobile phone, a tablet computer, a wearable computing device, or some other portable and/or battery-powered device. However, other embodiments are possible.
- Mobile device 900 may include various elements, such as a body 902 , a front-facing camera 904 , a multi-element display 906 , a shutter button 908 , and other buttons 910 .
- Digital camera device 900 could further include three rear-facing cameras 912 A to 912 C.
- Front-facing camera 904 may be positioned on a side of body 902 typically facing a user while in operation, or on the same side as the multi-element display 906 .
- Rear-facing cameras 912 A to 912 C may be positioned on a side of body 902 opposite front-facing camera 904 . Referring to the cameras as front and rear facing is arbitrary, and digital camera device 900 may include multiple cameras positioned on various sides of body 902 .
- the lenses of rear-facing cameras 912 A and 912 B are arranged on the upper corner on the back of digital camera device 900 , while the lens of camera 912 C is arranged on the lower corner on the back of device 900 , opposite of camera 912 A. Further, the lenses of rear-facing cameras 912 to 912 C are oriented in substantially the same direction. (Note that herein, references to a cameras being oriented in the same direction should be understood to mean that the lenses of the cameras point in substantially the same direction.)
- the lenses of three or more cameras which are all oriented in substantially the same direction, may be arranged in different formations on a surface of the phone.
- the lenses of three or more cameras which are all oriented in substantially the same direction, may be arranged in different formations on a surface of the phone.
- FIGS. 9B to 9E several other multi-camera arrangements are described herein with respect to FIGS. 9B to 9E .
- FIG. 9B shows a mobile device 920 with an arrangement of four cameras 922 A to 922 D oriented in the same direction, including two cameras 922 A and 922 B in the upper corner of the mobile device (similar to FIG. 9A ), and two additional cameras 922 C and 922 D that are located at the lower corners of the mobile device 920 .
- FIG. 9C shows another arrangement with four cameras oriented in the same direction. In particular, the arrangement in FIG. 9C includes one camera 944 A to 944 D in each corner of the device 940 .
- FIG. 9D shows an arrangement with six cameras 966 A to 966 F facing in the same direction.
- the six cameras 966 A to 966 F are placed on the back of the mobile device 960 in less-structured, “organic,” arrangement.
- an arrangement with three or more cameras may provide multiple baselines between different pairs of cameras.
- a six-camera arrangement such as that shown in FIG. 9D may provide up to 15 different baselines (illustrated in FIG. 9D as dashed lines) for, e.g., stereoscopic imaging.
- an arrangement of n cameras that are oriented in substantially the same direction may provide up to C(n, k) baselines.
- FIG. 9E shows an arrangement with nine cameras 982 A to 9821 facing in the same direction.
- the nine cameras 982 A to 982 F are placed on the back of the mobile device 980 along two axes 984 X and 984 Y. Accordingly, different pairs of cameras may be selected from cameras 982 A to 982 F to provide a number of different baselines along the two axes 984 X and 984 Y, as well as a number of different baselines that are not on along the two axes 984 X, 984 Y.
- multi-camera arrangements may include more or fewer cameras than those shown in FIGS. 9A to 9E .
- the lenses in a multi-camera arrangement may be oriented at a different angle with respect to the surface on which the lenses are arranged.
- multi-camera arrangements may be implemented on other sides of a mobile or other device. Other variations on the multi-camera arrangements shown in the figures are also possible.
- Multi-element display 906 could represent a cathode ray tube (CRT) display, a light emitting diode (LED) display, a liquid crystal (LCD) display, a plasma display, or any other type of display known in the art.
- multi-element display 906 may display a digital representation of the current image being captured by front-facing camera 904 and/or one, some, or all of rear-facing cameras 912 A to 912 C, or an image that could be captured or was recently captured by any one of or any combination of these cameras.
- multi-element display 906 may serve as a viewfinder for one or more cameras.
- Multi-element display 906 may also support touchscreen and/or presence-sensitive functions that may be able to adjust the settings and/or configuration of any aspect of mobile device 900 .
- Front-facing camera 904 may include an image sensor and associated optical elements such as lenses. Front-facing camera 904 may offer zoom capabilities or could have a fixed focal length. In other embodiments, interchangeable lenses could be used with front-facing camera 904 . Front-facing camera 904 may have a variable mechanical aperture and a mechanical and/or electronic shutter. Front-facing camera 904 also could be configured to capture still images, video images, or both. Further, front-facing camera 904 could represent a monoscopic, stereoscopic, or multiscopic camera. Rear-facing cameras 912 A to 912 C may be similarly or differently arranged and/or configured. Additionally, front-facing camera 904 , each rear-facing camera 912 A to 912 C, or any combination of these cameras, may in fact be an array of one or more cameras (or an array of lenses that direct light onto a common image sensor).
- any one or any combination of front facing camera 904 and rear-facing cameras 912 A to 912 C may include or be associated with an illumination component that provides a light field to illuminate a target object or scene.
- an illumination component could provide flash or constant illumination of the target object or scene.
- An illumination component could also be configured to provide a light field that includes one or more of structured light, polarized light, and light with specific spectral content.
- Other types of light fields known and used to recover three-dimensional (3D) models from an object or environment are possible within the context of the embodiments herein.
- any one or any combination of front facing camera 904 and rear-facing cameras 912 A to 912 C may include or be associated with an ambient light sensor that may continuously or from time to time determine the ambient brightness of a scene that the camera can capture.
- the ambient light sensor can be used to adjust the display brightness of a screen associated with the camera (e.g., a viewfinder). When the determined ambient brightness is high, the brightness level of the screen may be increased to make the screen easier to view. When the determined ambient brightness is low, the brightness level of the screen may be decreased, also to make the screen easier to view as well as to potentially save power. Additionally, the ambient light sensor's input may be used to determine an exposure setting of an associated camera, or to help in this determination.
- Mobile device 900 could be configured to use multi-element display 906 and either front-facing camera 904 or one, some, or all of rear-facing cameras 912 A to 912 C to capture images of a target object or scene.
- the captured images could be a plurality of still images or a video stream.
- the image capture could be triggered by activating shutter button 908 , pressing a softkey on multi-element display 906 , or by some other mechanism.
- the images could be captured automatically at a specific time interval, for example, upon pressing shutter button 908 , upon appropriate lighting conditions of the target object, upon moving digital camera device 900 a predetermined distance, upon adjusting a focus of one or more cameras of the mobile device 900 , or according to a predetermined capture schedule.
- a controller of a device could utilize images generated by two or more cameras, or by other image capture devices, to generate a disparity map for one or both of the pair of images.
- This disparity map could be generated by the methods described herein or by some other methods.
- vertex-disparity loss functions could be determined for a plurality of vertices in a vertex space, based on a plurality of disparity loss functions of pixel locations in one or both of a pair of images.
- the determined vertex-disparity loss functions could be applied to determine respective vertex disparity values for the plurality of vertices and the determined vertex disparity values of one or more vertices associated with a pixel location of an image could be used to determine a disparity value for the pixel location.
- the controller could then utilize such pixel location disparity information to perform some application. For example, the controller could construct a depth map for an image, related to the distance between cameras that captured the image (e.g., cameras of a mobile device) and objects in a scene imaged by the cameras, based on the determined disparity information and information about the configuration of the cameras (e.g., a baseline separation distance between the cameras, a focal length of the cameras).
- the determination of disparity values from images captured by a pair of cameras i.e., from stereo image pairs captured by, e.g., cameras of the mobile device 900 ) or generated by some other means (e.g., image processing, virtual reality, computer-generated images, augmented reality) and/or determination of other information based on such determined disparity values could be performed by one or more processors (e.g., by one or more digital signal processors (DSPs), graphics processing units (GPUs), central processing units (CPUs), floating point units (FPUs), application-specific integrated circuits (ASICs), some other type of processors, and/or some combination of one or more types of processors).
- DSPs digital signal processors
- GPUs graphics processing units
- CPUs central processing units
- FPUs floating point units
- ASICs application-specific integrated circuits
- certain operations related to determination of disparity values could be performed by a first processor or type of processor (e.g., DSPs, GPUs) and other certain operations related to determination of disparity values (e.g., step size optimization during a gradient descent operation to minimize a cost function) could be performed by a second processor or type of processor (e.g., a general-purpose CPU).
- a first processor or type of processor e.g., DSPs, GPUs
- other certain operations related to determination of disparity values e.g., step size optimization during a gradient descent operation to minimize a cost function
- a second processor or type of processor e.g., a general-purpose CPU
- the processor(s) could be disposed in a device containing cameras used to generate the captured images (e.g., the cameras and processor(s) could be disposed in a mobile device, e.g., 900 , 920 , 940 , 960 , 980 ).
- a stereo image pair captured by a pair of cameras e.g., by cameras of the mobile device 900 , of a robot, of a drone, or of some other apparatus
- a processor e.g., a processor of a mobile device
- This disparity map could be generated by the methods described herein or by some other methods.
- a device including two or more cameras and a controller could be in communication with a remote system (e.g., a server) and the controller could operate the cameras to generate a stereo image pair of a scene (e.g., of an environment of the device).
- the controller could then operate to transmit the stereo image pair to the remote system, and the remote system could responsively determine disparity information for the stereo image pair (e.g., using the methods described herein, or other methods).
- the disparity information determined by the remote system could then be used for a variety of applications. In some examples, the determined disparity information could be transmitted to the device to enable an operation of the device
- Disparity information determined from a stereo image pair could be used to allow for a variety of applications.
- Applications could include performing a disparity-selective image processing on one or both of the images of the stereo image pair (e.g., to blur aspects of an image outside of a specified depth within the image), to identify a human, to determine a user interface command, to tag objects in an augmented reality display, to map an environment, or to perform some other operation.
- a system including two cameras as described herein could be configured to navigate an environment.
- the system could include a bipedal robot, a wheeled robot, a drone, a self-driving car, an autonomous vehicle, or some other device configured to navigate (e.g., to wholly or partially control its movement within) an environment (e.g., an airspace, a body of water, a roadway, an internal environment, an external environment).
- a bipedal robot e.g., a wheeled robot, a drone, a self-driving car, an autonomous vehicle, or some other device configured to navigate (e.g., to wholly or partially control its movement within) an environment (e.g., an airspace, a body of water, a roadway, an internal environment, an external environment).
- an environment e.g., an airspace, a body of water, a roadway, an internal environment, an external environment.
- the determined disparity information could be used to determine commands to operate elements of the device (e.g., motors, servos, actuators, propellers, legs, flight control surfaces) to navigate the device through an environment (e.g., to avoid obstacles in the environment, to maintain a course relative to objects (e.g., traffic control structures or roadway indicators) in the environment). Additional applications of determined disparity information are anticipated.
- elements of the device e.g., motors, servos, actuators, propellers, legs, flight control surfaces
- an environment e.g., to avoid obstacles in the environment, to maintain a course relative to objects (e.g., traffic control structures or roadway indicators) in the environment. Additional applications of determined disparity information are anticipated.
- FIG. 10 is a simplified block diagram showing some of the components of an example computing device 1000 that may include camera components 1024 .
- Camera components 1024 may include multiple cameras, such as cameras 912 A to 912 C.
- computing device 1000 may be a cellular mobile telephone (e.g., a smartphone), a still camera, a video camera, a fax machine, a computer (such as a desktop, notebook, tablet, or handheld computer), a personal digital assistant (PDA), a home automation component, a digital video recorder (DVR), a digital television, a remote control, a wearable computing device, a robot, a drone, an autonomous vehicle, or some other type of device equipped with at least some image capture and/or image processing capabilities.
- computing device 1000 may represent a physical camera device such as a digital camera, a particular physical hardware platform on which a camera application operates in software, or other combinations of hardware and software that are configured to carry out image capture and/or processing functions.
- computing device 1000 may include a communication interface 1002 , a user interface 1004 , a processor 1006 , data storage 1008 , and camera components 1024 , all of which may be communicatively linked together by a system bus, network, or other connection mechanism 1010 .
- Communication interface 1002 may function to allow computing device 1000 to communicate, using analog or digital modulation of electric, magnetic, electromagnetic, optical, or other signals, with other devices, access networks, and/or transport networks.
- communication interface 1002 may facilitate circuit-switched and/or packet-switched communication, such as plain old telephone service (POTS) communication and/or Internet protocol (IP) or other packetized communication.
- POTS plain old telephone service
- IP Internet protocol
- communication interface 1002 may include a chipset and antenna arranged for wireless communication with a radio access network or an access point.
- communication interface 1002 may take the form of or include a wireline interface, such as an Ethernet, Universal Serial Bus (USB), or High-Definition Multimedia Interface (HDMI) port.
- USB Universal Serial Bus
- HDMI High-Definition Multimedia Interface
- Communication interface 1002 may also take the form of or include a wireless interface, such as a Wifi, BLUETOOTH®, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or 3GPP Long-Term Evolution (LTE)).
- a wireless interface such as a Wifi, BLUETOOTH®, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or 3GPP Long-Term Evolution (LTE)).
- GPS global positioning system
- LTE 3GPP Long-Term Evolution
- communication interface 1002 may comprise multiple physical communication interfaces (e.g., a Wifi interface, a BLUETOOTH® interface, and a wide-area wireless interface).
- communication interface 1002 may function to allow computing device 1000 to communicate, with other devices, remote servers, access networks, and/or transport networks.
- the communication interface 1002 may function to access first and second images (e.g., stereo image pairs) via communication with a remote server or other remote device or system in order to allow the computing device 1000 to generate a disparity map based on the accessed first and second images.
- the remote system could be a server contained a memory containing one or more stereo image pairs.
- the remote system could include two or more cameras, or other image capture devices, configured to generate stereo image pairs that could be accessed as described above by computing device 1000 .
- Such disparity information could be generated by the computing device 1000 according to the methods described herein or by some other methods.
- User interface 1004 may function to allow computing device 1000 to interact with a user, for example to receive input from and/or to provide output to the user.
- user interface 1004 may include input components such as a keypad, keyboard, touch-sensitive or presence-sensitive panel, computer mouse, trackball, joystick, microphone, and so on.
- User interface 1004 may also include one or more output components such as a display screen which, for example, may be combined with a presence-sensitive panel. The display screen may be based on CRT, LCD, and/or LED technologies, or other technologies now known or later developed.
- User interface 1004 may also be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices.
- user interface 1004 may include a display that serves as a viewfinder for still camera and/or video camera functions supported by computing device 1000 (e.g., for functions of images captured using the camera components 1024 ). Additionally, user interface 1004 may include one or more buttons, switches, knobs, and/or dials that facilitate the configuration and focusing of the camera components 1024 and the capturing of images using the camera components 1024 . It may be possible that some or all of these buttons, switches, knobs, and/or dials are implemented as functions on a touch- or presence-sensitive panel.
- Processor 1006 may comprise one or more general purpose processors—e.g., microprocessors—and/or one or more special purpose processors—e.g., digital signal processors (DSPs), graphics processing units (GPUs), floating point units (FPUs), network processors, or application-specific integrated circuits (ASICs).
- DSPs digital signal processors
- GPUs graphics processing units
- FPUs floating point units
- ASICs application-specific integrated circuits
- special purpose processors may be capable of image processing, image alignment, and merging images, among other applications or functions.
- Data storage 1008 may include one or more volatile and/or non-volatile storage components, such as magnetic, optical, flash, or organic storage, and may be integrated in whole or in part with processor 1006 .
- Data storage 1008 may include removable and/or non-removable components.
- Processor 1006 may be capable of executing program instructions 1018 (e.g., compiled or non-compiled program logic and/or machine code) stored in data storage 1008 to carry out the various functions described herein. Therefore, data storage 1008 may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by computing device 1000 , cause computing device 1000 to carry out any of the methods, processes, or functions disclosed in this specification and/or the accompanying drawings. The execution of program instructions 1018 by processor 1006 may result in processor 1006 using data 1012 .
- program instructions 1018 e.g., compiled or non-compiled program logic and/or machine code
- program instructions 1018 may include an operating system 1022 (e.g., an operating system kernel, device driver(s), and/or other modules) and one or more application programs 1020 (e.g., camera functions, address book, email, web browsing, social networking, and/or gaming applications) installed on computing device 1000 .
- data 1012 may include operating system data 1016 and application data 1014 .
- Operating system data 1016 may be accessible primarily to operating system 1022
- application data 1014 may be accessible primarily to one or more of application programs 1020 .
- Application data 1014 may be arranged in a file system that is visible to or hidden from a user of computing device 1000 .
- Application programs 1020 may communicate with operating system 1022 through one or more application programming interfaces (APIs). These APIs may facilitate, for instance, application programs 1020 reading and/or writing application data 1014 , transmitting or receiving information via communication interface 1002 , receiving and/or displaying information on user interface 1004 , capturing images using camera components 1024 , and so on.
- APIs application programming interfaces
- the determination of disparity values from images captured by a pair of cameras i.e., from stereo image pairs captured by, e.g., camera components 1024 ) or generated by some other means (e.g., image processing, virtual reality, computer-generated images, augmented reality) and/or determination of other information based on such determined disparity values could be performed by one or more components of the processor 1006 (e.g., by one or more digital signal processors (DSPs), graphics processing units (GPUs), central processing units (CPUs), floating point units (FPUs), application-specific integrated circuits (ASICs), some other type of computational component, and/or some combination of one or more types of computational components, of the processor 1006 ).
- DSPs digital signal processors
- GPUs graphics processing units
- CPUs central processing units
- FPUs floating point units
- ASICs application-specific integrated circuits
- certain operations related to determination of disparity values could be performed by a first computational element or type of computational element (e.g., DSPs, GPUs) of the processor 1006 and other certain operations related to determination of disparity values (e.g., step size optimization during a gradient descent operation to minimize a cost function) could be performed by a second computational element or type of computational element (e.g., a general-purpose CPU) of the processor 1006 .
- a first computational element or type of computational element e.g., DSPs, GPUs
- other certain operations related to determination of disparity values e.g., step size optimization during a gradient descent operation to minimize a cost function
- a second computational element or type of computational element e.g., a general-purpose CPU
- Application programs 1020 may take the form of “apps” that could be downloadable to computing device 1000 through one or more online application stores or application markets (via, e.g., the communication interface 1002 ). However, application programs can also be installed on computing device 1000 in other ways, such as via a web browser or through a physical interface (e.g., a USB port) of the computing device 1000 .
- Camera components 1024 may include, but are not limited to, an aperture, shutter, recording surface (e.g., photographic film and/or an image sensor), lens, and/or shutter. Camera components 1024 may be controlled at least in part by software executed by processor 1006 (e.g., by application programs 1020 ). Further, camera components 1024 may include multiple camera systems, which each camera system includes a respective aperture, shutter, recording surface, lens, image sensor, processor, and/or other elements.
- each camera could include its own aperture, lens, and image sensor, while sharing other components such as a processor, a data bus, and/or a shutter button.
- each camera could include its own lens, but share the same image sensor.
- an individual camera system's components may be utilized only for that individual camera system, and not shared with other camera systems.
- a still camera may capture one or more images each time an image capture is triggered (e.g., that an electronic and/or mechanical shutter is operated).
- a video camera may continuously capture images at a particular rate (e.g., 24 images—or frames—per second) as long as image capture remains triggered (e.g., while a shutter button is held down, until a stop button is pressed).
- Some digital still cameras may open the shutter when the camera device or application is activated, and the shutter may remain in this position until the camera device or application is deactivated. While the shutter is open, the camera device or application may capture and display a representation of a scene on a viewfinder. When image capture is triggered, one or more distinct digital images of the current scene may be captured.
- example embodiments may utilize cameras with electronic shutters and/or mechanical shutters.
- Captured digital images may be represented as a one-dimensional, two-dimensional, or multi-dimensional array of pixels.
- Each pixel may be represented by one or more values that may encode the respective pixel's color and/or brightness.
- the YCbCr color model (which may also be referred to as the YUV color model).
- the Y color channel may represent the brightness of a pixel
- the Cb (U) and Cr (V) color channels may represent the blue chrominance and red chrominance, respectively, of the pixel.
- each of these color channels may take values from 0 to 255 (i.e., the tonal range that a single 8-bit byte can offer).
- the brightness of a pixel may be represented by a 0 or a value near zero if the pixel is black or close to black, and by a 255 or a value near 255 if the pixel is white or close to white.
- the value of 255 is a non-limiting reference point, and some implementations may use different number of bits for pixel value representation (e.g., 10, 12, etc.).
- the YCbCr color model is just one possible color model, and other color models such as a red-green-blue (RGB) color model or a cyan-magenta-yellow-key (CMYK) may be employed with the embodiments herein.
- the pixels in an image may be represented in various file formats, including raw (uncompressed) formats, or compressed formats such as Joint Photographic Experts Group (JPEG), Portable Network Graphics (PNG), Graphics Interchange Format (GIF), and so on.
- JPEG Joint Photographic Experts Group
- PNG Portable Network Graphics
- GIF Graphics Interchange Format
- a color camera e.g., a camera including a color image sensor
- the color camera could be operated to generate black-and-white images by, e.g., determining a brightness of individual pixels based on color information of the individual pixels.
- the color camera could be configured to generate a red, green, and blue detected light value for an individual pixel, and a brightness value for the individual
- each step, block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments.
- Alternative embodiments are included within the scope of these example embodiments.
- functions described as steps, blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including in substantially concurrent or in reverse order, depending on the functionality involved.
- more or fewer steps, blocks and/or functions may be used with any of the message flow diagrams, scenarios, and flow charts discussed herein, and these message flow diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- a step or block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique.
- a step or block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data).
- the program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique.
- the program code and/or related data may be stored on any type of computer-readable medium, such as a storage device, including a disk drive, a hard drive, or other storage media.
- the computer-readable medium may also include non-transitory computer-readable media such as computer-readable media that stores data for short periods of time like register memory, processor cache, and/or random access memory (RAM).
- the computer-readable media may also include non-transitory computer-readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, and/or compact-disc read only memory (CD-ROM), for example.
- the computer-readable media may also be any other volatile or non-volatile storage systems.
- a computer-readable medium may be considered a computer-readable storage medium, for example, or a tangible storage device.
- a step or block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
Abstract
Description
f i(d i)=max(0,d i −u i)+max(0,l i −d i)
Â=DAD
d=S Tν
Â=DAD
C d =SDS T
and:
C i =SS T
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/394,314 US9736451B1 (en) | 2014-09-16 | 2016-12-29 | Efficient dense stereo computation |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/487,387 US9571819B1 (en) | 2014-09-16 | 2014-09-16 | Efficient dense stereo computation |
US15/394,314 US9736451B1 (en) | 2014-09-16 | 2016-12-29 | Efficient dense stereo computation |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/487,387 Continuation US9571819B1 (en) | 2014-09-16 | 2014-09-16 | Efficient dense stereo computation |
Publications (1)
Publication Number | Publication Date |
---|---|
US9736451B1 true US9736451B1 (en) | 2017-08-15 |
Family
ID=57965005
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/487,387 Active 2035-08-02 US9571819B1 (en) | 2014-09-16 | 2014-09-16 | Efficient dense stereo computation |
US15/394,314 Active US9736451B1 (en) | 2014-09-16 | 2016-12-29 | Efficient dense stereo computation |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/487,387 Active 2035-08-02 US9571819B1 (en) | 2014-09-16 | 2014-09-16 | Efficient dense stereo computation |
Country Status (1)
Country | Link |
---|---|
US (2) | US9571819B1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170132769A1 (en) * | 2015-11-05 | 2017-05-11 | Google Inc. | Edge-Aware Bilateral Image Processing |
US20170330034A1 (en) * | 2016-05-11 | 2017-11-16 | Baidu Usa Llc | System and method for providing augmented virtual reality content in autonomous vehicles |
US10909706B1 (en) * | 2017-06-02 | 2021-02-02 | Apple Inc. | Robust hardware efficient disparity estimation using total-variation L1 regularization |
Families Citing this family (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9571819B1 (en) * | 2014-09-16 | 2017-02-14 | Google Inc. | Efficient dense stereo computation |
KR102350232B1 (en) * | 2014-11-20 | 2022-01-13 | 삼성전자주식회사 | Method and apparatus for matching stereo images |
CN111371986A (en) * | 2015-03-16 | 2020-07-03 | 深圳市大疆创新科技有限公司 | Apparatus and method for focus adjustment and depth map determination |
EP3299240A4 (en) * | 2015-05-19 | 2019-01-30 | LG Electronics Inc. | Vehicle driving assisting device and vehicle |
US10198790B1 (en) * | 2015-07-16 | 2019-02-05 | Hrl Laboratories, Llc | Multi-domain foveated compressive sensing system for adaptive imaging |
GB2541884A (en) * | 2015-08-28 | 2017-03-08 | Imp College Of Science Tech And Medicine | Mapping a space using a multi-directional camera |
CN110282132B (en) * | 2016-01-26 | 2021-04-02 | 深圳市大疆创新科技有限公司 | Unmanned vehicles and many meshes imaging system |
US10701335B2 (en) * | 2016-02-25 | 2020-06-30 | Intel Corporation | Calculation of temporally coherent disparity from sequence of video frames |
US10742878B2 (en) * | 2016-06-21 | 2020-08-11 | Symbol Technologies, Llc | Stereo camera device with improved depth resolution |
JP6422924B2 (en) * | 2016-09-12 | 2018-11-14 | 株式会社ソニー・インタラクティブエンタテインメント | Imaging apparatus and subject information acquisition method |
US10621446B2 (en) * | 2016-12-22 | 2020-04-14 | Texas Instruments Incorporated | Handling perspective magnification in optical flow processing |
US10523918B2 (en) * | 2017-03-24 | 2019-12-31 | Samsung Electronics Co., Ltd. | System and method for depth map |
WO2018179191A1 (en) * | 2017-03-29 | 2018-10-04 | 株式会社日立製作所 | Control device and control system |
DE102018202514A1 (en) * | 2018-02-20 | 2019-08-22 | Bayerische Motoren Werke Aktiengesellschaft | System and method for automatically creating a video of a trip |
WO2019230813A1 (en) * | 2018-05-30 | 2019-12-05 | パナソニック インテレクチュアル プロパティ コーポレーション オブ アメリカ | Three-dimensional reconstruction method and three-dimensional reconstruction device |
CN110547766B (en) * | 2019-08-22 | 2023-04-28 | 苏州佳世达光电有限公司 | Operation method of mouth sweeping machine |
CN111709938B (en) * | 2020-06-18 | 2023-07-07 | 武汉唯理科技有限公司 | Pavement defect and casting detection method based on depth map |
AU2021311622A1 (en) * | 2020-07-22 | 2023-04-13 | Blue River Technology Inc. | Methods and apparatus for using filtering to improve image patch matching and/or for using depth information generated from stereo images |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6046763A (en) | 1997-04-11 | 2000-04-04 | Nec Research Institute, Inc. | Maximum flow method for stereo correspondence |
WO2007017834A2 (en) | 2005-08-09 | 2007-02-15 | Koninklijke Philips Electronics N.V. | Disparity value generator |
US7248968B2 (en) | 2004-10-29 | 2007-07-24 | Deere & Company | Obstacle detection using stereo vision |
US8275195B2 (en) | 2005-11-30 | 2012-09-25 | Telecom Italia S.P.A. | Method for determining scattered disparity fields in stereo vision |
WO2013078479A1 (en) | 2011-11-23 | 2013-05-30 | Thomson Licensing | Method and system for three dimensional visualization of disparity maps |
US9571819B1 (en) * | 2014-09-16 | 2017-02-14 | Google Inc. | Efficient dense stereo computation |
-
2014
- 2014-09-16 US US14/487,387 patent/US9571819B1/en active Active
-
2016
- 2016-12-29 US US15/394,314 patent/US9736451B1/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6046763A (en) | 1997-04-11 | 2000-04-04 | Nec Research Institute, Inc. | Maximum flow method for stereo correspondence |
US7248968B2 (en) | 2004-10-29 | 2007-07-24 | Deere & Company | Obstacle detection using stereo vision |
WO2007017834A2 (en) | 2005-08-09 | 2007-02-15 | Koninklijke Philips Electronics N.V. | Disparity value generator |
US8275195B2 (en) | 2005-11-30 | 2012-09-25 | Telecom Italia S.P.A. | Method for determining scattered disparity fields in stereo vision |
WO2013078479A1 (en) | 2011-11-23 | 2013-05-30 | Thomson Licensing | Method and system for three dimensional visualization of disparity maps |
US9571819B1 (en) * | 2014-09-16 | 2017-02-14 | Google Inc. | Efficient dense stereo computation |
Non-Patent Citations (8)
Title |
---|
Adams, Andrew et al., Fast High-Dimensional Filtering Using the Permutohedral Lattice, Computer Graphics Forum, May 2010, vol. 29 No. 2. |
Bao, Linchao et al., Fast Edge-Preserving PatchMatch for Large Displacement Optical Flow, IEEE Transactions on Image Processing, 2014. |
Kiao, Jiangjian et al., Bilateral Filtering-based Optical Flow Estimation with Occlusion Detection, Computer Vision-ECCV 2006, May 2012. |
Kiao, Jiangjian et al., Bilateral Filtering-based Optical Flow Estimation with Occlusion Detection, Computer Vision—ECCV 2006, May 2012. |
Krahenbuhl, Philipp and Koltun, Vladlen, Efficient Nonlocal Regularization for Optical Flow, Computer Vision-ECCV 2012, Oct. 2012. |
Krahenbuhl, Philipp and Koltun, Vladlen, Efficient Nonlocal Regularization for Optical Flow, Computer Vision—ECCV 2012, Oct. 2012. |
Scharstein, Daniel and Szeliski, Righard, A taxonomy and evaluation of dense two-frame stereo correspondence algorithms, International Journal of Computer Vision, Apr. 2002, vol. 47 No. 1-3. |
Tao, Michael et al., SimpleFlow: a Non-iterative, Sublinear Optical Flow Algorithm, Computer Graphics Forum, May 2012, vol. 31 No. 2pt1. |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170132769A1 (en) * | 2015-11-05 | 2017-05-11 | Google Inc. | Edge-Aware Bilateral Image Processing |
US9892496B2 (en) * | 2015-11-05 | 2018-02-13 | Google Llc | Edge-aware bilateral image processing |
US20170330034A1 (en) * | 2016-05-11 | 2017-11-16 | Baidu Usa Llc | System and method for providing augmented virtual reality content in autonomous vehicles |
US10366290B2 (en) * | 2016-05-11 | 2019-07-30 | Baidu Usa Llc | System and method for providing augmented virtual reality content in autonomous vehicles |
US10909706B1 (en) * | 2017-06-02 | 2021-02-02 | Apple Inc. | Robust hardware efficient disparity estimation using total-variation L1 regularization |
Also Published As
Publication number | Publication date |
---|---|
US9571819B1 (en) | 2017-02-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9736451B1 (en) | Efficient dense stereo computation | |
KR101936919B1 (en) | Edge-aware bidirectional image processing | |
US9918065B2 (en) | Depth-assisted focus in multi-camera systems | |
US9544574B2 (en) | Selecting camera pairs for stereoscopic imaging | |
US9325968B2 (en) | Stereo imaging using disparate imaging devices | |
US10726580B2 (en) | Method and device for calibration | |
US10949700B2 (en) | Depth based image searching | |
JP2017520050A (en) | Local adaptive histogram flattening | |
US10063832B2 (en) | Image processing method and apparatus | |
CN105678736B (en) | Change the image processing system and its operating method of estimation of Depth with aperture | |
US9978120B2 (en) | Warping panoramic images to fit a boundary | |
CN109661815B (en) | Robust disparity estimation in the presence of significant intensity variations of the camera array | |
KR102587298B1 (en) | Real-time omnidirectional stereo matching method using multi-view fisheye lenses and system therefore | |
EP4064193A1 (en) | Real-time omnidirectional stereo matching using multi-view fisheye lenses | |
CN113344957B (en) | Image processing method, image processing apparatus, and non-transitory storage medium | |
JP2014164497A (en) | Information processor, image processing method and program | |
KR102505659B1 (en) | Three demension scanning apparatus using light based on smartphone | |
KR102587233B1 (en) | 360 rgbd image synthesis from a sparse set of images with narrow field-of-view | |
CN117557887A (en) | Sparse depth completion training method and related equipment | |
CN114926321A (en) | Image perspective transformation method and device, electronic equipment and computer storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:BARRON, JONATHAN TILTON;REEL/FRAME:041284/0390Effective date: 20140915 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |