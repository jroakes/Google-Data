US9383919B2 - Touch-based text entry using hidden Markov modeling - Google Patents
Touch-based text entry using hidden Markov modeling Download PDFInfo
- Publication number
- US9383919B2 US9383919B2 US13/632,042 US201213632042A US9383919B2 US 9383919 B2 US9383919 B2 US 9383919B2 US 201213632042 A US201213632042 A US 201213632042A US 9383919 B2 US9383919 B2 US 9383919B2
- Authority
- US
- United States
- Prior art keywords
- input
- intended
- possibly
- movement
- states
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000000034 method Methods 0.000 claims abstract description 70
- 230000007704 transition Effects 0.000 claims description 86
- 230000000007 visual effect Effects 0.000 claims description 25
- 230000000694 effects Effects 0.000 claims description 10
- 230000001133 acceleration Effects 0.000 claims description 9
- 230000014509 gene expression Effects 0.000 abstract description 3
- 230000008569 process Effects 0.000 description 17
- 238000012549 training Methods 0.000 description 14
- 230000006870 function Effects 0.000 description 13
- 238000010586 diagram Methods 0.000 description 9
- 238000004891 communication Methods 0.000 description 7
- 239000000463 material Substances 0.000 description 4
- 239000000047 product Substances 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 238000009826 distribution Methods 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 238000013179 statistical model Methods 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 239000011248 coating agent Substances 0.000 description 2
- 238000000576 coating method Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 210000005069 ears Anatomy 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 241000699800 Cricetinae Species 0.000 description 1
- 230000001174 ascending effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000004069 differentiation Effects 0.000 description 1
- 230000003203 everyday effect Effects 0.000 description 1
- 230000005057 finger movement Effects 0.000 description 1
- 210000003128 head Anatomy 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 238000012905 input function Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007257 malfunction Effects 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 230000002035 prolonged effect Effects 0.000 description 1
- 210000001525 retina Anatomy 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
- 238000010408 sweeping Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
Definitions
- Computing systems such as personal computers, laptop computers, tablet computers, cellular phones, among many other types of computing systems, are increasingly prevalent in numerous aspects of modern life. As computers become progressively more integrated with users' everyday life, the convenience, efficiency, and intuitiveness of the user-interfaces by which users interact with computing devices becomes progressively more important.
- a user-interface may include various combinations of hardware and software which enable the user to, among other things, interact with a computing system.
- One example of a modern user-interface is a “pointing device” that may allow a user to input spatial data into a computing system.
- the spatial data may be received and processed by the computing system, and may ultimately be used by the computing system as a basis for executing certain computing functions.
- One type of pointing device may, generally, be based on a user moving an object. Examples of common such pointing devices include a computer mouse, a trackball, a joystick, a pointing stick, and a roller mouse. Other examples of pointing devices based on a user moving an object may exist as well.
- the object includes sensors that are arranged to transmit, to the computing system, data that indicates the distance and direction of movement of the object.
- the computing system may be equipped with a graphical display that may, for example, provide a visual depiction of a graphical pointer that moves in accordance with the movement of the object.
- the graphical display may also provide a visual depiction of other objects that the user may manipulate, including, for example, a visual depiction of a graphical user-interface. The user may refer to such a graphical user-interface when inputting data.
- pointing device may, generally, be based on a user touching a surface.
- Examples of common such pointing devices include a touchpad and a touch screen.
- Other examples of pointing devices based on a user touching a surface may exist as well.
- the surface is a flat surface that can detect contact with the user's finger (and/or another pointing tool such as a stylus).
- the surface may include electrode sensors that are arranged to transmit, to the computing system, data that indicates the distance and direction of movement of the finger on the surface.
- the computing system may be equipped with a graphical display similar to the graphical display described above. Implementations of a touchpad typically involve a graphical display that is physically remote from the touchpad.
- a touchscreen is typically characterized by a touchpad embedded into a graphical display such that users may interact directly with a visual depiction of the graphical user-interface, and/or other elements displayed on the graphical display, by touching the graphical display itself.
- User-interfaces may be arranged to provide various combinations of keys, buttons, and/or, more generally, input regions. Often, user-interfaces will include input regions that are associated with multiple characters and/or computing commands. Typically, users may select various characters and/or various computing commands, by performing various input actions on the user-interface.
- input systems must likewise become smaller. Such smaller input systems can impair the accuracy of user-input. For example, if an input system that is controlled by a user's finger includes input buttons that are around the size of, or even smaller than, the user's fingertip, it may become difficult for the user to interact with the input system and therefore the overall accuracy and/or efficiency of the system may suffer. Further, as input systems become smaller, the speed with which a user may use the system may suffer. An improvement is therefore desired.
- the disclosure herein may help to facilitate more accurate, more efficient, and/or faster use of an input system of a computing device. More particularly, the disclosure herein relates to techniques for touch-based text entry that involve Hidden Markov Modeling (HMM).
- HMM Hidden Markov Modeling
- a system may include: (1) a processor; (2) a non-transitory computer readable medium; and (3) program instructions stored on the non-transitory computer readable medium and executable by the processor to cause a computing device to: (a) provide a user-interface including a set of input regions, where each of the input regions is associated with at least one character from a set of characters; (b) receive data indicating an input movement corresponding at least to (i) at least two intended input states, where each of the at least two intended input states is associated with at least one input region, and (ii) a transition between the at least two intended input states; (c) determine, for each of one or more possibly-intended inputs, a respective likelihood that each respective possibly-intended input corresponds to the input movement, where each of the one or more possibly-intended inputs is associated with (i) at least two possible input states, where each of the at least two possible input states is associated with at least one input region and (ii) a possible transition between the at
- a non-transitory computer-readable medium may include instructions including: (a) providing a user-interface including a set of input regions, where each of the input regions is associated with at least one character from a set of characters; (b) receiving data indicating an input movement corresponding at least to (i) at least two intended input states, where each of the at least two intended input states is associated with at least one input region, and (ii) a transition between the at least two intended input states; (c) determining, for each of one or more possibly-intended inputs, a respective likelihood that each respective possibly-intended input corresponds to the input movement, where each of the one or more possibly-intended inputs is associated with (i) at least two possible input states, where each of the at least two possible input states is associated with at least one input region and (ii) a possible transition between the at least two possible input states, and where each respective likelihood is determined based at least in part on (i) a likelihood that the at
- a method may involve: (a) providing a user-interface including a set of input regions, where each of the input regions is associated with at least one character from a set of characters; (b) receiving data indicating an input movement corresponding at least to (i) at least two intended input states, where each of the at least two intended input states is associated with at least one input region, and (ii) a transition between the at least two intended input states; (c) determining, for each of one or more possibly-intended inputs, a respective likelihood that each respective possibly-intended input corresponds to the input movement, where each of the one or more possibly-intended inputs is associated with (i) at least two possible input states, where each of the at least two possible input states is associated with at least one input region and (ii) a possible transition between the at least two possible input states, and where each respective likelihood is determined based at least in part on (i) a likelihood that the at least two intended input states correspond to the at least two possible input states associated with the
- FIG. 1 is a simplified block diagram of an example method.
- FIG. 2A shows aspects of an example user-interface.
- FIG. 2B shows an example input movement across the example user-interface shown in FIG. 2A .
- FIG. 2C shows another example input movement across the example user-shown in FIG. 2A .
- FIG. 2D shows an example input movement and aspects of an example user-interface.
- FIG. 3A shows an example state diagram
- FIG. 3B shows an example state diagram
- FIG. 3C shows an example state diagram
- FIG. 4A illustrates an example display system.
- FIG. 4B illustrates an alternate view of the system illustrated in FIG. 4A .
- FIG. 5A illustrates an example system for receiving, transmitting, and displaying data.
- FIG. 5B illustrates an example system for receiving, transmitting, and displaying data.
- FIG. 6 shows an example system for dynamically controlling a virtual input area.
- FIG. 7 shows a simplified block diagram depicting example components of an example computing system.
- FIG. 8 shows an example input movement and aspects of an example user-interface.
- FIG. 9 is a chart illustrating detection of dwell points.
- FIG. 10 is a histogram based on these inter-coordinate distances.
- a computing system performing certain functions including, for example, detecting, processing, storing, and displaying functions.
- some exemplary embodiments may not involve the computing system directly performing all recited functions.
- a computing system that is communicatively-connected to external sensors may detect an input movement by receiving data representing the input movement from the connected sensors, instead of directly detecting the movement.
- a computing device may cause an analysis of an input movement to be performed by transmitting data representing the input movement to a remote server system and then receiving analysis data from the remote system.
- the exemplary methods and corresponding apparatuses and systems discussed herein enable a system to receive character string input (e.g., words, numbers, mathematical expressions, symbolic strings, etc.) by detecting and interpreting an input movement across a user-interface.
- a touch-based computing system may, for instance, detect an input movement by tracking the path of a pointing element (e.g., a stylus or finger) as it is dragged across a contact-sensitive input surface (e.g., a touch-sensitive screen or external touch pad). Then, the system may interpret the detected input movement. For example, if the system presents numerical digits on a graphical interface, and a detected input movement starts near the “5” digit and moves towards the “3” digit, then the system may interpret this pattern as intended to represent the number “53”.
- the system may interpret the detected input movement using, at least in part, a Hidden Markov Model (HMM).
- HMM Hidden Markov Model
- an HMM assumes that the process of inputting the input movement includes a series of successive transitions from one input state to another. For instance, if a user is attempting to input “53”, the user would begin at one input state (in this case, inputting a “5”) and then transition to a second input state (inputting a “3”). Since the system ultimately determines what the user intends to input by interpreting the input movement, the intended input states are considered “hidden” from the system. The system may, therefore, use a statistical model (in this case, an HMM) to infer which input states were intended for input. In practice, this technique may include taking several possible user-inputs and determining a likelihood that each possible user-input is the intended user-input.
- an exemplary computing system may select one or more of the possible user-inputs based on the determined likelihoods. Then, the system may cause the selected user-inputs to be displayed so as to indicate to the user what the computing system has inferred are the possibly-intended user-inputs. In some cases, after the one or more selected user-inputs are displayed, the system may receive further user-input particularly selecting one of the displayed user-inputs. In some embodiments, the determination of likelihoods, selection, and display of possible user-inputs may be performed while the user-input is being received (i.e., as the user continues to enter characters of a particular character string).
- FIG. 1 is a simplified flow-chart illustrating an exemplary method 100 .
- method 100 involves providing a user-interface that includes a set of input regions, with each input region representing one or more characters.
- Method 100 also involves, at block 104 , detecting an input movement that corresponds to an intended input.
- method 100 involves determining a likelihood that each of a set of possible inputs is the intended input.
- the method also involves, at block 108 , selecting at least one possible input based on the likelihoods determined at block 106 .
- the method involves displaying the at least one possible input that was selected at block 108 .
- example method 100 involves providing a user-interface including a set of input regions that are each associated with at least one of a set of characters.
- the user-interface may be any user-interface that provides a set of input regions, regardless of, for example, shape, size, number, or arrangement of the input regions.
- the user-interface may be communicatively coupled to a graphical display that may provide a visual depiction of the input regions of the user-interface along with a visual depiction of the position of a pointer relative to the input regions.
- the graphical display may provide other visual depictions, including those involving the user-interface and those not involving the user-interface, as well.
- example user-interface 200 is shown. It should be understood, however, that example user-interface 200 is shown for purposes of example and explanation only, and should not be taken to be limiting.
- Example user-interface 200 is arranged to provide plurality of input regions (for example, input regions 202 A-D), with each input region representing a character (for example, input regions 202 A-D represent the letters “U”, “I”, “O”, and “P” respectively).
- user-interface 200 may be a touchscreen, having a touchpad embedded into a graphical display, and may be arranged to depict the set of input regions.
- user-interface 200 may be a touchpad.
- a visual depiction of the user-interface may be provided on a graphical display that is physically remote from the movable object or touchpad (e.g., a graphical display of an HMD).
- Other embodiments of user-interface 200 are certainly possible as well, including, but not limited to, any of the user-interfaces discussed above.
- the methods described herein may be applied to a movable pointing device that is communicatively coupled to a graphical display.
- the input regions may exist entirely virtually (as depicted by a graphical display) and a user may carry out input movements between such input regions by moving a virtual pointer between the input regions using the movable pointing device.
- each input region may be associated with one or more characters.
- an input region may be associated with a primary character from a set of primary characters.
- an input region may also, or alternatively, be associated with a subset of secondary characters from a set of secondary characters.
- Input regions may also be associated with one or more computing commands.
- graphical display 201 depicts a plurality of input regions.
- Graphical display 201 is, in appearance, an arrangement that users may be familiar with by way of their use of, for example, computer keyboards and/or typewriting machines. More specifically, graphical display 201 provides input regions arranged in a common QWERTY keyboard layout, with an input region associated with each of the numbers “0” through “9”, an input region associated with each letter of the English alphabet, and an input region associated with the “.” character. It should be understood, however, that example embodiments are in no way limited to this specific set of characters or a particular layout.
- graphic display 201 may display various input regions 202 A to 202 E of user-interface 200 .
- Each input region 202 A to 202 E may be associated with a primary character from a set of primary characters and a subset of secondary characters from a set of secondary characters.
- input region 202 A of user-interface 200 is associated with primary character “U”.
- input region 202 A may also be associated with a secondary character (e.g., “u”).
- input region 202 B, of user-interface 200 is associated with primary character “I”, and, in turn, may also be associated with secondary character “i”.
- the graphical display may indicate the secondary (or tertiary, etc.) character with which an input region is associated.
- input regions shown in user-interface 200 may be referred to by reference to the primary character with which they are associated.
- input region 202 A as depicted by graphical display 201 of user-interface 200 , may be referred to as the “U region”
- input region 202 B as depicted by graphical display 201 of user-interface 200
- I region and so on.
- input regions may also, or alternatively, be associated with one or more computing commands.
- input region 202 E is associated with a computing command indicated by the command SHIFT. This computing command may cause the computing system to, for example, toggle between primary and secondary characters.
- the computing commands that a given input region may be associated with may include computing commands that are generally used for entering, modifying, or otherwise editing strings of characters.
- the computing commands may include a command to delete a character, a command to enter a space, a command to enter a suggested string of characters, a command to display additional symbols that various input regions may be associated with, and/or a command to enter a “hard return.”
- the computing commands with which a given input region is associated may include computing commands that are generally associated with executing a particular application on a computing system.
- a computing command may execute a sound recording application, which may be used for purposes of speech recognition.
- the computing commands referred to herein may be any computing function that may be executed by a computing system.
- a user-interface may also contain elements other than the input section.
- user-interface 200 contains viewing region 203 , which may display, for example, user-inputs.
- viewing region 203 may display, for example, user-inputs.
- the set of input regions in graphical display 201 also have spaces between adjacent input regions. Such spacing may help to better define the location of each input region.
- input regions may be associated with other characters, computing commands, or other computing functions.
- user-interface 200 may be capable of being dynamically arranged in a customized manner so that the input regions of the user-interface are associated with characters and/or computing commands that are most useful to the user.
- a user may assign to a particular input region a command to execute the particular computing application.
- a user may assign to a particular input region a computing command to execute a particular macro routine so that the particular macro routine may be conveniently executed while using, for instance, a computing application that the user regularly runs.
- User-interface 200 may be part of, or otherwise communicatively coupled to, a computing system as will be discussed further with reference to FIG. 7 .
- example user-interface 200 may be generally configured to detect, recognize, track, or otherwise sense movement of a pointing element to, from, and/or between input regions.
- exemplary method 100 involves detecting an input movement that corresponds to an intended input.
- user-interface 200 is a touchscreen or touchpad
- user-interface 200 may be configured to detect the movement of a user's finger, a stylus, or other pointing element, across the surface of the touchscreen or touchpad to, from, and/or between input regions.
- user-interface 200 may be configured to generate data indicating the movement, which user-interface 200 may generally make available for use by other components of a computing system (such as computing system 700 , discussed further below), perhaps by way of system bus (such as system bus 706 , discussed further below). For example, as generally discussed above, user-interface 200 may transmit data indicating an input movement to a processor (such as processor 702 , discussed further below) for purposes of executing any of the functions described herein, including, but not limited to, those functions described with respect to FIG. 1 .
- a processor such as processor 702 , discussed further below
- an input movement may represent any movement of a pointing element across or over a portion of the user-interface.
- FIG. 2B shows an exemplary graphical interface 200 that is divided into several input regions (for example, input regions 202 ), with each input region representing a character.
- input regions 202 A to 202 D represent the characters “U”, “I”, “O”, and “P”, respectively.
- FIG. 2B also shows input movement 204 that traces an example path of pointing element 206 across graphical display 201 of user-interface 200 .
- input movement 204 begins at point 208 and then moves through inflection points 210 , 212 , and 214 , finally stopping at endpoint 216 .
- each point 208 , 210 , 212 , 214 , and 216 may be an observed input state (i.e., a state detected by the computing system).
- An input movement may relate to an intended user-input.
- An intended user-input may, for instance, be a character string.
- Examples of character strings may include words, numbers, mathematical expressions, sets of words, formulas, multi-stroke symbols, and/or any combination of character strings.
- Each character string may include one or more individual characters and each character may be associated with one or more input regions. Accordingly, a portion of each input movement may represent each character.
- input movement 204 includes input states represented by points 208 - 216 .
- One possible intended user-input that may be inferred from input movement 204 may correspond to the word “TRACK”.
- beginning point 208 may be associated with a T region
- inflection point 210 may be associated with an R region
- inflection point 212 may be associated with an A region, and so on.
- an input movement may include transitions between successive input states. For instance, in the example of input movement 204 representing the word “TRACK”, straight portion 218 of input movement 204 may represent a transition between an input state representing an “A” and an input state representing a “C”.
- Input movement 204 may be detected and represented in a variety of ways.
- a computing system may receive data representing the position of pointing element 206 at given time increments and store this position data as a representation of the detected input movement.
- Such position data may be represented according to an “x coordinate” and/or a “y coordinate.” Other representations may be possible as well.
- position data may be processed to indicate the instantaneous rate of movement of the input movement at each of the given time intervals.
- a system may apply numerical differentiation algorithms to the position data to determine the speed and direction with which pointing element 206 was moving across user-interface 200 at a given time.
- a system may only store rate-of-movement data to represent the input movement instead of storing any position data.
- Input movement data may be further processed to determine higher-order movement data, such as data representing the direction and magnitude of acceleration related to a given input movement. Other examples of analysis of, detection of, and/or representation of input movements may exist as well.
- An input movement may relate to the movement that a pointing device makes while maintaining continuous contact with a contact-sensitive element of the user-interface.
- a computing system may begin tracking the movement of a pointing device when the pointing device makes prolonged contact with the contact-sensitive element and then stop tracking the input movement when the pointing device has broken contact with the contact-sensitive element for longer than a specified time limit.
- continuous in this sense is intended to mean “continuous contact” with the contact-sensitive element of the user-interface rather than, for example, necessarily continuous movement of the pointing device.
- Some computing systems may be configured to recognize particular patterns in an input movement and, in response to a particular pattern, carry out a specific command or function. For example, a system may interpret a zigzag across a single input region as indicating that the character associated with the input region should be used twice. As another example, an input movement that stops on a particular input region for longer than a specified time may indicate that a secondary character should be used as opposed to the primary character for this input region.
- input movements may be tracked outside of the borders of the input regions. For example, if the entire screen of a device is touch-sensitive, and the input regions only occupy part of the screen, then the device may still detect the position of an input movement that is on the screen but outside of all of the input regions. In some implementations, movement outside of all input regions may be interpreted as a portion of a transition between input regions.
- input movements beyond the boundaries of all input regions may produce specific actions from the computing system.
- a computing system may be programmed to carry out a specific function when an input movement moves to outside of all the input regions.
- the device may perform an ENTER command.
- the system may switch from using primary characters to using secondary characters for each input region.
- Other commands, implementations, and functions may be used in addition to or instead of the described techniques for interpret input movements that are outside of all input regions.
- user-preferences may specify how the system should receive and interpret input movements, including input movements that are outside of all input regions.
- Example input movement 204 shows straight movements from the center of one input region to the center of another input region, with clear inflection points at the center of the T, R, A, C, and K regions.
- an input movement may not conform to such an ideal path.
- an input movement may not have an inflection point at each intended input region.
- an input movement across user-interface 200 that represents the word “PUT” may not change direction at the U region, because a straight path between the P region and the T region passes through the U region without needing to change directions.
- transitions between input regions may move in more circuitous routes from one region to the next region. Additionally, an input movement may not pass through the center of each intended input region (or through any portion of some intended regions).
- FIG. 2C shows user-interface 200 with an exemplary input movement 220 that may represent the word “TRACK”. Unlike input movement 204 shown in FIG. 2B , input movement 220 does not move directly through the center of each intended input region and the transitions are not straight between each intended input region.
- Some deviations from ideal input movements may be considered contextual effects, because the context of the input state will influence the behavior of the input movement. Examples of such context may include a previous input state, a subsequent input state, a relationship between previous input states, a relationship between subsequent input states, a relationship between previous and subsequent input states, the location of previous or subsequent input states, the location of the current input state, the previous or subsequent use of input command regions, and/or the previous or subsequent use of special patterns. Other contextual effects may exist as well. In short, any given input state may be impacted by the state the proceeded or follows it.
- Some deviations from ideal input movements may result from user- or system-errors. For example, a user may move a pointing element to an unintended input region before arriving at an intended input region. As another example, before beginning an input movement, a user may make unintentional contact with a contact-sensitive element. In such a case a “fumble model” may be used to remove any ambiguity from the user's input resulting from any touch interaction the user may have with the touch pad prior to the user initiating the intended input movement.
- a “fumble model” is briefly described in section 4 below.
- a contact-sensitive element may malfunction and erroneously detect that a pointing element has broken contact with the element.
- the computing system may be configured to recognize potential errors and ignore the erroneous results. For example, if a brief contact or break in contact comes very near the start or end of an input movement, a system may ignore the brief contact or break in contact.
- a single input state may be indicative of more than one character. For instance, as previously discussed, specific patterns
- HMM Hidden Markov Model
- each character may represent a state.
- the possible states of the process are all the characters in the user-interface, and the string of characters represents several successive transitions from one character to another.
- a Markov model for this process may determine, given the current character, the probability that each possible character will be the next character in the string. For instance, if the current input state represents entering the letter “T”, a Markov model may give, for each possible character, a respective probability that the character will be the next in the string.
- the transition probability of “T” to a particular character may be determined by referring to a list of likely character strings (e.g., a dictionary) and counting the frequency of this transition relative to all other possible transitions (i.e., transitions from “T” to a character other than the particular character).
- a list of likely character strings e.g., a dictionary
- FIG. 3A shows features of an exemplary Markov model 300 as a state diagram.
- input states “T”, “R”, “A”, “C”, and “K” are shown on the state diagram as input states 304 - 312 .
- FIG. 3A also shows several possible state transitions 314 - 330 between input states 304 - 312 .
- transition 314 represents the possible transition from T state 304 back to T state 304
- transition 328 represents the transition from A state 308 to C state 310 .
- Markov model 300 may define transition probabilities associated with these transitions. Given a starting input state of “T”, the probability of entering the string “TRACK” may be determined as the product of the probabilities associated with transitions 324 - 330 .
- Markov model 300 may also include many other characters and transitions.
- An HMM models a Markov process in which the states may not be directly observable. For instance, if a system is trying to infer an intended character string from observations (e.g., an input movement), then, in addition to transition probabilities, the current state may also be uncertain. Therefore, the system may use an HMM to determine the probability that a certain part of the observation relates to a particular state. For example, a system that recognizes an inflection point as indicative of an input state may determine the probability that a given inflection point is indicative of a particular character.
- input movement 220 has an inflection point 224 near the center 226 of the A region 228 .
- An exemplary system may, based on an HMM, assign a state probability to the possible “A” input state based on the distance of inflection point 224 from center 226 . If, therefore, the inflection point were further from center 226 , the system may assign a lower probability to the “A” input state. Additionally, the system may assign state probabilities for each of the other characters represented on user-interface 200 based on the distance of each representative input state from inflection point 224 . As will be discussed, a system may also use numerous other features of an input movement or process to assign state probabilities from an HMM.
- FIG. 3B shows features of an exemplary HMM 342 as state diagram 302 with associated input movement features 344 - 352 from input movement 220 .
- input states 304 - 312 are each associated with one observation state 344 - 352 by a state probability 354 - 362 .
- observation 348 is also shown to be associated with input states 306 and 310 by probabilities 364 and 366 .
- a system may determine the probability that the next input state will be an “A” by using the state probability 356 in combination with the transition probability 326 . This step may be simplified by associating observation 346 with only one state probability 356 .
- the probability that the next input state will be “A” state 308 may be determined using a combination of state probabilities 358 , 364 , and 366 in combination with transition probabilities 318 and 326 .
- the probability of state 308 may be the sum of three terms: (1) the product of state probability 364 and transition probability 326 , (2) the product of state probability 358 and transition probability 318 , and (3) the product of state probability 366 and transition probability 328 .
- Other numerical implementations may also be used.
- an input state may represent a transition. Then, a respective state probability may be assigned for each section of the input movement as indicative of either an input state or a transition state.
- an observation state may represent one detected position, and each transition may represent the changing or not changing to a new state.
- FIG. 3C illustrates aspects of such an HMM 368 . As shown, given a starting state of 304 , the system may continue to receive user-input representing a “T” (by path 370 ) or change to transition state 372 (by path 371 ). Therefore, each detected point on an input movement may be indicative of one state or another and each observation may have associate state probabilities from which to determine a likely state.
- a system may determine the state probability of a particular character or set of characters, based on several factors. For example, a system may use the distance of the input movement from the center of an input region as a basis for determining the probability that the input state represents the input of a character. Additionally, the rate of movement near an input region may be used as a factor to determine whether a character is intended. Further, the system may use the acceleration of the input movement near an input region as a factor for determining the state probability of a particular character. Further still, a system may consider contextual effects, preferences, and/or unintended input states as bases for determining how likely it is that a portion of the input movement represents a particular character.
- a computing system may use the distance of the input movement from an input region as a basis for determining a state probability for a particular character.
- a system may use the shortest distance that the input movement comes to the center of the input region as the distance of the input movement from the input region.
- Some implementations may additionally or alternatively use the distance of the input movement from other points within or near an input region. For example, if a character is associated with an input region that is close to the left edge of the user-interface, the system may use the distance of the input movement from a point that is on the right-hand side of the associated input region to determine the state probability of the character.
- the acceleration of the input movement may be a basis for determining the state probability of a particular character. For example, if the input movement is changing direction when it moves closest to an input region associated with a character, a computing system may increase the likelihood of this character based on the acceleration of the input movement. As another example, if an input movement slows down and/or speeds up very near an input region, then a system may increase the likelihood that the character associated with this input region is an intended character. Correspondingly, if an input movement does not accelerate as it passes over several input regions, a system may treat these input regions as less-likely associated with an intended input state.
- the rate of movement of the input movement may be a basis for determining the state probability of a particular character.
- a system may set a threshold rate of movement (e.g., a certain speed or direction) and, when the input movement moves with a slower rate than the threshold rate of movement, the system may increase the likelihood that an input state may be located near this slower portion.
- the rate of movement may be used as a weighting factor for the likelihood that an input region is associated with an intended character. For example, if the instantaneous rate of movement of an input movement is particularly slow at its shortest distance from a certain input region, the likelihood of this input region may be increased as a result. In like manner, the likelihood of an input region may be decreased if the input region is closest to the input movement at a point where the input movement has a relatively high rate of movement.
- Contextual effects, unintended states, and preferences may also be used as factors in determining state probabilities for possibly-intended characters.
- a contextual effect if a user-input represents an English word, and the input movement does not move directly through any vowels (e.g., movement 220 only touches the corners of the A region and the E region) then the likelihood of vowels that are near the input movement may be increased since most English words contain at least one vowel.
- a system may employ any of several techniques. For example, a system may compare all inputs that include the input state to a list of likely user-inputs (for instance, a dictionary, a table of formulas, etc.) and treat the input state as unlikely if few or no user-input states from the list include this input state. In a similar example, if an input state represents a common error in a particular user-input (for example, if a user misspells a commonly-misspelled word), then the system may increase the likelihood that this mistake represents one or more unintended states or states that are not in their intended order (e.g., “HAMPSTER” instead of “HAMSTER” or “PEICE” instead of “PIECE”).
- HAMPSTER instead of “HAMSTER” or “PEICE” instead of “PIECE”.
- a system may also determine an input state to be unintended because of a particular feature of the movement during this state. For example, an input movement that pauses in one position for a long time may indicate that the user is searching for the next intended character. Hence, movements directly before such a pause may represent an erroneous movement, because the user was unsure of where to move the pointing element next.
- Such unintended input states may be considered “garbage states” and may ultimately be disregarded by the computing system.
- User- or system-preferences may be used in a variety of ways to affect the determination of a state probability for a particular character.
- user-preferences may indicate a customized or ranked list of likely-intended user-inputs.
- user-preferences may indicate the typical rates of movement, accelerations, and positions with which a user produces an input movement to determine more accurate probabilities for input states.
- System preferences may determine, for instance, how intensive a determination step the system should use. For example, if the computing system has very low processing resources, then the system preference may indicate that fewer criteria should be checked in determining whether an input state is likely intended.
- the transition probabilities may also be determined based on a number of similar factors.
- an HMM may be specially-trained to a particular purpose. In the example of words in one language, a preliminary training may be accomplished by simply counting the number of times a particular transition occurs in the language used. More advanced training may include contextual effects, such as where in a word the transition typically occurs or how often this transition occurs in combination with another particular transition. Then, the transition probabilities may be further refined as user-inputs are entered in actual use, so that the HMM represents a model that specifically fits its application.
- transitions may also be compared using distance, rate of movement, acceleration, context, and other techniques, as opposed to simply using predefined probabilities. For instance, the transition from the A region to the P region in user-interface 200 is relatively long, producing many possible input states between these input regions. Therefore, to determine if this transition is likely, the rate of motion between these regions may be compared to a typical rate of motion for such transitions and the transition probability may be weighted accordingly.
- an HMM may determine the likelihood of states and transitions, separate from the likelihood of a whole user-input, the determination of the whole user-input may be carried out while the user-input is still being entered. For instance, as a system recognizes several likely input states and transitions, the system may compare these successive states to a list of likely user-inputs. Especially in longer character strings, this process may determine that one or more user-inputs are quite likely, before the user-input is fully entered. Then, the steps of selecting possible user-inputs (block 108 ) and displaying these user-inputs (block 110 ) may be carried out to reduce input time.
- a computing system may determine the likelihood of user-inputs without using a predefined list of likely user-inputs. For example, if several input states are likely based on an HMM analysis of the input movement, but the user-input is not recognized in a dictionary, the system may create a new submission to the dictionary, based on the input movement. Some implementations may supplement this feature with a predefined list of likely inputs. Then, user-inputs that are not included in the list may be assigned a lower likelihood than corresponding user-inputs that do appear in the predefined list.
- the forward algorithm or Viterbi algorithm may be applied to many user-inputs.
- a forward-backward algorithm in which a probability is determined from the start of a string transitioning toward the end and a probability is determined from the end of a string transitioning toward the start
- a forward-backward algorithm in which a probability is determined from the start of a string transitioning toward the end and a probability is determined from the end of a string transitioning toward the start
- training typically involves two types of training: supervised and unsupervised.
- Supervised training involves the creation of a general model by analyzing the observations that result from known (i.e., not hidden) sequences of states.
- Unsupervised training involves a system continuing to receive data and iteratively creating a model. Unsupervised training may begin with a general model and involve adapting this model based on new data.
- Supervised training may involve a number of different techniques. For instance, counting the occurrences of each character in a database of likely user-inputs may help a system to generate respective probabilities for each character. For instance, since the letter “E” is used in English more often than the letter “Z”, a system for entering English words may assign a higher state probability to the state of intending to input an “E” than the state of inputting a “Z”.
- supervised training of a character entry system may involve the system prompting users to enter predefined character strings, receiving user-input, and then correlating the observed inputs to the known intended input to determine state probabilities.
- a system may find the closest point that each input movement comes to the center of the A region and use the set of closest points as a representative distribution of positions that represent an “A” input state.
- the HMM may associate each position on a user-interface with a probability that an input movement through this position represents each character.
- Such a training technique may also be applied to any other factors that influence state probabilities (e.g., contextual effects, acceleration tracking, rate of movement tracking, unintended states, etc.). For instance, if the letter “G” is intended input in a supervised training process, the system may chose many contexts in which a G may be used to create a general position distribution for the input “G” state. Then, the system may use subsets of this data (for instance, all “G” inputs at the end of a word or all G inputs coming after an “O” input state), in order to determine specific characteristic that relate to context.
- state probabilities e.g., contextual effects, acceleration tracking, rate of movement tracking, unintended states, etc.
- a system may use supervised training to determine transition probabilities. For example, by counting the occurrences of each transition in a database of likely user-inputs, a computer system may generate respective probabilities for each transition.
- Unsupervised training may likewise involve a variety of procedures. For instance, a system may store all the previous data that was used to create the existing HMM and then, each time another character string is entered, create a new HMM based on the combination of the previous data and the new data.
- a system may store only the parameters of the HMM, without storing the actual previous data. Then, the system may update the model using new data by assigning a particular weight to both the old parameters and new data and then creating a new model. Alternatively, the system may only adjust individual models based on new data.
- a system may periodically prompt users to enter predefined input strings.
- a system may, after receiving input data, prompt a user to confirm the intended input (for example, by selecting the input from a list of possible user-inputs or by entering the intended input by a different input procedure).
- a confirmation may be implicit. For instance, if a computing system allows a user to correct mistakes made in inferring inputs, than the user-input that was incorrectly inferred may be associated with its intended input.
- exemplary method 100 involves selecting one or more possible user-inputs based on the determined likelihoods.
- a system may further base this selection on user-preferences, user-input, context data, or other factors.
- user-preferences may indicate a threshold level of likelihood that must be reached for a potential user-input to be selected.
- user-input may specify a particular number of potential user-inputs to select.
- the context in which a string is used e.g., if the structure of the sentence before an input word indicates that the word should be a particular element of language, etc.
- FIG. 2D shows user-interface 200 , including graphical element 201 that further includes several input regions (for example, input regions 202 A-D) and a viewing region 203 . Additionally, FIG. 2D shows input movement 220 of pointing element 206 . Since input movement 220 moves across several input regions, this input movement may represent one of several possible user-inputs. By applying an HMM to input movement 220 , the computing system has selected several more-likely possible user-inputs. As shown, a set of these possible user-inputs 208 is displayed in viewing region 203 .
- a visual display may include only a single possible user-input.
- the system may display this user-input by adding the selected user-input to an open file or program window. For example, if the user-input is part of an SMS message, then the system may simply add the selected user-input to the message text as the system would add any other user-input.
- FIG. 2D shows possible user-inputs 208 in viewing region 203 .
- a system may receive further user-input indicating one user-input to use as the intended input.
- user-interface 200 includes, in viewing region 203 , a prompt 210 asking a user which word represents the intended user-input.
- an exemplary embodiment may involve the computing system selecting a single user-input to use as the intended user-input.
- the computing system may select the most likely user-input, based on the hidden Markov modeling as well as other factors described in previous sections.
- the system may also display the possible user-inputs in a ranked order so that the most likely user-input is placed prominently.
- the most likely user-input may be “TRACK”. Therefore, “TRACK” is displayed in the upper left-hand corner of the list of possible user-inputs.
- An exemplary system may list selected user-inputs in other ways as well (e.g., alphabetical order, string length order, etc.)
- possible user-input may be selected from predefined lists of possible user-inputs, newly defined user-inputs based on good agreement to the model, or a combination of both. If a computing system selects a combination of predefined and newly-defined user-inputs, the computing system may draw a distinction between these types of possible user-inputs.
- user-interface 200 and graphical display 201 may be integrated into a system that is configured for receiving, transmitting, and displaying data, such as a wearable computing device.
- a wearable computing device is a head-mounted display (HMD).
- An HMD typically provides a heads-up display near to the user's eyes in such a manner that the user perceives the computer-generated graphics and the physical world simultaneously.
- the heads-up display may include a graphical display.
- FIG. 4A illustrates an example system 400 for receiving, transmitting, and displaying data.
- the system 400 is shown in the form of a wearable computing device. While FIG. 4A illustrates a head-mounted device 402 as an example of a wearable computing device, other types of wearable computing devices could additionally or alternatively be used.
- the head-mounted device 402 has frame elements including lens-frames 404 , 406 and a center frame support 408 , lens elements 410 , 412 , and extending side-arms 414 , 416 .
- the center frame support 408 and the extending side-arms 414 , 416 are configured to secure the head-mounted device 402 to a user's face via a user's nose and ears, respectively.
- Each of the frame elements 404 , 406 , and 408 and the extending side-arms 414 , 416 may be formed of a solid structure of plastic and/or metal, or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through the head-mounted device 402 . Other materials may be possible as well.
- each of the lens elements 410 , 412 may be formed of any material that can suitably display a projected image or graphic.
- Each of the lens elements 410 , 412 may also be sufficiently transparent to allow a user to see through the lens element. Combining these two features of the lens elements may facilitate an augmented reality or heads-up display where the projected image or graphic is superimposed over a real-world view as perceived by the user through the lens elements 410 , 412 .
- the extending side-arms 414 , 416 may each be projections that extend away from the lens-frames 404 , 406 , respectively, and may be positioned behind a user's ears to secure the head-mounted device 402 to the user.
- the extending side-arms 414 , 416 may further secure the head-mounted device 402 to the user by extending around a rear portion of the user's head.
- the system 400 may connect to or be affixed within a head-mounted helmet structure. Other possibilities exist as well.
- the system 400 may also include an on-board computing system 418 , a video camera 420 , a sensor 422 , and a finger-operable touch pad 424 .
- the on-board computing system 418 is shown to be positioned on the extending side-arm 414 of the head-mounted device 402 ; however, the on-board computing system 418 may be provided on other parts of the head-mounted device 402 or may be positioned remote from the head-mounted device 402 (e.g., the on-board computing system 418 could be connected by wires or wirelessly connected to the head-mounted device 402 ).
- the on-board computing system 418 may include a processor and memory, for example.
- the on-board computing system 418 may be configured to receive and analyze data from the video camera 420 , the sensor 422 , and the finger-operable touch pad 424 (and possibly from other sensory devices, user-interfaces, or both) and generate images for output by the lens elements 410 and 412 .
- the on-board computing system 418 may additionally include a speaker or a microphone for user input (not shown).
- the video camera 420 is shown positioned on the extending side-arm 414 of the head-mounted device 402 ; however, the video camera 420 may be provided on other parts of the head-mounted device 402 .
- the video camera 420 may be configured to capture images at various resolutions or at different frame rates. Video cameras with a small form-factor, such as those used in cell phones or webcams, for example, may be incorporated into an example embodiment of the system 400 .
- FIG. 4A illustrates one video camera 420
- more video cameras may be used, and each may be configured to capture the same view, or to capture different views.
- the video camera 420 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by the video camera 420 may then be used to generate an augmented reality where computer generated images appear to interact with the real-world view perceived by the user.
- the sensor 422 is shown on the extending side-arm 416 of the head-mounted device 402 ; however, the sensor 422 may be positioned on other parts of the head-mounted device 402 .
- the sensor 422 may include one or more of a gyroscope or an accelerometer, for example. Other sensing devices may be included within, or in addition to, the sensor 422 or other sensing functions may be performed by the sensor 422 .
- the finger-operable touch pad 424 is shown on the extending side-arm 414 of the head-mounted device 402 . However, the finger-operable touch pad 424 may be positioned on other parts of the head-mounted device 402 . Also, more than one finger-operable touch pad may be present on the head-mounted device 402 .
- the finger-operable touch pad 424 may be used by a user to input commands.
- the finger-operable touch pad 424 may sense at least one of a position and a movement of a finger via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities.
- the finger-operable touch pad 424 may be capable of sensing finger movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the pad surface.
- the finger-operable touch pad 424 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 424 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when the user's finger reaches the edge, or other area, of the finger-operable touch pad 424 . If more than one finger-operable touch pad is present each finger-operable touch pad may be operated independently, and may provide a different function.
- FIG. 4B illustrates an alternate view of the system 400 illustrated in FIG. 4A .
- the lens elements 410 , 412 may act as display elements.
- the head-mounted device 402 may include a first projector 428 coupled to an inside surface of the extending side-arm 416 and configured to project a display 430 onto an inside surface of the lens element 412 .
- a second projector 432 may be coupled to an inside surface of the extending side-arm 414 and configured to project a display 434 onto an inside surface of the lens element 410 .
- the lens elements 410 , 412 may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors 428 , 432 .
- a reflective coating may be omitted (e.g., when the projectors 428 , 432 are scanning laser devices).
- the lens elements 410 , 412 themselves may include: a transparent or semi-transparent matrix display, such as an electroluminescent display or a liquid crystal display, one or more waveguides for delivering an image to the user's eyes, or other optical elements capable of delivering an in focus near-to-eye image to the user.
- a corresponding display driver may be disposed within the frame elements 404 , 406 for driving such a matrix display.
- a laser or light emitting diode (LED) source and scanning system could be used to draw a raster display directly onto the retina of one or more of the user's eyes. Other possibilities exist as well.
- FIG. 5A illustrates an example system 500 for receiving, transmitting, and displaying data.
- the system 500 is shown in the form of a wearable computing device 502 .
- the wearable computing device 502 may include frame elements and side-arms such as those described with respect to FIGS. 4A and 4B .
- the wearable computing device 502 may additionally include an on-board computing system 504 and a video camera 506 , such as those described with respect to FIGS. 4A and 4B .
- the video camera 506 is shown mounted on a frame of the wearable computing device 502 ; however, the video camera 506 may be mounted at other positions as well.
- the wearable computing device 502 may include a single display 508 which may be coupled to the device.
- the display 508 may be formed on one of the lens elements of the wearable computing device 502 , such as a lens element described with respect to FIGS. 4A and 4B , and may be configured to overlay computer-generated graphics in the user's view of the physical world.
- the display 508 is shown to be provided in a center of a lens of the wearable computing device 502 ; however, the display 508 may be provided in other positions.
- the display 508 is controllable via the computing system 504 that is coupled to the display 508 via an optical waveguide 510 .
- FIG. 5B illustrates an example system 520 for receiving, transmitting, and displaying data.
- the system 520 is shown in the form of a wearable computing device 522 .
- the wearable computing device 522 may include side-arms 523 , a center frame support 524 , and a bridge portion with nosepiece 525 .
- the center frame support 524 connects the side-arms 523 .
- the wearable computing device 522 does not include lens-frames containing lens elements.
- the wearable computing device 522 may additionally include an on-board computing system 526 and a video camera 528 , such as those described with respect to FIGS. 4A and 4B .
- the wearable computing device 522 may include a single lens element 530 that may be coupled to one of the side-arms 523 or the center frame support 524 .
- the lens element 530 may include a display such as the display described with reference to FIGS. 4A and 4B , and may be configured to overlay computer-generated graphics upon the user's view of the physical world.
- the single lens element 530 may be coupled to a side of the extending side-arm 523 .
- the single lens element 530 may be positioned in front of or proximate to a user's eye when the wearable computing device 522 is worn by a user.
- the single lens element 530 may be positioned below the center frame support 524 , as shown in FIG. 5B .
- FIG. 6 shows an example system for dynamically controlling a virtual input area. More particularly, FIG. 6 shows an example portable computing system 602 that includes a user-interface 604 . It should be understood, however, that example computing system 602 is shown for purposes of example and explanation only, and should not be taken to be limiting.
- Example computing system 602 is shown in the form of a cell phone that includes user-interface 604 . While FIG. 6 depicts cell phone 602 as an example of a portable computing system, other types of portable computing systems could additionally or alternatively be used (e.g. a tablet device, among other examples). As illustrated in FIG. 6 , cell phone 602 includes a rigid frame 606 , a plurality of input buttons 608 , and user-interface 604 .
- User-interface 604 may be a touchscreen, having a touchpad configured to receive touch inputs embedded into a graphical display, and may be arranged to depict various input areas. Alternatively, user-interface 604 may be a trackpad, having a touchpad configured to receive touch inputs, but no graphical display.
- the example computing system 602 may include plurality of input buttons 608 as well as user-interface 604 , although this is not necessary. In another embodiment, for example, computing system 602 may include only user-interface 604 and not plurality of buttons 608 . Other embodiments of computing system 602 may be possible as well.
- FIG. 7 shows a simplified block diagram depicting example components of an example computing system 700 .
- Computing system 700 may include at least one processor 702 and system memory 704 .
- computing system 700 may include a system bus 706 that communicatively connects processor 702 and system memory 704 , as well as other components of computing system 700 .
- processor 702 can be any type of processor including, but not limited to, a microprocessor (g), a microcontroller (X), a digital signal processor (DSP), or any combination thereof.
- system memory 704 can be of any type of memory now known or later developed including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof.
- An example computing system 700 may include various other components as well.
- computing system 700 includes an A/V processing unit 708 for controlling graphical display 710 and speaker 712 (via A/V port 714 ), one or more communication interfaces 716 for connecting to other computing devices 718 , and a power supply 720 .
- Graphical display 710 may be arranged to provide a visual depiction of various input regions provided by user-interface 200 , such as the depiction provided by graphical display 201 .
- user-interface 200 may be compatible with one or more additional user-interface devices 728 as well.
- computing system 700 may also include one or more data storage devices 724 , which can be removable storage devices, non-removable storage devices, or a combination thereof.
- removable storage devices and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and/or any other storage device now known or later developed.
- Computer storage media can include volatile and nonvolatile, removable and non-removable media
- Computing system 700 communicates using a communication link 716 (e.g., a wired or wireless connection) to a remote device 718 .
- the remote device 718 may be any type of computing device or transmitter including a laptop computer, a mobile telephone, or tablet computing device, etc., that is configured to transmit data to computing system 700 .
- the remote device 718 and the computing system 700 may contain hardware to enable the communication link 716 , such as processors, transmitters, receivers, antennas, etc.
- the communication link 716 is illustrated as a wireless connection; however, wired connections may also be used.
- the communication link 716 may be a wired serial bus such as a universal serial bus or a parallel bus, among other connections.
- the communication link 716 may also be a wireless connection using, e.g., Bluetooth® radio technology, communication protocols described in IEEE 802.11 (including any IEEE 802.11 revisions), Cellular technology (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), or Zigbee® technology, among other possibilities. Either of such a wired and/or wireless connection may be a proprietary connection as well.
- the remote device 730 may be accessible via the Internet and may include a computing cluster associated with a particular web service (e.g., social-networking, photo sharing, address book, etc.).
- the users may be provided with an opportunity to opt in/out of programs or features that involve such personal information (e.g., information about a user's preferences or a user's contributions to social content providers).
- certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be anonymized so that no personally identifiable information can be determined for the user and so that any identified user preferences or user interactions are generalized (for example, generalized based on user demographics) rather than associated with a particular user.
- An example of one such other computing system may include a motion-sensing input device that is configured to provide a virtual keyboard.
- the motion-sensing input device may include a camera and the camera may be arranged to detect input movements that correspond to a user's gestures. Such gestures may be made “in the air,” in front of the camera such that the camera is capable of observing the gestures.
- the input movement may comprise depth-based input movements.
- the user may move a first hand through the air to select various virtual input areas of the virtual keyboard. Further, in an embodiment, the user may initially move a hand “forward” to indicate that the user would like to begin an input gesture. And, in an embodiment, the user may move the hand “backward” to indicate the user is done with the input gesture.
- the user's other hand may be used for additional and/or alternative input functions. For instance, a user may use the other hand to select and/or disambiguate between “possibly intended” inputs. Additionally, the other hand may be used to select punctuation. Other examples of uses for the other hand may exist as well.
- a “fumble method” may be carried out prior to, or concurrent with, the execution of the blocks described with respect to FIG. 1 .
- Such a fumble method may enable the system to identify, and ultimately disregard, touch inputs that the user may not intend to be interpreted by the computing system as a part of an intended character-string input.
- a user may “fumble” (or jitter) when initially coming into contact with a touch surface (i.e., the user may incidentally contact the surface prior to an intended input).
- the user may stray from an intended input during input of an intended character-string. In some cases it would be desirable if such a “fumble” (or jitter) were disregarded by the computing system.
- the computing system may be configured to simplify the input stroke by removing extraneous data, such as jitter, so as to facilitate easier comparison with the predefined strokes.
- the computing system may be configured to simplify the input stroke by identifying dwell points 806 , shown in FIG. 8 , in the input stroke, such as the individual letters of the word “STROKE” 804 , and generating a polyline that has those dwell points as its vertices.
- dwell points may be detected when the user dwells on a letter for some detectible extent of time. Due to the nature of how the user targets individually the letters of a word, the user may pause slightly, thereby generating dwell times 902 associated with the detected dwell points.
- the computing system may then compare that resulting simplified stroke with predefined strokes to find a closest match. For instance, the computing system may take the N generated equidistant points along the simplified stroke, and compare those points pairwise with the N points representing each predefined stroke. For the pairwise point comparison, the computing system may be configured to evaluate an average sum of the squares of the distances between the paired points of the simplified and predefined strokes, and to select the predefined stroke for which the least difference exists. Further, a language model could be used to resolve ambiguity in this selection process.
- the computing system may be configured to analyze the gesture stroke to determine dwelling times 902 , thereby identifying corresponding dwell points, as shown in FIG. 9 .
- the computing system may further evaluate distances separating sequential pairs of sensor input points, which are separated in time by the constant rate provisioning. That is, inter-coordinate distances for consecutive pairs of coordinates are evaluated.
- the computing system may then be configured to generate a histogram based on these inter-coordinate distances. Because closer or more proximate sequential pairs of input points 808 are located near dwell points 806 , as shown in FIG. 8 , the generated histogram may have an “L” shape. As shown in FIG. 10 , by connecting successive columns 1004 of the generated histogram 1002 , a curve 1004 , having an elbow 1006 , is generated. Points with coordinates near those of the letters of interest are to the left of elbow 1010 .
- the computing system determines a dwell point threshold distance, such that points that are located at a distance less than or equal to the threshold distance, are determined to be potentially associated with letters of the intended word, and the remaining points are discarded. Consecutive sets of points associated with potential letters of the intended word are grouped together in a sequence of points. When the endpoints of the sequence are not identical, the computing system reduces the sequence to the endpoints. When the endpoints are identical, the computing system reduces the sequence to a single endpoint. The computing system further combines the endpoints together to reduce the user's input stroke to a set of straight line segments, which may be referred to as a simplified stroke. The computing system is configured to re-sample the simplified stroke to identify N equidistant points, as discussed above, in order to perform a comparison between the simplified stroke and the set of predefined stroke templates.
- the computing system is configured to avoid comparing against all stroke templates, as that could be prohibitively costly in time and unnecessary in most cases. Because the user lifts his/her finger when close to the last letter of the intended word, the computing system may filter out the predefined stroke templates for words that do not end near the coordinates as that of the evaluated simplified stroke. For each remaining stroke template, as stated above, N equidistant points are compared one to one to those of the simplified stroke. The computing system may then use the above-discussed average sum of the square of the distances between each pair of points, with one pair point associated with simplified stroke and the other pair point associated with the remaining stroke template, as an error metric for matching each remaining template to the simplified stroke.
- the remaining set of stroke templates is then ordered using the error metric in ascending order.
- the computing system enters the corresponding word without requiring the user to manually select the best matching stroke template.
- the computing system may present the user with a list of top words, corresponding to lowest error metrics. The user may then select the intended word, if on the list.
- the computing system is configured to measure a total length of the simplified stroke, and to filter out predefined stroke templates that have a total length that is greater than a predetermined length difference threshold.
- the computing system is configured to recognize that the user may land on the keyboard at a location other than that of the first letter of the intended word. In this event, the computing system may determine a line segment from the landing location to the first subsequent determined dwell point, and may add that line segment to the beginning of each predefined stroke template with which the simplified stroke is to be compared. As such, when the landing location is not the intended location, the computing system may still perform the matching of the simplified stroke with a predefined stroke template. When the landing location is substantially near the intended location, the computing system may disregard the substantially negligible line segment, as its impact on the stroke comparison is minimal.
- the computing system may disregard the line segments from and to the spacebar, i.e., from the last dwell point and to a first subsequent dwell point.
- the computing system may be configured to generate a visible response to the user's finger or stylus interaction with the touch-based interface. For example, when the user's finger or stylus lands, the computing system may animate a collapsing circle around the landing location, like a reverse sonar wave. This circle animation can help the user figure out where he/she has contacted the touch-based interface.
- an empty circle can be represented.
- the circle may start to fill in from its center or may fill in like a growing slice of a pie or a sweeping clock hand.
- the circle may fade when the user dwells on a particular letter.
Abstract
Description
Claims (46)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/632,042 US9383919B2 (en) | 2012-01-06 | 2012-09-30 | Touch-based text entry using hidden Markov modeling |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261584213P | 2012-01-06 | 2012-01-06 | |
US13/632,042 US9383919B2 (en) | 2012-01-06 | 2012-09-30 | Touch-based text entry using hidden Markov modeling |
Publications (2)
Publication Number | Publication Date |
---|---|
US20150177981A1 US20150177981A1 (en) | 2015-06-25 |
US9383919B2 true US9383919B2 (en) | 2016-07-05 |
Family
ID=53400051
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/632,042 Active 2034-07-23 US9383919B2 (en) | 2012-01-06 | 2012-09-30 | Touch-based text entry using hidden Markov modeling |
Country Status (1)
Country | Link |
---|---|
US (1) | US9383919B2 (en) |
Families Citing this family (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU343992S (en) * | 2012-02-08 | 2012-08-21 | Lg Electronics Inc | Television receiver |
KR102100911B1 (en) * | 2013-08-30 | 2020-04-14 | 엘지전자 주식회사 | Wearable glass-type device, systme habving the samde and method of controlling the device |
USD771646S1 (en) * | 2014-09-30 | 2016-11-15 | Apple Inc. | Display screen or portion thereof with graphical user interface |
WO2017096096A1 (en) * | 2015-12-01 | 2017-06-08 | Quantum Interface, Llc. | Motion based systems, apparatuses and methods for establishing 3 axis coordinate systems for mobile devices and writing with virtual keyboards |
US10884610B2 (en) * | 2016-11-04 | 2021-01-05 | Myscript | System and method for recognizing handwritten stroke input |
US11003839B1 (en) | 2017-04-28 | 2021-05-11 | I.Q. Joe, Llc | Smart interface with facilitated input and mistake recovery |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7042442B1 (en) | 2000-06-27 | 2006-05-09 | International Business Machines Corporation | Virtual invisible keyboard |
US20060262115A1 (en) * | 2005-05-02 | 2006-11-23 | Shapiro Graham H | Statistical machine learning system and methods |
US20070040813A1 (en) * | 2003-01-16 | 2007-02-22 | Forword Input, Inc. | System and method for continuous stroke word-based text input |
US20110210850A1 (en) * | 2010-02-26 | 2011-09-01 | Phuong K Tran | Touch-screen keyboard with combination keys and directional swipes |
US20110254765A1 (en) * | 2010-04-18 | 2011-10-20 | Primesense Ltd. | Remote text input using handwriting |
-
2012
- 2012-09-30 US US13/632,042 patent/US9383919B2/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7042442B1 (en) | 2000-06-27 | 2006-05-09 | International Business Machines Corporation | Virtual invisible keyboard |
US20070040813A1 (en) * | 2003-01-16 | 2007-02-22 | Forword Input, Inc. | System and method for continuous stroke word-based text input |
US7382358B2 (en) | 2003-01-16 | 2008-06-03 | Forword Input, Inc. | System and method for continuous stroke word-based text input |
US20060262115A1 (en) * | 2005-05-02 | 2006-11-23 | Shapiro Graham H | Statistical machine learning system and methods |
US20110210850A1 (en) * | 2010-02-26 | 2011-09-01 | Phuong K Tran | Touch-screen keyboard with combination keys and directional swipes |
US20110254765A1 (en) * | 2010-04-18 | 2011-10-20 | Primesense Ltd. | Remote text input using handwriting |
Also Published As
Publication number | Publication date |
---|---|
US20150177981A1 (en) | 2015-06-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20210406578A1 (en) | Handwriting-based predictive population of partial virtual keyboards | |
JP6655064B2 (en) | Disambiguation of keyboard input | |
JP6987067B2 (en) | Systems and methods for multiple input management | |
US9547430B2 (en) | Provision of haptic feedback for localization and data input | |
US9383919B2 (en) | Touch-based text entry using hidden Markov modeling | |
US9064436B1 (en) | Text input on touch sensitive interface | |
US8316319B1 (en) | Efficient selection of characters and commands based on movement-inputs at a user-inerface | |
JP6914260B2 (en) | Systems and methods to beautify digital ink | |
US10996843B2 (en) | System and method for selecting graphical objects | |
EP3644163B1 (en) | Temporal based word segmentation | |
US11656762B2 (en) | Virtual keyboard engagement | |
KR20210033394A (en) | Electronic apparatus and controlling method thereof | |
US20180143964A1 (en) | Data input system using trained keypress encoder | |
EP3535652B1 (en) | System and method for recognizing handwritten stroke input | |
WO2016018518A1 (en) | Optical tracking of a user-guided object for mobile platform user input | |
US9021379B1 (en) | Matching of gesture keyboard strokes with predefined stroke templates | |
KR20130010252A (en) | Apparatus and method for resizing virtual keyboard | |
US9310997B2 (en) | Method, touch device and computer program product for converting touch points into characters | |
JP7392315B2 (en) | Display device, display method, program | |
WO2022071448A1 (en) | Display apparatus, display method, and program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:STARNER, THAD;PATEL, NIRMAL;ZHAI, SHUMIN;SIGNING DATES FROM 20121025 TO 20121126;REEL/FRAME:029347/0382 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: CORRECTIVE ASSIGNMENT TO CORRECT THE ASSIGNEE'S ZIP CODE RECORDED ON REEL 029347, FRAME 0382;ASSIGNORS:STARNER, THAD;PATEL, NIRMAL;ZHAI, SHUMIN;SIGNING DATES FROM 20121025 TO 20121126;REEL/FRAME:029459/0915 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |