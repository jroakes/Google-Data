CN112368694A - Context estimation of link information gain - Google Patents
Context estimation of link information gain Download PDFInfo
- Publication number
- CN112368694A CN112368694A CN201880095102.2A CN201880095102A CN112368694A CN 112368694 A CN112368694 A CN 112368694A CN 201880095102 A CN201880095102 A CN 201880095102A CN 112368694 A CN112368694 A CN 112368694A
- Authority
- CN
- China
- Prior art keywords
- user
- documents
- information
- document
- new
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3344—Query execution using natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3347—Query execution using vector based model
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/338—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/93—Document management systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/40—Document-oriented image-based pattern recognition
- G06V30/41—Analysis of document content
- G06V30/418—Document matching, e.g. of document images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
Abstract
Techniques for determining information gain scores for one or more documents of interest to a user and presenting information from the documents based on the information gain scores are described herein. The information gain score for a given document indicates that other information included in the document exceeds information contained in documents previously viewed by the user. In some implementations, the information gain scores for one or more documents may be determined by applying data from the documents across a machine learning model to generate the information gain scores. Based on the information gain scores for the set of documents, the documents may be provided to the user in a manner that reflects the possible information gains that the user may be able to obtain if viewing the documents.
Description
Background
Information from one or more documents of a shared topic may be provided to an individual (also referred to herein as a "user") via one or more computing devices. For example, a user may submit a search request and be provided with a collection of documents and/or links to documents in response to the submitted search request. Also, for example, documents may be provided to a user based on the user's identified interests, the user's previously viewed documents, and/or other criteria that may be used to identify and provide documents of interest. Information from the documents may be provided via, for example, an automated assistant and/or provided as a result to a search engine. Further, information from the document may be provided to the user in response to the search request and/or may be automatically served to the user based on continuing the search after the user has ended the search session.
In some cases, a subset of the information may be extracted from the document for presentation to the user. For example, when a user engages in a spoken human-machine conversation through an automatic assistant software process (also referred to as a "chat bot," "interactive personal assistant," "intelligent personal assistant," "personal voice assistant," "session proxy," "virtual assistant," etc.), the automatic assistant may perform various types of processing to extract salient information from the document so that the automatic assistant may present the information in an abbreviated form. As another example, in response to a user's search query, some search engines will provide summary information from one or more responsive and/or related documents in addition to or in lieu of links to responsive and/or related documents.
However, when identifying a set of documents that share a topic, many documents may include similar information. For example, a user may submit a search related to solving a computer problem (e.g., a failure, malware, configuration, etc.), and may then be provided with a plurality of documents including a similar list of solutions, remedial steps, resources, etc. Thus, while two documents sharing a topic may be relevant to a user's request or interest, after the same or similar information in a first document or set of documents has been viewed, the user may be less interested in viewing a second document.
Disclosure of Invention
Embodiments described herein relate to determining information gain scores for one or more documents of potential interest to a user and presenting information from one or more of the documents selected based on their respective information gain scores. The information gain score for a given document indicates additional information included in the given document beyond that contained in other documents already presented to the user. Information from the document can be presented to the user in a variety of ways, such as opening the entire document (e.g., in a web browser or other suitable software application), audibly reading the entire content of the document to the user, extracting and audibly/visually presenting to the user the salient information extracted from the document, and so forth.
In some implementations, the information gain scores of one or more documents may be determined by applying data indicative of the documents, such as their entire content, prominently extracted information, semantic representations (e.g., embeddings, feature vectors, bag of words representations, histograms generated from words/phrases in the documents, etc.), across a machine learning model to generate the information gain scores. For example, in some implementations, data indicative of one or more previously presented documents and data indicative of one or more not yet presented (or "new") documents may be applied as inputs across a trained machine learning model to generate an output indicative of information gain scores for one or more new documents. Based on the information gain scores, information contained in one or more new documents may be selectively provided to the user in a manner that reflects the likely information gain that the user may obtain if the user presented information from the selected document.
In some implementations, a first set of documents that have been presented to a user (e.g., viewed by the user, audibly rendered in whole or in part, etc.) is identified. For example, documents of the first collection share a common topic and may be identified based on previous offers to the user by the automated assistant in response to natural language input by the user, in a web browser, in another applicable software application, and so forth. The user may search for topics and may return one or more document responsive files (e.g., as a search result list and/or queued by an automated assistant for potential presentation). As an example, the user may ask the automated assistant "Give me information about repairing my computer", and the automated assistant may provide data indicating one or more responsive (and/or related) files via a visual interface or via text-to-speech. The first responsive document may include information related to troubleshooting a software application problem, and the user may view and/or listen to the content of the first document. Further, the set may include a second document that includes hardware repair information, and the user may additionally view the second document.
A second set of documents is identified that is also related to the topic of the first set of documents, but has not yet been viewed by the user. For example, a document that includes information related to hardware and software repairs may be identified. For each new document in the second set of documents, an information gain score is determined that indicates whether, for the new document, the new document includes information that is not contained in the documents of the first set of documents (i.e., the documents for which information has been presented to the user). For example, if a user has previously viewed a document that includes information about common software application fixes, the information gain score of a document that includes information related to general computer repair (possibly including software troubleshooting information) may be less indicative of an information gain.
Based on the information gain scores, one or more new documents may be selected to provide to the user and/or the new documents may be ranked based on their respective information scores. For example, ten documents may be identified in the second set of documents, and information for the document having the information gain score that is at most indicative of the information gain may be presented to the user by the automated assistant. In some implementations, the new documents may be ranked and the second set of documents may be re-ranked based on new information gain scores as the user views additional documents. For example, the information scores for the second set of documents may be recalculated based on one or more new documents presented to the user, and thus the first set of documents may be moved from the second set.
In some implementations, data from each document of the second set of documents may be applied as input across the machine learning model, e.g., in conjunction with data from the first set of documents, and the information gain score may be determined based on an output of the machine learning model. In some implementations, rather than applying the content of the documents themselves across machine learning models, alternative representations of the documents, such as semantic feature vectors or embeddings, "bag of words" representations, etc., may be generated from each document and applied as input across machine learning models.
In some implementations, one or more new documents in the second set may be presented in a manner selected based on the information gain scores. For example, one or more new documents may be rendered as part of a search results interface that is presented to a user in response to a query that includes topics of documents (such as references to one or more documents). In some implementations, the search results can be ranked based at least in part on their respective information gain scores. The user may then select one of the references and the information contained in the particular document may be presented to the user. Subsequently, the user may return the search results and may again be provided with a reference to the document, but the document may be updated based on the new information gain scores of the referenced document. In some implementations, the references can be re-ranked and/or one or more documents can be excluded (or significantly degraded) from the search results based on new information gain scores determined based on documents that the user has viewed. When the user views the attached document, the updated information gain score will be updated to reflect the information that the user has viewed as well as the information contained in the unviewed document.
The foregoing is provided as an overview of some embodiments disclosed herein. Further description of these and other embodiments is provided below.
In some implementations, there is provided a method performed by one or more processors, the method comprising: identifying a first set of one or more documents that share a topic, wherein information extracted from each document in the first set by an automated assistant executing at least in part on one or more computing devices operated by a user in response to free-form natural language input from the user was previously presented to the user; identifying a second set of a plurality of new documents that also share the topic, wherein the automatic assistant has not presented to the user the information contained in each new document in the second set; determining an information gain score for each new document in the second set, wherein the information gain score for a given new document of the second set indicates additional information that the user would obtain beyond the information extracted from one or more documents in the first set if the information contained in the given new document were extracted and presented to the user; selecting a new document from the second set based on the information gain scores; and presenting, via one or more computing devices, the information extracted from the selected new document to the user.
In various implementations, the method may further include ranking the second set of new documents based on the information gain scores, wherein the selecting is based on the ranking. In various implementations, determining the information gain score for the given new document may include: applying, across a machine learning model, first data indicative of the information extracted from the first set of documents and second data indicative of information extracted from the given new document to generate an output, wherein the information gain score is determined based on the output.
In various embodiments, the first data may include a semantic feature vector generated from the information extracted from the first set of documents, and the second data may include a semantic feature vector generated from the information extracted from the given new document.
In another aspect, a method implemented using one or more processors may comprise: identifying a first set of one or more documents that share a topic, wherein each document in the first set is accessed by a user on one or more computing devices operated by the user; identifying a second set of a plurality of new documents that also share the topic, wherein the user has not previously accessed the plurality of new documents in the second set; determining an information gain score for each new document of the second set, wherein the information gain score for a given new document of the second set indicates that the user would obtain additional information beyond that obtained by the user when accessing the first set of one or more documents if the user accessed the given new document; selecting a new document from the second set based on the information gain scores; and presenting, via one or more computing devices, information to the user indicating one or more new documents in the second set, wherein the information indicating one or more new documents in the second set is presented in a manner selected based on the information gain scores.
In various embodiments, the method may further comprise: presenting a ranked list of the second set of new documents, wherein the list is ranked based on the information gain scores. In various embodiments, identifying the first set may include: presenting references to one or more documents in the first set as part of a search results interface presented to the user in response to a query that includes the topic; receiving user input from the search results interface indicating a selection of a reference to a particular document of the first set, wherein at least a portion of the particular document is provided to the user in response to the selection; receiving user input indicating a command to return the search results interface; and in response to receiving the user input indicating the command to return the search results interface, rendering a reference to one or more documents in the second set as part of the search results interface.
In various implementations, rendering the reference to the second set of one or more documents may include excluding from the search results interface at least a reference to a particular document of the first set. In various implementations, determining the information gain score for the given new document may include: applying, across a machine learning model, first data indicative of the first set of documents and second data indicative of the given new document to generate an output, wherein the information gain score is determined based on the output.
In another related aspect, a method implemented using one or more processors may comprise: identifying a first document comprising a first information element and a second information element, wherein the first information element is different from the second information element; processing one or more portions of the document including the first information element and the second information element using text-to-speech ("TTS") processing to generate a TTS output conveying the first information element and the second information element; providing the TTS output at one or more computing devices operated by a user; identifying a second document comprising the second information element and a third information element, wherein the third information element is different from the first information element and the second information element; based on the providing, determining that the second information element has been communicated to the user; in response to the determination, processing one or more portions of the second document that include the third information element using additional TTS processing to generate additional TTS output that conveys the third information element and excludes the second information element; and providing the additional TTS output at one or more computing devices operated by a user.
Additionally, some embodiments include one or more processors (e.g., Central Processing Units (CPUs), Graphics Processing Units (GPUs), and/or Tensor Processing Units) (TPUs) of the one or more computing devices, wherein the one or more processors are operable to execute instructions stored in the associated memory and the instructions are configured to cause performance of any of the foregoing methods. Some embodiments also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the foregoing methods.
It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram of an example environment in which embodiments disclosed herein may be implemented.
FIG. 2 illustrates an interface for an automated assistant that may provide information in a document to a user based on an information gain score.
FIG. 3 illustrates an interface for a search interface that may provide a reference to a document based on the information gain score of the document.
FIG. 4 illustrates a document collection that is marked based on whether a user has viewed a document.
FIG. 5 illustrates a flow chart of an example method of presenting information extracted from a new document to a user based on an information gain score of the new document.
Fig. 6 illustrates an example architecture of a computing device.
Detailed Description
Turning now to fig. 1, an example environment is illustrated in which the techniques disclosed herein may be implemented. The example environment includes a client device 105 and a remote computer 110. Although both client device 105 and remote computer 110 are illustrated in fig. 1 as single components, it is to be understood that one or more modules and/or any aspect may be implemented in whole or in part by one or more other devices. For example, in some implementations, a first set of modules and/or aspects are implemented by one or more processors of a first remote system, and a second set of modules and/or aspects are implemented by one or more processors of one or more separate remote server devices in network communication with remote computer 110. The remote server device may be, for example, a cluster of high-performance remote server devices that process requests from one or more client devices as well as requests from additional devices.
The client device 105 may be a mobile phone computing device, a tablet computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), and/or a wearable device of a user, including a computing device (e.g., a watch of a user having a computing device, glasses of a user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided. Further, one or more components of client device 105 may be implemented on a separate device. For example, the microphone 108 and/or the automated assistant client 106 may be implemented on one or more alternative computing devices in communication with the client device 105. Components of client device 105 and components of remote computer 1110 may communicate via a communication network. The communication network may include, for example, a Wide Area Network (WAN) (e.g., the internet). Further, a component of client device 105 may communicate with one or more other components via a communication network. For example, the communication network may include a Local Area Network (LAN) and/or bluetooth, and may communicate with one or more other devices (e.g., an automated assistant device that communicates with a user's handheld computing device) via the LAN and/or bluetooth.
Client device 105 includes a microphone 108, which microphone 108 allows a user to provide natural language input to one or more modules of client device 105. The user's utterance may be captured by the microphone 108, and one or more modules of the client device 105 may process the resulting audio data to generate text and/or other audio data, which may be further processed by one or more modules of the client device 105 and/or provided to the remote computer 110 for further processing via network communications. For example, the client device may include a speech-to-text ("STT") module (not depicted) that processes the captured audio into text for provision to the remote computer 110. Additionally or alternatively, the audio data received by the microphone 108 may be provided to a remote computer 110, which may include its own STT module (not depicted) for further processing of the audio data.
In some implementations, the audio data captured by the microphone 108 can include one or more topics of interest to the user. For example, a user may issue an audio phrase "Help me fix my computer" via microphone 108 to automated assistant client 106 executing on client device 105. The automated assistant client 106 may be part of an executing automated assistant engine having components on the client device 105 and components on the remote computer 110 (e.g., the remote automated assistant 115, also referred to as a "cloud-based automated assistant component"). Automated assistant client 106 may receive audio data captured by microphone 108 and/or from one or more other input devices available to the user. For example, a user may provide natural language input via a keyboard, which may then be provided to the automated assistant client 106 and/or the search interface 107, as described herein.
The automated assistant client 106 and the remote automated assistant 115 may process the user's natural language input and provide a response in the form of a conversation that includes one or more conversation rounds. A dialog turn may include, for example, a natural language input provided by a user and a response to the natural language input by an automated assistant. Thus, a dialog between the user and the automated assistant may be generated that allows the user to interact in a conversational manner with the automated assistant (the phrase "automated assistant" as used herein without a reference numeral may refer to any combination of 105 and 115). As an example, a user may submit the natural language input "Help me fix my computer" to automated assistant client 106. The automated assistant can process the natural language input and identify one or more documents (e.g., files related to computer maintenance) that are responsive to and/or relevant to the user's request. Automated assistant client 106 may provide one or more documents (or portions of one or more documents) via a graphical interface of client device 105. Additionally or alternatively, the automated assistant client 106 may include a text-to-speech ("TTS") module (not depicted) that converts a portion of the identified document to speech and renders the speech via a speaker (not depicted) of the client device 105. The user may then submit additional natural language input to the automated assistant client 106 for further refined topic searches, query other detailed information about the document, and/or submit new topics.
As used herein, a "conversation session" may include a logically self-contained exchange of one or more messages (or turns) between a user and an automated assistant (in some cases, other human participants). The automatic assistant may distinguish between multiple conversation sessions with the user based on various signals, such as the passage of time between sessions, a change in user context between sessions (e.g., location, before/during/after a scheduled meeting, etc.), the detection of one or more intervening interactions between the user and the client device other than the conversation between the user and the automatic assistant (e.g., the user switches applications for a while, the user walks away and then back to a stand-alone voice activated product), the locking/sleeping of the client device between sessions, changes in the client device for interfacing with one or more instances of the automatic assistant, etc.
For audio output and in particular, using the audio output of a TTS module, the information output may be considered linear or serial. That is, in a "dialog session," a dialog turn may include natural language input provided by the user and responses to the natural language input by the automated assistant, and thus the user may wait for substantially all responses to be output before proceeding. The user can passively receive audio information compared to reading, but the time taken for output is longer and the ability to scan or scroll/skip information is reduced. Thus, in the context of audio output, it is important that the output information is relevant, accurate and concise in order to avoid unnecessarily long outputs, redundant outputs or additional dialog turns. The information density of the output information is particularly important to improve the efficiency of the dialog session. The techniques described herein address these issues by reducing and/or eliminating the presentation of information that a user has provided for inclusion in the context of audio human-machine dialog.
The remote computer 110 further includes a search engine 120, the search engine 120 identifying one or more documents relevant to the search request. In some implementations, natural language input received by the remote automated assistant 115 can be converted to text via the STT module and provided to the search engine 120. In some implementations, the client device 105 includes a search interface that allows a user to submit text directly to the search engine 120. For example, the search interface 107 may be implemented via a web browser, and a user may submit natural language input via the search interface 107 for further processing by the search engine 120. The automated assistant may also submit natural language requests from the user to the search engine 120.
The search engine 120 identifies topics entered by the user and identifies documents responsive to and/or relevant to the input. As an example, a user may submit the natural language input "Help me fix my computer," and the search engine 120 may identify references to one or more topics in the search phrase to identify responsive documents and/or related documents. The identified documents may be scored via a variety of methods to determine the relevance of each document based on submitted natural language input, as well as the likelihood of the user being interested in the document. For example, in response to a user submitting the search phrase "Help me fix my computer," the search engine 120 may identify documents that include information related to computer hardware repairs, computer software repairs, common problems with computer software, troubleshooting guides for particular software applications, and/or other documents.
One or more of the identified documents, portions of the documents, and/or references to one or more documents may then be provided to the user based on the identified documents and the determined scores for each document. For example, in response to a user submitting the spoken phrase "Help me fix my computer," search engine 120 may identify a plurality of documents and may provide the highest scoring document to automated assistant client 106. A portion of the highest scoring document may be converted to speech via a TTS module, and then the automatic assistant client 106 may provide the portion of the document as audio in a dialog session between the user and the automatic assistant client 106. For example, the automated assistant may provide the first few sentences of the document that include troubleshooting guidelines for common software to the user as dialog turns in response to the user providing the natural language input "Help me fix my computer". Also, for example, additional information may be provided to the user from the document by submitting audio indicating an interest in continuing with the current document (e.g., "Tell me more"), or may indicate an interest in receiving information about other documents in future turns of the session (e.g., "What other information is found").
By way of example, referring to FIG. 2, an example interface for an automated assistant is provided. The interface includes a dialog session between the user and the automated assistant as text. However, in some implementations, the text for each turn of the conversation session may instead be a phrase provided by the user and/or the automated assistant client 106 during the audio conversation session. For example, turn 201 includes a user's natural language input that may be provided to automated assistant client 106 as audio input via microphone 108 or as text input via a keyboard. The response 202 of the automated assistant may be provided as audio via a speaker of the client device 105 or via a visual interface similar to the interface 200 of fig. 2. The response 202 includes information from the document that may be identified by the search engine 120 from the natural language input of the turn 201. Subsequently, the user indicates "What else had you found? (what did you find) as dialog turn 203, and then another dialog turn 204 is provided in response to dialog turn 203, which includes information from a second document that can be identified by search engine 120.
In some implementations, the first provided document (i.e., the document 202 having information provided in the turn 202) may have a generated score that is more indicative of the topic of the initially submitted phrase 201 than the second provided document. For example, the search engine 120 may rank the result response documents and initially and subsequently provide information from the first document for each ranked document so that the information provided to the user may be from a higher ranked document before a lower ranked document.
Additionally or alternatively, a list of documents and/or a list of references to documents identified by the search engine 120 may be provided to the user in response to natural language input that includes a topic. For example, referring to FIG. 3, an interface 300 for providing search results is illustrated. The search results include a reference to first document 301, a reference to second document 302, a reference to third document 303, and a reference to fourth document 304. The interface may be provided to the user's client device 105 through a component that shares one or more characteristics with the search interface 107. The user may select (e.g., clickable) each of the references, and when the user selects a reference, the search interface may provide the user with the relevant document, a portion of the document, and/or information extracted from the document. In addition, each selectable link includes information from the associated document to allow the user to view a portion or summary of the document before selecting a reference. For each reference, the search engine 120 may determine a score for the associated document and may provide the reference to the user via the interface 300 in a manner that reflects the score of the document. For example, first reference document 301 may have a generated score that indicates more relevance and/or relevance to a topic of interest to the user than second reference document 302, which second reference document 302 may have a score that indicates more of the same criteria than third reference document 303.
Referring back to FIG. 1, user document database 140 includes references to and/or documents that have been presented to a user in whole, in part, and/or in summary form. When a user is presented with information from a document, the user document database 140 may be updated to include information related to the presented document, such as entities in the document, identifiers of locations of the document, semantic vectors representing text of the document, and so forth. The data stored in the user document database 140 may be applied as input across the trained machine learning model, e.g., in conjunction with data indicative of one or more new documents, to determine information gain scores for the one or more new documents.
Referring to FIG. 4, a collection of documents 400 may be provided to a user, for example, in response to a search query provided to a search engine and/or in response to natural language input provided to an automated assistant. The collection 400 includes four documents that have not yet been presented to the user. These documents may be responsive documents and/or related documents provided in response to gesture queries, for example, in search results interface 300 of fig. 3, and/or documents related to and/or in response to natural language input provided by the user to the automated assistant and which have not been used by the automated assistant to provide information in the dialog turn of fig. 2.
Assume that the user is presented with information indicating document 1, such as during a dialog turn with an automated assistant or as a result of the user selecting a search result from an interface such as that depicted in fig. 3. Sets 401a and 401b depict how these four documents may be affected. Set 401a includes three documents of set 400 and set 401b includes one document of set 400. The collection 401a represents documents that have not been viewed by the user, and the collection 401b represents documents that have not been presented to the user (e.g., information in documents from the collection 401b provided in a dialog turn of the automated assistant and/or information in documents that the user has selected for reference in the interface of FIG. 3).
Now, assume that the user is next presented with information indicating document 2. Collections 402a and 402b describe how these four documents can be affected. The collection 402a includes two documents that have not yet been presented to the user, and the collection 402b includes two documents that have previously been presented to the user (e.g., after a subsequent dialog turn and/or after viewing the documents, returning to the reference of fig. 3 and selecting a different reference to the new document). In each case, as information from more documents is presented to the user, the user document database 140 may be updated to include information relating to each presented document. In some implementations, the reference in the user document database 140 may be stored after the search session has terminated. In some implementations, each session (i.e., each time a user provides a topic) will have a separate set of viewing documents that are not persistent through multiple topic submissions.
In various implementations, the information scoring engine 125 may be configured to determine an information score gain for each document identified by the search engine 120 that has not yet been presented to the user. For example, referring again to FIG. 4, the user has not yet viewed all of the documents of collection 400, and thus may not generate an information gain score for any document (or may assign an arbitrary score to each document that indicates an information gain score that is equal for all documents). Once the user has been presented with information from the documents in collection 401b, a new information gain score may be determined for each document in collection 401 a. For a given non-presented document, the generated information gain score indicates an information gain for the given non-presented document in view of documents already presented to the user. Thus, the information gain scores are calculated based on the documents already presented to the user and/or the information contained in those documents, taking into account the information contained in the new documents.
As an example, a user may provide the topic "Help me fix my computer" and be provided with the four documents of FIG. 4. The user may select document 1 and view the document. Based on viewing the documents, information gain scoring engine 125 may determine information gain scores for document 2, document 3, and document 4. Assuming that the user has viewed document 1, each information gain score indicates an information gain for that document. For example, document 1 may include common software troubleshooting techniques for repairing a computer, while document 2 may additionally include common software application issues that may be resolved by a user. However, document 3 may include common computing hardware troubleshooting techniques as well as software solutions. Finally, document 4 may only include hardware solutions to computer problems. Thus, because document 1 and document 2 have overlapping content that may be the same, information gain scoring engine 125 may determine fewer information gain scores indicative of the information gain of document 2 than documents 3 and 4, which may include information not included in document 1. Further, the score determined for document 4 may be more indicative of an information gain that the information in document 4 is not included at all in document 2 and document 3 (i.e., a gain of completely different information) than documents 2 and 3.
In various implementations, information to be provided to a user may be extracted from documents selected based on the information gain scores. In this way, the information provided to the user can be simplified by preferentially outputting new information. In some implementations, unnecessary output of duplicate or redundant information may be reduced by determining an information gain score. In embodiments including audio or TTS output as described above, a corresponding reduction in total output time, number of user interruptions or number of additional dialog turns may be provided.
In some implementations, the information gain scoring engine 125 can utilize documents that have been presented to the user as well as other documents related to the same topic as the input across the machine learning model. For example, the input to the machine learning model may include a pair of documents<d1,d2>Wherein d is1Is a document that has been consumed/presented, and d2Is a document that has not been consumed/presented. The output of the machine learning model may indicate the document d2Exceeding document d1The information gain score of (1). Various types of machine learning models may be employed, such as various types of neural networks (e.g., feedforward, convolution, etc.), support vector machines, bayesian classifiers, and so forth.
In some embodiments, documents that also include pairs may be used<d1,d2>Training data (or pairs of semantic representations such as embedded generated from pairs of documents) to train the machine learning model. Each pair of documents (or corresponding semantic representation pair) can be used with a second document d2The associated information gain score is marked (i.e. indicates if there is already present information from d1Is then presented by the user with the information from d2How much information the user should obtain). Training examples<d1,d2>Can be applied as input across the training models to generate output. The output may be compared to the labels assigned to the training examples to determine an error. The error may be used to train the machine learning model, for example, by using techniques such as gradient descent (e.g., stochastic, batch processing) and/or back propagation (in the case of neural networks).
In some such implementations, the labels assigned to the training examples may be generated manually. For example, one or more individuals may read a document and then provide a masterThe information gain score of the view, which indicates the d is being consumed1Thereafter at consumption d2How much additional (or novel) information is obtained in the list. The information gain scores, whether represented in training data labels or output by a trained machine learning model, may take various forms, such as values between zero and one, values along a numerical range, and so forth. This information gain score may be assigned as a label for a training example that includes a pair of documents consumed by the user.
Additionally or alternatively, in some embodiments, training data may be developed while one or more individuals simply search for and consume documents in their daily lives. As they view the successive documents returned in response to the search query, they may be asked questions (e.g., via a web browser plug-in) such as "What this document/information file in view of What you've availability read? (in view of what do you have read this document/information help you)? (is this document redundant. Training can then be performed in a similar manner as previously described.
In some implementations, a semantic representation of each document presented to the user and each other document identified by the search engine 120 but not presented to the user can be provided as an input to the machine learning model and the output of the machine learning model can be used to determine an information gain score for each unviewed document. For example, a representation of the documents of the collection 402b may be provided as input across the machine learning model and indicated as "not rendered", and a representation of two documents of the collection 402a may be provided as input and indicated as "previously rendered" (e.g., based on identifying documents that have been viewed in the user document database 140). The output from the machine learning model may be utilized to determine the information gain score for document 3 and the information gain score for document 4, assuming that the user has viewed document 1 and document 2, each of which indicates an information gain for that document. Further, if the user then views document 3 (or information is provided from document 3 by the automated assistant client 106), the new information gain score for document 4 may be determined, assuming that the information contained in the other three documents has already been presented to the user.
In some implementations, a semantic vector for each document can be determined and utilized as input across the machine learning model. Semantic vectors may be, for example, digital representations of documents as vectors, and may be used to determine similarities between documents based on proximity in vector space. In some implementations, the semantic vector may be generated by another trained machine learning model, such as a word2vec model.
For example, an auto-encoder (e.g., word2vec) may be trained to receive text as input, encode the semantic representation using an encoder portion, and then reconstruct the original text input using a decoder portion. Once trained, the encoder portion alone may be used to generate a semantic representation (e.g., a semantic vector) of the document, which may then be used as an input across the aforementioned trained machine learning. Thus, for each document identified by the search engine 120, a semantic vector may be generated. When a user views a document, semantic vectors may be provided as input across the machine learning model, where the tags of "viewed" and "not viewed" also change when the user views the document.
As previously described, in some implementations, the information gain annotation engine 130 can generate annotated training data based on annotations of a human curator of the document. For example, a curator may be provided with a first document and a second document (d described previously1And d2) And the curator may direct a second document d2An assignment value is assigned that indicates an information gain of the second document after viewing the first document. Further, for example, during a search session, a user in the session of the search interface 107 may be asked to rate and/or rank the information gain of the documents (e.g., pop-up asking the user to rate information obtained from viewed documents based on other documents viewed during the session). Annotations may be used to generate indications to be determined by a human curatorTraining data for a given information gain, and the training data may be provided as an input to a machine learning model to train the model to determine a meaningful output (e.g., a score indicative of the information gain for a given set of documents).
In some implementations, semantic representations of multiple documents that have been presented to a user can be applied as input across the model along with semantic representations of new documents. For example, a first set of n inputs may be retained for semantic features extracted from an already presented document, and a second set of m inputs may be retained for semantic features extracted from a new document. Thus, the overall dimension of the input is set to n + m. In some cases, if all n inputs of the first set are not needed (e.g., enough semantic features are not extracted), the inputs may be left empty or filled with arbitrary values (such as zero) to minimize or eliminate any impact they may have on the final output of the machine learning model.
In some implementations, documents that have not been viewed by the user may be re-ranked based on their information gain scores. For example, the information from the first document may be provided to the user by the automated assistant client 106 based on the first document having a score indicating that it is most relevant to the topic provided by the user. Other documents (i.e., new documents) may then be re-ranked based on the previously determined scores and based on information gain scores generated from providing information to the user from the first document. The re-ranking based on the information gain scores may result in one or more documents being promoted and/or demoted in the ranked list. Thus, if the user subsequently requests information for a new document, the user may be provided with a document that has a greater information gain than the document that has already been viewed, rather than merely being based on the initial ranked list of documents.
As an example, a user may submit a natural language input "Help me fix my computer". As described herein, documents may be identified by the search engine 120 and ranked into a ranked list, such as the list of the collection 400 in fig. 4, based on content. Information from document 1 may be provided as the most relevant document based on the ranking, and may be provided as turn 202 of fig. 2. The remaining files may be scored by the information gain scoring engine 125 and re-ranked according to the determined scores. In re-ranking, document 4 may be determined to have a greater information gain based on the user being provided with document 1 (e.g., document 4 includes more information not included in document 1 than documents 2 and 3). Thus, if the user subsequently requests a new document, as indicated in dialog turn 203, the information in dialog box 204 may be generated from document 4 instead of from document 2 (i.e., the document ranked higher before considering the information gain score).
In this way, the user may be directed to another document with greater information gain, which may result in the user receiving the desired or required information items faster, or with fewer input interactions. In this way, query sessions between users and search engines may be made shorter or more efficient by preferentially ranking documents based on greater information gain. In some embodiments, a shorter query session or fewer dialog turns may provide a corresponding reduction in resource requirements of the system, for example, with respect to memory and/or power usage of the system.
Similarly, references to documents provided in FIG. 4 may be updated based on calculating information gain scores for documents associated with the references. The user may be provided with a list of ranked references first, so that document 1 appears first, then document 2 appears, and so on. If the user selects the link to document 1, the remaining documents may be scored by the information gain scoring engine 125, re-ranked, and presented in a different order if the user navigates back to the reference list (e.g., document 4 may now appear higher in the list than document 2). Also, for example, based on the determined information gain score, one or more documents may no longer appear in the list of ranked documents. For example, if the user subsequently navigates back to the list, it may be determined that the document includes the same information as another unviewed document and that the same unviewed document may be removed (or at least substantially degraded) from the list of references. If the user views the same new document, a reference that is no longer included in the ranked list of documents may indicate, for example, a user-determined gain of zero information.
FIG. 5 illustrates a flow chart of an example method of presenting information from a document to a user based on an information gain score determined for an unviewed document. The steps of fig. 5 may be performed by one or more processors, such as one or more processors of a client device. Other embodiments may include additional steps in addition to those shown in fig. 5, may perform the steps of fig. 5 in a different order and/or in parallel, and/or may omit one or more of the steps of fig. 5.
At step 505, a first set of documents that share a common topic and have been viewed by a user is identified. Topics may be identified based on natural language input by a user to an automatic assistant (such as the automatic assistant client 106 of fig. 1). In some implementations, the input can be spoken and received by a component that shares one or more characteristics with the microphone 108. In some implementations, the input can be textual and can be provided by the user via a search interface. The first set of documents includes documents that may have been viewed (or listened to) by the user based on previous dialog rounds of the dialog session with the automated assistant client 106 and/or based on the user previously selecting documents via the search interface 107.
At step 510, a second set of documents that also share the topics of the first set of documents but have not been viewed by the user is identified. The second set of documents may be identified by a component that shares one or more characteristics with the search engine 120. For example, topics identified from the natural language input may be used by the search engine 120 to identify documents responsive to and/or relevant to the search topic. In addition, components that share one or more characteristics with user document database 140 may include information about documents that a user has viewed. Thus, the second set of documents may be a subset of a larger set of documents, with removed documents that have been viewed by the user.
At step 515, an information gain score is determined for each document in the second set of documents. For documents in the second set, the information gain score indicates an information gain for the user that presented information from the documents, assuming that the user was already presented with information from documents in the first set of documents. In some implementations, the information gain scores may be determined based on the output of a machine learning model that has been provided with information about documents in the first set of documents and the second set of documents as input. For example, a semantic vector for each document in a first set of documents and a semantic vector for a new document in a second set of documents may be provided as inputs across the machine learning model, and an output of the machine learning model may be utilized to determine an information gain score for the new document. The score may be a quantitative score, for example, between 0.00 and 1.00, where a score of 0.00 indicates that no information gain is expected if the user views the document assuming the user has already viewed the document, and a score of 1.00 indicates that the new document includes only information not included in the previously viewed document (e.g., a total information gain).
At step 520, one or more new documents are selected from the second set of documents based on the information gain scores. In some implementations, a single new document is selected based on the information gain score of the document being more indicative of an information gain than all other documents of the second set. In some implementations, one or more scores that have been associated with documents in the second set of documents may be utilized with the information gain scores to select documents. For example, each document in the second set of documents may be scored by search engine 120, and the score may be adjusted based on the information gain score determined for the document by information gain scoring engine 125.
At step 525, one or more documents in the second set and/or information extracted from the one or more documents are presented to the user based on the information gain scores. In some implementations, the information extracted from the selected new document may be provided to the user via a component that shares one or more characteristics with the automated assistant client 106. For example, the TTS module may convert a portion of the new document to speech and provide the portion of text to the user via the speaker of the client device 105. In some implementations, the list of references to the second set of documents may be provided to the user via a component that shares one or more characteristics with the search interface 107. For example, a ranked list of documents may be provided to the user, the ranked list of documents being ranked based on the information gain score and/or one or more other scores determined for the documents. In some implementations, the information gain scoring engine 125 may score the list of references and/or the collection of documents as the user views the attached documents. For example, the user may be provided with a first ranked list of references, select one of the new documents to view, and navigate back to the list of references. When the user returns to the list based on the updated information gain scores, the references may be updated (e.g., re-ranked, reference removed, references presented differently).
In this way, the information presented to the user may be improved in view of the amount of information provided. The information density of the output provided by the automated assistant client 106 may be increased by, for example, prioritizing the output of new information. In this way, the automated assistant client 106 may avoid outputting duplicate or redundant information. In some implementations, a TTS module may be required to convert a smaller amount of text to speech output, providing a corresponding reduction in the processing required by the TTS module and the total output time, number of user interruptions, and number of additional dialog turns.
For example, assume that a user engages with an automated assistant using vocal natural language input. Assume further that the user seeks help regarding a computer error message and speaks the request: "any am I receiving the error message, < contextual error message text >? (why do i receive error messages, < hypothetical error message text >) ". A first document responsive to a user request may be identified. Assume that the first document includes a first information element (e.g., topic, fact, etc.) associated with a first potential cause of the error message and a second information element associated with a second, different potential cause of the error message.
In various implementations, the automatic assistant may process one or more portions of the first document including the first information element and the second information element using text-to-speech ("TTS") processing to generate a TTS output conveying the first information element and the second information element. For example, the automated assistant may identify a first potential cause of and/or one or more sentences related to the error message. Based on these identified one or more sentences, the automated assistant may generate a natural language output (e.g., directly from the document as a snippet or using a natural language generation technique such as Tensorflow to summarize a first information element) that conveys a first potential cause of the error message. The automated assistant may do the same for the second potential cause of the error message. TTS output may be provided at one or more computing devices of a user, for example, as audible natural language output.
Now, assume that a second document is also identified in response to the user's original request, and that after presenting the previously discussed TTS output to the user, the user issues a subsequent request, such as "what else core be the case of the user of the error message? (what is also the reason for this error message. It is further assumed that the second document comprises a second information element and a third information element different from the first and second information elements. For example, the third information element may be a third potential cause of the computer error message that is different from the first and second potential causes of the computer error message. In various implementations, the second information element can be determined, for example, by the automated assistant based on a previous provision of the TTS output, to have been communicated to the user.
Accordingly, the automatic assistant may process one or more portions of the second document that include the third information element using additional TTS processing to generate additional TTS output that conveys the third information element and excludes the second information element. For example, one or more sentences, phrases, or other phrases or segments related to or referring to a third potential cause of the computer error in the third document may be identified. TTS processing may then be applied to generate, for example, snippets that summarize the sentences, phrases, or other short texts or snippets directly from the second document and/or the natural language output. The additional TTS output may be provided at one or more computing devices operated by the user. Thus, summary information about each of the first, second, and third potential causes of the computer error message may be presented to the user without providing redundant information. As described above, even in summary form, it takes a long time for information from a document to be audibly provided to a user, and eliminating any redundant information can substantially improve the user experience.
FIG. 6 is a block diagram of an example computing device 610, which example computing device 610 may optionally be used to perform one or more aspects of the techniques described herein. Computing device 610 typically includes at least one processor 614, which communicates with a number of peripheral devices via a bus subsystem 612. These peripheral devices may include a storage subsystem 624, including, for example, a memory subsystem 625 and a file storage subsystem 626; an interface output device 620; a user interface input device 622, and a network interface subsystem 616. The input and output devices allow a user to interact with the computing device 610. Network interface subsystem 616 provides an interface to external networks and couples to corresponding interface devices in other computing devices.
User interface input device 622 may include a keyboard; a pointing device such as a mouse, trackball, touchpad, or tablet; a scanner; a touch screen incorporated into the display; an audio input device such as a voice recognition system; a microphone; and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 610 or a communication network.
User interface output device 620 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or other mechanism for creating a visual image. The display subsystem may also provide non-visual displays, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 610 to a user or to another machine or computing device.
These software modules are typically executed by processor 614 alone or in combination with other processors. Memory 625 used in storage subsystem 624 may include a number of memories, including a main Random Access Memory (RAM)630 for storing instructions and data during program execution and a Read Only Memory (ROM)632 that stores fixed instructions. File storage subsystem 626 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 626 in storage subsystem 624, or in other machines accessible to processor 614.
The computing device 610 can be of various types, including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 610 are possible with more or fewer components than the computing device depicted in fig. 6.
Certain embodiments discussed herein may provide a user with one or more opportunities to control whether to collect information, whether to store personal information, whether to use personal information, and how to collect, store, and use information about the user in the event that personal information about the user (e.g., user data extracted from other electronic communications, information about the user's social network, the user's location, the user's time, the user's biometric information, as well as the user's activities and demographic information, relationships between users, etc.) is collected or used. That is, the systems and methods discussed herein collect, store, and/or use a user's personal information only after receiving explicit authorization to do so from the relevant user.
For example, the user is provided with control over whether the program or feature collects user information about that particular user or other users related to the program or feature. Each user for whom personal information is to be collected is presented with one or more options to allow control of information collection in relation to that user to provide permission or authorization as to whether information is collected and what portions of the information are to be collected. For example, one or more such control options may be provided to the user over a communications network. In addition, some data may be processed in one or more ways before it is stored or used, so that personally identifiable information is deleted. As one example, the identity of the user may be processed such that personally identifiable information cannot be determined. As another example, the geographic location of the user may be generalized to a larger area such that a particular location of the user cannot be determined.
While several embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized and each of such variations and/or modifications is deemed to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are intended to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (23)
1. A method implemented using one or more processors, comprising:
identifying a first set of one or more documents that share a topic, wherein information extracted from each document in the first set was previously presented to a user by an automatic assistant executing at least in part on one or more computing devices operated by the user in response to free-form natural language input from the user;
identifying a second set of a plurality of new documents that also share the topic, wherein the automatic assistant has not presented to the user the information contained in each new document in the second set for presentation;
determining an information gain score for each new document in the second set, wherein the information gain score for a given new document in the second set indicates additional information that the user would obtain beyond the information extracted from the one or more documents in the first set if the information contained in the given new document were extracted and presented to the user;
selecting a new document from the second set based on the information gain scores; and
presenting, via one or more of the computing devices, the information extracted from the selected new document to the user.
2. The method of claim 1, further comprising ranking the second set of new documents based on the information gain scores, wherein the selecting is based on the ranking.
3. The method of claim 1, wherein determining the information gain score for the given new document comprises:
applying, across a machine learning model, first data indicative of the information extracted from a first set of documents and second data indicative of information extracted from the given new document to generate an output, wherein the information gain score is determined based on the output.
4. The method of claim 3, wherein the first data comprises a semantic feature vector generated from the information extracted from the first set of documents and the second data comprises a semantic feature vector generated from the information extracted from the given new document.
5. A method implemented using one or more processors, comprising:
identifying a first set of one or more documents that share a topic, wherein each document in the first set is accessed by a user on one or more computing devices operated by the user;
identifying a second set of a plurality of new documents that also share the topic, wherein the plurality of new documents in the second set have not been previously accessed by the user;
determining an information gain score for each new document in the second set, wherein the information gain score for a given new document in the second set indicates that the user would obtain additional information beyond that obtained by the user when accessing the first set of one or more documents if the user accessed the given new document; and
presenting, via one or more of the computing devices, information to the user indicating one or more of the new documents in the second set, wherein the information indicating one or more of the new documents in the second set is presented in a manner selected based on the information gain scores.
6. The method of claim 5, further presenting a ranked list of the second set of new documents, wherein the list is ranked based on the information gain scores.
7. The method of claim 5, wherein identifying the first set comprises:
presenting references to one or more documents of the first set as part of a search results interface presented to the user in response to a query that includes the topic;
receiving user input from the search results interface indicating a selection of a reference to a particular document in the first set, wherein at least a portion of the particular document is provided to the user in response to the selection;
receiving user input indicating a command to return the search results interface; and
in response to receiving the user input indicating the command to return the search results interface, rendering a reference to one or more documents in the second set as part of the search results interface.
8. The method of claim 7, wherein rendering the reference to one or more documents in the second set comprises excluding from the search results interface at least a reference to a particular document in the first set.
9. The method of claim 5, wherein determining the information gain score for the given new document comprises:
applying, across a machine learning model, first data indicative of a first set of documents and second data indicative of the given new document to generate an output, wherein the information gain score is determined based on the output.
10. A system comprising one or more processors and a memory operably coupled with the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to:
identifying a first set of one or more documents that share a topic, wherein information extracted from each document in the first set was previously presented to a user by an automatic assistant executing at least in part on one or more computing devices operated by the user in response to free-form natural language input from the user;
identifying a second set of a plurality of new documents that also share the topic, wherein the automatic assistant has not presented to the user the information contained in each new document in the second set for presentation;
determining an information gain score for each new document in the second set, wherein the information gain score for a given new document in the second set indicates additional information that the user would obtain beyond the information extracted from the one or more documents in the first set if the information contained in the given new document were extracted and presented to the user;
selecting a new document from the second set based on the information gain scores; and
presenting, via one or more of the computing devices, the information extracted from the selected new document to the user.
11. The system of claim 10, further comprising ranking the second set of new documents based on the information gain scores, wherein the selecting is based on the ranking.
12. The system of claim 10, wherein determining the information gain score for the given new document comprises:
applying, across a machine learning model, first data indicative of the information extracted from a first set of documents and second data indicative of information extracted from the given new document to generate an output, wherein the information gain score is determined based on the output.
13. The system of claim 12, wherein the first data comprises a semantic feature vector generated from the information extracted from the first set of documents and the second data comprises a semantic feature vector generated from the information extracted from the given new document.
14. A system comprising one or more processors and a memory operably coupled with the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to:
identifying a first set of one or more documents that share a topic, wherein each document in the first set is accessed by a user on one or more computing devices operated by the user;
identifying a second set of a plurality of new documents that also share the topic, wherein the plurality of new documents in the second set have not been previously accessed by the user;
determining an information gain score for each new document in the second set, wherein the information gain score for a given new document in the second set indicates that the user would obtain additional information beyond that obtained by the user when accessing the first set of one or more documents if the user accessed the given new document;
selecting a new document from the second set based on the information gain scores; and
presenting, via one or more of the computing devices, information to the user indicating one or more of the new documents in the second set, wherein the information indicating one or more of the new documents in the second set is presented in a manner selected based on the information gain scores.
15. The system of claim 14, further presenting a ranked list of the second set of new documents, wherein the list is ranked based on the information gain scores.
16. The system of claim 14, wherein identifying the first set comprises:
rendering references to one or more documents in the first set as part of a search results interface presented to the user in response to a query that includes the topic;
receiving user input from the search results interface indicating a selection of a reference to a particular document in the first set, wherein at least a portion of the particular document is provided to the user in response to the selection;
receiving user input indicating a command to return the search results interface; and
in response to receiving the user input indicating the command to return the search results interface, rendering a reference to one or more documents in the second set as part of the search results interface.
17. The system of claim 16, wherein rendering the reference to one or more documents in the second set comprises excluding from the search results interface at least the reference to the particular document in the first set.
18. The system of claim 14, wherein determining the information gain score for the given new document comprises:
applying, across a machine learning model, first data indicative of a first set of documents and second data indicative of the given new document to generate an output, wherein the information gain score is determined based on the output.
19. The system of claim 18, wherein the machine learning model comprises a neural network.
20. The system of claim 18, wherein the first data indicative of the first set of documents includes one or more semantic representations generated from the first set of documents, and the second data indicative of the given new document includes a semantic representation generated from the given new document.
21. A method implemented using one or more processors, comprising:
identifying a first document comprising a first information element and a second information element, wherein the first information element is different from the second information element;
processing one or more portions of the first document including the first information element and the second information element using text-to-speech ("TTS") processing to generate a TTS output conveying the first information element and the second information element;
providing the TTS output at one or more computing devices operated by a user;
identifying a second document comprising the second information element and a third information element, wherein the third information element is different from the first information element and the second information element;
based on the providing, determining that the second information element has been communicated to the user;
in response to the determination, processing one or more portions of the second document that include the third information element using additional TTS processing to generate additional TTS output that conveys the third information element and excludes the second information element; and
the additional TTS output is provided at one or more computing devices operated by a user.
22. The method of claim 21, wherein the first document and the second document are identified based on free-form natural language input identifying topics shared between the first document and the second document.
23. A method implemented using one or more processors, comprising:
providing references to a plurality of documents sharing a topic as part of a search results interface presented to a user in response to free-form natural language input from the user including the topic, wherein the query is directed by the user to an automated assistant executing at least in part on one or more computing devices operated by the user;
receiving user input from the search results interface indicating a selection of a reference to a particular document of the plurality of documents, wherein a first information element contained in the particular document and associated with the topic is provided to the user in response to the selection;
receiving user input indicating a command to return the search results interface;
in response to receiving the user input indicating the command to return the search results interface, updating the search results interface to exclude references to one or more documents of the plurality of documents that also include the first information element; and
providing the updated search results interface to the user.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2018/056483 WO2020081082A1 (en) | 2018-10-18 | 2018-10-18 | Contextual estimation of link information gain |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112368694A true CN112368694A (en) | 2021-02-12 |
Family
ID=64110225
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880095102.2A Pending CN112368694A (en) | 2018-10-18 | 2018-10-18 | Context estimation of link information gain |
Country Status (4)
Country | Link |
---|---|
US (3) | US11354342B2 (en) |
EP (1) | EP3665543A1 (en) |
CN (1) | CN112368694A (en) |
WO (1) | WO2020081082A1 (en) |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11354342B2 (en) | 2018-10-18 | 2022-06-07 | Google Llc | Contextual estimation of link information gain |
US11250039B1 (en) * | 2018-12-06 | 2022-02-15 | A9.Com, Inc. | Extreme multi-label classification |
US11620296B2 (en) | 2019-10-18 | 2023-04-04 | Splunk Inc. | Online machine learning algorithm for a data intake and query system |
US11620157B2 (en) | 2019-10-18 | 2023-04-04 | Splunk Inc. | Data ingestion pipeline anomaly detection |
US11704490B2 (en) | 2020-07-31 | 2023-07-18 | Splunk Inc. | Log sourcetype inference model training for a data intake and query system |
US11663176B2 (en) | 2020-07-31 | 2023-05-30 | Splunk Inc. | Data field extraction model training for a data intake and query system |
IL303090A (en) * | 2020-11-30 | 2023-07-01 | Liveperson Inc | Method and apparatus for automated inter-account interactions |
US11687438B1 (en) | 2021-01-29 | 2023-06-27 | Splunk Inc. | Adaptive thresholding of data streamed to a data processing pipeline |
US11880339B2 (en) * | 2021-11-01 | 2024-01-23 | Microsoft Technology Licensing, Llc | Activity based sorting in collaborative applications |
US11620441B1 (en) * | 2022-02-28 | 2023-04-04 | Clearbrief, Inc. | System, method, and computer program product for inserting citations into a textual document |
Family Cites Families (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7580929B2 (en) * | 2004-07-26 | 2009-08-25 | Google Inc. | Phrase-based personalization of searches in an information retrieval system |
US7536408B2 (en) * | 2004-07-26 | 2009-05-19 | Google Inc. | Phrase-based indexing in an information retrieval system |
US7711679B2 (en) * | 2004-07-26 | 2010-05-04 | Google Inc. | Phrase-based detection of duplicate documents in an information retrieval system |
US7424488B2 (en) * | 2006-06-27 | 2008-09-09 | International Business Machines Corporation | Context-aware, adaptive approach to information selection for interactive information analysis |
US9779441B1 (en) * | 2006-08-04 | 2017-10-03 | Facebook, Inc. | Method for relevancy ranking of products in online shopping |
US7937397B2 (en) * | 2006-08-22 | 2011-05-03 | Fuji Xerox Co., Ltd. | Apparatus and method for term context modeling for information retrieval |
US8965865B2 (en) * | 2008-02-15 | 2015-02-24 | The University Of Utah Research Foundation | Method and system for adaptive discovery of content on a network |
US7996390B2 (en) * | 2008-02-15 | 2011-08-09 | The University Of Utah Research Foundation | Method and system for clustering identified forms |
US20100121840A1 (en) * | 2008-11-12 | 2010-05-13 | Yahoo! Inc. | Query difficulty estimation |
GB2472250A (en) * | 2009-07-31 | 2011-02-02 | Stephen Timothy Morris | Method for determining document relevance |
US10346453B2 (en) * | 2010-12-21 | 2019-07-09 | Microsoft Technology Licensing, Llc | Multi-tiered information retrieval training |
US9536269B2 (en) * | 2011-01-19 | 2017-01-03 | 24/7 Customer, Inc. | Method and apparatus for analyzing and applying data related to customer interactions with social media |
US8903198B2 (en) * | 2011-06-03 | 2014-12-02 | International Business Machines Corporation | Image ranking based on attribute correlation |
US8903714B2 (en) * | 2011-12-21 | 2014-12-02 | Nuance Communications, Inc. | Concept search and semantic annotation for mobile messaging |
US9461876B2 (en) * | 2012-08-29 | 2016-10-04 | Loci | System and method for fuzzy concept mapping, voting ontology crowd sourcing, and technology prediction |
US9715493B2 (en) * | 2012-09-28 | 2017-07-25 | Semeon Analytics Inc. | Method and system for monitoring social media and analyzing text to automate classification of user posts using a facet based relevance assessment model |
US10007730B2 (en) * | 2015-01-30 | 2018-06-26 | Microsoft Technology Licensing, Llc | Compensating for bias in search results |
US10481861B2 (en) * | 2016-08-30 | 2019-11-19 | Google Llc | Using user input to adapt search results provided for presentation to the user |
US11295118B2 (en) * | 2017-06-29 | 2022-04-05 | Avast Software, S.R.O. | Online user verification without prior knowledge of the user |
US11354342B2 (en) * | 2018-10-18 | 2022-06-07 | Google Llc | Contextual estimation of link information gain |
-
2018
- 2018-10-18 US US16/608,628 patent/US11354342B2/en active Active
- 2018-10-18 CN CN201880095102.2A patent/CN112368694A/en active Pending
- 2018-10-18 WO PCT/US2018/056483 patent/WO2020081082A1/en unknown
- 2018-10-18 EP EP18797433.2A patent/EP3665543A1/en not_active Ceased
-
2022
- 2022-04-22 US US17/727,237 patent/US11720613B2/en active Active
-
2023
- 2023-06-27 US US18/215,032 patent/US20230342384A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20220245182A1 (en) | 2022-08-04 |
US20200349181A1 (en) | 2020-11-05 |
US20230342384A1 (en) | 2023-10-26 |
US11354342B2 (en) | 2022-06-07 |
US11720613B2 (en) | 2023-08-08 |
WO2020081082A1 (en) | 2020-04-23 |
EP3665543A1 (en) | 2020-06-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11720613B2 (en) | Contextual estimation of link information gain | |
US11762865B2 (en) | Automatically augmenting message exchange threads based on tone of message | |
JP7443407B2 (en) | Automated assistant with conferencing capabilities | |
CN111033492B (en) | Providing command bundle suggestions for automated assistants | |
CN109983430B (en) | Determining graphical elements included in an electronic communication | |
CN110770694B (en) | Obtaining response information from multiple corpora | |
US20180189628A1 (en) | Determining semantically diverse responses for providing as suggestions for inclusion in electronic communications | |
KR20200003871A (en) | Proactive integration of unsolicited content in human-to-computer conversations | |
US11238242B2 (en) | Generating output for presentation in response to user interface input, where the input and/or the output include chatspeak | |
EP3955243A2 (en) | Speech generation using crosslingual phoneme mapping | |
CN114078468B (en) | Voice multi-language recognition method, device, terminal and storage medium | |
US20220020365A1 (en) | Automated assistant with audio presentation interaction | |
US11842206B2 (en) | Generating content endorsements using machine learning nominator(s) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |