WO2024019710A1 - Ad-hoc navigation instructions - Google Patents
Ad-hoc navigation instructions Download PDFInfo
- Publication number
- WO2024019710A1 WO2024019710A1 PCT/US2022/037679 US2022037679W WO2024019710A1 WO 2024019710 A1 WO2024019710 A1 WO 2024019710A1 US 2022037679 W US2022037679 W US 2022037679W WO 2024019710 A1 WO2024019710 A1 WO 2024019710A1
- Authority
- WO
- WIPO (PCT)
- Prior art keywords
- user
- navigation
- destination
- request
- computing device
- Prior art date
Links
- 230000004044 response Effects 0.000 claims abstract description 221
- 238000000034 method Methods 0.000 claims abstract description 47
- 230000000977 initiatory effect Effects 0.000 claims abstract description 17
- 238000012545 processing Methods 0.000 description 47
- 238000003058 natural language processing Methods 0.000 description 29
- 238000010801 machine learning Methods 0.000 description 23
- 238000004891 communication Methods 0.000 description 13
- 238000012549 training Methods 0.000 description 13
- 238000013518 transcription Methods 0.000 description 13
- 230000035897 transcription Effects 0.000 description 13
- 230000000007 visual effect Effects 0.000 description 13
- 238000013528 artificial neural network Methods 0.000 description 11
- 230000006870 function Effects 0.000 description 10
- 235000013550 pizza Nutrition 0.000 description 9
- 210000002569 neuron Anatomy 0.000 description 7
- 238000010276 construction Methods 0.000 description 5
- 230000003993 interaction Effects 0.000 description 4
- 230000001413 cellular effect Effects 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 230000009471 action Effects 0.000 description 2
- 230000004913 activation Effects 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 238000005352 clarification Methods 0.000 description 2
- 230000001143 conditioned effect Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- 230000001755 vocal effect Effects 0.000 description 2
- 239000008186 active pharmaceutical agent Substances 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 239000006227 byproduct Substances 0.000 description 1
- 230000010267 cellular communication Effects 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 230000008054 signal transmission Effects 0.000 description 1
- 239000004984 smart glass Substances 0.000 description 1
- 238000010561 standard procedure Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3605—Destination input or retrieval
- G01C21/3608—Destination input or retrieval using speech input, e.g. using speech recognition
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3605—Destination input or retrieval
- G01C21/3617—Destination input or retrieval using user history, behaviour, conditions or preferences, e.g. predicted or inferred from previous use or current movement
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
- G01C21/3629—Guidance using speech or audio output, e.g. text-to-speech
Definitions
- the present disclosure generally relates to navigation systems and, more particularly, to providing responses to ad-hoc requests regarding navigation before the user has initiated a navigation session.
- the user does not request navigation directions and does not initiate a navigation session via a navigation application.
- the user may have questions about the trip. For example, if there is traffic the user may want to know how much longer it will take before the user arrives at the destination. In another example, the user may want to know whether they should take an alternative route to avoid some of the traffic.
- an ad-hoc navigation system may receive a request for navigation information from the user regarding a current trip prior to initiating a navigation session.
- the request may be, “Hey Maps, I’m on my way to the office. Should I make a left here?”
- the ad-hoc navigation system may determine the destination based on the information included in the request (e.g., “the office”). In other scenarios, the ad-hoc navigation system may determine the destination based on information included in a previous request, such as a request for navigation directions a few hours before the user began their trip. In yet other scenarios, the ad-hoc navigation system may infer the destination based on user context data, such as calendar events, the user’s typical destinations at the particular day of the week/time of day, etc.
- the ad-hoc navigation system generates navigation directions from the user’s current location to the determined destination for providing a response to the request. For example, if the user is heading east on their current trip, the ad-hoc navigation system may generate navigation directions to the destination that begin with heading east from the current location. In some implementations, the ad-hoc navigation system generates multiple sets of navigation directions for comparison to provide a response to the request. Also in some implementations, the ad-hoc navigation system may use cached navigation directions for example, from a previous request to the destination that overlap with the current trajectory of the user.
- the ad-hoc navigation system then analyzes the set(s) of navigation directions using attributes from the request to provide a response to the request. For example, in response to the user’s request as to whether they should turn left, the ad-hoc navigation system analyzes the set of navigation directions to the user’s office to determine whether the next maneuver in the set is a left turn. Then the ad-hoc navigation system provides a response to the user indicating whether the next maneuver in the set is a left turn. For example, the response may be an audio response confirming that the user is correct and should turn left. The response may also be a text or visual response indicating that the user is correct and should turn left.
- the response may indicate that the user is incorrect and may ask the user if they would like to receive navigation directions to the destination. Then the ad-hoc navigation system may initiate a navigation session and provide audio and/or visual navigation instructions to the user.
- the ad-hoc navigation system may analyze multiple routes to the user’s office to determine which route includes a left turn for the upcoming maneuver. Then the ad-hoc navigation system may provide a response to the user indicating that the next maneuver in one route is a left turn but not in another route. For example, the response may indicate that the next maneuver in the fastest route to the office is to turn left, but if the user prefers the route with the shortest distance the user should continue straight. [0009] By providing responses to requests for navigation information while a user is on a current trip prior to initiating a navigation session, the user does not need to initiate a navigation session at the beginning of a route.
- the ad-hoc navigation system further reduces bandwidth requirements by caching navigation directions from previous requests and using the cached navigation directions, in some scenarios, to respond a request rather than obtaining the navigation directions from a server.
- One example embodiment of the techniques of this disclosure is a method for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session.
- the method includes receiving a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session, and determining a destination for the current trip.
- the method also includes generating one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user, and providing a response to the request for navigation information based on the generated one or more sets of navigation directions.
- Another example embodiment is a computing device for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session.
- the computing device includes one or more processors, and a computer- readable memory, which is optionally non-transitory, coupled to the one or more processors and storing instructions thereon that, when executed by the one or more processors, cause the computing device to receive a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session, and determine a destination for the current trip.
- the instructions also cause the computing device to generate one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user, and provide a response to the request for navigation information based on the generated one or more sets of navigation directions.
- Yet another example embodiment is a computer-readable medium, which is optionally non-transitory, storing instructions for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, that when executed by one or more processors cause the one or more processors to: receive a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session, and determine a destination for the current trip.
- the instructions also cause the one or more processors to generate one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user, and provide a response to the request for navigation information based on the generated one or more sets of navigation directions.
- FIG. 1 is a block diagram of an example communication system in which techniques for providing ad-hoc navigation information can be implemented
- FIG. 2 illustrates an example scenario within a vehicle interior where a user requests navigation information without previously initiating a navigation session and includes a destination in the request;
- FIG. 3 illustrates another example scenario within a vehicle interior where a user requests navigation information without previously initiating a navigation session and without including a destination in the request;
- FIG. 4 illustrates yet another example scenario within a vehicle interior where a user requests navigation information without previously initiating a navigation session and without including a destination in the request;
- FIG. 5 illustrates an example interaction between the user and the user computing device of Fig. 1, where the user computing device requests the user to clarify a navigation information request;
- Fig. 6 illustrates another example interaction between the user and the user computing device of Fig. 1, where the user computing device provides alternative responses for multiple potential destinations;
- FIG. 7 illustrates yet another example interaction between the user and the user computing device of Fig. 1, where the user computing device provides a prompt for the user to initiate a navigation session to receive navigation directions to the user’s destination; and
- Fig. 8 is a flow diagram of an example method for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, which can be implemented in a computing device, such as the user computing device of Fig. 1.
- the techniques for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session can be implemented in one or several user computing devices, one or several network servers, or a system that includes a combination of these devices.
- a user on a current trip provides a request for navigation information to a user computing device.
- the user computing device analyzes the request for example, using natural language processing techniques, to determine whether a destination is included in the request.
- the user computing device may also analyze previous requests, and/or user context data, such as calendar events, the user’s typical destinations at the particular day of the week/time of day, etc. to determine the destination of the current trip.
- the user computing device may also determine the current location and/or the current trajectory of the user computing device, for example, using sensor data from sensors in the user computing device.
- the current trajectory may indicate that the user is heading East on Highway 101.
- the user computing device may provide the current location, current trajectory, and/or destination to an external server.
- the external server may generate set(s) of navigation directions from the current location to the destination which include(s) the current trajectory. In some implementations, the external server then analyzes the set(s) of navigation directions using attributes from the request to generate a response to the request. The external server may then provide the response to the request to the user computing device. In other implementations, the external server provides the generated set(s) of navigation directions to the user computing device. The user computing device then analyzes the set(s) of navigation directions using attributes from the request to generate a response to the request. Then the user computing device presents the response to the user as an audio response and/or a visual response, via a user interface of the user computing device.
- an example communication system 100 in which techniques for providing ad-hoc navigation information can be implemented includes a user computing device 102.
- the user computing device 102 may be a portable device such as a smart phone or a tablet computer, for example.
- the user computing device 102 may also be a laptop computer, a desktop computer, a personal digital assistant (PDA), a wearable device such as a smart watch or smart glasses, a virtual reality headset, etc.
- the user computing device 102 may be removably mounted in a vehicle, embedded into a vehicle, and/or may be capable of interacting with a head unit of a vehicle to provide navigation instructions.
- the user computing device 102 may include one or more processor(s) 104 and a memory 106 storing machine-readable instructions executable on the processor(s) 104.
- the processor(s) 104 may include one or more general-purpose processors (e.g., CPUs), and/or special-purpose processing units (e.g., graphical processing units (GPUs)).
- the memory 106 can be, optionally, a non-transitory memory and can include one or several suitable memory modules, such as random access memory (RAM), read-only memory (ROM), flash memory, other types of persistent memory, etc.
- the memory 106 may store instructions for implementing a navigation application 108 that can provide navigation directions (e.g., by displaying directions or emitting audio instructions via the user computing device 102), display an interactive digital map, request and receive routing data to provide driving, walking, or other navigation directions, provide various geo-located content such as traffic, points-of-interest (POIs), and weather information, etc. While the examples described herein for providing ad-hoc navigation information include driving information, the techniques may be applied to navigation information for any suitable mode of transportation, such as walking, biking, public transportation, etc.
- the navigation application 108 may include an ad-hoc navigation response engine 160.
- the ad-hoc navigation response engine 160 may receive audio or text requests from a user for navigation information regarding the user’s current trip when the user has not initiated a navigation session for the current trip.
- the user may provide a particular trigger phrase or hot word which causes the ad-hoc navigation response engine 160 to receive the audio request from the user, such as “Hey Maps.”
- the ad-hoc navigation response engine 160 may then analyze the request using the natural language processing techniques described below to identify a destination and/or other attributes included in the request.
- the ad-hoc navigation response engine 160 may obtain previous navigation requests from the user and may infer the destination according to the destinations in the previous navigation requests. For example, the ad-hoc navigation response engine 160 may obtain previous navigation requests stored at the user computing device 102 or at the external server 120. The ad-hoc navigation response engine 160 may also obtain user context data stored at the user computing device 102 or at the external server 120 to infer the destination, such as calendar data, typical destinations at the particular day of the week/time of day, etc.
- the ad-hoc navigation response engine 160 may transmit a request for navigation directions to the external server 120 from the user’s current location to the destination along the current trajectory.
- the ad- hoc navigation response engine 160 may then receive one or more set(s) of navigation directions to the destination from the external server 120 and may analyze the set(s) of navigation directions to provide a response to the request for navigation information.
- the ad-hoc navigation response engine 160 may obtain an offline set(s) of navigation directions cached at the user computing device 102 and obtained from the external server 120 in a previous request, for example.
- the memory 102 may include a language processing module 109a configured to implement and/or support the techniques of this disclosure for providing ad-hoc navigation information through natural conversation.
- the language processing module 109a may include an automatic speech recognition (ASR) engine 109al that is configured to transcribe speech inputs from a user into sets of text.
- the language processing module 109a may include a text-to-speech (TTS) engine 109a2 that is configured to convert text into audio outputs, such as audio responses, audio requests, navigation instructions, and/or other outputs for the user.
- ASR automatic speech recognition
- TTS text-to-speech
- the language processing module 109a may include a natural language processing (NLP) model 109a3 that is configured to output textual transcriptions, intent interpretations, and/or audio outputs related to a speech input received from a user of the user computing device 102.
- NLP natural language processing
- the ASR engine 109al and/or the TTS engine 109a2 may be included as part of the NLP model 109a3 in order to transcribe user speech inputs into a set of text, convert text outputs into audio outputs, and/or any other suitable function described herein as part of a conversation between the user computing device 102 and the user.
- the language processing module 109a may include computer-executable instructions for training and operating the NLP model 109a3.
- the language processing module 109a may train one or more NLP models 109a3 by establishing a network architecture, or topology, and adding layers that may be associated with one or more activation functions (e.g., a rectified linear unit, softmax, etc.), loss functions and/or optimization functions.
- activation functions e.g., a rectified linear unit, softmax, etc.
- loss functions e.g., a rectified linear unit, softmax, etc.
- optimization functions e.g., a rectified linear unit, softmax, etc.
- Such training may generally be performed using a symbolic method, machine learning (ML) models, and/or any other suitable training method.
- the language processing module 109a may train the NLP models 109a3 to perform two techniques that enable the user computing device 102, and/or any other suitable device (e.g., vehicle computing device 151) to understand the words spoken by a user and/or words generated by a tex t-to- speech program (e.g., TTS engine 109a2) executed by the processor 104: syntactic analysis and semantic analysis.
- vehicle computing device 151 e.g., vehicle computing device 151
- a tex t-to- speech program e.g., TTS engine 109a2
- Syntactic analysis generally involves analyzing text using basic grammar rules to identify overall sentence structure, how specific words within sentences are organized, and how the words within sentences are related to one another. Syntactic analysis may include one or more sub-tasks, such as tokenization, part of speech (PoS) tagging, parsing, lemmatization and stemming, stop-word removal, and/or any other suitable sub-task or combinations thereof.
- the NLP model 109a3 may generate textual transcriptions from the speech inputs from the user. Additionally, or alternatively, the NLP model 109a3 may receive such textual transcriptions as a set of text from the ASR engine 109al in order to perform semantic analysis on the set of text.
- Semantic analysis generally involves analyzing text in order to understand and/or otherwise capture the meaning of the text.
- the NLP model 109a3 applying semantic analysis may study the meaning of each individual word contained in a textual transcription in a process known as lexical semantics. Using these individual meanings, the NLP model 109a3 may then examine various combinations of words included in the sentences of the textual transcription to determine one or more contextual meanings of the words.
- Semantic analysis may include one or more sub-tasks, such as word sense disambiguation, relationship extraction, sentiment analysis, and/or any other suitable sub- tasks or combinations thereof.
- the NLP model 109a3 may generate one or more intent interpretations based on the textual transcriptions from the syntactic analysis.
- the language processing module 109a may include an artificial intelligence (Al) trained conversational algorithm (e.g., the natural language processing (NLP) model 109a3) that is configured to interact with a user that is accessing the navigation app 108.
- the user may be directly connected to the navigation app 108 to provide verbal input/responses (e.g., speech inputs), and/or the user request may include textual inputs/responses that the TTS engine 109a2 (and/or other suitable engine/model/algorithm) may convert to audio inputs/responses for the NLP model 109a3 to interpret.
- Al artificial intelligence trained conversational algorithm
- NLP natural language processing
- the inputs/responses spoken by the user and/or generated by the TTS engine 109a2 may be analyzed by the NLP model 109a3 to generate textual transcriptions and intent interpretations.
- the language processing module 109a may train the one or more NLP models 109a3 to apply these and/or other NLP techniques using a plurality of training speech inputs from a plurality of users.
- the NLP model 109a3 may be configured to output textual transcriptions and intent interpretations corresponding to the textual transcriptions based on the syntactic analysis and semantic analysis of the user’s speech inputs.
- one or more types of machine learning may be employed by the language processing module 109a to train the NLP model(s) 109a3.
- the ML may be employed by the ML module 109b, which may store a ML model 109b 1.
- the ML model 109b 1 may be configured to receive a set of text corresponding to a user input, and to output an intent based on the set of text.
- the NLP model(s) 109a3 may be and/or include one or more types of ML models, such as the ML model 109b 1.
- the NLP model 109a3 may be or include a machine learning model (e.g., a large language model (LLM)) trained by the ML module 109b using one or more training data sets of text in order to output one or more training intents and one or more training destinations, as described further herein.
- a machine learning model e.g., a large language model (LLM)
- LLM large language model
- artificial neural networks, recurrent neural networks, deep learning neural networks, a Bayesian model, and/or any other suitable ML model 109bl may be used to train and/or otherwise implement the NLP model(s) 109a3.
- training may be performed by iteratively training the NLP model(s) 109a3 using labeled training samples (e.g., training user inputs).
- training of the NLP model(s) 109a3 may produce byproduct weights, or parameters which may be initialized to random values.
- the weights may be modified as the network is iteratively trained, by using one of several gradient descent algorithms, to reduce loss and to cause the values output by the network to converge to expected, or “learned”, values.
- a regression neural network may be selected which lacks an activation function, wherein input data may be normalized by mean centering, to determine loss and quantify the accuracy of outputs. Such normalization may use a mean squared error loss function and mean absolute error.
- the artificial neural network model may be validated and cross-validated using standard techniques such as hold-out, K-fold, etc.
- multiple artificial neural networks may be separately trained and operated, and/or separately trained and operated in conjunction.
- the one or more NLP models 109a3 may include an artificial neural network having an input layer, one or more hidden layers, and an output layer.
- Each of the layers in the artificial neural network may include an arbitrary number of neurons.
- the plurality of layers may chain neurons together linearly and may pass output from one neuron to the next, or may be networked together such that the neurons communicate input and output in a non-linear way.
- the input layer may correspond to input parameters that are given as full sentences, or that are separated according to word or character (e.g., fixed width) limits.
- the input layer may correspond to a large number of input parameters (e.g., one million inputs), in some embodiments, and may be analyzed serially or in parallel. Further, various neurons and/or neuron connections within the artificial neural network may be initialized with any number of weights and/or other training parameters. Each of the neurons in the hidden layers may analyze one or more of the input parameters from the input layer, and/or one or more outputs from a previous one or more of the hidden layers, to generate a decision or other output.
- the output layer may include one or more outputs, each indicating a prediction. In some embodiments and/or scenarios, the output layer includes only a single output.
- Fig. 1 illustrates the navigation application 108 as a standalone application
- the functionality of the navigation application 108 also can be provided in the form of an online service accessible via a web browser executing on the user computing device 102, as a plug-in or extension for another software application executing on the user computing device 102, etc.
- the navigation application 108 generally can be provided in different versions for different operating systems.
- the maker of the user computing device 102 can provide a Software Development Kit (SDK) including the navigation application 108 for the AndroidTM platform, another SDK for the iOSTM platform, etc.
- SDK Software Development Kit
- the memory 106 may also store an operating system (OS) 110, which can be any type of suitable mobile or general-purpose operating system.
- the user computing device 102 may further include one or several sensors such as a global positioning system (GPS) 112 or another suitable positioning module, an accelerometer, a gyroscope, a compass, an inertial measurement unit (IMU), etc., a network module 114, a user interface 116 for displaying map data and directions, and input/output (I/O) module 118.
- GPS global positioning system
- IMU inertial measurement unit
- the network module 114 may include one or more communication interfaces such as hardware, software, and/or firmware of an interface for enabling communications via a cellular network, a Wi-Fi network, or any other suitable network such as a network 144, discussed below.
- the I/O module 118 may include I/O devices capable of receiving inputs from, and providing outputs to, the ambient environment and/or a user.
- the I/O module 118 may include a touch screen, display, keyboard, mouse, buttons, keys, microphone, speaker, etc.
- the user computing device 102 can include fewer components than illustrated in Fig. 1 or, conversely, additional components.
- the user computing device 102 may communicate with an external server 120 and/or a vehicle computing device 150 via a network 144.
- the network 144 may include one or more of an Ethernet-based network, a private network, a cellular network, a local area network (LAN), and/or a wide area network (WAN), such as the Internet.
- the navigation application 108 may transmit map data, navigation directions, and other geo-located content to the vehicle computing device 150 for display on the cluster display unit 151.
- the user computing device 102 may be directly connected to the vehicle computing device 150 through any suitable direct communication link 140, such as a wired connection (e.g., a USB connection).
- the network 144 may include any communication link suitable for short-range communications and may conform to a communication protocol such as, for example, Bluetooth TM (e.g., BLE), Wi-Fi (e.g., Wi-Fi Direct), NFC, ultrasonic signals, etc. Additionally, or alternatively, the network 144 may be, for example, Wi-Fi, a cellular communication link (e.g., conforming to 3G, 4G, or 5G standards), etc. In some scenarios, the network 144 may also include a wired connection.
- a communication protocol such as, for example, Bluetooth TM (e.g., BLE), Wi-Fi (e.g., Wi-Fi Direct), NFC, ultrasonic signals, etc.
- the network 144 may be, for example, Wi-Fi, a cellular communication link (e.g., conforming to 3G, 4G, or 5G standards), etc. In some scenarios, the network 144 may also include a wired connection.
- the external server 120 may be a remotely located server that includes processing capabilities and executable instructions necessary to perform some/all of the actions described herein with respect to the user computing device 102.
- the external server 120 may include a language processing module 120a that is similar to the language processing module 109a included as part of the user computing device 102, and the module 120a may include one or more of the ASR engine 109al, the TTS engine 109a2, and/or the NLP model 109a3.
- the external server 120 may also include a navigation app 120b and a ML module 120c that are similar to the navigation app 108 and ML module 109b included as part of the user computing device 102.
- the ad-hoc navigation response engine 160 and the navigation app 120b can operate as components of an ad-hoc navigation system.
- the ad-hoc navigation system can include only server- side components and simply provide the ad-hoc navigation response engine 160 with responses to requests for navigation information.
- ad-hoc navigation techniques in these embodiments can be implemented transparently to the ad-hoc navigation response engine 160.
- the entire functionality of the navigation app 120b can be implemented in the ad-hoc navigation response engine 160.
- the language processing module 109a, 120a may include separate components at the user computer device 102 and the external server 120, can only include server-side components and provide language processing outputs to the ad-hoc navigation response engine 160, or the entire functionality of the language processing module 109a, 120a can be implemented at the user computing device 102.
- the ML module 109b, 120c may include separate components at the user computer device 102 and the external server 120, can only include server-side components and provide ML outputs to the language processing module 109a, 120a, or the entire functionality of the ML module 109b, 120c can be implemented at the user computing device 102
- the vehicle computing device 150 includes one or more processor(s) 152 and a memory 153 storing computer-readable instructions executable by the processor(s) 152.
- the memory 153 may store a language processing module 153a, a navigation application 153b, and a ML module 153c that are similar to the language processing module 153a, the navigation application 108, and the ML module 109b, respectively.
- the navigation application 153b may support similar functionalities as the navigation application 108 from the vehicle-side and may facilitate rendering of information displays, as described herein.
- the user computing device 102 may provide the vehicle computing device 150 with an accepted route that has been accepted by a user, and the corresponding navigation instructions to be provided to the user as part of the accepted route.
- the navigation application 153b may then proceed to render the navigation instructions within the cluster unit display 151 and/or to generate audio outputs that verbally provide the user with the navigation instructions via the language processing module 153a.
- the external server 120 may be communicatively coupled to various databases, such as a map database 156, a traffic database 157, and a point-of-interest (POI) database 159, from which the external server 120 can retrieve navigation-related data.
- the map database 156 may include map data such as map tiles, visual maps, road geometry data, road type data, speed limit data, etc.
- the map database 156 may also include route data for providing navigation directions, such as driving, walking, biking, or public transit directions, for example.
- the traffic database 157 may store historical traffic information as well as realtime traffic information.
- the POI database 159 may store descriptions, locations, images, and other information regarding landmarks or points-of-interest. While Fig. 1 depicts databases 156, 157, and 159, the external server 120 may be communicatively coupled to additional, or conversely, fewer, databases. For example, the external server 120 may be communicatively coupled to a database storing weather data.
- Fig. 2 illustrates an example scenario 200 where a user requests navigation information without previously initiating a navigation session.
- the user request may be an audio request. More specifically, the user asks, “Hey Maps, should I take the freeway to get to my office?” 202.
- the user may be a driver, a front seat passenger, a back seat passenger in the vehicle 12, etc. While the example scenarios in Figs. 2-4 include requests related to driving directions these are merely a few examples for ease of illustration only.
- the ad-hoc navigation system can be used for any suitable mode of transportation including driving, walking, biking, or public transportation.
- the ad-hoc navigation system can also be used in the context of a touch-based or visual interface.
- user requests for navigation information can be entered via free-form textual input or through UI elements (e.g. a dropdown menu).
- Responses to the user requests can be displayed to the user via the user interface.
- Embodiments disclosed herein that are described in the context of a speech-based interface may also be applied to the context of a touch-based interface. All embodiments disclosed herein in which inputs or outputs are described in the context of a speech-based interface may be adapted to apply to the context of a touch-based interface.
- the example vehicle 12 in Fig. 2 includes a client device 10 and a head unit 14.
- the client device 10 communicates with the head unit 14 of the vehicle 12 via a communication link 16, which may be wired (e.g., Universal Serial Bus (USB)) or wireless (e.g., Bluetooth, Wi-Fi Direct).
- the client device 10 also can communicate with various content providers, servers, etc. via a wireless communication network such as a fourth- or third-generation cellular network (4G or 3G, respectively).
- a wireless communication network such as a fourth- or third-generation cellular network (4G or 3G, respectively).
- the head unit 14 can include a display 18 for presenting navigation information such as a digital map.
- the display 18 in some implementations is a touchscreen and includes a software keyboard for entering text input, which may include the name or address of a destination, point of origin, etc.
- Hardware input controls 20 and 22 on the head unit 14 and the steering wheel, respectively, can be used for entering alphanumeric characters or to perform other functions for requesting navigation directions.
- the head unit 14 also can include audio input and output components such as a microphone 24 and speakers 26, for example. The speakers 26 can be used to play audio instructions or audio notifications sent from the client device 10.
- the user has not initiated a navigation session.
- the user does not request navigation directions from the user’s current location to the user’s office prior to beginning their current trip to the office.
- the user may not have launched the navigation application 108 and may simply begin interacting with the user computing device 102 by asking, “Hey Maps, should I take the freeway to get to my office?”
- the phrase “Hey Maps” may be a hot word or trigger to activate the navigation application 108.
- the navigation application 108 may be activated and may continuously or periodically obtain location data for the user computing device 102 for example, to determine a current trajectory of the user computing device 102 but may not be executing a navigation session and/or providing navigation directions to the user’s office.
- having “not initiated a navigation session” may mean that the user computer device 102 is in a state requiring less battery usage and/or less processing power than when a navigation session has been initiated.
- the user computer device 102 may consume relatively more power when the navigation application 108 has been launched as compared to prior to launch.
- the navigation application 108 may communicate with the language processing module 109a, 120a at the user computing device 102 or the external server 120 to interpret the audio request.
- the user computing device 102 receives the audio request through an input device (e.g., microphone as part of the VO module 118).
- the user computing device 102 then utilizes the processor 104 to execute instructions included as part of the language processing module 109a to transcribe the audio request into a set of text.
- the user computing device 102 may cause the processor 104 to execute instructions comprising, for example, an ASR engine (e.g., ASR engine 109al) in order to transcribe the audio request from the speech-based input received by the VO module 118 into the textual transcription of the user input.
- an ASR engine e.g., ASR engine 109al
- the execution of the ASR engine to transcribe the user input into the textual transcription may be performed by the user computing device 102, the external server 120, the vehicle computing device 150, and/or any other suitable component or combinations thereof.
- This transcription of the audio request may then be analyzed, for example, by the processor 104 executing instructions comprising the language processing module 109a and/or the machine learning module 109b to interpret the textual transcription and determine attributes of the request, such as the destination.
- the user computing device 102 may identify the destination by comparing terms in the audio request to POIs from the POI database 159, addresses included in the map database 156, or predetermined destinations stored in a user profile, such as “Home,” “Work,” “My Office,” etc.
- the audio request does not include a destination or the language processing module 109a may identify a term corresponding to a destination, but the term refers to a destination category without specifying a particular destination (e.g., “the restaurant”).
- the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc.
- the ad-hoc navigation response engine 160 may obtain previous navigation requests and/or user context data stored at the user computing device 102 or at the external server 120. The ad-hoc navigation response engine 160 may then analyze the previous navigation requests and/or user context data in view of the audio request to infer the destination.
- the ad-hoc navigation response engine 160 may analyze recent navigation requests, set(s) of navigation directions, and/or map data requests which included restaurants as destinations.
- the ad-hoc navigation response engine 160 may also analyze the user context data to determine whether the user has an upcoming calendar event to go to a particular restaurant, whether based on the time of day/day of the week, the user frequents a particular restaurant (e.g., the user always goes to John’s Pizza for lunch on Tuesdays), etc.
- the ad-hoc navigation response engine 160 may rank the restaurants as candidate destinations. For example, the candidate destinations may be ranked according to the recency of the request. More specifically, a navigation request for directions to a first restaurant from ten minutes before the audio request may result in a higher ranking for the first restaurant than a navigation request for directions to a second restaurant from an hour ago.
- the candidate destinations may be scored and/or ranked using any suitable factors (e.g., recency of the search, likelihood that the user visits the candidate destination at the particular time of day/day of the week, whether the user has a calendar event for the candidate destination, whether the candidate destination is along the user’s current trajectory (e.g., the user is heading south and the candidate destination is south of the user’s current location), etc.).
- recency of the search likelihood that the user visits the candidate destination at the particular time of day/day of the week, whether the user has a calendar event for the candidate destination, whether the candidate destination is along the user’s current trajectory (e.g., the user is heading south and the candidate destination is south of the user’s current location), etc.
- the ad-hoc navigation response engine 160 may select the highest ranked or scored candidate destination as the destination, and may infer that the highest ranked or scored candidate destination is the destination referred to in the audio request. In other implementations, the ad-hoc navigation response engine 160 may only select a candidate destination when the candidate destination scores above a threshold score or has above a threshold likelihood of being the destination referred to in the audio request. Otherwise, the ad-hoc navigation response engine 160 may provide a response to the user asking the user to clarify the destination. [0059] In addition to identifying the destination in the user’s request for navigation information, the language processing module 109a may identify other attributes of the user’s request.
- the language processing module 109a may identify the type of request, such as information regarding an upcoming maneuver on the current trip, information regarding the duration of the current trip, traffic information for the current trip, a set of navigation directions for the current trip, etc.
- the language processing module 109a may also identify specific parameters within the request, such as the specific maneuver in the request (e.g.. a left turn, a highway entrance or exit), the location of the specific maneuver, a specified duration to compare to the remaining duration or the duration for a maneuver (e.g., “Will I arrive at my destination in 15 minutes?” “Will I still be on the freeway in 20 minutes?” etc.).
- the ad-hoc navigation response engine 160 may analyze the destination, attributes, and/or parameters included in the user’s request or referred to in the user’s request to respond to the user’s request. More specifically, the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions (e.g. from the external server) from the user’s current location to the determined or inferred destination.
- set(s) of navigation directions e.g. from the external server
- the ad-hoc navigation response engine 160 may determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request or when the navigation application 108 is activated. The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road. The ad-hoc navigation response engine 160 may then filter out sets of navigation directions from the user’s current location to the destination that do not begin by traveling in the same direction and/or on the same road as the user. In this manner, the ad- hoc navigation response engine 160 may assume that the user’s route to the destination includes the user’s current trajectory.
- the ad-hoc navigation response engine 160 may then analyze the filtered set(s) of navigation directions to generate a response to the user’s request for navigation information.
- the ad-hoc navigation response engine 160 may determine that the destination in the request 202 is the user’s office.
- the ad-hoc navigation response engine 160 may obtain the location of the user’s office for example, from a user profile stored at the user computing device 102 or the external server 120. Then the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions from the user’s current location to the user’s office.
- the ad-hoc navigation response engine 160 may also determine that the user is currently headed east and may filter sets of navigation directions which do not begin by heading east.
- the ad-hoc navigation response engine 160 may also determine, via the language processing module 109a, 120a, that the request type is for information regarding an upcoming maneuver on the current trip, that the upcoming maneuver is for a highway entrance, and the highway entrance is the nearest highway which is also closest to the destination, which is Highway 90. The ad-hoc navigation response engine 160 may then analyze the set(s) of navigation directions to determine whether any include traveling on Highway 90. In some implementations, the ad-hoc navigation response engine 160 may compare a set of navigation directions for a route that includes traveling on Highway 90 to alternative routes that do not include Highway 90. The ad-hoc navigation response engine 160 may determine attributes of the route such as whether the route that includes Highway 90 is the fastest route, whether the route includes construction, or other attributes of the route.
- the ad-hoc navigation response engine 160 may generate a response to the user’s request for example, based on the attributes of the route. For example, when the route that includes Highway 90 is the fastest route, the ad-hoc navigation response engine 160 may generate a response indicating that the user should take the freeway. When there is an alternative route which would get the user to their office faster, the ad-hoc navigation response engine 160 may generate a response indicating that the user should avoid the freeway. In another example, when the route that includes Highway 90 has a similar duration (e.g.
- the ad-hoc navigation response engine 160 may generate a response indicating that the user should avoid the freeway. More generally, the response to the user’s request may include one or more sets of navigation directions for traveling to the destination, route information for traveling to the destination, traffic information for traveling to the destination, a duration of a remaining portion of the current trip to the destination, a duration for a segment of the route (e.g., the highway portion of the route), traffic information for a segment of the route, etc.
- the described process therefore minimizes battery usage and optimizes processing efficiency by only providing the navigation directions or travel information as a response to the user’s request.
- Such benefits are present as the client device 10 or head unit 14, for example, may have their screens/displays powered off, be in a device sleep mode, or otherwise be in a device battery optimization mode until the response to the user’s request is required to be provided as an output from such devices.
- the route that includes Highway 90 is not the fastest route. Accordingly, the ad-hoc navigation response engine 160 generates a response 204 recommending that the user avoids the freeway due to traffic. The ad-hoc navigation response engine 160 may then provide the response 204 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output.
- the user computing device 102 may generate the text of the response 204 by utilizing the language processing module 109a, and in certain aspects, a large language model (LLM) (e.g., language model for dialogue applications (LaMDA)) (not shown) included as part of the language processing module 109a.
- LLM large language model
- Such an LLM may be conditioned/trained to generate the response text based on characteristics of the response, and/or the LLM may be trained to receive a natural language representation of responses to requests for navigation information as input and to output a set of text representing the audio response based on the characteristics.
- the device 102 may proceed to synthesize the text into speech for audio output of the request to the user.
- the user computing device 102 may transmit the text of the response 204 to a TTS engine (e.g., TTS engine 109a2) in order to audibly output the response 204 through a speaker (e.g., speaker 26), so that the user may hear and interpret the response.
- a TTS engine e.g., TTS engine 109a2
- a speaker e.g., speaker 26
- the user computing device 102 may also visually prompt the user by displaying the text of the response on a display screen (e.g., cluster display unit 151, user interface 116), so that the user may interact (e.g., click, tap, swipe, etc.) with the display screen and/or verbally acknowledge the response 204.
- a display screen e.g., cluster display unit 151, user interface 116
- FIG. 3 illustrates another example scenario 300 where a user requests navigation information without previously initiating a navigation session.
- the user does not include a destination in the request. More specifically, the user asks, “Hey Maps, at what exit should I leave this roundabout?” 302.
- the user did not previously initiate a navigation session before providing this request for navigation information.
- the user request does not include a destination.
- the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc. For example, the ad- hoc navigation response engine 160 may determine that the user’s most recent navigation request was for navigation directions to John’s Pizza. In another example, the ad-hoc navigation response engine 160 may determine that the user’s calendar includes a meeting at John’s Pizza within a threshold duration of the current time (e.g., 15 minutes, 30 minutes, an hour, etc.). As a result, the ad-hoc navigation response engine 160 may infer that the destination is John’s Pizza.
- a threshold duration of the current time e.g. 15 minutes, 30 minutes, an hour, etc.
- the language processing module 109a may identify other attributes of the user’s request. For example, the language processing module 109a may identify the type of request as information regarding an upcoming maneuver on the current trip. The language processing module 109a may also identify specific parameters within the request, such as the roundabout exit for the next maneuver on the current trip.
- the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions (e.g. from the external server) from the user’s current location to the inferred destination (John’s Pizza). The ad-hoc navigation response engine 160 may also determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request, when the navigation application 108 is activated, or in any other suitable situation, such as when the user computing device 102 detects that the user is traveling (e.g., the speed of the user computing device 102 is above a threshold speed, such as 10 mph). The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road.
- a threshold speed such as 10 mph
- the ad-hoc navigation response engine 160 may determine the user’s current trajectory based on the user’s request for navigation information. More specifically, the request indicates the user is traveling on a roundabout, so the ad-hoc navigation response engine 160 may determine that the particular road on which the user is traveling is a roundabout. The ad-hoc navigation response engine 160 may compare the trajectory obtained from location data to trajectory information indicated in the request (e.g., a roundabout) to ensure that the trajectory obtained from the location data matches the trajectory information indicated in the request. If there is a mismatch, the ad-hoc navigation response engine 160 may adjust the trajectory obtained from the location data to a proximate road that matches the trajectory information indicated in the request or may provide a response to the user asking for clarification regarding the request.
- trajectory information indicated in the request e.g., a roundabout
- the ad-hoc navigation response engine 160 may then filter out sets of navigation directions from the user’s current location to the destination that do not include a roundabout and/or do not include exiting a roundabout as the next maneuver.
- the ad-hoc navigation response engine 160 may then analyze the filtered set(s) of navigation directions to generate a response to the user’s request for navigation information. More specifically, the ad-hoc navigation response engine 160 may analyze the filtered set(s) of navigation directions which each include exiting a roundabout to determine the roundabout exit for traveling to John’s Pizza. In some scenarios, the ad-hoc navigation response engine 160 may identify the same roundabout exit for each of the filtered set(s) of navigation directions (e.g., the third roundabout exit), and may determine that this is the exit for traveling to John’s Pizza.
- the ad-hoc navigation response engine 160 may identify different roundabout exits for different sets of navigation directions. In these scenarios, the ad-hoc navigation response engine 160 may identify the roundabout exit for the fastest route to John’s Pizza, for the shortest route to John’s Pizza, for the route that avoids traffic or construction, etc. The ad-hoc navigation response engine 160 may then provide the response 304 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output. The response 304 may indicate the destination (e.g., To reach the restaurant, you need to take the third exit on the roundabout) so that the user can confirm that the inferred destination is the correct destination. If the inferred destination is not the correct destination, the user may provide a follow-up request.
- the destination e.g., To reach the restaurant, you need to take the third exit on the roundabout
- FIG. 4 illustrates yet another example scenario 400 where a user requests navigation information without previously initiating a navigation session.
- the user also does not include a destination in the request. More specifically, the user asks, “Hey Maps, how much longer?” 402.
- the user did not previously initiate a navigation session before providing this request for navigation information.
- the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc. For example, the ad- hoc navigation response engine 160 may determine that at the current time of day and/or day of the week (e.g., 5 p.m. on a Monday) the user typically travels home from work. As a result, the ad-hoc navigation response engine 160 may infer that the destination is the user’s home.
- the language processing module 109a may identify other attributes of the user’s request. For example, the language processing module 109a may identify the type of request as information regarding the duration of the current trip. The language processing module 109a may also identify specific parameters within the request, such as the amount of time remaining to reach the destination.
- the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions (e.g. from the external server) from the user’s current location to the inferred destination (the user’s home). The ad-hoc navigation response engine 160 may also determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request or when the navigation application 108 is activated. The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road.
- set(s) of navigation directions e.g. from the external server
- the ad-hoc navigation response engine 160 may also determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request or when the navigation application 108 is activate
- the ad-hoc navigation response engine 160 may then filter out sets of navigation directions from the user’s current location to the destination that do not begin by traveling in the same direction and on the same road as the user.
- the ad-hoc navigation response engine 160 may then analyze the filtered set(s) of navigation directions to generate a response to the user’s request for navigation information. More specifically, the ad-hoc navigation response engine 160 may analyze the filtered set(s) of navigation directions and/or current traffic data on the respective route(s) to identify the durations of each set of navigation directions.
- the ad-hoc navigation response engine 160 may then select the duration for the fastest route as the duration to include in the response 404.
- the ad-hoc navigation response engine 160 may select the duration for the most common route as the duration to include in the response 404. More specifically, the fastest route may include several side streets and/or shortcuts that most users are not typically aware of. Another route may include roads users are more likely to take such as highways, and the ad-hoc navigation response engine 160 may select the duration for this route as the most likely duration for the user. In another example, the most common route may be the route that the user typically takes according to the user’s navigation history.
- the ad-hoc navigation response engine 160 may compare the filtered set(s) of navigation directions to the route(s) from the user’s place of work to the user’s home to identify one of the filtered set(s) of navigation directions which matches the route that the user typically takes from work to their home.
- the ad-hoc navigation response engine 160 may then provide the response 404 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output.
- the response 404 may include the selected duration and may indicate that the user will reach their destination in about 30 minutes.
- Figs. 5-7 illustrate example interactions between a user 502 and the user computing device 102 when the user requests navigation information prior to initiating a navigation session.
- the user 502 does not include a destination in the request. More specifically, the user 502 asks 504a, “Hey Maps, should I get off the bus at the next stop?”
- the ad-hoc navigation response engine 160 may then infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc.
- the ad-hoc navigation response engine 160 may identify multiple candidate destinations (e.g., the user’s office and John’s Restaurant) and may rank or score the candidate destinations using any suitable factors (e.g., recency of the search, likelihood that the user visits the candidate destination at the particular time of day/day of the week, whether the user has a calendar event for the candidate destination, etc.).
- neither of the candidate destinations has a score above a threshold score, a likelihood of being the destination referred to in the request above a threshold likelihood, and/or a score that is more than a threshold amount above the score for the other candidate destination.
- the user computing device 102 may transmit 504b a first response 506 to the user 502 prompting the user 502 to clarify whether the destination is John’s Restaurant or the user’s office.
- the user 502 may then respond by transmitting 504c a clarification response indicating that the destination is the user’s office.
- the user computing device 102 may then determine the user’s current trajectory based on location data for the user computing device 102 and/or trajectory information included in the user’s request for navigation information (e.g., traveling on a bus).
- the user computing device 102 may also obtain set(s) of navigation directions from the user’s current location to the user’s office and may filter out sets of navigation directions that do not include traveling on a bus from the user’s current location.
- the user computing device 102 may then analyze the filtered set(s) of navigation directions to generate a second response 508 to the user’s request for navigation information.
- the user computing device 102 may identify the same bus stop for each of the filtered set(s) of navigation directions (e.g., Willoughby which is the next stop), and may determine that this is the bus stop for traveling to the user’s office.
- the user computing device 102 may identify different bus stops for different sets of navigation directions. In these scenarios, the user computing device 102 may identify the bus stop for the fastest route to the user’s office, for the shortest route to the user’s office, for the route that avoids traffic or construction, etc. The user computing device 102 may then provide the second response 508 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output.
- the user computing device 102 may generally allow the user 502 several seconds (e.g., 5-10 seconds) to respond following transmission of the first response 506 which includes an audio request through the speaker 26 in order to give the user 502 enough time to think of a proper response without continually listening to the interior of the automobile.
- the user computing device 102 may not activate a microphone and/or other listening device (e.g., included as part of the I/O module 118) while running the navigation app 108, and/or while processing information received through the microphone by, or in accordance with, for example, the processor 104, the language processing module 109a, the machine learning module 109b, and/or the OS 110.
- the user computing device 102 may not actively listen to a vehicle interior during a navigation session and/or at any other time, except when the user computing device 102 provides an audio request to the user 502, to which, the user computing device 102 may expect a verbal response from the user 502 within several seconds of transmission.
- Fig. 6 illustrates an example scenario 600 which is similar to the example scenario 500 of Fig. 5.
- the user 502 asks 604, “Hey Maps, should I get off the bus at the next stop?” This is the same user request as in the example scenario 500.
- the user computing device 102 identifies multiple candidate destinations (e.g., the user’s office and John’s Restaurant) for the user request, where neither of the candidate destinations has a score above a threshold score, a likelihood of being the destination referred to in the request above a threshold likelihood, and/or a score that is more than a threshold amount above the score for the other candidate destination.
- candidate destinations e.g., the user’s office and John’s Restaurant
- the user computing device 102 instead of prompting the user 502 to clarify whether the destination is John’s Restaurant or the user’s office, the user computing device 102 generates a response 606 which provides navigation information for traveling to both candidate destinations.
- the response 606 states, “To get to your office, exit the bus at Willoughby which is the next stop. To go to John’s Restaurant, exit the bus at Main Street which is in three stops.” Then the user 502 may determine the stop based on the user’s 502 destination.
- the user computing device 102 may include alternative responses for each candidate destination in the response 606, instead of selecting one of the candidate destinations as the destination for the response 606 or prompting the user 502 to clarify the destination.
- FIG. 7 illustrates yet another example scenario 700 where the user computing device prompts the user 502 to initiate a navigation session in response to the user’s request for navigation information.
- the user asks 704, “Hey Maps, how much longer will it take to get to Bill’s house?”
- the user computing device 102 may identify the address of Bill’s house from contact information stored for Bill at the user computing device 102, from previous navigation requests to Bill’s house, from a pre-stored address for Bill’s house, etc. The user computing device 102 may then obtain set(s) of navigation directions from the user’s current location to Bill’s house. [0092] The user computing device 102 may also determine the user’s 502 current trajectory based on location data for the user computing device 102. For example, the user computing device 102 may determine that the user 502 is currently traveling south on Highway 90. Then the user computing device 102 may analyze the set(s) of navigation directions in view of the user’s 502 current trajectory to generate a response to the user’s 502 request for navigation information.
- the user computing device 102 may determine that the fastest route to Bill’s house continuing on the freeway will take about 30 minutes. However, the user computing device 102 may also determine that there is a faster route if the user 502 exits Highway 90 and travels on other roads to Bill’s house.
- the user computing device 102 may generate a response 706 to the user’s 502 request indicating that the expected duration while traveling on the freeway is about 30 minutes to Bill’s house. However, there is a faster route 726 if the user 502 exits the freeway.
- the response 706 may be presented as an audio response 706 and also as a text response 724 on the user interface 116 of the user computing device 102.
- the text response 724 and/or the audio response 706 may include a prompt with user controls 724a, 724b for the user 502 to select whether they want to receive turn-by-tum directions for the faster route 726.
- the user computing device 102 may initiate the navigation session and provide navigation directions for the faster route to Bill’s house as audio directions and/or via a navigation display 722 on the user computing device 102.
- Fig. 8 is a flow diagram of an example method 800 for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, which can be implemented in a computing device, such as the user computing device 102 of Fig. 1. It is to be understood that, throughout the description of Fig. 8, actions described as being performed by the user computing device 102 may, in some implementations, be performed by the external server 120, the vehicle computing device 150, and/or may be performed by the user computing device 102, the external server 120, and/or the vehicle computing device 150 in parallel.
- the user computing device 102, the external server 120, and/or the vehicle computing device 150 may utilize the language processing module 109a, 120a, 153a and/or the machine learning module 109b, 120c, 153c to provide ad-hoc navigation information.
- the method 800 can be implemented in a set of instructions stored on a computer- readable memory and executable at one or more processors of the user computing device 102 (e.g., the processor(s) 104). For example, the method 800 may be executed by the ad-hoc navigation response engine 160.
- the ad-hoc navigation response engine 160 may receive a request for navigation information regarding a current trip prior to the user initiating a navigation session.
- the request may be received as an audio request or via text input.
- the user may provide a particular trigger phrase or hot word which causes the ad-hoc navigation response engine 160 to receive the audio request from the user, such as “Hey Maps.”
- the user may not have launched the navigation application 108 prior to providing the request.
- the navigation application 108 may be activated and may continuously or periodically obtain location data for the user computing device 102 for example, to determine a current trajectory of the user computing device 102 but may not be executing a navigation session and/or providing navigation directions to the destination.
- the ad-hoc navigation response engine 160 may determine a destination for the user’s current trip. For example, the ad-hoc navigation response engine 160 may identify the destination by comparing terms in the request for navigation information to POIs from the POI database 159, addresses included in the map database 156, or predetermined destinations stored in a user profile, such as “Home,” “Work,” “My Office,” etc.
- the request does not include a destination or the language processing module 109a may identify a term corresponding to a destination, but the term refers to a destination category without specifying a particular destination (e.g., “the restaurant”).
- the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc.
- the ad-hoc navigation response engine 160 may rank the candidate destinations.
- the candidate destinations may be scored and/or ranked using any suitable factors (e.g., recency of the search, likelihood that the user visits the candidate destination at the particular time of day/day of the week, whether the user has a calendar event for the candidate destination, etc.).
- the ad-hoc navigation response engine 160 may select the highest ranked or scored candidate destination as the destination, and may infer that the highest ranked or scored candidate destination is the destination referred to in the audio request. In other implementations, the ad-hoc navigation response engine 160 may only select a candidate destination when the candidate destination scores above a threshold score or has above a threshold likelihood of being the destination referred to in the audio request. Otherwise, the ad-hoc navigation response engine 160 may provide a response to the user asking the user to clarify the destination. In yet other implementations, the ad-hoc navigation response engine 160 may select multiple destinations and may include alternative sets of navigation information for each of the destinations in the response.
- the ad-hoc navigation response engine 160 may generate a set(s) of navigation directions for traveling from the user’s current location to the destination along a route(s) based on the user’s current trajectory.
- the ad-hoc navigation response engine 160 may communicate with the external server 120 to receive the set(s) of navigation directions from the map database 156.
- the ad-hoc navigation response engine 160 may obtain offline navigation directions cached at the user computing device 102 for example, after receiving the navigation directions from a previous request for a route to the destination.
- the ad-hoc navigation response engine 160 may determine the user’s current trajectory based on location data for the user computing device 102.
- the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request or when the navigation application 108 is activated. The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road. The ad-hoc navigation response engine 160 may filter out sets of navigation directions from the user’s current location to the destination that do not begin by traveling in the same direction and on the same road as the user.
- the ad-hoc navigation response engine 160 may provide a response to the request for navigation information based on the generated set(s) of navigation directions.
- the ad-hoc navigation response engine 160 may generate the text of the response by utilizing the language processing module 109a, and in certain aspects, an LLM (e.g., LaMDA) included as part of the language processing module 109a.
- an LLM e.g., LaMDA
- Such an LLM may be conditioned/trained to generate the response text based on characteristics of the response, and/or the LLM may be trained to receive a natural language representation of responses to requests for navigation information as input and to output a set of text representing the audio response based on the characteristics.
- the user computing device 102 may proceed to synthesize the text into speech for audio output of the request to the user.
- the user computing device 102 may transmit the text of the response to a TTS engine (e.g., TTS engine 109a2) in order to audibly output the response through a speaker (e.g., speaker 206), so that the user may hear and interpret the response.
- a TTS engine e.g., TTS engine 109a2
- speaker e.g., speaker 206
- the user computing device 102 may also visually prompt the user by displaying the text of the response on a display screen (e.g., cluster display unit 151, user interface 116), so that the user may interact (e.g., click, tap, swipe, etc.) with the display screen and/or verbally acknowledge the response.
- a display screen e.g., cluster display unit 151, user interface 116
- the user may interact (e.g., click, tap, swipe, etc.) with the display screen and/or verbally acknowledge the response.
- Modules may constitute either software modules (e.g., code stored on a machine-readable medium) or hardware modules.
- a hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner.
- one or more computer systems e.g., a standalone, client or server computer system
- one or more hardware modules of a computer system e.g., a processor or a group of processors
- software e.g., an application or application portion
- a hardware module may be implemented mechanically or electronically.
- a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application- specific integrated circuit (ASIC)) to perform certain operations.
- a hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.
- hardware should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein.
- “hardware-implemented module” refers to a hardware module. Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time. For example, where the hardware modules comprise a general-purpose processor configured using software, the general- purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.
- Hardware modules can provide information to, and receive information from, other hardware. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware modules. In embodiments in which multiple hardware modules are configured or instantiated at different times, communications between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access. For example, one hardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).
- a resource e.g., a collection of information
- the method 800 may include one or more function blocks, modules, individual functions or routines in the form of tangible computer-executable instructions that are stored in a computer-readable storage medium, optionally a non-transitory computer-readable storage medium, and executed using a processor of a computing device (e.g., a server device, a personal computer, a smart phone, a tablet computer, a smart watch, a mobile computing device, or other client computing device, as described herein).
- the method 800 may be included as part of any backend server (e.g., a map data server, a navigation server, or any other type of server computing device, as described herein), client computing device modules of the example environment, for example, or as part of a module that is external to such an environment.
- the method 800 can be utilized with other objects and user interfaces. Furthermore, although the explanation above describes steps of the method 800 being performed by specific devices (such as a user computing device), this is done for illustration purposes only. The blocks of the method 800 may be performed by one or more devices or other parts of the environment.
- processors may be temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions.
- the modules referred to herein may, in some example embodiments, comprise processor-implemented modules.
- the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or more processors or processor- implemented hardware modules. The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.
- the one or more processors may also operate to support performance of the relevant operations in a “cloud computing” environment or as an SaaS.
- a “cloud computing” environment or as an SaaS.
- at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., APIs).
Abstract
A computing device (10) may implement a method for providing navigation information regarding a current trip by a user (502) without the user (502) having previously initiated a navigation session. The method (800) may include receiving (802) a request for navigation information regarding a current trip by a user (502) prior to a user (502) initiating a navigation session, and determining (804) a destination for the current trip. The method (800) may also include generating (806) one or more sets of navigation directions for traveling from a current location of the user (502) to the destination along one or more routes based on a current trajectory of the user (502), and providing (808) a response to the request for navigation information based on the generated one or more sets of navigation directions.
Description
AD-HOC NAVIGATION INSTRUCTIONS
FIELD OF THE DISCLOSURE
[0001] The present disclosure generally relates to navigation systems and, more particularly, to providing responses to ad-hoc requests regarding navigation before the user has initiated a navigation session.
BACKGROUND
[0002] The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
[0003] Today, many users request map and navigation data for various geographic locations. Software applications executing in computers, smartphones, embedded devices, etc., generate step-by-step navigation directions in response to receiving input from a user, specifying the starting point and the destination. The navigation directions are typically generated for a route which guides the user to the destination in the shortest amount of time.
SUMMARY
[0004] In some scenarios such as when a user is familiar with the route from the user’s starting location (e.g., their home) to a destination (e.g., their place of work), the user does not request navigation directions and does not initiate a navigation session via a navigation application. However, while the user is on the route the user may have questions about the trip. For example, if there is traffic the user may want to know how much longer it will take before the user arrives at the destination. In another example, the user may want to know whether they should take an alternative route to avoid some of the traffic.
[0005] To receive answers to these questions as the user is on a current trip to a destination without having to initiate a navigation session in the middle of the trip, an ad-hoc navigation system may receive a request for navigation information from the user regarding a current trip prior to initiating a navigation session. For example, the request may be, “Hey Maps, I’m on my way to the office. Should I make a left here?” The ad-hoc navigation system may determine the destination based on the information included in the request (e.g., “the office”). In other scenarios, the ad-hoc navigation system may determine the destination based on
information included in a previous request, such as a request for navigation directions a few hours before the user began their trip. In yet other scenarios, the ad-hoc navigation system may infer the destination based on user context data, such as calendar events, the user’s typical destinations at the particular day of the week/time of day, etc.
[0006] Then based on the determined destination and/or the user’s current trajectory, the ad-hoc navigation system generates navigation directions from the user’s current location to the determined destination for providing a response to the request. For example, if the user is heading east on their current trip, the ad-hoc navigation system may generate navigation directions to the destination that begin with heading east from the current location. In some implementations, the ad-hoc navigation system generates multiple sets of navigation directions for comparison to provide a response to the request. Also in some implementations, the ad-hoc navigation system may use cached navigation directions for example, from a previous request to the destination that overlap with the current trajectory of the user.
[0007] The ad-hoc navigation system then analyzes the set(s) of navigation directions using attributes from the request to provide a response to the request. For example, in response to the user’s request as to whether they should turn left, the ad-hoc navigation system analyzes the set of navigation directions to the user’s office to determine whether the next maneuver in the set is a left turn. Then the ad-hoc navigation system provides a response to the user indicating whether the next maneuver in the set is a left turn. For example, the response may be an audio response confirming that the user is correct and should turn left. The response may also be a text or visual response indicating that the user is correct and should turn left. If the user should not turn left, the response may indicate that the user is incorrect and may ask the user if they would like to receive navigation directions to the destination. Then the ad-hoc navigation system may initiate a navigation session and provide audio and/or visual navigation instructions to the user.
[0008] In some scenarios, the ad-hoc navigation system may analyze multiple routes to the user’s office to determine which route includes a left turn for the upcoming maneuver. Then the ad-hoc navigation system may provide a response to the user indicating that the next maneuver in one route is a left turn but not in another route. For example, the response may indicate that the next maneuver in the fastest route to the office is to turn left, but if the user prefers the route with the shortest distance the user should continue straight.
[0009] By providing responses to requests for navigation information while a user is on a current trip prior to initiating a navigation session, the user does not need to initiate a navigation session at the beginning of a route. This saves battery power on the user’s client device, as well as processing power and bandwidth requirements, at least for the portion of the trip where the user does not request navigation information. Additionally, this reduces unnecessary distraction for the user when the user is familiar with most of the route as no unwanted navigation information is provided, and increases driver safety by preventing a driver from having to initiate a navigation session in the middle of a trip if the driver is lost or unsure of the next maneuver, for example. Moreover, the ad-hoc navigation system further reduces bandwidth requirements by caching navigation directions from previous requests and using the cached navigation directions, in some scenarios, to respond a request rather than obtaining the navigation directions from a server.
[0010] One example embodiment of the techniques of this disclosure is a method for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session. The method includes receiving a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session, and determining a destination for the current trip. The method also includes generating one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user, and providing a response to the request for navigation information based on the generated one or more sets of navigation directions.
[0011] Another example embodiment is a computing device for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session. The computing device includes one or more processors, and a computer- readable memory, which is optionally non-transitory, coupled to the one or more processors and storing instructions thereon that, when executed by the one or more processors, cause the computing device to receive a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session, and determine a destination for the current trip. The instructions also cause the computing device to generate one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user, and provide a response to the request for navigation information based on the generated one or more sets of navigation directions.
[0012] Yet another example embodiment is a computer-readable medium, which is optionally non-transitory, storing instructions for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, that when executed by one or more processors cause the one or more processors to: receive a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session, and determine a destination for the current trip. The instructions also cause the one or more processors to generate one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user, and provide a response to the request for navigation information based on the generated one or more sets of navigation directions.
BRIEF DESCRIPTION OF THE DRAWINGS
[0013] Fig. 1 is a block diagram of an example communication system in which techniques for providing ad-hoc navigation information can be implemented;
[0014] Fig. 2 illustrates an example scenario within a vehicle interior where a user requests navigation information without previously initiating a navigation session and includes a destination in the request;
[0015] Fig. 3 illustrates another example scenario within a vehicle interior where a user requests navigation information without previously initiating a navigation session and without including a destination in the request;
[0016] Fig. 4 illustrates yet another example scenario within a vehicle interior where a user requests navigation information without previously initiating a navigation session and without including a destination in the request;
[0017] Fig. 5 illustrates an example interaction between the user and the user computing device of Fig. 1, where the user computing device requests the user to clarify a navigation information request;
[0018] Fig. 6 illustrates another example interaction between the user and the user computing device of Fig. 1, where the user computing device provides alternative responses for multiple potential destinations;
[0019] Fig. 7 illustrates yet another example interaction between the user and the user computing device of Fig. 1, where the user computing device provides a prompt for the user to initiate a navigation session to receive navigation directions to the user’s destination; and
[0020] Fig. 8 is a flow diagram of an example method for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, which can be implemented in a computing device, such as the user computing device of Fig. 1.
DETAILED DESCRIPTION
Overview
[0021] Generally speaking, the techniques for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session can be implemented in one or several user computing devices, one or several network servers, or a system that includes a combination of these devices. However, for clarity, the examples below focus primarily on an embodiment in which a user on a current trip provides a request for navigation information to a user computing device. The user computing device analyzes the request for example, using natural language processing techniques, to determine whether a destination is included in the request. The user computing device may also analyze previous requests, and/or user context data, such as calendar events, the user’s typical destinations at the particular day of the week/time of day, etc. to determine the destination of the current trip.
[0022] The user computing device may also determine the current location and/or the current trajectory of the user computing device, for example, using sensor data from sensors in the user computing device. For example, the current trajectory may indicate that the user is heading East on Highway 101. The user computing device may provide the current location, current trajectory, and/or destination to an external server.
[0023] The external server may generate set(s) of navigation directions from the current location to the destination which include(s) the current trajectory. In some implementations, the external server then analyzes the set(s) of navigation directions using attributes from the request to generate a response to the request. The external server may then provide the response to the request to the user computing device. In other implementations, the external server provides the generated set(s) of navigation directions to the user computing device. The user computing device then analyzes the set(s) of navigation directions using attributes from the request to generate a response to the request. Then the user computing device presents the response to the user as an audio response and/or a visual response, via a user interface of the user computing device.
Example hardware and software components
[0024] Referring to Fig. 1, an example communication system 100 in which techniques for providing ad-hoc navigation information can be implemented includes a user computing device 102. The user computing device 102 may be a portable device such as a smart phone or a tablet computer, for example. The user computing device 102 may also be a laptop computer, a desktop computer, a personal digital assistant (PDA), a wearable device such as a smart watch or smart glasses, a virtual reality headset, etc. In some embodiments, the user computing device 102 may be removably mounted in a vehicle, embedded into a vehicle, and/or may be capable of interacting with a head unit of a vehicle to provide navigation instructions.
[0025] The user computing device 102 may include one or more processor(s) 104 and a memory 106 storing machine-readable instructions executable on the processor(s) 104. The processor(s) 104 may include one or more general-purpose processors (e.g., CPUs), and/or special-purpose processing units (e.g., graphical processing units (GPUs)). The memory 106 can be, optionally, a non-transitory memory and can include one or several suitable memory modules, such as random access memory (RAM), read-only memory (ROM), flash memory, other types of persistent memory, etc. The memory 106 may store instructions for implementing a navigation application 108 that can provide navigation directions (e.g., by displaying directions or emitting audio instructions via the user computing device 102), display an interactive digital map, request and receive routing data to provide driving, walking, or other navigation directions, provide various geo-located content such as traffic, points-of-interest (POIs), and weather information, etc. While the examples described herein for providing ad-hoc navigation information include driving information, the techniques may be applied to navigation information for any suitable mode of transportation, such as walking, biking, public transportation, etc.
[0026] The navigation application 108 may include an ad-hoc navigation response engine 160. The ad-hoc navigation response engine 160 may receive audio or text requests from a user for navigation information regarding the user’s current trip when the user has not initiated a navigation session for the current trip. The user may provide a particular trigger phrase or hot word which causes the ad-hoc navigation response engine 160 to receive the audio request from the user, such as “Hey Maps.” The ad-hoc navigation response engine
160 may then analyze the request using the natural language processing techniques described below to identify a destination and/or other attributes included in the request.
[0027] When the request does not include a destination, the ad-hoc navigation response engine 160 may obtain previous navigation requests from the user and may infer the destination according to the destinations in the previous navigation requests. For example, the ad-hoc navigation response engine 160 may obtain previous navigation requests stored at the user computing device 102 or at the external server 120. The ad-hoc navigation response engine 160 may also obtain user context data stored at the user computing device 102 or at the external server 120 to infer the destination, such as calendar data, typical destinations at the particular day of the week/time of day, etc.
[0028] In response to determining or inferring the destination, the ad-hoc navigation response engine 160 may transmit a request for navigation directions to the external server 120 from the user’s current location to the destination along the current trajectory. The ad- hoc navigation response engine 160 may then receive one or more set(s) of navigation directions to the destination from the external server 120 and may analyze the set(s) of navigation directions to provide a response to the request for navigation information. In other implementations, the ad-hoc navigation response engine 160 may obtain an offline set(s) of navigation directions cached at the user computing device 102 and obtained from the external server 120 in a previous request, for example.
[0029] Further, the memory 102 may include a language processing module 109a configured to implement and/or support the techniques of this disclosure for providing ad-hoc navigation information through natural conversation. Namely, the language processing module 109a may include an automatic speech recognition (ASR) engine 109al that is configured to transcribe speech inputs from a user into sets of text. Further, the language processing module 109a may include a text-to-speech (TTS) engine 109a2 that is configured to convert text into audio outputs, such as audio responses, audio requests, navigation instructions, and/or other outputs for the user. In some scenarios, the language processing module 109a may include a natural language processing (NLP) model 109a3 that is configured to output textual transcriptions, intent interpretations, and/or audio outputs related to a speech input received from a user of the user computing device 102. It should be understood that, as described herein, the ASR engine 109al and/or the TTS engine 109a2 may be included as part of the NLP model 109a3 in order to transcribe user speech inputs
into a set of text, convert text outputs into audio outputs, and/or any other suitable function described herein as part of a conversation between the user computing device 102 and the user.
[0030] Generally, the language processing module 109a may include computer-executable instructions for training and operating the NLP model 109a3. In general, the language processing module 109a may train one or more NLP models 109a3 by establishing a network architecture, or topology, and adding layers that may be associated with one or more activation functions (e.g., a rectified linear unit, softmax, etc.), loss functions and/or optimization functions. Such training may generally be performed using a symbolic method, machine learning (ML) models, and/or any other suitable training method. More generally, the language processing module 109a may train the NLP models 109a3 to perform two techniques that enable the user computing device 102, and/or any other suitable device (e.g., vehicle computing device 151) to understand the words spoken by a user and/or words generated by a tex t-to- speech program (e.g., TTS engine 109a2) executed by the processor 104: syntactic analysis and semantic analysis.
[0031] Syntactic analysis generally involves analyzing text using basic grammar rules to identify overall sentence structure, how specific words within sentences are organized, and how the words within sentences are related to one another. Syntactic analysis may include one or more sub-tasks, such as tokenization, part of speech (PoS) tagging, parsing, lemmatization and stemming, stop-word removal, and/or any other suitable sub-task or combinations thereof. For example, using syntactic analysis, the NLP model 109a3 may generate textual transcriptions from the speech inputs from the user. Additionally, or alternatively, the NLP model 109a3 may receive such textual transcriptions as a set of text from the ASR engine 109al in order to perform semantic analysis on the set of text.
[0032] Semantic analysis generally involves analyzing text in order to understand and/or otherwise capture the meaning of the text. In particular, the NLP model 109a3 applying semantic analysis may study the meaning of each individual word contained in a textual transcription in a process known as lexical semantics. Using these individual meanings, the NLP model 109a3 may then examine various combinations of words included in the sentences of the textual transcription to determine one or more contextual meanings of the words. Semantic analysis may include one or more sub-tasks, such as word sense disambiguation, relationship extraction, sentiment analysis, and/or any other suitable sub-
tasks or combinations thereof. For example, using semantic analysis, the NLP model 109a3 may generate one or more intent interpretations based on the textual transcriptions from the syntactic analysis.
[0033] In these aspects, the language processing module 109a may include an artificial intelligence (Al) trained conversational algorithm (e.g., the natural language processing (NLP) model 109a3) that is configured to interact with a user that is accessing the navigation app 108. The user may be directly connected to the navigation app 108 to provide verbal input/responses (e.g., speech inputs), and/or the user request may include textual inputs/responses that the TTS engine 109a2 (and/or other suitable engine/model/algorithm) may convert to audio inputs/responses for the NLP model 109a3 to interpret. When a user accesses the navigation app 108, the inputs/responses spoken by the user and/or generated by the TTS engine 109a2 (or other suitable algorithm) may be analyzed by the NLP model 109a3 to generate textual transcriptions and intent interpretations.
[0034] The language processing module 109a may train the one or more NLP models 109a3 to apply these and/or other NLP techniques using a plurality of training speech inputs from a plurality of users. As a result, the NLP model 109a3 may be configured to output textual transcriptions and intent interpretations corresponding to the textual transcriptions based on the syntactic analysis and semantic analysis of the user’s speech inputs.
[0035] In certain aspects, one or more types of machine learning (ML) may be employed by the language processing module 109a to train the NLP model(s) 109a3. The ML may be employed by the ML module 109b, which may store a ML model 109b 1. The ML model 109b 1 may be configured to receive a set of text corresponding to a user input, and to output an intent based on the set of text. The NLP model(s) 109a3 may be and/or include one or more types of ML models, such as the ML model 109b 1. More specifically, in these aspects, the NLP model 109a3 may be or include a machine learning model (e.g., a large language model (LLM)) trained by the ML module 109b using one or more training data sets of text in order to output one or more training intents and one or more training destinations, as described further herein. For example, artificial neural networks, recurrent neural networks, deep learning neural networks, a Bayesian model, and/or any other suitable ML model 109bl may be used to train and/or otherwise implement the NLP model(s) 109a3. In these aspects, training may be performed by iteratively training the NLP model(s) 109a3 using labeled training samples (e.g., training user inputs).
[0036] In instances where the NLP model(s) 109a3 is an artificial neural network, training of the NLP model(s) 109a3 may produce byproduct weights, or parameters which may be initialized to random values. The weights may be modified as the network is iteratively trained, by using one of several gradient descent algorithms, to reduce loss and to cause the values output by the network to converge to expected, or “learned”, values. In embodiments, a regression neural network may be selected which lacks an activation function, wherein input data may be normalized by mean centering, to determine loss and quantify the accuracy of outputs. Such normalization may use a mean squared error loss function and mean absolute error. The artificial neural network model may be validated and cross-validated using standard techniques such as hold-out, K-fold, etc. In embodiments, multiple artificial neural networks may be separately trained and operated, and/or separately trained and operated in conjunction.
[0037] In embodiments, the one or more NLP models 109a3 may include an artificial neural network having an input layer, one or more hidden layers, and an output layer. Each of the layers in the artificial neural network may include an arbitrary number of neurons. The plurality of layers may chain neurons together linearly and may pass output from one neuron to the next, or may be networked together such that the neurons communicate input and output in a non-linear way. In general, it should be understood that many configurations and/or connections of artificial neural networks are possible. For example, the input layer may correspond to input parameters that are given as full sentences, or that are separated according to word or character (e.g., fixed width) limits. The input layer may correspond to a large number of input parameters (e.g., one million inputs), in some embodiments, and may be analyzed serially or in parallel. Further, various neurons and/or neuron connections within the artificial neural network may be initialized with any number of weights and/or other training parameters. Each of the neurons in the hidden layers may analyze one or more of the input parameters from the input layer, and/or one or more outputs from a previous one or more of the hidden layers, to generate a decision or other output. The output layer may include one or more outputs, each indicating a prediction. In some embodiments and/or scenarios, the output layer includes only a single output.
[0038] It is noted that although Fig. 1 illustrates the navigation application 108 as a standalone application, the functionality of the navigation application 108 also can be provided in the form of an online service accessible via a web browser executing on the user computing device 102, as a plug-in or extension for another software application executing
on the user computing device 102, etc. The navigation application 108 generally can be provided in different versions for different operating systems. For example, the maker of the user computing device 102 can provide a Software Development Kit (SDK) including the navigation application 108 for the Android™ platform, another SDK for the iOS™ platform, etc.
[0039] The memory 106 may also store an operating system (OS) 110, which can be any type of suitable mobile or general-purpose operating system. The user computing device 102 may further include one or several sensors such as a global positioning system (GPS) 112 or another suitable positioning module, an accelerometer, a gyroscope, a compass, an inertial measurement unit (IMU), etc., a network module 114, a user interface 116 for displaying map data and directions, and input/output (I/O) module 118. The network module 114 may include one or more communication interfaces such as hardware, software, and/or firmware of an interface for enabling communications via a cellular network, a Wi-Fi network, or any other suitable network such as a network 144, discussed below. The I/O module 118 may include I/O devices capable of receiving inputs from, and providing outputs to, the ambient environment and/or a user. The I/O module 118 may include a touch screen, display, keyboard, mouse, buttons, keys, microphone, speaker, etc. In various implementations, the user computing device 102 can include fewer components than illustrated in Fig. 1 or, conversely, additional components.
[0040] The user computing device 102 may communicate with an external server 120 and/or a vehicle computing device 150 via a network 144. The network 144 may include one or more of an Ethernet-based network, a private network, a cellular network, a local area network (LAN), and/or a wide area network (WAN), such as the Internet. The navigation application 108 may transmit map data, navigation directions, and other geo-located content to the vehicle computing device 150 for display on the cluster display unit 151. Moreover, the user computing device 102 may be directly connected to the vehicle computing device 150 through any suitable direct communication link 140, such as a wired connection (e.g., a USB connection).
[0041] In certain aspects, the network 144 may include any communication link suitable for short-range communications and may conform to a communication protocol such as, for example, Bluetooth ™ (e.g., BLE), Wi-Fi (e.g., Wi-Fi Direct), NFC, ultrasonic signals, etc. Additionally, or alternatively, the network 144 may be, for example, Wi-Fi, a cellular
communication link (e.g., conforming to 3G, 4G, or 5G standards), etc. In some scenarios, the network 144 may also include a wired connection.
[0042] The external server 120 may be a remotely located server that includes processing capabilities and executable instructions necessary to perform some/all of the actions described herein with respect to the user computing device 102. For example, the external server 120 may include a language processing module 120a that is similar to the language processing module 109a included as part of the user computing device 102, and the module 120a may include one or more of the ASR engine 109al, the TTS engine 109a2, and/or the NLP model 109a3. The external server 120 may also include a navigation app 120b and a ML module 120c that are similar to the navigation app 108 and ML module 109b included as part of the user computing device 102.
[0043] The ad-hoc navigation response engine 160 and the navigation app 120b can operate as components of an ad-hoc navigation system. Alternatively, the ad-hoc navigation system can include only server- side components and simply provide the ad-hoc navigation response engine 160 with responses to requests for navigation information. In other words, ad-hoc navigation techniques in these embodiments can be implemented transparently to the ad-hoc navigation response engine 160. As another alternative, the entire functionality of the navigation app 120b can be implemented in the ad-hoc navigation response engine 160.
[0044] Similarly, the language processing module 109a, 120a may include separate components at the user computer device 102 and the external server 120, can only include server-side components and provide language processing outputs to the ad-hoc navigation response engine 160, or the entire functionality of the language processing module 109a, 120a can be implemented at the user computing device 102. Additionally, the ML module 109b, 120c may include separate components at the user computer device 102 and the external server 120, can only include server-side components and provide ML outputs to the language processing module 109a, 120a, or the entire functionality of the ML module 109b, 120c can be implemented at the user computing device 102
[0045] The vehicle computing device 150 includes one or more processor(s) 152 and a memory 153 storing computer-readable instructions executable by the processor(s) 152. The memory 153 may store a language processing module 153a, a navigation application 153b, and a ML module 153c that are similar to the language processing module 153a, the navigation application 108, and the ML module 109b, respectively. The navigation
application 153b may support similar functionalities as the navigation application 108 from the vehicle-side and may facilitate rendering of information displays, as described herein. For example, in certain aspects, the user computing device 102 may provide the vehicle computing device 150 with an accepted route that has been accepted by a user, and the corresponding navigation instructions to be provided to the user as part of the accepted route. The navigation application 153b may then proceed to render the navigation instructions within the cluster unit display 151 and/or to generate audio outputs that verbally provide the user with the navigation instructions via the language processing module 153a.
[0046] In any event, the external server 120 may be communicatively coupled to various databases, such as a map database 156, a traffic database 157, and a point-of-interest (POI) database 159, from which the external server 120 can retrieve navigation-related data. The map database 156 may include map data such as map tiles, visual maps, road geometry data, road type data, speed limit data, etc. The map database 156 may also include route data for providing navigation directions, such as driving, walking, biking, or public transit directions, for example. The traffic database 157 may store historical traffic information as well as realtime traffic information. The POI database 159 may store descriptions, locations, images, and other information regarding landmarks or points-of-interest. While Fig. 1 depicts databases 156, 157, and 159, the external server 120 may be communicatively coupled to additional, or conversely, fewer, databases. For example, the external server 120 may be communicatively coupled to a database storing weather data.
Example requests for navigation information
[0047] Fig. 2 illustrates an example scenario 200 where a user requests navigation information without previously initiating a navigation session. The user request may be an audio request. More specifically, the user asks, “Hey Maps, should I take the freeway to get to my office?” 202. The user may be a driver, a front seat passenger, a back seat passenger in the vehicle 12, etc. While the example scenarios in Figs. 2-4 include requests related to driving directions these are merely a few examples for ease of illustration only. The ad-hoc navigation system can be used for any suitable mode of transportation including driving, walking, biking, or public transportation.
[0048] Additionally, while the example scenarios described herein are generally in the context of a speech based configuration, the ad-hoc navigation system can also be used in the context of a touch-based or visual interface. For example, user requests for navigation
information can be entered via free-form textual input or through UI elements (e.g. a dropdown menu). Responses to the user requests can be displayed to the user via the user interface. Embodiments disclosed herein that are described in the context of a speech-based interface may also be applied to the context of a touch-based interface. All embodiments disclosed herein in which inputs or outputs are described in the context of a speech-based interface may be adapted to apply to the context of a touch-based interface.
[0049] The example vehicle 12 in Fig. 2 includes a client device 10 and a head unit 14. The client device 10 communicates with the head unit 14 of the vehicle 12 via a communication link 16, which may be wired (e.g., Universal Serial Bus (USB)) or wireless (e.g., Bluetooth, Wi-Fi Direct). The client device 10 also can communicate with various content providers, servers, etc. via a wireless communication network such as a fourth- or third-generation cellular network (4G or 3G, respectively).
[0050] The head unit 14 can include a display 18 for presenting navigation information such as a digital map. The display 18 in some implementations is a touchscreen and includes a software keyboard for entering text input, which may include the name or address of a destination, point of origin, etc. Hardware input controls 20 and 22 on the head unit 14 and the steering wheel, respectively, can be used for entering alphanumeric characters or to perform other functions for requesting navigation directions. The head unit 14 also can include audio input and output components such as a microphone 24 and speakers 26, for example. The speakers 26 can be used to play audio instructions or audio notifications sent from the client device 10.
[0051] In the example scenario 200, the user has not initiated a navigation session. In other words, the user does not request navigation directions from the user’s current location to the user’s office prior to beginning their current trip to the office. In some implementations, the user may not have launched the navigation application 108 and may simply begin interacting with the user computing device 102 by asking, “Hey Maps, should I take the freeway to get to my office?” The phrase “Hey Maps” may be a hot word or trigger to activate the navigation application 108. In other implementations, the navigation application 108 may be activated and may continuously or periodically obtain location data for the user computing device 102 for example, to determine a current trajectory of the user computing device 102 but may not be executing a navigation session and/or providing navigation directions to the user’s office. In another example, having “not initiated a
navigation session” may mean that the user computer device 102 is in a state requiring less battery usage and/or less processing power than when a navigation session has been initiated. For example, the user computer device 102 may consume relatively more power when the navigation application 108 has been launched as compared to prior to launch.
[0052] In any event, in response to receiving the audio request, the navigation application 108 may communicate with the language processing module 109a, 120a at the user computing device 102 or the external server 120 to interpret the audio request.
[0053] More specifically, the user computing device 102 receives the audio request through an input device (e.g., microphone as part of the VO module 118). The user computing device 102 then utilizes the processor 104 to execute instructions included as part of the language processing module 109a to transcribe the audio request into a set of text. The user computing device 102 may cause the processor 104 to execute instructions comprising, for example, an ASR engine (e.g., ASR engine 109al) in order to transcribe the audio request from the speech-based input received by the VO module 118 into the textual transcription of the user input. It should be appreciated that the execution of the ASR engine to transcribe the user input into the textual transcription may be performed by the user computing device 102, the external server 120, the vehicle computing device 150, and/or any other suitable component or combinations thereof.
[0054] This transcription of the audio request may then be analyzed, for example, by the processor 104 executing instructions comprising the language processing module 109a and/or the machine learning module 109b to interpret the textual transcription and determine attributes of the request, such as the destination. The user computing device 102 may identify the destination by comparing terms in the audio request to POIs from the POI database 159, addresses included in the map database 156, or predetermined destinations stored in a user profile, such as “Home,” “Work,” “My Office,” etc.
[0055] In some implementations, the audio request does not include a destination or the language processing module 109a may identify a term corresponding to a destination, but the term refers to a destination category without specifying a particular destination (e.g., “the restaurant”). In these scenarios, the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc. For example, the ad-hoc navigation response engine 160 may obtain previous navigation
requests and/or user context data stored at the user computing device 102 or at the external server 120. The ad-hoc navigation response engine 160 may then analyze the previous navigation requests and/or user context data in view of the audio request to infer the destination.
[0056] For example, if the audio request includes the destination as “the restaurant” without specifying which restaurant, the ad-hoc navigation response engine 160 may analyze recent navigation requests, set(s) of navigation directions, and/or map data requests which included restaurants as destinations. The ad-hoc navigation response engine 160 may also analyze the user context data to determine whether the user has an upcoming calendar event to go to a particular restaurant, whether based on the time of day/day of the week, the user frequents a particular restaurant (e.g., the user always goes to John’s Pizza for lunch on Tuesdays), etc.
[0057] In some implementations, such as when multiple restaurants may correspond to “the restaurant” based on recent navigation requests and the context data, the ad-hoc navigation response engine 160 may rank the restaurants as candidate destinations. For example, the candidate destinations may be ranked according to the recency of the request. More specifically, a navigation request for directions to a first restaurant from ten minutes before the audio request may result in a higher ranking for the first restaurant than a navigation request for directions to a second restaurant from an hour ago. The candidate destinations may be scored and/or ranked using any suitable factors (e.g., recency of the search, likelihood that the user visits the candidate destination at the particular time of day/day of the week, whether the user has a calendar event for the candidate destination, whether the candidate destination is along the user’s current trajectory (e.g., the user is heading south and the candidate destination is south of the user’s current location), etc.).
[0058] In some implementations, the ad-hoc navigation response engine 160 may select the highest ranked or scored candidate destination as the destination, and may infer that the highest ranked or scored candidate destination is the destination referred to in the audio request. In other implementations, the ad-hoc navigation response engine 160 may only select a candidate destination when the candidate destination scores above a threshold score or has above a threshold likelihood of being the destination referred to in the audio request. Otherwise, the ad-hoc navigation response engine 160 may provide a response to the user asking the user to clarify the destination.
[0059] In addition to identifying the destination in the user’s request for navigation information, the language processing module 109a may identify other attributes of the user’s request. For example, the language processing module 109a may identify the type of request, such as information regarding an upcoming maneuver on the current trip, information regarding the duration of the current trip, traffic information for the current trip, a set of navigation directions for the current trip, etc. The language processing module 109a may also identify specific parameters within the request, such as the specific maneuver in the request (e.g.. a left turn, a highway entrance or exit), the location of the specific maneuver, a specified duration to compare to the remaining duration or the duration for a maneuver (e.g., “Will I arrive at my destination in 15 minutes?” “Will I still be on the freeway in 20 minutes?” etc.).
[0060] In any event, the ad-hoc navigation response engine 160 may analyze the destination, attributes, and/or parameters included in the user’s request or referred to in the user’s request to respond to the user’s request. More specifically, the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions (e.g. from the external server) from the user’s current location to the determined or inferred destination.
[0061] In some implementations, the ad-hoc navigation response engine 160 may determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request or when the navigation application 108 is activated. The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road. The ad-hoc navigation response engine 160 may then filter out sets of navigation directions from the user’s current location to the destination that do not begin by traveling in the same direction and/or on the same road as the user. In this manner, the ad- hoc navigation response engine 160 may assume that the user’s route to the destination includes the user’s current trajectory.
[0062] The ad-hoc navigation response engine 160 may then analyze the filtered set(s) of navigation directions to generate a response to the user’s request for navigation information. In the example scenario 200, the ad-hoc navigation response engine 160 may determine that the destination in the request 202 is the user’s office. The ad-hoc navigation response engine 160 may obtain the location of the user’s office for example, from a user profile stored at the
user computing device 102 or the external server 120. Then the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions from the user’s current location to the user’s office. The ad-hoc navigation response engine 160 may also determine that the user is currently headed east and may filter sets of navigation directions which do not begin by heading east.
[0063] The ad-hoc navigation response engine 160 may also determine, via the language processing module 109a, 120a, that the request type is for information regarding an upcoming maneuver on the current trip, that the upcoming maneuver is for a highway entrance, and the highway entrance is the nearest highway which is also closest to the destination, which is Highway 90. The ad-hoc navigation response engine 160 may then analyze the set(s) of navigation directions to determine whether any include traveling on Highway 90. In some implementations, the ad-hoc navigation response engine 160 may compare a set of navigation directions for a route that includes traveling on Highway 90 to alternative routes that do not include Highway 90. The ad-hoc navigation response engine 160 may determine attributes of the route such as whether the route that includes Highway 90 is the fastest route, whether the route includes construction, or other attributes of the route.
[0064] Then the ad-hoc navigation response engine 160 may generate a response to the user’s request for example, based on the attributes of the route. For example, when the route that includes Highway 90 is the fastest route, the ad-hoc navigation response engine 160 may generate a response indicating that the user should take the freeway. When there is an alternative route which would get the user to their office faster, the ad-hoc navigation response engine 160 may generate a response indicating that the user should avoid the freeway. In another example, when the route that includes Highway 90 has a similar duration (e.g. the durations are within one or two minutes of each other or the difference in durations is less than a threshold difference) to an alternative route but the alternative route does not have as much traffic or construction, the ad-hoc navigation response engine 160 may generate a response indicating that the user should avoid the freeway. More generally, the response to the user’s request may include one or more sets of navigation directions for traveling to the destination, route information for traveling to the destination, traffic information for traveling to the destination, a duration of a remaining portion of the current trip to the destination, a duration for a segment of the route (e.g., the highway portion of the route), traffic information for a segment of the route, etc. Prior to the generation of the response, it may be that no navigation directions or travel information (such as that described above) are being provided
to the user. Beneficially, the described process therefore minimizes battery usage and optimizes processing efficiency by only providing the navigation directions or travel information as a response to the user’s request. Such benefits are present as the client device 10 or head unit 14, for example, may have their screens/displays powered off, be in a device sleep mode, or otherwise be in a device battery optimization mode until the response to the user’s request is required to be provided as an output from such devices.
[0065] In the example scenario 200, the route that includes Highway 90 is not the fastest route. Accordingly, the ad-hoc navigation response engine 160 generates a response 204 recommending that the user avoids the freeway due to traffic. The ad-hoc navigation response engine 160 may then provide the response 204 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output.
[0066] The user computing device 102 may generate the text of the response 204 by utilizing the language processing module 109a, and in certain aspects, a large language model (LLM) (e.g., language model for dialogue applications (LaMDA)) (not shown) included as part of the language processing module 109a. Such an LLM may be conditioned/trained to generate the response text based on characteristics of the response, and/or the LLM may be trained to receive a natural language representation of responses to requests for navigation information as input and to output a set of text representing the audio response based on the characteristics.
[0067] In any event, when the user computing device 102 fully generates the text of the response 204, the device 102 may proceed to synthesize the text into speech for audio output of the request to the user. In particular, the user computing device 102 may transmit the text of the response 204 to a TTS engine (e.g., TTS engine 109a2) in order to audibly output the response 204 through a speaker (e.g., speaker 26), so that the user may hear and interpret the response. Additionally, or alternatively, the user computing device 102 may also visually prompt the user by displaying the text of the response on a display screen (e.g., cluster display unit 151, user interface 116), so that the user may interact (e.g., click, tap, swipe, etc.) with the display screen and/or verbally acknowledge the response 204.
[0068] Fig. 3 illustrates another example scenario 300 where a user requests navigation information without previously initiating a navigation session. In this scenario 300, the user does not include a destination in the request. More specifically, the user asks, “Hey Maps, at
what exit should I leave this roundabout?” 302. As in the example scenario 200, the user did not previously initiate a navigation session before providing this request for navigation information.
[0069] In the scenario 300, the user request does not include a destination. Accordingly, the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc. For example, the ad- hoc navigation response engine 160 may determine that the user’s most recent navigation request was for navigation directions to John’s Pizza. In another example, the ad-hoc navigation response engine 160 may determine that the user’s calendar includes a meeting at John’s Pizza within a threshold duration of the current time (e.g., 15 minutes, 30 minutes, an hour, etc.). As a result, the ad-hoc navigation response engine 160 may infer that the destination is John’s Pizza.
[0070] In addition to inferring the destination, the language processing module 109a may identify other attributes of the user’s request. For example, the language processing module 109a may identify the type of request as information regarding an upcoming maneuver on the current trip. The language processing module 109a may also identify specific parameters within the request, such as the roundabout exit for the next maneuver on the current trip.
[0071] Then the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions (e.g. from the external server) from the user’s current location to the inferred destination (John’s Pizza). The ad-hoc navigation response engine 160 may also determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request, when the navigation application 108 is activated, or in any other suitable situation, such as when the user computing device 102 detects that the user is traveling (e.g., the speed of the user computing device 102 is above a threshold speed, such as 10 mph). The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road.
[0072] Additionally, the ad-hoc navigation response engine 160 may determine the user’s current trajectory based on the user’s request for navigation information. More specifically, the request indicates the user is traveling on a roundabout, so the ad-hoc navigation response
engine 160 may determine that the particular road on which the user is traveling is a roundabout. The ad-hoc navigation response engine 160 may compare the trajectory obtained from location data to trajectory information indicated in the request (e.g., a roundabout) to ensure that the trajectory obtained from the location data matches the trajectory information indicated in the request. If there is a mismatch, the ad-hoc navigation response engine 160 may adjust the trajectory obtained from the location data to a proximate road that matches the trajectory information indicated in the request or may provide a response to the user asking for clarification regarding the request.
[0073] In any event, the ad-hoc navigation response engine 160 may then filter out sets of navigation directions from the user’s current location to the destination that do not include a roundabout and/or do not include exiting a roundabout as the next maneuver. The ad-hoc navigation response engine 160 may then analyze the filtered set(s) of navigation directions to generate a response to the user’s request for navigation information. More specifically, the ad-hoc navigation response engine 160 may analyze the filtered set(s) of navigation directions which each include exiting a roundabout to determine the roundabout exit for traveling to John’s Pizza. In some scenarios, the ad-hoc navigation response engine 160 may identify the same roundabout exit for each of the filtered set(s) of navigation directions (e.g., the third roundabout exit), and may determine that this is the exit for traveling to John’s Pizza.
[0074] In other scenarios, the ad-hoc navigation response engine 160 may identify different roundabout exits for different sets of navigation directions. In these scenarios, the ad-hoc navigation response engine 160 may identify the roundabout exit for the fastest route to John’s Pizza, for the shortest route to John’s Pizza, for the route that avoids traffic or construction, etc. The ad-hoc navigation response engine 160 may then provide the response 304 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output. The response 304 may indicate the destination (e.g., To reach the restaurant, you need to take the third exit on the roundabout) so that the user can confirm that the inferred destination is the correct destination. If the inferred destination is not the correct destination, the user may provide a follow-up request.
[0075] Fig. 4 illustrates yet another example scenario 400 where a user requests navigation information without previously initiating a navigation session. In this scenario 400, the user
also does not include a destination in the request. More specifically, the user asks, “Hey Maps, how much longer?” 402. As in the example scenarios 200, 300, the user did not previously initiate a navigation session before providing this request for navigation information.
[0076] In the scenario 400, the user request does not include a destination. Accordingly, the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc. For example, the ad- hoc navigation response engine 160 may determine that at the current time of day and/or day of the week (e.g., 5 p.m. on a Monday) the user typically travels home from work. As a result, the ad-hoc navigation response engine 160 may infer that the destination is the user’s home.
[0077] In addition to inferring the destination, the language processing module 109a may identify other attributes of the user’s request. For example, the language processing module 109a may identify the type of request as information regarding the duration of the current trip. The language processing module 109a may also identify specific parameters within the request, such as the amount of time remaining to reach the destination.
[0078] Then the ad-hoc navigation response engine 160 may obtain set(s) of navigation directions (e.g. from the external server) from the user’s current location to the inferred destination (the user’s home). The ad-hoc navigation response engine 160 may also determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request or when the navigation application 108 is activated. The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road.
[0079] The ad-hoc navigation response engine 160 may then filter out sets of navigation directions from the user’s current location to the destination that do not begin by traveling in the same direction and on the same road as the user. The ad-hoc navigation response engine 160 may then analyze the filtered set(s) of navigation directions to generate a response to the user’s request for navigation information. More specifically, the ad-hoc navigation response engine 160 may analyze the filtered set(s) of navigation directions and/or current traffic data
on the respective route(s) to identify the durations of each set of navigation directions. The ad-hoc navigation response engine 160 may then select the duration for the fastest route as the duration to include in the response 404.
[0080] In other implementations, the ad-hoc navigation response engine 160 may select the duration for the most common route as the duration to include in the response 404. More specifically, the fastest route may include several side streets and/or shortcuts that most users are not typically aware of. Another route may include roads users are more likely to take such as highways, and the ad-hoc navigation response engine 160 may select the duration for this route as the most likely duration for the user. In another example, the most common route may be the route that the user typically takes according to the user’s navigation history. The ad-hoc navigation response engine 160 may compare the filtered set(s) of navigation directions to the route(s) from the user’s place of work to the user’s home to identify one of the filtered set(s) of navigation directions which matches the route that the user typically takes from work to their home.
[0081] The ad-hoc navigation response engine 160 may then provide the response 404 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output. The response 404 may include the selected duration and may indicate that the user will reach their destination in about 30 minutes.
[0082] Figs. 5-7 illustrate example interactions between a user 502 and the user computing device 102 when the user requests navigation information prior to initiating a navigation session. In the example scenario 500 of Fig. 5, the user 502 does not include a destination in the request. More specifically, the user 502 asks 504a, “Hey Maps, should I get off the bus at the next stop?”
[0083] The ad-hoc navigation response engine 160 may then infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc. The ad-hoc navigation response engine 160 may identify multiple candidate destinations (e.g., the user’s office and John’s Restaurant) and may rank or score the candidate destinations using any suitable factors (e.g., recency of the search, likelihood that the user visits the candidate destination at the particular time of day/day of the week, whether the user has a calendar event for the candidate destination, etc.). In this scenario 500, neither of the candidate
destinations has a score above a threshold score, a likelihood of being the destination referred to in the request above a threshold likelihood, and/or a score that is more than a threshold amount above the score for the other candidate destination.
[0084] As a result, the user computing device 102 may transmit 504b a first response 506 to the user 502 prompting the user 502 to clarify whether the destination is John’s Restaurant or the user’s office. The user 502 may then respond by transmitting 504c a clarification response indicating that the destination is the user’s office. The user computing device 102 may then determine the user’s current trajectory based on location data for the user computing device 102 and/or trajectory information included in the user’s request for navigation information (e.g., traveling on a bus). The user computing device 102 may also obtain set(s) of navigation directions from the user’s current location to the user’s office and may filter out sets of navigation directions that do not include traveling on a bus from the user’s current location.
[0085] The user computing device 102 may then analyze the filtered set(s) of navigation directions to generate a second response 508 to the user’s request for navigation information. In some scenarios, the user computing device 102 may identify the same bus stop for each of the filtered set(s) of navigation directions (e.g., Willoughby which is the next stop), and may determine that this is the bus stop for traveling to the user’s office.
[0086] In other scenarios, the user computing device 102 may identify different bus stops for different sets of navigation directions. In these scenarios, the user computing device 102 may identify the bus stop for the fastest route to the user’s office, for the shortest route to the user’s office, for the route that avoids traffic or construction, etc. The user computing device 102 may then provide the second response 508 to the user’s request for navigation information as an audio output via a speaker, as a visual output on the user interface 116 and/or as a combination of audio/visual output.
[0087] It should be noted that the user computing device 102 may generally allow the user 502 several seconds (e.g., 5-10 seconds) to respond following transmission of the first response 506 which includes an audio request through the speaker 26 in order to give the user 502 enough time to think of a proper response without continually listening to the interior of the automobile. By default, the user computing device 102 may not activate a microphone and/or other listening device (e.g., included as part of the I/O module 118) while running the navigation app 108, and/or while processing information received through the microphone
by, or in accordance with, for example, the processor 104, the language processing module 109a, the machine learning module 109b, and/or the OS 110. Thus, the user computing device 102 may not actively listen to a vehicle interior during a navigation session and/or at any other time, except when the user computing device 102 provides an audio request to the user 502, to which, the user computing device 102 may expect a verbal response from the user 502 within several seconds of transmission.
[0088] Fig. 6 illustrates an example scenario 600 which is similar to the example scenario 500 of Fig. 5. In the example scenario 600, the user 502 asks 604, “Hey Maps, should I get off the bus at the next stop?” This is the same user request as in the example scenario 500. Accordingly, the user computing device 102 identifies multiple candidate destinations (e.g., the user’s office and John’s Restaurant) for the user request, where neither of the candidate destinations has a score above a threshold score, a likelihood of being the destination referred to in the request above a threshold likelihood, and/or a score that is more than a threshold amount above the score for the other candidate destination.
[0089] However, instead of prompting the user 502 to clarify whether the destination is John’s Restaurant or the user’s office, the user computing device 102 generates a response 606 which provides navigation information for traveling to both candidate destinations.
More specifically, the response 606 states, “To get to your office, exit the bus at Willoughby which is the next stop. To go to John’s Restaurant, exit the bus at Main Street which is in three stops.” Then the user 502 may determine the stop based on the user’s 502 destination. The user computing device 102 may include alternative responses for each candidate destination in the response 606, instead of selecting one of the candidate destinations as the destination for the response 606 or prompting the user 502 to clarify the destination.
[0090] Fig. 7 illustrates yet another example scenario 700 where the user computing device prompts the user 502 to initiate a navigation session in response to the user’s request for navigation information. In the example scenario 700, the user asks 704, “Hey Maps, how much longer will it take to get to Bill’s house?”
[0091] The user computing device 102 may identify the address of Bill’s house from contact information stored for Bill at the user computing device 102, from previous navigation requests to Bill’s house, from a pre-stored address for Bill’s house, etc. The user computing device 102 may then obtain set(s) of navigation directions from the user’s current location to Bill’s house.
[0092] The user computing device 102 may also determine the user’s 502 current trajectory based on location data for the user computing device 102. For example, the user computing device 102 may determine that the user 502 is currently traveling south on Highway 90. Then the user computing device 102 may analyze the set(s) of navigation directions in view of the user’s 502 current trajectory to generate a response to the user’s 502 request for navigation information. In the example scenario 700, the user computing device 102 may determine that the fastest route to Bill’s house continuing on the freeway will take about 30 minutes. However, the user computing device 102 may also determine that there is a faster route if the user 502 exits Highway 90 and travels on other roads to Bill’s house.
[0093] Accordingly, the user computing device 102 may generate a response 706 to the user’s 502 request indicating that the expected duration while traveling on the freeway is about 30 minutes to Bill’s house. However, there is a faster route 726 if the user 502 exits the freeway. The response 706 may be presented as an audio response 706 and also as a text response 724 on the user interface 116 of the user computing device 102. Also, the text response 724 and/or the audio response 706 may include a prompt with user controls 724a, 724b for the user 502 to select whether they want to receive turn-by-tum directions for the faster route 726. In response to receiving a selection of the user control 724a indicating that the user 502 would like to initiate a navigation session (e.g., via a touch selection or audio input), the user computing device 102 may initiate the navigation session and provide navigation directions for the faster route to Bill’s house as audio directions and/or via a navigation display 722 on the user computing device 102.
Example method for providing ad-hoc navigation information
[0094] Fig. 8 is a flow diagram of an example method 800 for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, which can be implemented in a computing device, such as the user computing device 102 of Fig. 1. It is to be understood that, throughout the description of Fig. 8, actions described as being performed by the user computing device 102 may, in some implementations, be performed by the external server 120, the vehicle computing device 150, and/or may be performed by the user computing device 102, the external server 120, and/or the vehicle computing device 150 in parallel. For example, the user computing device 102, the external server 120, and/or the vehicle computing device 150 may utilize the language
processing module 109a, 120a, 153a and/or the machine learning module 109b, 120c, 153c to provide ad-hoc navigation information.
[0095] The method 800 can be implemented in a set of instructions stored on a computer- readable memory and executable at one or more processors of the user computing device 102 (e.g., the processor(s) 104). For example, the method 800 may be executed by the ad-hoc navigation response engine 160.
[0096] At block 802, the ad-hoc navigation response engine 160 may receive a request for navigation information regarding a current trip prior to the user initiating a navigation session. The request may be received as an audio request or via text input. The user may provide a particular trigger phrase or hot word which causes the ad-hoc navigation response engine 160 to receive the audio request from the user, such as “Hey Maps.” In some implementations, the user may not have launched the navigation application 108 prior to providing the request. In other implementations, the navigation application 108 may be activated and may continuously or periodically obtain location data for the user computing device 102 for example, to determine a current trajectory of the user computing device 102 but may not be executing a navigation session and/or providing navigation directions to the destination.
[0097] Then at block 804, the ad-hoc navigation response engine 160 may determine a destination for the user’s current trip. For example, the ad-hoc navigation response engine 160 may identify the destination by comparing terms in the request for navigation information to POIs from the POI database 159, addresses included in the map database 156, or predetermined destinations stored in a user profile, such as “Home,” “Work,” “My Office,” etc.
[0098] In some implementations, the request does not include a destination or the language processing module 109a may identify a term corresponding to a destination, but the term refers to a destination category without specifying a particular destination (e.g., “the restaurant”). In these scenarios, the ad-hoc navigation response engine 160 may infer the destination using previous navigation requests having destinations and/or user context data, such as calendar data, typical destinations at the particular day of the week/time of day, etc.
[0099] In some implementations, such as when multiple destinations may correspond to the destination based on recent navigation requests and the context data, the ad-hoc navigation response engine 160 may rank the candidate destinations. The candidate
destinations may be scored and/or ranked using any suitable factors (e.g., recency of the search, likelihood that the user visits the candidate destination at the particular time of day/day of the week, whether the user has a calendar event for the candidate destination, etc.).
[0100] In some implementations, the ad-hoc navigation response engine 160 may select the highest ranked or scored candidate destination as the destination, and may infer that the highest ranked or scored candidate destination is the destination referred to in the audio request. In other implementations, the ad-hoc navigation response engine 160 may only select a candidate destination when the candidate destination scores above a threshold score or has above a threshold likelihood of being the destination referred to in the audio request. Otherwise, the ad-hoc navigation response engine 160 may provide a response to the user asking the user to clarify the destination. In yet other implementations, the ad-hoc navigation response engine 160 may select multiple destinations and may include alternative sets of navigation information for each of the destinations in the response.
[0101] At block 806, the ad-hoc navigation response engine 160 may generate a set(s) of navigation directions for traveling from the user’s current location to the destination along a route(s) based on the user’s current trajectory. For example, the ad-hoc navigation response engine 160 may communicate with the external server 120 to receive the set(s) of navigation directions from the map database 156. In other implementations, the ad-hoc navigation response engine 160 may obtain offline navigation directions cached at the user computing device 102 for example, after receiving the navigation directions from a previous request for a route to the destination. The ad-hoc navigation response engine 160 may determine the user’s current trajectory based on location data for the user computing device 102. For example, the ad-hoc navigation response engine 160 may continuously or periodically obtain the current location of the user computing device 102 in response to the user’s request or when the navigation application 108 is activated. The ad-hoc navigation response engine 160 may then determine that the user is traveling in a particular direction on a particular road. The ad-hoc navigation response engine 160 may filter out sets of navigation directions from the user’s current location to the destination that do not begin by traveling in the same direction and on the same road as the user.
[0102] Then at block 808, the ad-hoc navigation response engine 160 may provide a response to the request for navigation information based on the generated set(s) of navigation
directions. The ad-hoc navigation response engine 160 may generate the text of the response by utilizing the language processing module 109a, and in certain aspects, an LLM (e.g., LaMDA) included as part of the language processing module 109a. Such an LLM may be conditioned/trained to generate the response text based on characteristics of the response, and/or the LLM may be trained to receive a natural language representation of responses to requests for navigation information as input and to output a set of text representing the audio response based on the characteristics.
[0103] In any event, when the ad-hoc navigation response engine 160 fully generates the text of the response, the user computing device 102 may proceed to synthesize the text into speech for audio output of the request to the user. In particular, the user computing device 102 may transmit the text of the response to a TTS engine (e.g., TTS engine 109a2) in order to audibly output the response through a speaker (e.g., speaker 206), so that the user may hear and interpret the response. Additionally, or alternatively, the user computing device 102 may also visually prompt the user by displaying the text of the response on a display screen (e.g., cluster display unit 151, user interface 116), so that the user may interact (e.g., click, tap, swipe, etc.) with the display screen and/or verbally acknowledge the response.
Additional considerations
[0104] The following additional considerations apply to the foregoing discussion. Throughout this specification, plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements fall within the scope of the subject matter of the present disclosure.
[0105] Additionally, certain embodiments are described herein as including logic or a number of components, modules, or mechanisms. Modules may constitute either software modules (e.g., code stored on a machine-readable medium) or hardware modules. A hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments, one or more computer
systems (e.g., a standalone, client or server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein.
[0106] In various embodiments, a hardware module may be implemented mechanically or electronically. For example, a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application- specific integrated circuit (ASIC)) to perform certain operations. A hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.
[0107] Accordingly, the term hardware should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein. As used herein “hardware-implemented module” refers to a hardware module. Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time. For example, where the hardware modules comprise a general-purpose processor configured using software, the general- purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.
[0108] Hardware modules can provide information to, and receive information from, other hardware. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware modules. In embodiments in which multiple hardware modules are configured or instantiated at different times, communications
between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access. For example, one hardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).
[0109] The method 800 may include one or more function blocks, modules, individual functions or routines in the form of tangible computer-executable instructions that are stored in a computer-readable storage medium, optionally a non-transitory computer-readable storage medium, and executed using a processor of a computing device (e.g., a server device, a personal computer, a smart phone, a tablet computer, a smart watch, a mobile computing device, or other client computing device, as described herein). The method 800 may be included as part of any backend server (e.g., a map data server, a navigation server, or any other type of server computing device, as described herein), client computing device modules of the example environment, for example, or as part of a module that is external to such an environment. Though the figures may be described with reference to the other figures for ease of explanation, the method 800 can be utilized with other objects and user interfaces. Furthermore, although the explanation above describes steps of the method 800 being performed by specific devices (such as a user computing device), this is done for illustration purposes only. The blocks of the method 800 may be performed by one or more devices or other parts of the environment.
[0110] The various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions. The modules referred to herein may, in some example embodiments, comprise processor-implemented modules.
[0111] Similarly, the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or more processors or processor- implemented hardware modules. The performance of certain of the operations may be distributed among the one or more
processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.
[0112] The one or more processors may also operate to support performance of the relevant operations in a “cloud computing” environment or as an SaaS. For example, as indicated above, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., APIs).
[0113] Still further, the figures depict some embodiments of the example environment for purposes of illustration only. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein.
[0114] Upon reading this disclosure, those of skill in the art will appreciate still additional alternative structural and functional designs for determining places and routes through natural conversation through the disclosed principles herein. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. Various modifications, changes and variations, which will be apparent to those skilled in the art, may be made in the arrangement, operation and details of the method and apparatus disclosed herein without departing from the spirit and scope defined in the appended claims.
Claims
1. A method for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, the method comprising: receiving, at one or more processors, a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session; determining, by the one or more processors, a destination for the current trip; generating, by the one or more processors, one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user; and providing, by the one or more processors, a response to the request for navigation information based on the generated one or more sets of navigation directions.
2. The method of claim 1, wherein the request for navigation information is received prior to launching a navigation application.
3. The method of claim 1 or 2, wherein the request does not include the destination for the current trip.
4. The method of any preceding claim, wherein determining the destination for the current trip includes at least one of: determining, by the one or more processors, the destination for the current trip as a destination included in the request for navigation information; inferring, by the one or more processors, the destination for the current trip based on a destination included in a previous request; or inferring, by the one or more processors, the destination for the current trip based on user context data.
5. The method of any preceding claim, wherein providing a response to the request for navigation information includes providing at least one of: the generated one or more sets of navigation directions, route information for traveling to the destination,
traffic information for traveling to the destination, or a duration of a remaining portion of the current trip to the destination.
6. The method of any preceding claim, wherein the destination is a destination included in a previous request and wherein generating the one or more sets of navigation directions includes: obtaining, by the one or more processors, the one or more sets of navigation directions cached from the previous request.
7. The method of any preceding claim, wherein the request is an audio request and the response to the request is an audio response.
8. The method of any preceding claim, wherein the request is for a duration of a remaining portion of the current trip to a particular destination, and the response includes an indication of the duration of the remaining portion of the current trip to the particular destination based on a most likely route to the particular destination according to the current trajectory and current traffic data on the route.
9. A computing device for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, the computing device comprising: one or more processors; and a computer-readable memory coupled to the one or more processors and storing instructions thereon that, when executed by the one or more processors, cause the computing device to: receive a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session; determine a destination for the current trip; generate one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user; and provide a response to the request for navigation information based on the generated one or more sets of navigation directions.
10. The computing device of claim 9, wherein the request for navigation information is received prior to launching a navigation application.
11. The computing device of claim 9 or 10, wherein the request does not include the destination for the current trip.
12. The computing device of any of claims 9 to 11, wherein to determine the destination for the current trip, the instructions cause the computing device to at least one of: determine the destination for the current trip as a destination included in the request for navigation information; infer the destination for the current trip based on a destination included in a previous request; or infer the destination for the current trip based on user context data.
13. The computing device of any of claims 9 to 12, wherein to provide a response to the request for navigation information, the instructions cause the computing device to provide at least one of: the generated one or more sets of navigation directions, route information for traveling to the destination, traffic information for traveling to the destination, or a duration of a remaining portion of the current trip to the destination.
14. The computing device of any of claims 9 to 13, wherein the destination is a destination included in a previous request and wherein to generate the one or more sets of navigation directions, the instructions cause the computing device to: obtain the one or more sets of navigation directions cached from the previous request.
15. The computing device of any of claims 9 to 14, wherein the request is an audio request and the response to the request is an audio response.
16. The computing device of any of claims 9 to 15, wherein the request is for a duration of a remaining portion of the current trip to a particular destination, and the response includes an indication of the duration of the remaining portion of the current trip to the
particular destination based on a most likely route to the particular destination according to the current trajectory and current traffic data on the route.
17. A computer-readable medium storing instructions for providing navigation information regarding a current trip by a user without the user having previously initiated a navigation session, that when executed by one or more processors cause the one or more processors to: receive a request for navigation information regarding a current trip by a user prior to a user initiating a navigation session; determine a destination for the current trip; generate one or more sets of navigation directions for traveling from a current location of the user to the destination along one or more routes based on a current trajectory of the user; and provide a response to the request for navigation information based on the generated one or more sets of navigation directions.
18. The computer-readable medium of claim 17, wherein the request for navigation information is received prior to launching a navigation application.
19. The computer-readable medium of claim 17 or 18, wherein the request does not include the destination for the current trip.
20. The computer-readable medium of any of claims 17 to 19, wherein to determine the destination for the current trip, the instructions cause the one or more processors to at least one of: determine the destination for the current trip as a destination included in the request for navigation information; infer the destination for the current trip based on a destination included in a previous request; or infer the destination for the current trip based on user context data.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2022/037679 WO2024019710A1 (en) | 2022-07-20 | 2022-07-20 | Ad-hoc navigation instructions |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2022/037679 WO2024019710A1 (en) | 2022-07-20 | 2022-07-20 | Ad-hoc navigation instructions |
Publications (1)
Publication Number | Publication Date |
---|---|
WO2024019710A1 true WO2024019710A1 (en) | 2024-01-25 |
Family
ID=82899408
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2022/037679 WO2024019710A1 (en) | 2022-07-20 | 2022-07-20 | Ad-hoc navigation instructions |
Country Status (1)
Country | Link |
---|---|
WO (1) | WO2024019710A1 (en) |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090150156A1 (en) * | 2007-12-11 | 2009-06-11 | Kennewick Michael R | System and method for providing a natural language voice user interface in an integrated voice navigation services environment |
US20200333157A1 (en) * | 2012-06-05 | 2020-10-22 | Apple Inc. | Voice instructions during navigation |
-
2022
- 2022-07-20 WO PCT/US2022/037679 patent/WO2024019710A1/en active Application Filing
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090150156A1 (en) * | 2007-12-11 | 2009-06-11 | Kennewick Michael R | System and method for providing a natural language voice user interface in an integrated voice navigation services environment |
US20200333157A1 (en) * | 2012-06-05 | 2020-10-22 | Apple Inc. | Voice instructions during navigation |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102338990B1 (en) | Dialogue processing apparatus, vehicle having the same and dialogue processing method | |
US10347248B2 (en) | System and method for providing in-vehicle services via a natural language voice user interface | |
KR102426171B1 (en) | Dialogue processing apparatus, vehicle having the same and dialogue service processing method | |
KR102414456B1 (en) | Dialogue processing apparatus, vehicle having the same and accident information processing method | |
WO2016054230A1 (en) | Voice and connection platform | |
US11443747B2 (en) | Artificial intelligence apparatus and method for recognizing speech of user in consideration of word usage frequency | |
US20190385606A1 (en) | Artificial intelligence device for performing speech recognition | |
KR20200000155A (en) | Dialogue system and vehicle using the same | |
KR20200042127A (en) | Dialogue processing apparatus, vehicle having the same and dialogue processing method | |
KR20190131741A (en) | Dialogue system, and dialogue processing method | |
US11508358B2 (en) | Artificial intelligence apparatus and method for recognizing speech in consideration of utterance style | |
US20220299335A1 (en) | Content-aware navigation instructions | |
KR20200098079A (en) | Dialogue system, and dialogue processing method | |
KR20200006738A (en) | Dialogue system, and dialogue processing method | |
US11333518B2 (en) | Vehicle virtual assistant systems and methods for storing and utilizing data associated with vehicle stops | |
KR102487669B1 (en) | Dialogue processing apparatus, vehicle having the same and dialogue processing method | |
WO2024019710A1 (en) | Ad-hoc navigation instructions | |
KR102448719B1 (en) | Dialogue processing apparatus, vehicle and mobile device having the same, and dialogue processing method | |
US20220208187A1 (en) | Information processing device, information processing method, and storage medium | |
WO2024072392A1 (en) | Providing inverted directions and other information based on a current or recent journey | |
WO2023214959A1 (en) | Determining places and routes through natural conversation | |
WO2023234929A1 (en) | Requesting and receiving reminder instructions in a navigation session | |
US20240067128A1 (en) | Supporting multiple roles in voice-enabled navigation | |
KR20200095636A (en) | Vehicle equipped with dialogue processing system and control method thereof | |
US20240102816A1 (en) | Customizing Instructions During a Navigations Session |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
WWE | Wipo information: entry into national phase |
Ref document number: 18016732Country of ref document: US |
|
121 | Ep: the epo has been informed by wipo that ep was designated in this application |
Ref document number: 22754628Country of ref document: EPKind code of ref document: A1 |