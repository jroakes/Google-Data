JP6479020B2 - Hierarchical chunking of objects in a distributed storage system - Google Patents
Hierarchical chunking of objects in a distributed storage system Download PDFInfo
- Publication number
- JP6479020B2 JP6479020B2 JP2016543054A JP2016543054A JP6479020B2 JP 6479020 B2 JP6479020 B2 JP 6479020B2 JP 2016543054 A JP2016543054 A JP 2016543054A JP 2016543054 A JP2016543054 A JP 2016543054A JP 6479020 B2 JP6479020 B2 JP 6479020B2
- Authority
- JP
- Japan
- Prior art keywords
- journal
- chunk
- metadata
- stored
- instance
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims description 75
- 230000010076 replication Effects 0.000 claims description 31
- 230000003362 replicative effect Effects 0.000 claims 2
- 238000009877 rendering Methods 0.000 claims 1
- 238000004891 communication Methods 0.000 description 21
- 230000008569 process Effects 0.000 description 21
- 238000010586 diagram Methods 0.000 description 14
- 238000005056 compaction Methods 0.000 description 6
- 238000012217 deletion Methods 0.000 description 5
- 230000037430 deletion Effects 0.000 description 5
- 238000012545 processing Methods 0.000 description 5
- 230000006870 function Effects 0.000 description 4
- 239000007787 solid Substances 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 238000010572 single replacement reaction Methods 0.000 description 3
- 230000008859 change Effects 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000013467 fragmentation Methods 0.000 description 2
- 238000006062 fragmentation reaction Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 238000012800 visualization Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/27—Replication, distribution or synchronisation of data between databases or within a distributed database system; Distributed database system architectures therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/22—Indexing; Data structures therefor; Storage structures
- G06F16/2228—Indexing structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/22—Indexing; Data structures therefor; Storage structures
- G06F16/2291—User-Defined Types; Storage management thereof
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0602—Interfaces specially adapted for storage systems specifically adapted to achieve a particular effect
- G06F3/0614—Improving the reliability of storage systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0646—Horizontal data movement in storage systems, i.e. moving data in between storage devices or systems
- G06F3/065—Replication mechanisms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0668—Interfaces specially adapted for storage systems adopting a particular infrastructure
- G06F3/067—Distributed or networked storage systems, e.g. storage area networks [SAN], network attached storage [NAS]
Description
技術分野
開示される実現例は一般に、分散ストレージシステムに関し、より具体的には、オブジェクトをチャンクに分割し、当該チャンクを階層的に格納することに関する。
TECHNICAL FIELD The disclosed implementations generally relate to distributed storage systems, and more specifically to dividing an object into chunks and storing the chunks hierarchically.
背景
大規模データストレージは、中央サービスアーキテクチャから分散ストレージシステムにシフトしている。コモディティのコンピュータから構築される分散ストレージシステムは、モノリシックディスクアレイと比較して数分の１のコストで、高い性能（high performance）、可用性（availability）およびスケーラビリティ（scalability）を提供することが可能である。データは、異なる地理的位置における分散ストレージシステムの複数のインスタンスにわたってレプリケート（replicate）され、これにより可用性を増加するとともにクライアントからのネットワーク距離を低減する。
Background Large data storage is shifting from a central services architecture to a distributed storage system. Distributed storage systems built from commodity computers can provide high performance, availability, and scalability at a fraction of the cost of monolithic disk arrays. is there. Data is replicated across multiple instances of a distributed storage system at different geographic locations, thereby increasing availability and reducing network distance from clients.
分散ストレージシステムにおいて、オブジェクトは、制約に基づいて、分散ストレージシステムのさまざまなインスタンスに動的に配置される（すなわち当該インスタンスにおいて作成され、当該インスタンスから削除され、および／または、当該インスタンスに移動される）。何兆個ものオブジェクトと何ペタバイトものデータとを格納するとともにこの地球にわたって何十ものデータセンタを含むプラネットワイドな分散ストレージシステムにおける制約に従うオブジェクトを効率的に配置するための既存の技術はほとんど存在しない。 In a distributed storage system, objects are dynamically placed on various instances of the distributed storage system based on constraints (ie created, deleted from the instance, and / or moved to the instance). ) Few existing technologies exist to efficiently place objects that comply with constraints in planet-wide distributed storage systems that store trillions of objects and petabytes of data and include dozens of data centers across the globe .
新しい視覚化アプリケーション、マルチメディアアプリケーションおよび他のデータ集中的なアプリケーションは、何百ギガバイト以上であり得る非常に大きいオブジェクトを使用する。そのような非常に大きいオブジェクトを管理することにより、分散ストレージシステムについてさらなる複雑性が作り出されている。まず、分散ストレージシステムへそのようなオブジェクトをアップロードすることは典型的にストリーミングモードで行なわれ、オブジェクトをチャンクに分割し、各チャンクを個々に書き込む。これにより、アップロードについて長い遅延が課され得、これは、潜在的なクライアント障害およびサーバ障害によって悪化する。さらに、よりよい効率性のために、チャンクは、より大きなシャード（shard）になるように集合され得る。本願明細書において、「シャード」および「ジャーナル（journal）」という用語は、交換可能に使用され得る。結果として、クライアントがある時に利用可能な任意のクラスタに接続することを可能にする大規模システムの必要性によって前進するストレージ産業にとって、大きなオブジェクトの効率的なアップロードは、ますます重要になっている。さらに、単一のオブジェクトについてのメタデータのボリューム（たとえば１００ギガバイトのファイルについて２５０００個のチャンクがあり、各チャンクは４メガバイトである）では、レプリケーション（replication）およびコンパクション（compaction）はあまり効率的でない。 New visualization applications, multimedia applications, and other data intensive applications use very large objects that can be hundreds of gigabytes or more. Managing such very large objects creates additional complexity for distributed storage systems. First, uploading such objects to a distributed storage system is typically done in streaming mode, dividing the object into chunks and writing each chunk individually. This can impose a long delay on the upload, which is exacerbated by potential client and server failures. Further, for better efficiency, chunks can be aggregated into larger shards. In this specification, the terms “shard” and “journal” may be used interchangeably. As a result, efficient uploading of large objects is becoming increasingly important for the storage industry to move forward with the need for large systems that allow clients to connect to any available cluster at one time . In addition, with a volume of metadata for a single object (eg, there are 25000 chunks for a 100 gigabyte file, each chunk is 4 megabytes), replication and compaction are less efficient .
概要
開示される実現例は、大きいオブジェクトのアップロードを複数のストレージ位置に同時に分散する。本願明細書において使用されるように、ストレージ位置は、「シャード」、「集合シャード」または「ジャーナル」と称される。このスキームは、大きいオブジェクトを複数のチャンクへ分割することにより実現され、当該複数のチャンクの各々は、（異なる地理的位置にあってもよい）異なるストレージクラスタにアップロードされ得る。（たとえばシャードが「いっぱい」であるか、または、シャードが格納されるインスタンスがダウンしたので）シャードがアップロードの間に利用不可能になると、クライアントは、異なるクラスタに存在し得る新しいシャードにスイッチングする。このスキームは、ひとたびスタートされると、同じシャードにとどまることを必要としない。終了したオブジェクトは、チャンク参照の順序づけられたリストによって表わされる。
Overview The disclosed implementation distributes large object uploads to multiple storage locations simultaneously. As used herein, storage locations are referred to as “shards”, “collective shards” or “journals”. This scheme is realized by dividing a large object into multiple chunks, each of which can be uploaded to a different storage cluster (which may be at a different geographic location). If a shard becomes unavailable during an upload (for example, because the shard is "full" or the instance where the shard is stored is down), the client switches to a new shard that can exist in a different cluster . This scheme, once started, does not require staying in the same shard. A finished object is represented by an ordered list of chunk references.
いくつかのスキームにおいて、チャンクは格納の基本単位であり、各チャンクの位置はグローバルメタデータに格納される。非常に大きいオブジェクトの場合、このスキームでは、単一のオブジェクトについて、グローバルレベルでかなりの量のメタデータが格納されることになる。したがって、いくつかの実現例は、各オブジェクトについて格納されるグローバルメタデータの量を低減する階層チャンキングスキーム（hierarchical chunking scheme）を使用する。階層的な実現例において、「チャンク」という用語は、グローバルレベルに格納される対応するメタデータを有するトップレベルの分割を識別するために使用される。これらの実現例では、「ブロック」という用語は、実際の格納の基本単位を識別するために使用される（たとえば２メガバイトまたは８メガバイト）。ブロックは、各シャードについてローカルで管理される。非階層的システムにおいて、格納の基本単位は、グローバルメタデータが格納される基本単位であるので、「チャンク」という単一の用語は、両方のコンセプトを識別するために使用され得る。 In some schemes, chunks are the basic unit of storage, and the location of each chunk is stored in global metadata. For very large objects, this scheme will store a significant amount of metadata at the global level for a single object. Thus, some implementations use a hierarchical chunking scheme that reduces the amount of global metadata stored for each object. In a hierarchical implementation, the term “chunk” is used to identify a top level partition with corresponding metadata stored at the global level. In these implementations, the term “block” is used to identify the basic unit of actual storage (eg, 2 megabytes or 8 megabytes). Blocks are managed locally for each shard. In a non-hierarchical system, the basic unit of storage is the basic unit in which global metadata is stored, so the single term “chunk” can be used to identify both concepts.
階層チャンキングは、複数の態様で実現され得る。いくつかの実現例では、１つのブロックのみが存在する場合でも、各チャンクはブロックのリストを含む。これらの実現例では、チャンクに対応するデータのルックアップについての付加的な階層レベルが常に存在する。他の実現例は、大きなチャンクについて必要とされる場合にのみ階層が存在するように、ハイブリッドスキームを使用する。そのようなハイブリッド実現例では、小さなオブジェクトは、単一のブロックに対応する単一のチャンクを含み得る。他方、より大きなオブジェクトについては、各チャンクはブロックのリストである。 Hierarchical chunking can be implemented in multiple ways. In some implementations, each chunk contains a list of blocks, even if only one block exists. In these implementations, there are always additional hierarchical levels for data lookups corresponding to chunks. Other implementations use a hybrid scheme so that the hierarchy exists only when needed for large chunks. In such a hybrid implementation, a small object may contain a single chunk that corresponds to a single block. On the other hand, for larger objects, each chunk is a list of blocks.
開示される階層スキームは、グローバルメタデータの量を低減し、これにより、オブジェクトを管理するコストまたは１つのストレージクラスタから別のストレージクラスタにオブジェクトを移動するコストを低減する。オブジェクトチャンクはグローバルレベルにて管理される一方、チャンク内のブロックは、オブジェクトメタデータが典型的に１つのシャード当たり１つのチャンク参照のみを含むように、ローカルシャードレベルにて管理される。 The disclosed hierarchy scheme reduces the amount of global metadata, thereby reducing the cost of managing objects or moving objects from one storage cluster to another storage cluster. While object chunks are managed at the global level, blocks within a chunk are managed at the local shard level so that object metadata typically includes only one chunk reference per shard.
いくつかの実現例では、アップロードプロセスは、（１）アップロードのために利用可能なシャードを発見するステップと、（２）シャードが利用不可能（たとえばいっぱい）になるか、または、データがもはや存在しなくなるまで現在のシャードにデータを書き込むステップと、（３）チャンクの順序付けられたリストに現在のチャンク参照を追加するステップと、（４）オブジェクトアップロードが終わると、オブジェクトを終了させるステップと、そうでなければ、（５）オブジェクトの残りについてステップ（１）の開始を繰り返すステップとに従う。 In some implementations, the upload process includes (1) finding an available shard for upload, and (2) the shard becomes unavailable (eg full) or the data no longer exists Writing data to the current shard until it stops, (3) adding the current chunk reference to the ordered list of chunks, (4) terminating the object when the object upload is finished, and so on Otherwise, (5) repeat the start of step (1) for the rest of the object.
いくつかの実現例において、ストレージからオブジェクトを読み出すことは、（１）所望のオブジェクトについて、チャンク参照（少なくとも１つは常に存在する）のセットを見つけるステップと、（２）チャンク参照に基づいてシャードの位置を見つけるステップと、（３）チャンク識別子およびローカルシャードメタデータを使用して、シャード位置からデータを読み出すステップと、（４）各チャンク参照についてステップ２および３を繰り返すステップとに従う。
In some implementations, retrieving an object from storage includes (1) finding a set of chunk references (at least one is always present) for the desired object, and (2) a shard based on the chunk reference. , (3) reading data from the shard location using the chunk identifier and local shard metadata, and (4) repeating
たとえば、オブジェクトアップロードがシャード１にデータを書き込むことにより開始され、シャード１がいっぱいになるとシャード２にスイッチングするとする。（２つのシャード１およびシャード２は、同じまたは異なるインスタンスに存在し得る。）（グローバルである）オブジェクトメタデータは、２つのチャンク参照からなる一方、各シャードは各チャンクについてのブロックのローカルリストを管理する。たとえば、各シャードは、オブジェクトについて複数のブロックを格納し得る。この場合、ストレージは完全に階層的である。すなわち、オブジェクトはチャンクへ分割され、各チャンクはブロックへ分割される。他の実現例では、チャンクのうちの１つは、複数のブロックに分割され得る（そのようなチャンクは時に「スーパーチャンク」と称される）一方、別のチャンクは単一のブロックからなり得る。後者の場合において、チャンク識別子はブロック識別子であり得る。
For example, suppose that object upload is started by writing data to
シャード１およびシャード２は互いから独立しているので、それらのレプリカは異なるインスタンスで格納され得る。たとえば、シャード１はインスタンス１およびインスタンス２に格納され得、シャード２はインスタンス１およびインスタンス３に格納され得る。
Since
この開示された方法は実質的に、アップロードサービスの可用性およびストレージ効率の両方を向上させる。この方法は、（たとえばインスタンスが大きいオブジェクトのアップロードの間にダウンした際の）再開可能なアップロードと、（たとえばシャードがいっぱいになった場合での）アップロードの最中における新しいシャードへのスイッチングとをサポートする。さらに、この方法は、複数のシャードに同時に書き込むことをサポートし、これにより、非常に大きいオブジェクトについて性能を有意に向上させ得る。いくつかの実現例では、単一のオブジェクトについてのデータが、異なるインスタンスにおいて同時に２つ以上の異なるシャードに書き込まれ、同時に同じインスタンスにおいて２つ以上のシャードに書き込まれ、また、単一のジャーナル内でも、２つ以上のプロセススレッドが、単一のジャーナルに異なるデータブロックを同時に書き込み得る。もちろん、分散アップロードは、利用可能なリソースによって制限される。分散ストレージシステムは、オブジェクトを同時にアップロードする多くの異なるクライアントを有するので、１つのクライアントからの単一の非常に大きいオブジェクトが、利用可能なリソースを過度に消費することは許可されない。 This disclosed method substantially improves both the availability and storage efficiency of the upload service. This method allows resumable uploads (for example, when an instance goes down during an upload of a large object) and switching to a new shard (for example, when the shard is full) during the upload. to support. In addition, this method supports writing to multiple shards simultaneously, which can significantly improve performance for very large objects. In some implementations, data for a single object is simultaneously written to two or more different shards in different instances, simultaneously written to two or more shards in the same instance, and within a single journal However, two or more process threads can write different data blocks to a single journal simultaneously. Of course, distributed upload is limited by available resources. Since a distributed storage system has many different clients that upload objects simultaneously, a single very large object from one client is not allowed to consume excessive resources.
いくつかの実現例に従うと、分散ストレージシステムにおけるオブジェクトレプリカの配置を管理するための方法は、分散ストレージシステムの第１のインスタンスにおいて実行される。第１のインスタンスは１つ以上のサーバを有しており、当該１つ以上のサーバの各々は１つ以上のプロセッサおよびメモリを有する。メモリは、１つ以上のプロセッサによる実行のための１つ以上のプログラムを格納する。第１のインスタンスは、第１の配置ポリシーに関連付けられる第１のオブジェクトを受け取る。第１の配置ポリシーは、第１のオブジェクトのレプリカが分散ストレージシステムにおいてどこに格納されるかについて基準を特定する。いくつかの実現例では、各配置ポリシーは、対象数のオブジェクトレプリカと、それらのレプリカについての対象位置とを特定する。第１のインスタンスは、オブジェクトを複数のオブジェクトチャンクに分割し、複数のオブジェクトチャンクの第１のオブジェクトチャンクを複数のブロックへ分割する。第１のインスタンスは、関連付けられる配置ポリシーが第１の配置ポリシーとマッチする第１のジャーナルに複数のブロックを格納する。第１のインスタンスは、第１のオブジェクトについてグローバルメタデータを格納し、当該グローバルメタデータは、複数のオブジェクトチャンクのリストを含む。リストは、オブジェクトチャンクの各々についてのそれぞれの識別子を含む。第１のインスタンスは、第１のオブジェクトチャンクについてローカルメタデータを格納し、当該ローカルメタデータは、複数のブロックの各ブロックを識別するブロックリストを含む。ローカルメタデータは第１のジャーナルに関連付けられる。第１のジャーナルはその後、第１の配置ポリシーに従って、分散ストレージシステムの第２のインスタンスにレプリケートされる。グローバルメタデータはレプリケーションを反映するために更新され、ローカルメタデータはレプリケーションによって変更されない。 According to some implementations, a method for managing placement of object replicas in a distributed storage system is performed in a first instance of the distributed storage system. The first instance has one or more servers, each of the one or more servers having one or more processors and memory. The memory stores one or more programs for execution by one or more processors. The first instance receives a first object associated with a first placement policy. The first placement policy specifies a criterion for where a replica of the first object is stored in the distributed storage system. In some implementations, each placement policy identifies a target number of object replicas and target positions for those replicas. The first instance divides the object into a plurality of object chunks and divides the first object chunk of the plurality of object chunks into a plurality of blocks. The first instance stores a plurality of blocks in a first journal whose associated placement policy matches the first placement policy. The first instance stores global metadata for the first object, and the global metadata includes a list of a plurality of object chunks. The list includes a respective identifier for each of the object chunks. The first instance stores local metadata for the first object chunk, and the local metadata includes a block list that identifies each block of the plurality of blocks. Local metadata is associated with the first journal. The first journal is then replicated to the second instance of the distributed storage system according to the first placement policy. Global metadata is updated to reflect replication, and local metadata is not changed by replication.
いくつかの実現例に従うと、分散ストレージシステムにおけるオブジェクトレプリカの配置を管理するための方法は、分散ストレージシステムの第１のインスタンスにおいて実行される。第１のインスタンスは、各々が１つ以上のプロセッサおよびメモリを有する１つ以上のサーバを有する。メモリは、１つ以上のプロセッサによる実行のための１つ以上のプログラムを格納する。１つ以上のジャーナルがオブジェクトチャンクの格納のためにオープンにされる。各ジャーナルは、単一の配置ポリシーに関連付けられる。いくつかの実現例では、各配置ポリシーは、対象数のオブジェクトレプリカと、それらのレプリカについての対象位置とを特定する。第１のインスタンスは、少なくとも第１のオブジェクトチャンクを含む第１のオブジェクトを受け取る。第１のオブジェクトは第１の配置ポリシーに関連付けられる。第１のオブジェクトチャンクは第１の複数のブロックを含む。第１のインスタンスは、関連付けられる配置ポリシーが第１の配置ポリシーとマッチする第１のジャーナルに第１の複数のブロックを格納する。第１のジャーナルは、配置ポリシーが第１の配置ポリシーとマッチするオブジェクトについてのブロックのみを格納する。第１のインスタンスは、第１のオブジェクトについてグローバルメタデータを格納し、グローバルメタデータは、第１のオブジェクトに対応するオブジェクトチャンクの第１のリストを含む。第１のリストは、第１のオブジェクトチャンクの識別子を含む。第１のインスタンスはさらに、第１のオブジェクトチャンクについてローカルメタデータを格納し、ローカルメタデータは、第１の複数のブロックの各ブロックを識別するブロックリストを含む。ローカルメタデータは第１のジャーナルに関連付けられる。第１のジャーナルの場合、関連付けられる配置ポリシーが第１の配置ポリシーとマッチする第１の複数のオブジェクトについて受け取るステップおよび格納するステップが、第１の終了条件が発生するまで繰り返される。いくつかの実現例では、時間の予め規定されたスパンの後、または、第１のジャーナルが予め規定されたサイズしきい値を越えた後に、第１の終了条件が発生する。第１の終了条件が発生した後、第１のジャーナルはクローズされ、これにより、如何なる付加的なブロックも第１のジャーナルに格納されるのを防止する。その後、第１のジャーナルは、第１の配置ポリシーに従って、分散ストレージシステムの第２のインスタンスにレプリケートされる。グローバルメタデータはレプリケーションを反映するために更新され、ローカルメタデータはレプリケーションによって変更されない。 According to some implementations, a method for managing placement of object replicas in a distributed storage system is performed in a first instance of the distributed storage system. The first instance has one or more servers, each having one or more processors and memory. The memory stores one or more programs for execution by one or more processors. One or more journals are opened for storing object chunks. Each journal is associated with a single placement policy. In some implementations, each placement policy identifies a target number of object replicas and target positions for those replicas. The first instance receives a first object that includes at least a first object chunk. The first object is associated with the first placement policy. The first object chunk includes a first plurality of blocks. The first instance stores the first plurality of blocks in a first journal whose associated placement policy matches the first placement policy. The first journal stores only blocks for objects whose placement policy matches the first placement policy. The first instance stores global metadata for the first object, and the global metadata includes a first list of object chunks corresponding to the first object. The first list includes the identifier of the first object chunk. The first instance further stores local metadata for the first object chunk, and the local metadata includes a block list that identifies each block of the first plurality of blocks. Local metadata is associated with the first journal. For the first journal, receiving and storing for the first plurality of objects whose associated placement policy matches the first placement policy is repeated until a first termination condition occurs. In some implementations, the first termination condition occurs after a predefined span of time or after the first journal exceeds a predefined size threshold. After the first termination condition occurs, the first journal is closed, thereby preventing any additional blocks from being stored in the first journal. The first journal is then replicated to the second instance of the distributed storage system according to the first placement policy. Global metadata is updated to reflect replication, and local metadata is not changed by replication.
図面を通じて、同様の参照番号は対応する部分を指す。
実現例の説明
分散ストレージシステムにおいてオブジェクトの配置を管理するための技術について論じる前に、これらの技術が使用され得る例示的なシステムを提示することは有益である。
Like reference numerals refer to corresponding parts throughout the drawings.
Implementation Description Before discussing techniques for managing the placement of objects in a distributed storage system, it is beneficial to present an exemplary system in which these techniques can be used.
分散ストレージシステムの概略
図１に示されるように、開示された実現例は分散ストレージシステムを記載する。地球１００上のさまざまな位置に、複数のインスタンス１０２−１、１０２−２、…、１０２−Ｎが存在しており、ネットワーク通信リンク１０４−１、１０４−２、…１０４−Ｍによって接続されている。なお、この明細書において、「インスタンス」は「ストレージ位置」とも称される。また、１つ以上のインスタンス（ストレージ位置）が特定の物理的位置（たとえばビルディング、互いから所定の距離内のビルディングのセットなど）に位置してもよい。いくつかの実現例において、インスタンス（たとえばインスタンス１０２−１）は、データセンタに対応する。いくつかの実現例において、複数のインスタンスは、同じデータセンタに物理的に位置する。単一の実現例は、異なる地理的位置にある個々のインスタンスと、インスタンスの１つ以上のクラスタとの両方を有し得、各クラスタは、複数のインスタンスを含み、各クラスタ内のインスタンスは単一の地理的位置に存在する。
Schematic of Distributed Storage System As shown in FIG. 1, the disclosed implementation describes a distributed storage system. There are multiple instances 102-1, 102-2,..., 102-N at various locations on the Earth 100, connected by network communication links 104-1, 104-2,. Yes. In this specification, “instance” is also referred to as “storage location”. Also, one or more instances (storage locations) may be located at specific physical locations (eg, buildings, sets of buildings within a predetermined distance from each other). In some implementations, an instance (eg, instance 102-1) corresponds to a data center. In some implementations, multiple instances are physically located in the same data center. A single implementation may have both individual instances at different geographic locations and one or more clusters of instances, each cluster containing multiple instances, and instances within each cluster are simply In one geographic location.
図１の概念図は、特定の数のネットワーク通信リンク１０４−１などを示すが、典型的な実現例は、それより多いまたは少ないネットワーク通信リンクを有してもよい。いくつかの実現例において、インスタンスの同じ対同士の間に２つ以上のネットワーク通信リンクが存在する。たとえば、ネットワーク通信リンク１０４−５および１０４−６は、インスタンス１０２−２とインスタンス１０２−６との間のネットワーク接続を提供する。いくつかの実現例において、ネットワーク通信リンクは光ファイバーケーブルを含む。いくつかの実現例において、ネットワーク通信リンクのうちのいくつかは、マイクロ波のような無線技術を使用する。いくつかの実現例において、各ネットワーク通信リンクは、特定の帯域幅、および／または、その帯域幅の使用についての特定のコストを有する。いくつかの実現例において、ネットワーク通信リンクのうち１つ以上にわたるデータ転送に関して、スループットレート、可用性の時間、リンクの信頼性などを含む統計が維持される。典型的に、各インスタンスは、データストアおよび関連するデータベースを有しており、タスクのすべてを実行するためにサーバコンピュータ（図４に示されるような「インスタンスサーバ」）のファーム（farm）を利用する。いくつかの実現例において、分散ストレージシステムの１つ以上のインスタンスは限定された機能を有する。たとえば、限定された機能は、他のインスタンス同士の間のデータ送信のためのリピータ（repeater）として動作することを含み得る。なお、限定された機能のインスタンスは、データストアのうちのいずれかを含んでもよいし、含まなくてもよい。 Although the conceptual diagram of FIG. 1 shows a particular number of network communication links 104-1, etc., a typical implementation may have more or fewer network communication links. In some implementations, there are two or more network communication links between the same pair of instances. For example, network communication links 104-5 and 104-6 provide a network connection between instance 102-2 and instance 102-6. In some implementations, the network communication link includes a fiber optic cable. In some implementations, some of the network communication links use wireless technologies such as microwaves. In some implementations, each network communication link has a specific bandwidth and / or a specific cost for using that bandwidth. In some implementations, statistics are maintained regarding data transfer over one or more of the network communication links, including throughput rate, availability time, link reliability, and the like. Typically, each instance has a data store and an associated database and utilizes a farm on a server computer (an “instance server” as shown in FIG. 4) to perform all of the tasks. To do. In some implementations, one or more instances of the distributed storage system have limited functionality. For example, the limited functionality may include operating as a repeater for data transmission between other instances. Note that the instance of the limited function may or may not include any of the data stores.
図２は、いくつかの実現例に従った、分散ストレージシステム２００の要素を示すブロック図である。分散ストレージシステム２００は、インスタンス１０２−１、１０２−２、１０２−３、１０２−４、…、１０２−Ｎを含む。それぞれのインスタンス１０２−１は、インスタンス同士間でオブジェクトチャンク２３８をレプリケートするレプリケーションモジュール２２０を含む。いくつかの実現例において、オブジェクトチャンク２３８は、それぞれのインスタンス１０２−１のデータストア２２４に格納される。図６に示されるように、各オブジェクトチャンク２３８は、オブジェクト２２６、または、オブジェクト２２６の部分を含む。データストア２２４は、オブジェクトを格納することが可能である分散データベース、ファイルシステム、テープバックアップ、および、任意の他のタイプのストレージシステムまたはデバイスを含み得る。いくつかの実現例において、オブジェクト２２６またはジャーナル２３０をレプリケートするために、レプリケーションモジュール２２０は、１つ以上のレプリケーションキュー２２２−１、２２２−２、…、２２２−Ｌを使用する。レプリケートされるべきオブジェクトまたはジャーナルについてのレプリケーション要求がレプリケーションキュー２２２に配置され、リソース（たとえば帯域幅）が利用可能な場合、オブジェクトまたはジャーナルがレプリケートされる。いくつかの実現例において、レプリケーションキュー２２２におけるレプリケーション要求は、割り当てられたプライオリティを有しており、帯域幅が利用可能になると、最も高いプライオリティのレプリケーション要求がレプリケートされる。
FIG. 2 is a block diagram illustrating elements of a distributed storage system 200 in accordance with some implementations. The distributed storage system 200 includes instances 102-1, 102-2, 102-3, 102-4, ..., 102-N. Each instance 102-1 includes a
いくつかの実現例において、バックグラウンドレプリケーションプロセスは、配置ポリシー２１２、ならびに、統計サーバ２０８によって提供されるアクセスデータ２１０および／またはグローバル状態２１１に基づき、オブジェクトまたはジャーナルのコピーを作成および削除する。配置ポリシー２１２は、オブジェクトのどれだけ多くのコピーが所望であるか、どこに当該コピーが存在するべきであるか、および、どのタイプのデータストアに当該データが保存されるべきであるかを特定している。統計サーバ２０８によって提供されるアクセスデータ２１０（たとえばオブジェクトのレプリカがアクセスされたストレージ位置、オブジェクトのレプリカがストレージ位置にてアクセスされた時間、ストレージ位置でのオブジェクトのアクセスの頻度などに関するデータ）および／またはグローバル状態２１１とともに配置ポリシー２１２を使用して、位置割当デーモン（ＬＡＤ： location assignment daemon）２０６は、オブジェクトまたはジャーナルの新しいコピーをどこに作成するべきかと、どのコピーが削除され得るかとを決定する。新しいコピーが作成されるべきである場合、レプリケーション要求はレプリケーションキュー２２２に挿入される。いくつかの実現例において、ＬＡＤ２０６は、分散ストレージシステム２００についてグローバルにオブジェクトまたはジャーナルのレプリカを管理する。言いかえれば、分散ストレージシステム２００において１つのＬＡＤ２０６のみが存在する。配置ポリシー２１２の使用およびＬＡＤ２０６のオペレーションは以下により詳細に記載される。
In some implementations, the background replication process creates and deletes copies of objects or journals based on
なお、一般に、それぞれの配置ポリシー２１２は、保存するべきオブジェクトのレプリカの数、どのタイプのデータストアにレプリカが保存されるべきであるか、コピーが保存されるべきストレージ位置などを特定し得る。いくつかの実現例では、オブジェクトについてのそれぞれの配置ポリシー２１２は、分散ストレージシステムに存在しなければならないオブジェクトの最小数のレプリカと、分散ストレージシステムに存在することが許されるオブジェクトの最大数のレプリカと、オブジェクトのレプリカが格納されるべきであるストレージデバイスタイプと、オブジェクトのレプリカが格納され得る位置と、オブジェクトのレプリカが格納され得ない位置と、オブジェクトについての配置ポリシーが適用されるオブジェクトについてのある範囲の期間とからなる群から選択される基準を含む。たとえば、第１の配置ポリシーは、ウェブメールアプリケーションにおける各オブジェクトが、最低２つのレプリカおよび最大５つのレプリカを有さなければならないということを特定し得、当該オブジェクトのレプリカは中国の外のデータセンタに格納され得、各オブジェクトの少なくとも１つのレプリカがテープ上に格納されなければならない。ウェブメールアプリケーションについての第２の配置ポリシーはさらに、３０日より古いオブジェクトについて、最低１つのレプリカおよび最大３つのレプリカが分散ストレージシステム２００に格納されるということを特定し得、オブジェクトのレプリカは中国の外のデータセンタに格納され得、各オブジェクトの少なくとも１つのレプリカがテープ上に格納されなければならない。
In general, each
いくつかの実現例において、ユーザ２４０は、ウェブブラウザ２４４を実行可能であるコンピュータシステムまたは他のデバイスであり得るユーザシステム２４２とインタラクションする。ユーザアプリケーション２４６は、ウェブブラウザにおいて実行されており、ネットワークを使用して分散ストレージシステム２００に格納されたデータへのアクセスするよう、データベースクライアント２４８によって提供される機能を使用する。ネットワークは、インターネット、ローカルエリアネットワーク（ＬＡＮ）、ワイドエリアネットワーク（ＷＡＮ）、無線ネットワーク（ＷｉＦｉ）、ローカルイントラネット、または、これらの任意の組合せであり得る。いくつかの実現例では、データベースクライアント２４８は、要求に応答するために適切なインスタンスを識別するよう、グローバル構成ストア２０４における情報を使用する。いくつかの実現例において、ユーザアプリケーション２４６は、ウェブブラウザ２４４なしでユーザシステム２４２上で実行される。例示的なユーザアプリケーションは、電子メールアプリケーションおよびオンラインビデオアプリケーションを含む。
In some implementations,
いくつかの実現例において、各インスタンスは、分散ストレージシステムに格納されるオブジェクトの各々についてオブジェクトメタデータ２２８を格納する。いくつかのインスタンスは、インスタンス（「ローカルインスタンス」と称される）に格納されるレプリカを有するオブジェクトについてのみオブジェクトメタデータ２２８を格納する。いくつかのインスタンスは、分散ストレージシステムにおいて任意の場所に格納されたすべてのオブジェクトのついてのオブジェクトメタデータ２２８を格納する（「グローバルインスタンス」と称される）。オブジェクトメタデータ２２８は、図３、図４および図５に関してより詳細に記載される。
In some implementations, each instance stores
いくつかの実現例では、各インスタンスは、分散ストレージシステム２００に格納されるジャーナルの各々について、ジャーナルメタデータ２３６を格納する。いくつかのインスタンスは、インスタンスに格納されるレプリカを有するジャーナルについてのみ、ジャーナルメタデータ２３６を格納する。いくつかのインスタンスは、分散ストレージシステムにおけるいずれかの場所に格納されるすべてのジャーナルについてジャーナルメタデータを格納する。ジャーナルメタデータは、図３、図４、図５および図８に関して以下により詳細に記載される。
In some implementations, each instance stores
複数のタイプのジャーナルがデータストア２２４に格納される。ジャーナルの大多数は、クローズド（closed）ジャーナル２３０である。クローズドジャーナル２３０は、いずれの付加的なオブジェクトチャンクも格納しないが、コンテンツを削除およびコンパクト化（compacted）し得る。いくつかの実現例では、同じ配置ポリシー２１２についての２つ以上の小さなクローズドジャーナル２３０は、単一の置換クローズドジャーナル２３０を形成するためにともに結合（stitched）され得る。クローズドジャーナル２３０内のデータは削除およびコンパクト化され得るので、クローズドジャーナル２３０は、時間とともに小さくなり、これにより、結合の候補になり得る。
Multiple types of journals are stored in the
クローズドジャーナル２３０に加えて、インスタンス１０２はオープンジャーナル２３２および２３４を有し得る。図２に示されるように、オープンジャーナルは、プライマリジャーナル２３２またはセカンダリジャーナル２３４のいずれかとして指定される。プライマリジャーナル２３２およびセカンダリジャーナル２３４はペアとなり、異なるインスタンスに位置する。以下により詳細に記載されるように、プライマリジャーナル２３２は、格納のためのチャンク２３８を受け取り、対応するセカンダリジャーナル２３４が格納されているインスタンスにチャンク２３８のコピーを送信する。
In addition to the
図３は、いくつかの実現例に従ったサーバ３００のブロック図である。サーバ３００は典型的に、１つ以上の処理ユニット（ＣＰＵ）３０２と、現在の日付および／または時間を報告するクロック３０３と、１つ以上のネットワークまたは他の通信インターフェイス３０４と、メモリ３１４と、これらのコンポーネントを相互接続するための１つ以上の通信バス３１２とを含む。通信バス３１２は、システムコンポーネントを相互接続しシステムコンポーネント同士間の通信を制御する回路（チップセットとも時に称される）を含み得る。いくつかの実現例では、クロック３０３は、クロックサーバ（たとえばクォーラムクロックサーバまたはネットワーク上の任意の他のクロックサーバなど）と周期的に同期するローカルクロックである。サーバ３００は、表示デバイス３０８および入力デバイス３１０（たとえばキーボード、マウス、タッチスクリーン、キーパッドなど）を含むユーザインターフェイス３０６を随意に含み得る。メモリ３１４は、ＤＲＡＭ、ＳＲＡＭ、ＤＤＲ ＲＡＭまたは他のランダムアクセスソリッドステートメモリデバイスのような高速ランダムアクセスメモリを含み、また、１つ以上の磁気ディスクストレージデバイス、光学ディスクストレージデバイス、フラッシュメモリデバイス、または、他の不揮発性ソリッドステートストレージデバイスといった不揮発性メモリを含んでもよい。メモリ３１４は、ＣＰＵ３０２からリモートに位置する１つ以上のストレージデバイスを随意に含み得る。メモリ３１４、または、代替的にはメモリ３１４内の不揮発性メモリデバイスは、コンピュータ読取可能ストレージ媒体を含む。いくつかの実現例において、メモリ３１４は、以下のプログラム、モジュールおよびデータ構造またはそのサブセットを格納する。すなわち、
・さまざまな基本的なシステムサービスを処理するとともにハードウェア依存タスクを実行するためのプロシージャを含むオペレーティングシステム３１６、
・インターネット、他のワイドエリアネットワーク、ローカルエリアネットワーク、メトロポリタンエリアネットワークなどのような、１つ以上の通信インターフェイス３０４（有線または無線）および１つ以上の通信ネットワークを介して他のコンピュータにサーバ３００を接続するために使用される通信モジュール３１８、
・入力デバイス３１０を介してユーザからコマンドを受け取り、かつ、表示デバイス３０８においてユーザインターフェイスオブジェクトを生成する随意のユーザインターフェイスモジュール３２０、
・本願明細書に記載されるような構成２０４、
・本願明細書に記載されるようなＬＡＤ２０６、
・本願明細書に記載されるようなアクセスデータ２１０、
・本願明細書に記載されるようなグローバル状態２１１、
・本願明細書に記載されるような配置ポリシー２１２、
・分散ストレージシステムに格納されるオブジェクトについてのオブジェクトメタデータ２２８（オブジェクトメタデータ２２８は、分散ストレージシステム内のオブジェクトを一意に識別するオブジェクトＩＤ３３０を含み得る。メタデータ２２８は、人またはエンティティの名称および／または識別子（たとえば電子メールアドレス）であり得るオブジェクトのオーサ（author）３３２を含み得る。いくつかの実現例では識別子は一意である。メタデータは、オブジェクトが作成された（たとえば、分散ストレージシステムにアップロードされた）日付スタンプまたはタイムスタンプ３３４を含み得る。メタデータは、バイトまたはアロケーションブロックで典型的に測定されるオブジェクトのサイズ３３６を含み得る。メタデータは、個々に割り当てられ得るか、または、他の基準に基づき割り当てられ得る割当配置ポリシー３３８を含む（たとえば米国からアップロードされるすべてビデオは、同じ割当配置ポリシー３３８を有し得る）。配置ポリシーの使用は、図５〜図６および図９Ａ〜図９Ｃに関して以下により詳細に記載される。メタデータ２２８は、各オブジェクトについてのコンテンツチャンクを識別するチャンクＩＤ３４６のセットを含む。いくつかの実現例において、チャンクＩＤはオブジェクト内のオフセットとして特定される。たとえば、第１のチャンクは０のオフセットを有する。いくつかの実現例において、オフセットはメガバイトで特定される。いくつかの実現例において、チャンクＩＤは（ＧＵＩＤのような）一意識別子である。いくつかの実現例において、各チャンクＩＤは、チャンクのオフセットにオブジェクトＩＤを連結することにより形成される。いくつかの実現例では、チャンクＩＤは、コンテンツハッシュまたはコンテンツダイジェストを使用して形成される。各チャンクＩＤに対応するのは、割当ジャーナルＩＤ３４８であり、割当ジャーナルＩＤ３４８は、対応するチャンクがどのジャーナルに格納されるか示す。）、ならびに、
・分散ストレージシステム２００に格納される各ジャーナルについてのジャーナルメタデータ２３６（ジャーナルメタデータ２３６は、各ジャーナルについてのジャーナルＩＤ３７０と、ジャーナルが格納されるジャーナル位置３７２のセットとを含む。ジャーナル位置３７２は、ジャーナルが格納される各インスタンス１０２を特定し、ジャーナルを格納するインスタンス１０２にあるデータストア２２４を特定し得る。ジャーナルメタデータ２３６はさらに、各ジャーナルに関連付けられる配置ポリシーＩＤ３７４を含む。配置ポリシーＩＤ３７４は、ジャーナルに関連付けられる一意の配置ポリシー２１２を識別する。）
を格納する。
FIG. 3 is a block diagram of a server 300 according to some implementations. Server 300 typically includes one or more processing units (CPUs) 302, a
An
The server 300 to other computers via one or more communication interfaces 304 (wired or wireless) and one or more communication networks, such as the Internet, other wide area networks, local area networks, metropolitan area networks, etc. A
An optional
A
-LAD206 as described herein,
A
A
Is stored.
上記の識別された要素の各々は、以前に言及されたメモリデバイスのうちの１つ以上に格納され得、上に記載された機能を実行するための命令のセットに対応する。命令のセットは、１つ以上のプロセッサ（たとえばＣＰＵ３０２）によって実行され得る。上記の識別されたモジュールまたはプログラム（すなわち命令のセット）は、別個のソフトウェアプログラム、プロシージャまたはモジュールとして実現される必要はなく、したがって、これらのモジュールのさまざまなサブセットは、さまざまな実現例において組み合わせられ得るか、または、そうでなければ再構成され得る。いくつかの実現例において、メモリ３１４は、上で識別されたモジュールおよびデータ構造のサブセットを格納し得る。さらに、メモリ３１４は、上で記載されていない付加的なモジュールおよびデータ構造を格納し得る。 Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing the functions described above. The set of instructions may be executed by one or more processors (eg, CPU 302). The identified modules or programs (ie, the set of instructions) need not be implemented as separate software programs, procedures or modules, and thus various subsets of these modules can be combined in various implementations. Can be obtained or otherwise reconstituted. In some implementations, the memory 314 may store a subset of the modules and data structures identified above. In addition, memory 314 may store additional modules and data structures not described above.
図３は「サーバ」を示すが、図３は、本願明細書において記載される実現例の構造図としてよりも、サーバ３００のセットに存在し得るさまざまな特徴の機能説明としてより多く意図される。実際には、および、当業者によって認識されるように、別々に示された項目は、組み合わされ得、いくつかの項目は分離され得る。たとえば、図３に別々に示されるいくつかの項目は単一のサーバ上で実現され得、また、単一の項目は、１つ以上のサーバによって実現され得る。サーバの実際の数、および、特徴がどのようにそれらの間で割り当てられるかは、実現例ごとに変動することになり、ピーク使用期間の間および平均使用期間の間にシステムが処理しなければならないデータトラフィックの量に部分的に依存し得る。いくつかの実現例では、ＬＡＤ２０６のサブセット、アクセスデータ２１０、グローバル状態２１１および配置ポリシー２１２は、別個のサーバ上に位置する。たとえば、ＬＡＤ２０６はサーバ（またはサーバのセット）に位置し得、アクセスデータ２１０およびグローバル状態２１１は、統計サーバ２０８（または統計サーバ２０８のセット）に位置され得るとともに統計サーバ２０８（または統計サーバ２０８のセット）によって維持され得、配置ポリシー２１２は別のサーバ（または別のサーバのセット）に位置し得る。
Although FIG. 3 shows a “server”, FIG. 3 is more intended as a functional description of various features that may be present in a set of servers 300 than as a structural diagram of an implementation described herein. . In practice, and as will be appreciated by those skilled in the art, items shown separately may be combined and some items may be separated. For example, some items shown separately in FIG. 3 may be implemented on a single server, and a single item may be implemented by one or more servers. The actual number of servers and how features are allocated between them will vary from implementation to implementation and must be handled by the system during peak usage periods and average usage periods. It may depend in part on the amount of data traffic that must be. In some implementations, the subset of LAD 206,
図４は、いくつかの実現例に従った、インスタンス１０２についてのインスタンスサーバ４００のブロック図である。インスタンスサーバ４００は典型的に、モジュールを実行するための１つ以上の処理ユニット（ＣＰＵ）４０２と、現在の日付および／または時間を報告するクロック４０３と、メモリ４１４に格納され、これにより処理オペレーションを実行するプログラムおよび／または命令と、１つ以上のネットワークまたは他の通信インターフェイス４０４と、メモリ４１４と、これらのコンポーネントを相互接続するための１つ以上の通信バス４１２とを含む。いくつかの実現例において、クロック４０３は、周期的にクロックサーバと同期するローカルクロックである（たとえばクォーラムクロックサーバまたはネットワーク上の任意の他のクロックサーバなど）。いくつかの実現例において、インスタンスサーバ４００は、表示デバイス４０８および１つ以上の入力デバイス４１０を含むユーザインターフェイス４０６を含む。いくつかの実現例では、メモリ４１４は、ＤＲＡＭ、ＳＲＡＭ、ＤＤＲ ＲＡＭまたは他のランダムアクセスソリッドステートメモリデバイスのような高速ランダムアクセスメモリを含む。いくつかの実現例では、メモリ４１４は、１つ以上の磁気ディスクストレージデバイス、光学ディスクストレージデバイス、フラッシュメモリデバイス、または、他の不揮発性ソリッドステートストレージデバイスといった不揮発性メモリを含み、いくつかの実現例において、メモリ４１４は、ＣＰＵ４０２からリモートに位置する１つ以上のストレージデバイスを含む。メモリ４１４、または、代替的にはメモリ４１４内の不揮発性メモリデバイスは、コンピュータ読取可能ストレージ媒体を含む。いくつかの実現例では、メモリ４１４またはメモリ４１４のコンピュータ読取可能ストレージ媒体は、以下のプログラム、モジュールおよびデータ構造、または、そのサブセットを格納する。すなわち、
・さまざまな基本的なシステムサービスを処理するとともにハードウェア依存タスクを実行するためのプロシージャを含むオペレーティングシステム４１６、
・インターネット、他のワイドエリアネットワーク、ローカルエリアネットワーク、メトロポリタンエリアネットワークなどといった、１つ以上の通信ネットワークインターフェイス４０４（有線または無線）および１つ以上の通信ネットワークを介して他のインスタンスサーバまたはコンピュータにインスタンスサーバ４００を接続するために使用される通信モジュール４１８、
・入力デバイス４１０を介してユーザからコマンドを受け取り、かつ、表示デバイス４０８においてユーザインターフェイスオブジェクトを生成する随意のユーザインターフェイスモジュール４２０、
・本願明細書に記載されるようなレプリケーションモジュール２２０およびレプリケーションキュー２２２、
・図３に関して記載されような、ジャーナル２３０、２３２および２３４にオブジェクトチャンク２３８を格納するデータストア２２４（たとえば分散データベース、ファイルシステム、テープストレージ、ビッグテーブル（Big Tables）など）、
・サーバ３００に関して図３に記載されたような、オブジェクトメタデータ２２８および対応するメタデータ要素３３０〜３３８、３４６および３４８、ならびに、
・サーバ３００に関して図３に記載されるような、ジャーナルメタデータ２３６ならびに対応するジャーナルメタデータ要素３７０、３７２および３７４
を格納する。
FIG. 4 is a block diagram of an
An
Instances to other instance servers or computers via one or more communication network interfaces 404 (wired or wireless) and one or more communication networks, such as the Internet, other wide area networks, local area networks, metropolitan area networks, etc. A
An optional
A
A data store 224 (eg, distributed database, file system, tape storage, Big Tables, etc.) that stores object
Is stored.
上記の識別された要素の各々は、以前に言及されたメモリデバイスのうちの１つ以上に格納され得、上に記載された機能を実行するための命令のセットに対応する。命令のセットは、１つ以上のプロセッサ（たとえばＣＰＵ４０２）によって実行され得る。上記の識別されたモジュールまたはプログラム（すなわち命令のセット）は、別個のソフトウェアプログラム、プロシージャまたはモジュールとして実現される必要はなく、したがって、これらのモジュールのさまざまなサブセットは、さまざまな実現例において組み合わせられ得るか、または、そうでなければ再構成され得る。いくつかの実現例において、メモリ４１４は、上で識別されたモジュールおよびデータ構造のサブセットを格納し得る。さらに、メモリ４１４は、上で記載されていない付加的なモジュールおよびデータ構造を格納し得る。 Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing the functions described above. The set of instructions may be executed by one or more processors (eg, CPU 402). The identified modules or programs (ie, the set of instructions) need not be implemented as separate software programs, procedures or modules, and thus various subsets of these modules can be combined in various implementations. Can be obtained or otherwise reconstituted. In some implementations, the memory 414 may store a subset of the modules and data structures identified above. Further, memory 414 may store additional modules and data structures not described above.
図４は「インスタンスサーバ」を示すが、図４は、本願明細書において記載される実現例の構造図としてよりも、インスタンスサーバ４００のセットに存在し得るさまざまな特徴の機能説明としてより多く意図される。実際には、および、当業者によって認識されるように、別々に示された項目は組み合わされ得、いくつかの項目は分離され得る。たとえば、図４に別々に示されるいくつかの項目は単一のサーバ上で実現され得、また、単一の項目は、１つ以上のサーバによって実現され得る。サーバの実際の数、および、特徴がどのようにそれらの間で割り当てられるかは、実現例ごとに変動することになり、ピーク使用期間の間および平均使用期間の間にサーバが処理しなければならないデータトラフィックの量に部分的に依存し得る。たとえば、単一のインスタンス１０２では、１００個のインスタンスサーバ４００または何千個ものインスタンスサーバ４００が存在してもよい。
Although FIG. 4 shows an “instance server”, FIG. 4 is more intended as a functional description of various features that may be present in a set of
いくつかの実現例において、クライアントに対してより速い応答を提供し、かつ、フォルトトレランスを提供するために、インスタンスにおいて実行される各プログラムまたはプロセスは、複数のコンピュータの間で分散される。プログラムまたはプロセスの各々に割り当てられるインスタンスサーバ４００の数は変動し得るとともに、ワークロードに依存し得る。
In some implementations, each program or process executed on an instance is distributed among multiple computers to provide a faster response to the client and to provide fault tolerance. The number of
図５は、いくつかの実現例に従ったオブジェクトチャンクの格納のためのジャーナルの使用を示す。図５は、データストア２２４と、オブジェクトメタデータ２２８の部分と、ジャーナルメタデータ２３６の部分とを示しており、これらはすべて例示的なインスタンス１０２に存在する。このデータストア２２４には多くのジャーナル２３０、２３２および２３４が格納されているので、２次元グリッドにおいてそれらを視覚的に構成することは有用である。（もちろん、視覚表示は、データストアにおけるジャーナルの実際の物理的なストレージとは無関係である。）図において、ジャーナルはジャーナルの「ロウ（row）」にパーティショニングされ、各ロウは単一の配置ポリシー２１２に対応する。たとえば、第１のロウ５０２−Ｐ１は、配置ポリシーＰ１（２１２）に対応し、クローズドジャーナル２３０、オープンプライマリジャーナル２３２、および、オープンセカンダリジャーナル２３４を含む。第１のロウ５０２−Ｐ１におけるこれらのジャーナルのすべては、配置ポリシーＰ１に関連付けられる。第２のロウ５０２−Ｐ２は、配置ポリシーＰ２（２１２）に対応し、最後のロウ５０２−ＰＮは配置ポリシーＰＮ（２１２）に対応する。典型的に、配置ポリシーの数は１０個、２０個、５０個または恐らく１００個といったように少ない。配置ポリシーの数が増加すると、オブジェクトレプリカの管理はあまり効率的でなくなる。
FIG. 5 illustrates the use of a journal for storing object chunks according to some implementations. FIG. 5 shows a
さらに、データストア２２４におけるジャーナルは、図５において２つのカラムへ視覚的にパーティショニングされる。第１のカラムは、ジャーナルの大多数であるクローズドジャーナル２３０を識別する。第２のカラムは、オープンプライマリジャーナル２３２およびオープンセカンダリジャーナル２３４を含む。各ジャーナルにおいてさまざまな長方形２３８によって示されるように、各ジャーナル（クローズドジャーナル２３０、オープンプライマリジャーナル２３２またはオープンセカンダリジャーナル２３４）はオブジェクトチャンク２３８を含む。オブジェクトチャンクはさまざまなサイズであり得るが、実現例は典型的に固定最大サイズをセットする（たとえば２メガバイト、４メガバイトまたは８メガバイト）。ジャーナル内におけるオブジェクトチャンク２３８の図は、ジャーナルがさまざまなサイズの多くのオブジェクトチャンクを格納するが、そうでなければオブジェクトチャンクの実際の物理的な格納を示しているわけではないという事実を正確に伝える（たとえば、割り当てられていないスペースの初めに各新しいオブジェクトチャンク２３８が追加されるので、オブジェクトチャンク同士間には未使用のスペースは一般に存在しない）。
In addition, journals in
図５は、オープンジャーナル２３２および２３４のさまざまな組合せが各配置ポリシーについて可能であることを示す。本願明細書において図および説明において異なるジャーナルレプリカを識別するために、「２３２．Ｐ４．７」といった三部分ラベルが使用される場合がある。第１の部分（たとえば「２３２」）は、ジャーナルのタイプを識別し（２３０＝クローズド，２３２＝オープンプライマリ，２３４＝オープンセカンダリ）、第２の部分（たとえば「Ｐ４」）は、当該ジャーナルについての配置ポリシーを特定し、第３の部分（たとえば「７」）は、当該ジャーナルについてのシーケンシャル番号を単に特定する（たとえば「２３２．Ｐ４．７」における「７」は、配置ポリシーＰ４についての第７番目のオープンジャーナルを特定する）。
FIG. 5 shows that various combinations of
図５に示されるように、配置ポリシーＰ１について、単一のオープンプライマリジャーナル２３２．Ｐ１．１が存在し、オープンセカンダリジャーナルは存在しない。配置ポリシーＰ２について、２つのオープンプライマリジャーナル２３２．Ｐ２．１および２３２．Ｐ２．２が存在する。配置ポリシーＰＮについて、１つのオープンプライマリジャーナル２３２．ＰＮ．１および１つのオープンセカンダリジャーナル２３４．ＰＮ．１が存在する。これらの例が示すように、オープンプライマリジャーナル２３２およびオープンセカンダリジャーナル２３４の数は、配置ポリシー同士間で変動し得、典型的に、各配置ポリシー２１２についての新しいオブジェクト２２６の予想される数と、それらのオブジェクト２２６についての所望の位置とに基づき、各ポリシー２１２について構成される。
As shown in FIG. 5, for the placement policy P1, a single open
各インスタンス１０２はさらに、以前に図３に関して記載されたように、オブジェクトメタデータ２２８およびジャーナルメタデータ２３６の両方を格納する。各オブジェクト２２６について、オブジェクトメタデータ２２８は、（一意にオブジェクトを識別する）オブジェクトＩＤ３３０と、オブジェクトからオブジェクトチャンク２３８を識別する１つ以上のチャンクＩＤ３４６のセットと、各チャンクＩＤ２３６に関連付けられる割り当てられたジャーナルＩＤ３４８とを含む。オブジェクトが複数のチャンク２３８を有する場合、チャンク２３８は、（たとえばロードバランシングのために）同じジャーナルにすべて格納される必要は必ずしもないので、オブジェクトメタデータ２２８は、各チャンクＩＤ３４６に割り当てられるジャーナルＩＤ３４８をトラッキングしなければならない。
Each instance 102 further stores both object
各インスタンス１０２はさらに、インスタンス１０２に格納される各ジャーナルについてジャーナルメタデータ２３６を格納する。メタデータ２３６は、各ジャーナルについてのジャーナルＩＤ３７０と、位置３７２のセットとを含む。いくつかの実現例において、位置ＩＤは、ジャーナルが格納されるインスタンスを識別する。いくつかの実現例において、位置ＩＤはさらに、特定されたインスタンスにあるデータストアを識別する。いくつかの実現例において、インスタンス識別子およびデータストア識別子は、各ジャーナルについて別個の属性として格納される。いくつかの実現例において、ジャーナルは、単一のインスタンスにある２つ以上のデータストアに格納され得る（たとえばファイルシステムデータストアおよびテープバックアップデータストア）。ジャーナルメタデータ２３６はさらに、各ジャーナルに対応する一意の配置ポリシー２１２を特定する配置ポリシーＩＤ３７４を含む。各ジャーナルは、配置ポリシー３３８がジャーナルの配置ポリシーとマッチするオブジェクトチャンク２３８のみを格納する。
Each instance 102 further
図６は、いくつかの実現例が新しいオブジェクト２２６の格納をどのように管理するかを示す。図６に示されるように、各新しいオブジェクトはオブジェクトコンテンツ（すなわちオブジェクト２２６自体）と、オブジェクトＩＤ３３０（たとえば５８４４０９１２）と、割当配置ポリシー３３０（たとえばＰ３）とを有する。新しいオブジェクト２２６は、オンライン電子メールアプリケーションおよびビデオシェアリングウェブサイトなどといった多くの異なるアプリケーション２４６から得られ得る。分散ストレージシステム２００は、新しいオブジェクト２２６を受け取り、インスタンス１０２−１のような適切なインスタンスに新しいオブジェクト２２６を方向付け（６０２）する。いくつかの実現例において、アプリケーション２４６は新しいオブジェクト２２６を特定のインスタンス１０２−１に方向付けする。アプリケーション２４６によって選択されるインスタンス１０２−１が適切でない場合、いくつかの実現例は、オブジェクト２２６を適切なインスタンスに転送する（たとえば、配置ポリシー２１２がヨーロッパにおける格納を特定せず、オブジェクト２２６がヨーロッパにおけるインスタンスにおいて受け取られる場合、当該インスタンスは、オブジェクト２２６を別のインスタンスに転送し得る）。
FIG. 6 illustrates how some implementations manage the storage of
たいていのオブジェクトは、中程度のサイズ（たとえば３００キロバイト未満）を有するが、大きいオブジェクトがいくつか存在する。いくつかの実現例は、大きいオブジェクトを複数のチャンク２３８へ分割する（６０４）。一般に、各実現例は、典型的に単位がメガバイトで特定される（たとえば２、４、８、１６、または３２メガバイト）チャンクサイズをセットするか、または、当該チャンクサイズをセットするために構成可能であるパラメータを有する。チャンクサイズより大きい各オブジェクトは複数のチャンクへ分割され、チャンクサイズ以下のサイズを有する各オブジェクトは単一のチャンクからなる。図６における例示において、３つのチャンクＣ１、Ｃ２およびＣ３が存在する。この例示において、チャンクの各々は７文字の英数字チャンクＩＤ３４６を有しているが、一意に各オブジェクト内のチャンクを識別する多くの代替的なチャンクＩＤフォーマットが可能である。いくつかの実現例において、チャンクＩＤ３４６は、コンテンツハッシュまたはコンテンツダイジェストを使用して生成される。
Most objects have a medium size (eg, less than 300 kilobytes), but there are some large objects. Some implementations split a large object into multiple chunks 238 (604). In general, each implementation typically sets a chunk size specified in megabytes (
いくつかの実現例において、多くのオブジェクト重複物が存在する場合がある（たとえば、メール添付物がある人々のグループに送られ、その後、多くのさらなる人々に転送される）ので、重複除外（de-duplication）は効率的な格納に有用であり得る。したがって、いくつかの実施形態において、オープンプライマリジャーナルに「新しい」チャンク２３８のみを格納する（６０６）よう、各新しいチャンク２３８のコンテンツが、（たとえばコンテンツハッシュまたはコンテンツダイジェストを使用して）既存のオブジェクトチャンク２３８と比較される（６０６）。図５に示されるように、チャンクＣ２は新しく、配置ポリシーＰ３に対応しているので、チャンクＣ２は、配置ポリシーＰ３に対応するオープンプライマリジャーナル２３２．Ｐ３．１に格納される。もちろん、重複除外は配置ポリシーの文脈内にのみ存在する。２つのチャンクが同一であるが、異なる配置ポリシーに割り当てられている場合、２つのチャンクは異なるジャーナルに保存されることになる。言い換えれば、新しいチャンクが受け取られる場合、新しいチャンクは、同じ配置ポリシーについてのチャンクとのみ比較される。同じ配置ポリシーについて保存された同一のチャンクが既に存在する場合にのみ、チャンクは「重複物」である。
In some implementations, many object duplicates may exist (eg, mail attachments are sent to a group of people and then forwarded to many additional people), so deduplication (de -duplication) can be useful for efficient storage. Thus, in some embodiments, the content of each
オブジェクトチャンクＣ２が新しいかどうかにかかわらず、インスタンス１０２−１は、チャンク２３８についてオブジェクトメタデータ２２８を格納する（６０８）。図３〜図５に関して上述したように、メタデータ２２８は、オブジェクトＩＤ３３０と、チャンクＩＤ３４６と、各チャンクが格納されるジャーナルについてのジャーナルＩＤ３４８とを含む。いくつかの実現例において、オブジェクトチャンク２３８についてのチャンクＩＤ３４６は単に、オブジェクト内のチャンク２３８の開始に対するオフセットである。図６に示されるオブジェクトメタデータ２２８はさらに、単一のオブジェクトについてのチャンクが同じジャーナルに格納される必要はないということを示す。チャンクＣ１およびＣ３（チャンクＩＤ Ｃ１９００５６およびＣ０９８６６３）は、ジャーナルＩＤ Ｊ７７２９８０４５を有するジャーナル２３２．Ｐ３．２に存在し、チャンクＣ２（チャンクＩＤ Ｃ２５０１１６）は、ジャーナルＩＤ Ｊ８２１１７０９４を有するジャーナル２３２．Ｐ３．１に存在する。
Regardless of whether object chunk C2 is new, instance 102-1 stores object
チャンクＣ２はセカンダリジャーナル２３４．Ｐ３．１における格納のために、インスタンス１０２−２に送信され（６１０）、チャンクＣ１およびＣ３は、セカンダリジャーナル２３４．Ｐ３．２における格納のためにインスタンス１０２−２に送信される（６１２）。
Chunk C2 is the
図６はさらに、プライマリジャーナル２３２が、その対応するセカンダリジャーナルと物理的に同一である必要はないことを示す。まず、チャンクＣ１およびＣ３がプライマリジャーナル２３２．Ｐ３．２においてその順に格納され、これらのチャンクはセカンダリジャーナル２３４．Ｐ３．２においてその逆の順に格納されるということが分かる。ジャーナルがオープンである間、個々のチャンク２３８は、独立してレプリケートされるか、異なるネットワークパスを横断するか、または、異なるプロセッサ４０２によって処理され得るので、それらが同じ順でセカンダリジャーナル２３４．Ｐ３．２にロードされる保証はない。異なる順が存在し得るという事実は、図７に関して以下に記載されるように、各ジャーナル内のチャンクインデックスによって扱われる。さらに、プライマリジャーナル２３２．Ｐ３．１は、当該図において「Ｇ」とラベル付されたガーベッジ「チャンク」６２０の存在を示す。アップロードの間に時として、スペースを消費する障害またはグリッチが存在する場合がある。たとえばアップロードの間に、オブジェクトのためのスペースが割り当てられるが、チャンクが実際に追加されない場合がある。ソフトウェアはアップロードを再び試み、これにより、チャンクのための新しいスペースが割り当てられる。これは、ジャーナル２３２内にホール（hole）またはガーベッジ（garbage）を残し得る。この場合、ガーベッジ６２０はセカンダリジャーナルに送信されないので、プライマリジャーナルはセカンダリジャーナルと物理的に異なる。
FIG. 6 further illustrates that a
図７は、いくつかの実現例に従ったオープンジャーナルの構造を示す。図７はオープンプライマリジャーナル２３２を記載するが、オープンセカンダリジャーナル２３４の構造は同じまたは同様である。ジャーナル２３２は、ヘッダー７０２およびストレージスペース７１４のブロックを有する。ストレージスペース７１４は、既にオブジェクトチャンク２３８を格納している充填部分７１０と、現在未使用である非充填部分７１２とを含む。これらの記述子は、いくつかの理由により完全には正確ではない。第１に、「充填」スペース７１０は、有用なコンテンツを有さないガーベッジ部分６２０を含み得る。第２に、未使用スペースは必ずしもすべて同時に割り当てられるわけではない。いくつかの実現例は、ジャーナルについて全スペースを一度に割り当て、充填されると、（端部に少ない量の未使用スペースを潜在的に残して）当該ジャーナルをクローズする。しかしながら、他の実現例では、付加的なスペースのブロックは、ジャーナルがあるサイズの限界に達するか、または、ある量の時間（たとえば１日）が経過するまで、必要に応じて割り当てられる。
FIG. 7 shows the structure of an open journal according to some implementations. Although FIG. 7 describes the open
ジャーナルについてのヘッダー７０２は、ジャーナル２３２に関する重要な内部情報を含む。ヘッダー７０２は、未使用スペース７１２がジャーナルにおいてどこで開始するかを特定するフィールド７０４を含む。新しいチャンク２３８が充填スペース７１０の端部に追加されるごとに、オフセット７０４は、ジャーナル２３２が次のチャンクを格納するよう準備されるように、チャンク２３８のサイズだけインクリメントされる。
The
ヘッダー７０２はさらにチャンクインデックス７０６を含む。ジャーナル２３２についてのチャンクインデックス７０６は、各チャンク２３８がジャーナル２３２内のどこに位置するのかと、そのサイズとを特定し、これにより、（不揮発性ストレージまたはキャッシュからにかかわらず）チャンクデータの高速読出を可能にする。チャンクインデックス７０６についてのキーは、一意にチャンクを識別するチャンクＩＤ３４６である。なお、複数の異なるオブジェクトＩＤ３３０は、同じ物理的なチャンクを指し得る。多くのエントリが同じオブジェクトチャンク２３８を指す大きなチャンクインデックス７０４を回避するために、実現例は典型的に、同じ物理的なコンテンツを参照するために単一のチャンクＩＤを利用する。たとえば、チャンクＩＤ３４６は、コンテンツハッシュまたはコンテンツダイジェスト（またはこれらの組合せ）であり得る。各チャンクＩＤ３４６について、チャンクインデックス７２０は、ストレージスペース７１４内のチャンク２３８についてのオフセット７２０およびサイズ７２２を特定する。オフセット７２０は、ジャーナル２３２の始めからのオフセット、または、充填スペース７１０の始めからのオフセットとして特定され得る。いくつかの実現例では、チャンクインデックスは、チャンクが削除されて充填スペース７１０がコンパクト化される際に後で使用される削除マーカといった付加的な情報を有する。
The
ヘッダー７０２は同様に、実現例の詳細に対応するよう、他のジャーナルデータ７０８を含み得る。たとえば、他のジャーナルデータ７０８は、ジャーナルの始めからストレージスペース７１４の始めまでのオフセットを特定し得る（すなわちヘッダーのサイズ）。いくつかの実現例では、他のジャーナルデータは、短いライフスパンを有するように指定されるジャーナルについての「生存時間（time to live）」パラメータを含む。
The
図７におけるジャーナルの構造はオープンプライマリジャーナル２３２のためのものであるが、同じ基本構造は、オープンセカンダリジャーナル２３４およびクローズドジャーナル２３０に同様に適用される。
Although the journal structure in FIG. 7 is for the open
図８は、いくつかの実現例に従って、ジャーナルが１つのインスタンスから別のインスタンスにレプリケートされる場合、オブジェクトメタデータ２２８およびジャーナルメタデータ２３６に何が起こるかを示す。この例示において、ジャーナルＩＤ Ｊ８２１１７０９４を有するクローズドジャーナル２３０は、（インスタンスＩＤ＝７２３を有する）インスタンス１０２−１から（インスタンスＩＤ４２８を有する）インスタンス１０２−４にレプリケート（８２０）される。ジャーナル２３０自体が単位としてレプリケートされるので、全コンテンツが正確にレプリケートされる。たとえば、（チャンクＩＤ Ｃ４０８３３５を有する）チャンクＣ８は、ジャーナル内のまったく同じ位置にある。もちろん、レプリケーションの後、インスタンス１０２−１および１０２−４は独立して削除およびコンパクションを扱うので、それらの物理構造は、レプリケーションの後、同じままであることは保証されない。
FIG. 8 illustrates what happens to object
図８はさらに、レプリケーション８２０の前および後のオブジェクトメタデータ２２８およびジャーナルメタデータ２３６の部分を示す。示されるように、オブジェクトメタデータ２２８におけるレコード８０２〜８１４はレプリケーション８２０によって変更されていない。各オブジェクト２２６は同じチャンク２３８を有し、チャンク２３８は同じジャーナル２３０に格納される。たとえば、（ロウ８０４における）チャンクＩＤ Ｃ４０８３３５を有するチャンクは変更されない。他方、ジャーナルＩＤ Ｊ８２１１７０９４（３７０−１）を有するジャーナル２３０についてのジャーナルメタデータ２３６は変更される。ジャーナル位置３７２のセットは、３７２−１（Ａ）から、（インスタンス１０２−４についての）新しい位置４２８を含む３７２−１（Ｂ）に変更する。
FIG. 8 further shows portions of
図９Ａ〜図９Ｃは、いくつかの実現例に従った分散ストレージシステム２００においてオブジェクトレプリカの配置を管理する（９０２）方法９００を示す。当該方法は、１つ以上のプロセッサおよびメモリを有する分散ストレージシステムの第１のインスタンス１０２において実行（９０４）される。メモリは、複数のオブジェクトを格納する（９０６）。メモリはさらに、１つ以上のプロセッサによる実行のための１つ以上のプログラムを格納する（９０８）。いくつかの実現例において、方法９００のすべてまたは一部は、位置割当デーモン２０６によって実行される。いくつかの実現例において、分散ストレージシステムは複数のインスタンスを有する（９１０）。これらの実現例のうちのいくつかにおいて、インスタンスの少なくともサブセットは、異なる地理的位置にて存在する（９１０）。いくつかの実現例において、各インスタンスはデータセンタに対応する。いくつかの実現例において、各データセンタは１つ以上のインスタンスを含む。
9A-9C illustrate a
第１のインスタンスでは、１つ以上のジャーナル２３２が、オブジェクトチャンクの格納のためにオープンにされる（９１２）。各ジャーナルは、単一のそれぞれの配置ポリシー２１２に関連付けられる（９１４）。いくつかの実現例では、各配置ポリシーは、対象数のオブジェクトレプリカと、オブジェクトレプリカについての位置の対象セットとを特定する（９２６）。いくつかの実現例では、配置ポリシー２１２は、データストア２２４のどのタイプがインスタンスのうちのいくつかにおいて使用されるべきであるか（たとえばディスクまたはテープ）を特定し得る。いくつかの実現例では、分散ストレージシステム２００は、どのジャーナルに各オブジェクトチャンク２３８が格納されているかを特定するオブジェクトメタデータ２２８を含む（９１８）。これは、図３〜図５に関して以前に記載された。いくつかの実現例では、各それぞれのジャーナルは、それぞれのジャーナルに格納された各オブジェクトの位置を特定するチャンクインデックス７０６を含む（９２０）。これは、図７により詳細に記載された。特に、ジャーナル内の各チャンクの位置は、ジャーナル自身に対して識別され、したがって、チャンクインデックス７０６は、ジャーナルがどこに格納されるかにかかわらず正確である。たとえば、オフセットとしてジャーナル内のチャンクの位置を特定することによって、チャンクはリラティブアドレッシング（relative addressing）によってアクセスされ得る。
In the first instance, one or
開示された実現例は典型的に、各ジャーナルが格納される位置３７２を特定するジャーナルメタデータ２３６を含む（９２２）。これは、図３〜図５および図８において以前に記載された。
The disclosed implementation typically includes
オープンプライマリジャーナル２３２およびオープンセカンダリジャーナル２３４の分散は、利用可能なインスタンス１０２と、配置ポリシー２１２と、配置ポリシー２１２による新しいオブジェクト２２６の予想される分散と、新しいオブジェクトがどこ（たとえばヨーロッパ、北米、アジア）からロードされるかと、利用可能なインスタンス１０２の各々での処理リソースと、さまざまなインスタンス同士間のネットワーク帯域幅とを含む多くのファクタに依存する。たとえば、多くのオブジェクトが、特定のインスタンスにおいて特定の配置ポリシーでアップロードされる場合、そのインスタンスでの同じ配置ポリシーについて、複数のジャーナルがオープンにされる（９２４）。いくつかのシナリオにおいて、ロードバランシングに必要とされる場合、単一のインスタンス１０２において、同じ配置ポリシー２１２について、５個、１０個、またはそれ以上のオープンジャーナルが存在し得る。
The distribution of the open
図５および図６に関して上述したように、いくつかの実現例は、第１のインスタンスにおいてオープンにされたジャーナルに対応するジャーナルをオープンにするよう、メッセージを分散ストレージシステム２００の第３のインスタンスに送信する（９１６）。このシナリオにおいて、第１のインスタンスにおいてオープンにされたジャーナル２３２はプライマリジャーナルと称され、第３のインスタンスでオープンにされたジャーナル２３４はセカンダリジャーナルと称される。（もちろん、第１のインスタンスはセカンダリジャーナルも有し得、第３のインスタンスはプライマリジャーナルを有し得る）。
As described above with respect to FIGS. 5 and 6, some implementations may send messages to the third instance of the distributed storage system 200 to open the journal corresponding to the journal that was opened in the first instance. Transmit (916). In this scenario, the
第１のインスタンス１０２では、少なくとも第１のオブジェクトチャンクを含む（９２８）第１のオブジェクト２２６が受け取られる（９２８）。これは、図６に関して上に記載された。第１のオブジェクト２２６は第１の配置ポリシー２１２に関連付けられており、したがって、オブジェクト２２６を含むオブジェクトチャンク２３８のすべては第１の配置ポリシー２１２に関連付けられる。第１のオブジェクトチャンク２３８は、関連付けられる配置ポリシーが第１の配置ポリシー２１２とマッチする第１のジャーナル２３２に格納される（９３０）。第１のジャーナル２３２は、配置ポリシーが第１の配置ポリシーとマッチするオブジェクトについてのオブジェクトチャンクのみを格納する（９３２）。いくつかの実現例では、第１のジャーナル２３２に格納された各オブジェクトチャンク２３８は、第３のジャーナル２３４における格納のために、第３のインスタンスに送信される（９３４）。
In the first instance 102, a
受け取られたオブジェクトがチャンクサイズより大きい場合、オブジェクトは複数のチャンク２３８へ分割される。この場合、第１のオブジェクト２２６は２つ以上のオブジェクトチャンクを含む（９３６）。典型的に、第２のオブジェクトチャンクは、第１のオブジェクトチャンクと異なる（９３６）。（単一のオブジェクト内に２つの同一のチャンクを有することは稀であるが、たとえば、オブジェクトが空のスペースの非常に大きな部分を有する場合に起こり得る。）いくつかの状況では、第２のオブジェクトチャンクが、関連付けられる配置ポリシーが第１の配置ポリシーとマッチする、第１のジャーナルとは異なる第２のジャーナル２３２に格納される（９３８）。第２のジャーナルは、配置ポリシーが第１の配置ポリシーとマッチするオブジェクトについてのオブジェクトチャンクのみを格納する（９３８）。これにより、多くのチャンクを含むオブジェクトは、多くの異なるジャーナルにわたって分散されるチャンクを有し得る。
If the received object is larger than the chunk size, the object is divided into
オブジェクト２２６を受け取り、かつ、第１のジャーナル２３２にチャンク２３８を格納するこのプロセスは、関連付けられる配置ポリシー３３８が第１の配置ポリシー２１２にマッチする複数のオブジェクト２２６について、第１の終了条件が発生するまで繰り返される（９４０）。いくつかの実現例では、第１の終了条件は、第１のジャーナルのサイズが予め規定されたしきい値を越える（９４２）場合に発生する。いくつかの実現例では、第１の終了条件は、第１のジャーナルが予め規定された時間スパンの間オープンであった場合（９４４）に発生する。いくつかの実現例は、さまざまな態様でサイズおよび時間を組み合わせる。たとえば、いくつかの実現例は、時間スパンおよびサイズ限界の両方を特定しており、終了条件は、上記のうちのいずれか一方が最初に発生することである。
This process of receiving the
終了条件が発生した後、第１のジャーナルはクローズされ（９４６）、これにより、如何なる付加的なオブジェクトチャンクも第１のジャーナル２３２に格納されることを防止する。一般に、実現例は、第１のジャーナルをクローズする前に、同じ配置ポリシーについての他のジャーナル２３２がまだオープンである（または新しいジャーナルがオープンである）ことを確認する。新しいオブジェクトがいつでも到着し得るので、格納のためにオープンジャーナルを利用可能にしておくことが重要である。別のインスタンスに対応するセカンダリジャーナル２３４が存在する場合、第１のインスタンスは、第１の終了条件が発生する場合に対応するセカンダリジャーナルをクローズするよう、他のインスタンスにメッセージを送信する（９４８）。
After the termination condition occurs, the first journal is closed (946), thereby preventing any additional object chunks from being stored in the
第１のジャーナル２３２がクローズされた後、ジャーナルはその配置ポリシーに従う。配置ポリシー２１２を満足させることは、ジャーナルレプリカを移動させること、ジャーナルレプリカの新しいコピーを作成すること、または、ジャーナルのレプリカを削除することを必要とし得る。いくつかの状況では、第１のジャーナル２３２は、配置ポリシー２１２に従って、分散ストレージシステム２００の第２のインスタンス１０２にレプリケートされる（９５０）。（他の状況において、第１のジャーナルのレプリカは削除される。）プライマリオープンジャーナル２３２およびセカンダリオープンジャーナル２３４を有する実現例では、それらがひとたびクローズされると、２つの同等なクローズドジャーナル２３０が存在することになる。したがって、レプリケーション９５０のためのソースとして、レプリカのいずれかが使用され得る。レプリケーション９５０が発生する（すなわちトランザクションの一部として）と、第２のインスタンスにジャーナルのコピーが存在することを示すよう、第１のジャーナルについてのジャーナルメタデータ２３６は更新される（９５２）。これは図８に関して上に記載された。
After the
ジャーナル２３０がクローズされた後、オブジェクトチャンク２３８が削除され得る。たとえば、オブジェクトはメール添付物に対応し得る。電子メールの受信者が電子メールを削除すれば、当該添付物についての格納分は削除され得る。ある期間の後、当該削除により各ジャーナル内にホールが存在するので、当該無駄にされているスペースを除去するようジャーナルをコンパクト化することが有用である。これは、揮発性メモリのフラグメンテーション、および、未使用スペースをより大きな連続ブロックへと統合するデフラグメンテーションのプロセスに類似している。
After the
格納されたオブジェクトチャンクが多くの（たとえば何百、何千または何百万の）異なるオブジェクトに対応し得るので、ジャーナルにおけるオブジェクトチャンクは、当該オブジェクトチャンクへの参照がもはや存在しない場合にのみ削除され得る。したがって、第１のクローズドジャーナル２３０がひとたび選択される（９５４）と、プロセス９００は、オブジェクトメタデータ２２８において参照が存在しない第１のクローズドジャーナル２３０に格納された１つ以上のオブジェクトチャンクを識別する（９５６）。これらの識別されたチャンク２３８について、対応するレコードを除去するようチャンクインデックス７０６が更新される（９５８）。いくつかの実現例では、識別されたオブジェクトチャンクに以前に割り当てられたスペースは、上書きされる（たとえば各バイトがＡＳＣＩＩ０にセットされる）が、他の実現例では、当該スペースはもはや参照されない。いくつかの実現例では、割り当てを解除されたストレージスペースは、他のジャーナルデータ７０８の一部としてトラッキングされる。たとえば、いくつかの実現例は、割り当てを解除されたストレージスペース（たとえばオフセットおよびサイズ）のリストを維持するか、または、割り当てを解除されたスペースをリンク付けされたリストとしてトラッキングする。
Since a stored object chunk can correspond to many (eg, hundreds, thousands or millions) of different objects, the object chunk in the journal is deleted only if there is no longer a reference to that object chunk. obtain. Thus, once the first
いくつかの実現例では、ガーベッジコレクションアルゴリズムは、クローズドジャーナルの各々をコンパクト化する（９６０）よう、周期的に実行される。当該コンパクションプロセスは、格納されたオブジェクトチャンクを連続的なブロックへ統合し（９６０）、これにより、ジャーナル２３０のサイズを低減する。時間にわたって、より多くのオブジェクトチャンクが削除されると、ジャーナル２３０は小さくなり得る。多くの小さなジャーナルを管理することには、個々のオブジェクトを管理することと同様のオーバーヘッドが存在するので、ジャーナルの格納の利益は縮小される。この問題に対応するために、いくつかの実現例は、２つ以上のクローズドジャーナル同士を結合して（９６２）単一の置換ジャーナルを形成し、当該２つ以上のジャーナルに以前に格納されたオブジェクトチャンクは置換ジャーナルに格納されているということを示すようオブジェクトメタデータ２２８を更新する（９６２）。結合オペレーションは、完全に新しいジャーナルを形成することと、関連するオブジェクトのすべてについてメタデータを更新することとを必要とするので、結合は通常、ジャーナルが相対的に小さくなったシナリオに限定される。
In some implementations, the garbage collection algorithm is executed periodically to compact (960) each of the closed journals. The compaction process consolidates the stored object chunks into successive blocks (960), thereby reducing the size of the
図１０Ａおよび図１０Ｂは、分散ストレージシステムにおいて、オブジェクトおよび関連付けられるメタデータを格納するための２つの実現例を示す。図１０Ａにおいて示される実現例は、完全に階層的（hierarchical）である。すなわち、すべてのオブジェクトは１つ以上のチャンクへ分割され、すべてのチャンクは１つ以上のブロックへ分割されている。なお、１つのチャンクまたは１つのブロックのみが存在する場合も、当該構造は階層的である。他方、図１０Ｂに示される実現例は単に、部分的に階層的である。この実現例において、いくつかのチャンクは、ブロックのリストを指す「スーパーチャンク」である。たとえばスーパーチャンクは、１００個のブロック、１０００個のブロック、またはそれ以上のブロックを有するブロックリストを有し得る。スーパーチャンクでないチャンクは単にブロックである。すなわち、チャンク識別子は、ブロックのリストではなく、オブジェクトデータの実際のストレージを指す。このハイブリッドアプローチは、（階層が必要でない）小さなオブジェクトと、ストレージ階層が非常により効率的である非常に大きいオブジェクトとの両方を含む分散ストレージシステムに有用であり得る。 10A and 10B show two implementations for storing objects and associated metadata in a distributed storage system. The implementation shown in FIG. 10A is completely hierarchical. That is, all objects are divided into one or more chunks, and all chunks are divided into one or more blocks. Note that even when only one chunk or one block exists, the structure is hierarchical. On the other hand, the implementation shown in FIG. 10B is only partially hierarchical. In this implementation, some chunks are “super chunks” that point to a list of blocks. For example, a super chunk may have a block list with 100 blocks, 1000 blocks, or more blocks. A chunk that is not a super chunk is simply a block. That is, the chunk identifier refers to the actual storage of object data, not a list of blocks. This hybrid approach may be useful for distributed storage systems that include both small objects (no hierarchy is required) and very large objects where the storage hierarchy is much more efficient.
図２〜図４に示されるように、グローバルメタデータ１００２は、オブジェクトメタデータ２２８およびジャーナルメタデータ２３６を含む。いくつかの実現例において、各チャンクＩＤ３４６は、チャンクについてのデータを複合化するために使用される復号化キー１０４０を割り当てられる。これらの実現例では、同じ復号化キーは、複数のブロックへ分割されるチャンクについて、チャンクにおけるすべてのブロックに適用されることになる。各チャンクは、事実上一意である自身の復号化キー１０４０を有する。新しいキーが生成される場合、いくつかの実現例は一意性を保証するが、いくつかの実現例は新しいキーをランダムに生成し、キーの繰り返しが起こる可能性はあまり高くない。復号化キー１０４０は、新しいオブジェクトが格納される際に、新しいオブジェクトを暗号化するために使用される暗号キーに対応する。復号化キー１０４０は各オブジェクトチャンクにアクセスするために必要とされるので、復号化キーを削除することは、オブジェクトチャンクの「ソフトな（soft）」削除として使用され得る。復号化キーがなくなると、暗号化されたストレージは「ガーベッジ」となり、実際のデータはアクセス不能となる。これにより、ガーベッジコレクションアルゴリズムは、コンパクション同士の間により多くの時間を得ることが可能になり、ガーベッジコレクションプロセスは、実行されると、より多くのストレージスペースを回収することができる。
As shown in FIGS. 2 to 4, the global metadata 1002 includes
さらに、（オープンとして示される）ジャーナル２３２と、対応するローカルメタデータ１００４とが図１０Ａにおいて示される。いくつかの実現例では、ジャーナル２３２についてのローカルメタデータ１００４は、ヘッダー７０２の一部として、ジャーナル自身に格納される。他の実現例では、ジャーナルについてのローカルメタデータ１００４は別個のファイルとして（またはデータベースなどに）格納され、ジャーナルに関連付けられる。非階層的なストレージについてのジャーナル２３２の構造は図７に関して上で示された。この実現例では、チャンクを格納するのではなく、ストレージの基本単位は、ブロック１０１６−１，１０１６−２，…，１０１６−Ｎといったブロック１０１６である。各実現例は典型的に、２メガバイト、４メガバイト、８メガバイトまたは１６メガバイトといったような最大のブロックサイズを特定する。
In addition, journal 232 (shown as open) and corresponding local metadata 1004 are shown in FIG. 10A. In some implementations, local metadata 1004 for
上に示されるように、ローカルメタデータ１００４はジャーナル２３２のヘッダー７０２に格納され得るか、または、別々に格納され得る。各チャンク識別子３４６について、１つ以上のブロック識別子１００８を含む対応するブロックリスト１００６（典型的に一意のブロックリスト）が存在する。小さなチャンクについては、ブロックリスト１００６は、単一のブロック識別子１００８を含み得る。ローカルメタデータ１００４はさらに、各ブロックがジャーナル２３２内でどこに位置するかを特定するブロックインデックス１０１０を含む。いくつかの実現例において、ブロックの位置はオフセットおよびサイズによって特定される。いくつかの実現例におけるブロックオフセット１０１２は、ストレージスペース７１４の始めからのオフセットまたはジャーナルファイル２３２の始めについてのオフセットである。典型的に、ブロックサイズ１０１４はバイトで特定されるが、他の実現例は、サイズの代替的な基本単位（たとえば２バイト、４バイトまたは８バイト）を使用する。ローカルメタデータの１つの局面は、ジャーナルが別のインスタンスに移動またはレプリケートされる場合に、ローカルメタデータは変更しないということである。すなわち、あるチャンクについてのブロックリスト１００６は同じままであり、ブロックＩＤ１００８は同じままであり、ジャーナル内のブロックオフセット１０１２は同じままであり、ブロックサイズは同じままである。
As indicated above, the local metadata 1004 may be stored in the
図１０Ｂは図１０Ａと同様であるが、部分的に階層的な構造を示す。図１０Ｂの部分的に階層的な構造において、グローバルメタデータ１００２は、各チャンクが通常のブロックか、または、ブロックのリストを参照する（すなわちスーパーチャンクである）かどうかを示す「スーパーチャンク」フィールド１０２０を含む。いくつかの実現例において、ほとんどのオブジェクトは小さく、単一のチャンクからなる。この場合、チャンクＩＤ３４６は、チャンク／ブロックインデックス１０２４においてブロックを直接的に識別する。すなわち、チャンクＩＤ３４６はチャンク／ブロックＩＤ１０２６である。したがって、スーパーチャンクでないチャンクの場合、ジャーナル２３２における対応するブロック１０１６についてオフセット１０２８およびサイズ１０３０を見つけるために、チャンク／ブロックインデックス１０２４における適切なレコードをルックアップするよう、チャンクＩＤ３４６が使用され得る。
FIG. 10B is similar to FIG. 10A, but shows a partially hierarchical structure. In the partially hierarchical structure of FIG. 10B, global metadata 1002 includes a “super chunk” field that indicates whether each chunk is a regular block or references a list of blocks (ie, is a super chunk). 1020 included. In some implementations, most objects are small and consist of a single chunk. In this case, the
スーパーチャンクの場合、チャンクＩＤ３４６は、ローカルメタデータ１００４においてルックアップされ得る（スーパー）チャンクＩＤ１０２２である。スーパーチャンクＩＤ１０２２に対応するのはブロックリスト１００６であり、ブロックリスト１００６はブロックＩＤ１００８のセットを含む。この場合、ブロックＩＤの各々は、スーパーチャンクＩＤ１０２２についてのブロックリスト１００６においてブロックＩＤ１０２６の各々についてオフセット１０２８およびサイズ１０３０を識別するよう、チャンク／ブロックインデックス１０２４においてルックアップされ得る。上記のように、オフセット１０２８およびサイズ１０３０は、ジャーナル２３２のストレージスペース７１４において実際のブロックストレージの位置を識別する。したがって、スーパーチャンクはさらなるレベルの階層を有するが、グローバルメタデータ１００２に格納されるチャンクメタデータの量を低減する。これは、１つのインスタンスから別のインスタンスにシャードを移動させることをより容易かつ効率的にする。
For super chunks, the
図１１Ａ〜図１１Ｄは、いくつかの実現例に従った、分散ストレージシステム２００においてオブジェクトレプリカの配置を管理する（１１０２）方法１１００を示す。この方法は、１つ以上のプロセッサおよびメモリを有する分散ストレージシステムの第１のインスタンス１０２において実行される（１１０４）。メモリは、１つ以上のプロセッサによる実行のための１つ以上のプログラムを格納する（１１０６）。いくつかの実現例において、方法１１００のすべてまたは部分は位置割当デーモン２０６によって実行される。いくつかの実現例において、分散ストレージシステムは複数のインスタンスを有する（１１０８）。これらの実現例のうちのいくつかでは、インスタンスの少なくともサブセットは、異なる地理的位置に存在する（１１０８）。いくつかの実現例において、各インスタンスはデータセンタに対応する。いくつかの実現例において、各データセンタは１つ以上のインスタンスを含む。
11A-11D illustrate a
第１のインスタンスでは、１つ以上のジャーナル２３２がオブジェクトチャンクの格納のためにオープンにされる（１１１０）。各ジャーナルは、単一のそれぞれの配置ポリシー２１２に関連付けられる（１１１２）。いくつかの実現例では、各配置ポリシーは、対象数のオブジェクトレプリカと、オブジェクトレプリカについての位置の対象セットとを特定する（１１２２）。いくつかの実現例では、配置ポリシー２１２は、データストア２２４のどのタイプがインスタンスのうちのいくつかにおいて使用されるべきであるか（たとえばディスクまたはテープ）を特定し得る。いくつかの実現例では、分散ストレージシステム２００は、どのジャーナルに各オブジェクトチャンク２３８が格納されているかを特定するオブジェクトメタデータ２２８（グローバルメタデータ１００２の部分）を含む（１１１４）。これは、図３〜図５、図１０Ａおよび図１０Ｂに関して以前に記載された。いくつかの実現例では、各それぞれのジャーナルは、それぞれのジャーナルに格納された各ブロックの位置を特定するブロックインデックス１０１０または１０２６を含む（１１１６）。これは、図７（非階層的）、図１０Ａおよび図１０Ｂにより詳細に記載された。特に、ジャーナル２３２内の各ブロック１０１６の位置は、ジャーナル自身に対して識別され、したがって、ブロックインデックス１０１０または１０２６は、ジャーナル２３２がどこに格納されるかにかかわらず正確である。たとえば、オフセットとしてジャーナル２３２内のブロック１０１６の位置を特定することによって、ブロック１０１６はリラティブアドレッシング（relative addressing）によってアクセスされ得る。
In the first instance, one or
開示された実現例は典型的に、各ジャーナルが格納される位置３７２を特定するジャーナルメタデータ２３６（グローバルメタデータ１００２の一部）を含む（１１１８）。これは、図３〜図５および図８に以前に記載された。
The disclosed implementation typically includes journal metadata 236 (part of global metadata 1002) that identifies the
オープンプライマリジャーナル２３２およびオープンセカンダリジャーナル２３４の分散は、利用可能なインスタンス１０２と、配置ポリシー２１２と、配置ポリシー２１２有する新しいオブジェクト２２６の予想される分散と、新しいオブジェクトがどこ（たとえばヨーロッパ、北米、アジア）からロードされるかと、利用可能なインスタンス１０２の各々での処理リソースと、さまざまなインスタンス同士間のネットワーク帯域幅とを含む多くのファクタに依存する。たとえば、多くのオブジェクトが、特定のインスタンスにて特定の配置ポリシーでアップロードされる場合、そのインスタンスでの同じ配置ポリシーについて、複数のジャーナルがオープンにされる（１１２０）。いくつかのシナリオにおいて、ロードバランシングに必要とされる場合、単一のインスタンス１０２において、同じ配置ポリシー２１２について、５個、１０個、またはそれ以上のオープンジャーナルが存在し得る。
The distribution of the open
第１のインスタンス１０２では、少なくとも第１のオブジェクトチャンクを含む（１１２４）第１のオブジェクト２２６が受け取られる（１１２４）。これは、図６に関して上に記載された。第１のオブジェクト２２６は第１の配置ポリシー２１２に関連付けられており（１１２４）、したがって、オブジェクト２２６を含むオブジェクトチャンク２３８のすべては第１の配置ポリシー２１２に関連付けられる。第１のオブジェクトチャンクは、図１０Ａおよび図１０Ｂに関して上述したように、第１の複数のブロックを含む（１１２６）。いくつかの実現例において、プロセス１１００は、チャンクおよびブロックに既にパーティショニングされたオブジェクト２３８を受け取る（１１２４）。たとえば、オブジェクトをアップロードするクライアントデバイスによって分割が実行され得る。他の実現例において、プロセス１１００は、オブジェクトをストリームとして受け取り、格納された基準（たとえば対象ブロックおよびチャンクサイズ、利用可能なオープンジャーナル、利用可能なインスタンス、利用可能な帯域幅など）に従ってチャンクおよびブロックへオブジェクトを分割する。いくつかの実現例では、まだオブジェクトについてデータを受け取っている間、チャンクの動的割り当てが実行され、その一方、他の実現例は、全オブジェクトが受け取られた後にのみオブジェクトをチャンクおよびブロックへ分割する。
In the first instance 102, a
チャンクおよびブロックの階層は、さまざまな態様で、オブジェクトのサイズのようなさまざまなファクタに基づいて形成され得る。いくつかの実現例において、階層は、アップロードプロセスの間に動的に構築される。たとえば、第１のオブジェクトチャンクが作成され、しきい値数のブロックがチャンクに割り当てられるまで、データのストリームが、第１のオブジェクトチャンクに割り当てられるブロックへ分割される。その時点において、第２のチャンクが作成され、新しいブロックが第２のチャンクに追加される。別の実現例では、データのストリームは最初は、ストレージのブロックとして格納され、ブロックがもはや存在しない場合、ブロックはチャンクへとグループ化される。 The hierarchy of chunks and blocks can be formed based on various factors, such as the size of the object, in various ways. In some implementations, the hierarchy is built dynamically during the upload process. For example, a stream of data is divided into blocks that are assigned to the first object chunk until a first object chunk is created and a threshold number of blocks are assigned to the chunk. At that point, a second chunk is created and a new block is added to the second chunk. In another implementation, the stream of data is initially stored as a block of storage, and if the block no longer exists, the block is grouped into chunks.
いくつかの実現例において、すべてのオブジェクトチャンク２３８は１つ以上のブロックを含む（１１２８）。これは、図１０Ａに関して上で示された。いくつかの実現例において、グローバルメタデータ１００２は、各オブジェクトチャンク２３８がブロックまたはブロックのリストであるかどうかを特定するフィールド１０２０を含む（１１３０）。これは、図１０Ｂにおいて上で示される。いくつかのインスタンスにおいて、第１のオブジェクトチャンクはブロック（すなわちスーパーチャンク）のリストであり（１１３２）、第２のチャンクは通常のブロック（スーパーチャンクでない）である（１１３２）。
In some implementations, every
第１の複数のブロック１０１６は、関連付けられる配置ポリシーが第１の配置ポリシー２１２とマッチする第１のジャーナル２３２に格納される（１１３４）。第１のジャーナル２３２は、配置ポリシーが第１の配置ポリシーとマッチするオブジェクトについてのブロックのみを格納する（１１３６）。
The first plurality of
受け取られたオブジェクトが特定されたサイズ（たとえばチャンクサイズまたはブロックサイズ）より大きい場合、オブジェクトは、複数のチャンク２３８および／または複数のブロック１０１６へ分割される。いくつかのインスタンスにおいて、第１のオブジェクト２２６は２つ以上のオブジェクトチャンクを含む（１１３８）。典型的に、第２のオブジェクトチャンクは、第１のオブジェクトチャンクと異なる（１１３８）。（単一のオブジェクト内に２つの同一のチャンクを有することは稀であるが、たとえば、オブジェクトが空のスペースの非常に大きな部分を有する場合に起こり得る。）いくつかの状況では、第２のオブジェクトチャンクが、関連付けられる配置ポリシーが第１の配置ポリシーとマッチする第１のジャーナルとは異なる第２のジャーナル２３２に格納される（１１４０）。第２のジャーナルは、配置ポリシーが第１の配置ポリシーとマッチするオブジェクトについてのオブジェクトチャンクのみを格納する（１１４０）。これにより、多くのチャンクを含むオブジェクトは、多くの異なるジャーナルにわたって分散されるチャンクを有し得る。
If the received object is larger than a specified size (eg, chunk size or block size), the object is divided into
いくつかの実現例では、そのプロセスは、各オブジェクトチャンクについてデータを暗号化し（１１４２）、グローバルメタデータにおける各オブジェクトチャンクについての復号化キーを格納する（１１４２）。これは、図１０Ａおよび図１０Ｂにおいて上で示された。いくつかの実現例では、チャンクが複数のブロックへ分割される場合、チャンク内のブロックの各々は同じ暗号キーで暗号化されるので、同じ復号化キーで複合化され得る。他の実現例において、各ブロックは、ブロックインデックス１０１０または１０２６の一部として格納され得るそれ自身の復号化キーを有する。グローバルメタデータ１００２に復号化キー１０４０を格納する実現例では、チャンクは単純に、復号化キーを削除すること（１１４４）によって事実上削除され得る。オリジナルチャンクについてデータを抽出する方法がないので、チャンクはアクセス不能である。これはいくつかの利点を提供する。第１に、チャンクを削除することは迅速かつ有効である。第２に、削除されたデータにアクセスする現実のリスクがないので、より効率的なガーベッジコレクションプロセスが実現され得る。特に、ガーベッジコレクションは適切な間隔でスケジューリングされ得、ディスクからストレージの物理的な削除（physical deletes of storage）をバッチ処理し得る。コンパクションはリソース集中的なプロセスであるので、多くの削除を一緒にバッチ処理する能力は、効率を劇的に増加させ得る。第３に、いくつかの実現例は、暗号化された「意味のないこと（gibberish）」は、意味のあるコンテンツに戻すように変換され得ないので、ストレージスペースの物理的な消去を必要としない。
In some implementations, the process encrypts data for each object chunk (1142) and stores a decryption key for each object chunk in the global metadata (1142). This was shown above in FIGS. 10A and 10B. In some implementations, if a chunk is split into multiple blocks, each of the blocks in the chunk is encrypted with the same encryption key and can be decrypted with the same decryption key. In other implementations, each block has its own decryption key that can be stored as part of the
プロセス１１００は、第１のオブジェクトについてのグローバルメタデータを格納する（１１４６）。これは、図３〜図５、図１０Ａおよび図１０Ｂにおいて上で示された。グローバルメタデータ１００２は、第１のオブジェクトに対応するオブジェクトチャンクの第１のリストを含む（１１４８）。特に、第１のリストは、第１のオブジェクトチャンク２３８についてのオブジェクト識別子３３０を含む（１１５０）。グローバルメタデータ１００２はさらに、ジャーナルの各々についての位置と同様に各チャンクが格納されるジャーナルを識別する。
グローバルメタデータ１００２に加えて、ローカルメタデータ１００４が各ジャーナル２３２について格納される。いくつかの実現例では、各ジャーナルについてのローカルメタデータ１００４は、ジャーナル２３２自身のヘッダー７０２に格納される。他の実現例において、ローカルメタデータ１００４はジャーナルとは別々に格納される。別々に格納される場合、各ジャーナルについてのローカルメタデータ１００４は別々に格納されてもよく（たとえば各ジャーナルに対応する異なるメタデータファイル）、または、ローカルメタデータは（たとえばデータベースにおいて）一緒にグループ化されてもよい。
In addition to the global metadata 1002, local metadata 1004 is stored for each
第１のインスタンスは、第１のオブジェクトチャンク２３８についてのローカルメタデータ１００４を格納する（１１５２）。ローカルメタデータ１００４は、第１の複数のブロックにおける各ブロックを識別するブロックリストを含む（１１５４）。なお、ブロックリストは、グローバルメタデータ１００２ではなく、ローカルメタデータ１００４に格納される。ローカルメタデータ１００４に格納されたブロックリスト１００６は、ブロックがどのように各ジャーナル内に割り当てられるかをトラッキングする。第１のジャーナル２３２についてのローカルメタデータは、第１のジャーナル２３２に関連付けられる（１１５６）。いくつかの実現例では、ジャーナルとのローカルメタデータの関連付けは、ジャーナルにローカルメタデータを格納することによって行なわれており、これによりジャーナルがより独立する（self-contained）。いくつかの実現例では、ジャーナル２３２についてのローカルメタデータは別々に（たとえば別個のファイルにおいて）格納され、（たとえば、ジャーナルの名称と、関連付けられるメタデータファイルの名称とにジャーナルＩＤ３７０を含むことによって）ジャーナルに関連付けられる。データベースにローカルメタデータを格納する実現例において、ジャーナルＩＤ３７０は典型的にメタデータテーブルについてのプライマリキーの一部である。
The first instance stores local metadata 1004 for the first object chunk 238 (1152). The local metadata 1004 includes a block list that identifies each block in the first plurality of blocks (1154). Note that the block list is stored not in the global metadata 1002 but in the local metadata 1004. A
オブジェクト２２６を受け取り、かつ、第１のジャーナル２３２にチャンク２３８を格納するこのプロセスは、関連付けられる配置ポリシー３３８が第１の配置ポリシー２１２にマッチする複数のオブジェクト２２６について、第１の終了条件が発生するまで繰り返される（１１５８）。いくつかの実現例では、第１の終了条件は、第１のジャーナルのサイズが予め規定されたしきい値を越える（１１６０）場合に発生する。いくつかの実現例では、第１の終了条件は、第１のジャーナルが予め規定された時間スパンの間オープンであった場合（１１６２）に発生する。いくつかの実現例は、さまざまな態様でサイズおよび時間を組み合わせる。たとえば、いくつかの実現例は、時間スパンおよびサイズ限界の両方を特定しており、終了条件は、上記のうちのいずれか一方が最初に発生することである。
This process of receiving the
終了条件が発生した後、第１のジャーナルはクローズされ（１１６４）、これにより、如何なる付加的なブロックも第１のジャーナル２３２に格納されることを防止する。一般に、実現例は、第１のジャーナルをクローズする前に、同じ配置ポリシーについての他のジャーナル２３２がまだオープンである（または新しいジャーナルがオープンである）ことを確認する。新しいオブジェクトがいつでも到着し得るので、格納のためにオープンジャーナルを利用可能にしておくことが重要である。
After the termination condition occurs, the first journal is closed (1164), thereby preventing any additional blocks from being stored in the
第１のジャーナル２３２がクローズされた後、ジャーナルはその配置ポリシーに従う。配置ポリシー２１２を満足させることは、ジャーナルレプリカを移動させること、ジャーナルレプリカの新しいコピーを作成すること、または、ジャーナルのレプリカを削除することを必要とし得る。いくつかの状況では、第１のジャーナル２３２は、配置ポリシー２１２に従って、分散ストレージシステム２００の第２のインスタンス１０２にレプリケートされる（１１６６）。（他の状況において、第１のジャーナルのレプリカは削除される。）プライマリオープンジャーナル２３２およびセカンダリオープンジャーナル２３４を有する実現例では、それらがひとたびクローズされると、２つの同等なクローズドジャーナル２３０が存在することになる。したがって、レプリケーション１１６６のためのソースとして、レプリカのいずれかが使用され得る。レプリケーション１１６６が発生する（たとえばトランザクションの一部として）と、第２のインスタンスにジャーナルのコピーが存在することを示すよう、第１のジャーナルについてのグローバルメタデータ１００２が更新される（１１６８）。他方、ローカルメタデータ１００４はレプリケーションによって変更されない（１１６８）。これは、図８、図１０Ａおよび図１０Ｂに関して上で記載された。
After the
ジャーナル２３０がクローズされた後、オブジェクトチャンク２３８が削除され得る。たとえば、オブジェクトはメール添付物に対応し得る。電子メールの受信者が電子メールを削除すれば、当該添付物についての格納分は削除され得る。ある期間の後、当該削除により各ジャーナル内にホールが存在するので、当該無駄にされているスペースを除去するようジャーナルをコンパクト化することが有用である。これは、揮発性メモリのフラグメンテーション、および、未使用スペースをより大きな連続するストレージへと統合するデフラグメンテーションのプロセスに類似している。
After the
格納されたオブジェクトチャンクが多くの（たとえば何百、何千または何百万の）異なるオブジェクトに対応し得るので、ジャーナルにおけるオブジェクトチャンクは、当該オブジェクトチャンクへの参照がもはや存在しない場合にのみ削除され得る。したがって、第１のクローズドジャーナル２３０がひとたび選択される（１１７０）と、プロセス１１００は、オブジェクトメタデータ２２８において参照が存在しない第１のクローズドジャーナル２３０に格納された１つ以上のオブジェクトチャンクを識別する（１１７２）。いくつかの実現例では、ガーベッジコレクションアルゴリズムは、クローズドジャーナルの各々をコンパクト化する（１１７４）よう、周期的に実行される。当該コンパクションプロセスは、格納されたブロックを連続的なストレージへと統合し（１１７４）、これにより、ジャーナル２３０のサイズを低減する。
Since a stored object chunk can correspond to many (eg, hundreds, thousands or millions) of different objects, the object chunk in the journal is deleted only if there is no longer a reference to that object chunk. obtain. Thus, once the first
時間にわたって、より多くのオブジェクトチャンクが削除されると、ジャーナル２３０は小さくなり得る。多くの小さなジャーナルを管理することには、個々のオブジェクトを管理することと同様のオーバーヘッドが存在するので、ジャーナルの格納の利益は縮小される。この問題に対応するために、いくつかの実現例は、２つ以上のクローズドジャーナル同士を結合して（１１７６）単一の置換ジャーナルを形成し、当該２つ以上のジャーナルに以前に格納されたオブジェクトチャンクは置換ジャーナルに格納されているということを示すようオブジェクトメタデータ２２８を更新する（１１７６）。結合オペレーションは、完全に新しいジャーナルを形成することと、関連するオブジェクトのすべてについてメタデータを更新することとを必要とするので、結合は通常、ジャーナルが相対的に小さくなったシナリオに限定される。
As more object chunks are deleted over time, the
図１２は、いくつかの実現例に従った、図１０Ｂに関して以前に示されるように分散ストレージシステムにチャンクを格納する例を示す。この例において、２つのチャンク２３８−１および２３８−２が示される。チャンク２３８−１は、チャンクＩＤ３４６−１の通常のチャンクである（すなわちスーパーチャンクでない）。チャンク２３８−１は、通常のチャンクであるので、チャンク／ブロックインデックス１０２４において直接的にルックアップされ得る。この図において、チャンク／ブロックインデックス１０２４は、データが格納されるジャーナル２３２のヘッダー７０２に格納される。このチャンク／ブロックについては、オフセット１０２８はｙ（１０２８−ｙ）である。このオフセットを使用すると、対応するブロックＢ１０１６−Ｂは、ストレージスペース７１４において発見され得る。
FIG. 12 illustrates an example of storing chunks in a distributed storage system as previously shown with respect to FIG. 10B, according to some implementations. In this example, two chunks 238-1 and 238-2 are shown. Chunk 238-1 is a normal chunk with chunk ID 346-1 (ie not a super chunk). Since chunk 238-1 is a normal chunk, it can be looked up directly in chunk /
しかしながら、チャンク２３８−２は（スーパー）チャンクＩＤ３４６−２を有するスーパーチャンクである。ここで示されるように、スーパーチャンク２３８−２は、ブロックリストテーブル１００６におけるエントリを指す。各スーパーチャンクＩＤ１０２２について、複数の対応するブロックＩＤ１００８が存在する。図１２は、２つの対応するブロック１００８−１および１００８−２を示すが、非常に大きいオブジェクトの場合、単一のチャンクについて非常に多くのブロックが存在し得る。その後、ブロックＩＤ１００８−１および１００８−２は、ブロックについてオフセット１０２８−ｘおよび１０２８−ｚを発見するよう、チャンク／ブロックインデックス１０２４においてルックアップされる。最後に、オフセット１０２８−ｘおよび１０２８−ｚを使用して、対応するブロック１０１６−Ａおよび１０１６−Ｃはストレージスペース７１４に位置する。この例では、２つのブロックは連続しておらず、実際は、チャンク２３８−１についてのブロック１０１６−Ｂは、チャンク２３８−２についての２つのブロックを分離する。もちろん、各ブロックについての適切なデータのみが読み出されるように、各ブロックのサイズも使用される。これは、図１０Ｂに関して上で記載された。
However, chunk 238-2 is a super chunk with (super) chunk ID 346-2. As shown here, super chunk 238-2 refers to an entry in block list table 1006. For each
（チャンク２３８−１のような）通常のチャンクを許可しない実現例は完全に階層的である。また、チャンクとブロックとの間の割り当ては、実現例または他の動的なファクタに基づき変動する。たとえば、１００個のブロックを有する単一のチャンクまたは２５個のブロックを各々備えた４つのチャンクとして、同じオブジェクトが格納され得る。いくつかの実現例は、実際の使用からの経験的なフィードバックに基づいて、チャンクの数を変更する。 Implementations that do not allow regular chunks (such as chunk 238-1) are completely hierarchical. Also, the allocation between chunks and blocks varies based on implementations or other dynamic factors. For example, the same object can be stored as a single chunk with 100 blocks or as four chunks, each with 25 blocks. Some implementations change the number of chunks based on empirical feedback from actual usage.
説明の目的のためである上記の記載は、特定の実現例を参照して記載された。しかしながら、上記の例示的な議論は、網羅的であるように意図されておらず、または、本発明を開示されたそのままの形態に限定するよう意図されていない。上記の教示に鑑みて多くの修正例および変形例が可能である。上記の実現例は、本発明の原理およびその実際的な適用を最もよく説明するために選択および記載されたものであり、これにより他の当業者が、考慮される特定の使用に好適なさまざまな修正例とともに、本発明およびさまざまな実現例を最もよく利用するのが可能になる。 The above description, for illustrative purposes, has been described with reference to specific implementations. However, the above exemplary discussion is not intended to be exhaustive or intended to limit the invention to the precise forms disclosed. Many modifications and variations are possible in light of the above teaching. The above implementations have been chosen and described in order to best explain the principles of the invention and its practical application so that others skilled in the art will With this modification, the present invention and various implementations can be best utilized.
Claims (19)
１つ以上のプロセッサと、前記１つ以上のプロセッサによる実行のための１つ以上のプログラムを格納するメモリとを有する、前記分散ストレージシステムの第１のインスタンスにおいて、
第１の配置ポリシーに関連付けられる第１のオブジェクトを受け取ることを含み、前記第１の配置ポリシーは、前記第１のオブジェクトのレプリカが前記分散ストレージシステムにおいてどこに格納されるかについて基準を特定し、前記方法はさらに、
前記オブジェクトを複数のオブジェクトチャンクに分割し、前記複数のオブジェクトチャンクの第１のオブジェクトチャンクを複数のブロックへ分割することと、
関連付けられる配置ポリシーが前記第１の配置ポリシーにマッチする第１のジャーナルに前記複数のブロックを格納することと、
前記第１のオブジェクトについてグローバルメタデータを格納することとを含み、前記グローバルメタデータは前記複数のオブジェクトチャンクのリストを含み、前記リストは前記オブジェクトチャンクの各々についてそれぞれの識別子を含み、前記方法はさらに、
前記第１のオブジェクトチャンクについてローカルメタデータを格納することを含み、前記ローカルメタデータは、前記複数のブロックの各ブロックを識別するブロックリストを含み、前記ローカルメタデータは前記第１のジャーナルに関連付けられ、前記方法はさらに、
前記第１の配置ポリシーに従って前記第１のジャーナルを前記分散ストレージシステムの第２のインスタンスにレプリケートすることを含み、前記グローバルメタデータは当該レプリケーションを反映するために更新され、前記ローカルメタデータは前記レプリケーションによって変更されない、方法。 A method for managing the placement of object replicas in a distributed storage system, comprising:
In a first instance of the distributed storage system, comprising: one or more processors; and a memory storing one or more programs for execution by the one or more processors.
Receiving a first object associated with a first placement policy, wherein the first placement policy specifies criteria for where a replica of the first object is stored in the distributed storage system; The method further comprises:
Dividing the object into a plurality of object chunks, dividing a first object chunk of the plurality of object chunks into a plurality of blocks;
Storing the plurality of blocks in a first journal whose associated placement policy matches the first placement policy;
Storing global metadata for the first object, the global metadata including a list of the plurality of object chunks, the list including a respective identifier for each of the object chunks, the method comprising: further,
Storing local metadata for the first object chunk, wherein the local metadata includes a block list identifying each block of the plurality of blocks, the local metadata being associated with the first journal. And the method further comprises:
Replicating the first journal to a second instance of the distributed storage system in accordance with the first placement policy, the global metadata is updated to reflect the replication, and the local metadata is the The method that is not changed by replication.
第１のクローズドジャーナルのレプリカを選択することと、
前記グローバルメタデータに参照が存在しない前記レプリカに格納される１つ以上のオブジェクトチャンクを識別することと、
前記レプリカをコンパクト化し、これにより前記レプリカに格納される前記ブロックを連続するストレージに統合することとをさらに含む、請求項１〜３のいずれか１項に記載の方法。 In the first instance,
Selecting a replica of the first closed journal;
Identifying one or more object chunks stored in the replica for which no reference exists in the global metadata;
4. The method according to any one of claims 1 to 3, further comprising compacting the replica, thereby integrating the blocks stored in the replica into a continuous storage.
１つ以上のプロセッサと、前記１つ以上のプロセッサによる実行のための１つ以上のプログラムを格納するメモリとを有する、前記分散ストレージシステムの第１のインスタンスにおいて、
オブジェクトチャンクの格納のために１つ以上のジャーナルをオープンにすることを含み、各それぞれのジャーナルは単一のそれぞれの配置ポリシーに関連付けられ、前記方法はさらに、
少なくとも第１のオブジェクトチャンクを含む第１のオブジェクトを受け取ることを含み、前記第１のオブジェクトは第１の配置ポリシーに関連付けられ、前記第１のオブジェクトチャンクは第１の複数のブロックを含み、前記方法はさらに、
関連付けられる配置ポリシーが前記第１の配置ポリシーとマッチする第１のジャーナルに前記第１の複数のブロックを格納することを含み、前記第１のジャーナルは、配置ポリシーが前記第１の配置ポリシーとマッチするオブジェクトについてのブロックのみを格納し、前記方法はさらに、
前記第１のオブジェクトについてのグローバルメタデータを格納することを含み、前記グローバルメタデータは、前記第１のオブジェクトに対応するオブジェクトチャンクの第１のリストを含み、前記第１のリストは前記第１のオブジェクトチャンクの識別子を含み、前記方法はさらに、
前記第１のオブジェクトチャンクについてローカルメタデータを格納することを含み、前記ローカルメタデータは、前記第１の複数のブロックの各ブロックを識別するブロックリストを含み、前記ローカルメタデータは前記第１のジャーナルに関連付けられ、前記方法はさらに、
前記第１のジャーナルについて、関連付けられる配置ポリシーが前記第１の配置ポリシーとマッチする第１の複数のオブジェクトについての前記受け取るステップおよび格納するステップを、第１の終了条件が発生するまで繰り返すことと、
前記第１の終了条件が発生した後、前記第１のジャーナルをクローズし、これにより、如何なる付加的なブロックも前記第１のジャーナルに格納されるのを防止することと、
前記第１の配置ポリシーに従って前記分散ストレージシステムの第２のインスタンスへ前記第１のジャーナルをレプリケートすることとを含み、前記グローバルメタデータは当該レプリケーションを反映するよう更新され、前記ローカルメタデータは前記レプリケーションによって変更されない、方法。 A method for managing the placement of object replicas in a distributed storage system, comprising:
In a first instance of the distributed storage system, comprising: one or more processors; and a memory storing one or more programs for execution by the one or more processors.
Opening one or more journals for storing object chunks, each respective journal being associated with a single respective placement policy, the method further comprising:
Receiving a first object including at least a first object chunk, wherein the first object is associated with a first placement policy, the first object chunk includes a first plurality of blocks; The method is further
Storing the first plurality of blocks in a first journal whose associated placement policy matches the first placement policy, the first journal having a placement policy with the first placement policy. Store only blocks for matching objects, and the method further includes:
Storing global metadata about the first object, the global metadata including a first list of object chunks corresponding to the first object, wherein the first list is the first list. The identifier of the object chunk, the method further comprising:
Storing local metadata for the first object chunk, wherein the local metadata includes a block list identifying each block of the first plurality of blocks, and the local metadata is the first metadata Associated with a journal, the method further comprising:
Repeating the receiving and storing steps for the first plurality of objects for which the associated placement policy matches the first placement policy for the first journal until a first termination condition occurs; ,
After the first termination condition has occurred, closing the first journal, thereby preventing any additional blocks from being stored in the first journal;
Replicating the first journal to a second instance of the distributed storage system according to the first placement policy, the global metadata is updated to reflect the replication, and the local metadata is the The method that is not changed by replication.
第１のクローズドジャーナルのレプリカを選択することと、
前記グローバルメタデータに参照が存在しない前記レプリカに格納される１つ以上のオブジェクトチャンクを識別することと、
前記レプリカをコンパクト化し、これにより前記レプリカに格納される前記ブロックを連続するストレージに統合することとをさらに含む、請求項７〜１２のいずれか１項に記載の方法。 In the first instance,
Selecting a replica of the first closed journal;
Identifying one or more object chunks stored in the replica for which no reference exists in the global metadata;
13. The method according to any one of claims 7 to 12 , further comprising compacting the replica, thereby consolidating the blocks stored in the replica into contiguous storage.
１つ以上のプロセッサと、
メモリと、
前記メモリに格納される１つ以上のプログラムとを含み、前記１つ以上のプログラムは、請求項１〜１７のいずれか１項に記載の方法を実行するために前記１つ以上のプロセッサによって実行可能である命令を含む、コンピュータシステム。 A computer system for managing the placement of object replicas in a distributed storage system having multiple instances, wherein each respective instance
One or more processors;
Memory,
Includes one or more programs stored in the memory, the one or more programs, executed by the one or more processors to perform the method according to any one of claims 1 to 17 A computer system containing instructions that are possible.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/142,706 | 2013-12-27 | ||
US14/142,706 US9158472B2 (en) | 2013-06-25 | 2013-12-27 | Hierarchical chunking of objects in a distributed storage system |
PCT/US2014/072356 WO2015100416A1 (en) | 2013-12-27 | 2014-12-24 | Hierarchical chunking of objects in a distributed storage system |
Publications (3)
Publication Number | Publication Date |
---|---|
JP2017500670A JP2017500670A (en) | 2017-01-05 |
JP2017500670A5 JP2017500670A5 (en) | 2018-02-08 |
JP6479020B2 true JP6479020B2 (en) | 2019-03-06 |
Family
ID=53479702
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2016543054A Active JP6479020B2 (en) | 2013-12-27 | 2014-12-24 | Hierarchical chunking of objects in a distributed storage system |
Country Status (8)
Country | Link |
---|---|
US (2) | US9158472B2 (en) |
EP (1) | EP3087513B1 (en) |
JP (1) | JP6479020B2 (en) |
CN (1) | CN105940396B (en) |
AU (1) | AU2014369830B2 (en) |
CA (1) | CA2935215C (en) |
DE (1) | DE202014010898U1 (en) |
WO (1) | WO2015100416A1 (en) |
Families Citing this family (167)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8589640B2 (en) | 2011-10-14 | 2013-11-19 | Pure Storage, Inc. | Method for maintaining multiple fingerprint tables in a deduplicating storage system |
US9158472B2 (en) * | 2013-06-25 | 2015-10-13 | Google Inc. | Hierarchical chunking of objects in a distributed storage system |
US11960371B2 (en) | 2014-06-04 | 2024-04-16 | Pure Storage, Inc. | Message persistence in a zoned system |
US9003144B1 (en) | 2014-06-04 | 2015-04-07 | Pure Storage, Inc. | Mechanism for persisting messages in a storage system |
US11068363B1 (en) | 2014-06-04 | 2021-07-20 | Pure Storage, Inc. | Proactively rebuilding data in a storage cluster |
US9367243B1 (en) | 2014-06-04 | 2016-06-14 | Pure Storage, Inc. | Scalable non-uniform storage sizes |
US9836234B2 (en) | 2014-06-04 | 2017-12-05 | Pure Storage, Inc. | Storage cluster |
US9218244B1 (en) | 2014-06-04 | 2015-12-22 | Pure Storage, Inc. | Rebuilding data across storage nodes |
US10574754B1 (en) | 2014-06-04 | 2020-02-25 | Pure Storage, Inc. | Multi-chassis array with multi-level load balancing |
US11652884B2 (en) | 2014-06-04 | 2023-05-16 | Pure Storage, Inc. | Customized hash algorithms |
US9613078B2 (en) | 2014-06-26 | 2017-04-04 | Amazon Technologies, Inc. | Multi-database log with multi-item transaction support |
US11886308B2 (en) | 2014-07-02 | 2024-01-30 | Pure Storage, Inc. | Dual class of service for unified file and object messaging |
US9021297B1 (en) | 2014-07-02 | 2015-04-28 | Pure Storage, Inc. | Redundant, fault-tolerant, distributed remote procedure call cache in a storage system |
US11604598B2 (en) | 2014-07-02 | 2023-03-14 | Pure Storage, Inc. | Storage cluster with zoned drives |
US8868825B1 (en) | 2014-07-02 | 2014-10-21 | Pure Storage, Inc. | Nonrepeating identifiers in an address space of a non-volatile solid-state storage |
US9836245B2 (en) | 2014-07-02 | 2017-12-05 | Pure Storage, Inc. | Non-volatile RAM and flash memory in a non-volatile solid-state storage |
US9747229B1 (en) | 2014-07-03 | 2017-08-29 | Pure Storage, Inc. | Self-describing data format for DMA in a non-volatile solid-state storage |
US9811677B2 (en) | 2014-07-03 | 2017-11-07 | Pure Storage, Inc. | Secure data replication in a storage grid |
US10853311B1 (en) | 2014-07-03 | 2020-12-01 | Pure Storage, Inc. | Administration through files in a storage system |
US9483346B2 (en) | 2014-08-07 | 2016-11-01 | Pure Storage, Inc. | Data rebuild on feedback from a queue in a non-volatile solid-state storage |
US9082512B1 (en) | 2014-08-07 | 2015-07-14 | Pure Storage, Inc. | Die-level monitoring in a storage cluster |
US9495255B2 (en) | 2014-08-07 | 2016-11-15 | Pure Storage, Inc. | Error recovery in a storage cluster |
US10983859B2 (en) | 2014-08-07 | 2021-04-20 | Pure Storage, Inc. | Adjustable error correction based on memory health in a storage unit |
US10079711B1 (en) | 2014-08-20 | 2018-09-18 | Pure Storage, Inc. | Virtual file server with preserved MAC address |
US10025802B2 (en) | 2014-09-19 | 2018-07-17 | Amazon Technologies, Inc. | Automated configuration of log-coordinated storage groups |
US9799017B1 (en) | 2014-09-19 | 2017-10-24 | Amazon Technologies, Inc. | Cross-data-store operations in log-coordinated storage systems |
US10303796B2 (en) * | 2015-01-09 | 2019-05-28 | Ariba, Inc. | Updating distributed shards without compromising on consistency |
US9904722B1 (en) | 2015-03-13 | 2018-02-27 | Amazon Technologies, Inc. | Log-based distributed transaction management |
US9940234B2 (en) | 2015-03-26 | 2018-04-10 | Pure Storage, Inc. | Aggressive data deduplication using lazy garbage collection |
US9916458B2 (en) * | 2015-03-31 | 2018-03-13 | EMC IP Holding Company LLC | Secure cloud-based storage of data shared across file system objects and clients |
US10191914B2 (en) | 2015-03-31 | 2019-01-29 | EMC IP Holding Company LLC | De-duplicating distributed file system using cloud-based object store |
US10178169B2 (en) | 2015-04-09 | 2019-01-08 | Pure Storage, Inc. | Point to point based backend communication layer for storage processing |
US9672125B2 (en) | 2015-04-10 | 2017-06-06 | Pure Storage, Inc. | Ability to partition an array into two or more logical arrays with independently running software |
US10846275B2 (en) | 2015-06-26 | 2020-11-24 | Pure Storage, Inc. | Key management in a storage device |
US11609890B1 (en) | 2015-06-29 | 2023-03-21 | Amazon Technologies, Inc. | Schema management for journal-based storage systems |
US10866968B1 (en) | 2015-06-29 | 2020-12-15 | Amazon Technologies, Inc. | Compact snapshots of journal-based storage systems |
US11599520B1 (en) | 2015-06-29 | 2023-03-07 | Amazon Technologies, Inc. | Consistency management using query restrictions in journal-based storage systems |
US10866865B1 (en) | 2015-06-29 | 2020-12-15 | Amazon Technologies, Inc. | Storage system journal entry redaction |
US10983732B2 (en) | 2015-07-13 | 2021-04-20 | Pure Storage, Inc. | Method and system for accessing a file |
US10031935B1 (en) * | 2015-08-21 | 2018-07-24 | Amazon Technologies, Inc. | Customer-requested partitioning of journal-based storage systems |
US10108355B2 (en) | 2015-09-01 | 2018-10-23 | Pure Storage, Inc. | Erase block state detection |
US11341136B2 (en) | 2015-09-04 | 2022-05-24 | Pure Storage, Inc. | Dynamically resizable structures for approximate membership queries |
US10229142B2 (en) * | 2015-09-14 | 2019-03-12 | International Business Machines Corporation | Method and system for handling binary large objects |
US9768953B2 (en) | 2015-09-30 | 2017-09-19 | Pure Storage, Inc. | Resharing of a split secret |
US10853266B2 (en) | 2015-09-30 | 2020-12-01 | Pure Storage, Inc. | Hardware assisted data lookup methods |
US10762069B2 (en) | 2015-09-30 | 2020-09-01 | Pure Storage, Inc. | Mechanism for a system where data and metadata are located closely together |
US9843453B2 (en) | 2015-10-23 | 2017-12-12 | Pure Storage, Inc. | Authorizing I/O commands with I/O tokens |
US10007457B2 (en) | 2015-12-22 | 2018-06-26 | Pure Storage, Inc. | Distributed transactions with token-associated execution |
US9971822B1 (en) | 2015-12-29 | 2018-05-15 | Amazon Technologies, Inc. | Replicated state management using journal-based registers |
CN105677250B (en) | 2016-01-04 | 2019-07-12 | 北京百度网讯科技有限公司 | The update method and updating device of object data in object storage system |
US10261690B1 (en) | 2016-05-03 | 2019-04-16 | Pure Storage, Inc. | Systems and methods for operating a storage system |
US10838620B2 (en) | 2016-05-26 | 2020-11-17 | Nutanix, Inc. | Efficient scaling of distributed storage systems |
CN107622067B (en) * | 2016-07-13 | 2020-11-20 | 杭州海康威视数字技术股份有限公司 | Method and device for storing, reading and displaying multiple multimedia files |
US11861188B2 (en) | 2016-07-19 | 2024-01-02 | Pure Storage, Inc. | System having modular accelerators |
US10768819B2 (en) | 2016-07-22 | 2020-09-08 | Pure Storage, Inc. | Hardware support for non-disruptive upgrades |
US9672905B1 (en) | 2016-07-22 | 2017-06-06 | Pure Storage, Inc. | Optimize data protection layouts based on distributed flash wear leveling |
US11604690B2 (en) | 2016-07-24 | 2023-03-14 | Pure Storage, Inc. | Online failure span determination |
US10366004B2 (en) | 2016-07-26 | 2019-07-30 | Pure Storage, Inc. | Storage system with elective garbage collection to reduce flash contention |
US11886334B2 (en) | 2016-07-26 | 2024-01-30 | Pure Storage, Inc. | Optimizing spool and memory space management |
US11797212B2 (en) | 2016-07-26 | 2023-10-24 | Pure Storage, Inc. | Data migration for zoned drives |
US10203903B2 (en) | 2016-07-26 | 2019-02-12 | Pure Storage, Inc. | Geometry based, space aware shelf/writegroup evacuation |
US11734169B2 (en) | 2016-07-26 | 2023-08-22 | Pure Storage, Inc. | Optimizing spool and memory space management |
US11422719B2 (en) | 2016-09-15 | 2022-08-23 | Pure Storage, Inc. | Distributed file deletion and truncation |
US9747039B1 (en) | 2016-10-04 | 2017-08-29 | Pure Storage, Inc. | Reservations over multiple paths on NVMe over fabrics |
US10866945B2 (en) | 2016-10-10 | 2020-12-15 | AlphaPoint | User account management via a distributed ledger |
US11550481B2 (en) | 2016-12-19 | 2023-01-10 | Pure Storage, Inc. | Efficiently writing data in a zoned drive storage system |
US10209901B2 (en) | 2017-01-04 | 2019-02-19 | Walmart Apollo, Llc | Systems and methods for distributive data storage |
US11307998B2 (en) | 2017-01-09 | 2022-04-19 | Pure Storage, Inc. | Storage efficiency of encrypted host system data |
US9747158B1 (en) | 2017-01-13 | 2017-08-29 | Pure Storage, Inc. | Intelligent refresh of 3D NAND |
US11955187B2 (en) | 2017-01-13 | 2024-04-09 | Pure Storage, Inc. | Refresh of differing capacity NAND |
US10481988B2 (en) * | 2017-01-24 | 2019-11-19 | Zerto Ltd. | System and method for consistency verification of replicated data in a recovery system |
US10552341B2 (en) * | 2017-02-17 | 2020-02-04 | International Business Machines Corporation | Zone storage—quickly returning to a state of consistency following an unexpected event |
CA3052832C (en) * | 2017-02-27 | 2021-11-16 | Timescale, Inc. | Scalable database system for querying time-series data |
US11360942B2 (en) | 2017-03-13 | 2022-06-14 | Wandisco Inc. | Methods, devices and systems for maintaining consistency of metadata and data across data centers |
US10528488B1 (en) | 2017-03-30 | 2020-01-07 | Pure Storage, Inc. | Efficient name coding |
US11016667B1 (en) | 2017-04-05 | 2021-05-25 | Pure Storage, Inc. | Efficient mapping for LUNs in storage memory with holes in address space |
US10141050B1 (en) | 2017-04-27 | 2018-11-27 | Pure Storage, Inc. | Page writes for triple level cell flash memory |
US10516645B1 (en) | 2017-04-27 | 2019-12-24 | Pure Storage, Inc. | Address resolution broadcasting in a networked device |
US10701154B2 (en) | 2017-05-22 | 2020-06-30 | Microsoft Technology Licensing, Llc | Sharding over multi-link data channels |
US11782625B2 (en) | 2017-06-11 | 2023-10-10 | Pure Storage, Inc. | Heterogeneity supportive resiliency groups |
US10425473B1 (en) | 2017-07-03 | 2019-09-24 | Pure Storage, Inc. | Stateful connection reset in a storage cluster with a stateless load balancer |
US11016937B2 (en) * | 2017-07-17 | 2021-05-25 | Microsoft Technology Licensing, Llc | Updateable distributed file framework |
US10761743B1 (en) | 2017-07-17 | 2020-09-01 | EMC IP Holding Company LLC | Establishing data reliability groups within a geographically distributed data storage environment |
US10817388B1 (en) | 2017-07-21 | 2020-10-27 | EMC IP Holding Company LLC | Recovery of tree data in a geographically distributed environment |
US10402266B1 (en) | 2017-07-31 | 2019-09-03 | Pure Storage, Inc. | Redundant array of independent disks in a direct-mapped flash storage system |
US11210211B2 (en) | 2017-08-21 | 2021-12-28 | Western Digital Technologies, Inc. | Key data store garbage collection and multipart object management |
US11055266B2 (en) | 2017-08-21 | 2021-07-06 | Western Digital Technologies, Inc. | Efficient key data store entry traversal and result generation |
US11210212B2 (en) | 2017-08-21 | 2021-12-28 | Western Digital Technologies, Inc. | Conflict resolution and garbage collection in distributed databases |
US10824612B2 (en) | 2017-08-21 | 2020-11-03 | Western Digital Technologies, Inc. | Key ticketing system with lock-free concurrency and versioning |
US10880040B1 (en) | 2017-10-23 | 2020-12-29 | EMC IP Holding Company LLC | Scale-out distributed erasure coding |
US10572191B1 (en) | 2017-10-24 | 2020-02-25 | EMC IP Holding Company LLC | Disaster recovery with distributed erasure coding |
US10496330B1 (en) | 2017-10-31 | 2019-12-03 | Pure Storage, Inc. | Using flash storage devices with different sized erase blocks |
US10545687B1 (en) | 2017-10-31 | 2020-01-28 | Pure Storage, Inc. | Data rebuild when changing erase block sizes during drive replacement |
US10884919B2 (en) * | 2017-10-31 | 2021-01-05 | Pure Storage, Inc. | Memory management in a storage system |
US10860475B1 (en) | 2017-11-17 | 2020-12-08 | Pure Storage, Inc. | Hybrid flash translation layer |
US10740306B1 (en) * | 2017-12-04 | 2020-08-11 | Amazon Technologies, Inc. | Large object partitioning system |
WO2019122971A1 (en) * | 2017-12-20 | 2019-06-27 | Telefonaktiebolaget Lm Ericsson (Publ) | Datafall: a policy-driven algorithm for decentralized placement and reorganization of replicated data |
US10382554B1 (en) * | 2018-01-04 | 2019-08-13 | Emc Corporation | Handling deletes with distributed erasure coding |
CN110058790B (en) * | 2018-01-18 | 2022-05-13 | 伊姆西Ip控股有限责任公司 | Method, apparatus and computer program product for storing data |
US10467527B1 (en) | 2018-01-31 | 2019-11-05 | Pure Storage, Inc. | Method and apparatus for artificial intelligence acceleration |
US10976948B1 (en) | 2018-01-31 | 2021-04-13 | Pure Storage, Inc. | Cluster expansion mechanism |
US11036596B1 (en) | 2018-02-18 | 2021-06-15 | Pure Storage, Inc. | System for delaying acknowledgements on open NAND locations until durability has been confirmed |
US10817374B2 (en) | 2018-04-12 | 2020-10-27 | EMC IP Holding Company LLC | Meta chunks |
US11385792B2 (en) | 2018-04-27 | 2022-07-12 | Pure Storage, Inc. | High availability controller pair transitioning |
US10579297B2 (en) | 2018-04-27 | 2020-03-03 | EMC IP Holding Company LLC | Scaling-in for geographically diverse storage |
US11023130B2 (en) | 2018-06-15 | 2021-06-01 | EMC IP Holding Company LLC | Deleting data in a geographically diverse storage construct |
US10594340B2 (en) | 2018-06-15 | 2020-03-17 | EMC IP Holding Company LLC | Disaster recovery with consolidated erasure coding in geographically distributed setups |
US10936196B2 (en) | 2018-06-15 | 2021-03-02 | EMC IP Holding Company LLC | Data convolution for geographically diverse storage |
US10885054B2 (en) * | 2018-07-03 | 2021-01-05 | Mastercard International Incorporated | Tracking metadata changes in multiple data stores and generating alerts for detected data impacts |
US11354058B2 (en) | 2018-09-06 | 2022-06-07 | Pure Storage, Inc. | Local relocation of data stored at a storage device of a storage system |
US11868309B2 (en) | 2018-09-06 | 2024-01-09 | Pure Storage, Inc. | Queue management for data relocation |
US11500570B2 (en) | 2018-09-06 | 2022-11-15 | Pure Storage, Inc. | Efficient relocation of data utilizing different programming modes |
US10922142B2 (en) | 2018-10-31 | 2021-02-16 | Nutanix, Inc. | Multi-stage IOPS allocation |
US11436203B2 (en) * | 2018-11-02 | 2022-09-06 | EMC IP Holding Company LLC | Scaling out geographically diverse storage |
US10691378B1 (en) * | 2018-11-30 | 2020-06-23 | International Business Machines Corporation | Data replication priority management |
US10901635B2 (en) | 2018-12-04 | 2021-01-26 | EMC IP Holding Company LLC | Mapped redundant array of independent nodes for data storage with high performance using logical columns of the nodes with different widths and different positioning patterns |
US11119683B2 (en) | 2018-12-20 | 2021-09-14 | EMC IP Holding Company LLC | Logical compaction of a degraded chunk in a geographically diverse data storage system |
US10931777B2 (en) | 2018-12-20 | 2021-02-23 | EMC IP Holding Company LLC | Network efficient geographically diverse data storage system employing degraded chunks |
US10892782B2 (en) | 2018-12-21 | 2021-01-12 | EMC IP Holding Company LLC | Flexible system and method for combining erasure-coded protection sets |
US11023331B2 (en) | 2019-01-04 | 2021-06-01 | EMC IP Holding Company LLC | Fast recovery of data in a geographically distributed storage environment |
US10942827B2 (en) | 2019-01-22 | 2021-03-09 | EMC IP Holding Company LLC | Replication of data in a geographically distributed storage environment |
CN109885539A (en) * | 2019-01-28 | 2019-06-14 | 南京邮电大学 | A kind of log file management method |
US10942825B2 (en) | 2019-01-29 | 2021-03-09 | EMC IP Holding Company LLC | Mitigating real node failure in a mapped redundant array of independent nodes |
US10866766B2 (en) | 2019-01-29 | 2020-12-15 | EMC IP Holding Company LLC | Affinity sensitive data convolution for data storage systems |
US10936239B2 (en) | 2019-01-29 | 2021-03-02 | EMC IP Holding Company LLC | Cluster contraction of a mapped redundant array of independent nodes |
US10846003B2 (en) | 2019-01-29 | 2020-11-24 | EMC IP Holding Company LLC | Doubly mapped redundant array of independent nodes for data storage |
US11029865B2 (en) | 2019-04-03 | 2021-06-08 | EMC IP Holding Company LLC | Affinity sensitive storage of data corresponding to a mapped redundant array of independent nodes |
US10944826B2 (en) | 2019-04-03 | 2021-03-09 | EMC IP Holding Company LLC | Selective instantiation of a storage service for a mapped redundant array of independent nodes |
US11099986B2 (en) | 2019-04-12 | 2021-08-24 | Pure Storage, Inc. | Efficient transfer of memory contents |
US11113146B2 (en) | 2019-04-30 | 2021-09-07 | EMC IP Holding Company LLC | Chunk segment recovery via hierarchical erasure coding in a geographically diverse data storage system |
US11119686B2 (en) | 2019-04-30 | 2021-09-14 | EMC IP Holding Company LLC | Preservation of data during scaling of a geographically diverse data storage system |
US11121727B2 (en) | 2019-04-30 | 2021-09-14 | EMC IP Holding Company LLC | Adaptive data storing for data storage systems employing erasure coding |
US11748004B2 (en) | 2019-05-03 | 2023-09-05 | EMC IP Holding Company LLC | Data replication using active and passive data storage modes |
KR20220140639A (en) * | 2019-05-22 | 2022-10-18 | 묘타, 인크. | Method and system for distributed data storage with enhanced security, resilience, and control |
US11281394B2 (en) | 2019-06-24 | 2022-03-22 | Pure Storage, Inc. | Replication across partitioning schemes in a distributed storage system |
US11209996B2 (en) | 2019-07-15 | 2021-12-28 | EMC IP Holding Company LLC | Mapped cluster stretching for increasing workload in a data storage system |
US11449399B2 (en) | 2019-07-30 | 2022-09-20 | EMC IP Holding Company LLC | Mitigating real node failure of a doubly mapped redundant array of independent nodes |
US11023145B2 (en) | 2019-07-30 | 2021-06-01 | EMC IP Holding Company LLC | Hybrid mapped clusters for data storage |
US11228322B2 (en) | 2019-09-13 | 2022-01-18 | EMC IP Holding Company LLC | Rebalancing in a geographically diverse storage system employing erasure coding |
US11449248B2 (en) | 2019-09-26 | 2022-09-20 | EMC IP Holding Company LLC | Mapped redundant array of independent data storage regions |
US11893126B2 (en) | 2019-10-14 | 2024-02-06 | Pure Storage, Inc. | Data deletion for a multi-tenant environment |
US11288139B2 (en) | 2019-10-31 | 2022-03-29 | EMC IP Holding Company LLC | Two-step recovery employing erasure coding in a geographically diverse data storage system |
US11435910B2 (en) | 2019-10-31 | 2022-09-06 | EMC IP Holding Company LLC | Heterogeneous mapped redundant array of independent nodes for data storage |
US11119690B2 (en) | 2019-10-31 | 2021-09-14 | EMC IP Holding Company LLC | Consolidation of protection sets in a geographically diverse data storage environment |
US11435957B2 (en) | 2019-11-27 | 2022-09-06 | EMC IP Holding Company LLC | Selective instantiation of a storage service for a doubly mapped redundant array of independent nodes |
US11847331B2 (en) | 2019-12-12 | 2023-12-19 | Pure Storage, Inc. | Budgeting open blocks of a storage unit based on power loss prevention |
US11416144B2 (en) | 2019-12-12 | 2022-08-16 | Pure Storage, Inc. | Dynamic use of segment or zone power loss protection in a flash device |
US11704192B2 (en) | 2019-12-12 | 2023-07-18 | Pure Storage, Inc. | Budgeting open blocks based on power loss protection |
US11144220B2 (en) | 2019-12-24 | 2021-10-12 | EMC IP Holding Company LLC | Affinity sensitive storage of data corresponding to a doubly mapped redundant array of independent nodes |
US11231860B2 (en) | 2020-01-17 | 2022-01-25 | EMC IP Holding Company LLC | Doubly mapped redundant array of independent nodes for data storage with high performance |
US11188432B2 (en) | 2020-02-28 | 2021-11-30 | Pure Storage, Inc. | Data resiliency by partially deallocating data blocks of a storage device |
US11507308B2 (en) | 2020-03-30 | 2022-11-22 | EMC IP Holding Company LLC | Disk access event control for mapped nodes supported by a real cluster storage system |
US11474986B2 (en) | 2020-04-24 | 2022-10-18 | Pure Storage, Inc. | Utilizing machine learning to streamline telemetry processing of storage media |
US11288229B2 (en) | 2020-05-29 | 2022-03-29 | EMC IP Holding Company LLC | Verifiable intra-cluster migration for a chunk storage system |
US11868407B2 (en) * | 2020-09-24 | 2024-01-09 | Dell Products L.P. | Multi-level data structure comparison using commutative digesting for unordered data collections |
US11693983B2 (en) | 2020-10-28 | 2023-07-04 | EMC IP Holding Company LLC | Data protection via commutative erasure coding in a geographically diverse data storage system |
US11620214B2 (en) * | 2020-10-30 | 2023-04-04 | Nutanix, Inc. | Transactional allocation and deallocation of blocks in a block store |
US11487455B2 (en) | 2020-12-17 | 2022-11-01 | Pure Storage, Inc. | Dynamic block allocation to optimize storage system performance |
US11847324B2 (en) | 2020-12-31 | 2023-12-19 | Pure Storage, Inc. | Optimizing resiliency groups for data regions of a storage system |
US11614880B2 (en) | 2020-12-31 | 2023-03-28 | Pure Storage, Inc. | Storage system with selectable write paths |
US11847141B2 (en) | 2021-01-19 | 2023-12-19 | EMC IP Holding Company LLC | Mapped redundant array of independent nodes employing mapped reliability groups for data storage |
US11625174B2 (en) | 2021-01-20 | 2023-04-11 | EMC IP Holding Company LLC | Parity allocation for a virtual redundant array of independent disks |
US11507597B2 (en) | 2021-03-31 | 2022-11-22 | Pure Storage, Inc. | Data replication to meet a recovery point objective |
US11354191B1 (en) | 2021-05-28 | 2022-06-07 | EMC IP Holding Company LLC | Erasure coding in a large geographically diverse data storage system |
US11449234B1 (en) | 2021-05-28 | 2022-09-20 | EMC IP Holding Company LLC | Efficient data access operations via a mapping layer instance for a doubly mapped redundant array of independent nodes |
WO2023147842A1 (en) * | 2022-02-01 | 2023-08-10 | Huawei Technologies Co., Ltd. | Data storage and methods of deduplication of data storage, storing new file and deleting file |
CN114567503A (en) * | 2022-03-04 | 2022-05-31 | 南京联成科技发展股份有限公司 | Encryption method for centrally controlled and trusted data acquisition |
Family Cites Families (31)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7085789B1 (en) | 2001-07-30 | 2006-08-01 | Microsoft Corporation | Compact garbage collection tables |
US7574699B1 (en) | 2003-03-19 | 2009-08-11 | Sun Microsystems, Inc. | Compact type format data system and method |
US8180742B2 (en) | 2004-07-01 | 2012-05-15 | Emc Corporation | Policy-based information management |
US7778984B2 (en) | 2004-11-19 | 2010-08-17 | Microsoft Corporation | System and method for a distributed object store |
US7464247B2 (en) | 2005-12-19 | 2008-12-09 | Yahoo! Inc. | System and method for updating data in a distributed column chunk data store |
US8589574B1 (en) | 2005-12-29 | 2013-11-19 | Amazon Technologies, Inc. | Dynamic application instance discovery and state management within a distributed system |
US7856437B2 (en) | 2007-07-31 | 2010-12-21 | Hewlett-Packard Development Company, L.P. | Storing nodes representing respective chunks of files in a data store |
US8103628B2 (en) | 2008-04-09 | 2012-01-24 | Harmonic Inc. | Directed placement of data in a redundant data storage system |
WO2010075401A2 (en) | 2008-12-22 | 2010-07-01 | Google Inc. | Asynchronous distributed garbage collection for replicated storage clusters |
US8301671B1 (en) | 2009-01-08 | 2012-10-30 | Avaya Inc. | Method and apparatus providing removal of replicated objects based on garbage collection |
US8171202B2 (en) * | 2009-04-21 | 2012-05-01 | Google Inc. | Asynchronous distributed object uploading for replicated content addressable storage clusters |
US8560639B2 (en) | 2009-04-24 | 2013-10-15 | Microsoft Corporation | Dynamic placement of replica data |
US20100306171A1 (en) | 2009-06-02 | 2010-12-02 | Microsoft Corporation | Timeline Experience for Restore User Interface |
CN103038742B (en) * | 2010-02-09 | 2015-09-30 | 谷歌公司 | For the method and system of Dynamical data replication in distributed memory system |
US8886602B2 (en) | 2010-02-09 | 2014-11-11 | Google Inc. | Location assignment daemon (LAD) for a distributed storage system |
US8341118B2 (en) | 2010-02-09 | 2012-12-25 | Google Inc. | Method and system for dynamically replicating data within a distributed storage system |
US8868508B2 (en) | 2010-02-09 | 2014-10-21 | Google Inc. | Storage of data in a distributed storage system |
JP5569074B2 (en) * | 2010-03-19 | 2014-08-13 | 日本電気株式会社 | Storage system |
US8898798B2 (en) | 2010-09-01 | 2014-11-25 | Apixio, Inc. | Systems and methods for medical information analysis with deidentification and reidentification |
JP5639871B2 (en) * | 2010-12-13 | 2014-12-10 | 日清工業有限公司 | Electronic flash device |
US8380681B2 (en) | 2010-12-16 | 2013-02-19 | Microsoft Corporation | Extensible pipeline for data deduplication |
US8874515B2 (en) | 2011-04-11 | 2014-10-28 | Sandisk Enterprise Ip Llc | Low level object version tracking using non-volatile memory write generations |
EP2710510A4 (en) * | 2011-05-14 | 2015-05-06 | Bitcasa Inc | Cloud file system with server-side deduplication of user-agnostic encrypted files |
JP2013025742A (en) * | 2011-07-26 | 2013-02-04 | Nippon Telegr & Teleph Corp <Ntt> | Distributed file management device, distributed file management method and program |
WO2013074665A1 (en) | 2011-11-14 | 2013-05-23 | Google Inc. | Data processing service |
CA2877284A1 (en) | 2012-06-18 | 2013-12-27 | Actifio, Inc. | Enhanced data management virtualization system |
KR102050723B1 (en) | 2012-09-28 | 2019-12-02 | 삼성전자 주식회사 | Computing system and data management method thereof |
US20140201151A1 (en) | 2013-01-11 | 2014-07-17 | Commvault Systems, Inc. | Systems and methods to select files for restoration from block-level backup for virtual machines |
US8775485B1 (en) | 2013-03-15 | 2014-07-08 | Joyent, Inc. | Object store management operations within compute-centric object stores |
US9158472B2 (en) * | 2013-06-25 | 2015-10-13 | Google Inc. | Hierarchical chunking of objects in a distributed storage system |
US9600558B2 (en) | 2013-06-25 | 2017-03-21 | Google Inc. | Grouping of objects in a distributed storage system based on journals and placement policies |
-
2013
- 2013-12-27 US US14/142,706 patent/US9158472B2/en active Active
-
2014
- 2014-12-24 JP JP2016543054A patent/JP6479020B2/en active Active
- 2014-12-24 EP EP14874957.5A patent/EP3087513B1/en active Active
- 2014-12-24 WO PCT/US2014/072356 patent/WO2015100416A1/en active Application Filing
- 2014-12-24 DE DE202014010898.6U patent/DE202014010898U1/en active Active
- 2014-12-24 CN CN201480074231.5A patent/CN105940396B/en active Active
- 2014-12-24 AU AU2014369830A patent/AU2014369830B2/en active Active
- 2014-12-24 CA CA2935215A patent/CA2935215C/en active Active
-
2015
- 2015-10-13 US US14/882,205 patent/US9400828B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
AU2014369830B2 (en) | 2019-01-17 |
EP3087513A1 (en) | 2016-11-02 |
EP3087513B1 (en) | 2020-11-18 |
EP3087513A4 (en) | 2017-08-09 |
JP2017500670A (en) | 2017-01-05 |
DE202014010898U1 (en) | 2017-01-13 |
AU2014369830A1 (en) | 2016-07-14 |
US9158472B2 (en) | 2015-10-13 |
US9400828B2 (en) | 2016-07-26 |
US20160034549A1 (en) | 2016-02-04 |
WO2015100416A1 (en) | 2015-07-02 |
CN105940396B (en) | 2019-07-26 |
CA2935215C (en) | 2019-12-31 |
CN105940396A (en) | 2016-09-14 |
CA2935215A1 (en) | 2015-07-02 |
US20150186043A1 (en) | 2015-07-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6479020B2 (en) | Hierarchical chunking of objects in a distributed storage system | |
US9600558B2 (en) | Grouping of objects in a distributed storage system based on journals and placement policies | |
US10764045B2 (en) | Encrypting object index in a distributed storage environment | |
US7899850B2 (en) | Relational objects for the optimized management of fixed-content storage systems | |
US8838595B2 (en) | Operating on objects stored in a distributed database | |
US9396202B1 (en) | Weakly synchronized garbage collection and compaction for aggregated, replicated object stores | |
US7590672B2 (en) | Identification of fixed content objects in a distributed fixed content storage system | |
US9268806B1 (en) | Efficient reference counting in content addressable storage | |
US9305069B2 (en) | Method and system for uploading data into a distributed storage system | |
US10659225B2 (en) | Encrypting existing live unencrypted data using age-based garbage collection | |
TW202117529A (en) | Log-structured storage systems | |
TW202111520A (en) | Log-structured storage systems | |
KR20150061314A (en) | Method and System for recovery of iSCSI storage system used network distributed file system | |
Gutierrez et al. | uStorage-A Storage Architecture to Provide Block-Level Storage Through Object-Based Storage |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20171220 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20171220 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20181130 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20190108 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20190205 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6479020Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |