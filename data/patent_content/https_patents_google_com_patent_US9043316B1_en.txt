US9043316B1 - Visual content retrieval - Google Patents
Visual content retrieval Download PDFInfo
- Publication number
- US9043316B1 US9043316B1 US13/433,137 US201213433137A US9043316B1 US 9043316 B1 US9043316 B1 US 9043316B1 US 201213433137 A US201213433137 A US 201213433137A US 9043316 B1 US9043316 B1 US 9043316B1
- Authority
- US
- United States
- Prior art keywords
- image
- search results
- image search
- query
- images
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G06K9/46—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G06F17/30247—
Definitions
- This specification relates to information retrieval.
- Conventional information retrieval systems are used to identify a wide variety of resources, for example, images, audio files, web pages, or documents, e.g., news articles. Additionally, search results presented to a user that identify particular resources responsive to a query are typically ranked according to particular criteria.
- a representation of an image is a feature vector, where each element of the feature vector is a representation of a feature extracted from the image. All possible feature vectors for an image can define a feature space for the image.
- This specification describes technologies relating to image content retrieval.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving first image search results responsive to a text query, each first image search result associated with a respective first score indicating a relevance of an image represented by the first image search result to the text query; receiving second image search results responsive to a query image, each second image search result associated with a respective second score indicating a measure of similarity between an image represented by the second image search result and the query image, wherein the measure of similarity is calculated based in part on a distance between a content descriptor associated with the query image and a content descriptor associated with the image; selecting one or more of the first image search results that also occur in the second image search results and whose respective first scores satisfy a first threshold; selecting a set of final image search results including combining first scores and second scores of the selected first image search results; and ordering each of the final image search results by a score derived from a distance between the content descriptor associated with the query image and the content descriptor associated with the respective image
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- a content descriptor for an associated image is generated based at least in part on a feature representation of features extracted from the image.
- Generating the content descriptor comprises hashing a feature vector of the image with one or more hash functions to generate one or more hash values of the representation of features.
- the feature vector of the image is compressed into a feature representation using principal component analysis.
- the feature representation is compressed using delta encoding.
- the actions further include increasing the first score of one or more highest-ranked images in the first image search results.
- the actions further include removing duplicate and near-duplicate images, wherein near-duplicate images are identified by comparing content descriptors associated with respective images.
- the actions further include reordering each of the final image search results by a distance between a content descriptor associated with an image represented by a highest-ranked result in the set of final image search results and an associated content descriptor for the image represented by the set of final image search results.
- the actions further include reducing the first score of one or more of the first image search results whose first score is beyond a dynamic threshold.
- another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of deriving a set of visual keys by sorting training images into a plurality of image sets wherein each visual key represents one of the image sets and wherein the training images are sorted according to a respective feature representation of each training image; associating one or more of the visual keys with each image in a second set of images, wherein each visual key associated with each image corresponds to one of the image sets to which the image belongs; identifying one or more subsets of images in a collection of images, each subset associated with respective visual keys that match the one or more visual keys associated with the query image; and combining the identified subsets of images into a final set of images.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- the actions further include associating one or more visual keys with each image in the collection of images wherein each visual key associated with each image in the collection of images corresponds to one of the image sets to which the image belongs.
- the actions further include ordering the final set of images by a distance computed between each image in the final set and the query image, wherein the distance is based on respective content descriptors for each image in the final set and a content descriptor for the query image. Combining the identified subsets of images into a final set of images comprises generating a union or an intersection of the identified subsets to form the final set of images.
- the actions further include generating the visual keys by traversing a spill tree with a feature representation for each image.
- the actions further include training the spill tree with a plurality of training images, wherein training comprises generating a pair of decisional feature representations at each non-leaf node of the spill tree.
- the actions further include generating the decisional feature representations by averaging feature representations of training images at each non-leaf node of the spill tree.
- the actions further include ranking the visual keys by accumulated spill in the spill tree, wherein the accumulated spill is based on the difference between a distance between a feature representation of each image and each decisional feature representation in the spill tree.
- another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of deriving a set of visual keys by sorting training images into a plurality of image sets wherein each visual key represents one of the image sets and wherein the training images are sorted according to a respective feature representation of each training image; associating one or more of the visual keys with each image in a second set of images, wherein each visual key associated with each image corresponds to one of the image sets to which the image belongs; comparing a plurality of visual keys generated for a first image of the second set to a plurality of visual keys generated for a second image of the second set; and producing one or more sets of near-duplicate images, wherein if none of a number of visual keys match between the first image and the second image, the first image and second image are not included in a same set of near-duplicate images.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods
- the actions further include generating visual keys for images in the second set of images by traversing a spill tree with a feature representation for each image in the second set of images.
- the actions further include training the spill tree with a plurality of training images, whereby wherein training comprises generating a pair of decisional feature representations at each non-leaf node of the spill tree.
- the actions further include generating the decisional feature representations including averaging feature representations of training images at each non-leaf node of the spill tree.
- the actions further include ranking the visual keys by accumulated spill in the spill tree, wherein the accumulated spill is based on the difference between a distance between a feature representation of each image and each decisional feature representation in the spill tree. Comparing a plurality of visual keys generated for a first image of the second set to a plurality of visual keys generated for a second image of the second set comprises comparing a number of highest-ranked visual keys for each image. If one or more visual keys of the first image and the second image match, the actions further include determining that the first image and the second image are near-duplicates including computing a distance between feature representations associated with each respective image; and comparing the computed distance to a threshold.
- the actions further include adding each image search result to a category of near-duplicate images.
- the actions further include comparing respective feature representations associated with each of N highest-ranked images in the second set of images to one another; identifying near-duplicate images based on the comparison of respective feature representations; and adding near-duplicate images to categories of near-duplicate images.
- the actions further include comparing respective feature representations associated with each image ranked between N+1 and an upper bound M to respective feature representations associated with each of the N highest-ranked images in the second set of images, and adding near-duplicate images to categories of near-duplicate images.
- the actions further include training the spill tree with a plurality of training images, wherein training comprises generating a pair of decisional feature representations at each non-leaf node of the spill tree.
- the actions further include generating the decisional feature representations by averaging feature representations of training images at each non-leaf node of the spill tree.
- Another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving feature vectors u and v; generating a hash vector x, including applying multiple hash functions to u to compute multiple hash values, wherein elements of hash vector x are the hash values of u; generating a hash vector y, including applying multiple hash functions to v to compute multiple hash values, wherein elements of hash vector y are the hash values of v; computing an approximation A of the probability of a hash collision between u and v as the number of hash values that match between corresponding elements of the hash vector x and the hash vector y, divided by the length of the hash vector x or the hash vector y; and computing an approximation of an intersection kernel between u and v, wherein the approximation of the intersection kernel satisfies the equation
- IntK ⁇ ( u , v ) A ⁇ ( ⁇ u ⁇ 1 + ⁇ v ⁇ 1 ) 1 + A , wherein
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a representation r of the feature vector u is generated from a kernel PCA process, including using the hash vector x to compute a kernel vector comprising approximations of an intersection kernel between u and each vector in a plurality of reference vectors, wherein the feature vector v occurs in the plurality of reference vectors.
- the feature vector u corresponds to content of a data object.
- the content of the data object comprises image content.
- a measure of similarity is determined between content of the data object corresponding to u and content of a second data object corresponding to a second representation s including computing a distance between the representation r and the representation s.
- Another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a plurality of vectors; computing an approximation of an intersection kernel between each vector u in the plurality of vectors and each vector v in a set of reference vectors, including generating a hash vector x, including applying multiple hash functions to u to compute multiple hash values, wherein elements of hash vector x are the hash values of u, generating a hash vector y, including applying multiple hash functions to v to compute multiple hash values, wherein elements of hash vector y are the hash values of v, computing an approximation, A, of the probability of a hash collision between u and v as the number of hash values that match between corresponding elements of the hash vector x and the hash vector y, divided by the length of the hash vector x or the hash vector y, and computing an approximation of the intersection kernel between u and v, wherein the approximation of
- IntK ⁇ ( u , v ) A ⁇ ( ⁇ u ⁇ 1 + ⁇ v ⁇ 1 ) 1 + A , wherein
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- the actions further include generating a square matrix T using corresponding approximated intersection kernel values between vectors u and vectors v; and generating a projection matrix P for kernel PCA analysis using matrix T.
- Each vector corresponds to the content of an image.
- a respective representation r is generated for each vector u in the plurality of vectors using a kernel PCA process, including using a hash vector x comprising hash values of u to compute a kernel vector comprising approximations of an intersection kernel between u and each vector v in the set of reference vectors.
- the actions further include receiving a query image; computing a feature vector for the query image; generating a representation s for the query image using the kernel PCA process; computing a measure of similarity between the query image and each image corresponding to each vector u, including computing a distance between s and each respective representation r; and ranking images corresponding to the representations r using respective measures of similarity.
- another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a feature vector u, wherein the feature vector u corresponds to the content of a data object; generating a hash vector x, including applying multiple hash functions to u to compute multiple hash values, wherein elements of hash vector x are the hash values of u; generating a representation r of the feature vector u by a kernel PCA process, including using the hash vector x to compute a kernel vector, wherein the kernel vector comprises approximations of an intersection kernel between u and each vector in a plurality of reference vectors.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- the actions further include computing an approximation, A, of the probability of a hash collision between u and each vector v in the plurality of reference vectors as the number of hash values that match between corresponding elements of the hash vector x and a hash vector y comprising hash values of v, divided by the length of the hash vector x or the hash vector y, wherein an approximation of the intersection kernel between u and v satisfies the equation:
- IntK ⁇ ( u , v ) A ⁇ ( ⁇ u ⁇ 1 + ⁇ v ⁇ 1 ) 1 + A , wherein
- a measure of similarity is determined between content of a first data object corresponding to representation r and content of a second data object corresponding to a representation s, including computing a distance between representation r and representation s.
- the first data object and the second data object are images.
- Another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a vector corresponding to the content of an image; dividing the vector into blocks; identifying a vector maximum, the vector maximum being a maximum of the absolute values of all elements of the vector; dividing a range between 0 and the a global maximum into a plurality of first intervals; encoding the vector maximum as a value that identifies a particular first interval of the plurality of first intervals; identifying for each block a respective block maximum, wherein each block maximum is a maximum of the absolute values of all elements in each block; dividing a range between 0 and the vector maximum into a plurality of second intervals; encoding each block maximum B max as a value that identifies a particular second interval of the plurality of second intervals; dividing in each block a range between ⁇ B max and B max into a plurality of third intervals; encoding each vector element in a block as a value that identifies a particular third intervals;
- the actions further include multiplying the vector by a rotation matrix to normalize variances between elements of the vector.
- the vector corresponding to the content of an image is generated by a kernel PCA process.
- the actions further include decoding the image content descriptor into a decoded feature vector; and computing a measure of similarity between the image and another image, including computing a distance between the decoded feature vector and another feature vector corresponding to the other image.
- Decoding the image content descriptor into a feature vector includes identifying a first value in an interval corresponding to the encoded vector maximum; identifying a second value in an interval corresponding to each encoded block maximum; identifying a third value in an interval corresponding to each encoded vector element; computing a decoded vector maximum including multiplying the first value by the global maximum; computing respective decoded block maxima including multiplying each second value by the decoded vector maximum; computing respective decoded vector elements including multiplying each third value by each respective decoded block maximum; and generating the decoded feature vector using the decoded vector elements.
- the global maximum is a maximum value of all absolute values of all vector elements in a collection of vectors.
- Generating visual keys for images allows similar image content to be identified in constant time and instead of performing a comparison on every image in a collection.
- Combining visual keys with text queries allows users to perform a more-focused search where a text query alone might have multiple meanings or is ambiguous (e.g., “jaguar”).
- Generating visual keys can improve identification of near-duplicate images by speeding up the rejection of images that are not near-duplicates.
- Generating visual keys for images also allows integration with existing systems for text-based retrieval, eliminating the need for additional servers.
- Using visual keys for image retrieval can outperform offline similar image systems. A system using visual keys can search for unseen content in an existing index, whereas an offline system can only provide results for images known in advance.
- FIG. 1 illustrates the inputs and outputs of an example search system that accepts queries having a text portion and an image content portion.
- FIG. 2 is a diagram of an example search system.
- FIG. 3 is a flow chart of an example process for generating an image content descriptor.
- FIG. 4 is a flow chart of an example encoding process.
- FIG. 5 is a diagram of an example spill tree with a root node.
- FIG. 6 is a flow chart of an example process for generating visual keys.
- FIG. 7 is a flow chart of an example process for combining search results for query text and a query image.
- FIG. 8 is a flow chart of an example process for retrieving images by visual keys.
- FIG. 9 is a flow chart of an example process for identifying near-duplicate images.
- FIG. 10 is a flow chart of another example process for identifying near-duplicate images.
- Text queries alone may be inadequate in a variety of circumstances.
- a text query may have multiple meanings when a user is interested in only one specific meaning of a text query.
- image content retrieval by a search system can be improved when the system is configured to accept queries that include both a text portion and an image content portion.
- FIG. 1 illustrates the inputs and outputs of an example search system 110 that accepts queries having a text portion and an image content portion.
- a query 105 with text and images portions includes query text 130 and a query image 120 .
- a query with a text portion and an image portion can be used by the search system to identify images that are relevant to the query text 130 and which are also visually similar to the query image 120 .
- image search results 140 , 150 , and 160 are all images of birds, and they are also visually similar to the query image 120 .
- the search system can optionally order the image search results 140 , 150 , and 160 for presentation to a user by a measure of similarity to the query image 120 , or the search system can use the measure of similarity as an element in computing a ranking score for ranking the search results for presentation. Generally, higher ranking or more similar images are presented first, so as illustrated in FIG. 1 , image search result 140 is most similar to the query image 120 , followed by image search results 150 and 160 .
- the search system 110 can optionally omit images that are identical to query image 120 , or which are near-duplicates of query image 120 .
- FIG. 2 is a diagram of an example search system 210 .
- the search system 210 can be implemented on or more computers operating in one or more locations in an Internet, an intranet, or another client and server environment.
- the search system 210 can provide image search results 216 that satisfy query text 212 and also provide image search results identifying images that are determined by the system to be visually similar to a query image 214 .
- the search system 210 is an example of an information retrieval system in which the systems, components, and techniques described below can be implemented.
- the search system 210 can be implemented as, for example, one or more computer programs running on one or more computers in one or more geographic locations. The computers can exchange information through one or more networks.
- the search system 210 includes an image search engine 230 .
- the search system 210 responds to the query text 212 and query image 214 by generating image search results 216 , which are transmitted through the network to the user device 204 in a form that can be presented to the user 202 , e.g., as an interactive web page to be displayed in a web browser running on user device 204 .
- a user 202 can interact with the search system 210 through a user device 204 .
- the user device 204 can be a data processing apparatus such as a personal computer, a laptop computer, a tablet computer, a smart phone, or a personal digital assistant.
- the user device 204 will generally include a random access memory (RAM) 206 and a processor 208 .
- RAM random access memory
- the user device 204 can communicate with other devices through one or more local area networks (LANs) or wide area networks (WANs) 201 .
- LANs local area networks
- WANs wide area networks
- a user 202 can submit queries to an image search engine 230 .
- a query can include query text 212 , a query image 214 or multiple query images, or both.
- the query image 214 can be an image uploaded to search system 210 by a user of the user device 204 , and the query image 214 can have been captured using a digital camera integrated into the user device 204 , for example.
- the query image 214 can also be selected by a user 202 of the user device 204 from a list of image search results provided in response to a query.
- Image search results can be identified from a collection of images using one or more index databases.
- An indexing engine 220 can index images in a collection of images, e.g., images available on the Internet, in an image index database 232 .
- the indexing engine can index images in image index database 232 according to text associated with the image including, for example, text from surrounding context, e.g., in a web page, text that occurs in the image, and text labels determined by image classifiers.
- the indexing engine 220 can also index images in the image index database 232 according to visual keys derived from an image's content.
- the system can use visual keys to identify indexed images that share one or more similar features.
- images can be indexed by text and by visual keys in separate indexes.
- a ranking engine 222 can rank image search results by a combination of scores that are based on data indicating relevance of an image to the query text 212 and a similarity to the query image 214 .
- the similarity between images is computed using their respective image content descriptors.
- An image content descriptor can be, for example, a compressed version of a representation of features of an image.
- the features of an image can be extracted by computing a numerical representation of each feature, i.e., a feature value.
- Computing feature values of an image can be referred to as extracting features from the image.
- the feature values can be used as vector elements of a feature vector, that is, a vector of feature values.
- the elements of a feature vector can also be referred to as “features.” Whether “feature” refers to a property of image content or a numerical value in a feature vector will be apparent from the context in which the term is used. In the usage of the field, an image is said to have a feature vector.
- Image feature values can include, for example, histogram values of image color or gray scale data, e.g., the number of red pixels in an image; image locations of edges, corners, textures, centers of gravity, or other interest points; and other geometric information.
- Image features values for the same abstract feature can be extracted multiple times from multiple scales of an input image, with each appearing in a feature vector for the input image.
- the feature values of a feature vector can be weighted so that a particular feature value or set of feature values does not dominate the feature vector.
- the features values extracted from an image and associated weights can be determined by a machine learning system that is trained so that the L1 distance between feature vectors gives a reasonable approximation of visual similarity between images.
- the training data for such a system can be pairs of images having visual similarity ratings provided by human raters.
- a search system can compress a feature vector of an image into a significantly smaller representation of the visual content of an image, called an image content descriptor.
- FIG. 3 is a flow chart of an example process 300 for generating an image content descriptor.
- the process 300 will be described as being performed by a computer system of one or more computers.
- the process 300 can be carried out in parallel on multiple input images 310 .
- the process compresses a feature vector of an input image 310 into an image content descriptor 360 .
- the process 300 reduces the amount of storage space required image feature representations, enabling more feature representations to be loaded into a finite amount of random-access memory.
- the system receives an input image 310 .
- the system extracts image features into a feature vector ( 320 ).
- the features will generally include both directly measurable quantities, e.g., the number of pixels where the RGB color is between (245, 0, 0) and (255, 10, 10)—i.e., red pixels, and quantities that are the result of a possibly complex computation, e.g., a linear combination of a weighted set of histogram values.
- the feature vector of input image 310 can be large, e.g., 500,000 dimensions, and may therefore occupy a significant amount of storage space.
- the system can use kernel principal component analysis.
- Principal component analysis is a conventional technique that transforms input data into a smaller number of components called principal components.
- the PCA process generates principal components that correspond to as much of the variability or “energy” in the input data as possible.
- Kernel PCA is a variation of PCA that uses kernel methods, which allows the dot products performed by the PCA process to be replaced by nonlinear operations.
- the system can use a kernel that preserves the L1 distances between the image feature vectors.
- the intersection kernel is an example of such a kernel.
- IntK(u, v) between two feature vectors u and v of size D is given by:
- the kernel PCA process begins with a set of training examples, which can be feature vectors of images in a collection of images.
- the system can select N, e.g. 10,000, images from a collection of images and compute feature vectors for the selected images as training examples.
- the system can then build an N ⁇ N matrix T, where each row and each column in the matrix corresponds to a particular training example, and each value in the matrix is the intersection kernel between the corresponding training examples.
- the system then performs an eigendecomposition on the matrix T to compute an N ⁇ M projection matrix P, where M is the desired number of output dimensions.
- the system computes a kernel vector K which is an N-element row vector of intersection kernels between the vector x and each of the N training examples.
- intersection kernels directly can be computationally expensive, particularly if there are many training examples or if the image feature vectors have many dimensions. For example, with 10,000 training examples and 500,000-dimension feature vectors, the system would need to compute 10,000 intersection kernels, and each computation would require summing 500,000 values. Therefore, the system reduces the computation required for the kernel PCA process by approximating the intersection kernels using hash functions, rather than computing the actual intersection kernels. Other techniques could also be used to calculate approximate values of the intersection kernels. See, e.g., Subhransu Maji et al., Classification Using Intersection Kernel Support Vector Machines is Efficient , In Proceedings, CVPR 2008, Anchorage, Ak.
- Hash functions can be used to transform a feature vector of an image into a representation that requires less storage space but which can still be used to determine approximate measures of similarity between feature vectors.
- a hash function maps input data to a hash value. For example, a single hash function can map a vector of 500,000 dimensions into a three-bit value.
- Locality-sensitive hashing is a conventional method that uses a particular family of probabilistic hash functions to map input data to hashes.
- a hash collision occurs when a hash function maps two different vectors to the same hash value. The probability of a hash collision between hashes of the input vectors can be used as a measure of similarity between the vectors. Multiple hashes can be used to approximate the probability of a hash collision empirically. The probability of a hash collision can then be used to compute a similarity measure between the input vectors, for example, the Jaccard similarity.
- the system generates the hashes using a consistent weighted sampling process, e.g., weighted minhash.
- a consistent weighted sampling process can generate, for an input vector S, a hash value that depends on sampled independent random variables and non-zero elements of S, with the property that the probability of a hash collision between the respective hash values of two vectors, S and T, equals the Jaccard similarity of S and T.
- the system can repeat the sampling process multiple times, e.g., 1,000, 5,000, or 15,000 times, to generate multiple hash values for the image feature vector.
- the multiple hash values can be stored in a vector of hash values, which can be used to compute an approximation of an intersection kernel between the two original vectors.
- the probability of a hash collision between two vectors u and v can be approximated by generating two hash vectors, x and y, where x contains multiple hash values from multiple hash functions applied to vector u, and y contains multiple hash values from the same multiple hash functions applied to vector v.
- the approximate probability of a hash collision can then be computed. In other words, the number of hash values that match between x and y divided by the total number of hash values, or the length L of x or y.
- approximation A of the probability of a hash collision between u and v can be computed by:
- the probability of a hash collision can be used to compute the intersection kernel between u and v as follows.
- the probability of a hash collision is equal to the Jaccard similarity between u and v, J(u, v), which is given by:
- J ⁇ ( u , v ) ⁇ i D ⁇ min ⁇ ( u i , v i ) ⁇ i D ⁇ max ⁇ ( u i , v i ) .
- the numerator is the intersection kernel between u and v.
- the denominator can be computed using the intersection kernel and the L1 norms of u and v, given by:
- ⁇ i D ⁇ max ⁇ ( u i , v i ) ⁇ u ⁇ 1 + ⁇ v ⁇ 1 - ⁇ i D ⁇ min ⁇ ( u i , v i ) .
- intersection kernel The last term is also the intersection kernel. Therefore, the system can compute an approximation of the intersection kernel using only the approximated probability of a hash collision, A, the L1 norm of u,
- the system can efficiently compute an approximation of the intersection kernel between the original vectors u and v by using only the hash vectors x and y and the L1 norms for u and v.
- the system can therefore also reduce the storage space needed for feature vectors by storing only the hash vectors and L1 norm of each image feature vector for images in a collection of images, rather than, e.g., all 500,000 dimensions of the original feature vectors.
- the system performs kernel principal component analysis ( 340 ).
- the system can generate the kernel vector K using intersection kernels between a feature vector and each training example.
- the system can then multiply kernel vector K by the projection matrix P to give the reduced-dimension projection R.
- the size of the reduced-dimension projection R depends on the number of dimensions chosen for the kernel PCA process. In some implementations, the projection R contains around 500 dimensions.
- the system compresses the projection R using delta encoding (350).
- the system can use a two-level delta encoding method to compress the output of the kernel PCA process, as described in more detail below. Because the PCA process can order the output projection R by components with the highest variability, a number of first elements of the M elements of R can be selected for compression.
- the system can reduce the size of R using encoding. For example, the first 59 elements of R can be encoded with a two-level delta encoding process into a content descriptor of 32 bytes. Other sizes of content descriptors are possible. For example, 119 dimensions can be compressed into 64 bytes, and 479 dimensions can be compressed into 256 bytes.
- Some elements of output projection R may have variances that differ dramatically from those of other elements, due to the kernel PCA process. Therefore, compressing R with delta encoding may require allocating more bits to some dimensions than others.
- the system can normalize the variances of the elements of R by first multiplying the projection R by a randomly chosen rotation matrix, i.e., an orthonormal matrix.
- the system generates a rotation matrix by selecting matrix entries from a Gaussian distribution with unit variance and performing Gram-Schmidt orthogonalization on the matrix.
- the system can use one or multiple rotation matrices to normalize the projection R.
- the system instead of multiplying every projection R by the rotation matrix, the system can multiply a generated rotation matrix by the projection matrix P, effectively storing the rotation matrix values within projection matrix P.
- the system can generate a block diagonal rotation matrix, which can allow the R to be truncated to any appropriate length while maintaining a reasonable L2 distance between corresponding output vector representations.
- the system can generate a different rotation matrix for each block. For example, the system can choose a rotation matrix that corresponds to four 60 ⁇ 60 blocks by selecting 4 rotation matrices to build the larger, block diagonal rotation matrix. Other entries in the matrix can be set to 0.
- a 240 ⁇ 240 block diagonal rotation matrix can be generated with 60 ⁇ 60 square rotation matrices R1, R2, R3, and R4, and 60 ⁇ 60 square zero matrices, 0, as follows:
- the projection R By multiplying R, or the projection matrix P, by a block diagonal rotation matrix, the projection R can be truncated at a block boundary and still maintain a reasonable L2 distance with other representations.
- FIG. 4 is a flow chart of an example encoding process 400 .
- the process 400 can be performed by a computer system of one or more computers.
- the process 400 can operate on the output of the kernel PCA process as described with reference to FIG. 3 , but need not be performed by the same device.
- the process 400 can take as input a vector, e.g., a vector of floating point numbers, e.g., a projection of the feature representation of an image, and output an encoded sequence of values that requires less storage space than the input.
- a vector e.g., a vector of floating point numbers, e.g., a projection of the feature representation of an image
- the system determines a global maximum ( 410 ).
- the global maximum can be a maximum of the absolute values of all vector elements of all input vectors.
- the global maximum can be a magnitude of a feature vector element which no individual vector element in any feature vector is expected to exceed.
- the system can use a single, system-wide value as a global maximum, which can be based on a vector element having the largest absolute value over a collection of vectors, e.g., over all projections R.
- the system computes a vector maximum for each input vector ( 420 ).
- the vector maximum is a maximum of the absolute values of all vector elements in the vector.
- the system partitions the vector into blocks ( 430 ).
- the partitions need not be of equal size and need not be contiguous. Partitioning the vector into blocks can help to maintain the variability of input elements in each block when encoding the vector. For example, if the vector is the output of a PCA process, the elements in a first block can be of much higher significance than elements in other blocks. Partitioning the vector into blocks can maintain variability of the elements by preventing all elements in a particular block from being encoded to a single value, e.g., zero. As mentioned above, the block partitioning can follow the blocks selected for a block diagonal rotation matrix.
- the system computes block maxima of absolute values of vector elements for each block ( 440 ).
- the system encodes the vector maximum of each input vector relative to the global maximum ( 450 ), i.e., with an encoded value that represents the ratio of the vector maximum to the global maximum.
- the encoding process can include mapping an input value to one of a series of intervals between a minimum value, e.g., zero, and a maximum value, e.g., the global maximum. For example, if four bits are available for encoding the vector maximum relative to the global maximum, the system can compute 16 intervals, because four bits can represent 16 intervals, between zero and the global maximum. The system can then determine in which interval the vector maximum falls.
- the system can encode the vector maximum V max relative to the global maximum U in N bits for a vector having elements x i as:
- the system can compute 16 intervals at 0-0.4, 0.4-0.8, 0.8-1.2, etc. If the vector maximum is 5.91, the system can encode this value as 14, which is the number of the interval containing 5.91. Using 4 bits, the system can represent the vector maximum in binary as 1110 , or, in hexadecimal notation, 0xE. Considered as a ratio, the value has an implicit binary point before the first bit.
- the system encodes each block maximum relative to the vector maximum ( 460 ).
- the system can compute a number of intervals corresponding to a number of bits allocated to encoding each block maximum.
- the system can encode the block maximum B max relative to the vector maximum V max in N bits for elements y i in a block as:
- the system can compute 16 intervals at 0-0.2, 0.2-0.4, etc.
- the system can then encode each block maximum relative to the vector maximum, and can represent each block maximum with four bits. For example, if the block maximum is 0.53, the system can encode this value relative to the vector maximum as hexadecimal value 0x2, the number of the interval containing 0.53.
- the system encodes each vector element relative to its respective block maximum ( 470 ). Because the value of each vector element to be encoded can be signed, i.e. negative or positive, the last encoding relative to the block maximum can represent signed values. Because no vector element will exceed its block maximum, system can encode the vector element relative to the block maximum as a value between ⁇ 1 and 1. Therefore, with N bits, the system can define 2 ⁇ N intervals between ⁇ 1 and 1 for each vector element and encode the vector element as one of these intervals. The system can encode the vector element v i relative to the block maximum B max in N bits as:
- the system outputs the compressed content descriptor ( 480 ).
- the system can pack the encoded bit representations of the vector maximum, the block maxima, and each vector element into a compressed content descriptor. For example, for a 59-element input vector, the system can partition the vector into four blocks of 15, 15, 15, and 14 elements. After encoding each element in 4 bits indicating the value of the element relative to the vector maximum, the four block maxima, and the 59 individual vector elements, the compressed output content descriptor can occupy a total of 32 bytes.
- the system can first decompress the content descriptors before using the content descriptors to compute a distance metric.
- An exact reconstruction of the input vector is generally not possible due to quantization from the encoding process.
- the system can reconstruct the input vector values approximately by choosing a midpoint value of each encoded interval. For example, if an encoded interval corresponds to 3.0-4.0, the system can choose 3.5 as the midpoint value.
- the system can reconstruct the vector maximum V max approximately from its encoded value E(V max ) and the global maximum U as:
- V max E ⁇ ( V max ) ⁇ U 2 N .
- the system can reconstruct a block maximum B max approximately from its encoded value E(B max ) and the reconstructed vector maximum V max as:
- B max E ⁇ ( B max ) ⁇ V max 2 N .
- the system can reconstruct each vector element v i approximately from its encoded value E(v i ) and the reconstructed approximate block maximum B max as:
- v i B max ⁇ [ E ⁇ ( v i ) 2 N - 1 + 1 2 N - 1 ] .
- the system can compute the distance between each decompressed content descriptor using a distance metric, e.g., the L2 distance.
- FIG. 5 is a diagram of an example spill tree 500 with a root node 510 .
- a spill tree is a data structure similar to a binary tree, but in which branches in the tree need not strictly partition the data into two partitions. Instead, data elements can be shared by sibling child nodes at each branch in the spill tree; or equivalently, input data elements can follow one or more children at each branch.
- a spill tree can have any arbitrary number of child nodes at each branch. For example, a spill tree can be implemented as a quad tree with four nodes at each branch.
- a spill tree can be used to assign visual keys to images. Visual keys identify a subset of images that are likely to share one or more similar features.
- a spill tree e.g. spill tree 500
- a spill tree can be used on any feature representation of an image, including a feature representation at any stage of dimensionality reduction and compression, e.g., the stages shown in FIG. 3 .
- the spill tree 500 can be used on the original feature vector, the hash vector computed from the feature vector, the output projection of a kernel PCA process, or the compressed content descriptor.
- a set of training images can be used to compute the branch conditions at each branch in the spill tree.
- branch conditions determine how the system will traverse the spill tree with subsequent input feature representations, and therefore, which visual keys the system will assign to the corresponding images.
- Branch conditions can be defined in numerous ways.
- a branch condition can include chosen or generated decisional feature representations, each associated with a particular child node at that branch.
- the system computes a distance between the image's feature representation and each decisional feature representation for a particular non-leaf node. In this context, the system can be said to be computing the branch to which the image is closest at that node.
- the system will then traverse the tree to the child node associated with the closest branch, as determined by the computed distance between the image's feature representation and the decisional feature representations of each branch. In some circumstances, e.g., if the image is between two branches, the system can traverse the spill tree to multiple child nodes.
- the system can traverse the spill tree with feature representations of images in a collection of images to one or more leaf nodes for each image.
- the feature representation of the image will be compared to each decisional feature representation at a particular non-leaf node to determine which child node, or nodes, the system should follow.
- the comparison can be based on a particular distance function or a particular similarity function.
- the distance could be the L1 distance, L2 distance, or Jaccard distance.
- the system can then traverse the spill tree to the child node whose associated decisional feature representation resulted in the smallest distance, or greatest similarity.
- the system can additionally follow one or more other child nodes if the difference between the computed distances or similarities is within a threshold.
- the system can select or compute decisional feature representations for each child node.
- the system generates decisional feature representations by machine learning using a set of training images.
- the feature representations of the training images are clustered into a number of clusters corresponding to the number of child nodes.
- a clustering algorithm can be used to generate two clusters of feature representations.
- the decisional feature representations can be calculated based on the feature representations in each cluster.
- the feature representations in each cluster are averaged together to generate each decisional feature representation.
- feature representations of the training images can be clustered into two clusters 516 and 518 .
- the feature representations in cluster 516 can be averaged to generate decisional feature representation A 1 512 , which corresponds to child node 520 .
- each element of decisional feature representation 512 is calculated as a mean or other central value of corresponding elements of feature representations in cluster 516 .
- feature representations in cluster 518 can be averaged to generate decisional feature representation B 1 514 , which corresponds to child node 530 .
- the clustering is performed by conventional clustering algorithms, e.g., k-means clustering.
- images in cluster 516 will be used to generate decisional feature representations for child node 520
- images in cluster 518 be used to generate decisional feature representations for child node 530
- the difference in the distance between (1) the particular feature representation and A 1 512 and (2) the particular feature representation and B 1 514 will be below a threshold.
- Region 517 illustrates a range of feature representations for which the difference in the distance to A 1 512 and B i 514 was below a threshold.
- the training process can be repeated at each non-leaf child node.
- feature representations used to compute decisional feature representations for child node 520 i.e., those feature representations in cluster 516 as well as feature representations in region 517 , are again divided into clusters 526 and 528 .
- Feature representations in cluster 526 are used to generate decisional feature representation A 2 522 corresponding to child node 540 .
- Feature representations in cluster 528 are used to generate decisional feature representation B 2 524 corresponding to child node 550 .
- Feature representations in region 527 will be used to compute decisional feature representations of both child nodes 540 and 550 due to the difference between the distance to A 2 522 and the distance to B 2 524 being below a threshold.
- leaf node 540 contains feature representations 501 , 502 , 503 , and 504 . Because data can be shared at each branch of a non-leaf node, feature representation 501 is also contained in leaf node 530 . Likewise, feature representations 502 and 503 are also contained in leaf node 550 . Feature representations 505 , 506 , and 507 appear in only a single leaf node. In some implementations, growth of a particular spill tree branch is stopped when a node contains fewer than a threshold number of feature representations. For example, further partitioning of feature representations contained in leaf node 530 can be prevented because leaf node 530 contains fewer than a threshold number of feature representations.
- the spill tree 500 next expands, i.e. generates additional child nodes from, leaf nodes that contain the highest number of feature representations. This approach can reduce the disparity in the number of feature representations contained in the final set of leaf nodes. Expanding leaf nodes with the largest number of feature representations can also result in an unbalanced spill tree, e.g., spill tree 500 .
- each non-leaf node contains decisional feature representations for each child node.
- other data associated with the spill tree can be discarded, such as clustering information or images in leaf nodes. Thus, only the decisional feature representations remain.
- each leaf node can be assigned a unique visual key e.g., a number.
- leaf node 540 can be assigned the visual key 1.
- leaf nodes 550 and 530 can be assigned visual keys 2 and 3 respectively.
- FIG. 6 is a flow chart of an example process 600 for generating visual keys.
- a computer system can traverse the spill tree with feature representations for each image in a collection of images to associate one or more visual keys with each image. Images can then be associated with their respective visual keys and generated content descriptors.
- the system generates a feature representation for an image ( 610 ).
- the feature representation can be, for example, one or more hashes, a vector of extracted feature representations, or a 32-byte content descriptor generated as described with reference to FIG. 3 .
- the generated feature representation is the same type of feature representation as that used to train the spill tree as described with reference to FIG. 5 .
- the system traverses the spill tree with the image feature representation ( 620 ).
- the system traverses the spill tree to one or more leaf nodes according to the feature representation. At each non-leaf node, the system may traverse to one or multiple child nodes.
- the system accumulates the spill at each branch ( 630 ), as will be described below. At any given branch in the spill tree, the image will be closer to the feature representation of one branch than to that of other branch.
- the system can traverse to the child node of the closest branch, but the system can also traverse to one or more other child nodes if the difference between the distances between the image and the decisional feature representations is within a threshold.
- the system will be traversing the spill tree with two separate instances of the image's feature representation. For each instance of the feature representation, the system can accumulate “spill,” which can be the difference between the distance from the image feature representation and each decisional feature representation.
- the system can compute the distances D(X, A) and D(X, B). The system will traverse to the child node corresponding to A if D(X, A) is less than D(X, B), but the system will also traverse to the child node corresponding to B with another instance of feature representation X if D(X, B) ⁇ D(X, A) is within a threshold.
- the system can compute the difference between D(X, B) and D(X, A) as the “spill” for the instance of X that the system uses to traverse to the child node corresponding to B.
- the system identifies visual keys for the image ( 640 ). After the system traverses the spill tree with an image's feature representation to one or more leaf nodes of the spill tree, the system can associate the image with each visual key corresponding to each leaf node to which the system traversed using instances of the image's feature representation. The system can also associate the accumulated spill of a particular feature representation with visual keys associated with the image.
- the system ranks the visual keys by accumulated spill ( 650 ).
- Among the visual keys associated with an image there will be one visual key associated with no accumulated spill, corresponding to a leaf node that the system traversed to by always following child nodes corresponding to the closest decisional feature representation at each branch. Every other visual key can have associated with it a nonzero amount of spill.
- the system can rank the visual keys by the amount of associated spill, with lesser amounts of spill being considered a higher-ranking visual key. In other words, the visual key that resulted in no accumulated spill can be the highest-ranked visual key.
- the system can associate each visual key, e.g., in a posting list, with all images that generated that visual key.
- each image in a collection of images that generated visual key 43 can be associated with visual key 43 .
- Each image in the collection can be associated with multiple visual keys.
- FIG. 7 is a flow chart of an example process 700 for combining search results for query text and a query image.
- the process 700 can be implemented by a search system to return search results in response to a query containing both an image content portion, i.e. a query image, and a text portion, i.e. query text, for example, search results as shown in FIG. 1 .
- the system receives first image search results responsive to query text ( 710 ).
- images in a collection of images are associated with text labels.
- the system can generate first image search results that link to images that are responsive to query text of a query.
- Each identified image of the first image search results can be associated with a text score representing the relevance of the image to the query text.
- the text score can be an Information Retrieval (IR) score determined by a search engine for an image search result that is responsive to query text of the query.
- IR Information Retrieval
- the system receives second image search results responsive to a query image ( 720 ).
- the system can receive images in a collection of images identified as being similar to the query image.
- each identified image has a respective score, e.g., an IR score, determined by a search engine that indicates the similarity of the identified image to the query image.
- a feature representation is generated for the query image. This can be done, for example, by the process as shown in FIG. 3 .
- the system can traverse a trained spill tree with the feature representation for the query image to generate one or more visual keys for the query image.
- the system can retrieve all images in the collection associated with each of the visual keys generated from the query image.
- the system associates each of the second image search results with a respective score indicating a measure of similarity between the query image and each image of the second image search results.
- the system generates only those second image search results whose associated score meets a similarity threshold.
- the measure of similarity can be based on the distance between the content descriptor for the query image and the respective content descriptor of each image identified by the second image search results.
- the system can also remove second image search results that correspond to duplicate and near duplicate images to the query image, which can be identified based on the image content descriptors for the images. Other measures of similarity are possible.
- the system selects one or more images of the first image search results that also occur in the second image search results and whose respective text scores satisfy a first threshold ( 730 ).
- the system can omit first images whose text scores do not meet the first threshold as well as images that did not appear in both of the first image search results and the second image search results.
- the system retrieves the first image search results and second image search results in parallel from a single collection of images.
- the system generates a set of final image search results by combining the first scores and the second scores of the selected first image search results ( 740 ).
- the system can give a score boost to first image search results whose text score is within the scores of a threshold number of the highest-ranked first image search results. For example, the system can multiply the text scores by 150% if the first image search results are within the threshold number of highest-ranked first image search results.
- the system can also adjust the text scores of certain first image search results that have a text score that is less than a dynamic percentile threshold, e.g., the bottom 30% of first image search results.
- a dynamic percentile threshold e.g., the bottom 30% of first image search results.
- the system computes a median of text scores and computes the percentile threshold as:
- percentile_threshold 100 ⁇ ( 1 - 20 ⁇ median ( 1 + ( 20 ⁇ median ) 1 / 3 ) 3
- a demotion factor can then be calculated as:
- demotion text_score threshold , where threshold is the text score at the percentile_threshold.
- the demotion factor can be weakened if the image is particularly visually similar to the query image, which is determined by a closeness factor as:
- closeness_factor max ⁇ ( 0 , mean_of ⁇ _visual ⁇ _dist - 3 ⁇ dist std_deviation ⁇ _visual ⁇ _dist ) , where mean_of_visual_dist is the mean of distances to the query image, std_deviation_visual_dist is the standard deviation of distances to the query image and dist is the distance between the image and the query image.
- the system orders the final image search results by a score derived from a distance between the query image and each respective image in the set of final image search results ( 750 ).
- the distance between the query image and each image represented in the final image search results is computed as a distance between content descriptors, e.g., the content descriptors computed by the process described in reference to FIG. 3 .
- the system reorders the final image search results a number of times, each time ordering the search results by distance to the second, third, fourth, etc., image in the final search results.
- the remaining final image search results are ordered based on the distance between each image and the highest-ranked image, as opposed to the query image.
- the remaining final image search results are ordered based on the distance between each image and the second-highest-ranked image, as opposed to the highest-ranked image or query image.
- the ordinal position of each remaining image can be based on the distance to the top N images, rather than on the distance to the query image.
- the system can additionally remove duplicate and near-duplicate images from the set of final image search results, as will be described below with reference to FIG. 8 .
- FIG. 8 is a flow chart of an example process 800 for retrieving images by visual keys. After training a spill tree and generating visual keys for each image in a collection of images, a computer system can use visual keys from a query image to identify similar images by visual keys.
- the system derives a set of visual keys by sorting training images into a plurality of image sets according to respective feature representations of the training images ( 810 ).
- the system trains a spill tree using a set of training images, where the first image sets correspond to the leaf nodes of the spill tree.
- the system uses hash functions to assign each image to one or more image sets.
- the system receives a query image ( 820 ).
- the query image can be an image for which images with content similar to the query image are to be identified.
- the system associates one or more visual keys with the query image.
- the query image's associated visual keys each correspond to one of the image sets to which the query image belongs ( 830 ).
- the system generates a feature representation for the query image and uses the feature representation to traverse a spill tree to generate visual keys for the query image.
- the system distinguishes one or more subsets of images in a collection of images. Each subset is associated with respective visual keys that match the one or more visual keys associated with the query image ( 840 ).
- the system uses a feature representation for each image in the collection to traverse a spill tree. Leaf nodes traversed to by the feature representation are associated with each image as visual keys for the image.
- the system combines the subsets of images into a final set of images ( 850 ).
- the system can combine the image subsets by a union in which all images from the subsets are included in the final set.
- the system can also combine the image sets by an intersection in which only images that appear in all the subsets are included in the final set of images.
- the system can additionally combine any arbitrary number of image sets by any number of unions and intersections.
- the system can also perform any number of unions or intersections with sets of images that satisfy a text query.
- the system can produce the intersection of five second image sets.
- the system retrieves images that share at least five visual keys with the query image.
- the system when processing a text and image query, can return an intersection of the text query with a union of three second image sets. In other words, the system can retrieve images that are relevant to the text query and which share any of three visual keys with the query image.
- the system can also use machine learning to learn which combinations of visual keys are associated with certain text labels. For example, the system could learn which combination of visual keys is associated with a certain breed of dog, e.g., beagle. The system can then accept a search specifying “dog” (the text query) and visual keys associated with beagles.
- dog the text query
- the system can combine image sets by specifying visual keys that should not be included in the final set of images. For example, the system can omit images with visual keys that have been identified as undesirable. The system can allow users to specify both image content that is desirable and image content that is undesirable.
- FIG. 9 is a flow chart of an example process 900 for identifying near-duplicate images.
- Near-duplicate images can be identified by computing distances between images using, for example, content descriptors associated with each image. However, it can be computationally expensive to compute each distance between each image in a set of images. Generated visual keys can be used to determine that two images are not near-duplicates without explicitly computing the distance between the images. In some implementations, however, identifying images as near-duplicates requires explicitly computing a distance between the images.
- the system derives a set of visual keys by sorting training images into a plurality of image sets according to respective feature representations of the training images ( 910 ).
- the system trains a spill tree using a set of training images, where the image sets correspond to the leaf nodes of the spill tree.
- the system uses hash functions to assign each image to one or more first image sets. The system can then defines a distinct visual key corresponding to each image set.
- the system receives a set of images ( 920 ).
- the set of images can be received, for example, from a process that combines images that match a text query and images that match an image query, such as the process described with reference to FIG. 6 .
- the system associates one or more visual keys with each image in the set of images.
- Each image's associated visual keys each correspond to one of the image sets to which the image belongs ( 930 ).
- the system uses a feature representation for each image to traverse a spill tree. Leaf nodes traversed to by the feature representation are associated with each image as visual keys for the image.
- the system compares a number of visual keys generated for a first image to a number of visual keys generated for a second image ( 940 ). For example, the system can compare the two highest-ranked visual keys for each image. Visual keys for an image can be ranked according to the amount of spill that resulted when reaching that visual key for the image by traversing the spill tree.
- the highest-ranked visual keys are stored in a bit vector for each image, and a bitwise AND operation is performed between bit vectors of images to determine whether any visual keys match between the respective images.
- a 256-bit vector can contain all zeroes except for at vector positions of visual keys for the associated image, which instead contain ones. If more visual keys exist than the size of the bit vector, the visual key numbers can be assigned with a modulo operator. After assigning visual keys to a bit vector associated with each image, if a bitwise AND between respective bit vectors yields any nonzero bits, the images cannot be ruled out as near-duplicates and the full distance between the images can be calculated to determine if the images are near-duplicates.
- the system produces one or more sets of near-duplicate images; if no visual keys match between the first image and the second image, the first image and the second image are not included in a same set of near-duplicate images ( 950 ). If none of the highest-ranked visual keys match between two images, the system deems it likely that the images are not near-duplicates. When no highest-ranked visual keys match between images, the system does not calculate the distance between the images to determine whether the images are near-duplicates, resulting in saved computational expense. The system can take additional measures to improve performance of identifying near-duplicate images.
- FIG. 10 is a flow chart of an example process 1000 for identifying near-duplicate images.
- the example process 1000 can be combined with the process 900 described in reference to FIG. 9 for efficiently identifying near-duplicate images.
- the process 1000 will be described as being performed on a ranked set of received images, among which near-duplicate images will be identified.
- the system compares the highest-ranked N images to each other ( 1010 ).
- N 100, in which case each of the highest-ranked 100 images is compared to each of the other highest-ranked 99 images.
- the images are compared by computing distances between their respective content descriptors, or some other feature representation. If the computed distance between two images is below a threshold, the system determines that the two images are near-duplicates. The system can also determine that two images are not near-duplicates if they have no visual keys in common.
- the system places image X in a category for image Y, and the system places image Yin a category for image X ( 1020 ).
- Grouping near-duplicate images into categories allows transitivity between near-duplicate images. That is, if X and Y are near-duplicates and Y and Z are near-duplicates, then the system can consider X and Z also to be near-duplicates by virtue of appearing in a same category.
- the system compares each image ranked between N+1 and an upper bound M in the ranked set to the N highest-ranked images, placing near-duplicates in additional categories ( 1030 ). If N is 100 and M is 1000, the system can, for example, compare each image ranked 101-1000 to each of the top 100 ranked images. If any near-duplicates A and B are identified, the near-duplicates are added to additional categories. In other words, A is added to a category for B, and B is added to a category for A.
- the system compares each image ranked between N+1 and M to each image in a window of images around the image, placing near-duplicates in additional categories ( 1040 ).
- the window is 3-5 images higher or lower than each image in the ranked set of images.
- the system can compare an image A ranked at position 203 to images ranked 200 , 201 , 202 , i.e. the three adjacent images ranked higher than the image, as well as images ranked 204 , 205 , and 206 , i.e. the three adjacent images ranked lower than the image.
- the system provides one image per category for the final ranked set of images ( 1050 ).
- the system can select a single image for each category for the final ranked set of images. In some implementations, the system selects the highest-ranked image in each category.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non-transitory program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
wherein |u|1 is the L1 norm of u, and |v|1 is the L1 norm of v. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
wherein |u|1 is the L1 norm of u and |v|1 is the L1 norm of v. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
wherein |u|1 is the L1 norm of u and |v|1 is the L1 norm of v. A measure of similarity is determined between content of a first data object corresponding to representation r and content of a second data object corresponding to a representation s, including computing a distance between representation r and representation s. The first data object and the second data object are images.
R=KP.
where xi==yi has the value one (1) if xi is equal to yi and the value zero otherwise.
where threshold is the text score at the percentile_threshold.
where mean_of_visual_dist is the mean of distances to the query image, std_deviation_visual_dist is the standard deviation of distances to the query image and dist is the distance between the image and the query image. A visual promotion score can be calculated using the closeness factor as:
visual promote=1+(0.6×closeness_factor).
score=text_score×max(1,demotion×visual_promote).
score(I k)=1×dist(I 1 ,I k)+0.8×dist(I 2 ,I k)+0.6×dist(I 3 ,I k)+0.4×dist(I 4 ,I k)+0.2×dist(I 5 ,I k).
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/433,137 US9043316B1 (en) | 2011-03-28 | 2012-03-28 | Visual content retrieval |
US13/621,039 US8983941B1 (en) | 2011-03-28 | 2012-09-15 | Visual content retrieval |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161468532P | 2011-03-28 | 2011-03-28 | |
US201261605132P | 2012-02-29 | 2012-02-29 | |
US13/433,137 US9043316B1 (en) | 2011-03-28 | 2012-03-28 | Visual content retrieval |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/621,039 Continuation US8983941B1 (en) | 2011-03-28 | 2012-09-15 | Visual content retrieval |
Publications (1)
Publication Number | Publication Date |
---|---|
US9043316B1 true US9043316B1 (en) | 2015-05-26 |
Family
ID=52632363
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/433,137 Active 2032-09-22 US9043316B1 (en) | 2011-03-28 | 2012-03-28 | Visual content retrieval |
US13/621,039 Expired - Fee Related US8983941B1 (en) | 2011-03-28 | 2012-09-15 | Visual content retrieval |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/621,039 Expired - Fee Related US8983941B1 (en) | 2011-03-28 | 2012-09-15 | Visual content retrieval |
Country Status (1)
Country | Link |
---|---|
US (2) | US9043316B1 (en) |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140074453A1 (en) * | 2012-09-13 | 2014-03-13 | International Business Machines Corporation | Computer implemented method, program, and system for identifying non-text element suitable for communication in multi-language environment |
US20140258248A1 (en) * | 2013-03-06 | 2014-09-11 | Dell Products, Lp | Delta Compression of Probabilistically Clustered Chunks of Data |
US20150066957A1 (en) * | 2012-03-29 | 2015-03-05 | Rakuten, Inc. | Image search device, image search method, program, and computer-readable storage medium |
US9747305B2 (en) | 2012-03-29 | 2017-08-29 | Rakuten, Inc. | Image search device, image search method, program, and computer-readable storage medium |
WO2020014770A1 (en) * | 2018-07-17 | 2020-01-23 | Avigilon Corporation | Hash-based appearance search |
US10846554B2 (en) | 2018-07-17 | 2020-11-24 | Avigilon Corporation | Hash-based appearance search |
US20210157833A1 (en) * | 2019-11-26 | 2021-05-27 | Dash Hudson | Visual image search using text-based search engines |
Families Citing this family (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8949253B1 (en) * | 2012-05-24 | 2015-02-03 | Google Inc. | Low-overhead image search result generation |
US9910864B2 (en) * | 2014-01-22 | 2018-03-06 | Stmicroelectronics S.R.L. | Method for object recognition, corresponding system, apparatus and computer program product |
US10275819B1 (en) * | 2015-05-13 | 2019-04-30 | Amazon Technologies, Inc. | Reducing incompatible pairings |
US11609946B2 (en) | 2015-10-05 | 2023-03-21 | Pinterest, Inc. | Dynamic search input selection |
US11055343B2 (en) | 2015-10-05 | 2021-07-06 | Pinterest, Inc. | Dynamic search control invocation and visual search |
US11003692B2 (en) * | 2015-12-28 | 2021-05-11 | Facebook, Inc. | Systems and methods for online clustering of content items |
US10397528B2 (en) | 2016-02-26 | 2019-08-27 | Amazon Technologies, Inc. | Providing status information for secondary devices with video footage from audio/video recording and communication devices |
US10841542B2 (en) | 2016-02-26 | 2020-11-17 | A9.Com, Inc. | Locating a person of interest using shared video footage from audio/video recording and communication devices |
KR102459633B1 (en) | 2016-02-26 | 2022-10-28 | 아마존 테크놀로지스, 인크. | Sharing video footage from audio/video recording and communication devices |
US10489453B2 (en) | 2016-02-26 | 2019-11-26 | Amazon Technologies, Inc. | Searching shared video footage from audio/video recording and communication devices |
US11393108B1 (en) | 2016-02-26 | 2022-07-19 | Amazon Technologies, Inc. | Neighborhood alert mode for triggering multi-device recording, multi-camera locating, and multi-camera event stitching for audio/video recording and communication devices |
US9965934B2 (en) | 2016-02-26 | 2018-05-08 | Ring Inc. | Sharing video footage from audio/video recording and communication devices for parcel theft deterrence |
US10748414B2 (en) * | 2016-02-26 | 2020-08-18 | A9.Com, Inc. | Augmenting and sharing data from audio/video recording and communication devices |
US9760690B1 (en) * | 2016-03-10 | 2017-09-12 | Siemens Healthcare Gmbh | Content-based medical image rendering based on machine learning |
US20180101540A1 (en) * | 2016-10-10 | 2018-04-12 | Facebook, Inc. | Diversifying Media Search Results on Online Social Networks |
US11841735B2 (en) | 2017-09-22 | 2023-12-12 | Pinterest, Inc. | Object based image search |
US10942966B2 (en) | 2017-09-22 | 2021-03-09 | Pinterest, Inc. | Textual and image based search |
US11126653B2 (en) * | 2017-09-22 | 2021-09-21 | Pinterest, Inc. | Mixed type image based search results |
JP6877374B2 (en) * | 2018-02-16 | 2021-05-26 | 株式会社日立製作所 | How to train a model that outputs a vector that represents the tag set that corresponds to the image |
US11556581B2 (en) * | 2018-09-04 | 2023-01-17 | Inception Institute of Artificial Intelligence, Ltd. | Sketch-based image retrieval techniques using generative domain migration hashing |
CN112099725A (en) * | 2019-06-17 | 2020-12-18 | 华为技术有限公司 | Data processing method and device and computer readable storage medium |
CN111310852B (en) * | 2020-03-08 | 2022-08-12 | 桂林电子科技大学 | Image classification method and system |
US11750364B2 (en) * | 2020-03-17 | 2023-09-05 | Brainlab Ag | Fuzzy datamatching using homomorphic encryption |
CN111563181B (en) * | 2020-05-12 | 2023-05-05 | 海口科博瑞信息科技有限公司 | Digital image file query method, device and readable storage medium |
US11709882B2 (en) * | 2021-10-21 | 2023-07-25 | International Business Machines Corporation | Image storage system for images with duplicate parts |
CN116662588B (en) * | 2023-08-01 | 2023-10-10 | 山东省大数据中心 | Intelligent searching method and system for mass data |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20050055344A1 (en) * | 2000-10-30 | 2005-03-10 | Microsoft Corporation | Image retrieval systems and methods with semantic and feature based relevance feedback |
US20060271559A1 (en) * | 2005-05-26 | 2006-11-30 | Nicholas Stavrakos | Method and system for delta compression |
US20070081744A1 (en) | 2005-05-09 | 2007-04-12 | Gokturk Salih B | System and method for use of images with recognition analysis |
US20080082426A1 (en) | 2005-05-09 | 2008-04-03 | Gokturk Salih B | System and method for enabling image recognition and searching of remote content on display |
US20080263042A1 (en) * | 2007-04-18 | 2008-10-23 | Microsoft Corporation | Object similarity search in high-dimensional vector spaces |
US20100088295A1 (en) * | 2008-10-03 | 2010-04-08 | Microsoft Corporation | Co-location visual pattern mining for near-duplicate image retrieval |
-
2012
- 2012-03-28 US US13/433,137 patent/US9043316B1/en active Active
- 2012-09-15 US US13/621,039 patent/US8983941B1/en not_active Expired - Fee Related
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20050055344A1 (en) * | 2000-10-30 | 2005-03-10 | Microsoft Corporation | Image retrieval systems and methods with semantic and feature based relevance feedback |
US20070081744A1 (en) | 2005-05-09 | 2007-04-12 | Gokturk Salih B | System and method for use of images with recognition analysis |
US20080082426A1 (en) | 2005-05-09 | 2008-04-03 | Gokturk Salih B | System and method for enabling image recognition and searching of remote content on display |
US20060271559A1 (en) * | 2005-05-26 | 2006-11-30 | Nicholas Stavrakos | Method and system for delta compression |
US20080263042A1 (en) * | 2007-04-18 | 2008-10-23 | Microsoft Corporation | Object similarity search in high-dimensional vector spaces |
US20100088295A1 (en) * | 2008-10-03 | 2010-04-08 | Microsoft Corporation | Co-location visual pattern mining for near-duplicate image retrieval |
Non-Patent Citations (1)
Title |
---|
Liu et al. Article title: 'Clustering Billions of Images with Large Scale Nearest Neighbor Search'. WACV '07-IEEE Workshop Applications of Computer Vision, 2007. Austin, TX, Feb. 2007. |
Cited By (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9747305B2 (en) | 2012-03-29 | 2017-08-29 | Rakuten, Inc. | Image search device, image search method, program, and computer-readable storage medium |
US9940366B2 (en) * | 2012-03-29 | 2018-04-10 | Rakuten, Inc. | Image search device, image search method, program, and computer-readable storage medium |
US20150066957A1 (en) * | 2012-03-29 | 2015-03-05 | Rakuten, Inc. | Image search device, image search method, program, and computer-readable storage medium |
US9514127B2 (en) * | 2012-09-13 | 2016-12-06 | International Business Machines Corporation | Computer implemented method, program, and system for identifying non-text element suitable for communication in multi-language environment |
US20140074453A1 (en) * | 2012-09-13 | 2014-03-13 | International Business Machines Corporation | Computer implemented method, program, and system for identifying non-text element suitable for communication in multi-language environment |
US9798731B2 (en) * | 2013-03-06 | 2017-10-24 | Dell Products, Lp | Delta compression of probabilistically clustered chunks of data |
US20140258248A1 (en) * | 2013-03-06 | 2014-09-11 | Dell Products, Lp | Delta Compression of Probabilistically Clustered Chunks of Data |
WO2020014770A1 (en) * | 2018-07-17 | 2020-01-23 | Avigilon Corporation | Hash-based appearance search |
US10846554B2 (en) | 2018-07-17 | 2020-11-24 | Avigilon Corporation | Hash-based appearance search |
EP3807782A1 (en) * | 2018-07-17 | 2021-04-21 | Avigilon Corporation | Hash-based appearance search |
EP3807782A4 (en) * | 2018-07-17 | 2022-03-23 | Avigilon Corporation | Hash-based appearance search |
US20210157833A1 (en) * | 2019-11-26 | 2021-05-27 | Dash Hudson | Visual image search using text-based search engines |
US11574004B2 (en) * | 2019-11-26 | 2023-02-07 | Dash Hudson | Visual image search using text-based search engines |
Also Published As
Publication number | Publication date |
---|---|
US8983941B1 (en) | 2015-03-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9043316B1 (en) | Visual content retrieval | |
US10311096B2 (en) | Online image analysis | |
Anandh et al. | Content based image retrieval system based on semantic information using color, texture and shape features | |
US9176969B2 (en) | Integrating and extracting topics from content of heterogeneous sources | |
WO2013129580A1 (en) | Approximate nearest neighbor search device, approximate nearest neighbor search method, and program | |
EP3166020A1 (en) | Method and apparatus for image classification based on dictionary learning | |
CN109271486B (en) | Similarity-preserving cross-modal Hash retrieval method | |
Yang et al. | An improved Bag-of-Words framework for remote sensing image retrieval in large-scale image databases | |
CN108595688A (en) | Across the media Hash search methods of potential applications based on on-line study | |
WO2020114100A1 (en) | Information processing method and apparatus, and computer storage medium | |
CN102890700A (en) | Method for retrieving similar video clips based on sports competition videos | |
Picard et al. | Efficient image signatures and similarities using tensor products of local descriptors | |
CN110825894A (en) | Data index establishing method, data index retrieving method, data index establishing device, data index retrieving device, data index establishing equipment and storage medium | |
US20180276244A1 (en) | Method and system for searching for similar images that is nearly independent of the scale of the collection of images | |
Chen | Scalable spectral clustering with cosine similarity | |
Faheema et al. | Feature selection using bag-of-visual-words representation | |
Baena-García et al. | TF-SIDF: Term frequency, sketched inverse document frequency | |
US20200364259A1 (en) | Image retrieval | |
Li et al. | Sketch4Image: a novel framework for sketch-based image retrieval based on product quantization with coding residuals | |
Xiao et al. | Complementary relevance feedback-based content-based image retrieval | |
Elleuch et al. | Multi-index structure based on SIFT and color features for large scale image retrieval | |
Li et al. | Near duplicate image detecting algorithm based on bag of visual word model | |
Arun et al. | Optimizing visual dictionaries for effective image retrieval | |
US11967128B2 (en) | Decompositional learning for color attribute prediction | |
Shaikh et al. | Contemporary integration of content based image retrieval |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MURPHY-CHUTORIAN, ERIK;ROSENBERG, CHARLES J.;PETROVIC, NEMANJA;AND OTHERS;SIGNING DATES FROM 20120329 TO 20120402;REEL/FRAME:028081/0504 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |