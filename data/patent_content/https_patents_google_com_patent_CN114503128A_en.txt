CN114503128A - Accelerating embedded layer computations - Google Patents
Accelerating embedded layer computations Download PDFInfo
- Publication number
- CN114503128A CN114503128A CN202080060592.XA CN202080060592A CN114503128A CN 114503128 A CN114503128 A CN 114503128A CN 202080060592 A CN202080060592 A CN 202080060592A CN 114503128 A CN114503128 A CN 114503128A
- Authority
- CN
- China
- Prior art keywords
- input
- address
- neural network
- processor
- batch
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/02—Digital function generators
- G06F1/03—Digital function generators working, at least partly, by table look-up
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5083—Techniques for rebalancing the load in a distributed system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
- G06N20/10—Machine learning using kernel methods, e.g. support vector machines [SVM]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
Methods, systems, devices, and computer-readable media for performing neural network computations using a system configured to implement a neural network on hardware circuitry are described. The system includes a host receiving a batch input to a neural network layer. Each input is stored in a memory location identified by an address. The system identifies one or more duplicate addresses in one or more entered address lists. For each duplicate address, the system generates a unique identifier that identifies the duplicate address in the address list. The system (i) obtains a first input from a memory location identified by an address corresponding to the unique identifier; and (ii) generating an output of the layer from the obtained first input.
Description
Cross Reference to Related Applications
This application claims priority to U.S. application 62/909,667 filed on 2.10.2019, which is incorporated herein by reference.
Background
This specification relates generally to performing neural network computations using circuitry.
Neural networks are machine learning models that employ one or more layers of nodes to generate outputs, such as classifications, for received inputs. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as an input to one or more other layers in the network, such as other hidden layers or output layers of the network. Some layers of the network produce outputs from the received inputs in accordance with the current values of the respective parameter sets.
Some neural networks are Convolutional Neural Networks (CNNs) (e.g., for image processing) or Recurrent Neural Networks (RNNs) (e.g., for speech and language processing). Each of these neural networks includes a respective set of convolutional or recurrent neural network layers. The neural network layer may have a set of associated kernels (Kernel) and an embedding layer for processing the inputs to generate a set of vectors for training the neural network. The kernel can be expressed as a Tensor (Tensor) of weights, i.e. a multidimensional array. For example, the embedding layer may process a set of inputs, such as inputs of image pixel data or activation values generated by the neural network layer. The set of inputs or the set of activation values may also be denoted as a tensor.
Disclosure of Invention
Techniques for accelerating computation of an embedding layer of an artificial neural network are described herein. The techniques may be used in a computing system, such as a large scale distributed system, that includes circuitry configured to perform deduplication (deduplicating) operations on a list of addresses. During processing of input at an embedding layer of a neural network, such as during example forward pass computation (forward pass computation) of the embedding layer, deduplication operations are performed to reduce load imbalance on a distributed system.
This specification describes methods, systems, and devices, and computer readable media for performing neural network computations using a system configured to implement a neural network on hardware circuitry. The system comprises a host computer, wherein the host computer receives batch input of a neural network layer. Each input is stored in a memory location identified by an address. The system identifies one or more duplicate addresses in one or more entered address lists. For each duplicate address, the system generates a unique identifier that identifies the duplicate address in the address list. The system (i) obtains a first input from a memory location identified by an address corresponding to the unique identifier; and (ii) generating an output of the layer from the obtained first input.
One aspect of the subject matter described in this specification can be embodied in a method that performs neural network computations using a system configured to implement a neural network on hardware circuitry. The method comprises the following steps: a host receives batch inputs to a neural network layer, wherein each input in the batch inputs is stored in a memory location identified by an address; identifying one or more duplicate addresses in the list of addresses for one or more of the batch entries. For each duplicate address, the method comprises: generating a unique identifier that identifies the duplicate address in the address list; for the batch input, obtaining a first input from a memory location identified by an address corresponding to the unique identifier; and generating an embedded output of the neural network layer from the obtained first input.
Each of these and other implementations can optionally include one or more of the following features. For example, in some embodiments, the neural network layer is an embedded layer of the neural network, the embedded output includes embedded feature vectors, and the method further comprises a scatter circuit of the system determining an inverse mapping of the unique identifier to a duplicate address of a particular input of the batch of inputs to the embedded layer; and generating one or more embedding vectors as an output of the embedding layer based on the inverse mapping of the unique identifier to the duplicate address of the particular input.
In some embodiments, the method comprises: generating a filtered list of addresses for the batch input based on one or more unique identifiers; determining address partitions between each of a plurality of processors for addresses in the filtered list of addresses; and wherein obtaining the first input comprises: for each address partition, a first input is obtained from a memory location identified by an address in the address partition, using a respective processor allocated to retrieve inputs from the address partition.
In some embodiments, generating the embedded output of the neural network layer based on the inverse mapping comprises: parsing the address list including the duplicate address to map the embedded feature vector generated for the duplicate address corresponding to the unique identifier back to the particular input of the batch of inputs of the neural network layer.
The method further comprises the following steps: the request processor providing to the remote processor an address of a first input corresponding to the unique identifier and a request for the first input; the request processor receiving the first input from the remote processor, the first input stored in a memory location of a data slice allocated to the remote processor; and performing one or more reduction operations to generate the embedded output of the neural network layer from a first input obtained from the data slice.
In some embodiments, the address list is used to form a batch input of an input feature sample comprising a plurality of sets of input features, and generating the unique identifier comprises: generating the unique identifier for each duplicate address on each respective set of input features in the sample of input features. In some embodiments, determining the reverse mapping of the unique identifier to a duplicate address comprises: determining the reverse mapping of the unique identifier to a duplicate address of a particular input of the plurality of sets of input features.
In some examples, each address partition is assigned to a data slice corresponding to an activation value or a gain value of a vector element of the multi-dimensional sparse tensor; and each data slice of activation or gain values is assigned to a particular processor for multiplication with a corresponding weight vector to perform the neural network computation. In some implementations, the duplicate address is a particular address of a memory location of the input stored to the neural network layer, the particular address being common among corresponding data slices of each processor of the plurality of processors and the particular address being duplicate among (i) a set of input feature sets, or (ii) the input feature samples.
Generating the embedded output of the neural network layer comprises: providing a plurality of inputs from a remote processor to a requesting processor, the plurality of inputs stored in memory locations of a data slice allocated to the remote processor core; and performing a lookup to obtain a respective weight vector comprising respective weight values for multiplying with an input of the plurality of inputs to generate a partial activation for a subsequent neural network layer. In some implementations, a portion of the plurality of inputs is received from the memory location identified by an address in the original list where no duplicate address exists, or from a memory location identified by an address in the original list where no duplicate address exists.
Other embodiments and other aspects herein include corresponding systems, apparatus, and computer programs configured to perform the methods encoded on computer storage devices. A system of one or more computers may be configured by means of software, firmware, hardware or a combination thereof installed on the system such that in operation the system is caused to perform actions. One or more computer programs may be configured by having instructions which, when executed by a data processing apparatus, cause the apparatus to perform the actions.
The subject matter described in this specification can be implemented in particular embodiments to realize one or more of the following advantages. A computing system includes circuitry operable to implement a processing scheme for accelerating embedded layer processing of multiple batch inputs. The circuits and processing schemes may be used to reduce or mitigate load imbalances on multi-core processing units, such as distributed systems of processors, by filtering out duplicate addresses in an address list used to retrieve inputs to an embedded layer of a neural network.
Having multiple duplicate addresses can be particularly problematic when calculating or updating the embedding of the neural network. For example, the samples in the batch input may include a variable number of features that need to be embedded, such as in a feature space of an embedding layer of a neural network. Each feature (or input) value has an embedding, e.g., a vector of values representing trainable weights of the neural network stored in an embedding table of the neural network. While each value in a sample (or batch) has an embedding, different samples in a batch (and even different features within the same sample) may include the same feature value. In some cases, even the same feature within a sample may include the same feature value.
For example, when calculating or updating an embedding, the same feature value may need to be extracted for the same embedding table but for different features in a sample or batch, or for various features in different sample or batch inputs. Because the same input value may need to be fetched multiple times (e.g., from the same address in memory), the method for filtering out repeated occurrences of addresses may reduce the computational load on the distributed system and help improve or speed up the computation of the embedding layer of the neural network when calculating or updating the embedding.
The technique of deduplication may be performed at the source or requesting processor before the addresses in the list are broadcast to the different remote processors of the distributed system that is assigned to retrieve specific data elements of a given batch input for processing at the embedding layer. The techniques described herein may be used to more efficiently compute embedded outputs of a neural network layer from retrieved data elements input for a batch. For example, the process of generating an embedded output can include retrieving an embedding of one or more inputs in a batch, and performing a reduction on the embedding of relevant input features in the batch.
The techniques described in this specification can be used to reduce or minimize the amount of computation performed to retrieve embedded data, perform the reduction, and return a reduced vector of embedded values relative to existing systems. When generating the embedded layer output, the techniques provide an improved method of balancing the computational load between a request processor that processes a batch of inputs for a reduced embedding value and a remote processor that retrieves the embedding corresponding to the batch of inputs.
The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Potentially other features, aspects, and advantages of the present subject matter will become apparent from the description, the drawings, and the claims.
Brief description of the drawings
FIG. 1 is a block diagram of an example computing system.
FIG. 2 is a block diagram of example circuitry including an example multi-core processing unit.
FIG. 3 illustrates an example host broadcasting a set of addresses to processors of an example multi-core processing unit.
4A-4D illustrate diagrams associated with examples of processing schemes involving two or more processors of an example multi-core processing unit.
FIG. 5 illustrates an example processor of an example multi-core processing unit.
FIG. 6 is a flow diagram illustrating an exemplary process of generating an output of a neural network layer.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification describes techniques for accelerating embedded layer computations to mitigate processing delays that may occur due to processor load imbalances in a distributed computing system. For example, the delay may occur during processor computations that generate the embedded layer outputs of the multi-layer neural network. In particular, when a particular processor or core of a distributed system is required to perform a large number of query and compute operations (e.g., reduce or connect (occlusion) retrieval values) to produce an embedding, the particular processor may experience processing delays corresponding to load imbalances.
An imbalance may exist between a source processor that submits a query or request to retrieve data for an input value and a remote processor that performs a lookup to retrieve the data. The data may be an embedding of an embedded table (e.g., a weight), and the remote processor may be tasked to process the request to return the embedded data or weight value. For example, in a forward pass computation operation of the embedded layer, the remote processor processes a request from the source processor that causes the remote processor to look up an address to retrieve data stored in memory and perform a computation using the retrieved data. In existing distributed architectures, remote processors perform minimal deduplication of specified addresses in requests.
For example, in existing systems a remote processor receives one or more duplicate addresses and performs data retrieval and associated simplification operations corresponding to the entry of all received addresses (including duplicate addresses). However, the architectures and processing schemes used by some existing systems are prone to load imbalance problems, particularly when the computational load at one or more remote processors is unbalanced with the computational load at the source processor, or is unbalanced between two or more remote processors. These load imbalances can cause computational operations that generate the output of the embedding layer to stall or significantly delay.
Typically, an embedding layer of a neural network is used to embed features into a feature/embedding space corresponding to the embedding layer. A feature may be an attribute (attribute) or property (property) shared by the individual units on which analysis or prediction is to be performed. For example, an individual element may be a group of words in a vocabulary or image pixels that constitute a part of an item such as an image and other documents. An algorithm to train embedding layer embedding may be executed by the neural network processor to map features to an embedding vector. In some embodiments, the embedding of the embedded table is learned in conjunction with other layers of the neural network that are to use the embedding. This type of learning occurs by propagating gradients back to update the embedded table.
In other embodiments, the embedding may be learned independently of other layers of the neural network to be used with the embedding, such as when the embedding is pre-trained. For example, by processing information about discrete input features, the neural network processor may use the algorithm to compute embeddings to determine mappings or placements (placements) of similar inputs to embedding vectors that are geometrically close in embedding space. In some cases, the process of computing the embedding may represent a technique of feature learning or feature engineering that allows the system to automatically discover the representation required for feature detection from raw input data.
In some implementations, a given "input" may have one or more characteristics of one or more types, and the embedding layer generates a respective embedding for each of these types. For example, the input may be for a search query having several different feature types. The feature types may include attributes of the user or user device (e.g., location, preferences, device type, etc.), query tokens, previously submitted queries, or other relevant types of attributes that may correspond to search queries. For any feature type where a given input has more than one feature, the computing system may be operable to retrieve separate embeddings of each of these features. The system is further operable to combine the retrieved embeddings, for example by calculating an average of the embedding values, to generate a final embedding of the feature type.
Based on the context discussed above, this specification describes techniques for implementing dedicated hardware circuitry configured to perform de-duplication of addresses at a source processor for lookup in a portion of memory allocated to a remote processor prior to sending a list of addresses to the remote processor. The hardware circuitry includes Identifier (ID) scatter circuitry (scatter circuitry) configured to parse the original address list to filter out one or more "duplicate" addresses. The scatter circuitry is configured to filter duplicate addresses prior to bucketing addresses belonging to each remote processor.
For example, a system including hardware circuitry may generate instructions to cause scatter circuitry to bucket addresses of data fragments (Shard) belonging to particular remote processors assigned to the system. The system then sends the address to the particular remote processor assigned to the data slice, which stores the embedded vector corresponding to the input (feature) stored at the address. The embedding vector may be a weight value or an embedded portion of an embedding table that is stored on the data slice. In some cases, the embedded vector of the embedded table may be partitioned among multiple remote processors (processor nodes or cores) of the computing system. The system host is operable to send a set of addresses or vector indices (vector indices) to a subset of the remote processors to perform operations associated with retrieving a weight vector, such as an embedded vector, that is mapped to a particular feature stored at an address of the set. In some examples, the remote processor performs one or more query and compute operations prior to providing or returning the weight vector to the system host.
The hardware circuitry includes an inverse mapper configured to enhance (authority) or modify the address by adding metadata such as a hash key (Hashed key) to the address. The metadata is used to reverse map a particular set of retrieved or computed data (e.g., collected vectors) back to the corresponding original address before filtering the duplicate addresses. In some cases, the circuitry is configured to perform a Sparse reduction (Sparse reduction) operation on data received from the remote core to form a dense matrix.
For example, prior to deduplication, the circuitry is enhanced to traverse each address in the original list of address locations and collect vector outputs corresponding to particular input addresses using the metadata tags generated by the inverse mapper, and perform a reduction operation on the collected vectors. These techniques allow a particular master processor handling multiple batch inputs to perform computations in a distributed system of processors, rather than being executed by remote processors mapped to a particular portion of memory holding weight values (e.g., parameters) corresponding to one or more of the batch inputs.
FIG. 1 illustrates a block diagram of an example computing system 100, the example computing system 100 configured to retrieve data elements stored in a memory of the system 100 for performing embedding layer operations to generate an embedded set for training a neural network. In some cases, training the neural network may include determining one or more learning objectives based on an output vector, the output vector representing the neural network embedding. For example, the embedded output may correspond to one or more output feature sets that include a floating point/parameter vector for a given output dimension (64-d, 256-d, etc.).
The embedded output is generated when the neural network of the training system 100 performs certain computational functions, such as image or speech recognition. In some embodiments, training the neural network includes updating an embedded set previously stored in an embedded table of the neural network, e.g., during a previous phase of training the neural network. In other words, the embedding of the neural network may be pre-trained. For example, the embedding of the embedding layer of the neural network may not be trained with the neural network to be embedded. Thus, the techniques described in this specification can be used to update the embedding of a pre-trained neural network, improving efficiency over existing methods, such as the methods used for pre-trained embedding.
Computing system 100 includes a host 102, a multi-core processing unit 104, and data slices 106a-106k, where k is an integer greater than 1. In general, host 102 may be a processing unit, such as a processor, a plurality of processors, or a plurality of processor cores. Thus, the host 102 may include one or more processors, and the host 102 may be operable to generate or process instructions to access the target dense matrix and to send the instructions 110 to the multi-core processing unit 104 to generate the target dense matrix. As described in more detail below, performing the embedding layer operations may include transforming sparse elements from one or more matrices to generate dense matrices.
The multiple core processing unit 104 accesses a respective element 108a-108n from one or more of the data slices 106a-106k, where n is an integer greater than 1. The multiple core processing unit 104 generates the destination dense matrix 112 using the corresponding elements 108a-108n and provides the destination dense matrix 112 to the host 102 for further processing. The elements 108a-108n may be two-dimensional matrices having different sizes, and the multi-core processing unit 104 may generate the destination dense matrix 112 by transforming each of the elements 108a-108n into a vector, and concatenating (contracting) the n vectors into a single vector.
Typically, in the case of embedding, the "sparse" information corresponding to the sparse elements may be a single-hot-spot (one-hot) vector that identifies the eigenvalues. For example, if there are five possible values for a given feature (e.g., a, B, C, D, E), the sparse vector identifies the feature value "a" as (1, 0, 0, 0, 0, 0), and the embedding layer maps (1, 0, 0, 0, 0) to a dense embedded vector of the feature value "a". In some implementations, during training of the embedding layer to learn embedding, elements 108a-108n may be weight values of an embedding table that is converted into a vector, which is an embedding vector such as feature value "B" or "C". The weight values may be transformed using a neural network processor of the multicore processing unit 104, and the multicore processing unit 104 executes a training algorithm based at least on the mapping of the features to the embedding vectors to compute the embedding.
The host 102 may process the instruction to update the target dense matrix and send the updated dense matrix to the multi-core processing unit 104. For example, the target dense matrix may correspond to an embedding of a pre-trained neural network. Thus, the host 102 may process the instructions to update the embedding of the pre-trained neural network to generate an updated dense matrix. For example, during a subsequent iteration (subsequential iteration) of training the neural network to update the embedding, a back pass (backward pass) may be performed to update the embedding by determining a new mapping of the input features to the embedding vector, and generating an updated dense matrix based on the new mapping. In some implementations, the multi-core processing unit 104 may be operated to convert the updated dense matrix into corresponding sparse elements and update one or more sparse elements (e.g., weights) stored in the data slices 106a-106k accordingly.
As described above, host 102 is configured to process instructions for execution within computing system 100. In some implementations, the host 102 is configured to process the target dense matrix 112 generated by the multi-core processing unit 104. In some other embodiments, the host 102 may be configured to request the multi-core processing unit 104 to generate the destination-dense matrix 112, and another processing unit may be configured to process the destination-dense matrix 112.
Each processor of the multiple core processing unit 104 is configured to retrieve data elements stored in a memory of the system 100. The memory may include a plurality of data slices 106a-106k that store data including elements 108a-108 n. The data may include inputs, activations, gain values, or weight values corresponding to parameters or kernels (Kernel) of the weight matrix structure. In some implementations, the data slices 106a-106k may be a volatile memory unit or units. In some implementations, the data slices 106a-106k may be a non-volatile memory cell or cells. The data slices 106a-106k may also be another form of computer-readable medium, such as a device in a storage area network or other configuration. The data slices 106a-106k may be coupled to the multi-core processing unit 104 using electrical, optical, or wireless connections. In some implementations, the data slices 106a-106k may be part of the multi-core processing unit 104 and are based on a Processor-in-memory (PIM) architecture.
The multi-core processing unit 104 is configured to determine a dense matrix based on the sparse elements. In some embodiments, the multiple core processing unit 104 may be configured to determine the locations of the sparse elements based on the dense matrix. The multiple core processing unit 104 includes a plurality of interconnected processors or processor cores. For example, the multiple core processing unit 104 may be a distributed processing system including multiple interconnected processor cores. In general, the terms "processor" and "processor core" are used interchangeably to describe the discrete interconnected processing resources of multi-core processing unit 104.
FIG. 2 is a block diagram of an example application specific circuit 200 including an example multi-core processing unit 104. In general, computing system 100 may be implemented using one or more circuits 200 (described below). In the embodiment of FIG. 2, multi-core processing unit 104 has an interconnected set of processors including processors 0 through 15. As described in more detail below, each of the processors 0-15 of the multiple core processing unit 104 may be assigned to one or more of the data slices 106a-106k to obtain data (e.g., eigenvalues and weights) stored in the memory locations of the data slices assigned to the processor. Each processor of the multi-core processing unit 104 may be operated to fetch (fetch) or retrieve data from a memory location identified by a request received from the host 102 or from another processor in the multi-core processing unit 104.
The processors cooperate to retrieve data elements that are mapped to the embedded table. Data elements are retrieved to perform embedding layer operations, such as computations for generating embedded output vectors (e.g., dense vectors) representing a neural network. In some implementations, the embedded output is a mapping of discrete inputs in the bulk input to dense vectors of numeric values (e.g., floating point values) that can be used to define relationships between objects such as terms or words in a vocabulary. For one input in a given batch, the embedding may be a vector, while for the input of the entire batch, the embedding may be a matrix, e.g., a collection of vectors. In some embodiments, the vector of embedded values corresponds to trainable weights learned by the neural network during training. The learned weights may encode one or more words by looking up dense vectors stored in an embedded table, where a dense vector corresponds to a particular word. The plurality of dense vectors may correspond to a dense matrix used to train the neural network to perform a particular task or function, which is related to recognizing word sequences, such as speech processing.
In some examples, the embedded layer computation is a large-scale data lookup problem that is typically used during the training phase of the neural network. For example, during training, a large weight value table may be sliced (shard) or partitioned (partition) in multiple dimensions of the multidimensional tensor, such as in the x, y, or z dimensions of the multidimensional tensor. In particular, the table of weight values may be sliced across the data slices 106a-106k such that the respective weight values are stored at corresponding locations in a memory that includes the data slices.
Further, one or more of the data slices 106a-106k may be partitioned into specific processors 0-15 in the multi-core processing unit 104. Each slice or partition of the table corresponds to one data slice 106k that is assigned to a particular processor or processor core of the multi-core processing unit 104. In some examples, each processor core is a respective neural network processor. In other examples, each processor core is a respective core of a single neural network processor. In general, the computing system 100 may include multiple processors or multiple processor cores that are configured based on the desired configuration of the system.
Referring again to fig. 2 and hardware circuitry 200, the circuitry includes an Identifier (ID) scatter circuit 202 ("scatter circuit 202") that includes a hash generator 204 and an inverse mapper 206. Each of the hash generator 204 and the inverse mapper 206 is described below.
The scatter circuitry 202 is configured to process a list of original addresses that may be received by the host 102. The list of original addresses identifies memory locations in a memory of the system 100, such as a memory including the data slices 106a-106 k. In FIG. 2, the dispersion circuit 202 is shown external to the host 102 and the multiple core processing unit 104 or separate from the host 102 and the multiple core processing unit 104. However, in some embodiments, the dispersion circuit 202 may be part of the host 102, such as a dispersion unit or computing module, or a sub-circuit of the host 102.
Similarly, in some embodiments, the dispersion circuit 202 may be included in the multiple core processing unit 104 as part of a processor in the multiple core processing unit 104. For example, the dispersion circuit 202 may be included in a source/host processor (e.g., processor 0) of the multi-core processing unit 104. The source processor may be tasked by the host 102 to generate one or more requests and provide the requests to a plurality of other processors (e.g., processors 1-15) in the multi-core processing unit 104 to retrieve data elements from memory locations of the data slices 106a-106k assigned to those processors.
The scatter circuit 202 communicates with the host 102 to receive the original address list. For example, the host 102 receives batch inputs (a lots of inputs) to the neural network layer, such as from an external source or controller of the system 100 that manages machine learning operations performed by the system. For example, machine learning operations may be used to train a neural network to process image or audio data so that the neural network may learn how to recognize particular objects in the image/video or how to detect certain word sequences in the audio stream.
The host 102 receives input of one or more lots. In some implementations, the host 102 receives multiple batches of input corresponding to one or more samples. For example, a first set of multiple batch inputs may correspond to one sample 210-1, while a second set of multiple batch inputs may correspond to another sample 210-2. Each sample 210-1, 210-2 may include input corresponding to a set of features of an object (e.g., an image or audio stream). For example, each of the samples 210-1 and 210-2 may include a respective batch input of features corresponding to features of different regions of the same image, or corresponding to different regions of multiple different images.
Each of the inputs in a batch of inputs is stored in a memory location, and that location may be identified by an address (e.g., [99], [736], [37], [99], etc.) in the original list of addresses received by scatter circuit 202 from host 102. In some embodiments, each of the inputs has a corresponding set of weights, which may be stored as data or sparse elements of an embedded table, which is sharded at data shards 106a-106k and partitioned at processors 0 through 15. In some embodiments, the embedding of various features in the batch input is stored in a distributed manner, e.g., various data shards across the system 100.
For example, features may be distributed across multiple processor nodes or cores (0-15), with each node being allocated a memory location for storing the location of the embedded slice. As described in more detail below, the host 102 (e.g., a source processor) provides a request including one or more subsets of addresses to one or more of the processors 0-15 (e.g., remote processors) for processing to retrieve data elements used to generate the dense matrix.
As noted above, in some existing systems, a remote processor receiving a duplicate address performs data retrieval, arithmetic operations (e.g., multiplication), and associated reduction operations on the received address including the duplicate address. This can lead to load imbalance problems, particularly when the computational load at the remote processor is unbalanced with the computational load at the requesting or source processor.
The scatter circuitry is configured to parse the original address list to filter out one or more "duplicate" addresses. The list of addresses received by the host 102 includes one or more duplicate addresses corresponding to the input features in the batch. For example, the original address list received at the system 100 may include the addresses [26], [96], [99], [1], [7], [312], [99] and [912] of the batch input. In this example, the original address list includes addresses [99] that are duplicate addresses in the list. The scatter circuitry 202 is configured to filter duplicate addresses before bucketing addresses belonging to each remote processor and providing requests to remote processors to retrieve data elements corresponding to packet addresses.
The scatter circuitry 202 identifies one or more duplicate addresses in the address list for each of the batch inputs. For example, a single embedded table may be referenced by multiple features. Thus, there may be duplicate addresses on a number of different features. For example, the embedded table T may be referenced by features F1 and F2. In the present example, the address (e.g., [99]) may be repeated within the input of F1 (in one or more samples) and within the input of F2 (in one or more samples).
The scatter circuitry 202 is configured to perform de-duplication of the original address by filtering out duplicate addresses and providing the unique address (identifier) to the multi-core processing unit 104 for processing at one or more of the processors 0-15. For each duplicate address: the scatter circuitry 202 generates an identifier that uniquely identifies a duplicate address in the original address list. Then, based on certain identifiers indicating duplicate addresses in the original address list, scatter circuitry 202 filters the original address list to generate a filtered address list such that the filtered address list includes only unique identifiers corresponding to non-duplicate addresses.
More specifically, for each address in the original list, scatter circuitry 202 uses Hash generator 204 to generate an identifier corresponding to a Hash (Hash) ID. The memory of system 100 may have address values (e.g., [99]) for different memory offsets (offsets) (e.g., Offset 0, Offset 1, etc.). The hash generator 204 may generate the hash ID based on the address value, a memory offset of the address value, or a combination of each. For example, for repeated occurrences of addresses in the list, a portion of the hash ID generated based on the address value (e.g., [99]) will be repeated, but a portion of the hash ID will be different than a second portion of the hash ID generated for a memory offset based, or a combination of the address value and the memory offset. In some examples, scatter circuitry 202 may append the respective first portion of the hash ID to an address in the original list as a prefix of the address. The scatter circuitry 202 may recursively (recursively) scan the list of addresses to identify repeat occurrences of the appended prefixes and filter out the particular address based on the repeat occurrences.
In some implementations, the first portion of the appended prefix, or hash ID, is an example of a metadata tag that is used by scatter circuitry 202 to enhance the addresses in the original list to generate a filtered list of addresses. Based on the unique identifier generated by the hash generator 204, the scatter circuitry 202 generates a filtered list of addresses for the batch input. The filtered list of addresses includes only unique identifiers or addresses (e.g., non-duplicate addresses) of the memory locations of the data slice 106. The filtered list of addresses is then provided to multi-core processing unit 104 so that processors 0-15 receive only unique addresses and not multiple occurrences of duplicate addresses.
In this manner, the circuit 200 provides dedicated hardware that is operable to traverse each address in the original list of address locations prior to deduplication, and collect vector outputs corresponding to particular input addresses using the metadata tags generated by the inverse mapper 206. As described in more detail below, the circuit 200 is operable to perform a reduction operation on the collected vectors to generate a dense matrix. For example, an example source processor of the host 102 or the multi-core processing unit 104 may include an example sparse reduction circuit (sparse reduction circuit) configured to perform a reduction of data elements received from a remote core to form a dense matrix. This will be described in more detail below with reference to fig. 5.
FIG. 3 illustrates a host 102 broadcasting a set of addresses to each of processors 0, 4, 8, and 12 of a multi-core processing unit 104. As shown in FIG. 3, the host 102 sends the unique identifier or address of the batch input to the processors of the multi-core processing unit 104. In some implementations, the host 102 can send a unique identifier for the respective batch input corresponding to each of the sample 210-1 and the sample 210-1. For example, host 102 may send addresses [0, 26, 96, 99] of batch input 302 to processor 12 and addresses [2, 9, 17, 736] of batch input 304 to processor 0. As shown in FIG. 2, the address of the batch input 302 is for sample 210-1, and the address of the batch input 304 is for sample 210-2.
In some embodiments, the host 102 may be operated to provide different batches of input to different processors and different column group (column group) processors in the multi-core processing unit 104. For example, the host 102 may send different batches of input to different processors in a sequential manner or in a parallel manner. Similarly, the host 102 may send different batches of input to different column group processors in a sequential manner or in a parallel manner.
4A-4D each illustrate a respective diagram associated with an example of a processing scheme 402, which processing scheme 402 may be implemented at system 100 using at least two or more processors in a host 102 and a multi-core processing unit 104.
As described above, processors 0-15 cooperate to retrieve data elements mapped to an embedded table, such as a sparse embedded table that includes a plurality of sparse elements. The data elements are retrieved in response to the remote processor processing a request provided by the source (or requesting) processor. In some cases, data elements are retrieved to perform embedding layer operations, such as with calculations that produce output vectors (e.g., dense vectors) representative of neural network embedding, or calculations for producing partial activations to generate partial activations in response to multiplying inputs (e.g., gain values or activation values) with weight vectors.
As described above, the embedded table may be a table of weight values that are sliced between the data slices 106a-106k such that the respective weight values are stored in corresponding locations in memory. The data slices 106a-106k may be partitioned to particular processors 0-15 in the multi-core processing unit 104 such that the weight values of the embedded tables correspond to the data slices assigned to the particular processors 0-15 of the multi-core processing unit 104.
As described below (and with reference to FIG. 5), a request provided by a source processor to a remote processor to retrieve a data element may include an address (e.g., [912]) that identifies a memory location of the data slice. The memory locations store incoming data, such as gain values or activations. The request is provided to cause the remote processor to retrieve a data element representing an input, such as one of a plurality of inputs in a batch input received at the host 102. In some implementations, the request is provided to cause the remote processor to perform a lookup operation to retrieve a particular or unique weight corresponding to a particular input of the batch inputs.
In the example of FIG. 4A, block 402 illustrates an exemplary processing scheme in which at least processors 0-3 cooperate to retrieve data elements mapped to a portion of an embedded table. In this example, processor 0 may be the source (or requesting) processor that submits respective requests to each of processor 1, processor 2, and processor 3. More specifically, processor 0 is a source processor, and each of processor 1, processor 2, and processor 3 is a remote processor with respect to processor 0.
In some cases, processors 0, 1, 2, and 3 form subset 404. In this subset 404, each of processor 0, processor 1, processor 2, and processor 3 may be a source processor relative to the other processors in the subset 404, where the other processors are remote processors that receive and process requests from the source processors in the subset 404. In some cases, the source and remote processors in the subset 404 may still receive and process requests from another processor in the multi-core processing unit 104 or from the host 102, where the host 102 may be a master, source processor not included in the subset.
The exemplary scheme illustrated at block 402 may be part of a processing technique implemented at the system 100 to generate an output of a neural network layer, such as an embedded output embedded in the neural network layer or an output of a first hidden layer of the neural network. The processing techniques include one or more remote processors that provide a data element including a plurality of inputs and corresponding query values to a requesting processor. The data elements are stored in memory locations of the data slice allocated to the remote processor.
In some implementations, in response to each remote processor performing a lookup, data elements are provided to a source processor to obtain a weight vector that includes a respective weight value multiplied by a particular input of the plurality of inputs. For example, a remote processor may be assigned a portion of memory that holds a kernel matrix (kernel matrix) weight that is multiplied by the activation stored at the corresponding address in the address list received by the source processor. The inputs are multiplied by the weight values, e.g., a subsequent neural network layer (described at least with reference to fig. 4D below) generates a partial activation. Example operations will be described to further illustrate the processing technique.
During exemplary operation at the system 100 processing multi-batch inputs, an original list of multiple addresses is received by the host 102 or another source processor, such as processor 0 of the multi-core processing unit 104. The address [912] may be included in a plurality of addresses in the list. For a given batch or sample, [912] may appear more than once in the received list. In particular, the address [912] may be one of a plurality of addresses that are duplicated between (i) a set of input features or (ii) samples of input features. In some implementations, each repeated occurrence of address [912] in a batch or sample can be used for a particular memory Offset (Offset) in a given data slice.
The address [912] may be a location in memory where an input/feature value is stored, the input/feature value being an identifier (e.g., a thermal identifier) of a word in a given vocabulary, the word being, for example, the word "car". For example, the embedding table may map a hot identifier of a word to an embedding. More specifically, the address [912] may store instances of input features that occur multiple times within the feature values of a given sample and/or within the input of a given batch. Each occurrence of the address [912] may be a particular weight or vector with embedded features, e.g., trainable weight values stored in an embedding table in memory.
The system 100 may perform deduplication on the original list to remove multiple occurrences of the address [912], thereby generating a filtered list of addresses that includes only unique addresses. The reverse mapper 206 may append a metadata tag to a particular occurrence of the address [912] in the batch or sample to map the collected vector (e.g., weight vector) back to the particular occurrence of the address [912] in the original list. As described in more detail below, the example source processor may use the reverse map to perform computations locally, such as multiplying between: (i) the input value stored at this particular occurrence of address [912], and (ii) the collected vector received from the remote processor.
In general, address [912] may identify a memory location of a data slice allocated to processor 0, the memory location storing a particular input value. In some embodiments, address [912] in the data slice assigned to processor 0 may also be repeated, for example, 100 times in each batch of the multi-batch input, but with reference to a different data slice assigned to the other processors of processing unit 104. For example, address [912] may be a specific address that is common among respective data slices allocated to one or more processors in multi-core processing unit 104.
An example source processor, such as host 102 or processor 0, may perform 16 memory lookups, one for each processor in the multi-core processing units 104 of system 100, but will not perform computations related to reduction or matrix multiplication. In the example of fig. 4A (and 4C discussed below), processor 3 may only send processor 0 its respective input values (e.g., activation or gain) and weight values corresponding to unique addresses that may be generated for multiple occurrences of address [912] in a particular batch.
In the example of fig. 4B, remote processor 104n is configured to locally reduce its own lookup value based on instructions from the requesting processor. In some implementations, each of the processors in the multi-core processing unit 104 may represent a neural network processor chip that is assigned to lookup and Locally (Locally) reduce data elements stored at a particular data slice 106a-106 k. The respective remote processor is configured to find and locally reduce its embedded table data elements based on instructions provided by the host 102 or based on instructions received directly from the requesting/source processor in the subset comprising the respective remote processor.
For example, processor 3 may be a respective remote processor in a subset of processors including processor 0, processor 1, and processor 2. In this example, processor 0 may be a requesting processor in a subset that includes processor 3, processor 3 being a remote processor relative to at least processor 0. In this manner, processor 3 is configured to look up and locally reduce data elements, such as unique weight values stored at the input of address [912], to generate an exemplary weight vector [ f2 ].
In the example of fig. 4C, a source or requesting processor (e.g., processor 0) receives locally reduced data elements at each remote processor and performs one or more operations on these data values. In some embodiments, the request handler is configured to perform an additional reduction operation on the received value. For example, when the processor 3 retrieves and locally reduces the input stored at address [912] and the corresponding lookup data to generate the value of vector [ f2], the system 100 may be operated to determine a reduction in the input relative to the corresponding sample or batch input to which the input is assigned.
For example, using request processor 0 of subset 404, system 100 may determine that input [912] needs to be reduced by five times for sample 210-1 and two times for sample 210-2. Additionally, request processor 0 is configured to perform the decrementing of all data values received from each remote processor in subset 404. For example, the request processor 0 is configured to locally reduce each of the weight vector [ f2], the weight vector [ g1], and the weight vector [ f1] received from the remote processor 1, the remote processor 2, and the remote processor 3, respectively. In some embodiments, request processor 0 is configured to look up its own data elements, such as the input unique weight values stored at addresses [1], [7], [2], [9] and [17], and locally reduce the look-up values on these features to generate example weight vectors [ f0] and [ g0 ]. The requesting processor may perform the reduction on its own lookup value and the data value received from each remote processor in the subset 404.
In the example of fig. 4C, each of the subsets 406 and 408 may be used to implement a processing scheme similar to or the same as the processing scheme 402 described above. For example, in subset 404 processor 7 is configured to look up and locally reduce the unique weight value of the input of the data element, such as stored at address [37], to generate an exemplary weight vector [ e1 ]. In subset 406, the source or requesting processor (e.g., processor 4) receives the data elements of the locally reduced vector [ e1] at remote processor 7 and performs one or more operations on these data values. Processor 4 may then generate exemplary weight vectors [ d0] and [ e0] based on one or more cascading or reducing operations performed for the embedding retrieved for various input values in the batch or sample.
In some embodiments, each of processors 1-15 may send its respective input value (e.g., activation or gain) and weight value to processor 0, the input value and weight value corresponding to a unique address that may be generated for multiple iterations of address [912] in a particular batch. In other embodiments, each of processors 0-15 may be assigned a respective batch for processing, and each processor will perform its respective computation for 100 iterations of address [912] in its respective batch.
The processing techniques described in fig. 4A-4D allow a particular master processor possessing multiple batch inputs to process calculations in a distributed system of processors, rather than being executed by remote processors mapped to particular portions of memory that hold weight values (e.g., parameters) corresponding to one or more of the batch inputs. These techniques may also translate into improved load balancing and scalability of the system 100.
In the example of fig. 4D, processor 12 may represent a processing node of an example distributed processing system of system 100. In some embodiments, the processor 12 may be a master processor that owns the processing of multiple batches of inputs. The processors 12 are configured to receive each of the vector outputs (e.g., dense vectors), which are reduced by a respective requesting processor 104n in the multi-core processing unit 104. More specifically, the processor 12 may receive vector outputs corresponding to the data elements of each output matrix (representing the embedding of the neural network), the vector outputs being reduced locally by the respective remote processor.
The processor 12 is configured to perform a calculation for generating a partial activation, in response to multiplying an input, e.g. a gain value or an activation value, with a weight value of a weight vector, thereby obtaining a partial activation. For example, the processor 12 is configured to generate one or more sets of partial activations based on multiplication between the inputs and the weight values of the weight vectors [ a ] and [ b ].
FIG. 5 illustrates an example processor 104n of the multiple core processing unit 104, where n is an integer greater than 1. For example, the processor 104n may be any of the processors 0-15 described above with reference to FIG. 2. In general, the processor 104n is configured to receive a request 542 from the node network 520 to extract or retrieve data (e.g., sparse elements) stored in one or more data slices 530.
For example, for a dense matrix generated using sparse elements, the source processor 502 sends a request 542 for processor 104n in the node network 520. The network of nodes 520 may be a two-dimensional mesh (mesh) network. In some embodiments, the request to extract data 542 includes an instruction to transform the extracted sparse elements into a dense matrix. As described above, the request to transform the extracted sparse elements into a dense matrix may be performed during training of the embedding layer to learn embedding. For example, the extracted elements may be weight values of an embedding table that is transformed into an embedding vector based at least on a mapping of the input features to the embedding vector. The source processor 502 may broadcast a request 542 to the processor 104 n. The routing of broadcast request 542 may be similar to that described with reference to FIG. 2 for host 102 providing requests to one or more processors 104 n. In some implementations, the source processor 502 may be the host 102 or another processor 104n included in the multi-core processing unit 104.
In general, the processor 104n is configured to receive a request 542 to extract data elements stored in one or more data slices 530, and to determine whether the processor 104n is assigned an address to access a location in memory, the memory storing data indicated by the request 542. In some implementations, the processor 104n can determine whether it is assigned to access the data element indicated by the request 542 using a lookup table. For example, if the address of the data element of the particular request (e.g., address [99] at memory offset 1 of lookup table No. 5) is included in the lookup table, the processor 104n may send a signal to the example data retrieval unit of the processor to cause the data retrieval unit to retrieve the data element of the particular request. In some implementations, the processor 104n can be configured to broadcast the received request 542 to another processor 104n on the node network 520.
The processor 104n is configured to retrieve one or more requested data elements from a particular data slice 530 assigned to the processor 104n in response to the processing request 542. In some implementations, the processor 104n is a Vector processor, a Vector Processing Unit (VPU), an array Processing Unit, or any suitable data computing resource for Processing arrays and matrices of Vector elements. In some implementations, each of the processors 104n included in the multi-core processing unit 104 may be assigned to a particular data element stored in the data slice 530, such as a particular data element of an embedded table, based on the address and corresponding memory location used to store the data element.
In some implementations, the source processor 502 can generate one or more requests 542 for data elements corresponding to multiple batches of input to be retrieved and processed using one or more processors 104 n. In some cases, each batch of input may be associated with a respective request 542 for a subset of the requested data elements. Each processor 104n is configured to independently retrieve the assigned data element from its assigned data slice 530. In some embodiments, the processor 104n causes the retrieved data to be forwarded to one or more units of the processor for further processing, as described in more detail below.
As shown in fig. 5, the processor 104n includes a sparseness reduction unit 506, a concatenation unit 508, and a compression/decompression unit 510. The processor 104n may forward the retrieved data elements to the sparsity reduction unit 506. The sparsity reducing unit 506 is configured to reduce the dimensionality of the retrieved data elements. For example, the processor 104n may generate vector elements having dimensions of 100 × 1. The sparsity reduction unit 506 is operable to receive extracted data elements having dimensions of 100 x k and to reduce the dimensions of the extracted data elements 346 to 100 x 1 by logical operations, arithmetic operations, or a combination of both to produce sparsity reduction elements 548. The sparseness reduction unit 506 is configured to output a sparseness reduction element 548 to the concatenation unit 508.
The concatenation unit 508 is configured to rearrange and concatenate the sparsity reducing elements 548 to generate a concatenation element 550. For example, the processor 104n may be configured to access data elements including inputs to the neural network layer and a corresponding set of weight values (e.g., sparse elements No.1 through No.100 of database table No. 1). The processor 104n is operable to return the retrieved sparse element No.10 (e.g., the first weight value) to the sparsity reduction unit 506 before returning the retrieved sparse element No.5 (e.g., the second weight value). The concatenation unit 508 is configured to rearrange the later-received sparse element No.5 to be sorted before the earlier-received sparse element No.10, and is configured to concatenate the sparse elements No.1 to No.100 as the concatenation element 550.
The compression/decompression unit 510 is configured to compress the concatenated elements 550 to generate a dense matrix 552 of the network of nodes 520. For example, the compression/decompression unit 510 is operable to compress the zeros in the concatenation element 550 to improve the bandwidth of the node network 520. In some cases, the plurality of zeros may be generated in response to an arithmetic operation between the input and the weight, such as a multiplication between the input and the weight value having a zero value, the arithmetic operation generating a matrix having a plurality of zeros as the concatenation elements 550 of the multiplication result. The compression/decompression unit 510 may compress the matrix to generate an exemplary dense matrix 552 of the node network 520.
In some embodiments, compression/decompression unit 510 may decompress the received dense matrices. For example, a first processor 104n (e.g., processor 9 in fig. 2) may receive a dense matrix from a neighboring second processor 104n (e.g., processor 10 in fig. 2) via a node network 520. The neighboring second processor 104n may decompress the received dense matrix and may concatenate the decompressed dense matrix with the concatenation elements 550 to form updated concatenation elements that may be compressed and then output to the node network 520.
FIG. 6 is a flow diagram illustrating an exemplary process 600 of generating an output of a neural network layer. The process 600 may be implemented or performed using the system 100 described above. The description of process 600 may refer to the computing resources of system 100 described above. In some implementations, the steps or actions of the process 600 are enabled by programmed firmware or software instructions that are executable by one or more processors and resources of the devices described in this document.
Referring now to process 600, a host of system 100 receives batch inputs to a neural network layer (602). For example, the host 102 may receive one or more batches of input for processing by the multi-core processing unit 104. Each input in the one or more batches of inputs is stored in a memory location identified by an address. For at least one batch input, circuitry of system 100 identifies one or more duplicate addresses in the address list for each input in the batch input (604). For example, scatter circuitry 202 may be operative to scan the address list to identify one or more duplicate addresses in the address list for each of the batch inputs.
For each duplicate address identified, the circuitry generates a unique identifier that identifies the duplicate address in the address list (606). For example, the scatter circuitry 202 may be operable to generate a respective unique identifier for each duplicate address to identify a particular address as a duplicate in the address list. The address list may be used to form a batch input of a sample of input features, the input features comprising a plurality of sets of input features. The circuitry of system 100 is operable to generate a unique identifier for each duplicate address in a corresponding set of input signatures in the input signature sample.
Based on the one or more unique identifiers generated by the circuit, the system 100 generates a filtered list of addresses for the lot input (608). For example, host 102 interacts with scatter circuitry 202 to receive a respective unique identifier for each duplicate address and to generate a filtered list of addresses for the batch input based at least on the one or more unique identifiers generated by scatter circuitry 202.
For the batch input, the system 100 obtains a first input from a memory location identified by an address corresponding to the unique identifier based on the filtered list of addresses (610). For example, based on the filtered list of addresses, the system 100 may use a particular processor assigned to the respective data slice to obtain the first input from a memory location of the data slice, the memory location identified by an address corresponding to the unique identifier.
To obtain the first input, the request processor may provide a request to the remote processor for processing by the remote processor to obtain the first input. The request may include an address of the first input corresponding to the unique identifier. In some cases, the request and address may be sent separately, but simultaneously or in parallel. In some cases, the request and address may be sent sequentially, one before the other.
In response to the remote processor processing the request, the requesting processor receives a first input from the remote processor, the first input being stored at a memory location of a data slice allocated to the remote processor. In some embodiments, the request processor is operable to perform one or more reduction operations to generate an output of the neural network layer from a first input, the first input being obtained from the data slice, as described in more detail below.
The system 100 may determine the partition of the address in the filtered list between each of the plurality of processors in the multi-core processing unit 104. For example, system 100 may determine that the data element associated with address [26] is partitioned into data slices assigned to processor 13, the data elements associated with addresses [96] and [99] are partitioned into data slices assigned to processor 15, or the data element associated with address [912] is partitioned into data slices assigned to processor 3.
In this way, and for each partition of an address, the system may obtain a first input from a memory location identified by the address in the address partition using the respective processor and other respective data elements (e.g., weight values) in the data slice assigned to retrieve the input, the data slice being assigned to the respective processor. For example, each partition of an address may be assigned to a data slice having activation values or gain values corresponding to vector elements of a multi-dimensional sparse tensor. In some embodiments, each data slice having an activation or gain value is assigned to a particular processor for multiplication with a corresponding weight vector to perform neural network calculations.
The system 100 generates an output, such as an embedded output, of the neural network layer from the obtained first input (612). In some embodiments, the neural network layer is an embedded layer of the neural network, and the embedded output includes embedded feature vectors. The scatter circuitry 202 is configured to determine an inverse mapping of the unique identifier to a duplicate address of a particular input in the batch input of the embedding layer. In some implementations, an inverse mapping of the unique identifier to the duplicate address is determined for a particular input of the plurality of sets of input features.
Based on the reverse mapping of the unique identifier to the duplicate address of the particular input, the system 100 may be operated to generate one or more vectors (e.g., embedded feature vectors) as the output of the embedding layer. For example, generating an output (e.g., an embedded output) of the neural network layer based on the reverse mapping may include analyzing an address list that resolves the duplicate addresses to map embedded feature vectors, generated for the duplicate addresses and corresponding to the unique identifier, back to a particular input in the batch input of the neural network layer.
In some examples, generating the output of the embedding layer may include generating one or more dense matrices in response to the processing data associated with the sparse table of embedding values. The data associated with the table may be represented in the form of a matrix. The processor and computational resources of system 100 may be used to manipulate data based on linear algebraic algorithms including matrix multiplication. The matrix may be a one-dimensional vector or a multi-dimensional matrix. The matrix may be represented by a data structure, such as a database table or a variable. The embedded table may be sliced, for example, in row and column dimensions, and stored in a portion of memory that is mapped to a particular processor of the plurality of processors forming the multi-core processing unit 104.
The subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, tangibly embodied in computer software or firmware, in computer hardware, to include the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory program carrier for execution by, or to control the operation of, data processing apparatus.
Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
The term "computing system" includes various apparatuses, devices, and machines for processing data, including by way of example programmable processors, computers, or multiple processors or computers. An apparatus may comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, or as a component, subroutine, or other unit suitable for use in a computing environment.
The computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, such as files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array), an ASIC (application-specific integrated circuit), or a GPGPU (general purpose graphics processing unit).
For example, a computer suitable for executing a computer program may be based on a general purpose or special purpose microprocessor, or both, or any other type of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. Some elements of a computer are a central processing unit for executing and implementing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Further, a computer may be embedded in another device, such as a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; as well as CDROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., an LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described herein or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated into a single software product or packaged into multiple software products.
Specific embodiments of the present subject matter have been described. Other implementations are within the scope of the following claims. For example, the actions in the claims can be performed in a different order and still achieve desirable results. As one example, the processes illustrated in the figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (22)
1. A method of performing neural network computations using a system configured to implement a neural network on hardware circuitry, the method comprising:
a host receives batch inputs to a neural network layer, wherein each input in the batch inputs is stored in a memory location identified by an address;
identifying one or more duplicate addresses in the list of addresses for one or more of the batch inputs; for each duplicate address:
generating a unique identifier that identifies the duplicate address in the address list;
for the batch input, obtaining a first input from a memory location identified by an address corresponding to the unique identifier;
generating an embedded output of the neural network layer from the obtained first input.
2. The method of claim 1, wherein the neural network layer is an embedded layer of the neural network, the embedded output comprises embedded feature vectors, and the method further comprises:
scatter circuitry of the system determines an inverse mapping of the unique identifier to a duplicate address of a particular input of the batch inputs to the embedding layer; and is
Generating one or more embedding vectors as an output of the embedding layer based on the inverse mapping of the unique identifier to the duplicate address of the particular input.
3. The method of claim 2, wherein the method comprises:
generating a filtered list of addresses for the batch input based on one or more unique identifiers;
determining address partitions between each of a plurality of processors for addresses in the filtered list of addresses; and
wherein obtaining the first input comprises: for each address partition, a first input is obtained from a memory location identified by an address in the address partition using a respective processor assigned to retrieve the input from the address.
4. The method of claim 2, wherein generating the embedded output of the neural network layer based on the inverse mapping comprises:
parsing the address list including the duplicate address to map the embedded feature vector generated for the duplicate address corresponding to the unique identifier back to the particular input of the batch of inputs of the neural network layer.
5. The method of claim 2, wherein the method comprises:
the request processor providing to the remote processor an address of a first input corresponding to the unique identifier and a request for the first input;
the request processor receiving the first input from the remote processor, the first input stored in a memory location of a data slice allocated to the remote processor; and is
Performing one or more reduction operations to generate the embedded output of the neural network layer from a first input obtained from the data slice.
6. The method of claim 2, wherein the list of addresses is used to form a batch input of input feature samples comprising a plurality of sets of input features, and wherein generating the unique identifier comprises:
generating the unique identifier for each duplicate address on a respective set of input features in the input feature sample.
7. The method of claim 6, wherein determining the reverse mapping of the unique identifier to a duplicate address comprises:
determining the reverse mapping of the unique identifier to a duplicate address of a particular input of the plurality of sets of input features.
8. The method of claim 6, wherein:
the duplicate address is a specific address of a memory location stored to an input of the neural network layer,
the particular address is common between corresponding data slices of each of the plurality of processors, and
the particular address is repeated between (i) a set of input features, or (ii) the input feature samples.
9. The method of claim 2, wherein:
allocating each address partition to a data slice corresponding to an activation value or a gain value of a vector element of the multidimensional sparse tensor; and is provided with
Each data slice of activation values or gain values is assigned to a particular processor for multiplication with a corresponding weight vector to perform the neural network computation.
10. The method of claim 1, wherein generating the embedded output of the neural network layer comprises:
providing a plurality of inputs from a remote processor to a requesting processor, the plurality of inputs stored in memory locations of a data slice allocated to the remote processor core; and is
Performing a lookup to obtain a respective weight vector comprising respective weight values for multiplying with an input of the plurality of inputs to generate a partial activation for a subsequent neural network layer.
11. The method of claim 10, wherein:
from the memory location identified by a non-duplicate address; or from a memory location identified by an address in the original list where no duplicate address exists.
12. A system for implementing a neural network on hardware circuitry to perform neural network computations, the system comprising:
one or more processing devices; and
one or more non-transitory machine-readable storage devices to store instructions that are executable by the one or more processing devices to perform operations comprising:
a host receives batch inputs to a neural network layer, wherein each input in the batch inputs is stored in a memory location identified by an address;
identifying one or more duplicate addresses in the list of addresses for one or more of the batch inputs;
for each duplicate address:
generating a unique identifier that identifies the duplicate address in the address list;
for the batch input, obtaining a first input from a memory location identified by an address corresponding to the unique identifier;
generating an embedded output of the neural network layer from the obtained first input.
13. The system of claim 12, wherein the neural network layer is an embedded layer of the neural network, the embedded output comprises embedded feature vectors, and the method further comprises:
scatter circuitry of the system determines an inverse mapping of the unique identifier to a duplicate address of a particular input of the batch inputs to the embedding layer; and is
Generating one or more vectors as an output of the embedding layer based on the inverse mapping of the unique identifier to the duplicate address of the particular input.
14. The system of claim 13, further comprising:
generating a filtered list of addresses for the batch input based on one or more unique identifiers;
determining address partitions between each of a plurality of processors for addresses in the filtered list of addresses; and
wherein obtaining the first input comprises: for each address partition, a first input is obtained from a memory location identified by an address in the address partition using a respective processor assigned to retrieve the input from the address.
15. The system of claim 13, wherein generating the embedded output of the neural network layer based on the inverse mapping comprises:
parsing the address list including the duplicate address to map the embedded feature vector generated for the duplicate address corresponding to the unique identifier back to the particular input of the batch of inputs of the neural network layer.
16. The system of claim 13, further comprising:
requesting the processor to provide to a remote processor an address of a first input, the address of the first input corresponding to a unique identifier and a request for the first input;
the request processor receiving the first input from the remote processor, the first input stored in a memory location of a data slice allocated to the remote processor; and is
Performing one or more reduction operations to generate the embedded output of the neural network layer from a first input obtained from the data slice.
17. The system of claim 13, wherein the list of addresses is used to form a batch input of input feature samples comprising a plurality of sets of input features, and wherein generating the unique identifier comprises:
generating the unique identifier for each duplicate address on a respective set of input features in the input feature sample.
18. The system of claim 17, wherein determining the reverse mapping of the unique identifier to a duplicate address comprises,
determining the reverse mapping of the unique identifier to a duplicate address of a particular input of the plurality of sets of input features.
19. The system of claim 17, wherein:
the duplicate address is a specific address of a memory location stored to an input of the neural network layer,
the particular address is common between corresponding data slices of each of the plurality of processors, and
the particular address is repeated between (i) a set of input features, or (ii) the input feature samples.
20. The system of claim 13, wherein:
assigning each address partition to a data slice corresponding to an activation value or a gain value of a vector element of the multi-dimensional sparse tensor; and is
Each data slice of activation values or gain values is assigned to a particular processor for multiplication with a corresponding weight vector to perform the neural network computation.
21. The system of claim 21, wherein generating the embedded output of the neural network layer comprises:
providing a plurality of inputs from a remote processor to a requesting processor, the plurality of inputs stored in memory locations of a data slice allocated to the remote processor core; and is
Performing a lookup to obtain respective weight vectors comprising respective weight values for multiplying with inputs of the plurality of inputs to generate partial activations for a subsequent neural network layer.
22. One or more non-transitory machine-readable storage devices to implement a neural network on hardware circuitry to perform neural network computations and to store instructions executable by one or more processing devices to perform operations comprising:
a host receives batch inputs to a neural network layer, wherein each input in the batch inputs is stored in a memory location identified by an address;
identifying one or more duplicate addresses in the list of addresses for one or more of the batch inputs; for each duplicate address:
generating a unique identifier that identifies the duplicate address in the address list;
for the batch input, obtaining a first input from a memory location identified by an address corresponding to the unique identifier;
generating an embedded output of the neural network layer from the obtained first input.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962909667P | 2019-10-02 | 2019-10-02 | |
US62/909,667 | 2019-10-02 | ||
PCT/US2020/053442 WO2021067376A1 (en) | 2019-10-02 | 2020-09-30 | Accelerated embedding layer computations |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114503128A true CN114503128A (en) | 2022-05-13 |
Family
ID=72744594
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080060592.XA Pending CN114503128A (en) | 2019-10-02 | 2020-09-30 | Accelerating embedded layer computations |
Country Status (7)
Country | Link |
---|---|
US (2) | US11651209B1 (en) |
EP (1) | EP3800590A1 (en) |
JP (2) | JP7325615B2 (en) |
KR (1) | KR20220034236A (en) |
CN (1) | CN114503128A (en) |
TW (1) | TW202131236A (en) |
WO (1) | WO2021067376A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117318892A (en) * | 2023-11-27 | 2023-12-29 | 阿里云计算有限公司 | Computing system, data processing method, network card, host computer and storage medium |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN115730116A (en) * | 2021-08-31 | 2023-03-03 | 华为技术有限公司 | Data retrieval method and related equipment |
KR102609562B1 (en) * | 2021-10-15 | 2023-12-05 | 인하대학교 산학협력단 | Hybrid Near Memory Processing architecture and method for embedding of recommendation systems |
US20230146611A1 (en) * | 2021-11-11 | 2023-05-11 | Samsung Electronics Co., Ltd. | Neural network training with acceleration |
KR102515159B1 (en) * | 2022-07-12 | 2023-03-29 | 인하대학교 산학협력단 | Near-memory Processing of Embeddings Method and System for Reducing Memory Size and Energy in Deep Learning-based Recommendation Systems |
Family Cites Families (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU2001249805A1 (en) | 2000-04-03 | 2001-10-15 | 3-Dimensional Pharmaceuticals, Inc. | Method, system, and computer program product for representing object relationships in a multidimensional space |
EP2303569B1 (en) | 2008-07-17 | 2020-07-08 | Basf Se | Method for continuously producing multi-layered composite bodies |
US8380647B2 (en) | 2009-08-14 | 2013-02-19 | Xerox Corporation | Training a classifier by dimension-wise embedding of training data |
JP6161396B2 (en) | 2013-05-15 | 2017-07-12 | オリンパス株式会社 | Arithmetic unit |
US9304971B2 (en) * | 2013-06-27 | 2016-04-05 | International Business Machines Corporation | Lookup table sharing for memory-based computing |
EP3091450B1 (en) * | 2015-05-06 | 2017-04-05 | Örjan Vestgöte | Method and system for performing binary searches |
KR20180033670A (en) * | 2016-09-26 | 2018-04-04 | 에스케이하이닉스 주식회사 | Semiconductor memory device and operating method thereof |
CN110023963B (en) | 2016-10-26 | 2023-05-30 | 渊慧科技有限公司 | Processing text sequences using neural networks |
EP3542319B1 (en) | 2016-11-15 | 2023-07-26 | Google LLC | Training neural networks using a clustering loss |
CN109952583A (en) | 2016-11-15 | 2019-06-28 | 谷歌有限责任公司 | The semi-supervised training of neural network |
US20180150256A1 (en) | 2016-11-29 | 2018-05-31 | Intel Corporation | Technologies for data deduplication in disaggregated architectures |
US11453121B2 (en) | 2017-03-17 | 2022-09-27 | Google Llc | Mirror loss neural networks |
EP3596665A1 (en) | 2017-05-19 | 2020-01-22 | Google LLC | Depthwise separable convolutions for neural machine translation |
US20190073580A1 (en) | 2017-09-01 | 2019-03-07 | Facebook, Inc. | Sparse Neural Network Modeling Infrastructure |
US10943171B2 (en) | 2017-09-01 | 2021-03-09 | Facebook, Inc. | Sparse neural network training optimization |
GB2567190B (en) * | 2017-10-05 | 2020-02-26 | Advanced Risc Mach Ltd | Error recovery for intra-core lockstep mode |
US10997065B2 (en) * | 2017-11-13 | 2021-05-04 | SK Hynix Inc. | Memory system and operating method thereof |
US20190164036A1 (en) * | 2017-11-29 | 2019-05-30 | Electronics And Telecommunications Research Institute | Method and apparatus for generating address of data of artificial neural network |
US10678508B2 (en) * | 2018-03-23 | 2020-06-09 | Amazon Technologies, Inc. | Accelerated quantized multiply-and-add operations |
US11507846B2 (en) | 2018-03-26 | 2022-11-22 | Nvidia Corporation | Representing a neural network utilizing paths within the network to improve a performance of the neural network |
KR102619954B1 (en) * | 2018-03-29 | 2024-01-02 | 삼성전자주식회사 | Method for processing data and electronic device for supporting the same |
US10621489B2 (en) * | 2018-03-30 | 2020-04-14 | International Business Machines Corporation | Massively parallel neural inference computing elements |
CN108447520B (en) * | 2018-05-03 | 2023-10-13 | 长鑫存储技术有限公司 | Memory circuit device and memory detection method |
KR102524804B1 (en) * | 2019-01-04 | 2023-04-24 | 삼성전자주식회사 | One time programmable memory cell, and otp memory and memory system having the same |
SE543186C2 (en) * | 2019-01-11 | 2020-10-20 | Zeropoint Tech Ab | Systems, methods and devices for eliminating duplicates and value redundancy in computer memories |
US11227120B2 (en) * | 2019-05-02 | 2022-01-18 | King Fahd University Of Petroleum And Minerals | Open domain targeted sentiment classification using semisupervised dynamic generation of feature attributes |
KR20210000414A (en) * | 2019-06-25 | 2021-01-05 | 에스케이하이닉스 주식회사 | Memory system |
EP3767486B1 (en) * | 2019-07-19 | 2023-03-22 | Microsoft Technology Licensing, LLC | Multi-record index structure for key-value stores |
EP3786959A1 (en) * | 2019-08-30 | 2021-03-03 | TTTech Computertechnik Aktiengesellschaft | Method of error detection in a ternary content addressable memory |
US11429573B2 (en) * | 2019-10-16 | 2022-08-30 | Dell Products L.P. | Data deduplication system |
US11620503B2 (en) * | 2020-03-18 | 2023-04-04 | Arm Limited | Neural network processing |
US11212219B1 (en) * | 2020-06-26 | 2021-12-28 | Lenovo Enterprise Solutions (Singapore) Pte. Ltd. | In-band telemetry packet size optimization |
US20220100518A1 (en) * | 2020-09-25 | 2022-03-31 | Advanced Micro Devices, Inc. | Compression metadata assisted computation |
-
2019
- 2019-10-21 US US16/659,527 patent/US11651209B1/en active Active
-
2020
- 2020-09-30 CN CN202080060592.XA patent/CN114503128A/en active Pending
- 2020-09-30 JP JP2022511014A patent/JP7325615B2/en active Active
- 2020-09-30 TW TW109134099A patent/TW202131236A/en unknown
- 2020-09-30 KR KR1020227005451A patent/KR20220034236A/en not_active Application Discontinuation
- 2020-09-30 WO PCT/US2020/053442 patent/WO2021067376A1/en active Application Filing
- 2020-10-02 EP EP20199770.7A patent/EP3800590A1/en active Pending
-
2023
- 2023-04-21 US US18/305,297 patent/US11948086B2/en active Active
- 2023-08-01 JP JP2023125258A patent/JP2023162181A/en active Pending
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117318892A (en) * | 2023-11-27 | 2023-12-29 | 阿里云计算有限公司 | Computing system, data processing method, network card, host computer and storage medium |
CN117318892B (en) * | 2023-11-27 | 2024-04-02 | 阿里云计算有限公司 | Computing system, data processing method, network card, host computer and storage medium |
Also Published As
Publication number | Publication date |
---|---|
EP3800590A1 (en) | 2021-04-07 |
JP2023162181A (en) | 2023-11-08 |
JP7325615B2 (en) | 2023-08-14 |
US20230376759A1 (en) | 2023-11-23 |
JP2022550254A (en) | 2022-12-01 |
TW202131236A (en) | 2021-08-16 |
US11651209B1 (en) | 2023-05-16 |
KR20220034236A (en) | 2022-03-17 |
US11948086B2 (en) | 2024-04-02 |
WO2021067376A1 (en) | 2021-04-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN114503128A (en) | Accelerating embedded layer computations | |
CN107045493B (en) | Matrix processing device | |
CN107045492B (en) | Matrix processing device | |
US20210019570A1 (en) | Dynamic minibatch sizes | |
CN109213972B (en) | Method, device, equipment and computer storage medium for determining document similarity | |
CN111767287A (en) | Data import method, device, equipment and computer storage medium | |
CN115114283A (en) | Data processing method and device, computer readable medium and electronic equipment | |
Wang | An algorithm for constructing spatial vector data storage based on KD-tree and density estimation | |
Kim | Performance of Distributed Database System built on Multicore Systems | |
CN113988282A (en) | Programmable access engine architecture for graph neural networks and graph applications |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
REG | Reference to a national code |
Ref country code: HKRef legal event code: DERef document number: 40072706Country of ref document: HK |