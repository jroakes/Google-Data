US8886826B2 - System for multipoint infrastructure transport in a computer network - Google Patents
System for multipoint infrastructure transport in a computer network Download PDFInfo
- Publication number
- US8886826B2 US8886826B2 US10/618,369 US61836903A US8886826B2 US 8886826 B2 US8886826 B2 US 8886826B2 US 61836903 A US61836903 A US 61836903A US 8886826 B2 US8886826 B2 US 8886826B2
- Authority
- US
- United States
- Prior art keywords
- multicast group
- node
- mint
- entries
- entry
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L12/00—Data switching networks
- H04L12/02—Details
- H04L12/16—Arrangements for providing special services to substations
- H04L12/18—Arrangements for providing special services to substations for broadcast or conference, e.g. multicast
-
- H04L12/5695—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L12/00—Data switching networks
- H04L12/02—Details
- H04L12/16—Arrangements for providing special services to substations
- H04L12/18—Arrangements for providing special services to substations for broadcast or conference, e.g. multicast
- H04L12/185—Arrangements for providing special services to substations for broadcast or conference, e.g. multicast with management of multicast group membership
-
- H04L29/06—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/15—Flow control; Congestion control in relation to multipoint traffic
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/24—Traffic characterised by specific attributes, e.g. priority or QoS
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/70—Admission control; Resource allocation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/70—Admission control; Resource allocation
- H04L47/78—Architectures of resource allocation
- H04L47/783—Distributed allocation of resources, e.g. bandwidth brokers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/70—Admission control; Resource allocation
- H04L47/80—Actions related to the user profile or the type of traffic
- H04L47/806—Broadcast or multicast traffic
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L9/00—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols
- H04L9/40—Network security protocols
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/24—Negotiation of communication capabilities
Definitions
- This invention relates generally to the field of computer networks, and more particularly, to a multipoint transfer protocol for use in a computer network.
- Multicasting is the process of transmitting information from a host on a data network to a select plurality of hosts on the data network.
- the select plurality is often referred to a “multicast group.”
- the present invention provides a method and apparatus for implementing a Multipoint Infrastructure Transport (MINT) protocol in a data network.
- MINT Multipoint Infrastructure Transport
- the MINT protocol provides a reliable information delivery mechanism between a single node in the data network and all other infrastructure as well as end-host nodes in the data network that are subscribed to a particular group.
- the present invention is suitable for use with groups formed using IP Multicast routing protocols like sparse mode PIM or core based trees (CBT), or in other multicast protocols wherein the multicast group has an associated a rendezvous point or node.
- IP Multicast routing protocols like sparse mode PIM or core based trees (CBT), or in other multicast protocols wherein the multicast group has an associated a rendezvous point or node.
- McCanne '869 An example of such a protocol is described in McCanne '869, wherein a description of an Overlay Multicast Network (OMN) is disclosed.
- OTN Overlay Multicast Network
- One embodiment of the present invention provides a method for distributing data in a data network.
- the data network connects a plurality of nodes and at least a portion of the plurality of the nodes form a multicast group.
- One of the nodes in the multicast group is designated as a rendezvous node.
- the method includes a step of maintaining a data store containing a group state at each of the nodes in the multicast group. State updates, received at the rendezvous node are used to update the group state in the data store at the rendezvous node. The state updates are propagated, using a reliable protocol, from the rendezvous node to the other nodes in the multicast group. Finally, the group state in the data stores at the other nodes in the multicast group are updated.
- a processing agent for processing data at a node in a data network connects a plurality of nodes and at least a portion of the plurality of the nodes form a multicast group.
- One of the nodes in the multicast group is designated as a rendezvous node.
- the processing agent comprises a state memory and a protocol processor.
- the protocol processor has logic to couple to a selected node in the data network and has logic to transmit and receive data with other processing agents in the data network over a data channel using a reliable protocol.
- the protocol processor also couples to the state memory and has logic to store and retrieve the data to and from the state memory, respectively.
- FIG. 1A shows a computer suitable for use with the present invention
- FIG. 1B shows subsystems in the computer of FIG. 1A ;
- FIG. 2 shows a data network constructed in accordance with the present invention
- FIG. 3 shows a block diagram of a MINT processing agent 300 constructed in accordance with the present, invention
- FIG. 4 shows exemplary MINT information
- FIG. 5 shows a multicast group defined in the data network of FIG. 2 ;
- FIG. 6 shows a block diagram for a method of propagating MINT information over the network shown in FIG. 5 ;
- FIG. 7 shows a modified multicast group modified to include a joining node
- FIG. 8 shows a block diagram for a method of propagating MINT information over the modified multicast group of FIG. 7 ;
- FIG. 9 shows a block diagram of a method of releasing a node in a multicast group and terminating its associated MINT binding
- FIG. 10 is a diagram further illustrating transaction steps during the operation of the method shown in FIG. 9 ;
- FIG. 11 is a diagram further illustrating transaction steps during the operation of the method shown in FIG. 9 .
- the present invention provides a method and apparatus for implementing a MINT protocol in a data network to provide a reliable information delivery mechanism between a sender node in the data network and members of a multicast group, infrastructure and/or end-hosts in the data network.
- senders associate named values to a multicast group which are published into and across the data network, thereby allowing other group members as well as network entities to query this “database” of distributed state.
- Each tuple in the database called a “mint”, is identified by its owner (the multicast sender), name and multicast group.
- the mints are disseminated reliably to all parts of the network with active group participants. Preferably, mints flow only to routers that fall along a path from the source to the set of active receivers for that group. This results in efficient routing of the MINT information which is an advantage over prior systems, that operate by flooding the entire network with information without regard to efficient routing and distribution.
- An end host may query the multicast subsystem to discover and/or enumerate all known mints published by each owner.
- the mint values can be queried by reference to the name/owner, and the agent performing the query can be asynchronously notified when the owner modifies the values.
- specific mints are reserved for system specific functions that, for instance, map a group to an application type or describe the attributes of a group so that the group can be mapped into locally defined traffic classes in different parts of the network. For example, if a transmitted data stream requires application-level processing and/or traffic management, a special “setup mint” provides the requisite information and precedes the transmission of data.
- an information source can use the MINT protocol to publish mints that annotate data streams injected into the group.
- Specialized packet forwarding engines located at each node on the multicast tree for the group in question, process the received data streams based on the stream annotations. For example, the packet forwarding engines can allocate network bandwidth to the data streams based on the stream annotations.
- FIG. 1A is an illustration of computer system 1 suitable for use with the present invention.
- the computer system 1 includes display 3 having display screen 5 .
- Cabinet 7 houses standard computer components (not shown) such as a disk drive, CDROM drive, display adapter, network card, random access memory (RAM), central processing unit (CPU), and other components, subsystems and devices.
- User input devices such as mouse 11 having buttons 13 , and keyboard 9 are shown.
- Other user input devices such as a trackball, touch-screen, digitizing tablet, etc. can be used.
- the computer system is illustrative of but one type of computer system, such as a desktop computer, suitable for use with the present invention.
- Computers can be configured with many different hardware components and can be made in many dimensions and styles (e.g., laptop, palmtop, pentop, server, workstation, mainframe). Any hardware platform suitable for performing the processing described herein is suitable for use with the present invention.
- FIG. 1B illustrates subsystems that might typically be found in a computer such as the cabinet 7 of computer 1 .
- subsystems are directly interfaced to internal bus 22 .
- Subsystems include input/output (I/O) controller 24 , System Random Access Memory (RAM) 26 , Central Processing Unit (CPU) 28 , Display Adapter 30 , Serial Port 40 , Fixed Disk 42 and Network Interface Adapter 44 .
- the use of bus 22 allows each of the subsystems to transfer data among the subsystems and, most importantly, with the CPU.
- External devices can communicate with the CPU or other subsystems via bus 22 by interfacing with a subsystem on the bus.
- Monitor 46 connects to the bus through Display Adapter 30 .
- a relative pointing device (RPD) 48 such as a mouse connects through Serial Port 40 .
- Some devices such as Keyboard 50 can communicate with the CPU by direct means without using the main data bus as, for example, via an interrupt controller and associated registers (not shown).
- FIG. 1B is illustrative of but one suitable configuration. Subsystems, components or devices other than those shown in FIG. 1B can be added. A suitable computer system can be achieved without using all of the subsystems shown in FIG. 1B . Other subsystems such as a CDROM drive, graphics accelerator, etc. can be included in the configuration without affecting the performance of the system of the present invention.
- FIG. 2 shows a portion of a data network 200 constructed in accordance with the present invention.
- the network 200 comprises routing nodes (or native routers) 202 , 204 , 206 , 208 , 210 and 212 .
- the nodes are interconnected by bi-directional links, such as those shown at 214 , 216 and 218 . Included in the network 200 are information sources 220 and 222 . Also shown in the network 200 are clients 224 , 226 , 228 and 230 , which are coupled to routing nodes via additional bi-directional links.
- the network 200 illustrates that routing nodes 204 and 210 may, in fact, be part of other network domains, such as domain X and domain Y, respectively.
- the network 200 is representative of but one embodiment of the present invention. It will be apparent to those with skill in the art that other embodiments of the present invention may be used in other types of network architectures or topologies.
- MINT processing agents 232 , 234 , 236 , 238 , 240 and 242 Coupled to each of the nodes of the network 200 are MINT processing agents 232 , 234 , 236 , 238 , 240 and 242 .
- the MINT processing agents are shown as being external to the routing nodes, however, the MINT processing agents can be incorporated within each of the routing nodes.
- the MINT processing agents receive and transmit information via their associated node to implement the MINT protocol.
- the network 200 is capable of forming multicast groups as in, for example, IP Multicast routing protocols like sparse mode PIM or core based trees, wherein the multicast group has an associated rendezvous point or node.
- FIG. 3 shows an exemplary embodiment of a MINT processing agent 300 constructed in accordance with the present invention.
- the MINT processing agent 300 is representative of the MINT processing agents shown in FIG. 2 , such as MINT processing agent 232 .
- the MINT processing agent 300 can be used in the data network 200 to implement the MINT protocol.
- the MINT processing agent comprises a MINT Protocol Module (PM) 302 , a data store 304 and an optional packet forwarding engine 306 .
- PM MINT Protocol Module
- the MINT-PM 302 couples to a routing node in the data network via link 308 .
- the MINT-PM uses the link 308 to communicate with the routing node and to form a MINT channel that allows the MINT processing agents in the data network to communicate with one another.
- the MINT channel is used to transmit and receive information between the MINT processing agents and/or between the MINT processing agents and clients, information sources and any other end-hosts in the data network.
- the data store 304 couples to the MINT-PM and stores the mint information which forms a database of distributed state.
- the optional packet forwarding engine 306 can be used when the MINT processing agents are used to regulate traffic streams based on mint information as described in McCanne '865.
- the packet forwarding engine 306 receives data packets 310 transmitted on the network 200 and processes the received data packets to form an output data stream 312 for transmission on the network.
- the packet forwarding engine 302 couples to the MINT-PM 302 and the data store 304 , to exchange information that is used to determine how the packet forwarding engine 306 processes the received data packets. For example, mint information retrieved from the data store 304 is used by the packet forwarding 306 engine to determine bandwidth allocations on the data network for the received data packets 310 . In another example, mint information retrieved from the data store 304 is used by the packet forwarding 306 engine to schedule packets in the output data stream 312 based on priority information contained in the mint information.
- the packet forwarding engine 306 is omitted from the MINT processing agent and is assumed to exist within the incident native router.
- the MINT processing agent is used to process and transmit mints in the data network but performs no processing on data packets transmitted in the network.
- the MINT processing agent would be limited to the tasks of processing mints and providing the reliable delivery of mints in the data network.
- the MINT protocol provides a group-oriented, reliable information delivery mechanism to the subset of nodes in a data network that span the multicast routing tree supporting the corresponding group.
- end-host sources may publish data into the network by directing MINT instructions to the rendezvous point for the group in question.
- the MINT protocol provides a mechanism whereby a set of published values are maintained at all MINT processing agents associated with active nodes in the spanning tree as members come and go. Additional features of the MINT protocol provide for queries by arbitrary network clients or management agents to obtain the most recent set of published values.
- a MINT channel is associated with each active multicast group.
- the MINT channel might be a reliable control connection using TCP that adheres to a MINT access protocol which comprises a number of MINT instructions.
- Applications publish named data tuples called “mints” into the MINT channel by directing MINT instructions to the rendezvous point; in turn, the MINT-PM at the rendezvous point ensures that each such mint is propagated to all MINT processing agents associated with routing nodes that are incident to the distribution tree for that group. This allows edge applications to publish state into the network and communicate with application-level processing agents (i.e., plugins) that may exist in the network or may be provided as part of the MINT processing agents.
- application-level processing agents i.e., plugins
- the packet forwarding engines may run application-level processing agents that can communicate with edge applications via the MINT channel, to allocate network bandwidth to the edge applications.
- the MINT protocol also provides a well-defined communication abstraction for disseminating mints along the paths of the spanning tree in a dynamic fashion as sub-trees come and go. Whenever a router, or node, grafts on a branch to a given group's spanning tree, all the mints for that group are flooded, or propagated, along the newly created branch. As a result, state is reliably propagated to all MINT processing agents along the newly grafted branch.
- the data model assumed by the MINT protocol is a persistent data store of named tuples or mints.
- An origin node (or owner) may publish mints into the network or may relinquish its attachment to the persistent data store using the MINT access instructions. If a node fails or becomes otherwise disconnected from the network, all of its published bindings are expunged from its associated data store when the corresponding leg of the multicast routing tree (for the group in question) is torn down.
- mints are persistent, the MINT processing agent may run out of resources to maintain all the mints published into the network. In this case, the mint publishing process fails. To notify the end clients of this failure, a special, reserved error mint is attached to the group and has priority over all existing mints.
- Static priorities may be assigned to mints. This controls the relative ordering of mints as they are propagated between MINT processing agents as legs of the distribution tree come and go.
- Each mint is named with a structured hierarchical name, thereby providing a rich mechanism for reviewing a class of mints by prefix, regular expression or other reviewing technique.
- MINT data names are represented as textual strings while MINT values are arbitrary binary data.
- FIG. 4 shows exemplary MINT information 400 that may occur at the data store 304 of the MINT processing agent 360 .
- the MINT information 400 comprises tuples of mint parameters representing any type of information.
- the mint parameters may be descriptive of information transmitted in the data network.
- the MINT information 400 is comprised of mints having a group 401 , origin 402 , name 404 , value 406 and priority 408 . Since a node in the data network may be associated with one or more multicast groups, the MINT information may contain mint parameters associated with one or more multicast groups. As shown in FIG. 4 , an origin may be associated with several named values. In the specific embodiment of the MINT information 400 , the name and value pairs describe video data streams. It will be apparent to those with skill in the art that the name and value pairs may represent any type of information.
- names that index the MINT data store naturally form a namespace. Associated with each group is an autonomous namespace, i.e., each group's mints are completely independent of all other groups. To support rich and efficient queries over these namespaces, names are represented in a structured yet simple form. Specifically, the names form a hierarchical namespace, wherein the hierarchy demarcations are denoted by a “/” separator, just as the Unix file system arranges directory names into a hierarchy and uses the “/” separator to indicate the relative elements of the path through the tree-based hierarchy.
- the hierarchical namespace representation allows matching queries to be run against the existing name space.
- a broadcast system might publish advertisement information as a series of mints under the prefix “/ad/info”.
- a client might want to query the database to see what names exists under this prefix with a “globbing” match, e.g., “/ad/info/*”.
- a network agent might want to be notified whenever this information changes, so that an event callback can occur when any mints that match “/ad/info/*” are created, deleted, or modified.
- Each MINT processing agent in the network includes a MINT protocol module (MINT-PM) that maintains the data store, indexed by origin, group and name of all published mints known to that agent.
- MINT-PM MINT protocol module
- the publisher of a mint is called its origin or owner. While an origin may be located anywhere in the data network, it must publish mints for a particular group via that group's rendezvous point.
- each mint is published indefinitely and there is no refresh process.
- the MINT protocol in association with the underlying routing protocol
- the routing system responds by tearing down, and then re-establishing the group as necessary; consequently, any mints attached to the group in question will be propagated by the MINT protocol as the group is reconstructed.
- the group state can be managed consistently and there is no need for a refresh/timeout process.
- the amount of mint state that any single node can inject into the network is limited by a configurable parameter.
- the data stored for each tuple associated with any given group includes the following elements:
- the name of the multicast group origin The primary IP address of the node that published this entry.
- name The string-based name of this entry.
- Each tuple has a unique ⁇ name relative to its origin.
- value The value of the named data. This may be arbitrary binary data whose semantics are outside the scope of this protocol specification.
- priority A small integer that represents the delivery priority of this (pri) item compared to other published values (higher priority values tend to be sent before lower priority values).
- protocol messages There are three types of protocol messages:
- the data value will be reliably delivered to all nodes in the network that span the multicast routing tree for G, which will each be able to access to the value by name with the query call.
- the pri parameter is a small integer that represents the delivery priority of this item compared to other published values (higher priority values tend to be sent before lower priority values). This binding is maintained until the origin terminates or relinquishes the binding with a relinquish command. A value may not be successfully published when the amount of state exceeds the supportable limit. In this case, an error is delivered asynchronously some time after the publishing call is made.
- the reliability of the present invention is based on a novel delivery mechanism tied to the group membership protocol. Since the MINT protocol is run on a per-group basis, we will refer to the group in question generically as “group G.”
- group G A peer from which a MINT-PM receives published mints will generally be the on the reverse-path shortest route back to the rendezvous point. This may not always be the case, as the path is dependent on the underlying routing processes. This peer can be referred to as the “parent” peer.
- published mints may be received from different parents, as a result of the routing changes that may occur with changes in the spanning tree for group G. All of these mints are maintained in the data store associated with the MINT-PM.
- MINT-PMs associated with peers in a group communicate mints with each other over the MINT channel in a unicast fashion.
- the MINT channel is a reliable connection, for example, a reliable TCP connection, that is congruent with the underlying router's peering relationships.
- a MINT-PM When a MINT-PM receives a new mint from a MINT-PM associated with its parent peer, it enters the mint into its data store and forwards a copy of the mint to MINT-PMs associated with other peers on the current multicast spanning tree for G. For example, a MINT-PM receives a published mint from its parent peer, it updates its data store and then publishes the mint as a parent to other MINT-PMs. Note that this action is carried out atomically against any changes to the spanning tree.
- the goal is to maintain the invariant that all MINT-PMs associated with peers on the spanning tree for group G, reliably distribute all mints stored in the data store of their respective parent, wherein the ultimate parent is the MINT-PM associated with the rendezvous point for the group.
- the MINT-PM receives a mint (from its parent) that is already in its table, it checks to see if the data value is different. If not, it increments an error counter (accessible via a network management protocol), because the peer should have known not to send a redundant update. If the value is different, the MINT-PM updates its data store and propagates the change (by re-publishing the mint as a parent peer) to each of its child peers on the current multicast spanning tree for G. In effect, the MINT-PM issues another publish command to peer MINT-PMs, as if it were the parent.
- the MINT-PM receives a mint from a peer that is not its parent for group G, then it records the mint update in a shadow table for that peer. If that peer later becomes its parent for G, then this shadow table becomes the actual data store (and any differences encountered while changing tables are treated as normal mint arrivals, changes, or deletions).
- a group G node receives a graft message, and the requesting node is grafted to the group G, all mints associated with group G are sent to the MINT-PM associated with the requesting node.
- the mints are sent in static priority order (according to the priority field in the tuple). The collection of all known mints must be formed atomically against later mint arrivals and other state changes. If the node receives a prune message from another node in group G, then it need not do anything and must assume that the downstream peer has forgotten all the mints for group G.
- a MINT-PM receives a mint from a peer that is not on the multicast spanning tree for group G, it ignores the update and increments an error counter. This is an error condition, since a peer cannot send mints for group G unless it had previously joined the group.
- the MINT-PM at the rendezvous point removes the corresponding mint from its data store and propagates a relinquish message to each of its child peers on the current multicast spanning tree for G.
- the MINT-PM When the MINT-PM receives a relinquish message for a mint from a parent peer, it consults its data store (indexed by owner and name). If a tuple with the same owner and name exists, it removes the corresponding mint from its data store and propagates a relinquish message to each of its child peers on the current multicast spanning tree for G. If no mint with that name and owner exists, an error counter is incremented to indicate the error condition. If a relinquish message is received from a non-parent peer, a shadow table is updated and will be used if that non-parent peer becomes a parent. Any events associated with the relinquishment of a mint are dispatched when the mint is deleted from the data store.
- network groups may be formed by routing protocols that use a rendezvous point (RP) to serve as an anchor for the group.
- RP rendezvous point
- each routing node can directly or indirectly access a specific group and its associated RP.
- FIG. 5 shows the data network 200 and associated MINT processing agents in accordance with the present invention.
- a multicast group A is formed that has routing node 206 as the RP for the group.
- the group A includes routing nodes 202 , 206 , 208 and 204 , which route information to other group A members located downstream from these nodes.
- the present invention is suitable for use with groups formed in a single domain or with groups formed from nodes in multiple domains, as shown by the inclusion of node 204 of domain X in group A.
- FIG. 6 shows a block diagram for a method 600 of operating the network 200 of FIG. 5 in accordance with the present invention.
- the method 600 begins with the formation of group A having the RP for group A located at node 206 as shown in FIG. 5 .
- the information source 220 publishes a mint to group A. For example, in one embodiment, information source 220 transmits a publish command to group A (which includes mint information), to node 202 . As part of the underlying routing protocol, node 202 is aware that node 206 is the RP for the group A. As a result, node 202 routes the publish instruction toward the RP where it is eventually received. In another embodiment, the information source 220 can another embodiment, the information source 220 can query the network, using a directory
- the RP receives the publish command where it is forwarded to the MINT processing agent 236 as shown by transaction path 504 .
- the MINT processing agent 236 updates its data store with the new mint information. This operation occurs when the MINT-PM 302 receives the published mint over the MINT channel 308 and uses the mint information to update its associated data store 304 .
- the updated mint information is propagated to other MINT processing agents in group A, namely, agents 232 , 234 and 240 .
- the MINT-PM associated with the RP distributes the new mint information to the other MINT processing agents in the group A via the MINT channel.
- the MINT processing agent 236 publishes the new MINT information to group A and the update follows the group A routing as determined by the underlying routing protocol.
- transaction path 506 shows the routing of the newly published mint information from the MINT processing agent 236 to the other MINT processing agents in group A. Consistency is maintained by following the mint distribution rules as set forth above. As a result, the mint information published by information source 220 is reliably distributed to all the MINT processing agents in group A.
- FIG. 7 shows the network 200 of FIG. 5 wherein changes to the membership of group A are shown. Note, for example, that the membership for group A is increased from the group designated by line 750 , to the group designated by line 752 , which includes node 212 .
- FIG. 8 shows a block diagram for a method 800 of operating the network of FIG. 7 in accordance with the present invention.
- the client 226 requests to join the group A, and as a result, the group A is expanded to include node 212 and MINT processing agent 242 .
- the method 800 describes how mint information is reliably propagated to accommodate the new group member.
- the current membership of group A includes nodes 202 , 204 , 206 and 208 as shown at 750 in FIG. 7 .
- the client 228 transmits a request to node 212 to join group A.
- the client 228 may wish to receive information currently being multicasted over group A.
- the node 212 forwards the join request to node 206 , which is already a member of group A.
- the join request is shown by transaction path 702 .
- node 212 When node 212 receives the join request from node 212 , node 212 will be included in the spanning tree for group A, so that the group A membership is shown by 752 .
- the MINT processing agent 242 which is associated with node 212 also becomes a member of group A.
- node 206 notifies the MINT processing agent 236 that node 212 , and its associated MINT processing agent 242 , have joined group A. This is shown by transaction path 704 .
- the MINT processing agent 236 propagates mints relating to group A from its MINT data store to newly added MINT processing agent 242 . The mints are propagated over the MINT channel when the MINT-PM of agent 236 publishes mint information to the MINT-PM of agent 242 . This is shown by transaction path 706 .
- the MINT processing agent 242 updates its data store with the new MINT information so that all of the MINT processing agents in group A have identical group A MINT information. Note that MINT agent 236 is the parent peer of MINT agent 242 . If the MINT processing agent 242 was a parent peer to other newly attached MINT processing agents, it would re-publish the new mint information to those other MINT processing agents.
- the mint propagation may follow the route established as a result of the join request.
- the mint information propagates in the reverse direction (compared to the join request); hop by hop starting from the node in the group that received the join request back to the join requestor.
- Each MINT processing agent in the reverse hop by hop route is updated until all the MINT processing agents associated with the new branch of the spanning tree for the group are updated.
- the source 220 desires to publish updated mint information to group A.
- the source 220 transmits a publish command to the RP via node 202 . This is shown by transaction path 708 .
- the source may use one of several ways to transmit the publish command to the RP for group A.
- the RP receives the publish command from the source 220 .
- the RP notifies the MINT processing agent 236 of the publish command (path 708 ) and the MINT processing agent 236 receives the mints and updates its data store based on the mints in the new publish command.
- the MINT processing agent 236 propagates (as parent) the new mint information to all peer MINT processing agents (child peers) associated with the group A.
- the MINT processing agent 236 issues a publish command over the MINT channel to other members of group A, as shown by transaction path 710 .
- the new mint information is reliably propagated to the nodes 202 , 204 , 208 and 212 , which are all part of group A and child peers to agent 236 .
- the new mint information published by agent 236 only need propagate one hop to reach the child peers as shown in FIG. 7 .
- the child peers can re-publish the mint information (as parents) to other nodes in group A.
- the new mint information would propagate hop by hop (from parent to child) down the spanning tree to all nodes (and MINT processing agents) associated with group A.
- the MINT processing agents 232 , 234 , 240 and 242 all receive the new mint information and update their associated data stores with the new mint information.
- the newly published mint information is reliably distributed to all MINT processing agents associated with active nodes in the spanning tree of group A.
- the method 800 also illustrates how the mint information can be queried in accordance with the present invention.
- client 228 wishes to query mint information associated with group A.
- the client 228 transmits a query instruction to node 212 that specifies group A as the group of interest.
- the type of query used will return all known names (and respective origins) of data bindings that have been published into the network for group A. For example, the name based query instruction [query_name(A)] above will return this information.
- the MINT processing agent 242 receives the query instruction. This is shown by transaction path 712 .
- the MINT processing agent 242 responds with the requested mint information by transmitting the result of the query to the client 228 as shown by transaction path 714 . This occurs when the MINT-PM at agent 242 retrieve the requested information from its associated mint data store and transmits the result over the MINT channel to the client 228 .
- the client 228 receives the requested mint information, and as a result, the client 228 can use the returned mint information to determine group A status or take action to receive a data stream transmitted in group A.
- FIG. 9 shows a block diagram for a method 900 wherein the client 228 terminates its connection to the group A and relinquishes its MINT binding.
- the method 900 will be discussed with reference to FIG. 10 and FIG. 11 .
- FIGS. 10 and 11 show the network 200 and associated transactions that occur during the operation of the method 900 .
- client 228 is attached to the group A, which consists of nodes and MINT processing agents as shown in FIG. 10 .
- the client 228 requests termination from group A by transmitting a leave request from client 228 to node 212 .
- the route of the leave request is shown at 1002 .
- the node 212 notifies agent 242 that client 228 is terminating its membership from group A, and thus node 212 will be pruned from the group.
- agent 242 discards mints relating to group A. Note, however, that if node 212 is a member of other groups, mints relating to those other groups will be maintained by agent 242 . In other embodiments, agent 242 may maintain mints after leaving the group in accordance with another aspect of the invention as describe in a section below.
- the node 212 propagates the leave request toward the RP (node 206 ) where it will eventually be received.
- the RP notifies agent 236 of the leave request (by client 228 ) as shown at transaction path 1004 .
- the agent 236 maintains it store of mints for the group A since it is associated with the RP for the group. As long as group A exists, the agent 236 will maintain its data store of mints, in case it is required to propagate them to other group members.
- the RP processes the leave request from client 228 , and as a result, the node 212 is pruned from the group A. After this occurs, the resulting group comprises nodes 202 , 206 and 208 as shown by the group A of FIG. 11 .
- the information source 220 publishes a new mint relating to the group A.
- the node 202 receives the publish command and routes it toward the RP.
- the information source may find the location of the RP and issue the publish command directly to the RP.
- the node 202 may know the location of the RP, as a result of the underlying group routing protocol, and therefore, route the publish command toward the RP. This transaction is shown at transaction path 1102 .
- the RP receives the publish command and forwards the published mints to the MINT processing agent 236 , as shown at transaction path 1104 .
- the MINT processing agent 236 updates its data store with the new mint information.
- the MINT processing agent propagates the new mint information to the other MINT processing agents in the group A, namely, agents 232 , 234 and 240 . This is shown by transaction paths 1106 .
- the mint propagation occurs when the agent 236 issues a publish command with the new mint information to other nodes in the group A.
- the MINT processing agent 242 will no longer be updated with new mint information for group A.
- the MINT protocol will continue to reliably update the mint data stores for MINT processing agents that are active members of the group A. Should node 212 request to join the group A in the future, the updated mints would again be propagated to node 212 and thereafter to MINT processing agent 242 .
- the MINT protocol operates to overcome problems associated with excessive routing fluctuations. During excessive routing fluctuations, where particular nodes repeatedly leave and then re-join the group, the mint information in the data stores associated with those nodes is repeatedly discarded and repopulated. This results in excessive transmission of mint information on the data network. To avoid this problem, enhancements to the MINT protocol avoid discarding and repopulating the data stores as a result of excessive routing changes.
- a MINT digest is computed over the mints in the data store.
- the MINT digest may represent all mints in the data store or selected portions of the mints in the data store. Instead of discarding the mint information when a node leaves the group, the mint information associated with that node is preserved in the data store along with its associated MINT digest. When that node rejoins the group, it transmits its MINT digest to the group. If the MINT digest at the node is different from the current MINT digest for the group, then the node is updated with a new copy of the mint information. The node then updates its mint data store and its associated digest. If the MINT digest from the node matches the MINT digest for the group, then it is not necessary to transmit a new copy of the mint information to the node. Therefore, the enhanced MINT protocol averts the excessive transmission of mint information in the network.
- a time parameter is used to prevent the resources of the data stores from being utilized to store outdated mint information.
- the MINT processing agent associated with that node uses the time parameter to determine how long to preserve the mint information in the data store.
- the time parameter value can be determined by a network administrator.
- the mint data store can be purged of mints for that group, thereby freeing up resources of the data store. Therefore, the MINT processing agent preserves the data store to prevent redundant mint transmissions during network flapping, and after expiration of a selected time period, purges the data store to free up valuable resources to store additional mints.
Abstract
Description
group | The name of the multicast group |
origin | The primary IP address of the node that published this entry. |
name | The string-based name of this entry. Each tuple has a unique |
ε | name relative to its origin. |
value | The value of the named data. This may be arbitrary binary data |
whose semantics are outside the scope of this protocol | |
specification. | |
priority | A small integer that represents the delivery priority of this |
(pri) | item compared to other published values (higher priority values |
tend to be sent before lower priority values). | |
-
- publish messages cause a MINT to be created, propagated, and maintained across the broadcast tree spanned by the group in question,
- relinquish messages explicitly tear down a mint binding on behalf of its origin.
- query messages allow the MINT data store to be queried for name and value information.
publish(G, name, | Publish a named value into the network on the MINT |
value, pri) | channel for group G. The data value will be reliably |
delivered to all nodes in the network that span the | |
multicast routing tree for G, which will each be able to | |
access to the value by name with the query call. The | |
pri parameter is a small integer that represents the | |
delivery priority of this item compared to other | |
published values (higher priority values tend to be | |
sent before lower priority values). This binding is | |
maintained until the origin terminates or relinquishes | |
the binding with a relinquish command. A value may | |
not be successfully published when the amount of state | |
exceeds the supportable limit. In this case, an error | |
is delivered asynchronously some time after the | |
publishing call is made. | |
relinquish(G, | Relinquish the named value that was previously |
name) | published into the network on the MINT channel |
for group G. | |
query_names(G) | Return all known names (and respective origins) |
of data bindings that have been published into | |
the network for group G. | |
query(G, origin. | Query the value of the MINT that has been published |
name) | into the network for group G, whose key is name and |
origin is origin. Returns the value. | |
The Publish Message
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/618,369 US8886826B2 (en) | 1999-06-01 | 2003-07-10 | System for multipoint infrastructure transport in a computer network |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13715399P | 1999-06-01 | 1999-06-01 | |
US09/412,815 US6850987B1 (en) | 1999-06-01 | 1999-10-05 | System for multipoint infrastructure transport in a computer network |
US10/618,369 US8886826B2 (en) | 1999-06-01 | 2003-07-10 | System for multipoint infrastructure transport in a computer network |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US09/412,815 Division US6850987B1 (en) | 1999-06-01 | 1999-10-05 | System for multipoint infrastructure transport in a computer network |
Publications (2)
Publication Number | Publication Date |
---|---|
US20040139150A1 US20040139150A1 (en) | 2004-07-15 |
US8886826B2 true US8886826B2 (en) | 2014-11-11 |
Family
ID=26834975
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US09/412,815 Expired - Lifetime US6850987B1 (en) | 1999-06-01 | 1999-10-05 | System for multipoint infrastructure transport in a computer network |
US10/618,369 Active 2026-12-23 US8886826B2 (en) | 1999-06-01 | 2003-07-10 | System for multipoint infrastructure transport in a computer network |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US09/412,815 Expired - Lifetime US6850987B1 (en) | 1999-06-01 | 1999-10-05 | System for multipoint infrastructure transport in a computer network |
Country Status (7)
Country | Link |
---|---|
US (2) | US6850987B1 (en) |
EP (1) | EP1183820B1 (en) |
JP (1) | JP4685299B2 (en) |
KR (1) | KR100699018B1 (en) |
AU (1) | AU5321300A (en) |
DE (1) | DE60008102T2 (en) |
WO (1) | WO2000074312A1 (en) |
Families Citing this family (65)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6850987B1 (en) * | 1999-06-01 | 2005-02-01 | Fastforward Networks, Inc. | System for multipoint infrastructure transport in a computer network |
JP2001186142A (en) * | 1999-12-27 | 2001-07-06 | Toshiba Corp | Packet buffer device and packet switching device |
US8396950B1 (en) * | 2000-03-02 | 2013-03-12 | Rockstar Consortium Us Lp | Method and apparatus for the fast detection of connectivity loss between devices in a network |
EP1133101A1 (en) * | 2000-03-07 | 2001-09-12 | BRITISH TELECOMMUNICATIONS public limited company | Data distribution |
US6917985B2 (en) * | 2000-03-10 | 2005-07-12 | The Regents Of The University Of California | Core assisted mesh protocol for multicast routing in ad-hoc Networks |
US6886160B1 (en) * | 2000-11-29 | 2005-04-26 | Hyung Sup Lee | Distribution of mainframe data in the PC environment |
JP4225681B2 (en) * | 2000-12-06 | 2009-02-18 | 富士通株式会社 | Virtual closed network construction method and apparatus, and relay apparatus |
US7830787B1 (en) * | 2001-09-25 | 2010-11-09 | Cisco Technology, Inc. | Flooding control for multicast distribution tunnel |
GB2381427A (en) * | 2001-10-27 | 2003-04-30 | Hewlett Packard Co | Spanning tree in peer to peer networks |
US8688853B2 (en) * | 2001-12-21 | 2014-04-01 | Agere Systems Llc | Method and apparatus for maintaining multicast lists in a data network |
US7304955B2 (en) * | 2002-03-28 | 2007-12-04 | Motorola, Inc. | Scalable IP multicast with efficient forwarding cache |
US7302691B2 (en) * | 2002-05-10 | 2007-11-27 | Sonics, Incorporated | Scalable low bandwidth multicast handling in mixed core systems |
KR100871118B1 (en) | 2002-05-18 | 2008-11-28 | 엘지전자 주식회사 | Management method for multicast group |
US7089323B2 (en) * | 2002-06-21 | 2006-08-08 | Microsoft Corporation | Method for multicasting a message on a computer network |
US7180866B1 (en) * | 2002-07-11 | 2007-02-20 | Nortel Networks Limited | Rerouting in connection-oriented communication networks and communication systems |
US7406535B2 (en) * | 2002-12-20 | 2008-07-29 | Symantec Operating Corporation | Role-based message addressing for a computer network |
US7327741B1 (en) | 2002-12-20 | 2008-02-05 | Symantec Operating Corporation | Detecting and breaking cycles in a computer network |
US7467194B1 (en) | 2002-12-20 | 2008-12-16 | Symantec Operating Corporation | Re-mapping a location-independent address in a computer network |
US8275864B1 (en) | 2002-12-20 | 2012-09-25 | Symantec Operating Corporation | Peer-to-peer network with recovery capability |
US7404006B1 (en) | 2002-12-20 | 2008-07-22 | Symantec Operating Corporation | Publishing a network address in a computer network |
US8370523B1 (en) | 2002-12-20 | 2013-02-05 | Symantec Operating Corporation | Managing routing information for a computer network |
US7653059B1 (en) | 2002-12-20 | 2010-01-26 | Symantec Operating Corporation | Communication sessions for a computer network |
US7292585B1 (en) | 2002-12-20 | 2007-11-06 | Symantec Operating Corporation | System and method for storing and utilizing routing information in a computer network |
US7233987B2 (en) * | 2002-12-20 | 2007-06-19 | Alcatel Canada Inc. | System and method for converting requests between different multicast protocols in a communication network |
KR100552506B1 (en) * | 2003-03-28 | 2006-02-14 | 삼성전자주식회사 | method for construction of CBT direction based for overlay multicast CBT based |
US8886705B1 (en) | 2003-06-30 | 2014-11-11 | Symantec Operating Corporation | Goal-oriented storage management for a distributed data storage network |
US7197508B1 (en) * | 2003-07-25 | 2007-03-27 | Brown Iii Frederick R | System and method for obtaining, evaluating, and reporting market information |
KR100943901B1 (en) * | 2003-08-19 | 2010-02-24 | 엘지전자 주식회사 | Method of Sharing Radio Protocol Entity for Broadcasting and Multicast |
CN1898901A (en) | 2003-10-31 | 2007-01-17 | 丛林网络公司 | Enforcing access control on multicast transmissions |
US7555527B1 (en) | 2003-11-07 | 2009-06-30 | Symantec Operating Corporation | Efficiently linking storage object replicas in a computer network |
US8060619B1 (en) | 2003-11-07 | 2011-11-15 | Symantec Operating Corporation | Direct connections to a plurality of storage object replicas in a computer network |
US7680950B1 (en) | 2003-11-07 | 2010-03-16 | Symantec Operating Corporation | Efficient search for storage objects in a network |
US7570600B1 (en) | 2003-12-17 | 2009-08-04 | Symantec Operating Corporation | Overlay network with efficient routing and recovery |
JP2008503011A (en) * | 2004-06-08 | 2008-01-31 | ダートデバイセズ コーポレーション | Architecture device and method for device team recruitment and content rendition for universal device interoperability platform |
EP1759488A1 (en) * | 2004-06-14 | 2007-03-07 | Bryan Shadish | Distributed igmp processing |
CN100481013C (en) * | 2004-08-03 | 2009-04-22 | 索芙特瑞斯提股份有限公司 | System and method for controlling inter-application association through contextual policy control |
US7958262B2 (en) * | 2004-10-22 | 2011-06-07 | Microsoft Corporation | Allocating and reclaiming resources within a rendezvous federation |
US20060090003A1 (en) * | 2004-10-22 | 2006-04-27 | Microsoft Corporation | Rendezvousing resource requests with corresponding resources |
US8014321B2 (en) * | 2004-10-22 | 2011-09-06 | Microsoft Corporation | Rendezvousing resource requests with corresponding resources |
US7694167B2 (en) * | 2004-10-22 | 2010-04-06 | Microsoft Corporation | Maintaining routing consistency within a rendezvous federation |
US8095600B2 (en) * | 2004-10-22 | 2012-01-10 | Microsoft Corporation | Inter-proximity communication within a rendezvous federation |
US8392515B2 (en) * | 2004-10-22 | 2013-03-05 | Microsoft Corporation | Subfederation creation and maintenance in a federation infrastructure |
US8095601B2 (en) * | 2004-10-22 | 2012-01-10 | Microsoft Corporation | Inter-proximity communication within a rendezvous federation |
US7730220B2 (en) | 2004-10-22 | 2010-06-01 | Microsoft Corporation | Broadcasting communication within a rendezvous federation |
US8549180B2 (en) * | 2004-10-22 | 2013-10-01 | Microsoft Corporation | Optimizing access to federation infrastructure-based resources |
US8090880B2 (en) | 2006-11-09 | 2012-01-03 | Microsoft Corporation | Data consistency within a federation infrastructure |
US20110082928A1 (en) | 2004-10-22 | 2011-04-07 | Microsoft Corporation | Maintaining consistency within a federation infrastructure |
US7747573B2 (en) * | 2004-11-18 | 2010-06-29 | International Business Machines Corporation | Updating elements in a data storage facility using a predefined state machine, with serial activation |
FR2878676B1 (en) * | 2004-11-30 | 2007-02-09 | Thales Sa | MULTI-ROUTING-TOPOLOGY AND MULTIPLE-SERVICES SYSTEM |
US7400596B1 (en) | 2005-08-17 | 2008-07-15 | Rockwell Collins, Inc. | Dynamic, multicast routing using a quality of service manager |
US7698439B2 (en) * | 2006-09-25 | 2010-04-13 | Microsoft Corporation | Application programming interface for efficient multicasting of communications |
US8094585B2 (en) * | 2006-10-31 | 2012-01-10 | International Business Machines Corporation | Membership management of network nodes |
US9218213B2 (en) | 2006-10-31 | 2015-12-22 | International Business Machines Corporation | Dynamic placement of heterogeneous workloads |
JP4903642B2 (en) * | 2007-07-11 | 2012-03-28 | 日本無線株式会社 | Overlay multicast system |
US8798081B2 (en) * | 2009-02-16 | 2014-08-05 | Nec Corporation | Event delivery system, rendezvous node, broker node, load distribution method for event delivery system, load distribution method for rendezvous node, delivery route construction method for broker node, storage medium storing load distribution program, and storage medium storing delivery route construction program |
US8688779B2 (en) | 2009-04-08 | 2014-04-01 | Blackberry Limited | Publishing location for a limited time |
KR100928243B1 (en) | 2009-07-28 | 2009-11-24 | 엑스투커머스 주식회사 | Tree-based network hard disk copy system based on Linux |
US8850053B2 (en) | 2010-04-08 | 2014-09-30 | At&T Intellectual Property I, L.P. | System and method for providing information to users of a communication network |
KR20130047862A (en) | 2011-11-01 | 2013-05-09 | 삼성전자주식회사 | Method and apparatus for estimating available bandwidth of each of user equipments in a communication system |
US8868672B2 (en) * | 2012-05-14 | 2014-10-21 | Advanced Micro Devices, Inc. | Server node interconnect devices and methods |
US9137173B2 (en) | 2012-06-19 | 2015-09-15 | Advanced Micro Devices, Inc. | Devices and methods for interconnecting server nodes |
US8930595B2 (en) | 2012-06-21 | 2015-01-06 | Advanced Micro Devices, Inc. | Memory switch for interconnecting server nodes |
US9253287B2 (en) | 2012-08-20 | 2016-02-02 | Advanced Micro Devices, Inc. | Speculation based approach for reliable message communications |
US8875256B2 (en) | 2012-11-13 | 2014-10-28 | Advanced Micro Devices, Inc. | Data flow processing in a network environment |
KR102008670B1 (en) * | 2019-04-18 | 2019-08-08 | 주식회사 유니온플레이스 | Apparatus of monitoring multicast group |
Citations (49)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4491909A (en) * | 1981-03-18 | 1985-01-01 | International Business Machines Corporation | Data processing system having shared memory |
US4807224A (en) | 1987-08-21 | 1989-02-21 | Naron Steven E | Multicast data distribution system and method |
US5331637A (en) * | 1993-07-30 | 1994-07-19 | Bell Communications Research, Inc. | Multicast routing using core based trees |
US5517494A (en) * | 1994-09-30 | 1996-05-14 | Apple Computer, Inc. | Method and system of multicast routing for groups with a single transmitter |
US5530963A (en) * | 1993-12-16 | 1996-06-25 | International Business Machines Corporation | Method and system for maintaining routing between mobile workstations and selected network workstation using routing table within each router device in the network |
US5572678A (en) * | 1992-01-24 | 1996-11-05 | Hitachi, Ltd. | System for sending frames from sender to receiver using connectionless protocol and receiving acknowledging frame and retransmission request frame from receiver using connection oriented protocol |
US5606705A (en) * | 1994-04-15 | 1997-02-25 | Honeywell Inc. | Communication coordinator for messages to be sent from at least one data source to a plurality of clients |
US5634011A (en) | 1992-06-18 | 1997-05-27 | International Business Machines Corporation | Distributed management communications network |
US5640553A (en) * | 1995-09-15 | 1997-06-17 | Infonautics Corporation | Relevance normalization for documents retrieved from an information retrieval system in response to a query |
US5721951A (en) * | 1995-02-24 | 1998-02-24 | Digital Interactive Corporation Systems, Ltd. | Home entertainment system for playing software designed for play in home computer |
US5751338A (en) | 1994-12-30 | 1998-05-12 | Visionary Corporate Technologies | Methods and systems for multimedia communications via public telephone networks |
US5793962A (en) * | 1996-04-30 | 1998-08-11 | International Business Machines Corporation | System for managing membership of a group of processors in a distributed computing environment |
US5831975A (en) * | 1996-04-04 | 1998-11-03 | Lucent Technologies Inc. | System and method for hierarchical multicast routing in ATM networks |
US5898686A (en) * | 1995-04-25 | 1999-04-27 | Cabletron Systems, Inc. | Network bridge with multicast forwarding table |
US5903559A (en) | 1996-12-20 | 1999-05-11 | Nec Usa, Inc. | Method for internet protocol switching over fast ATM cell transport |
US5946316A (en) | 1997-01-17 | 1999-08-31 | Lucent Technologies, Inc. | Dynamic distributed multicast routing protocol |
US5956716A (en) | 1995-06-07 | 1999-09-21 | Intervu, Inc. | System and method for delivery of video data over a computer network |
US6003030A (en) | 1995-06-07 | 1999-12-14 | Intervu, Inc. | System and method for optimized storage and retrieval of data on a distributed computer network |
US6052718A (en) | 1997-01-07 | 2000-04-18 | Sightpath, Inc | Replica routing |
US6112239A (en) | 1997-06-18 | 2000-08-29 | Intervu, Inc | System and method for server-side optimization of data delivery on a distributed computer network |
US6138144A (en) * | 1997-06-24 | 2000-10-24 | At&T Corp. | Method for managing multicast addresses for transmitting and receiving multimedia conferencing information on an internet protocol (IP) network implemented over an ATM network |
US6148005A (en) | 1997-10-09 | 2000-11-14 | Lucent Technologies Inc | Layered video multicast transmission system with retransmission-based error recovery |
US6219706B1 (en) * | 1998-10-16 | 2001-04-17 | Cisco Technology, Inc. | Access control for networks |
US6243758B1 (en) * | 1996-06-06 | 2001-06-05 | Nec Corporation | Internetwork multicast routing using flag bits indicating selective participation of mobile hosts in group activities within scope |
US6286052B1 (en) | 1998-12-04 | 2001-09-04 | Cisco Technology, Inc. | Method and apparatus for identifying network data traffic flows and for applying quality of service treatments to the flows |
US6321270B1 (en) * | 1996-09-27 | 2001-11-20 | Nortel Networks Limited | Method and apparatus for multicast routing in a network |
US6331983B1 (en) * | 1997-05-06 | 2001-12-18 | Enterasys Networks, Inc. | Multicast switching |
US6411616B1 (en) * | 1996-11-12 | 2002-06-25 | Starguide Digital Networks, Inc. | High bandwidth broadcast system having localized multicast access to broadcast content |
US20020107966A1 (en) * | 2001-02-06 | 2002-08-08 | Jacques Baudot | Method and system for maintaining connections in a network |
US20030084371A1 (en) * | 2001-10-25 | 2003-05-01 | Alcatel | Fault-tolerant IS-IS routing system, and a corresponding method |
US20030165140A1 (en) * | 1999-04-30 | 2003-09-04 | Cheng Tang | System and method for distributing multicasts in virtual local area networks |
US6631420B1 (en) * | 1999-02-25 | 2003-10-07 | Nortel Networks Limited | Reducing convergence time by a protocol independent multicast (PIM) router |
US20030195962A1 (en) * | 2002-04-10 | 2003-10-16 | Satoshi Kikuchi | Load balancing of servers |
US20030217096A1 (en) * | 2001-12-14 | 2003-11-20 | Mckelvie Samuel J. | Agent based application using data synchronization |
US6684331B1 (en) * | 1999-12-22 | 2004-01-27 | Cisco Technology, Inc. | Method and apparatus for distributing and updating group controllers over a wide area network using a tree structure |
US6711163B1 (en) * | 1999-03-05 | 2004-03-23 | Alcatel | Data communication system with distributed multicasting |
US20040073646A1 (en) * | 2000-12-21 | 2004-04-15 | Cho Mi-Hwa | Apparatus and method for providing real-time information |
US20040078619A1 (en) * | 2002-06-27 | 2004-04-22 | Nishit Vasavada | Method and system for implementing IS-IS protocol redundancy |
US6735633B1 (en) * | 1999-06-01 | 2004-05-11 | Fast Forward Networks | System for bandwidth allocation in a computer network |
US6785274B2 (en) * | 1998-10-07 | 2004-08-31 | Cisco Technology, Inc. | Efficient network multicast switching apparatus and methods |
US6795433B1 (en) * | 1999-10-21 | 2004-09-21 | Nortel Networks Limited | Multicast routing cache |
US20040199808A1 (en) * | 2003-04-02 | 2004-10-07 | International Business Machines Corporation | State recovery and failover of intelligent network adapters |
US6850987B1 (en) * | 1999-06-01 | 2005-02-01 | Fastforward Networks, Inc. | System for multipoint infrastructure transport in a computer network |
US6873618B1 (en) * | 1999-03-16 | 2005-03-29 | Nortel Networks Limited | Multipoint network routing protocol |
US20050102427A1 (en) * | 2002-08-09 | 2005-05-12 | Daisuke Yokota | Stream contents distribution system and proxy server |
US6983317B1 (en) * | 2000-02-28 | 2006-01-03 | Microsoft Corporation | Enterprise management system |
US7092399B1 (en) * | 2001-10-16 | 2006-08-15 | Cisco Technology, Inc. | Redirecting multiple requests received over a connection to multiple servers and merging the responses over the connection |
US7111035B2 (en) * | 2001-12-26 | 2006-09-19 | Hewlett-Packard Development Company, L.P. | Fault tolerance associations for IP transport protocols |
US7162737B2 (en) * | 2001-02-13 | 2007-01-09 | Stonesoft | Synchronization of security gateway state information |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH03273727A (en) * | 1990-03-23 | 1991-12-04 | Hitachi Ltd | Changing system for network constitution |
US5706431A (en) * | 1995-12-29 | 1998-01-06 | At&T | System and method for distributively propagating revisions through a communications network |
US5748736A (en) * | 1996-06-14 | 1998-05-05 | Mittra; Suvo | System and method for secure group communications via multicast or broadcast |
JP3150624B2 (en) * | 1996-09-13 | 2001-03-26 | 株式会社ディジタル・ビジョン・ラボラトリーズ | Communication system and communication path control method |
US6154463A (en) * | 1997-08-26 | 2000-11-28 | Lucent Technologies, Inc. | System and method for multicast conferencing and online discussion groups |
US6115749A (en) * | 1997-10-14 | 2000-09-05 | Lucent Technologies Inc. | System and method for using a window mechanism to control multicast data congestion |
US6671276B1 (en) * | 1997-11-18 | 2003-12-30 | Nec Corporation | Switch based network architecture for IP multicast and integrated services |
US6487170B1 (en) * | 1998-11-18 | 2002-11-26 | Nortel Networks Limited | Providing admission control and network quality of service with a distributed bandwidth broker |
US6370565B1 (en) * | 1999-03-01 | 2002-04-09 | Sony Corporation Of Japan | Method of sharing computation load within a distributed virtual environment system |
US6269080B1 (en) * | 1999-04-13 | 2001-07-31 | Glenayre Electronics, Inc. | Method of multicast file distribution and synchronization |
JP3623694B2 (en) * | 1999-07-02 | 2005-02-23 | 株式会社エヌ・ティ・ティ・ドコモ | Information distribution system and information distribution method |
-
1999
- 1999-10-05 US US09/412,815 patent/US6850987B1/en not_active Expired - Lifetime
-
2000
- 2000-06-01 KR KR1020017015487A patent/KR100699018B1/en active IP Right Grant
- 2000-06-01 DE DE60008102T patent/DE60008102T2/en not_active Expired - Lifetime
- 2000-06-01 JP JP2001500493A patent/JP4685299B2/en not_active Expired - Lifetime
- 2000-06-01 EP EP00938128A patent/EP1183820B1/en not_active Expired - Lifetime
- 2000-06-01 AU AU53213/00A patent/AU5321300A/en not_active Abandoned
- 2000-06-01 WO PCT/US2000/015417 patent/WO2000074312A1/en active IP Right Grant
-
2003
- 2003-07-10 US US10/618,369 patent/US8886826B2/en active Active
Patent Citations (49)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4491909A (en) * | 1981-03-18 | 1985-01-01 | International Business Machines Corporation | Data processing system having shared memory |
US4807224A (en) | 1987-08-21 | 1989-02-21 | Naron Steven E | Multicast data distribution system and method |
US5572678A (en) * | 1992-01-24 | 1996-11-05 | Hitachi, Ltd. | System for sending frames from sender to receiver using connectionless protocol and receiving acknowledging frame and retransmission request frame from receiver using connection oriented protocol |
US5634011A (en) | 1992-06-18 | 1997-05-27 | International Business Machines Corporation | Distributed management communications network |
US5331637A (en) * | 1993-07-30 | 1994-07-19 | Bell Communications Research, Inc. | Multicast routing using core based trees |
US5530963A (en) * | 1993-12-16 | 1996-06-25 | International Business Machines Corporation | Method and system for maintaining routing between mobile workstations and selected network workstation using routing table within each router device in the network |
US5606705A (en) * | 1994-04-15 | 1997-02-25 | Honeywell Inc. | Communication coordinator for messages to be sent from at least one data source to a plurality of clients |
US5517494A (en) * | 1994-09-30 | 1996-05-14 | Apple Computer, Inc. | Method and system of multicast routing for groups with a single transmitter |
US5751338A (en) | 1994-12-30 | 1998-05-12 | Visionary Corporate Technologies | Methods and systems for multimedia communications via public telephone networks |
US5721951A (en) * | 1995-02-24 | 1998-02-24 | Digital Interactive Corporation Systems, Ltd. | Home entertainment system for playing software designed for play in home computer |
US5898686A (en) * | 1995-04-25 | 1999-04-27 | Cabletron Systems, Inc. | Network bridge with multicast forwarding table |
US6003030A (en) | 1995-06-07 | 1999-12-14 | Intervu, Inc. | System and method for optimized storage and retrieval of data on a distributed computer network |
US5956716A (en) | 1995-06-07 | 1999-09-21 | Intervu, Inc. | System and method for delivery of video data over a computer network |
US5640553A (en) * | 1995-09-15 | 1997-06-17 | Infonautics Corporation | Relevance normalization for documents retrieved from an information retrieval system in response to a query |
US5831975A (en) * | 1996-04-04 | 1998-11-03 | Lucent Technologies Inc. | System and method for hierarchical multicast routing in ATM networks |
US5793962A (en) * | 1996-04-30 | 1998-08-11 | International Business Machines Corporation | System for managing membership of a group of processors in a distributed computing environment |
US6243758B1 (en) * | 1996-06-06 | 2001-06-05 | Nec Corporation | Internetwork multicast routing using flag bits indicating selective participation of mobile hosts in group activities within scope |
US6321270B1 (en) * | 1996-09-27 | 2001-11-20 | Nortel Networks Limited | Method and apparatus for multicast routing in a network |
US6411616B1 (en) * | 1996-11-12 | 2002-06-25 | Starguide Digital Networks, Inc. | High bandwidth broadcast system having localized multicast access to broadcast content |
US5903559A (en) | 1996-12-20 | 1999-05-11 | Nec Usa, Inc. | Method for internet protocol switching over fast ATM cell transport |
US6052718A (en) | 1997-01-07 | 2000-04-18 | Sightpath, Inc | Replica routing |
US5946316A (en) | 1997-01-17 | 1999-08-31 | Lucent Technologies, Inc. | Dynamic distributed multicast routing protocol |
US6331983B1 (en) * | 1997-05-06 | 2001-12-18 | Enterasys Networks, Inc. | Multicast switching |
US6112239A (en) | 1997-06-18 | 2000-08-29 | Intervu, Inc | System and method for server-side optimization of data delivery on a distributed computer network |
US6138144A (en) * | 1997-06-24 | 2000-10-24 | At&T Corp. | Method for managing multicast addresses for transmitting and receiving multimedia conferencing information on an internet protocol (IP) network implemented over an ATM network |
US6148005A (en) | 1997-10-09 | 2000-11-14 | Lucent Technologies Inc | Layered video multicast transmission system with retransmission-based error recovery |
US6785274B2 (en) * | 1998-10-07 | 2004-08-31 | Cisco Technology, Inc. | Efficient network multicast switching apparatus and methods |
US6219706B1 (en) * | 1998-10-16 | 2001-04-17 | Cisco Technology, Inc. | Access control for networks |
US6286052B1 (en) | 1998-12-04 | 2001-09-04 | Cisco Technology, Inc. | Method and apparatus for identifying network data traffic flows and for applying quality of service treatments to the flows |
US6631420B1 (en) * | 1999-02-25 | 2003-10-07 | Nortel Networks Limited | Reducing convergence time by a protocol independent multicast (PIM) router |
US6711163B1 (en) * | 1999-03-05 | 2004-03-23 | Alcatel | Data communication system with distributed multicasting |
US6873618B1 (en) * | 1999-03-16 | 2005-03-29 | Nortel Networks Limited | Multipoint network routing protocol |
US20030165140A1 (en) * | 1999-04-30 | 2003-09-04 | Cheng Tang | System and method for distributing multicasts in virtual local area networks |
US6850987B1 (en) * | 1999-06-01 | 2005-02-01 | Fastforward Networks, Inc. | System for multipoint infrastructure transport in a computer network |
US6735633B1 (en) * | 1999-06-01 | 2004-05-11 | Fast Forward Networks | System for bandwidth allocation in a computer network |
US6795433B1 (en) * | 1999-10-21 | 2004-09-21 | Nortel Networks Limited | Multicast routing cache |
US6684331B1 (en) * | 1999-12-22 | 2004-01-27 | Cisco Technology, Inc. | Method and apparatus for distributing and updating group controllers over a wide area network using a tree structure |
US6983317B1 (en) * | 2000-02-28 | 2006-01-03 | Microsoft Corporation | Enterprise management system |
US20040073646A1 (en) * | 2000-12-21 | 2004-04-15 | Cho Mi-Hwa | Apparatus and method for providing real-time information |
US20020107966A1 (en) * | 2001-02-06 | 2002-08-08 | Jacques Baudot | Method and system for maintaining connections in a network |
US7162737B2 (en) * | 2001-02-13 | 2007-01-09 | Stonesoft | Synchronization of security gateway state information |
US7092399B1 (en) * | 2001-10-16 | 2006-08-15 | Cisco Technology, Inc. | Redirecting multiple requests received over a connection to multiple servers and merging the responses over the connection |
US20030084371A1 (en) * | 2001-10-25 | 2003-05-01 | Alcatel | Fault-tolerant IS-IS routing system, and a corresponding method |
US20030217096A1 (en) * | 2001-12-14 | 2003-11-20 | Mckelvie Samuel J. | Agent based application using data synchronization |
US7111035B2 (en) * | 2001-12-26 | 2006-09-19 | Hewlett-Packard Development Company, L.P. | Fault tolerance associations for IP transport protocols |
US20030195962A1 (en) * | 2002-04-10 | 2003-10-16 | Satoshi Kikuchi | Load balancing of servers |
US20040078619A1 (en) * | 2002-06-27 | 2004-04-22 | Nishit Vasavada | Method and system for implementing IS-IS protocol redundancy |
US20050102427A1 (en) * | 2002-08-09 | 2005-05-12 | Daisuke Yokota | Stream contents distribution system and proxy server |
US20040199808A1 (en) * | 2003-04-02 | 2004-10-07 | International Business Machines Corporation | State recovery and failover of intelligent network adapters |
Non-Patent Citations (41)
Title |
---|
Amir, E., McCanne, S., and Zhang, H., "An Application Level Video Gateway." ACM Multimedia 1995 (San Francisco, California) Nov. 1995. |
Arango, M., Dugan, A., Elliott, I, Huitema, C., and Pickett, S., "Media Gateway Control Protocol (MGCP) Version 1.0," RFC 2705, Oct. 1999. |
Atwood, J.W., Catrina, O., Fenton, J., and Strayer, W. Timothy, "Reliable Multicasting in the Xpress Transport Protocol," Proceedings of the 21st Local Computer Networks Conference (Minneapolis, Minnesota) Oct. 13-16, 1996. |
Ballardie, A., "Core Based Trees (CBT) Multicast Routing Architecture," Request for Comments, RFC 2201, Sep. 1997. |
Bannerjea, A. Faloutsos, M., and Pankaj, R., "Designing QoSMIC: A Quality of Service Sensitive Multicast Internet Protocol," Internet Draft, Internet Engineering Task Force, May 1998. |
Blazevic, L. and Le Boudec, J., "Distributed Core Multicast (DCM): A Routing Protocol for Many Small Groups with Application to Mobile IP Telephony," Internet Engineering Task Force, Jun. 1999. |
Bormann, C., Ott, J., Gehrcke, H.C., Kerschat, T., and Seifert, N., "Mtp-2: Towards achieving the S.E.R.O. Properties for Multicast Transport," International Conference on Computer Communications Networks, (San Francisco, California), Sep. 1994. |
Casey, L., Cunningham, I., and Eros, R., "IP VPN Realization Using MPLS Tunnels," Internet Engineering Task Force, Nov. 1998. |
Current Claims of 2001-500493, Nov. 2009 (6 pages). |
Decasper, D., Dittia, Z., Parulkar, G., Plattner, B. "Router Plugins: A Software Architecture for Next Generation Routers," ACM SIGCOMM '98 (Vancouver, Canada) Sep. 1998. |
Deering, S. and Cheriton, D., "Multicast Routing in Datagram Internetworks and Extended LANs," ACM Transactions on Computer Systems, vol. 8, No. 2, May 1990, pp. 85-110. |
Deering, S., Estrin, D., Farinacci, D., Jacobson, V., Liu, C., and Wei, L., "The Pim Architecture for Wide-Area Multicast Routing," IEEE/ACM Transactions on Networking, vol. 4, No. 2, Apr. 1996, pp. 153-162. |
Estrin, D., Farinacci, D., Helmy, A., Thaler, D., Deering, S., Handley, M., Jacobson, V., Liu, C., Sharma, P, and Wei, L., "Protocol Independent Multicast-Sparse Mode (PIM-SM)," Request for Comments, RFC 2362, Jun. 1998. |
Farinacci, D., Hanks, S., Li, T., and Traina, P., "Generic Routing Encapsulation Over IPv4 Networks," Request for Comments RFC 1702, Nov. 1994. |
Farinacci, D., Lin, A., Speakman, T., and Tweedly, A., "Pretty Good Multicast (PGM) Transport Protocol Specification," Internet Draft, Internet Engineering Task Force, Jan. 1998. |
Farinacci, D., Wei, L., and Meylor, J., "Use of Anycast Clusters for Inter-Domain Multicast Routing," Internet Draft, Internet Engineering Task Force, Mar. 1998. |
Fenner, W., "Internet Group Management Protocol," Version 2, RFC 2236, Nov. 1997. |
Finlayson, R. "IP Multicast and Firewalls," Internet Draft, May 1999. |
Finlayson, R. "The UDP Multicast Tunneling Protocol," Internet Draft, Jul. 1999. |
Finlayson, R., "A More Loss-Tolerant RTP Payload Format for MP3 Audio," Internet Draft, Jan. 1999. |
Hampton, D., Oran, D., Salama, H., and Shad, D., The IP Telephony Border Gateway Protocol (TBGP), Internet Engineering Task Force, Jun. 1999. |
Handley, M., Crowcroft, J., Borman, C., Ott, J., "Very Large Conferences on the Internet: The Internet Multimedia Conferencing Architecture," Computer Networks, vol. 31, No. 3, 1999 pp. 191-204. |
Hanks, S., Li, T., Farinacci, D., and Trains, P., "Generic Routing Encapsulation," Request for Comments RFC 1701, Oct. 1994. |
Hodel, H., "Policy Tree Multicast Routing: An Extension to Sparse Mode Source Tree Delivery," ACM Computer Communication Review, vol. 28, No. 2 Apr. 1998, pp. 78-97. |
Holbrook, H. and Cheriton, D.R., "IP Multicast Channels: EXPRESS Support for Large-Scale Single-Source Applications," Computer Communication Review, vol. 29, No. 4, Oct. 1999. |
Japanese Office Action received in related case 2001-500493 dated Sep. 2, 2009 (2 pages). |
Kumar, S., Radoslavov, P, Thaler, D., Alaettinoglu, C., Estrin, D., and Handley, M., "The MASC/BGMPL Architecture for Inter-Domain Multicast Routing," SIGCOMM '98 (Vancouver, Canada) Sep. 1998. |
Lin, J.C., and Paul, S., "Rmtp: A Reliable Multicast Transport Protocol," IEEE INFOCOM '96, pp. 1414-1424. |
Maffeis, S., Bischofberger, W.R., and Mätzel, K. -U., "GTS: A Generic Multicast Transport Service," UBILAB Technical Report, 94.6.1, 1994, pp. 1-7. |
Malis,Aa., Heinanen, J., Armitage, G., Gleeson, B., "A Framework for IP Based Virtual Private Networks Internet Draft," Aug. 1999. |
Mittra, S., "Iolus: A Framework for Scalable Secure Multicasting," ACM Computer Communication Review, vol. 27, Oct. 1997, pp. 277-288. |
Parsa, M., and Garcia-Luna-Aceves, J., "Scalable Internet Multicast Routing," Proceedings of ICCCN 95, (Las Vegas, Nevada), Sep. 1995. |
Patent Abstracts of Japan 09-200269 published Jul. 31, 2007 (1 page). |
Perkins, C., and Crowcroft, J., Real-Time Audio and Video Transmission of IEEE GLOBECOM '96 Over the Internet, IEEE Communications Magazine, vol. 35, Apr. 1997, pp. 30-33. |
Robertson, K., Miller, K., White, M., and Tweedly, A., "StarBurst Multicast File Transfer Protocol (MFTP) Specification," Internet Draft, Internet Engineering Task Force, Apr. 1998. |
Schulzrinne, H., Casner, S., Frederick, R., Jacobson, V., "RTP" A Transport Protocol for Real-Time Applications, Request for Comments RFC 1889, Jan. 1996. |
Sharma, P, Estrin, D., Floyd, S. and Jacobson, V., "Scalable Timers for Soft State Protocols," Proceedings IEEE Infocom '97 (Kobe, Japan) Apr. 1997. |
Thaler, D., Estrin, D., Meyer, D., "Border Gateway Multicast Protocol (BGMP): Protocol Specification," Internet Draft, Nov. 1998. |
Thyagarajan, A.S., and Deering, S.E., Hierarchical Distance-Vector Multicast Routing for the MBone, Sigcomm '95, (Cambridge, Massachusetts), Aug. 1995. |
Waitzman, D., Partridge, C., Deering, S. E., "Distance Vector Multicast Routing Protocol," Request for Comments RFC 1075, Nov. 1988. |
Yavatkar, R., Griffioen, J. and Sudan, M., "A Reliable Dissemination Protocol for Interactive Collaborative Applications," ACM Multimedia 1995 (San Francisco, California) Nov. 1995. |
Also Published As
Publication number | Publication date |
---|---|
DE60008102D1 (en) | 2004-03-11 |
US6850987B1 (en) | 2005-02-01 |
US20040139150A1 (en) | 2004-07-15 |
KR100699018B1 (en) | 2007-03-23 |
EP1183820B1 (en) | 2004-02-04 |
KR20020013567A (en) | 2002-02-20 |
AU5321300A (en) | 2000-12-18 |
JP4685299B2 (en) | 2011-05-18 |
EP1183820A1 (en) | 2002-03-06 |
WO2000074312A1 (en) | 2000-12-07 |
DE60008102T2 (en) | 2004-12-23 |
JP2003501881A (en) | 2003-01-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8886826B2 (en) | System for multipoint infrastructure transport in a computer network | |
Obraczka | Multicast transport protocols: a survey and taxonomy | |
Hall et al. | Corona: a communication service for scalable, reliable group collaboration systems | |
US6557111B1 (en) | Multicast-enhanced update propagation in a weakly-consistant, replicated data storage system | |
US6178453B1 (en) | Virtual circuit switching architecture | |
US8667173B2 (en) | Performing multicast communication in computer networks by using overlay routing | |
US9614750B2 (en) | System of distributing content data over a computer network and method of arranging nodes for distribution of data over a computer network | |
US7283463B2 (en) | Non-disruptive reconfiguration of a publish/subscribe system | |
Rodriguez et al. | Improving the WWW: caching or multicast? | |
US20020165981A1 (en) | Method and apparatus for establishing and sharing a virtual change notification list among a plurality of peer nodes | |
Jia | A total ordering multicast protocol using propagation trees | |
JP4463999B2 (en) | Method and apparatus in a communication network | |
EP1894381A1 (en) | Multicast downloading using path information | |
KR20010020190A (en) | System, device, and method for managing multicast group memberships in a multicast network | |
Liao | Global information broadcast: an architecture for internet push channels | |
Maihöfer et al. | Constructing height-balanced multicast acknowledgment trees with the Token Repository Service | |
Green et al. | Collaborative applications at the tactical edge through resilient group dissemination in dtn | |
Park | Architecture and Implementation of a Content-based Data Dissemination System | |
CN118018341A (en) | Method for realizing inter-cloud multicast function in public cloud by virtual instance | |
Bhavanasi et al. | M2MC: Middleware for many to many communication over broadcast networks | |
Dommel et al. | Efficient group coordination in multicast trees | |
Hanna | Towards a better architecture for reliable multicast protocols | |
Garcia-Luna-Aceves | Dean of Graduate Studies and Research | |
BROADCASTING | David Reeves Boggs | |
Mühl et al. | Distributed Notification Routing |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: YAHOO! INC.,CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:FASTFORWARD NETWORKS, INC.;REEL/FRAME:018015/0494Effective date: 20060502Owner name: YAHOO! INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:FASTFORWARD NETWORKS, INC.;REEL/FRAME:018015/0494Effective date: 20060502 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:YAHOO! INC.;REEL/FRAME:033868/0257Effective date: 20140630 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: FASTFORWARD NETWORKS, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MCCANNE, STEVEN;SWAN, ANDREW;REEL/FRAME:034222/0199Effective date: 19991208 |
|
CC | Certificate of correction | ||
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044127/0735Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |