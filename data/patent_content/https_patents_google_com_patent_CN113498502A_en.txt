CN113498502A - Gesture detection using external sensors - Google Patents
Gesture detection using external sensors Download PDFInfo
- Publication number
- CN113498502A CN113498502A CN201980093483.5A CN201980093483A CN113498502A CN 113498502 A CN113498502 A CN 113498502A CN 201980093483 A CN201980093483 A CN 201980093483A CN 113498502 A CN113498502 A CN 113498502A
- Authority
- CN
- China
- Prior art keywords
- processors
- data
- image data
- user
- wearable computing
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000001514 detection method Methods 0.000 title claims description 17
- 230000033001 locomotion Effects 0.000 claims abstract description 150
- 230000000007 visual effect Effects 0.000 claims abstract description 18
- 238000005259 measurement Methods 0.000 claims description 158
- 238000000034 method Methods 0.000 claims description 44
- 238000004891 communication Methods 0.000 claims description 36
- 230000001133 acceleration Effects 0.000 claims description 22
- 238000005516 engineering process Methods 0.000 abstract description 2
- 230000015654 memory Effects 0.000 description 25
- 230000009466 transformation Effects 0.000 description 19
- 230000006870 function Effects 0.000 description 10
- 210000003811 finger Anatomy 0.000 description 7
- 230000001360 synchronised effect Effects 0.000 description 6
- 230000008859 change Effects 0.000 description 4
- 238000012545 processing Methods 0.000 description 4
- 101100478969 Oryza sativa subsp. japonica SUS2 gene Proteins 0.000 description 3
- 101100004663 Saccharomyces cerevisiae (strain ATCC 204508 / S288c) BRR2 gene Proteins 0.000 description 3
- 101100504519 Saccharomyces cerevisiae (strain ATCC 204508 / S288c) GLE1 gene Proteins 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 238000003672 processing method Methods 0.000 description 3
- 238000000844 transformation Methods 0.000 description 3
- 241000699666 Mus <mouse, genus> Species 0.000 description 2
- 241000699670 Mus sp. Species 0.000 description 2
- 230000009286 beneficial effect Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000001131 transforming effect Effects 0.000 description 2
- 238000013475 authorization Methods 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000007423 decrease Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 210000004247 hand Anatomy 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000010079 rubber tapping Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 210000003813 thumb Anatomy 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 239000013598 vector Substances 0.000 description 1
- 210000000707 wrist Anatomy 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/017—Gesture based interaction, e.g. based on a set of recognized hand gestures
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S13/00—Systems using the reflection or reradiation of radio waves, e.g. radar systems; Analogous systems using reflection or reradiation of waves whose nature or wavelength is irrelevant or unspecified
- G01S13/86—Combinations of radar systems with non-radar systems, e.g. sonar, direction finder
- G01S13/867—Combination of radar systems with cameras
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S7/00—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00
- G01S7/02—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00 of systems according to group G01S13/00
- G01S7/41—Details of systems according to groups G01S13/00, G01S15/00, G01S17/00 of systems according to group G01S13/00 using analysis of echo signal for target characterisation; Target signature; Target cross-section
- G01S7/415—Identification of targets based on measurements of movement associated with the target
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0346—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of the device orientation or free movement in a 3D space, e.g. 3D mice, 6-DOF [six degrees of freedom] pointers using gyroscopes, accelerometers or tilt-sensors
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/20—Movements or behaviour, e.g. gesture recognition
- G06V40/28—Recognition of hand or arm movements, e.g. recognition of deaf sign language
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/30—Subject of image; Context of image processing
- G06T2207/30196—Human being; Person
Abstract
The present technology provides a system (110) for determining a gesture (fig. 3-8) provided by a user (210). In this regard, one or more processors (112) of the system may receive image data from one or more visual sensors (115) of the system that capture the user's motion, and may receive motion data from one or more wearable computing devices (120) worn by the user. The one or more processors may identify, based on the image data, a portion of the user's body corresponding to a gesture for performing a command (930). The one or more processors may also determine one or more associations between the image data and the received motion data (940). Based on the identified portion of the user's body and the one or more associations between the image data and the received motion data, the one or more processors may detect the gesture (950).
Description
Cross Reference to Related Applications
The present application claims the benefit of application serial No.16/373,901 filed on 03/04/2019, the disclosure of which is incorporated herein by reference.
Background
Computing devices, such as desktop and laptop computers, have various user interfaces that allow a user to interact with the computing device. For example, such interfaces may include keyboards, mice, touch pads, buttons, and the like. Through which a user may control various functions of the computing device and user applications installed on the computing device. However, interaction with these interfaces can be inconvenient or unnatural, such as manipulating three-dimensional objects on the screen by tapping on a keyboard or clicking on a mouse.
For wearable devices such as smart watches and head mounted devices (head mounts), interfaces such as keyboards and mice may be impractical or impossible due to the form factor of the wearable device. For example, a virtual keyboard on a smart watch may be too small for some users to operate reliably. As such, wearable devices may be designed to enable more convenient and natural user interaction when using such devices, such as through voice, touch, or gestures. To this end, wearable devices are equipped with various sensors, such as microphones and Inertial Measurement Units (IMUs), and users may use those sensors for the purpose of interacting with the device. Examples of IMUs may generally include accelerometers and gyroscopes.
Disclosure of Invention
The present disclosure provides for receiving, by one or more processors, image data from one or more visual sensors that capture motion of a user; receiving, by the one or more processors, motion data from one or more wearable computing devices worn by the user; recognizing, by the one or more processors, based on the image data, a portion of the user's body corresponding to a gesture for performing a command; determining, by the one or more processors, one or more associations between the image data and the received motion data; and detecting, by the one or more processors, the gesture based on the recognized portion of the user's body and one or more associations between the image data and the received motion data.
Determining the one or more associations may further include synchronizing a time stamp associated with the image data with a time stamp associated with the received motion data.
The method may further comprise: determining, by the one or more processors, a first coordinate system of a viewing angle from the one or more vision sensors; determining, by the one or more processors, a second coordinate system of a perspective from the one or more wearable computing devices; determining, by the one or more processors, one or more transforms between the first coordinate system and the second coordinate system, wherein determining the one or more associations further comprises determining the one or more transforms.
The method may further include determining, by the one or more processors, a location of one or more fingers of the user's hand where the identified portion of the user's body comprises the user's hand, wherein detecting the gesture is further based on the location of the one or more fingers.
The method may further include generating, by the one or more processors, a time-based motion data sequence for the identified portion of the user's body based on the image data, the generated time-based motion data sequence including at least one of a time-based position sequence, a time-based velocity sequence, and a time-based acceleration sequence. The received motion data may include a sequence of time-based inertial measurements, and wherein determining the one or more associations may include matching the sequence of time-based motion data generated based on the image data with the sequence of time-based inertial measurements.
The method may further include determining, by the one or more processors, depth information for the motion of the user based on the received motion data, wherein detecting the gesture is further based on the depth information.
The method may further include determining, by the one or more processors, an orientation of the one or more wearable computing devices based on the received motion data, wherein detecting the gesture is further based on the orientation of the one or more wearable computing devices.
The method may further include interpolating, by the one or more processors, an intermediate movement of the user between two consecutive frames of the image data based on the received motion data, wherein detecting the gesture is further based on the intermediate movement.
The method may further include receiving, by the one or more processors, a pairing request from the one or more wearable computing devices; requesting, by the one or more processors, authentication to pair with the one or more wearable computing devices to receive data over the communication link; receiving, by the one or more processors, an authentication to pair with the one or more wearable computing devices to receive data over the communication link.
The method may further include requesting, by the one or more processors, permission to use data from the one or more wearable computing devices for gesture detection; receiving, by the one or more processors, permission to use data from the one or more wearable computing devices for gesture detection.
The method may further include receiving, by the one or more processors, signal strength measurements of a connection with the one or more wearable computing devices; determining, by the one or more processors, one or more associations between the image data and the signal strength measurement, wherein detecting the gesture is further based on the one or more associations between the image data and the signal strength measurement. The method may further include determining, by the one or more processors, a distance between the one or more wearable computing devices and the one or more visual sensors based on the signal strength measurement, wherein detecting the gesture is further based on the distance between the one or more wearable computing devices and the one or more visual sensors.
The method may further include receiving, by the one or more processors, audio data from one or more audio sensors; receiving, by the one or more processors, audio data from the one or more wearable computing devices; determining, by the one or more processors, one or more associations between the image data and audio data from the one or more wearable computing devices; comparing, by the one or more processors, audio data received from the one or more wearable computing devices with audio data received from the one or more audio sensors, wherein detecting the gesture is further based on the comparison.
The method may further include receiving, by the one or more processors, radar measurements from a radar sensor; determining, by the one or more processors, one or more associations between the image data and the radar measurements, wherein detecting the gesture is further based on the one or more associations between the image data and the radar measurements.
The method may further include determining, by the one or more processors, a relative location of the one or more wearable computing devices, wherein the one or more wearable computing devices include a plurality of wearable computing devices, and wherein detecting the gesture is further based on the relative location of the one or more wearable computing devices.
The present disclosure further provides a system comprising one or more vision sensors configured to collect image data, and one or more processors configured to: receiving image data from one or more visual sensors that capture motion of a user; receiving motion data from one or more wearable computing devices worn by the user; identifying, based on the image data, a portion of the user's body corresponding to a gesture for performing a command; determining one or more associations between the image data and the received motion data; and detecting the gesture based on the recognized portion of the user's body and the one or more associations between the image data and the received motion data.
The one or more vision sensors may be a front-facing camera.
The motion data may include inertial measurements from at least one of an accelerometer and a gyroscope.
The system may further include a communication module configured to measure a signal strength of a connection with the one or more wearable computing devices, wherein the one or more processors are further configured to: receiving signal strength measurements of a connection with the one or more wearable computing devices; determining one or more associations between the image data and the signal strength measurements, wherein detecting the gesture is further based on the one or more associations between the image data and the signal strength measurements.
Drawings
Fig. 1 is a block diagram of an example system in accordance with aspects of the present disclosure.
Fig. 2 is a pictorial diagram illustrating an example system in accordance with aspects of the present disclosure.
FIG. 3 illustrates an example of detecting gestures using inertial measurements in accordance with aspects of the present disclosure.
Fig. 4 illustrates another example of detecting gestures using inertial measurements in accordance with aspects of the present disclosure.
Fig. 5 illustrates an example of detecting gestures using signal strength measurements in accordance with aspects of the present disclosure.
Fig. 6 illustrates an example of detecting gestures using audio data in accordance with aspects of the present disclosure.
Fig. 7 illustrates an example of detecting gestures using radar measurements in accordance with aspects of the present disclosure.
Fig. 8 illustrates an example of detecting gestures using sensor data from multiple wearable devices in accordance with aspects of the present disclosure.
Fig. 9 is a flow chart in accordance with aspects of the present disclosure.
Detailed Description
SUMMARY
The technology generally relates to detecting user gestures, that is, gestures provided by a user for the purpose of interacting with a computing device. Computing devices with limited sensors, such as laptops with a single front-facing camera, may collect and analyze image data in order to detect gestures provided by a user. For example, the gesture may be a hand sweep or rotation corresponding to a user command, such as scrolling or rotating the display downward. However, such cameras may not capture sufficient image data to accurately detect gestures. For example, all or part of the gesture may occur too quickly for a camera with a relatively slow frame rate to catch up. Further, the cameras of typical laptops have difficulty detecting complex gestures via the camera, as some cameras provide little, if any, depth information. To address these issues, a system may be configured to use data from sensors external to the system for gesture detection.
In this regard, the system may include one or more vision sensors configured to collect image data, and one or more processors configured to analyze the image data in conjunction with data from the external sensors. As a specific example, the system may be a laptop computer, wherein the one or more visual sensors may be a single front-facing camera provided on the laptop computer. Examples of external sensors may include various sensors provided in one or more wearable devices worn by the user, such as a smart watch or a head-mounted device.
The processor may receive image data from one or more visual sensors that captures user motion provided as gestures. For example, the image data may include a sequence of frames taken by a front-facing camera of a laptop computer that captures the motion of the user's hand. For example, from the perspective of the front camera, the sequence of frames may be captured at 30 frames/s, or 5 frames/s in a low battery state, where each frame is associated with a timestamp provided by the laptop's clock. The processor may generate motion data, such as a time-based sequence of hand positions, based on the image data. However, and as noted above, due to slow camera frame rates or the lack of depth information, the motion data may lack sufficient accuracy to fully capture all relevant information embodied in the motion.
As such, the processor may also receive motion data from one or more wearable devices worn by the user. For example, the motion data may include inertial measurements from a perspective of the smart watch measured by an IMU of the smart watch, and wherein each measurement may be associated with a timestamp provided by a clock of the smart watch. For example, the inertial measurements may include acceleration measurements from an accelerometer in the smart watch. As another example, the inertial measurements may include rotation or orientation measurements from a gyroscope of the smart watch.
The processor may determine one or more associations between the image data and motion data received from the one or more wearable devices. For example, determining an association may include synchronizing a time stamp of the image data with a time stamp of the inertial measurement. As another example, determining the association may include transforming inertial measurements from a coordinate system of data provided by the IMU to a coordinate system corresponding to the image data.
Based on the association between the image data and the motion data received from the one or more wearable devices, the processor may detect a gesture provided by the user. For example, since the acceleration measurements from the accelerometer may include values in three-dimensional space, the processor may determine depth information for the user's motion. As another example, the processor may use the rotation measurements from the gyroscope to determine whether the user's motion includes rotation. In yet another example, the processor may interpolate information about the user's medial movement between two frames of image data since inertial measurements may be taken at a higher frequency than the frame rate of the camera.
Additionally or alternatively, the system may be configured to use other types of data to detect gestures provided by a user. For example, the processor may receive signal strength measurements of connections with one or more wearable devices. For example, the connection may be a bluetooth connection, a WiFi connection, a radio frequency connection, and the like. Using the signal strength measurements, the processor may determine depth information of the user's motion.
In instances where the user agrees to use such data, the processor may receive audio data from one or more wearable devices, and may also receive audio data from one or more audio sensors in the system. For example, a microphone on a smart watch worn by the user may detect a voice command from the user as audio data, and the same voice command may also be detected by a microphone on a laptop as audio data. As such, the processor may compare audio data detected by the wearable device with audio data detected by the system to determine the relative position of the user's hand and the user's face.
The processor may also receive radar measurements from one or more radar sensors. For example, the system may include a radar sensor configured to measure the position and/or velocity of objects surrounding the system. As such, the processor may use the position and/or velocity measurements to determine depth information for the user's motion.
The processor may be further configured to receive sensor data from a plurality of wearable devices and determine an association therebetween, and use the associated sensor data for gesture detection. For example, determining the association may include synchronizing timestamps of sensor data from multiple wearable devices with the image data, individually or collectively. For another example, determining the association may include transforming information provided in a coordinate system of each wearable device to a coordinate system of the image data. As another example, determining the association may include determining a relative location of each wearable device.
This technique is beneficial because it allows a system with limited sensors to more accurately determine complex and fast gestures provided by a user. By correlating inertial measurements from the wearable device with image data captured by the system, the image data can be supplemented with depth and rotation information. When image data is captured at a lower frequency than inertial measurements, information about the user's movement in the middle between successive frames of image data may be more accurately interpolated, thus improving the accuracy of the system interpretation of the user input. Features of the technique further provide for using other types of data for detecting gestures, such as signal strength measurements, audio data, and radar measurements. Furthermore, many users may find this technique relatively easy to use, as the wearable device may have been paired with the system using second factor authentication. In addition, the techniques can allow the visual sensor to capture image data at a reduced frame rate or low resolution while maintaining gesture detection accuracy, thereby reducing power usage of the visual sensor.
Example System
Fig. 1 and 2 illustrate an example system 100 in which the described features may be implemented. It should not be taken as limiting the scope of the disclosure or usefulness of the features described herein. In this example, system 100 may include computing devices 110, 120, 130, and 140 and storage system 150. As shown for example, computing device 110 contains one or more processors 112, memory 114, and other components typically found in a general purpose computing device.
The memory 114 can store information accessible by the one or more processors 112, including instructions 116 executable by the one or more processors 112. The memory can also include data 118 that can be acquired, manipulated, or stored by the processor 112. The memory 114 can be any non-transitory type capable of storing information accessible by the processor, such as a hard disk, memory card, ROM, RAM, DVD, CD-ROM, writable memory, and read-only memory.
The instructions 116 can be any set of instructions to be executed directly by one or more processors, such as machine code, or indirectly by the one or more processors, such as scripts. In this regard, the terms "instructions," "applications," "steps," and "programs" may be used interchangeably herein. The instructions can be stored in an object code format for direct processing by a processor, or in any other computing device language, including scripts or collections of independent source code that have been previously interpreted or compiled as needed. The function, method, and routine of the instructions are explained in more detail below.
The data 118 can be retrieved, stored, or modified by the one or more processors 112 in accordance with the instructions 116. For example, although the subject matter described herein is not limited by any particular data structure, the data can be stored in computer registers, as a table with many different fields or records, in an associated database, or in an XML document. The data can also be formatted in any computer device readable format, such as but not limited to binary values, ASCII, or Unicode. Further, the data can include any information sufficient to identify the relevant information, such as a number, descriptive text, attribute code, pointer, reference to data stored in other memory, such as at other network locations, or information used by a function to compute the relevant data.
The one or more processors 112 can be any conventional processor, such as a commercially available CPU. Alternatively, the processor can be a dedicated component such as an Application Specific Integrated Circuit (ASIC) or other hardware based processor. Although not required, the computing device 110 may include specialized hardware components to perform specific computing processes faster or more efficiently, such as decoding video, matching video frames to images, distorting video, encoding distorted video, and so forth.
Although fig. 1 functionally illustrates the processors, memory, and other elements of the computing device 110 as being within the same block, the processors, computers, computing devices, or memory can actually comprise multiple processors, computers, computing devices, or memory, which may or may not be stored within the same physical housing. For example, the memory can be a hard disk or other storage medium located in a different housing than the housing of the computing device 110. Thus, references to a processor, computer, computing device, or memory are to be understood as including references to a collection of processors, computers, computing devices, or memories that may or may not operate in parallel. For example, computing device 110 may comprise a computing device operating in a distributed system, and so on. Still further, while some functions described below are indicated as occurring on a single computer with a single processor, various aspects of the subject matter described herein may be implemented by multiple computing devices, e.g., communicating information over network 160.
Each of the computing devices 110, 120, 130, 140 can be at a different node of the network 160 and can communicate, directly or indirectly, with other nodes of the network 160. Although only a few computing devices are depicted in fig. 1 and 2, it should be understood that a typical system may include a large number of connected computing devices, with each different computing device being at a different node of the network 160. The network 160 and the intermediate nodes described herein can be interconnected using various protocols and systems such that the network can be part of the internet, world wide web, ad hoc intranet, wide area network, or local area network. The network can utilize standard communication protocols such as ethernet, WiFi, and HTTP, protocols specific to one or more companies, and various combinations of the foregoing. While certain advantages are obtained in sending and receiving information as described above, other aspects of the subject matter described herein are not limited to any particular manner of information transfer.
Each of computing devices 120, 130, and 140 may be configured similarly to computing device 110, with one or more processors, memories, and instructions as described above. For example, as shown in fig. 1 and 2, computing devices 110, 120, and 130 may each be client computing devices intended for use by user 210, and have all of the components normally used in connection with a personal computing device, such as a Central Processing Unit (CPU), memory (e.g., RAM and internal hard drives) that stores data and instructions, input and/or output devices, sensors, communication modules, clocks, and so forth. As another example, as shown in fig. 1 and 2, the computing device 140 may be a server computer and may have all of the components normally used in connection with a server computer, such as a processor and memory storing data and instructions.
Although computing devices 110, 120, and 130 may each comprise a full-size personal computing device, they may alternatively comprise mobile computing devices capable of wirelessly exchanging data with a server over a network such as the internet. For example, computing device 110 may be a desktop or laptop computer as shown in FIG. 2, or a mobile phone or device capable of obtaining information via the Internet, such as a wireless-enabled PDA, tablet PC, or netbook. As another example, computing devices 120 and 130 may each be a wearable computing device, e.g., as shown in fig. 2, wearable computing device 120 may be a smart watch, and wearable computing device 130 may be a head-mounted device. The wearable computing device may include one or more mobile computing devices configured to be worn by and/or attached to a human body. Such wearable computing devices may form part of an article of clothing and/or be worn over/under clothing. Further examples of wearable computing devices include a glove and/or one or more finger rings.
For computing devices that are wearable computing devices, such as wearable computing device 120 shown as a smart watch in fig. 2 or wearable computing device 130 shown as a head-mounted device, sensors 125 and/or 135 may similarly include visual and audio sensors, but may also include additional sensors for measuring gestures provided by the user. For example, the sensors 125 and/or 135 may additionally include IMUs, radar sensors, and the like. According to some examples, the IMU may include an accelerometer (such as a 3-axis accelerometer) and a gyroscope (such as a 3-axis gyroscope). The sensors 125 and/or 135 for the wearable computing device may further include a barometer, a vibration sensor, a thermal sensor, a Radio Frequency (RF) sensor, a magnetometer, and an atmospheric pressure sensor. Additional or different sensors may also be employed.
To obtain and send information to and from remote devices, including to each other, computing devices 110, 120, 130 may each include a communication module, such as communication modules 117, 127, 137, respectively. The communication module may be capable of wireless network connections, wired proprietary connections, and/or wired connections. Via the communication module, the communication device may establish a communication link, such as a wireless link. For example, the communication modules 117, 127, and/or 137 may include one or more antennas, transceivers, and other components for operating at radio frequencies. The communication modules 117, 127, and/or 137 may be configured to support communication via cellular, LTE, 4G, WiFi, GPS, and other networking architectures. CommunicationModules 117, 127, and/or 137 may be configured to support communication modules 117, 127, and/or 137 may support wired connections such as USB, micro-USB, USB Type C, or other connectors, for example, to receive data and/or power from a laptop, tablet, smartphone, or other device.
Using their respective communication modules, one or more of the computing devices 110, 120, 130 may be paired with each other to send and/or receive data from each other. For example, wearable computing devices 120 and/or 130 may be within a predetermined distance of computing device 110, and may become accessible to computing device 110 viacomputing device 110 or the wearable computing devices 120 and/or 130 may initiate pairing. User authentication may be requested by computing device 110 or wearable computing devices 120 and/or 130 prior to pairing. In some instances, pairing may require two-way authentication, where a user must authenticate the pairing on both devices to be paired, such as on both computing devices 110 and 120, or on both computing devices 110 and 130, and so on.
The communication modules 117, 127, 137 may be configured to measure signal strength of a wireless connection. For example, the communication modules 117, 127, 137 may be configured to measure communication modules 117, 127, 137 may be configured to report the measured RSS to each other.
Like the memory 114, the storage system 150 can be any type of computerized storage capable of storing information accessible by one or more of the computing devices 110, 120, 130, 140, such as a hard disk, a memory card, ROM, RAM, DVD, CD-ROM, writeable memory, and read-only memory. Additionally, storage system 150 may comprise a distributed storage system in which data is stored on a plurality of different storage devices that may be physically located in the same or different physical locations. Storage system 150 may be connected to computing devices via network 160 as shown in fig. 1, and/or may be directly connected to any of computing devices 110, 120, 130, and 140 (not shown).
Example method
Further to the example systems described above, example methods are now described. Such a method may be performed using any of the systems described above, variations thereof, or systems having different configurations. It should be understood that the operations involved in the following methods need not be performed in the exact order described. Conversely, various operations may be processed in a different order or concurrently, and operations may be added or omitted.
For example, the processor 112 of the computing device 110 may receive input from the user 210 requesting interaction with the computing device 110 using gestures. As such, the processor 112 may control the sensors 115 of the computing device 110 to collect sensor data regarding the motion provided by the user as a gesture, and may also control the communication module 117 to collect additional sensor data from external sensors, such as from one or more wearable computing devices worn by the user 210. Once the sensor data from the sensors 115 and the sensor data from the external sensors are received by the processor 112, the processor 112 may analyze the sensor data to detect a gesture provided by the user.
Fig. 3 illustrates an example scenario for detecting gestures using motion data received from external sensors in accordance with aspects of the present disclosure. The motion data includes information associated with the motion of the user's body (including portions thereof, such as hands) through space. For example, motion data for a motion may include one or more vectors associated with angles and velocities of the motion, which may include a sequence of 3D coordinates associated with positions of the user's body or a portion of its body at different times. For example, the system may detect and analyze the speed and angle of movement of a person's thumb separately from the speed and angle of the index finger on the same hand of the person. Referring to fig. 3, an example gesture provided by the user 210 is shown in which the hand 220 of the user 210 is moving upward and toward the computing device 110 (indicated by the arrow). This motion may be captured by image data collected by one or more vision sensors of computing device 110, such as camera 115A, which is shown in fig. 3 as a single front-facing camera. For ease of reference, the aperture of camera 115A may be considered to be in a plane defined by the x-axis and the y-axis, with the z-axis perpendicular to the plane of the aperture. As such, the motion of the hand 220 may be captured by a sequence of images or frames taken by the camera 115A. For example, the camera 115A may be configured to capture images at a frame rate of 30 frames/s. As another example, the camera 115A may be configured to capture images at a lower frame rate, such as 5 frames/second, in a low power state. The series of frames may each be associated with a timestamp, which may be provided by a clock 119 of the computing device 110, for example.
The processor 112 may receive the image data and analyze the image data to detect a gesture provided by the user. For example, the processor 112 may arrange the frames of the image data in a chronological order according to their respective timestamps. The processor 112 may use image or video segmentation or other image processing techniques to separate the portion of each image corresponding to the hand 220 of the user 210 from the portions corresponding to other objects or backgrounds. In this regard, the processor 112 may use a pattern or object recognition model, such as a machine learning model, to recognize one or more portions of the user's body that correspond to gestures for executing commands. For example, the processor 112 may identify one or more portions of the hand 220 corresponding to the user 210 in each frame of image data. As another example, the processor 112 may identify one or more portions of the image data that appear to move between frames.
The processor 112 may generate a time-based motion data sequence for the identified portion of the user's body corresponding to the gesture. For example, the time-based motion data sequence may include a time-based sequence of positions of hand 220, shown as [ t 1; x1, y1, [ t 2; x2, y2, [ tn; xn, yn ]. This location may correspond to a single point of hand 220 as shown, or may correspond to multiple points of hand 220, such as the outline of hand 220. Other examples of motion data may include velocity and acceleration of hand 220. For example, processor 112 may generate a time-based sequence of velocities of hand 220 based on a time-based sequence of positions of hand 220. For another example, processor 112 may generate a time-based sequence of accelerations for hand 220 based on a time-based sequence of positions for hand 220. In other examples, processor 112 may further process the image data to determine additional details, such as the position of fingers of hand 220, rotation of hand 220, and so forth. Based on the time-based motion data sequence of hand 220, processor 112 may detect a gesture provided by the user, such as the upward motion shown in fig. 3.
As mentioned above, the image data may comprise a two-dimensional image without depth information, in particular if the camera is a single front-facing camera. In the example shown in fig. 3, the position of the hand 220 includes only numerical values with respect to two coordinates (shown as x and y) in the plane of the aperture of the camera 115A, and does not include numerical values with respect to a third coordinate corresponding to a direction perpendicular to the aperture of the camera 115A. As such, the processor 112 may not be able to determine whether the hand 220 is moving toward or away from the camera 115A, which may result in an incorrect gesture being detected and thus incorrectly determine the user's command. For example, in a case where a gesture in which the hand moves directly upward corresponds to a command to scroll down and a gesture in which the hand moves upward and forward corresponds to a command to move the window backward (behind other windows), if the command is determined based on only the two-dimensional image data, the user command to move the window backward may be incorrectly interpreted as a command for scrolling down. Although the processor 112 may attempt to determine a change in distance between the camera 115A and the hand 220 based on a change in size of the hand 220 captured in each frame, such a determination may be inaccurate, for example, due to rotation of the hand 220 and/or changes in the background.
Further as described above, the camera 115A may have a low frame rate, which may not adequately capture the fast motion of the user. For example, in each frame captured by the camera 115A, the hand 220 may appear to move upward in the y-direction, whereas there may be downward movement between two frames, which may not be captured by the camera 115A. Further, there may also be movement in the x-direction between two frames that may not have been captured by the camera 115A.
As such, the processor 112 may use sensor data from one or more external sensors in addition to the image data collected by the camera 115A to detect gestures provided by the user to the computing device 110. For example, as shown in fig. 3, the one or more external sensors may be one or more sensors in a wearable computing device worn by user 210. For example, the one or more sensors may be an accelerometer 125A in the IMU of the smart watch, such as a three-axis accelerometer capable of measuring acceleration in three-dimensional space.
In this regard, the computing device 110 may establish a communication link with the wearable computing device 120 to receive sensor data from the wearable computing device 120. For example, the communication link may be a wireless network communication link, such as a WiFi or radio frequency link, or a non-network wireless communication link, such aswearable computing device 120 may initiate pairing, in other instances, the computing device 110 may initiate pairing or pairing may be initiated by user input. For example, computing device 110 may receive a pairing request from wearable computing device 120.
As mentioned above, authentication may be required for pairing of the computing device 110 with the wearable device. For example,when the wearable computing device 120 comes within a predetermined distance of the computing device 110, the wearable computing device 120 may become accessible viacomputing device 110. As such, the computing device 110 or the wearable computing device 120 may request user authentication for pairing, which may include entering a username and password, a passcode, and so forth. In some instances, two-way authentication may be required for pairing, where the user must provide authentication on computing device 110 to pair with wearable computing device 120, and also provide authentication on wearable computing device 120 to pair with computing device 110.
In instances where the user agrees to use such data, the sensor data provided by the wearable computing device may be used to interpret the user's gestures to the paired computing device. For example, computing device 110 may display a prompt asking the user whether sensor data from wearable computing device 120 can be used for gesture detection. In some instances, the computing device 110 may allow the user to select the type of data that the user approves for permission to use in gesture detection by the computing device 110. Alternatively or additionally, the wearable computing device 120 may display a prompt asking the user whether the wearable computing device 120 can share one or more types of sensor data thereof with the computing device 110. Still further, the user may have previously configured authorization settings in wearable computing device 120 to permit detection by and/or sharing of data with computing device 110.
Thereafter, processor 112 may receive sensor data from wearable computing device 120 via the communication link. The received sensor data may include motion data, such as inertial measurements, detected by one or more sensors of the wearable computing device 120. In the example shown, the sensor data includes inertial measurements from the accelerometer 125A. For example, accelerometer 125A may measure acceleration of wearable computing device 120 relative to three axes in three-dimensional space. For example and as shown in fig. 3, two axes x ' and y ' may correspond to two directions in a plane of a surface of the wearable computing device 120 (e.g., a dial of a smart watch), and one axis z ' may correspond to a direction perpendicular to the surface of the wearable computing device 120. In other examples, the axes x ', y ', and z ' may be some other axes sufficient to define a three-dimensional space. Each acceleration measurement may be associated with a timestamp, such as a timestamp provided by the clock 129 of the wearable computing device 120.
As such, the processor 112 may receive a time-based sequence of acceleration measurements from the wearable computing device, shown as [ t 1'; a _ x1 ', a _ y 1', a _ z1 '], … …, [ tn'; a _ xn ', a _ yn ', a _ zn ' ]. For example, t1 'may be a timestamp at or near the start of a motion and tn' may be a timestamp at or near the end of a motion. As shown example, a _ x1 'may be a numerical value of acceleration along an x' axis in a dial plane of the smart watch, a _ y1 'may be a numerical value of acceleration along a y' axis also in the dial plane of the smart watch, and a _ z1 'may be a numerical value of acceleration along a z' axis perpendicular to the dial of the smart watch. In some instances, the processor 112 may generate additional motion data based on acceleration measurements received from the accelerometer 125A. As an example, a time-based velocity sequence may be generated based on time-based acceleration measurements, a time-based position sequence may be generated based on time-based acceleration measurements, and so on. In other instances, such additional motion data may be generated based on acceleration measurements made by processor 122 of wearable computing device 120 and received by processor 112 of computing device 110.
To detect gestures using both image data and motion data received from wearable computing device 120, processor 112 may determine one or more associations between the image data and the received motion data. For example, the processor 112 may determine one or more associations between image data from the camera 115A and inertial measurements from the wearable computing device 120. For example, a timestamp of the image data may be provided by the clock 119 of the computing device 110, and a timestamp of the received inertial measurements may be provided by the clock 129 of the wearable computing device 120. In this regard, determining the one or more associations may include matching each received inertial measurement to a frame in the image data having a temporally closest timestamp.
In such an example: where the duration of motion captured by the image data is different from the duration of motion captured by the received inertial measurements, and/or where the image data is taken at a different rate than the inertial measurements, inaccuracies may result from matching by the closest in time timestamps. In this regard, determining the one or more associations may include determining a delay between a timestamp of the image data and a timestamp of the received inertial measurements. For example, when a connection is made between computing device 110 and wearable computing device 120, processor 112 may be provided with a timestamp for the connection from its clock 119. Processor 112 may also receive a timestamp of the connection (via computing device 120) from clock 129. Also while connected, a server computer, such as computing device 140 shown in fig. 1 and 2, may be configured to send the first timestamp to computing device 110 and the second timestamp to computing device 120. Based on a comparison of the timestamp from clock 119, the timestamp from clock 129, and the first timestamp from the server, the delay between clocks 119 and 129 may be determined. Thereafter, resynchronization may be performed at a later time to prevent inaccuracies due to drift. For example, the resynchronization may be performed periodically at predetermined intervals, or based on requirements such as accuracy of gesture detection.
Once the delay between clock 119 and clock 129 is determined, processor 112 may match the received inertial measurements to corresponding motion data generated based on the image data using the corresponding time stamps. For example, the processor 112 may compare [ t 1'; a _ x1 ', a _ y 1', a _ z1] and [ t 1; x1, y1] match, and [ tn'; a _ xn ', a _ yn ', a _ zn ' ] and [ tn; xn, yn ], etc. In some instances, the wearable computing device 120 may take inertial measurements at a higher frequency than the frame rate of the camera 115A of the computing device 110. In this case, there may be additional inertial measurements between two frames of image data.
As further shown in fig. 3, the coordinate system of computing device 110 may be based on camera 115A (x, y, z), whereas the coordinate system of accelerometer 125A may be based on wearable computing device (x ', y ', z '). As such, determining the one or more associations may include determining a transformation between a coordinate system of the image data and a coordinate system of the inertial measurements. As further shown, the coordinate system of camera 115A may be stationary, while the coordinate system of the wearable computing device (x ', y ', z ') may move with hand 220. As such, a transformation may be determined between each frame of image data and the corresponding received inertial measurement. For example, let [ t 1'; a _ x1 ', a _ y 1', a _ z1] and [ t 1; x1, y1] may be associated with a transformation other than the transformation of [ tn'; a _ xn ', a _ yn ', a _ zn ' ] and [ tn; xn, yn ] associated transforms.
In this regard, the processor 112 may compare, for each frame of the sequence of frames, motion data generated based on the image data to corresponding motion data received from the wearable computing device 120. For example, the processor 112 may generate acceleration data [ a _ x1, a _ y1] for t1 based on the image data, and may compare these values to the acceleration measurements [ a _ x1 ', a _ y 1', a _ z1 '] received at t 1'. By comparing these values, the processor 112 can determine a transformation between the two coordinate systems. For example, the processor 112 may determine that a _ x1 is a _ x1 ' and a _ y1 is a _ z1 ', and thus the x-axis is parallel to the x ' -axis, the y-axis is parallel to the z ' -axis, and the z-axis is parallel to the y ' -axis. Additionally or alternatively, the processor 112 may use an object recognition model to detect the wearable computing device 120 in each frame of image data and use image processing methods to determine the orientation of the wearable computing device 120 in each frame of image data. The processor 112 may then compare the position and orientation of the computing device 120 based on the image data with the acceleration measurements received from the computing device 120 to determine a relationship between the two coordinate systems. In yet other examples, the rotation measurements described below with respect to fig. 4 may be used to determine one or more transformations since the rotation measurements may provide an orientation of the wearable computing device 120.
Once the transformation is determined, the processor 112 may transform the motion data received from the wearable computing device 120 into a numerical value with respect to the coordinate system of the computing device 110. For example, the processor 112 may transform inertial measurements from the accelerometer 115A into numerical values with respect to a coordinate system of the computing device 110. Since there may be more inertial measurements than frames of image data in the same duration, additional inertial measurements between two frames may be transformed based on the transformation of one of the two frames, or based on some average of the transformations of the two frames.
Processor 112 may then combine the motion data received from wearable computing device 120 with the motion data generated based on the image data in any of a variety of ways. For example, the combined motion data for each frame may include a location with additional depth information. For example, frame 1 may have a field such as [ t 1; (x1, y1), a _ z1], wherein a _ z1 is determined based on acceleration measurements from computing device 120 and the transformation described above. As another example, the combined motion data for each frame may include velocity values in three-dimensional space. For example, frame 1 may have a field such as [ t 1; v _ x1, v _ y1, v _ z1], where v _ x1, v _ y2 are generated based on the image data, and v _ z1 is determined based on the acceleration measurements from the computing device 120 and the transformations described above. Further, where there may be more inertial measurements than frames of image data, additional inertial measurements may be used to determine motion between the two frames. For example, while there may be no frame between two consecutive frames of t1 and t2, information about the intermediate movement of hand 220 between t1 and t2 may be interpolated based on the additional inertial measurements and the transformation described above. As another example, the combined motion data for each frame may include some average or weighted average of the motion data generated based on the image data and the received inertial measurements. Also for example, where the motion data generated based on the image data includes additional details based on image analysis, such as finger position of the hand 220 or rotation of the hand 220, the combined motion data may include such additional details.
Based on the recognized portion of the user's body corresponding to the gesture for executing the command and the combined motion data, the processor 112 may detect a gesture provided by the user. For example, the portion of the user's body corresponding to the gesture has been identified as the user's hand 220, and using the combined motion data including the depth information, the processor 112 can distinguish gestures in which the hand 220 moves directly upward from gestures in which the hand 220 moves upward and toward the computing device 110 as shown in fig. 3, which may correspond to different user commands, as described above. For another example, where the combined motion data further includes additional details such as finger positions of hand 220, processor 112 may be capable of distinguishing the gesture shown in fig. 3 in which hand 220 is open while moving in the trajectory shown from the gesture in which hand 220 is closed while moving in the trajectory shown. For example, while the gesture shown in fig. 3 with an open hand 220 may correspond to a user command to move a displayed window backward, the gesture with a closed hand 220 moving in the same trajectory as shown in fig. 3 may correspond to a user command to increase speaker volume of the computing device 110.
Upon detecting a gesture provided by the user 210, the processor 112 may determine whether the gesture corresponds to a user command, such as a user command stored in the memory 114 of the computing device 110. If so, the processor 112 may control one or more functions of the computing device 110 based on the user command. For example, the gesture shown in fig. 3 may correspond to a user command to move a window currently displayed by computing device 110 backward. As such, the processor 112 may control the computing device 110 to change the display accordingly.
Fig. 4 illustrates another example of detecting gestures using motion data received from external sensors in accordance with aspects of the present disclosure. As shown, example motions of the hand 220 include rotations of the hand 220, which may be captured by the camera 115A of the computing device 110. However, in some instances, such as a slight tilt, the processor 112 may not be able to detect rotation of the hand 220 simply by processing the image data using image processing methods. Further as mentioned above, the rotation may be too fast to be captured by the camera 115A. As such, the processor 112 may additionally use rotation measurements from the gyroscope 125B of the wearable computing device 120 for gesture detection. The received rotation measurements may include rotation angle and/or angular velocity measurements about three rotational axes. As such, the received rotation measurements may include roll, pitch, and yaw measurements of wearable computing device 120. In other words, the received rotation measurements provide the orientation of the wearable computing device 120 about its three axes of rotation. Each rotation measurement may be associated with a timestamp, which may be provided by a clock 129 of the computing device 120, for example. As such, the received rotation measurements may be a time-based sequence of rotation measurements.
To detect gestures using both image data and received rotation measurements, processor 12 may determine one or more associations between image data from camera 115A and rotation measurements from wearable computing device 120. For example, as described above with respect to fig. 3, the time stamp of the rotation measurement may be synchronized with the time stamp of the image data. As another example, a transformation between the coordinate systems of computing device 110 and wearable computing device 120 may be determined as described above with respect to fig. 3. However, since rotation from the perspective of hand 220 may be more difficult to detect using camera 115A than rotation from the perspective of camera 115A, in other examples, processor 112 may not transform the rotation measurements.
The processor 112 may combine the received rotation measurements with motion data generated based on the image data in any of a variety of ways. For example, the combined motion data for each frame may include a location with additional rotational information. For example, frame 1 may have a field such as [ t 1; (x1, y1), (α 1, β 1, γ 1) ], wherein α 1 is the roll angle, β 1 is the yaw angle, and γ 1 is the pitch angle. Further, where there may be more rotation measurements than frames of image data, the additional rotation measurements may be used to determine the rotation of hand 220 between two frames. For example, while there may be no frame between two consecutive frames of t1 and t2, information related to the intermediate rotation of hand 220 between t1 and t2 may be interpolated based on additional rotation measurements taken between t1 and t 2. As another example, the combined motion data for each frame may include some average or weighted average of the rotation data generated based on the image data and the received rotation measurements.
Based on the recognized portion of the user's body corresponding to the gesture for executing the command and the combined motion data, the processor 112 may detect the gesture provided by the user. For example, using the combined motion data including the rotation information, the processor 112 can detect a gesture in which the hand 220 is not moving but rotating about an axis relative to the camera 115A. Once the gesture provided by the user 210 is detected, the processor 112 can determine whether the gesture corresponds to a user command, such as a user command stored in the memory 114 of the computing device 110. If so, the processor 112 may control one or more functions of the computing device 110 based on the user command. For example, the gesture shown in fig. 4 may indicate that the user wants to rotate the image currently displayed by the computing device 110. As such, the processor 112 may control the computing device 110 to change the display accordingly.
In an example where the user agrees to use such data, fig. 5 illustrates an example scenario where a gesture is detected using signal strength measurements in accordance with aspects of the present disclosure. Fig. 5 illustrates the same example motion of hand 220 in front of computing device 110 as in fig. 3. Alternatively or in addition to using motion data, the computing device 110 may use the signal strength measurements to determine depth information for the motion. Using signal strength measurements in addition to motion data and image data may improve the accuracy of recognizing gestures. For example, the signal strength measurements may have been used by computing device 110 and/or wearable computing device 120 to establish and/or maintain a connection. As another example, signal strength measurements may not require sharing of data between two devices, which may improve the efficiency of gesture detection.
As described above in the management example system, the communication module 117 of the computing device 110 may measure the signal strength of the communication link between the computing device 110 and the wearable computing device 120 while the motion of the hand 220 is captured by the camera 115A. For example, the signal strength may becomputing device 110. As such, the processor 112 may receive a time-based sequence of signal strength measurements from the communication module 117, shown as [ t 1; RSS1],...,[tn；RSSn]。
Since both the signal strength measurement and the time stamp of the image data are provided by the clock 119, it may not be necessary to synchronize the time stamps. However, alternatively or additionally, the signal strength may be measured by the communication module 127 of the wearable computing device 120 and sent to the processor 112. In such instances, the time stamp of the signal strength measurement may be provided by clock 129, and thus may need to be synchronized with the time stamp of the image data as described above with respect to fig. 3.
Based on the time-based sequence of signal strength measurements, processor 112 may determine a distance between computing device 110 and wearable computing device 120. For example, for the purpose ofwearable computing device 120 and the computing device 110. For example, as shown in fig. 5, the processor 112 may determine that at time t1, the signal strength measurement RSS1 corresponds to a circle having a distance d1 from the computing device 110, and that at time tn, the signal strength measurement RSSn corresponds to a circle having a distance dn from the computing device 110. In the case where the signal strength measurements are between the known signal strength values of two consecutive circles, the distance may be determined by taking an average or weighted average of the distances of the two consecutive circles.
The processor 112 may combine the signal strength measurements with the motion data generated based on the image data in any of a variety of ways. For example, the combined motion data for each frame may include position and signal strength measurements, such as t 1; (x1, y1), RSS1 ]. As another example, the combined motion data for each frame may include a location and a distance between wearable computing device 120 and computing device 110, such as [ t 1; (x1, y1), d1], wherein d1 is determined based on the signal strength measurement RSS 1. Further, where there may be more signal strength measurements than frames of image data, additional distances may be used to determine the distance of the hand 220 from the computing device 110 between two consecutive frames. As another example, the combined motion data may be a location that includes a numerical value of z-coordinates (or depth information). For example, frame 1 may have a field such as [ t 1; x1, y1, z1], wherein z1 can be determined by finding the difference between the distance determined based on the signal strength measurements and the distance determined based on x1 and y1, or in other words, based on the relation d1^2 ^ x1^2+ y1^2+ z1^ 2.
As described with respect to fig. 3, based on the recognized portion of the user's body corresponding to the gesture for executing the command and the combined motion data, the processor 112 may detect the gesture provided by the user. Upon detecting a gesture provided by the user 210, the processor 112 may determine whether the gesture corresponds to a user command, such as a user command stored in the memory 114 of the computing device 110. If so, the processor 112 may control one or more functions of the computing device 110 based on the user command.
In an example where a user agrees to use such data, FIG. 6 illustrates an example scenario where audio data is used to detect gestures in accordance with aspects of the present disclosure. Fig. 6 illustrates the same example motion of hand 220 in front of computing device 110 as in fig. 3. Fig. 6 further illustrates that during the movement of the hand 220, the user 210 also provides a voice command 240. As described above with respect to the example system, the audio sensor 125C of the wearable computing device 120 may detect the voice command 240 as audio data. For example, the audio data may include various information about the voice command 240, such as volume, frequency, and the like. The audio data may be associated with a timestamp, for example, provided by the clock 129 of the wearable computing device 120. As such, the processor 112 may receive audio data from the wearable computing device 120, such as the audio data shown as [ t 1'; AU1 '], …, [ tn'; AUn' ] of a time-based audio data sequence.
The processor 112 may compare the audio data detected by the wearable computing device 120 with the audio data detected by another audio sensor to determine the relative distance between the hand 220 and the computing device 110. For example, as described with respect to the example system, the audio sensor 115C of the computing device 110 may also detect the voice command 240 from the user 210 as audio data. The audio data detected by the audio sensor 115C may be associated with a timestamp, such as provided by the clock 119. Thus, as shown in fig. 6, the audio data detected by the audio sensor 115C may also be a time-based series, such as [ t 1; AU1], …, [ tn; AUn ]. As described above with respect to fig. 3, the timestamp at which wearable computing device 120 detects the audio data and the timestamp at which computing device 110 detects the audio data may be synchronized.
The processor 112 may compare the audio data detected by the wearable computing device 120 with the audio data detected by the audio sensor 115C to determine the relative distance between the hand 220 and the computing device 110. For example, if the volume of audio data detected by audio sensor 125C of wearable computing device 120 is decreasing between times t1 and t2, but the audio data detected by audio sensor 115C of computing device 110 remains the same between times t1 and t2, processor 112 may determine that hand 220 has moved closer to computing device 110 between t1 and t 2. For another example, if the volume of audio data detected by audio sensor 125C of wearable computing device 120 is increased by 10dB between t1 and t2, but the volume of audio data detected by audio sensor 115C is increased by 5dB between t1 and t2, processor 112 may determine that hand 220 has moved away from computing device 110 between t1 and t 2.
In other instances, the audio data may come from any of a variety of other sources, rather than from the user 210 (such as the voice command 240). For example, the computing device 110 may output audio data that may be detected by the computing device 120, such as by the audio sensor 125C. The processor 112 may compare one or more characteristics of the audio data output by the computing device 110 to one or more characteristics of the audio data detected by the computing device 120. Further in this regard, while the audio data shown in this example is a voice command 240 within a human listening range, audio data outside of the human listening range may alternatively or additionally be used.
The processor 112 may combine the relative distance determined based on the audio data with the motion data generated based on the image data in any of a variety of ways. For example, the combined motion data for each frame may include a position and a direction of movement, such as t 1; (x1, y1), direction of movement toward computing device 110 ]. As another example, the combined motion data for each frame may include rotation measurements and movement directions, such as t 1; (α 1, β 1, γ 1), the microphone of the computing device 110 is moving away from the speech ].
As described with respect to fig. 3, based on the recognized portion of the user's body corresponding to the gesture for executing the command and the combined motion data, the processor 112 may detect the gesture provided by the user. Upon detecting a gesture provided by the user 210, the processor 112 may determine whether the gesture corresponds to a user command, such as a user command stored in the memory 114 of the computing device 110. If so, the processor 112 may control one or more functions of the computing device 110 based on the user command.
In an example where a user agrees to use such data, FIG. 7 illustrates an example scenario where radar measurements are used to detect gestures in accordance with aspects of the present disclosure. Fig. 7 illustrates the same example motion of hand 220 in front of computing device 110 as in fig. 3. As an alternative or in addition to inertial measurements, computing device 110 may use radar measurements to detect gestures.
As described above with respect to example systems, the computing device 110 may further include a radar sensor 115D for measuring the position and/or movement of objects in its surroundings. For example, the radar measurements may include the position and/or velocity of objects moving around the computing device 110. In some examples, the radar measurement may be a two-dimensional measurement. In such an example, two axes may be selected such that one axis corresponds to an axis perpendicular to the aperture of the camera 115A. In this way, the radar measurements may include depth information. For example, fig. 7 shows radar measurements in the x-axis and z-axis directions. In other examples where the radar measurement is a three-dimensional measurement, three axes may be selected to be parallel to the three axes of the camera 115A, respectively. Each radar measurement may be associated with a timestamp, such as a timestamp provided by a clock 119 of the computing device 110. As such, the processor 112 may receive a time-based sequence of radar measurements from the radar sensor 115D, e.g., as shown as [ t 1; (x1, z1), (v _ x1, v _ z1) ], …, [ tn; (xn, zn), (v _ xn, v _ zn) ].
However, since radar sensor 115D will detect any objects moving around it, processor 112 may need to determine a set of radar measurements that correspond to the motion of hand 220, rather than to some other object during the motion of the hand. For example, the processor 112 may determine a set of radar measurements that match some aspect of motion data generated based on the image data. For example, processor 112 may determine the set of radar measurements by matching a value of an x-axis in the radar measurements to an x-coordinate in a position of hand 220 generated based on the image data.
Since both the radar measurement and the time stamp of the image data are provided by the clock 119, it may not be necessary to synchronize the time stamps. However, alternatively or additionally, the radar measurements may be taken by a radar sensor in the wearable computing device 120 and sent to the processor 112. In such instances, the time stamp of the radar measurement may be provided by clock 129, and thus may need to be synchronized with the image data as described above with respect to fig. 3. Further in such instances, a transformation between the coordinate system of the radar sensor on the wearable computing device 120 and the coordinate system of the computing device 110 as described above with respect to fig. 3 may need to be determined.
The processor 112 may combine the radar measurements with the motion data generated based on the image data in any of a variety of ways. For example, the combined motion data for each frame may include a location with depth information, such as [ t 1; x1, y1, z1], wherein z1 is a radar measurement. Further, where there may be more radar measurements than frames of image data, this additional radar measurement may be used to interpolate information about the middle movement of hand 220 between two consecutive frames. As another example, the combined motion data for each frame may include a position and a velocity, such as t 1; (x1, y1, z1), (v _ x1, v _ z1) ], wherein z1, v _ x1 and v _ z1 are radar measurements.
As described with respect to fig. 3, based on the recognized portion of the user's body corresponding to the gesture for executing the command and the combined motion data, the processor 112 may detect the gesture provided by the user. Upon detecting a gesture provided by the user 210, the processor 112 may determine whether the gesture corresponds to a user command, such as a user command stored in the memory 114 of the computing device 110. If so, the processor 112 may control one or more functions of the computing device 110 based on the user command.
While each of the examples of fig. 3-7 describe combining image data with one other type of data for gesture detection, any of a number of combinations of the various types of data described above may be used for gesture detection. For example, the processor 112 may combine inertial measurements received from the accelerometers 125A, gyroscopes 125B with signal strength measurements received from the communication module 117. Further, while the examples of fig. 3-7 describe some types of sensor data, other types of data may additionally or alternatively be used. For example, the processor 112 may combine inertial measurements from the accelerometers 125A, gyroscopes 125B with image data from an infrared optical sensor on the wearable computing device 120.
The association between the image data and other types of data may be determined in any of a number of ways. For example, synchronization may be performed between the image data and sensor data from one sensor of the wearable computing device 120, which is then applied to all other sensor data from other sensors of the wearable computing device 120. Further in this regard, a transformation between the image data and the sensor data from one sensor of the wearable computing device 120 may be determined and then applied to all other sensor data from other sensors of the wearable computing device 120. In other examples, by comparing various types of sensor data together, synchronization and/or transformation between the image data and various sensors of wearable computing device 120 may be performed in order to achieve more accurate synchronization and/or transformation across multiple sensors.
Fig. 8 illustrates an example scenario of detecting gestures using data received from multiple wearable computing devices in accordance with aspects of the present disclosure. As shown in this example, the wearable computing device 120 may be a smart watch worn on the wrist of the user 210, and the wearable computing device 130 may be a head-mounted device worn on the head 230 of the user 210. Although fig. 8 shows only two wearable computing devices, in other examples, sensor data from more than two wearable computing devices may be used to detect gestures.
In this regard, processor 112 may receive sensor data from wearable computing device 120 and sensor data from wearable computing device 130. The sensor data from the wearable computing device 120 and the sensor data from the wearable computing device 130 may include the same type or different types of sensor data. For example, the sensor data from the wearable computing device 120 may include inertial measurements, signal strength measurements, and audio data, while the sensor data from the wearable computing device 130 may include signal strength measurements, audio data, and image data. The received sensor data may be associated with a timestamp, e.g., clock 129 may provide a timestamp for sensor data from wearable computing device 120 and clock 139 may provide a timestamp for sensor data from wearable computing device 130. The received sensor data may have different coordinate systems, e.g., the coordinate system of wearable computing device 120 defined by x ', y ', and z ' may be different from the coordinate system of wearable computing device 130 defined by x ", y", and z ".
To detect gestures using the image data and the received sensor data, the processor 112 may determine one or more associations between the image data from the camera 115A and the sensor data from the computing device 120, and one or more associations between the image data from the camera 115A and the sensor data from the computing device 130. For example, the timestamps of the received sensor data from both computing device 120 and computing device 130 may be synchronized with the timestamp of the image data as described above with respect to fig. 3. As another example, as described with respect to fig. 3, a transformation between the coordinate systems of computing device 110 and wearable computing device 120 and between the coordinate systems of computing device 110 and wearable computing device 130 may be determined. Further in this regard, the processor 112 may determine the relative location of the wearable computing devices 120 and 130 in order to determine the transformation. For example, the processor 112 may identify the relative position of the two wearable computing devices 120 and 130 in each frame of image data using image processing methods. Alternatively or additionally, the processor 112 may determine the relative location of the wearable computing devices 120 and 130 by comparing signal strength measurements and/or audio data from the two wearable computing devices 120 and 130.
The processor 112 may combine the received sensor data with motion data generated based on the image data in any of a number of ways. For example, the motion of hand 220 may be captured by camera 115A of computing device 110, camera 135A of wearable computing device 130, and accelerometer 125A of wearable computing device 120. For example, as described above with respect to fig. 3, the motion data may be generated based on image data from camera 115A and based on image data from camera 135A. These motion data may be associated and combined according to their synchronized time stamps. As described with respect to fig. 3, for yet another example, the motion data may be further associated and combined with inertial measurements from the accelerometer 125A. As another example, the voice command may be captured by audio sensor 115C of computing device 110, audio sensor 125C of wearable computing device 120, and audio sensor 135C of wearable computing device 130. For example, as described above with respect to fig. 6, audio data from the three computing devices 110, 120, 130 may be compared in order to determine the relative locations of the three computing devices 110, 120, 130.
As described with respect to fig. 3, the processor 112 may use the combined motion data to determine a gesture provided by the user. Upon detecting a gesture provided by the user 210, the processor 112 may determine whether the gesture corresponds to a user command, such as a user command stored in the memory 114 of the computing device 110. If so, the processor 112 may control one or more functions of the computing device 110 based on the user command.
Fig. 9 illustrates an example flow diagram that may be executed by one or more processors, such as one or more processors 112 of computing device 110. For example, the processor 112 of the computing device 110 may receive the data and make various determinations as shown in the flow chart. Referring to fig. 9, at block 910, image data capturing motion of a user may be received. At block 920, motion data from one or more wearable computing devices worn by the user may be received. At block 930, a portion of the user's body corresponding to a gesture for performing a command may be identified based on the image data. At block 940, one or more associations between the image data and the received motion data may be determined. At block 950, a gesture may be detected based on the identified portion of the user's body and one or more associations between the image data and the received motion data.
This technique is beneficial because it allows a system with limited sensors to accurately determine user inputs provided as complex and fast gestures, among other reasons. By correlating inertial measurements from the wearable device with image data captured by the system, the image data can be supplemented with depth and rotation information. When the image data is obtained at a lower frequency than the inertial measurements, information about the user's movement in the middle between successive frames of image data can be interpolated more accurately, thus improving the accuracy of the system's interpretation of the user input. Features of the technique further provide for using other types of data for detecting gestures, such as signal strength measurements, audio data, and radar measurements. In addition, many users may find this technique relatively easy to use, as the wearable device may have been paired with the system using second factor authentication.
Unless otherwise indicated, the above alternative examples are not mutually exclusive and may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. Additionally, the examples described herein and terms expressed as "such as," "including," and the like, provided herein should not be construed to limit the claimed subject matter to the specific examples; rather, this example is intended to be merely illustrative of one of many possible embodiments. Further, the same reference numbers in different drawings can identify the same or similar elements.
Claims (20)
1. A method, comprising:
receiving, by one or more processors, image data from one or more visual sensors that capture motion of a user;
receiving, by the one or more processors, motion data from one or more wearable computing devices worn by the user;
identifying, by the one or more processors, a portion of the user's body corresponding to a gesture for performing a command based on the image data;
determining, by the one or more processors, one or more associations between the image data and the received motion data; and is
Detecting, by the one or more processors, the gesture based on the identified portion of the user's body and the one or more associations between the image data and the received motion data.
2. The method of claim 1, wherein determining the one or more associations further comprises synchronizing a timestamp associated with the image data with a timestamp associated with the received motion data.
3. The method of any of claims 1 or 2, further comprising:
determining, by the one or more processors, a first coordinate system of a perspective from the one or more vision sensors;
determining, by the one or more processors, a second coordinate system of a perspective from the one or more wearable computing devices;
determining, by the one or more processors, one or more transforms between the first coordinate system and the second coordinate system, wherein determining the one or more associations further comprises determining the one or more transforms.
4. The method of any preceding claim, further comprising:
determining, by the one or more processors, a location of one or more fingers of the user's hand where the identified portion of the user's body comprises the user's hand, wherein detecting the gesture is further based on the location of the one or more fingers.
5. The method of any preceding claim, further comprising:
generating, by the one or more processors, a time-based motion data sequence for the identified portion of the user's body based on the image data, the generated time-based motion data sequence including at least one of: a time-based position sequence, a time-based velocity sequence, and a time-based acceleration sequence.
6. The method of claim 5, wherein the received motion data comprises a time-based inertial measurement sequence, and wherein determining the one or more associations comprises matching the time-based motion data sequence generated based on the image data with the time-based inertial measurement sequence.
7. The method of any preceding claim, further comprising:
determining, by the one or more processors, depth information for the motion of the user based on the received motion data, wherein detecting the gesture is further based on the depth information.
8. The method of any preceding claim, further comprising:
determining, by the one or more processors, an orientation of the one or more wearable computing devices based on the received motion data, wherein detecting the gesture is further based on the orientation of the one or more wearable computing devices.
9. The method of any preceding claim, further comprising:
interpolating, by the one or more processors, an intermediate movement of the user between two consecutive frames of the image data based on the received motion data, wherein detecting the gesture is further based on the intermediate movement.
10. The method of any preceding claim, further comprising:
receiving, by the one or more processors, a pairing request from the one or more wearable computing devices;
requesting, by the one or more processors, authentication for pairing with the one or more wearable computing devices to receive data over a communication link;
receiving, by the one or more processors, an authentication to pair with the one or more wearable computing devices to receive data over a communication link.
11. The method of any preceding claim, further comprising:
requesting, by the one or more processors, permission to use data from the one or more wearable computing devices for gesture detection;
receiving, by the one or more processors, permission to use data from the one or more wearable computing devices for gesture detection.
12. The method of any preceding claim, further comprising:
receiving, by the one or more processors, signal strength measurements of a connection with the one or more wearable computing devices;
determining, by the one or more processors, one or more associations between the image data and the signal strength measurements, wherein detecting the gesture is further based on the one or more associations between the image data and the signal strength measurements.
13. The method of claim 12, further comprising:
determining, by the one or more processors, a distance between the one or more wearable computing devices and the one or more visual sensors based on the signal strength measurements, wherein detecting the gesture is further based on the distance between the one or more wearable computing devices and the one or more visual sensors.
14. The method of any preceding claim, further comprising:
receiving, by the one or more processors, audio data from one or more audio sensors;
receiving, by the one or more processors, audio data from the one or more wearable computing devices;
determining, by the one or more processors, one or more associations between the image data and audio data from the one or more wearable computing devices;
comparing, by the one or more processors, audio data received from the one or more wearable computing devices with audio data received from the one or more audio sensors, wherein detecting the gesture is further based on the comparison.
15. The method of any preceding claim, further comprising:
receiving, by the one or more processors, radar measurements from a radar sensor;
determining, by the one or more processors, one or more associations between the image data and the radar measurements, wherein detecting the gesture is further based on the one or more associations between the image data and the radar measurements.
16. The method of any preceding claim, further comprising:
determining, by the one or more processors, a relative location of the one or more wearable computing devices, wherein the one or more wearable computing devices include a plurality of wearable computing devices, and wherein detecting the gesture is further based on the relative location of the one or more wearable computing devices.
17. A system, comprising:
one or more vision sensors configured to collect image data;
one or more processors configured to:
receiving image data from one or more visual sensors that capture motion of a user;
receiving motion data from one or more wearable computing devices worn by the user;
identifying, based on the image data, a portion of the user's body corresponding to a gesture for performing a command;
determining one or more associations between the image data and the received motion data; and is
Detecting a gesture based on the identified portion of the user's body and the one or more associations between the image data and the received motion data.
18. The system of claim 17, wherein the one or more visual sensors are front facing cameras.
19. The system of claim 17 or 18, wherein the motion data comprises inertial measurements from at least one of an accelerometer and a gyroscope.
20. The system of any of claims 17 to 19, further comprising:
a communication module configured to measure a signal strength of a connection with the one or more wearable computing devices;
wherein the one or more processors are further configured to:
receiving signal strength measurements of connections with the one or more wearable computing devices;
determining one or more associations between the image data and the signal strength measurements, wherein detecting the gesture is further based on the one or more associations between the image data and the signal strength measurements.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/373,901 | 2019-04-03 | ||
US16/373,901 US10908695B2 (en) | 2019-04-03 | 2019-04-03 | Gesture detection using external sensors |
PCT/US2019/057175 WO2020204996A1 (en) | 2019-04-03 | 2019-10-21 | Gesture detection using external sensors |
Publications (1)
Publication Number | Publication Date |
---|---|
CN113498502A true CN113498502A (en) | 2021-10-12 |
Family
ID=68536899
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980093483.5A Pending CN113498502A (en) | 2019-04-03 | 2019-10-21 | Gesture detection using external sensors |
Country Status (4)
Country | Link |
---|---|
US (5) | US10908695B2 (en) |
EP (1) | EP3906457A1 (en) |
CN (1) | CN113498502A (en) |
WO (1) | WO2020204996A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2022217598A1 (en) * | 2021-04-16 | 2022-10-20 | 华为技术有限公司 | Limb recognition method and apparatus |
KR20230060328A (en) * | 2021-10-27 | 2023-05-04 | (주)와이즈업 | AI studio systems for online lectures and a method for controlling them |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104094194A (en) * | 2011-12-09 | 2014-10-08 | 诺基亚公司 | Method and apparatus for identifying a gesture based upon fusion of multiple sensor signals |
US9417704B1 (en) * | 2014-03-18 | 2016-08-16 | Google Inc. | Gesture onset detection on multiple devices |
CN106055085A (en) * | 2015-04-08 | 2016-10-26 | 联发科技股份有限公司 | Wearable device interactive method and interactive wearable device |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2017100706A1 (en) * | 2015-12-09 | 2017-06-15 | Origin Wireless, Inc. | Method, apparatus, and systems for wireless event detection and monitoring |
US9569001B2 (en) | 2009-02-03 | 2017-02-14 | Massachusetts Institute Of Technology | Wearable gestural interface |
US10185416B2 (en) | 2012-11-20 | 2019-01-22 | Samsung Electronics Co., Ltd. | User gesture input to wearable electronic device involving movement of device |
US10852838B2 (en) * | 2014-06-14 | 2020-12-01 | Magic Leap, Inc. | Methods and systems for creating virtual and augmented reality |
US20170090582A1 (en) * | 2015-09-24 | 2017-03-30 | Intel Corporation | Facilitating dynamic and intelligent geographical interpretation of human expressions and gestures |
-
2019
- 2019-04-03 US US16/373,901 patent/US10908695B2/en active Active
- 2019-10-21 CN CN201980093483.5A patent/CN113498502A/en active Pending
- 2019-10-21 WO PCT/US2019/057175 patent/WO2020204996A1/en unknown
- 2019-10-21 EP EP19801999.4A patent/EP3906457A1/en active Pending
-
2020
- 2020-12-31 US US17/139,241 patent/US11301052B2/en active Active
-
2022
- 2022-03-11 US US17/692,833 patent/US11507198B2/en active Active
- 2022-10-20 US US17/969,823 patent/US11822731B2/en active Active
-
2023
- 2023-10-23 US US18/491,837 patent/US20240152213A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104094194A (en) * | 2011-12-09 | 2014-10-08 | 诺基亚公司 | Method and apparatus for identifying a gesture based upon fusion of multiple sensor signals |
US9417704B1 (en) * | 2014-03-18 | 2016-08-16 | Google Inc. | Gesture onset detection on multiple devices |
CN106055085A (en) * | 2015-04-08 | 2016-10-26 | 联发科技股份有限公司 | Wearable device interactive method and interactive wearable device |
Also Published As
Publication number | Publication date |
---|---|
EP3906457A1 (en) | 2021-11-10 |
US20240152213A1 (en) | 2024-05-09 |
US11301052B2 (en) | 2022-04-12 |
US20210173489A1 (en) | 2021-06-10 |
US11507198B2 (en) | 2022-11-22 |
US20200319713A1 (en) | 2020-10-08 |
US20220221941A1 (en) | 2022-07-14 |
US20230037981A1 (en) | 2023-02-09 |
US11822731B2 (en) | 2023-11-21 |
WO2020204996A1 (en) | 2020-10-08 |
US10908695B2 (en) | 2021-02-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11816853B2 (en) | Systems and methods for simultaneous localization and mapping | |
US11822731B2 (en) | Gesture detection using external sensors | |
US11743649B2 (en) | Shared earbuds detection | |
US11979038B2 (en) | Wireless charging alignment | |
US10466814B2 (en) | Electronic system, indicating device and operating method thereof | |
CN116348916A (en) | Azimuth tracking for rolling shutter camera | |
US20210096650A1 (en) | Methods and Systems for Identifying Three-Dimensional-Human-Gesture Input | |
US20190212834A1 (en) | Software gyroscope apparatus | |
US20240126369A1 (en) | Information processing system and information processing method | |
WO2022228056A1 (en) | Human-computer interaction method and device | |
WO2024006693A1 (en) | Virtual selfie-stick selfie | |
CN115562479A (en) | Control method of electronic equipment, control device of electronic equipment and wearable equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |