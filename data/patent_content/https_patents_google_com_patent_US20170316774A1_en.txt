US20170316774A1 - Adaptive text-to-speech outputs - Google Patents
Adaptive text-to-speech outputs Download PDFInfo
- Publication number
- US20170316774A1 US20170316774A1 US15/653,872 US201715653872A US2017316774A1 US 20170316774 A1 US20170316774 A1 US 20170316774A1 US 201715653872 A US201715653872 A US 201715653872A US 2017316774 A1 US2017316774 A1 US 2017316774A1
- Authority
- US
- United States
- Prior art keywords
- user
- text
- context
- computers
- client device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000003044 adaptive effect Effects 0.000 title description 5
- 238000000034 method Methods 0.000 claims description 62
- 230000004044 response Effects 0.000 claims description 29
- 238000004590 computer program Methods 0.000 claims description 11
- 230000015572 biosynthetic process Effects 0.000 claims description 10
- 238000003786 synthesis reaction Methods 0.000 claims description 10
- 230000015654 memory Effects 0.000 description 33
- 238000004891 communication Methods 0.000 description 24
- 230000008569 process Effects 0.000 description 23
- 238000010586 diagram Methods 0.000 description 12
- 230000000694 effects Effects 0.000 description 9
- 230000009471 action Effects 0.000 description 8
- 238000013507 mapping Methods 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 4
- 238000000926 separation method Methods 0.000 description 4
- 238000006467 substitution reaction Methods 0.000 description 4
- 230000006399 behavior Effects 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000013518 transcription Methods 0.000 description 3
- 230000035897 transcription Effects 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 238000004422 calculation algorithm Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000003292 diminished effect Effects 0.000 description 2
- 239000000284 extract Substances 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 238000007781 pre-processing Methods 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 230000001755 vocal effect Effects 0.000 description 2
- 230000006978 adaptation Effects 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 230000002238 attenuated effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 150000001875 compounds Chemical class 0.000 description 1
- 238000012937 correction Methods 0.000 description 1
- 238000013479 data entry Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
-
- G10L13/043—
-
- G06F17/2775—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/253—Grammatical analysis; Style critique
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
Definitions
- This specification generally describes electronic communications.
- Speech synthesis refers to the artificial production of human speech.
- Speech synthesizers can be implemented in software or hardware components to generate speech output corresponding to a text.
- a text-to-speech (TTS) system typically converts normal language text into speech by concatenating pieces of recorded speech that are stored in a database.
- Speech synthesis has become more central to user experience as a greater portion of electronic computing has shifted from desktop to mobile environments. For example, increases in the use of smaller mobile devices without displays have led to increases in the use of text-to-speech systems for accessing and using content that is displayed on mobile devices.
- TTS systems are often unable to adapt to varying language proficiencies of different users. This lack of flexibility often prevents users with limited language proficiencies from understanding complex text-to-speech outputs. For instance, non-native language speakers that use a TTS system can have difficulty understanding a text-to-speech output because of their limited language familiarity.
- a user's instantaneous ability to understand text-to-speech outputs can also vary based on a particular user context. For instance, some user contexts include background noise that can make it more difficult to understand longer or more complex text-to-speech outputs.
- a system adjusts the text used for a text-to-speech output based on the language proficiency of a user to increase a likelihood that the user can comprehend the text-to-speech output. For instance, the language proficiency of a user can be inferred from prior user activity and be used to adjust the text-to-speech output to an appropriate complexity that is commensurate with the language proficiency of the user.
- a system obtains multiple candidate text segments that correspond to different levels of language proficiency. The system then selects the candidate text segment that best matches and most closely corresponds to a user's language proficiency and provides a synthesized utterance of the selected text segment for output to the user.
- a system alters the text in a text segment to better correspond to the user's language proficiency prior to generating a text-to-speech output.
- Various aspects of a text segment can be adjusted, including the vocabulary, sentence structure, length, and so on.
- the system then provides a synthesized utterance of the altered text segment for output to the user.
- the users may be provided with an opportunity to control whether programs or features collect personal information, e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location, or to control whether and/or how to receive content from the content server that may be more relevant to the user.
- personal information e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location
- certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be anonymized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained, such as to a city, zip code, or state level, so that a particular location of a user cannot be determined.
- location information such as to a city, zip code, or state level, so that a particular location of a user cannot be determined.
- the user may have control over how information is collected about him or her and used by a content server.
- a computer-implemented method can include: determining, by the one or more computers, a language proficiency of a user of a client device; determining, by the one or more computers, a text segment for output by a text-to-speech module based on the determined language proficiency of the user; generating, by the one or more computers, audio data including a synthesized utterance of the text segment; and providing, by the one or more computers and to the client device, the audio data including the synthesized utterance of the text segment.
- the client device displays a mobile application that uses a text-to-speech interface.
- determining the language proficiency of the user includes inferring a language proficiency of the user based at least on previous queries submitted by the user.
- determining the text segment for output by the text-to-speech module includes: identifying multiple text segments as candidates for a text-to-speech output of the user, the multiple text segments having different levels of language complexity; and selecting from among the multiple text segments based at least on the determined language proficiency of the user of the client device.
- selecting from among the multiple text segments includes: determining a language complexity score for each of the multiple text segments; and selecting the text segment having the language complexity score that best matches a reference score that describes the language proficiency of the user of the client device.
- determining the text segment for output by the text-to-speech module includes: identifying a text segment for a text-to-speech output to the user; computing a complexity score of the text segment for the text-to-speech output; and modifying the text segment for the text-to-speech output to the user based at least on the determined language proficiency of the user and the complexity score of text segment for the text-to-speech output.
- modifying the text segment for the text-to-speech output to the user includes: determining an overall complexity score for the user based at least on the determining language proficiency of the user; determining a complexity score for individual portions within the text segment for the text-to-speech output to the user; identifying one or more individual portions within the text segment with complexity scores greater than the overall complexity score for the user; and modifying the one or more individual portions within the text segment to reduce complexity scores below the overall complexity score.
- modifying the text segment for the text-to-speech output to the user includes: receiving data indicating a context associated with the user; determining an overall complexity score for the context associated with the user; determining that the complexity score of the text segment exceeds the overall complexity score for the context associated with the user; and modifying the text segment to reduce the complexity score below the overall complexity score for the context associated with the user.
- a computer-implemented method includes: receiving data indicating a context associated with the user; determining an overall complexity score for the context associated with the user; identifying a text segment for a text-to-speech output to the user; determining that the complexity score of the text segment exceeds the overall complexity score for the context associated with the user; and modifying the text segment to reduce the complexity score below the overall complexity score for the context associated with the user.
- determining the overall complexity score for the context associated with the user includes: identifying terms included within previously submitted queries by the user when the user was determined to be in the context; and determining an overall determining an overall complexity score for the context associated with the user based at least on the identified terms.
- the data indicating the context associated with the user includes queries that were previously submitted by the user.
- the data indicating the context associated with the user includes a GPS signal indicating a current location associated with the user.
- FIG. 1 is a diagram that illustrates examples of processes for generating text-to-speech outputs based on language proficiency.
- FIG. 2 is a diagram that illustrates an example of a system for generating an adaptive text-to-speech output based on a user context.
- FIG. 3 is a diagram that illustrates an example of a system for modifying a sentence structure within a text-to-speech output.
- FIG. 4 is a block diagram that illustrates an example of a system for generating adaptive text-to-speech outputs based on using clustering techniques.
- FIG. 5 is a flow diagram that illustrates an example of a process for generating adaptive text-to-speech outputs.
- FIG. 6 is a block diagram of computing devices on which the processes described herein, or portions thereof, can be implemented.
- FIG. 1 is a diagram that illustrates examples of processes 100 A and 100 B for generating text-to-speech outputs based on language proficiency.
- the processes 100 A and 1006 are used to generate different text-to-speech outputs for a user 102 a with high language proficiency and a user 102 b with low language proficiency, respectively, for a text query 104 .
- the process 100 A after receiving a query 104 on the user devices 106 a and 106 b , the process 100 A generates a high-complexity text-to-speech output 108 a for the user 102 a whereas the process 100 B generates a low-complexity output 108 b for the user 102 b .
- the TTS systems that execute processes 100 A and 1006 can include a language proficiency estimator 110 , a text-to-speech engine 120 .
- the text-to-speech engine 120 can further include a text analyzer 122 , a linguistics analyzer 124 , and a waveform generator 126 .
- the content of text that is used to generate a text-to-speech output can be determined according to a language proficiency of a user.
- the text to be used to generate a text-to-speech output can be determined based on a context of the user, for example, the location or activity of a user, background noise present, a current task of the user and so on.
- the text to be converted to an audible form may be adjusted or determined using other information, such as indications that a user has failed to complete a task or is repeating an action.
- two users provide the same query 104 on user devices 106 a and 106 b , respectively, as input to an application, web page, or other search functionality.
- the query 104 can be a voice query sent to the user devices 106 a and 106 b to determine a weather forecast for the current day.
- the query 104 is then transmitted to the text-to-speech engine 120 to generate a text-to-speech output in response to the query 104 .
- the language proficiency estimator 110 can be a software module within a TTS system that determines a language proficiency score associated with a particular user (e.g., the user 102 a or the user 102 b ) based on user data 108 a .
- the language proficiency score can be an estimate of the user's ability to understand communications in a particular language, in particular, to understand speech in the particular language.
- One measure of language proficiency is the ability of a user to successfully complete a voice-controlled task. Many types of tasks, such as setting a calendar appointment, looking up directions, and so on, follow a sequence of interactions in which a user and device exchange verbal communication. The rate at which a user successfully completes these task workflows through a voice interface is a strong indicator of the user's language proficiency.
- a user that completes nine out of ten voice tasks that the user initiates likely has a high language proficiency.
- a user that fails to complete the majority of voice tasks that the user initiates can be inferred to have a low language proficiency, since the user may not have fully understood the communications from the device or may not have been able to provide appropriate verbal responses.
- the TTS may use adapted, simplified outputs that may increase the ability of the user to understand and complete various tasks.
- the user data 108 a can include words used within prior text queries submitted by the user, an indication whether English, or any other language utilized by the TTS system, is the native language of the user, and a set of activities and/or behaviors that are reflective of a user's language comprehension skills. For example, as depicted in FIG. 1 , a typing speed of the user can be used to determine language fluency of the user in a language. In addition, a language vocabulary complexity score or language proficiency score can be assigned to the user based on associating a pre-determined complexity to words that were used by the user in previous text queries. In another example, the number of misrecognized words in prior queries can also be used to determine the language proficiency score.
- a high number of misrecognized words can be used to indicate a low language proficiency.
- the language proficiency score is determined by looking up a stored score associated with the user, which was determined for the user prior to submission of the query 104 .
- FIG. 1 depicts the language proficiency estimator 110 as a separate component to the TTS engine 120
- the language proficiency estimator 110 can be an integrated software module within the TTS engine 120 . In such instances, operations involving the language proficiency estimation can be directly modulated by the TTS engine 120 .
- the language proficiency score assigned to the user may be based on a particular user context estimated for the user. For instance, as described more particularly with respect to FIG. 2 , a user context determination can be used to determine context-specific language proficiencies that can cause a user to temporarily have limited language comprehension abilities. For example, if the user context indicates significant background noise or if the user is engaged in a task such as driving, the language proficiency score can be used to indicate that the user's present language comprehension ability is temporarily diminished relative to other user contexts.
- the language proficiency score can instead be directly provided to the TTS engine 120 without the use of the language proficiency estimator 110 .
- a language proficiency score can be designated to a user based on user input during a registration process that specifies a user's level of language proficiency. For example, during the registration, the user can provide a selection that specifies the user's skill level, which can then be used to calculate the appropriate language proficiency for the user. In other examples, the user can provide other types of information such as demographic information, education level, places of residences, etc., that can be used to specify the user's level of language proficiency.
- the language proficiency score can either be a set of discrete values that are adjusted periodically based on recently generated user activity data, or a continuous score that is initially designated during a registration process.
- the value of the language proficiency score can be biased based on one or more factors that indicate that a user's present language comprehension and proficiency may be attenuated (e.g., a user context indicating significant background noise).
- the value of the language proficiency score can be preset after an initial calculation and adjusted only after specific milestone events that indicate that a user's language proficiency has increased (e.g., an increase in typing rate or a decrease in correction rate for a given language).
- a combination of these two techniques can be used to variably adjust the text-to-speech output based on a particular text input.
- multiple language proficiency scores that each represent a particular aspect of the user's language skills can be used to determine to how best adjust the text-to-speech output for the user. For example, one language proficiency score can represent a complexity of the user's vocabulary whereas another language proficiency score can be used to represent the user's grammar skills.
- the TTS engine 120 can use the language proficiency score to generate a text-to-speech output that is adapted to the language proficiency indicated by the user's language proficiency score. In some instances, the TTS engine 120 adapts the text-to-speech output based on selecting a particular TTS string from a set of candidate TTS strings for the text query 104 . In such instances, the TTS engine 120 selects the particular TTS string based on using the language proficiency score of the user to predict a likelihood that each of the candidate TTS strings will accurately be interpreted by the user. More particular descriptions related to these techniques are provided with respect to FIG. 2 .
- the TTS engine 120 can select a baseline TTS string and adjust the structure of the TTS string based on the user's level of language proficiency indicated by the language proficiency score. In such instances, the TTS engine 120 can adjust the grammar of the baseline TTS string, provide word substitutions and/or reduce the sentence complexity to generate an adapted TTS string that is more likely to be understood by the user. More particular descriptions related to these techniques are provided with respect to FIG. 3 .
- the TTS engine 120 may generate different text-to-speech outputs for users 102 a and 102 b because the language proficiency scores for the users are different.
- the language proficiency score 106 a indicates high English-language proficiency, inferred from the user data 108 a indicating that the user 102 a has a complex vocabulary, has English as a first language, and has a relatively high word per minute in prior user queries.
- the TTS engine 120 Based on the value of the language proficiency score 106 a , the TTS engine 120 generates a high complexity text-to-speech output 108 a that includes a complex grammatical structure.
- the text-to-speech output 108 a includes an independent clause that describes that today's forecast is sunny, in addition to a subordinate clause that includes additional information about the high temperature and the low temperature of the day.
- the language proficiency score 106 b indicates low English-language proficiency, inferred from user activity data 108 b indicating that the user 102 b has a simple vocabulary, has English as a second language, and has previously provided ten incorrect queries.
- the TTS engine 120 generates a low complexity text-to-speech output 108 b that includes a simpler grammatical structure relative to the text-to-speech output 108 a .
- the text-to-speech output 108 b includes a single independent clause that conveys the same primary information as the text-to-speech output 108 a (e.g., today's forecast being sunny), but does not include additional information related to the high and low temperatures for the day.
- a TTS engine of a server system may include functionality to adjust text based on a language proficiency score and then output audio including a synthesized utterance of the adjusted text.
- a pre-processing module of a server system may adjust text and pass the adjusted text to a TTS engine for speech synthesis.
- a user device may include a TTS engine, or a TTS engine and a text pre-processor, to be able to generate appropriate TTS outputs.
- a TTS system can include software modules that are configured to exchange communications with a third-party mobile application of a client device or a web page.
- the TTS functionality of the system can be made available to a third-party mobile application through an application package interface (API).
- the API can include defined set of protocols that an application or web site can use to request TTS audio from a server system that runs the TTS engine 120 .
- the API can make available TTS functionality that runs locally on a user's device.
- the API may be available to an application or web page through an inter-process communication (IPC), remote procedure call (RPC), or other system call or function.
- IPC inter-process communication
- RPC remote procedure call
- a TTS engine, and associated language proficiency analysis or text preprocessing may be run locally on the user's device to determine an appropriate text for the user's language proficiency and generate the audio for the synthesized speech also.
- the third-party application or web page can use the API to generate a set of voice instructions that are provided to the user based on a task flow of a voice interface of the third-party application or web page.
- the API can specify that the application or web page should provide text to be converted to speech. In some instances, other information can be provided, such as a user identifier or a language proficiency score.
- the TTS engine 120 can be used to determine whether a text segment from a third-party application should be adjusted prior to generating a text-to-speech output for the text.
- the API can include computer-implemented protocols that specify conditions within the third-party application that initiate the generation of an adaptive text-to-speech output.
- one API may permit an application to submit multiple different text segments as candidates for a TTS output, where the different text segments correspond to different levels of language proficiency.
- the candidates can be text segments having equivalent meanings but different complexity levels (e.g., a high complexity response, a medium complexity response, and a low complexity response).
- the TTS engine 120 may then determine the level of language proficiency needed to understand each candidate, determine an appropriate language proficiency score for the user, and select the candidate text that best corresponds to the language proficiency score.
- the TTS engine 120 then provides synthesized audio for the selected text back to the application, e.g., over a network using the API.
- the API can be locally available on the user devices 106 a and 106 b .
- the API can be accessible over various types of inter-process communication (IPC) or via a system call.
- IPC inter-process communication
- the output of the API on the user devices 106 a and 106 b can be the text-to-speech output of the TTS engine 120 since the API operates locally on the user devices 106 a and 106 b.
- an API can allow the third-party application to provide a single text segment and a value that indicates whether the TTS engine 120 is permitted to modify the text segment to generate a text segment with a different complexity. If the app or web page indicates that alteration is permitted, the TTS system 120 may make various changes to the text, for example, to reduce the complexity of the text when the language proficiency score suggests that the original text is more complex than the user can understand in a spoken response.
- an API allows the third-party application to also provide user data (e.g., prior user queries submitted on the third-party application) along with the text segment such that the TTS engine 120 can determine a user context associated with the user and adjust generate a particular text-to-speech output based on the determined user context.
- an API can allow an application to provide context data from a user device (e.g., a global positioning signal, accelerometer data, ambient noise level, etc.) or an indication of a user context to allow the TTS engine 120 to adjust the text-to-speech outputs that will ultimately be provided to the user through the third-party application.
- the third party application can also provide the API with data that can be used to determine a language proficiency of the user.
- the TTS engine 120 can adjust the text-to-speech output for a user query without using a language proficiency of the user or determining a context associated with the user.
- TTS engine 120 can determine that an initial text-to-speech output is too complex for a user based on receiving signals that a user has misunderstood the output (e.g., multiple retries on the same query or task).
- the TTS engine 120 can reduce the complexity of a subsequent text-to-speech response for a retried query or related queries.
- the TTS engine 120 may progressively reduce the amount of detail or language proficiency required to understand the TTS output until it reaches a level that the user understands.
- FIG. 2 is a diagram that illustrates an example of a system 200 that adaptively generates a text-to-speech output based on a user context.
- the system 200 can include a TTS engine 210 that includes a query analyzer 211 , a language proficiency estimator 212 , an interpolator 213 , a linguistics analyzer 214 , a re-ranker 215 , and a waveform generator 216 .
- the system 200 also includes a context repository 220 that stores a set of context profiles 232 , and a user history manager 230 that stores user history data 234 .
- the TTS engine 210 corresponds to the TTS engine 120 as described with respect to FIG. 1 .
- a user 202 initially submits a query 204 on a user device 208 that includes a request for information related to the user's first meeting for the day.
- the user device 208 can then transmit the query 204 and context data 206 associated with the user 202 to the query analyzer 211 and the language proficiency estimator 212 , respectively.
- Other types of TTS outputs that are not responses to queries, e.g., calendar reminders, notifications, task workflows, etc., may be adapted using the same techniques.
- the context data 206 can include information relating to a particular context associated with the user 202 such as time intervals between repeated text queries, global positioning signal (GPS) data indicating a location, speed, or movement pattern associated with the user 202 , prior text queries submitted to the TTS engine 210 within a particular time period, or other types of background information that can indicate user activity related to the TTS engine 210 .
- the context data 206 can indicate a type of query 204 submitted to the TTS engine 210 , such as whether the query 204 is a text segment associated with a user action, or an instruction transmitted to the TTS engine 210 to generate a text-to-speech output.
- the query analyzer 211 parses the query 204 to identify information that is responsive to the query 204 .
- the query analyzer 211 initially generates a transcription of the voice query, and then processes individual words or segments within the query 204 to determine information that is responsive to the query 204 , for example, by providing the query to a search engine and receive search results.
- the transcription of the query and the identified information can then be transmitted to the linguistics analyzer 214 .
- the language proficiency estimator 212 after receiving the context data 206 , the language proficiency estimator 212 computes a language proficiency for the user 202 based on the received context data 206 using techniques described with respect to FIG. 1 .
- the language proficiency estimator 212 parses through various context profiles 232 stored on the repository 220 .
- the context profile 232 can be an archived library including related types of information that are associated with a particular user context and can be included within a text-to-speech output.
- the context profile 232 additionally specifies a value, associated with each type of information, which represents an extent to which each type of information is likely to be understood by the user 202 when the user 202 is presently within a context associated with the context profile 232 .
- the context profile 232 specifies that the user 202 is presently in a context indicating that the user 202 is on his/her daily commute to work.
- the context profile 232 also specifies values for individual words and phrases that are likely to be comprehended by the user 202 . For instance, data or time information is associated with a value of “0.9” for “SINCE,” indicating that the user 202 is more likely to understand generalized information associated with a meeting (e.g., time of the next upcoming meeting) 204 rather than detailed information associated with a meeting (e.g., a party attending the meeting, or location of the meeting.
- the differences of the values indicate differences in the user's ability to understand particular types of information because the user's ability to understand complex or detailed information is diminished.
- the value associated with individual words and phrases can determined based on user activity data from previous user sessions where the user 202 was previously in the context indicated by the context data 206 . For instance, historical user data can be transmitted from the user history manager 230 , which retrieves data stored within the query logs 234 . In the example, the value for date and time information can be increased based on determining that the user commonly accesses date and time information associated with meetings more frequently than locations of the meetings.
- the language proficiency estimator 212 After the language proficiency estimator 212 selects a particular context profile 232 that corresponds to the received context data 206 , the language proficiency estimator 212 transmits the selected context profile 232 to the interpolator 213 .
- the interpolator 213 parses the selected context profile 232 , and extracts individual words and phrases included and their associated values. In some instances, the interpolator 213 transmits the different types of information and associated values directly to the linguistics analyzer 214 for generating a list of text-to-speech output candidates 240 a . In such instances, the interpolator 213 extracts specific types of information and associated values from the selected context profile 232 and transmits them to the linguistics analyzer 214 . In other instances, the interpolator 213 can also transmit the selected context profile 232 to the re-ranker 215 .
- the TTS engine 210 can be provided a set of structured data (e.g., fields of a calendar event).
- the interpolator 213 can convert the structured data to text at a level that matches the user's proficiency indicated by the context profile 232 .
- the TTS engine 210 may access data indicating one or more grammars indicating different levels of detail or complexity to express the information in the structured data, and select an appropriate grammar based on the user's language proficiency score.
- the TTS engine 210 can use dictionaries to select words that are appropriate given the language proficiency score.
- the linguistics analyzer 214 performs processing operations such as normalization on the information included within the query 204 .
- the query analyzer 211 can assign phonetic transcriptions to each word or snippet included within the query 204 , and divide the query 204 into prosodic units such as phrases, clauses, and sentences using a text-to-phenome conversion.
- the linguistics analyzer 214 also generates a list 240 a that includes multiple text-to-speech output candidates that are identified as being responsive to the query 204 .
- the list 240 a includes multiple text-to-speech output candidates with different levels of complexity. For example, the response “At 12:00 PM with Mr.
- John near Dupont Circle is the most complex response because it identifies a time for the meeting, a location for the meeting, an individual with whom the meeting will take place. In comparison, the response “In three hours” is the least complex because it only identifies a time for the meeting.
- the list 240 a also includes a baseline rank for the text-to-speech candidates based on the likelihood that each text-to-speech output candidate is likely to be responsive to the query 204 .
- the list 240 a indicates that most complex text-to-speech output candidate is the most likely to be responsive to the query 204 because it includes the greatest amount of information that is associated with the content of the query 204 .
- the re-ranker 215 After the linguistics analyzer generates the list 240 a of text-to-speech output candidates, the re-ranker 215 generates a list 240 b , which includes an adjusted rank for the text-to-speech output candidates based on the received context data 206 . For instance, the re-ranker 215 can adjust the rank based on the scores associated with particular types of information included within the selected context profile 232 .
- the re-ranker 215 ranks the simplest text-to-speech output as the highest based on the context profile 232 indicating that the user 202 is likely to comprehend date and time information within a text-to-speech response but not likely to understand party names or location information within the text-to-speech response given the present context of the user indicating that the user is commuting to work.
- the received context data 206 can be used to adjust the selection of a particular text-to-speech output candidate that to increase the likelihood that the user 202 will understand the contents of the text-to-speech output 204 c of the TTS engine 210 .
- FIG. 3 is a diagram that illustrates an example of a system 300 for modifying sentence structure within a text-to-speech output.
- a TTS engine 310 receives a query 302 and a language proficiency profile 304 for a user (e.g., the user 202 ). The TTS engine 310 then perform operations 312 , 314 , and 316 to generate an adjusted text-to-speech output 302 c that is responsive to the query 302 .
- the TTS engine 310 corresponds to the TTS engine 120 described with respect to FIG. 1 , or the TTS engine 210 described with respect to FIG. 2 .
- the TTS engine 310 can modify the sentence structure of a baseline text-to-speech output 306 a for the query 302 using different types of adjustment techniques.
- the TTS engine 310 can substitute words or phrases within the baseline text-to-speech output 306 a based on determining that a complexity score associated with individual words or phrases is greater than a threshold score indicated by the language complexity profile 304 of a user.
- the TTS engine 310 can rearrange individual sentence clauses such that the overall complexity of baseline text-to-speech output 306 a is reduced to a satisfactory level based on the language complexity profile 304 .
- the TTS engine 310 can also re-order words, split or combine sentences, and make other changes to adjust the complexity of text.
- the TTS engine 310 initially generates a baseline text-to-speech output 306 a that is responsive to the query 302 .
- the TTS engine 310 parses the baseline text-to-speech output 306 a into segments 312 a - 312 c .
- the TTS engine 310 also detects punctuation marks (e.g., commas, periods, semicolons, etc.) that indicate breakpoints between individual segments.
- the TTS engine 310 also computes a complexity score for each of the segments 312 a - 312 c . In some instances, the complexity score can be computed based on the frequency of a particular word within a particular language.
- Alternative techniques can include computing the complexity score based on the frequency of use by the user, or frequency of occurrence in historical content accessed by the user (e.g., news articles, webpages, etc.).
- the complexity score can be used to indicate words that are likely to be comprehended by the user and other words that are unlikely to be comprehended by the user.
- segments 312 a and 312 b are determined to be relatively complex based on the inclusion of high complex terms such as “FORECAST” and “CONSISTENT,” respectively.
- the segment 312 c is determined to be relatively simple because the terms included are relatively simple. This determination is represented by the segments 312 a and 312 b having higher complexity scores (e.g., 0.83, 0.75) compared to the complexity score for the segment 312 c (e.g., 0.41).
- the language proficiency profile 304 can be used to compute a threshold complexity score that indicates the maximal complexity that is comprehendible by the user.
- the threshold complexity score can be computed to be “0.7” such that the TTS 310 determines that the segments 312 a and 312 b are unlikely to be comprehended by the user.
- the TTS engine 310 substitutes the identified words with alternates that are predicted to be more likely to be understood by the user. As depicted in FIG. 3 , “FORECAST” can be substituted with “WEATHER,” and “CONSISTENT” can be substituted with “CHANGE.” In these examples, segments 314 an and 314 b represent simpler alternatives with lower complexity scores below the threshold complexity score indicated by the language proficiency profile 304 .
- TTS engine 310 can process word substitutions for high complexity words using a trained skip-gram model that uses unsupervised techniques to determine appropriately complex words to replace highly complex words. In some instances, the TTS engine 310 can also use thesaurus or synonym data to process word substitutions for high complex words.
- sentence clauses of a query can be adjusted based on computing complexities associated with particular sentence structures and determining whether is the user will be able to understand the sentence structure based on a language proficiency indicated by the language proficiency profile 304 .
- the TTS engine 310 determines that the baseline text-to-speech response 306 a has a high sentence complexity based on determining that the baseline text-to-speech response 306 a includes three sentence clauses (e.g., “today's forecast is sunny,” “but not consistent,” and “and warm”).
- the TTS engine 310 can generate adjusted sentence portions 316 a and 316 b , which combine a dependent clause and an independent clause into a single clause that does not include a segmenting punctuation mark.
- the adjusted text-to-speech response 306 b includes both simpler vocabulary (e.g., “WEATHER,” “CHANGE”) as well as a simpler sentence structure (e.g., no clause separations), increasing the likelihood that the user will understand the adjusted text-to-speech output 306 b .
- the adjusted text-to-speech output 306 b is then generated for output by the TTS engine 310 as the output 306 c.
- the TTS engine 310 can perform sentence structure adjustment based on using a user-specific restructuring algorithm that include adjusts the baseline query 302 a using weighting factors to avoid particular sentence structures that are identified to be problematic for the user.
- the user-specific restructuring algorithm can specify an option to down-weights the inclusion of subordinate clauses or up-weights sentence clauses that have simple subject verb object sequences.
- FIG. 4 is a block diagram that illustrates an example of a system 400 that adaptively generates text-to-speech outputs based on using clustering techniques.
- the system 400 includes a language proficiency estimator 410 , a user similarity determiner 420 , a complexity optimizer, and a machine learning system 400 .
- the language proficiency estimator 410 receives data from a plurality of users 402 .
- the language proficiency estimator 410 estimates a set of language complexity profiles 412 for each of the plurality of users 402 , which is then sent to the user similarity determiner 420 .
- the user similarity determiner 420 identifies user clusters 424 of similar users.
- the complexity optimizer 430 and the machine learning system 440 then analyzes the language complexity profiles 412 of each user within the user clusters 424 and the context data received from the plurality of users 402 in order to generate a complexity mapping 442 .
- the system 400 can be used to analyze relationships between active language complexity and passive language complexity for a population of users.
- Active language complexity refers to detected language input provided by the user (e.g., text queries, voice input, etc.).
- Passive language complexity refers to a user's ability to understand or comprehend speech signals that are provided to the user.
- the system 400 can use the determined relationship between the active language complexity and the passive language complexity for multiple users to determine an appropriate passive language complexity for each individual user where the particular user has the highest likelihood of understanding a text-to-speech output.
- the plurality of users 402 can be multiple users that use an application associated with a TTS engine (e.g., the TTS engine 120 ).
- the plurality of users 402 can be a set of users that use a mobile application that utilizes a TTS engine to provide users with text-to-speech features over a user interface of the mobile application.
- data from the plurality of users 402 e.g., prior user queries, user selections, etc.
- the language proficiency estimator 410 can initially measure passive language complexities for the plurality of users 402 using substantially similar techniques as those described previously with respect to FIG. 1 .
- the language proficiency estimator 410 can then generate the language complexity profiles 412 , which includes an individual language complexity profile for each of the plurality of users 402 .
- Each individual language complexity profile includes data indicating the passive language complexity and the active language complexity for each of the plurality of users 402 .
- the user similarity determiner 420 uses the language complexity data included within the set of language proficiency profiles 412 to identify similar users within the plurality of users 402 .
- the user similarity determiner 420 can group users that have similar active language complexities (e.g., similar language inputs, speech queries provided, etc.).
- the user similarity determiner 420 can determine similar users by comparing words included in prior user-submitted queries, particular user behaviors on a mobile application, or user locations. The user similarity determiner 420 then clusters the similar users to generate the user clusters 424 .
- the user similarity determiner 420 generates the user clusters 424 based stored on cluster data 422 that include aggregate data for users in specified clusters.
- the cluster data 422 can be grouped by specific parameters (e.g. number of incorrect query responses, etc.) that indicate a passive language complexity associated with the plurality of users 402 .
- the complexity optimizer 430 varies the complexity of the language output by a TTS system and measures a user's passive language complexity using a set of parameters that indicate a user's ability to understand language output by the TTS system (e.g., understanding rate, voice action flow completion rate, or answer success rate) to indicate user performance. For instance, the parameters can be used to characterize how well users within each cluster 424 understand a given text-to-speech output. In such instances, the complexity optimizer 430 can initially provide a low complexity speech signal to the user and recursively provide additional speech signals within a range of complexities.
- the complexity optimizer 430 can also determine the optimal passive language complexity for various user contexts associated with each user cluster 424 . For instance, after measuring the user's language proficiency using the set of parameters, the complexity optimizer 430 can then classify the measured data by context data received from the plurality of users 402 such that an optimal passive language complexity can be determined for each user context.
- the machine learning system 440 After gathering performance data for the range of passive language complexities, the machine learning system 440 then determines a particular passive language complexity where the performance parameters indicate that the user's language comprehension is the strongest. For instance, the machine learning system 440 aggregates the performance data all users within a particular user cluster 424 to determine relationships between the active language complexity, the passive language complexity, and the user context.
- the aggregate data for the user cluster 424 can then compared to individual data for each user within the user cluster 424 to determine an actual language complexity score for each user within the user cluster 424 .
- the complexity mapping 442 can represent the relationship between active language complexity and passive language complexity to infer the actual language complexity, which corresponds to the active language complexity mapped to the optimal passive language complexity.
- the complexity mapping 442 represents relationships between active language complexity, TTS complexity, and passive language complexity for all user clusters within the plurality of user 402 , which can then be used to predict the appropriate TTS complexity for a subsequent query by an individual user.
- user inputs e.g., queries, text messages, e-mails, etc.
- user clusters 424 For each cluster, the system provides TTS outputs requiring varying levels of language proficiency to understand. The system then assesses the responses received from users, and the rate of task completion for the varied TTS outputs, to determine a level of language complexity that is appropriate for the users in each cluster.
- the system stores a mapping 442 between cluster identifiers and TTS complexity scores corresponding to the identified clusters.
- the system uses the complexity mapping 442 to determine an appropriate level of complexity for a TTS output for a user. For example, the system identifies a cluster that represents a user's active language proficiency, looks up a corresponding TTS complexity score (e.g., indicating a level of passive language understanding) for the cluster in the mapping 442 , and generates a TTS output having a complexity level indicated by the retrieved TTS complexity score.
- a corresponding TTS complexity score e.g., indicating a level of passive language understanding
- the actual language complexity determined for a user can then be used to adjust the TTS system using techniques described with respect to FIGS. 1-3 .
- aggregate language complexity data from a group of similar users e.g., the user cluster 424
- FIG. 5 is a flow diagram that illustrates an example of a process 500 for adaptively generating text-to-speech output.
- the process 500 can include determining a language proficiency of a user of a client device ( 510 ), determining a text segment for output by a text-to-speech module ( 520 ), generating audio data including a synthesized utterance of the text segment ( 530 ), and providing the audio data to the client device ( 540 ).
- the process 500 can include determining a language proficiency of a user of a client device ( 510 ).
- the language proficiency estimator 110 can determine a language proficiency for a user using a variety of techniques.
- the language proficiency can represent an assigned score that indicates a level of language proficiency.
- the language proficiency can represent an assigned category from a plurality of categories of language proficiency.
- the language proficiency can be determined based on user input and/or behaviors indicating a proficiency level of the user.
- the language proficiency can be inferred from different user signals. For instance, as described with respect to FIG. 1 , language proficiency can be inferred from vocabulary complexity of user inputs, data entry rate of the user, a number of misrecognized words from a speech input, a number of completed voice actions for different levels of TTS complexity, or a level of complexity of texts viewed by the user (e.g., books, articles, text on webpages, etc.).
- the process 500 can include determining a text segment for output by a text-to-speech module ( 520 ).
- a TTS engine can adjust a baseline text segment based on the determine language proficiency of the user.
- the text segment for output can be adjusted based on a user context associated the with user.
- the text segment for output can also be adjusted by word substitution or sentence restructuring in order to reduce the complexity of the text segment.
- the adjustment can be based on how rare individual words included in the text segments, the type of verbs used (e.g., compound verbs, or verb tense), the linguistic structure of the text segment (e.g., number of subordinate clauses, amount of separation between related words, degree the that phrases are nested, etc.
- the adjustment can also be based on linguistic measures above with reference measurements for linguistic characteristics (e.g., average separation between subjects and verbs, separation between adjectives and nouns, etc.).
- reference measurements can represent averages, or could include ranges or examples for different complexity levels.
- determining the text segment for output can include selecting text segments that have scores that best match reference scores that describe a language proficiency level of the user.
- individual words or phrases can be scored for complexity, and then the most complex words can be substituted, deleted, or restructured such that overall complexity meets an appropriate level for the user.
- the process 500 can include generating audio data including a synthesized utterance of the text segment ( 530 ).
- the process 500 can include providing the audio data to the client device ( 540 ).
- FIG. 6 is a block diagram of computing devices 600 , 650 that can be used to implement the systems and methods described in this document, as either a client or as a server or plurality of servers.
- Computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 650 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices.
- computing device 600 or 650 can include Universal Serial Bus (USB) flash drives.
- USB flash drives can store operating systems and other applications.
- the USB flash drives can include input/output components, such as a wireless transmitter or USB connector that can be inserted into a USB port of another computing device.
- input/output components such as a wireless transmitter or USB connector that can be inserted into a USB port of another computing device.
- Computing device 600 includes a processor 602 , memory 604 , a storage device 606 , a high-speed interface 608 connecting to memory 604 and high-speed expansion ports 610 , and a low speed interface 612 connecting to low speed bus 614 and storage device 606 .
- Each of the components 602 , 604 , 606 , 608 , 610 , and 612 are interconnected using various busses, and can be mounted on a common motherboard or in other manners as appropriate.
- the processor 602 can process instructions for execution within the computing device 600 , including instructions stored in the memory 604 or on the storage device 606 to display graphical information for a GUI on an external input/output device, such as display 616 coupled to high speed interface 608 .
- multiple processors and/or multiple buses can be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 600 can be connected, with each device providing portions of the necessary operations, e.g., as a server bank, a group of blade servers, or a multi-processor system.
- the memory 604 stores information within the computing device 600 .
- the memory 604 is a volatile memory unit or units.
- the memory 604 is a non-volatile memory unit or units.
- the memory 604 can also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 606 is capable of providing mass storage for the computing device 600 .
- the storage device 606 can be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product can also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 604 , the storage device 606 , or memory on processor 602 .
- the high speed controller 608 manages bandwidth-intensive operations for the computing device 600 , while the low speed controller 612 manages lower bandwidth intensive operations. Such allocation of functions is exemplary only.
- the high-speed controller 608 is coupled to memory 604 , display 616 , e.g., through a graphics processor or accelerator, and to high-speed expansion ports 610 , which can accept various expansion cards (not shown).
- low-speed controller 612 is coupled to storage device 606 and low-speed expansion port 614 .
- the low-speed expansion port which can include various communication ports, e.g., USB, Bluetooth, Ethernet, wireless Ethernet can be coupled to one or more input/output devices, such as a keyboard, a pointing device, microphone/speaker pair, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 600 can be implemented in a number of different forms, as shown in the figure. For example, it can be implemented as a standard server 620 , or multiple times in a group of such servers. It can also be implemented as part of a rack server system 624 . In addition, it can be implemented in a personal computer such as a laptop computer 622 .
- components from computing device 600 can be combined with other components in a mobile device (not shown), such as device 650 .
- a mobile device not shown
- Each of such devices can contain one or more of computing device 600 , 650 , and an entire system can be made up of multiple computing devices 600 , 650 communicating with each other.
- the computing device 600 can be implemented in a number of different forms, as shown in the figure. For example, it can be implemented as a standard server 620 , or multiple times in a group of such servers. It can also be implemented as part of a rack server system 624 . In addition, it can be implemented in a personal computer such as a laptop computer 622 . Alternatively, components from computing device 600 can be combined with other components in a mobile device (not shown), such as device 650 . Each of such devices can contain one or more of computing device 600 , 650 , and an entire system can be made up of multiple computing devices 600 , 650 communicating with each other.
- Computing device 650 includes a processor 652 , memory 664 , and an input/output device such as a display 654 , a communication interface 666 , and a transceiver 668 , among other components.
- the device 650 can also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 650 , 652 , 664 , 654 , 666 , and 668 are interconnected using various buses, and several of the components can be mounted on a common motherboard or in other manners as appropriate.
- the processor 652 can execute instructions within the computing device 650 , including instructions stored in the memory 664 .
- the processor can be implemented as a chipset of chips that include separate and multiple analog and digital processors. Additionally, the processor can be implemented using any of a number of architectures.
- the processor 610 can be a CISC (Complex Instruction Set Computers) processor, a RISC (Reduced Instruction Set Computer) processor, or a MISC (Minimal Instruction Set Computer) processor.
- the processor can provide, for example, for coordination of the other components of the device 650 , such as control of user interfaces, applications run by device 650 , and wireless communication by device 650 .
- Processor 652 can communicate with a user through control interface 658 and display interface 656 coupled to a display 654 .
- the display 654 can be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 656 can include appropriate circuitry for driving the display 654 to present graphical and other information to a user.
- the control interface 658 can receive commands from a user and convert them for submission to the processor 652 .
- an external interface 662 can be provide in communication with processor 652 , so as to enable near area communication of device 650 with other devices. External interface 662 can provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces can also be used.
- the memory 664 stores information within the computing device 650 .
- the memory 664 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 674 can also be provided and connected to device 650 through expansion interface 672 , which can include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 674 can provide extra storage space for device 650 , or can also store applications or other information for device 650 .
- expansion memory 674 can include instructions to carry out or supplement the processes described above, and can include secure information also.
- expansion memory 674 can be provide as a security module for device 650 , and can be programmed with instructions that permit secure use of device 650 .
- secure applications can be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory can include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 664 , expansion memory 674 , or memory on processor 652 that can be received, for example, over transceiver 668 or external interface 662 .
- Device 650 can communicate wirelessly through communication interface 666 , which can include digital signal processing circuitry where necessary. Communication interface 666 can provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication can occur, for example, through radio-frequency transceiver 668 . In addition, short-range communication can occur, such as using a Bluetooth, Wi-Fi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 670 can provide additional navigation- and location-related wireless data to device 650 , which can be used as appropriate by applications running on device 650 .
- GPS Global Positioning System
- Device 650 can also communicate audibly using audio codec 660 , which can receive spoken information from a user and convert it to usable digital information. Audio codec 660 can likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 650 . Such sound can include sound from voice telephone calls, can include recorded sound, e.g., voice messages, music files, etc. and can also include sound generated by applications operating on device 650 .
- Audio codec 660 can receive spoken information from a user and convert it to usable digital information. Audio codec 660 can likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 650 . Such sound can include sound from voice telephone calls, can include recorded sound, e.g., voice messages, music files, etc. and can also include sound generated by applications operating on device 650 .
- the computing device 650 can be implemented in a number of different forms, as shown in the figure. For example, it can be implemented as a cellular telephone 480 . It can also be implemented as part of a smartphone 682 , personal digital assistant, or other similar mobile device.
- implementations of the systems and methods described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations of such implementations.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which can be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball by which the user can provide input to the computer.
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here, or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
- This application is a continuation of U.S. patent application Ser. No. 15/477,360, filed Apr. 3, 2017, which is a continuation of U.S. patent application Ser. No. 15/009,432, filed Jan. 28, 2016, the contents of which are hereby incorporated by reference.
- This specification generally describes electronic communications.
- Speech synthesis refers to the artificial production of human speech. Speech synthesizers can be implemented in software or hardware components to generate speech output corresponding to a text. For instance, a text-to-speech (TTS) system typically converts normal language text into speech by concatenating pieces of recorded speech that are stored in a database.
- Speech synthesis has become more central to user experience as a greater portion of electronic computing has shifted from desktop to mobile environments. For example, increases in the use of smaller mobile devices without displays have led to increases in the use of text-to-speech systems for accessing and using content that is displayed on mobile devices.
- One particular issue with existing TTS systems is that such systems are often unable to adapt to varying language proficiencies of different users. This lack of flexibility often prevents users with limited language proficiencies from understanding complex text-to-speech outputs. For instance, non-native language speakers that use a TTS system can have difficulty understanding a text-to-speech output because of their limited language familiarity. Another issue with existing TTS systems is that a user's instantaneous ability to understand text-to-speech outputs can also vary based on a particular user context. For instance, some user contexts include background noise that can make it more difficult to understand longer or more complex text-to-speech outputs.
- In some implementations, a system adjusts the text used for a text-to-speech output based on the language proficiency of a user to increase a likelihood that the user can comprehend the text-to-speech output. For instance, the language proficiency of a user can be inferred from prior user activity and be used to adjust the text-to-speech output to an appropriate complexity that is commensurate with the language proficiency of the user. In some examples, a system obtains multiple candidate text segments that correspond to different levels of language proficiency. The system then selects the candidate text segment that best matches and most closely corresponds to a user's language proficiency and provides a synthesized utterance of the selected text segment for output to the user. In other examples, a system alters the text in a text segment to better correspond to the user's language proficiency prior to generating a text-to-speech output. Various aspects of a text segment can be adjusted, including the vocabulary, sentence structure, length, and so on. The system then provides a synthesized utterance of the altered text segment for output to the user.
- For situations in which the systems discussed here collect personal information about users, or may make use of personal information, the users may be provided with an opportunity to control whether programs or features collect personal information, e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location, or to control whether and/or how to receive content from the content server that may be more relevant to the user. In addition, certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user's identity may be anonymized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained, such as to a city, zip code, or state level, so that a particular location of a user cannot be determined. Thus, the user may have control over how information is collected about him or her and used by a content server.
- In one aspect, a computer-implemented method can include: determining, by the one or more computers, a language proficiency of a user of a client device; determining, by the one or more computers, a text segment for output by a text-to-speech module based on the determined language proficiency of the user; generating, by the one or more computers, audio data including a synthesized utterance of the text segment; and providing, by the one or more computers and to the client device, the audio data including the synthesized utterance of the text segment.
- Other versions include corresponding systems, and computer programs, configured to perform the actions of the methods encoded on computer storage devices.
- One or more implementations can include the following optional features. For example, in some implementations, the client device displays a mobile application that uses a text-to-speech interface.
- In some implementations, determining the language proficiency of the user includes inferring a language proficiency of the user based at least on previous queries submitted by the user.
- In some implementations, determining the text segment for output by the text-to-speech module includes: identifying multiple text segments as candidates for a text-to-speech output of the user, the multiple text segments having different levels of language complexity; and selecting from among the multiple text segments based at least on the determined language proficiency of the user of the client device.
- In some implementations, selecting from among the multiple text segments includes: determining a language complexity score for each of the multiple text segments; and selecting the text segment having the language complexity score that best matches a reference score that describes the language proficiency of the user of the client device.
- In some implementations, determining the text segment for output by the text-to-speech module includes: identifying a text segment for a text-to-speech output to the user; computing a complexity score of the text segment for the text-to-speech output; and modifying the text segment for the text-to-speech output to the user based at least on the determined language proficiency of the user and the complexity score of text segment for the text-to-speech output.
- In some implementations, modifying the text segment for the text-to-speech output to the user includes: determining an overall complexity score for the user based at least on the determining language proficiency of the user; determining a complexity score for individual portions within the text segment for the text-to-speech output to the user; identifying one or more individual portions within the text segment with complexity scores greater than the overall complexity score for the user; and modifying the one or more individual portions within the text segment to reduce complexity scores below the overall complexity score.
- In some implementations, modifying the text segment for the text-to-speech output to the user includes: receiving data indicating a context associated with the user; determining an overall complexity score for the context associated with the user; determining that the complexity score of the text segment exceeds the overall complexity score for the context associated with the user; and modifying the text segment to reduce the complexity score below the overall complexity score for the context associated with the user.
- In another general aspect, a computer-implemented method includes: receiving data indicating a context associated with the user; determining an overall complexity score for the context associated with the user; identifying a text segment for a text-to-speech output to the user; determining that the complexity score of the text segment exceeds the overall complexity score for the context associated with the user; and modifying the text segment to reduce the complexity score below the overall complexity score for the context associated with the user.
- In some implementations, determining the overall complexity score for the context associated with the user includes: identifying terms included within previously submitted queries by the user when the user was determined to be in the context; and determining an overall determining an overall complexity score for the context associated with the user based at least on the identified terms.
- In some implementations, the data indicating the context associated with the user includes queries that were previously submitted by the user.
- In some implementations, the data indicating the context associated with the user includes a GPS signal indicating a current location associated with the user.
- The details of one or more implementations are set forth in the accompanying drawings and the description below. Other potential features and advantages will become apparent from the description, the drawings, and the claims.
- Other implementations of these aspects include corresponding systems, apparatus and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
-
FIG. 1 is a diagram that illustrates examples of processes for generating text-to-speech outputs based on language proficiency. -
FIG. 2 is a diagram that illustrates an example of a system for generating an adaptive text-to-speech output based on a user context. -
FIG. 3 is a diagram that illustrates an example of a system for modifying a sentence structure within a text-to-speech output. -
FIG. 4 is a block diagram that illustrates an example of a system for generating adaptive text-to-speech outputs based on using clustering techniques. -
FIG. 5 is a flow diagram that illustrates an example of a process for generating adaptive text-to-speech outputs. -
FIG. 6 is a block diagram of computing devices on which the processes described herein, or portions thereof, can be implemented. - In the drawings, like reference numbers represent corresponding parts throughout.
-
FIG. 1 is a diagram that illustrates examples ofprocesses processes 100A and 1006 are used to generate different text-to-speech outputs for auser 102 a with high language proficiency and a user 102 b with low language proficiency, respectively, for atext query 104. As depicted, after receiving aquery 104 on theuser devices process 100A generates a high-complexity text-to-speech output 108 a for theuser 102 a whereas theprocess 100B generates a low-complexity output 108 b for the user 102 b. In addition, the TTS systems that executeprocesses 100A and 1006 can include alanguage proficiency estimator 110, a text-to-speech engine 120. In addition, the text-to-speech engine 120 can further include atext analyzer 122, alinguistics analyzer 124, and awaveform generator 126. - In general, the content of text that is used to generate a text-to-speech output can be determined according to a language proficiency of a user. In addition, or as an alternative, the text to be used to generate a text-to-speech output can be determined based on a context of the user, for example, the location or activity of a user, background noise present, a current task of the user and so on. Further, the text to be converted to an audible form may be adjusted or determined using other information, such as indications that a user has failed to complete a task or is repeating an action.
- In the example, two users,
user 102 a and user 102 b, provide thesame query 104 onuser devices query 104 can be a voice query sent to theuser devices query 104 is then transmitted to the text-to-speech engine 120 to generate a text-to-speech output in response to thequery 104. - The
language proficiency estimator 110 can be a software module within a TTS system that determines a language proficiency score associated with a particular user (e.g., theuser 102 a or the user 102 b) based onuser data 108 a. The language proficiency score can be an estimate of the user's ability to understand communications in a particular language, in particular, to understand speech in the particular language. One measure of language proficiency is the ability of a user to successfully complete a voice-controlled task. Many types of tasks, such as setting a calendar appointment, looking up directions, and so on, follow a sequence of interactions in which a user and device exchange verbal communication. The rate at which a user successfully completes these task workflows through a voice interface is a strong indicator of the user's language proficiency. For example, a user that completes nine out of ten voice tasks that the user initiates likely has a high language proficiency. On the other hand, a user that fails to complete the majority of voice tasks that the user initiates can be inferred to have a low language proficiency, since the user may not have fully understood the communications from the device or may not have been able to provide appropriate verbal responses. As discussed further below, when a user does not complete workflows that include standard TTS outputs, resulting in a low language proficiency score, the TTS may use adapted, simplified outputs that may increase the ability of the user to understand and complete various tasks. - As shown, the
user data 108 a can include words used within prior text queries submitted by the user, an indication whether English, or any other language utilized by the TTS system, is the native language of the user, and a set of activities and/or behaviors that are reflective of a user's language comprehension skills. For example, as depicted inFIG. 1 , a typing speed of the user can be used to determine language fluency of the user in a language. In addition, a language vocabulary complexity score or language proficiency score can be assigned to the user based on associating a pre-determined complexity to words that were used by the user in previous text queries. In another example, the number of misrecognized words in prior queries can also be used to determine the language proficiency score. For instance, a high number of misrecognized words can be used to indicate a low language proficiency. In some implementations, the language proficiency score is determined by looking up a stored score associated with the user, which was determined for the user prior to submission of thequery 104. - Although
FIG. 1 depicts thelanguage proficiency estimator 110 as a separate component to theTTS engine 120, in some implementations, as depicted inFIG. 2 , thelanguage proficiency estimator 110 can be an integrated software module within theTTS engine 120. In such instances, operations involving the language proficiency estimation can be directly modulated by theTTS engine 120. - In some implementations, the language proficiency score assigned to the user may be based on a particular user context estimated for the user. For instance, as described more particularly with respect to
FIG. 2 , a user context determination can be used to determine context-specific language proficiencies that can cause a user to temporarily have limited language comprehension abilities. For example, if the user context indicates significant background noise or if the user is engaged in a task such as driving, the language proficiency score can be used to indicate that the user's present language comprehension ability is temporarily diminished relative to other user contexts. - In some implementations, instead of inferring language proficiency based on previous user activity, the language proficiency score can instead be directly provided to the
TTS engine 120 without the use of thelanguage proficiency estimator 110. For instance, a language proficiency score can be designated to a user based on user input during a registration process that specifies a user's level of language proficiency. For example, during the registration, the user can provide a selection that specifies the user's skill level, which can then be used to calculate the appropriate language proficiency for the user. In other examples, the user can provide other types of information such as demographic information, education level, places of residences, etc., that can be used to specify the user's level of language proficiency. - In the examples described above, the language proficiency score can either be a set of discrete values that are adjusted periodically based on recently generated user activity data, or a continuous score that is initially designated during a registration process. In the first instance, the value of the language proficiency score can be biased based on one or more factors that indicate that a user's present language comprehension and proficiency may be attenuated (e.g., a user context indicating significant background noise). In the second instance, the value of the language proficiency score can be preset after an initial calculation and adjusted only after specific milestone events that indicate that a user's language proficiency has increased (e.g., an increase in typing rate or a decrease in correction rate for a given language). In other instances, a combination of these two techniques can be used to variably adjust the text-to-speech output based on a particular text input. In such instances, multiple language proficiency scores that each represent a particular aspect of the user's language skills can be used to determine to how best adjust the text-to-speech output for the user. For example, one language proficiency score can represent a complexity of the user's vocabulary whereas another language proficiency score can be used to represent the user's grammar skills.
- The
TTS engine 120 can use the language proficiency score to generate a text-to-speech output that is adapted to the language proficiency indicated by the user's language proficiency score. In some instances, theTTS engine 120 adapts the text-to-speech output based on selecting a particular TTS string from a set of candidate TTS strings for thetext query 104. In such instances, theTTS engine 120 selects the particular TTS string based on using the language proficiency score of the user to predict a likelihood that each of the candidate TTS strings will accurately be interpreted by the user. More particular descriptions related to these techniques are provided with respect toFIG. 2 . Alternatively, in other instances, theTTS engine 120 can select a baseline TTS string and adjust the structure of the TTS string based on the user's level of language proficiency indicated by the language proficiency score. In such instances, theTTS engine 120 can adjust the grammar of the baseline TTS string, provide word substitutions and/or reduce the sentence complexity to generate an adapted TTS string that is more likely to be understood by the user. More particular descriptions related to these techniques are provided with respect toFIG. 3 . - Referring still to
FIG. 1 , theTTS engine 120 may generate different text-to-speech outputs forusers 102 a and 102 b because the language proficiency scores for the users are different. For example, inprocess 100A, thelanguage proficiency score 106 a indicates high English-language proficiency, inferred from theuser data 108 a indicating that theuser 102 a has a complex vocabulary, has English as a first language, and has a relatively high word per minute in prior user queries. Based on the value of thelanguage proficiency score 106 a, theTTS engine 120 generates a high complexity text-to-speech output 108 a that includes a complex grammatical structure. As depicted, the text-to-speech output 108 a includes an independent clause that describes that today's forecast is sunny, in addition to a subordinate clause that includes additional information about the high temperature and the low temperature of the day. - In the example of
process 100B, thelanguage proficiency score 106 b indicates low English-language proficiency, inferred fromuser activity data 108 b indicating that the user 102 b has a simple vocabulary, has English as a second language, and has previously provided ten incorrect queries. In this example, theTTS engine 120 generates a low complexity text-to-speech output 108 b that includes a simpler grammatical structure relative to the text-to-speech output 108 a. For instance, instead of including multiple clauses within a single sentence, the text-to-speech output 108 b includes a single independent clause that conveys the same primary information as the text-to-speech output 108 a (e.g., today's forecast being sunny), but does not include additional information related to the high and low temperatures for the day. - The adaptation of text for a TTS output can be performed by various different devices and software modules. For example, a TTS engine of a server system may include functionality to adjust text based on a language proficiency score and then output audio including a synthesized utterance of the adjusted text. As another example, a pre-processing module of a server system may adjust text and pass the adjusted text to a TTS engine for speech synthesis. As another example, a user device may include a TTS engine, or a TTS engine and a text pre-processor, to be able to generate appropriate TTS outputs.
- In some implementations, a TTS system can include software modules that are configured to exchange communications with a third-party mobile application of a client device or a web page. For instance, the TTS functionality of the system can be made available to a third-party mobile application through an application package interface (API). The API can include defined set of protocols that an application or web site can use to request TTS audio from a server system that runs the
TTS engine 120. In some implementations, the API can make available TTS functionality that runs locally on a user's device. For example, the API may be available to an application or web page through an inter-process communication (IPC), remote procedure call (RPC), or other system call or function. A TTS engine, and associated language proficiency analysis or text preprocessing, may be run locally on the user's device to determine an appropriate text for the user's language proficiency and generate the audio for the synthesized speech also. - For example, the third-party application or web page can use the API to generate a set of voice instructions that are provided to the user based on a task flow of a voice interface of the third-party application or web page. The API can specify that the application or web page should provide text to be converted to speech. In some instances, other information can be provided, such as a user identifier or a language proficiency score.
- In implementations where the
TTS engine 120 exchanges communications with a third-party application using an API, theTTS engine 120 can be used to determine whether a text segment from a third-party application should be adjusted prior to generating a text-to-speech output for the text. For example, the API can include computer-implemented protocols that specify conditions within the third-party application that initiate the generation of an adaptive text-to-speech output. - As an example, one API may permit an application to submit multiple different text segments as candidates for a TTS output, where the different text segments correspond to different levels of language proficiency. For example, the candidates can be text segments having equivalent meanings but different complexity levels (e.g., a high complexity response, a medium complexity response, and a low complexity response). The
TTS engine 120 may then determine the level of language proficiency needed to understand each candidate, determine an appropriate language proficiency score for the user, and select the candidate text that best corresponds to the language proficiency score. TheTTS engine 120 then provides synthesized audio for the selected text back to the application, e.g., over a network using the API. In some instances, the API can be locally available on theuser devices user devices TTS engine 120 since the API operates locally on theuser devices - In another example, an API can allow the third-party application to provide a single text segment and a value that indicates whether the
TTS engine 120 is permitted to modify the text segment to generate a text segment with a different complexity. If the app or web page indicates that alteration is permitted, theTTS system 120 may make various changes to the text, for example, to reduce the complexity of the text when the language proficiency score suggests that the original text is more complex than the user can understand in a spoken response. In yet other examples, an API allows the third-party application to also provide user data (e.g., prior user queries submitted on the third-party application) along with the text segment such that theTTS engine 120 can determine a user context associated with the user and adjust generate a particular text-to-speech output based on the determined user context. Similarly, an API can allow an application to provide context data from a user device (e.g., a global positioning signal, accelerometer data, ambient noise level, etc.) or an indication of a user context to allow theTTS engine 120 to adjust the text-to-speech outputs that will ultimately be provided to the user through the third-party application. In some instances, the third party application can also provide the API with data that can be used to determine a language proficiency of the user. - In some implementations, the
TTS engine 120 can adjust the text-to-speech output for a user query without using a language proficiency of the user or determining a context associated with the user. In such implementations,TTS engine 120 can determine that an initial text-to-speech output is too complex for a user based on receiving signals that a user has misunderstood the output (e.g., multiple retries on the same query or task). In response, theTTS engine 120 can reduce the complexity of a subsequent text-to-speech response for a retried query or related queries. Thus, when a user fails to successfully complete an action, theTTS engine 120 may progressively reduce the amount of detail or language proficiency required to understand the TTS output until it reaches a level that the user understands. -
FIG. 2 is a diagram that illustrates an example of asystem 200 that adaptively generates a text-to-speech output based on a user context. Briefly, thesystem 200 can include aTTS engine 210 that includes aquery analyzer 211, alanguage proficiency estimator 212, aninterpolator 213, alinguistics analyzer 214, a re-ranker 215, and awaveform generator 216. Thesystem 200 also includes acontext repository 220 that stores a set of context profiles 232, and a user history manager 230 that storesuser history data 234. In some instances, theTTS engine 210 corresponds to theTTS engine 120 as described with respect toFIG. 1 . - In the example, a
user 202 initially submits aquery 204 on auser device 208 that includes a request for information related to the user's first meeting for the day. Theuser device 208 can then transmit thequery 204 and context data 206 associated with theuser 202 to thequery analyzer 211 and thelanguage proficiency estimator 212, respectively. Other types of TTS outputs that are not responses to queries, e.g., calendar reminders, notifications, task workflows, etc., may be adapted using the same techniques. - The context data 206 can include information relating to a particular context associated with the
user 202 such as time intervals between repeated text queries, global positioning signal (GPS) data indicating a location, speed, or movement pattern associated with theuser 202, prior text queries submitted to theTTS engine 210 within a particular time period, or other types of background information that can indicate user activity related to theTTS engine 210. In some instances, the context data 206 can indicate a type ofquery 204 submitted to theTTS engine 210, such as whether thequery 204 is a text segment associated with a user action, or an instruction transmitted to theTTS engine 210 to generate a text-to-speech output. - After receiving the
query 204, thequery analyzer 211 parses thequery 204 to identify information that is responsive to thequery 204. For example, in some instances where thequery 204 is a voice query, thequery analyzer 211 initially generates a transcription of the voice query, and then processes individual words or segments within thequery 204 to determine information that is responsive to thequery 204, for example, by providing the query to a search engine and receive search results. The transcription of the query and the identified information can then be transmitted to thelinguistics analyzer 214. 204 - Referring to now to the
language proficiency estimator 212, after receiving the context data 206, thelanguage proficiency estimator 212 computes a language proficiency for theuser 202 based on the received context data 206 using techniques described with respect toFIG. 1 . In particular, thelanguage proficiency estimator 212 parses through various context profiles 232 stored on therepository 220. Thecontext profile 232 can be an archived library including related types of information that are associated with a particular user context and can be included within a text-to-speech output. Thecontext profile 232 additionally specifies a value, associated with each type of information, which represents an extent to which each type of information is likely to be understood by theuser 202 when theuser 202 is presently within a context associated with thecontext profile 232. - In the example depicted in
FIG. 2 , thecontext profile 232 specifies that theuser 202 is presently in a context indicating that theuser 202 is on his/her daily commute to work. In addition, thecontext profile 232 also specifies values for individual words and phrases that are likely to be comprehended by theuser 202. For instance, data or time information is associated with a value of “0.9” for “SINCE,” indicating that theuser 202 is more likely to understand generalized information associated with a meeting (e.g., time of the next upcoming meeting) 204 rather than detailed information associated with a meeting (e.g., a party attending the meeting, or location of the meeting. In this example, the differences of the values indicate differences in the user's ability to understand particular types of information because the user's ability to understand complex or detailed information is diminished. - The value associated with individual words and phrases can determined based on user activity data from previous user sessions where the
user 202 was previously in the context indicated by the context data 206. For instance, historical user data can be transmitted from the user history manager 230, which retrieves data stored within the query logs 234. In the example, the value for date and time information can be increased based on determining that the user commonly accesses date and time information associated with meetings more frequently than locations of the meetings. - After the
language proficiency estimator 212 selects aparticular context profile 232 that corresponds to the received context data 206, thelanguage proficiency estimator 212 transmits the selectedcontext profile 232 to theinterpolator 213. Theinterpolator 213 parses the selectedcontext profile 232, and extracts individual words and phrases included and their associated values. In some instances, theinterpolator 213 transmits the different types of information and associated values directly to thelinguistics analyzer 214 for generating a list of text-to-speech output candidates 240 a. In such instances, theinterpolator 213 extracts specific types of information and associated values from the selectedcontext profile 232 and transmits them to thelinguistics analyzer 214. In other instances, theinterpolator 213 can also transmit the selectedcontext profile 232 to the re-ranker 215. - In some instances, the
TTS engine 210 can be provided a set of structured data (e.g., fields of a calendar event). In such instances, theinterpolator 213 can convert the structured data to text at a level that matches the user's proficiency indicated by thecontext profile 232. For example, theTTS engine 210 may access data indicating one or more grammars indicating different levels of detail or complexity to express the information in the structured data, and select an appropriate grammar based on the user's language proficiency score. Similarly, theTTS engine 210 can use dictionaries to select words that are appropriate given the language proficiency score. - The
linguistics analyzer 214 performs processing operations such as normalization on the information included within thequery 204. For instance, thequery analyzer 211 can assign phonetic transcriptions to each word or snippet included within thequery 204, and divide thequery 204 into prosodic units such as phrases, clauses, and sentences using a text-to-phenome conversion. Thelinguistics analyzer 214 also generates alist 240 a that includes multiple text-to-speech output candidates that are identified as being responsive to thequery 204. In the example, thelist 240 a includes multiple text-to-speech output candidates with different levels of complexity. For example, the response “At 12:00 PM with Mr. John near Dupont Circle” is the most complex response because it identifies a time for the meeting, a location for the meeting, an individual with whom the meeting will take place. In comparison, the response “In three hours” is the least complex because it only identifies a time for the meeting. - The
list 240 a also includes a baseline rank for the text-to-speech candidates based on the likelihood that each text-to-speech output candidate is likely to be responsive to thequery 204. In the example, thelist 240 a indicates that most complex text-to-speech output candidate is the most likely to be responsive to thequery 204 because it includes the greatest amount of information that is associated with the content of thequery 204. - After the linguistics analyzer generates the
list 240 a of text-to-speech output candidates, the re-ranker 215 generates alist 240 b, which includes an adjusted rank for the text-to-speech output candidates based on the received context data 206. For instance, the re-ranker 215 can adjust the rank based on the scores associated with particular types of information included within the selectedcontext profile 232. - In the example, the re-ranker 215 ranks the simplest text-to-speech output as the highest based on the
context profile 232 indicating that theuser 202 is likely to comprehend date and time information within a text-to-speech response but not likely to understand party names or location information within the text-to-speech response given the present context of the user indicating that the user is commuting to work. In this regard, the received context data 206 can be used to adjust the selection of a particular text-to-speech output candidate that to increase the likelihood that theuser 202 will understand the contents of the text-to-speech output 204 c of theTTS engine 210. -
FIG. 3 is a diagram that illustrates an example of a system 300 for modifying sentence structure within a text-to-speech output. Briefly, aTTS engine 310 receives aquery 302 and alanguage proficiency profile 304 for a user (e.g., the user 202). TheTTS engine 310 then performoperations query 302. In some instances, theTTS engine 310 corresponds to theTTS engine 120 described with respect toFIG. 1 , or theTTS engine 210 described with respect toFIG. 2 . - In general, the
TTS engine 310 can modify the sentence structure of a baseline text-to-speech output 306 a for thequery 302 using different types of adjustment techniques. As an example, theTTS engine 310 can substitute words or phrases within the baseline text-to-speech output 306 a based on determining that a complexity score associated with individual words or phrases is greater than a threshold score indicated by thelanguage complexity profile 304 of a user. As another example, theTTS engine 310 can rearrange individual sentence clauses such that the overall complexity of baseline text-to-speech output 306 a is reduced to a satisfactory level based on thelanguage complexity profile 304. TheTTS engine 310 can also re-order words, split or combine sentences, and make other changes to adjust the complexity of text. - In more detail, during the
operation 312, theTTS engine 310 initially generates a baseline text-to-speech output 306 a that is responsive to thequery 302. TheTTS engine 310 then parses the baseline text-to-speech output 306 a intosegments 312 a-312 c. TheTTS engine 310 also detects punctuation marks (e.g., commas, periods, semicolons, etc.) that indicate breakpoints between individual segments. TheTTS engine 310 also computes a complexity score for each of thesegments 312 a-312 c. In some instances, the complexity score can be computed based on the frequency of a particular word within a particular language. Alternative techniques can include computing the complexity score based on the frequency of use by the user, or frequency of occurrence in historical content accessed by the user (e.g., news articles, webpages, etc.). In each of these examples, the complexity score can be used to indicate words that are likely to be comprehended by the user and other words that are unlikely to be comprehended by the user. - In the example,
segments 312 a and 312 b are determined to be relatively complex based on the inclusion of high complex terms such as “FORECAST” and “CONSISTENT,” respectively. However, thesegment 312 c is determined to be relatively simple because the terms included are relatively simple. This determination is represented by thesegments 312 a and 312 b having higher complexity scores (e.g., 0.83, 0.75) compared to the complexity score for thesegment 312 c (e.g., 0.41). - As described above, the
language proficiency profile 304 can be used to compute a threshold complexity score that indicates the maximal complexity that is comprehendible by the user. In the example, the threshold complexity score can be computed to be “0.7” such that theTTS 310 determines that thesegments 312 a and 312 b are unlikely to be comprehended by the user. - After identifying individual segments with associated complexity scores greater than the threshold complexity score indicated by the
language proficiency profile 304, during theoperation 314, theTTS engine 310 substitutes the identified words with alternates that are predicted to be more likely to be understood by the user. As depicted inFIG. 3 , “FORECAST” can be substituted with “WEATHER,” and “CONSISTENT” can be substituted with “CHANGE.” In these examples,segments 314 an and 314 b represent simpler alternatives with lower complexity scores below the threshold complexity score indicated by thelanguage proficiency profile 304. - In some implementations,
TTS engine 310 can process word substitutions for high complexity words using a trained skip-gram model that uses unsupervised techniques to determine appropriately complex words to replace highly complex words. In some instances, theTTS engine 310 can also use thesaurus or synonym data to process word substitutions for high complex words. - Referring now to
operation 316, sentence clauses of a query can be adjusted based on computing complexities associated with particular sentence structures and determining whether is the user will be able to understand the sentence structure based on a language proficiency indicated by thelanguage proficiency profile 304. - In the example, the
TTS engine 310 determines that the baseline text-to-speech response 306 a has a high sentence complexity based on determining that the baseline text-to-speech response 306 a includes three sentence clauses (e.g., “today's forecast is sunny,” “but not consistent,” and “and warm”). In response, theTTS engine 310 can generate adjustedsentence portions speech response 306 b includes both simpler vocabulary (e.g., “WEATHER,” “CHANGE”) as well as a simpler sentence structure (e.g., no clause separations), increasing the likelihood that the user will understand the adjusted text-to-speech output 306 b. The adjusted text-to-speech output 306 b is then generated for output by theTTS engine 310 as theoutput 306 c. - In some implementations, the
TTS engine 310 can perform sentence structure adjustment based on using a user-specific restructuring algorithm that include adjusts the baseline query 302 a using weighting factors to avoid particular sentence structures that are identified to be problematic for the user. For example, the user-specific restructuring algorithm can specify an option to down-weights the inclusion of subordinate clauses or up-weights sentence clauses that have simple subject verb object sequences. -
FIG. 4 is a block diagram that illustrates an example of a system 400 that adaptively generates text-to-speech outputs based on using clustering techniques. The system 400 includes alanguage proficiency estimator 410, auser similarity determiner 420, a complexity optimizer, and a machine learning system 400. - Briefly, the
language proficiency estimator 410 receives data from a plurality of users 402. Thelanguage proficiency estimator 410 then estimates a set oflanguage complexity profiles 412 for each of the plurality of users 402, which is then sent to theuser similarity determiner 420. Theuser similarity determiner 420 identifiesuser clusters 424 of similar users. Thecomplexity optimizer 430 and themachine learning system 440 then analyzes thelanguage complexity profiles 412 of each user within theuser clusters 424 and the context data received from the plurality of users 402 in order to generate acomplexity mapping 442. - In general, the system 400 can be used to analyze relationships between active language complexity and passive language complexity for a population of users. Active language complexity refers to detected language input provided by the user (e.g., text queries, voice input, etc.). Passive language complexity refers to a user's ability to understand or comprehend speech signals that are provided to the user. In this regard, the system 400 can use the determined relationship between the active language complexity and the passive language complexity for multiple users to determine an appropriate passive language complexity for each individual user where the particular user has the highest likelihood of understanding a text-to-speech output.
- The plurality of users 402 can be multiple users that use an application associated with a TTS engine (e.g., the TTS engine 120). For instance, the plurality of users 402 can be a set of users that use a mobile application that utilizes a TTS engine to provide users with text-to-speech features over a user interface of the mobile application. In such an instance, data from the plurality of users 402 (e.g., prior user queries, user selections, etc.) can be tracked by the mobile application and aggregated for analysis by the
language proficiency estimator 410. - The
language proficiency estimator 410 can initially measure passive language complexities for the plurality of users 402 using substantially similar techniques as those described previously with respect toFIG. 1 . Thelanguage proficiency estimator 410 can then generate the language complexity profiles 412, which includes an individual language complexity profile for each of the plurality of users 402. Each individual language complexity profile includes data indicating the passive language complexity and the active language complexity for each of the plurality of users 402. - The
user similarity determiner 420 uses the language complexity data included within the set oflanguage proficiency profiles 412 to identify similar users within the plurality of users 402. In some instances, theuser similarity determiner 420 can group users that have similar active language complexities (e.g., similar language inputs, speech queries provided, etc.). In other instances, theuser similarity determiner 420 can determine similar users by comparing words included in prior user-submitted queries, particular user behaviors on a mobile application, or user locations. Theuser similarity determiner 420 then clusters the similar users to generate theuser clusters 424. - In some implementations, the
user similarity determiner 420 generates theuser clusters 424 based stored on cluster data 422 that include aggregate data for users in specified clusters. For example, the cluster data 422 can be grouped by specific parameters (e.g. number of incorrect query responses, etc.) that indicate a passive language complexity associated with the plurality of users 402. - After generating the
user clusters 424, thecomplexity optimizer 430 varies the complexity of the language output by a TTS system and measures a user's passive language complexity using a set of parameters that indicate a user's ability to understand language output by the TTS system (e.g., understanding rate, voice action flow completion rate, or answer success rate) to indicate user performance. For instance, the parameters can be used to characterize how well users within eachcluster 424 understand a given text-to-speech output. In such instances, thecomplexity optimizer 430 can initially provide a low complexity speech signal to the user and recursively provide additional speech signals within a range of complexities. - In some implementations, the
complexity optimizer 430 can also determine the optimal passive language complexity for various user contexts associated with eachuser cluster 424. For instance, after measuring the user's language proficiency using the set of parameters, thecomplexity optimizer 430 can then classify the measured data by context data received from the plurality of users 402 such that an optimal passive language complexity can be determined for each user context. - After gathering performance data for the range of passive language complexities, the
machine learning system 440 then determines a particular passive language complexity where the performance parameters indicate that the user's language comprehension is the strongest. For instance, themachine learning system 440 aggregates the performance data all users within aparticular user cluster 424 to determine relationships between the active language complexity, the passive language complexity, and the user context. - The aggregate data for the
user cluster 424 can then compared to individual data for each user within theuser cluster 424 to determine an actual language complexity score for each user within theuser cluster 424. For instance, as depicted inFIG. 4 , thecomplexity mapping 442 can represent the relationship between active language complexity and passive language complexity to infer the actual language complexity, which corresponds to the active language complexity mapped to the optimal passive language complexity. - The
complexity mapping 442 represents relationships between active language complexity, TTS complexity, and passive language complexity for all user clusters within the plurality of user 402, which can then be used to predict the appropriate TTS complexity for a subsequent query by an individual user. For example, as described above, user inputs (e.g., queries, text messages, e-mails, etc.) can be used to group similar users intouser clusters 424. For each cluster, the system provides TTS outputs requiring varying levels of language proficiency to understand. The system then assesses the responses received from users, and the rate of task completion for the varied TTS outputs, to determine a level of language complexity that is appropriate for the users in each cluster. The system stores amapping 442 between cluster identifiers and TTS complexity scores corresponding to the identified clusters. The system then uses thecomplexity mapping 442 to determine an appropriate level of complexity for a TTS output for a user. For example, the system identifies a cluster that represents a user's active language proficiency, looks up a corresponding TTS complexity score (e.g., indicating a level of passive language understanding) for the cluster in themapping 442, and generates a TTS output having a complexity level indicated by the retrieved TTS complexity score. - The actual language complexity determined for a user can then be used to adjust the TTS system using techniques described with respect to
FIGS. 1-3 . In this regard, aggregate language complexity data from a group of similar users (e.g., the user cluster 424) can be used to intelligently adjust the performance of a TTS system with respect to a single user. -
FIG. 5 is a flow diagram that illustrates an example of a process 500 for adaptively generating text-to-speech output. Briefly, the process 500 can include determining a language proficiency of a user of a client device (510), determining a text segment for output by a text-to-speech module (520), generating audio data including a synthesized utterance of the text segment (530), and providing the audio data to the client device (540). - In more detail, the process 500 can include determining a language proficiency of a user of a client device (510). For instance, as described with respect to
FIG. 1 , thelanguage proficiency estimator 110 can determine a language proficiency for a user using a variety of techniques. In some instances, the language proficiency can represent an assigned score that indicates a level of language proficiency. In other instances, the language proficiency can represent an assigned category from a plurality of categories of language proficiency. In other instances, the language proficiency can be determined based on user input and/or behaviors indicating a proficiency level of the user. - In some implementations, the language proficiency can be inferred from different user signals. For instance, as described with respect to
FIG. 1 , language proficiency can be inferred from vocabulary complexity of user inputs, data entry rate of the user, a number of misrecognized words from a speech input, a number of completed voice actions for different levels of TTS complexity, or a level of complexity of texts viewed by the user (e.g., books, articles, text on webpages, etc.). - The process 500 can include determining a text segment for output by a text-to-speech module (520). For instance, a TTS engine can adjust a baseline text segment based on the determine language proficiency of the user. In some instances, as described with respect to
FIG. 2 , the text segment for output can be adjusted based on a user context associated the with user. In other instances, as described with respect toFIG. 3 , the text segment for output can also be adjusted by word substitution or sentence restructuring in order to reduce the complexity of the text segment. For example, the adjustment can be based on how rare individual words included in the text segments, the type of verbs used (e.g., compound verbs, or verb tense), the linguistic structure of the text segment (e.g., number of subordinate clauses, amount of separation between related words, degree the that phrases are nested, etc. In other examples, the adjustment can also be based on linguistic measures above with reference measurements for linguistic characteristics (e.g., average separation between subjects and verbs, separation between adjectives and nouns, etc.). In such examples, reference measurements can represent averages, or could include ranges or examples for different complexity levels. - In some implementations, determining the text segment for output can include selecting text segments that have scores that best match reference scores that describe a language proficiency level of the user. In other implementations, individual words or phrases can be scored for complexity, and then the most complex words can be substituted, deleted, or restructured such that overall complexity meets an appropriate level for the user.
- The process 500 can include generating audio data including a synthesized utterance of the text segment (530).
- The process 500 can include providing the audio data to the client device (540).
-
FIG. 6 is a block diagram ofcomputing devices Computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.Computing device 650 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. Additionally,computing device -
Computing device 600 includes aprocessor 602,memory 604, astorage device 606, a high-speed interface 608 connecting tomemory 604 and high-speed expansion ports 610, and alow speed interface 612 connecting tolow speed bus 614 andstorage device 606. Each of thecomponents processor 602 can process instructions for execution within thecomputing device 600, including instructions stored in thememory 604 or on thestorage device 606 to display graphical information for a GUI on an external input/output device, such asdisplay 616 coupled to high speed interface 608. In other implementations, multiple processors and/or multiple buses can be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 600 can be connected, with each device providing portions of the necessary operations, e.g., as a server bank, a group of blade servers, or a multi-processor system. - The
memory 604 stores information within thecomputing device 600. In one implementation, thememory 604 is a volatile memory unit or units. In another implementation, thememory 604 is a non-volatile memory unit or units. Thememory 604 can also be another form of computer-readable medium, such as a magnetic or optical disk. - The
storage device 606 is capable of providing mass storage for thecomputing device 600. In one implementation, thestorage device 606 can be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product can also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 604, thestorage device 606, or memory onprocessor 602. - The high speed controller 608 manages bandwidth-intensive operations for the
computing device 600, while thelow speed controller 612 manages lower bandwidth intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller 608 is coupled tomemory 604,display 616, e.g., through a graphics processor or accelerator, and to high-speed expansion ports 610, which can accept various expansion cards (not shown). In the implementation, low-speed controller 612 is coupled tostorage device 606 and low-speed expansion port 614. The low-speed expansion port, which can include various communication ports, e.g., USB, Bluetooth, Ethernet, wireless Ethernet can be coupled to one or more input/output devices, such as a keyboard, a pointing device, microphone/speaker pair, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. Thecomputing device 600 can be implemented in a number of different forms, as shown in the figure. For example, it can be implemented as astandard server 620, or multiple times in a group of such servers. It can also be implemented as part of arack server system 624. In addition, it can be implemented in a personal computer such as alaptop computer 622. Alternatively, components fromcomputing device 600 can be combined with other components in a mobile device (not shown), such asdevice 650. Each of such devices can contain one or more ofcomputing device multiple computing devices - The
computing device 600 can be implemented in a number of different forms, as shown in the figure. For example, it can be implemented as astandard server 620, or multiple times in a group of such servers. It can also be implemented as part of arack server system 624. In addition, it can be implemented in a personal computer such as alaptop computer 622. Alternatively, components fromcomputing device 600 can be combined with other components in a mobile device (not shown), such asdevice 650. Each of such devices can contain one or more ofcomputing device multiple computing devices -
Computing device 650 includes aprocessor 652,memory 664, and an input/output device such as adisplay 654, acommunication interface 666, and atransceiver 668, among other components. Thedevice 650 can also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of thecomponents - The
processor 652 can execute instructions within thecomputing device 650, including instructions stored in thememory 664. The processor can be implemented as a chipset of chips that include separate and multiple analog and digital processors. Additionally, the processor can be implemented using any of a number of architectures. For example, theprocessor 610 can be a CISC (Complex Instruction Set Computers) processor, a RISC (Reduced Instruction Set Computer) processor, or a MISC (Minimal Instruction Set Computer) processor. The processor can provide, for example, for coordination of the other components of thedevice 650, such as control of user interfaces, applications run bydevice 650, and wireless communication bydevice 650. -
Processor 652 can communicate with a user throughcontrol interface 658 anddisplay interface 656 coupled to adisplay 654. Thedisplay 654 can be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. Thedisplay interface 656 can include appropriate circuitry for driving thedisplay 654 to present graphical and other information to a user. Thecontrol interface 658 can receive commands from a user and convert them for submission to theprocessor 652. In addition, anexternal interface 662 can be provide in communication withprocessor 652, so as to enable near area communication ofdevice 650 with other devices.External interface 662 can provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces can also be used. - The
memory 664 stores information within thecomputing device 650. Thememory 664 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.Expansion memory 674 can also be provided and connected todevice 650 throughexpansion interface 672, which can include, for example, a SIMM (Single In Line Memory Module) card interface.Such expansion memory 674 can provide extra storage space fordevice 650, or can also store applications or other information fordevice 650. Specifically,expansion memory 674 can include instructions to carry out or supplement the processes described above, and can include secure information also. Thus, for example,expansion memory 674 can be provide as a security module fordevice 650, and can be programmed with instructions that permit secure use ofdevice 650. In addition, secure applications can be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner. - The memory can include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the
memory 664,expansion memory 674, or memory onprocessor 652 that can be received, for example, overtransceiver 668 orexternal interface 662. -
Device 650 can communicate wirelessly throughcommunication interface 666, which can include digital signal processing circuitry where necessary.Communication interface 666 can provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication can occur, for example, through radio-frequency transceiver 668. In addition, short-range communication can occur, such as using a Bluetooth, Wi-Fi, or other such transceiver (not shown). In addition, GPS (Global Positioning System)receiver module 670 can provide additional navigation- and location-related wireless data todevice 650, which can be used as appropriate by applications running ondevice 650. -
Device 650 can also communicate audibly usingaudio codec 660, which can receive spoken information from a user and convert it to usable digital information.Audio codec 660 can likewise generate audible sound for a user, such as through a speaker, e.g., in a handset ofdevice 650. Such sound can include sound from voice telephone calls, can include recorded sound, e.g., voice messages, music files, etc. and can also include sound generated by applications operating ondevice 650. - The
computing device 650 can be implemented in a number of different forms, as shown in the figure. For example, it can be implemented as a cellular telephone 480. It can also be implemented as part of asmartphone 682, personal digital assistant, or other similar mobile device. - Various implementations of the systems and methods described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations of such implementations. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which can be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” “computer-readable medium” refers to any computer program product, apparatus and/or device, e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs), used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- The systems and techniques described here can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here, or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- A number of embodiments have been described. Nevertheless, it will be understood that various modifications can be made without departing from the spirit and scope of the invention. In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps can be provided, or steps can be eliminated, from the described flows, and other components can be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.
Claims (20)
Priority Applications (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/653,872 US10109270B2 (en) | 2016-01-28 | 2017-07-19 | Adaptive text-to-speech outputs |
US16/135,885 US10453441B2 (en) | 2016-01-28 | 2018-09-19 | Adaptive text-to-speech outputs |
US16/573,492 US10923100B2 (en) | 2016-01-28 | 2019-09-17 | Adaptive text-to-speech outputs |
US17/153,463 US11670281B2 (en) | 2016-01-28 | 2021-01-20 | Adaptive text-to-speech outputs based on language proficiency |
US18/309,754 US20230267911A1 (en) | 2016-01-28 | 2023-04-28 | Adaptive text-to-speech outputs |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/009,432 US9799324B2 (en) | 2016-01-28 | 2016-01-28 | Adaptive text-to-speech outputs |
US15/477,360 US9886942B2 (en) | 2016-01-28 | 2017-04-03 | Adaptive text-to-speech outputs |
US15/653,872 US10109270B2 (en) | 2016-01-28 | 2017-07-19 | Adaptive text-to-speech outputs |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/477,360 Continuation US9886942B2 (en) | 2016-01-28 | 2017-04-03 | Adaptive text-to-speech outputs |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/135,885 Continuation US10453441B2 (en) | 2016-01-28 | 2018-09-19 | Adaptive text-to-speech outputs |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170316774A1 true US20170316774A1 (en) | 2017-11-02 |
US10109270B2 US10109270B2 (en) | 2018-10-23 |
Family
ID=57799938
Family Applications (7)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/009,432 Active US9799324B2 (en) | 2016-01-28 | 2016-01-28 | Adaptive text-to-speech outputs |
US15/477,360 Active US9886942B2 (en) | 2016-01-28 | 2017-04-03 | Adaptive text-to-speech outputs |
US15/653,872 Active US10109270B2 (en) | 2016-01-28 | 2017-07-19 | Adaptive text-to-speech outputs |
US16/135,885 Active US10453441B2 (en) | 2016-01-28 | 2018-09-19 | Adaptive text-to-speech outputs |
US16/573,492 Active US10923100B2 (en) | 2016-01-28 | 2019-09-17 | Adaptive text-to-speech outputs |
US17/153,463 Active 2036-09-11 US11670281B2 (en) | 2016-01-28 | 2021-01-20 | Adaptive text-to-speech outputs based on language proficiency |
US18/309,754 Pending US20230267911A1 (en) | 2016-01-28 | 2023-04-28 | Adaptive text-to-speech outputs |
Family Applications Before (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/009,432 Active US9799324B2 (en) | 2016-01-28 | 2016-01-28 | Adaptive text-to-speech outputs |
US15/477,360 Active US9886942B2 (en) | 2016-01-28 | 2017-04-03 | Adaptive text-to-speech outputs |
Family Applications After (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/135,885 Active US10453441B2 (en) | 2016-01-28 | 2018-09-19 | Adaptive text-to-speech outputs |
US16/573,492 Active US10923100B2 (en) | 2016-01-28 | 2019-09-17 | Adaptive text-to-speech outputs |
US17/153,463 Active 2036-09-11 US11670281B2 (en) | 2016-01-28 | 2021-01-20 | Adaptive text-to-speech outputs based on language proficiency |
US18/309,754 Pending US20230267911A1 (en) | 2016-01-28 | 2023-04-28 | Adaptive text-to-speech outputs |
Country Status (6)
Country | Link |
---|---|
US (7) | US9799324B2 (en) |
EP (2) | EP3378059B1 (en) |
JP (3) | JP6727315B2 (en) |
KR (4) | KR20200009133A (en) |
CN (2) | CN108604446B (en) |
WO (1) | WO2017131924A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11042597B2 (en) | 2018-06-28 | 2021-06-22 | International Business Machines Corporation | Risk-based comprehension intervention for important documents |
US20230033246A1 (en) * | 2021-07-27 | 2023-02-02 | Carnegie Mellon University | System, Method, and Device for Interactive Neurological Training |
Families Citing this family (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9799324B2 (en) | 2016-01-28 | 2017-10-24 | Google Inc. | Adaptive text-to-speech outputs |
WO2017132660A1 (en) * | 2016-01-29 | 2017-08-03 | Liquid Analytics, Inc. | Systems and methods for dynamic prediction of workflows |
CN105653738B (en) * | 2016-03-01 | 2020-05-22 | 北京百度网讯科技有限公司 | Search result broadcasting method and device based on artificial intelligence |
US10210147B2 (en) * | 2016-09-07 | 2019-02-19 | International Business Machines Corporation | System and method to minimally reduce characters in character limiting scenarios |
US10586527B2 (en) | 2016-10-25 | 2020-03-10 | Third Pillar, Llc | Text-to-speech process capable of interspersing recorded words and phrases |
KR102367778B1 (en) * | 2017-03-15 | 2022-02-25 | 삼성전자주식회사 | Method for processing language information and electronic device thereof |
US10909978B2 (en) * | 2017-06-28 | 2021-02-02 | Amazon Technologies, Inc. | Secure utterance storage |
RU2692051C1 (en) | 2017-12-29 | 2019-06-19 | Общество С Ограниченной Ответственностью "Яндекс" | Method and system for speech synthesis from text |
US10573298B2 (en) * | 2018-04-16 | 2020-02-25 | Google Llc | Automated assistants that accommodate multiple age groups and/or vocabulary levels |
EP3955243A3 (en) * | 2018-10-11 | 2022-05-11 | Google LLC | Speech generation using crosslingual phoneme mapping |
US11403463B2 (en) * | 2018-10-31 | 2022-08-02 | Microsoft Technology Licensing, Llc | Language proficiency inference system |
US10971134B2 (en) * | 2018-10-31 | 2021-04-06 | International Business Machines Corporation | Cognitive modification of speech for text-to-speech |
US11023470B2 (en) * | 2018-11-14 | 2021-06-01 | International Business Machines Corporation | Voice response system for text presentation |
JP7296029B2 (en) * | 2019-03-13 | 2023-06-22 | 株式会社エヌ・ティ・ティ・データ | Language teaching material generation system |
KR20210014909A (en) * | 2019-07-31 | 2021-02-10 | 삼성전자주식회사 | Electronic device for identifying language level of a target and method thereof |
US11861312B2 (en) * | 2019-09-10 | 2024-01-02 | International Business Machines Corporation | Content evaluation based on machine learning and engagement metrics |
CN110767209B (en) * | 2019-10-31 | 2022-03-15 | 标贝（北京）科技有限公司 | Speech synthesis method, apparatus, system and storage medium |
US20220366132A1 (en) * | 2019-11-19 | 2022-11-17 | Click Therapeutics, Inc. | Apparatus, system, and method for adaptive parsing and processing of text to facilitate user engagement |
US11886812B2 (en) * | 2020-03-02 | 2024-01-30 | Grammarly, Inc. | Proficiency and native language-adapted grammatical error correction |
CN111429880A (en) * | 2020-03-04 | 2020-07-17 | 苏州驰声信息科技有限公司 | Method, system, device and medium for cutting paragraph audio |
US11475226B2 (en) | 2020-09-21 | 2022-10-18 | International Business Machines Corporation | Real-time optimized translation |
CN113053357B (en) * | 2021-01-29 | 2024-03-12 | 网易（杭州）网络有限公司 | Speech synthesis method, apparatus, device and computer readable storage medium |
US20220351741A1 (en) * | 2021-04-29 | 2022-11-03 | Rovi Guides, Inc. | Systems and methods to alter voice interactions |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010049602A1 (en) * | 2000-05-17 | 2001-12-06 | Walker David L. | Method and system for converting text into speech as a function of the context of the text |
US20050015307A1 (en) * | 2003-04-28 | 2005-01-20 | Simpson Todd Garrett | Method and system of providing location sensitive business information to customers |
US20080162471A1 (en) * | 2005-01-24 | 2008-07-03 | Bernard David E | Multimodal natural language query system for processing and analyzing voice and proximity-based queries |
US20130325482A1 (en) * | 2012-05-29 | 2013-12-05 | GM Global Technology Operations LLC | Estimating congnitive-load in human-machine interaction |
Family Cites Families (41)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH0335296A (en) * | 1989-06-30 | 1991-02-15 | Sharp Corp | Text voice synthesizing device |
JP2810750B2 (en) * | 1990-01-31 | 1998-10-15 | 株式会社沖テクノシステムズラボラトリ | Language training equipment |
JP3225389B2 (en) | 1993-12-22 | 2001-11-05 | コニカ株式会社 | Method for producing coating solution for electrophotographic photosensitive member and electrophotographic photosensitive member |
PT956552E (en) | 1995-12-04 | 2002-10-31 | Jared C Bernstein | METHOD AND DEVICE FOR COMBINED INFORMATION OF VOICE SIGNS FOR INTERACTION ADAPTABLE TO EDUCATION AND EVALUATION |
JP2002171348A (en) * | 2000-12-01 | 2002-06-14 | Docomo Mobile Inc | System and method for providing audio information |
GB2372864B (en) * | 2001-02-28 | 2005-09-07 | Vox Generation Ltd | Spoken language interface |
JP2002312386A (en) * | 2001-04-12 | 2002-10-25 | Kobelco Systems Corp | Voice retrieval service system |
US7519529B1 (en) * | 2001-06-29 | 2009-04-14 | Microsoft Corporation | System and methods for inferring informational goals and preferred level of detail of results in response to questions posed to an automated information-retrieval or question-answering service |
JP2003225389A (en) | 2002-02-01 | 2003-08-12 | Ace Denken:Kk | Game machine |
US7096183B2 (en) | 2002-02-27 | 2006-08-22 | Matsushita Electric Industrial Co., Ltd. | Customizing the speaking style of a speech synthesizer based on semantic analysis |
JP2004193421A (en) | 2002-12-12 | 2004-07-08 | Olympus Corp | Connection structure of flexible board |
US7389228B2 (en) | 2002-12-16 | 2008-06-17 | International Business Machines Corporation | Speaker adaptation of vocabulary for speech recognition |
US7280968B2 (en) * | 2003-03-25 | 2007-10-09 | International Business Machines Corporation | Synthetically generated speech responses including prosodic characteristics of speech inputs |
US7512579B2 (en) * | 2004-12-17 | 2009-03-31 | Clairvista Llc | System and method for interactively delivering expert information to remote outlets |
US8150872B2 (en) * | 2005-01-24 | 2012-04-03 | The Intellection Group, Inc. | Multimodal natural language query system for processing and analyzing voice and proximity-based queries |
US7490042B2 (en) | 2005-03-29 | 2009-02-10 | International Business Machines Corporation | Methods and apparatus for adapting output speech in accordance with context of communication |
JP2006330629A (en) * | 2005-05-30 | 2006-12-07 | Kenwood Corp | Interaction device, program and method |
US8239762B2 (en) | 2006-03-20 | 2012-08-07 | Educational Testing Service | Method and system for automatic generation of adapted content to facilitate reading skill development for language learners |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20080109224A1 (en) * | 2006-11-02 | 2008-05-08 | Motorola, Inc. | Automatically providing an indication to a speaker when that speaker's rate of speech is likely to be greater than a rate that a listener is able to comprehend |
JP2010033139A (en) * | 2008-07-25 | 2010-02-12 | Nec Corp | Information processor, electronic dictionary providing method, and program |
JP2010145873A (en) * | 2008-12-19 | 2010-07-01 | Casio Computer Co Ltd | Text replacement device, text voice synthesizer, text replacement method, and text replacement program |
JP5281659B2 (en) * | 2009-01-20 | 2013-09-04 | 旭化成株式会社 | Spoken dialogue apparatus, dialogue control method, and dialogue control program |
US9547642B2 (en) * | 2009-06-17 | 2017-01-17 | Empire Technology Development Llc | Voice to text to voice processing |
JP5545467B2 (en) * | 2009-10-21 | 2014-07-09 | 独立行政法人情報通信研究機構 | Speech translation system, control device, and information processing method |
JP2011100191A (en) * | 2009-11-04 | 2011-05-19 | Nippon Telegr & Teleph Corp <Ntt> | Device, method, and program for retrieving document |
KR20110067517A (en) * | 2009-12-14 | 2011-06-22 | 주식회사 케이티 | Foreign language conversation learning method using user response sentence based on frame |
US10679605B2 (en) * | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US20120016674A1 (en) * | 2010-07-16 | 2012-01-19 | International Business Machines Corporation | Modification of Speech Quality in Conversations Over Voice Channels |
US8744855B1 (en) * | 2010-08-09 | 2014-06-03 | Amazon Technologies, Inc. | Determining reading levels of electronic books |
JP5727810B2 (en) * | 2011-02-10 | 2015-06-03 | 株式会社Ｎｔｔドコモ | Language ability determination device, language ability determination method, content distribution system, and program |
US20130031476A1 (en) * | 2011-07-25 | 2013-01-31 | Coin Emmett | Voice activated virtual assistant |
US9082414B2 (en) * | 2011-09-27 | 2015-07-14 | General Motors Llc | Correcting unintelligible synthesized speech |
US9824695B2 (en) * | 2012-06-18 | 2017-11-21 | International Business Machines Corporation | Enhancing comprehension in voice communications |
JP6040715B2 (en) | 2012-11-06 | 2016-12-07 | ソニー株式会社 | Image display apparatus, image display method, and computer program |
US9009028B2 (en) * | 2012-12-14 | 2015-04-14 | Google Inc. | Custom dictionaries for E-books |
JP6026881B2 (en) | 2012-12-26 | 2016-11-16 | 関西ペイント株式会社 | Coating composition and multilayer coating film forming method |
US20140188479A1 (en) * | 2013-01-02 | 2014-07-03 | International Business Machines Corporation | Audio expression of text characteristics |
JP6111802B2 (en) * | 2013-03-29 | 2017-04-12 | 富士通株式会社 | Spoken dialogue apparatus and dialogue control method |
US9412358B2 (en) | 2014-05-13 | 2016-08-09 | At&T Intellectual Property I, L.P. | System and method for data-driven socially customized models for language generation |
US9799324B2 (en) * | 2016-01-28 | 2017-10-24 | Google Inc. | Adaptive text-to-speech outputs |
-
2016
- 2016-01-28 US US15/009,432 patent/US9799324B2/en active Active
- 2016-12-29 WO PCT/US2016/069182 patent/WO2017131924A1/en active Application Filing
- 2016-12-29 KR KR1020207001575A patent/KR20200009133A/en not_active Application Discontinuation
- 2016-12-29 CN CN201680080197.1A patent/CN108604446B/en active Active
- 2016-12-29 JP JP2018539396A patent/JP6727315B2/en active Active
- 2016-12-29 CN CN202310511278.4A patent/CN116504221A/en active Pending
- 2016-12-29 EP EP16826663.3A patent/EP3378059B1/en active Active
- 2016-12-29 EP EP21212730.2A patent/EP4002353A1/en active Pending
- 2016-12-29 KR KR1020217004584A patent/KR20210021407A/en not_active Application Discontinuation
- 2016-12-29 KR KR1020187021923A patent/KR102219274B1/en active IP Right Grant
- 2016-12-29 KR KR1020207001576A patent/KR20200009134A/en not_active IP Right Cessation
-
2017
- 2017-04-03 US US15/477,360 patent/US9886942B2/en active Active
- 2017-07-19 US US15/653,872 patent/US10109270B2/en active Active
-
2018
- 2018-09-19 US US16/135,885 patent/US10453441B2/en active Active
-
2019
- 2019-09-17 US US16/573,492 patent/US10923100B2/en active Active
-
2020
- 2020-04-22 JP JP2020076068A patent/JP6903787B2/en active Active
-
2021
- 2021-01-20 US US17/153,463 patent/US11670281B2/en active Active
- 2021-06-22 JP JP2021103122A patent/JP7202418B2/en active Active
-
2023
- 2023-04-28 US US18/309,754 patent/US20230267911A1/en active Pending
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010049602A1 (en) * | 2000-05-17 | 2001-12-06 | Walker David L. | Method and system for converting text into speech as a function of the context of the text |
US20050015307A1 (en) * | 2003-04-28 | 2005-01-20 | Simpson Todd Garrett | Method and system of providing location sensitive business information to customers |
US20080162471A1 (en) * | 2005-01-24 | 2008-07-03 | Bernard David E | Multimodal natural language query system for processing and analyzing voice and proximity-based queries |
US20130325482A1 (en) * | 2012-05-29 | 2013-12-05 | GM Global Technology Operations LLC | Estimating congnitive-load in human-machine interaction |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11042597B2 (en) | 2018-06-28 | 2021-06-22 | International Business Machines Corporation | Risk-based comprehension intervention for important documents |
US20230033246A1 (en) * | 2021-07-27 | 2023-02-02 | Carnegie Mellon University | System, Method, and Device for Interactive Neurological Training |
US11899922B2 (en) * | 2021-07-27 | 2024-02-13 | Carnegie Mellon University | System, method, and device for interactive neurological training |
Also Published As
Publication number | Publication date |
---|---|
KR102219274B1 (en) | 2021-02-24 |
KR20210021407A (en) | 2021-02-25 |
EP3378059B1 (en) | 2021-12-08 |
US10923100B2 (en) | 2021-02-16 |
US9886942B2 (en) | 2018-02-06 |
US20210142779A1 (en) | 2021-05-13 |
CN108604446A (en) | 2018-09-28 |
JP7202418B2 (en) | 2023-01-11 |
KR20200009134A (en) | 2020-01-29 |
JP6903787B2 (en) | 2021-07-14 |
KR20200009133A (en) | 2020-01-29 |
US10453441B2 (en) | 2019-10-22 |
US20190019501A1 (en) | 2019-01-17 |
KR20180098654A (en) | 2018-09-04 |
JP2020126262A (en) | 2020-08-20 |
US11670281B2 (en) | 2023-06-06 |
CN116504221A (en) | 2023-07-28 |
EP4002353A1 (en) | 2022-05-25 |
JP2019511034A (en) | 2019-04-18 |
US20170221471A1 (en) | 2017-08-03 |
JP2021144759A (en) | 2021-09-24 |
US9799324B2 (en) | 2017-10-24 |
EP3378059A1 (en) | 2018-09-26 |
JP6727315B2 (en) | 2020-07-22 |
US20200013387A1 (en) | 2020-01-09 |
US20230267911A1 (en) | 2023-08-24 |
CN108604446B (en) | 2023-05-16 |
US20170221472A1 (en) | 2017-08-03 |
WO2017131924A1 (en) | 2017-08-03 |
US10109270B2 (en) | 2018-10-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11670281B2 (en) | Adaptive text-to-speech outputs based on language proficiency | |
US11532299B2 (en) | Language model biasing modulation | |
EP3381034B1 (en) | Determining dialog states for language models | |
EP3469489B1 (en) | Follow-up voice query prediction | |
US11282513B2 (en) | Negative n-gram biasing | |
US20240013782A1 (en) | History-Based ASR Mistake Corrections |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SHARIFI, MATTHEW;FOERSTER, JAKOB NICOLAUS;SIGNING DATES FROM 20160210 TO 20160211;REEL/FRAME:043044/0956 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |