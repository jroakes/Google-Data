CN108604254A - The closed caption of voice control is shown - Google Patents
The closed caption of voice control is shown Download PDFInfo
- Publication number
- CN108604254A CN108604254A CN201780011386.8A CN201780011386A CN108604254A CN 108604254 A CN108604254 A CN 108604254A CN 201780011386 A CN201780011386 A CN 201780011386A CN 108604254 A CN108604254 A CN 108604254A
- Authority
- CN
- China
- Prior art keywords
- media
- equipment
- projecting
- electronic equipment
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/63—Querying
- G06F16/635—Filtering based on additional data, e.g. user or group profiles
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/169—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being an integrated pointing device, e.g. trackball in the palm rest area, mini-joystick integrated between keyboard keys, touch pads or touch stripes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/18—Packaging or power distribution
- G06F1/181—Enclosures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/242—Query formulation
- G06F16/243—Natural language query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0354—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of 2D relative movements between the device, or an operating part thereof, and a plane or surface, e.g. 2D mice, trackballs, pens or pucks
- G06F3/03547—Touch pads, in which fingers can move on a surface
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0362—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of 1D translations or rotations of an operating part of the device, e.g. scroll wheels, sliders, knobs, rollers or belts
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/1066—Session management
- H04L65/1083—In-session procedures
- H04L65/1094—Inter-user-equipment sessions transfer or sharing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/60—Network streaming of media packets
- H04L65/61—Network streaming of media packets for supporting one-way streaming services, e.g. Internet radio
- H04L65/612—Network streaming of media packets for supporting one-way streaming services, e.g. Internet radio for unicast
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/60—Network streaming of media packets
- H04L65/61—Network streaming of media packets for supporting one-way streaming services, e.g. Internet radio
- H04L65/613—Network streaming of media packets for supporting one-way streaming services, e.g. Internet radio for the control of the source by the destination
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/239—Interfacing the upstream path of the transmission network, e.g. prioritizing client content requests
- H04N21/2393—Interfacing the upstream path of the transmission network, e.g. prioritizing client content requests involving handling client requests
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/4104—Peripherals receiving signals from specially adapted client devices
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/414—Specialised client platforms, e.g. receiver in car or embedded in a mobile appliance
- H04N21/4147—PVR [Personal Video Recorder]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42203—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS] sound input device, e.g. microphone
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42204—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42204—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor
- H04N21/42206—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor characterized by hardware details
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/472—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content
- H04N21/4722—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content for requesting additional data associated with the content
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R1/00—Details of transducers, loudspeakers or microphones
- H04R1/20—Arrangements for obtaining desired frequency or directional characteristics
- H04R1/32—Arrangements for obtaining desired frequency or directional characteristics for obtaining desired directional characteristic only
- H04R1/323—Arrangements for obtaining desired frequency or directional characteristics for obtaining desired directional characteristic only for loudspeakers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R2201/00—Details of transducers, loudspeakers or microphones covered by H04R1/00 but not provided for in any of its subgroups
- H04R2201/02—Details casings, cabinets or mounting therein for transducers covered by H04R1/02 but not provided for in any of its subgroups
- H04R2201/028—Structural combinations of loudspeakers with built-in power amplifiers, e.g. in the same acoustic enclosure
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R2227/00—Details of public address [PA] systems covered by H04R27/00 but not provided for in any of its subgroups
- H04R2227/005—Audio distribution systems for home, i.e. multi-room use
Abstract
A kind of method realizes on the server systems, the display for initiating the closed caption (CC) of media content by voice.Server system receives the speech message recorded by electronic equipment, and determine that speech message is that the first CC initiates request, it includes initiating the user voice command of closed caption and the user speech of the display equipment for the media content being activated being specified to playing closed caption that the first CC, which initiates request,.Server system identifies device for projecting associated with electronic equipment in the customer domain and being coupled to specified display equipment, and it sends the 2nd CC to device for projecting and initiates request, to make device for projecting be able to carry out media play application, which opens and shows the closed caption for the media content being currently displayed in given display device.
Description
Technical field
Present application relates generally to computer technologies, are including but not limited to used to control intelligence using Speech-activated electronic equipment
The method and system that closed caption in energy media environment is shown.
Background technology
The electronic equipment for being integrated with microphone has been widely used for collecting voice input from the user and defeated according to voice
Enter and realizes different voice activation functions.For example, many advanced mobile devices include being configured so that voice inputs to initiate
Call carries out restaurant search, starts route selection on map, create calendar activity, model is added to social networks, is known
Other song and the voice assistance system (for example, Siri and Google assistants) for completing many other tasks.Mobile device is usually wrapped
Display screen is included, the user for providing voice input is allowed to check the state of being asked by voice input for task.However, when applying
The voice activation work(similar with mobile device is realized with relatively simple structure and the electronic equipment that is manufactured with low cost
When energy, the cost of electronic equipment will be dramatically increased using display screen.Therefore, it is necessary to use simple and inexpensive user interface
Indicate to include one or more microphones and the state of voice input processing being used as in the electronic equipment of speech interface.
In addition, the current voice activation function of realizing in a mobile device is limited to remote server (for example, search is drawn
Hold up, social network server or voice secondary server) function Internet-based.The result of voice activation function is shown in
Or for controlling mobile device itself, and any other addressable remotely-or locally electronic equipment of user is not influenced.In view of
Voice input be for a user it is convenient, in addition to request be limited between remote server and mobile device based on interconnection
Except the function of net, allow user using voice input come to control other addressable electronic equipments of user be beneficial.
Invention content
Therefore, it is necessary to create smart media environment or Intelligence home environment, wherein electronic equipment offer exempts to regard and hands-free language
Sound interface is to activate the language on other apparatus for media playing or intelligent home equipment being coupling in smart media or home environment
Sound activates function.In some embodiments of the application, smart media environment includes that one or more Speech-activated electronics are set
Standby and multiple media display devices, each media display devices are arranged in different positions and are coupled to device for projecting (such as machine
Top box).Each Speech-activated electronic equipment is configured as record and determines that user speech is asked by cloud projection service server for it
The speech message of (for example, media play request, media transmission request or closed caption initiate request).Such as by speech message meaning
Show, then user speech request is directed to destination device for projecting by cloud projection service server.Speech-activated electronic equipment
It is additionally configured to show visual pattern via the full color LED array for indicating corresponding speech processes state.It can use similar
Arrangement come control intelligent home equipment in Intelligence home environment implement voice activation function.These methods are optionally supplemented
Or it substitutes and requires user using remote controler or client device to control the media device or intelligence in smart media or home environment
The conventional method of energy household equipment.
According to the one side of the application, a kind of method is realized at electronic equipment, for visually indicating speech processes
State.Electronic equipment includes full color LED array, one or more microphones, loud speaker, processor and storage for by processor
The memory of at least one program executed.This method includes being collected near electronic equipment via one or more microphones
Environment audio input, and handle audio input.Processing includes identification and responds the voice input of the user in environment
One or more of.This method further comprises the state that processing is determined from multiple predefined speech processes states, and
And for the identification of each full color LED and the associated corresponding predetermined LED illumination specification of identified speech processes state.Illumination
Specification includes one or more of LED illumination duration, pulsation rate, duty ratio, color sequences and brightness.This method is also wrapped
The LED illumination specification according to the full color LED identified is included, the illumination of synchronous full color LED array is to provide language determined by instruction
Sound handles the visual pattern of state.
According to the one side of the application, a kind of method is executed at the server system including processor and memory,
The memory stores at least one program executed by processor, for playing media content on media output devices.Media
Content playback method includes receiving the speech message of electronic equipment record, and determine that speech message is asked including the first media play
It asks.First media play request includes playing the user voice command of the media content on the media output devices of destination and to matchmaker
The user speech of body output equipment is specified, and user voice command includes at least the information and needs of the first media play application
The media content of broadcasting.Media content playback method further includes specified according to the voice to media output devices, identify (for example,
In device registry) with electronic device association and it is coupled to the device for projecting of media output devices in the customer domain.The throwing
Jet device is configured as executing one or more media play applications, and the media play is applied for controlling media output devices
Play the media content received from one or more media content trustships (host).Media content playback method further includes to projection
Equipment transmission includes the second media play request of the information that the first media play is applied and the media content that needs play, to
Device for projecting is set to be able to carry out the first media play application that control media output devices play media content.
According to the another aspect of the application, a kind of method executes at the server system including processor and memory,
The memory stores at least one program executed by processor, for initiating the closed caption to media content by voice
(CC) display.CC shows that media method includes receiving the speech message recorded by electronic equipment, and determine that the speech message is
First closed caption initiates request.It includes initiating user voice command and the broadcasting of closed caption that first closed caption, which initiates request,
Closed caption specifies the user speech of the display equipment for the media content being activated.CC display methods further comprises according to right
Show the specified of equipment, identification (for example, in device registry) in the customer domain it is associated with electronic equipment with and couple
To the device for projecting of specified display equipment.Device for projecting is configured as executing media play application, and media play, which is applied, to be used for
The specified display equipment of control shows the media content received from media content trustship.CC display methods further comprises to coupling
Device for projecting to given display device sends the initiation request of the second closed caption, so that device for projecting is able to carry out media
Application is played, initiating the specified display equipment opening of request control according to the second closed caption is currently displayed at specified display
The closed caption of media content in equipment and show closed caption.
According to the another aspect of the application, a kind of method executes at the server system including processor and memory,
The memory stores at least one program for being executed by processor, and described program is for showing media content from source media
Output equipment is moved to destination media output devices.Media conveying method includes the speech message for receiving electronic equipment record,
And determine that speech message includes media transmission request.Media transmission request includes for media content being played on to be transmitted to
The user voice command of destination media output devices and specified to the user speech of destination media output devices.Media transmit
Method further includes that the instant media play information for the media content being currently played is obtained from source device for projecting.Immediately letter is played
Breath include at least the first media play application information, currently playing media content and with play media content it is relevant when
Between position.The media conveying method further includes specified according to the voice to destination media output devices, is identified (for example, setting
In standby registration table) destination associated with electronic equipment in the customer domain and that be coupled to destination media output devices projects
Equipment, and destination device for projecting is configured as executing one or more media play applications, for control purposes media
Output equipment plays the media content received from one or more media content trustships.Media conveying method further includes to destination
Device for projecting transmission includes the media play request of instant media play information, so that destination device for projecting is able to carry out
Control the first media play application that destination media output devices play media content from time location.
According to some embodiments, device for projecting includes the device of the operation for executing any of the above described method.
Description of the drawings
Various described embodiments in order to better understand, should refer to following embodiment in conjunction with the following drawings
Description, wherein identical reference numeral indicates corresponding part in whole attached drawings.
Fig. 1 is the example smart media environment according to some embodiments.
Fig. 2A be according to the device for projecting of the wherein Speech-activated electronic equipment of some embodiments and smart media environment,
Client device or the Example Operating Environment of server system interaction.
Fig. 2 B are control path B control device for projecting and its associated media playing activities shown in A according to fig. 2
The example flow diagram of controlling media play process.
Fig. 3 is swashed according to the wherein device for projecting of some embodiments and client device, the voice of smart media environment
Another Example Operating Environment of electronic equipment or server system interaction living.
Fig. 4 A and Fig. 4 B are the front view and rearview according to the Speech-activated electronic equipment of some embodiments.
Fig. 4 C are to show raising included in the pedestal of electronic equipment 190 according to some embodiments with open configuration
The perspective view of the Speech-activated electronic equipment 190 of sound device.
Fig. 4 D and Fig. 4 E are swashed according to the voice for showing the electronic building brick being included in of some embodiments respectively
The side view and enlarged drawing of electronic equipment living.
Fig. 4 F (1)-Fig. 4 F (4) show the touch-sensing array in Speech-activated electronic equipment according to some embodiments
On four touch events detecting.
Fig. 4 F (5) show that user presses the button on the rear side of Speech-activated electronic equipment according to some embodiments.
Fig. 4 G are according to the top view of the Speech-activated electronic equipment of some embodiments, and Fig. 4 H are shown according to one
The six example visual patterns for being used to indicate speech processes state of a little embodiments shown by full color LED array.
Fig. 5 is to show to be applied to collect user in smart media environment as speech interface according to some embodiments
The block diagram of the exemplary electronic device of voice command.
Fig. 6 be show according to some embodiments be applied to smart media environment in media content display it is automatic
The block diagram of the example device for projecting of control.
Fig. 7 is the example of service in the server system 140 in the smart media environment shown according to some embodiments
The block diagram of device.Example server is one of cloud projection service server.
Fig. 8 is to show automatically controlling applied to what the media in smart media environment were shown according to some embodiments
The block diagram of example client end equipment.
Fig. 9 is the block diagram of the example intelligent home equipment in the smart media environment shown according to some embodiments.
Figure 10 is the flow chart for showing the method for visually indicating speech processes state according to some embodiments.
Figure 11 is the side that the closed caption of display media content is initiated by voice shown according to some embodiments
The flow chart of method.
Figure 12 be show the speech play by the media content on media output devices according to some embodiments come
The flow chart of the method for initiation.
Figure 13 is to show to be moved to mesh from source media output devices according to the broadcasting by media content of some embodiments
Ground media output devices method flow chart.
Through multiple views of attached drawing, similar reference numeral refers to corresponding part.
Specific implementation mode
Although digital revolution provides many benefits from open sharing information to global community's meaning, emerging technology warp
Often cause obscuring in consumer, suspect and frightened, to prevent consumer from being benefited from technology.Electronic equipment can be used easily
Make speech interface, voice activation function is inputted and initiate to receive voice from the user, exempts to regard (eyes-free) to provide
The prior art and emerging technology are handled with the solution of hands-free (hands-free).Specifically, even if the sight quilt of user
It blocks and his hand has been expired, the voice input received at electronic equipment can also carry instruction and information.In order to realize
Hands-free and exempt from the experience regarded, Speech-activated electronic equipment constantly or only listens to environment (that is, processing is received from environment when being triggered
The audio signal of collection).On the other hand, the language that user identity is used with the voice of user and user links.In order to protect user
Identity, Speech-activated electronic equipment are commonly used in the non-Public of protected, controlled and private space (such as family and automobile).
According to certain embodiments of the present invention, Speech-activated electronic equipment includes full color light emitting diode (LED) array.
When electronic equipment processing is from the audio input that one or more microphones are collected, the array of full LED is illuminated with according to basis
The LED illumination specification that processing state determines provides visual pattern.Full color LED array, which is configured to supply, to be respectively correspond toed at voice
Multiple visual patterns of reason state (for example, hot word detects, listens to, thinks deeply and talks).This LED for creating visual pattern
Design language is applied to solve the problems, such as that user is puzzled, worried and uneasy at least partly, and promotes to corresponding speech interfaces body
Understanding, use and the enjoyment tested.
In addition, according to certain embodiments of the present invention, Speech-activated electronic equipment inputs to initiate and control using voice
System shows the video playback in equipment.Specifically, server system (for example, cloud projection service server) is received by voice activation
The speech message of electronic equipment record, and determine that speech message includes further comprising playing the media on media output devices
The user voice command (optionally including Speech-activated electronic equipment itself) of content and the user speech to media output devices
Specified media play request.The media that user voice command includes at least the information of the first media play application and needs play
Content.According to specified to the voices of media output devices, server system identification in the customer domain it is associated with electronic equipment simultaneously
And it is coupled to the device for projecting of media output devices, and device for projecting is configured as executing one or more media plays and answer
With the media content received from one or more media content trustships for controlling media output devices broadcasting.Then server
System sends the information that the first media play is applied and the media content that needs play to device for projecting, to enable device for projecting
Enough the first media play applications for executing control media output devices and playing media content.
In some embodiments, when media content is displayed on media output devices, Speech-activated electronic equipment
Allow user using they voice open and close TV on subtitle, without regard to remote controler or the second screen equipment
Any user of (for example, mobile phone, tablet computer and laptop computer) interacts.Specifically, server system by with
It is set to and determines that the first closed caption initiates request from speech message, it includes initiating to hide word which, which initiates request,
The user voice command and broadcasting closed caption of curtain specify the user speech of the display equipment for the media content being activated.
Identification is associated with electronic equipment in the customer domain and is coupled to after the device for projecting of specified display equipment, server system
It unites and sends the initiation request of the second closed caption to device for projecting, it is described to make device for projecting be able to carry out media play application
The specified display equipment of media play application control opens hiding for the media content being currently displayed in specified display equipment
Subtitle, and request is initiated according to the second closed caption and shows closed caption.
In addition, according to certain embodiments of the present invention, when media content is displayed on the first media output devices,
Speech-activated electronic equipment allows user using their voice to initiate media content from the first media output devices to second
The media of media output devices transmit.At least through in the definite of the media content played on the first media output devices
Restore the media content on the second media output devices at point, transmission keeps corresponding media play state.
Specifically, server system is configured as determining media transmission request from speech message, media transmission request
Include media content being played on being transmitted to the user voice command of destination media output devices and to destination matchmaker
The user speech of body output equipment is specified.Then server system obtains the media content being currently played from source device for projecting
Instant media play information, the instant broadcast information include at least the first media play application information, be currently played
Media content and with play the related time location of media content.Identification in the customer domain it is associated with electronic equipment simultaneously
And be coupled to after the destination device for projecting of specified display equipment, server system includes to the transmission of destination device for projecting
The media play request of instant media play information, to make destination device for projecting be able to carry out control destination media output
Equipment plays the first media play application of media content from the time location.In some embodiments, destination projection is set
It is standby to be identified in device registry.
Now with detailed reference to embodiment, its example is shown in the drawings.In the following detailed description, many tools are elaborated
Body details is in order to provide the thorough understanding to various described embodiments.However, for those of ordinary skill in the art
For it is readily apparent that various described realize can be put into practice without these specific details.In other feelings
Under condition, it is not described in well known method, process, component, circuit and network, in order to avoid unnecessarily obscure each of embodiment
Aspect.
Smart media/home environment
Fig. 1 is the example smart media environment 100 according to some embodiments.Smart media environment 100 includes having respectively
The structure 150 (for example, house, office building, garage or mobile house) of kind integrated equipment.It should be understood that equipment can also
It is integrated into the not smart media environment 100 including total 150, such as apartment, condominium apartment or working space.It is retouched
The structure 150 painted includes the multiple rooms 152 detached at least partly each other via wall 154.Wall 154 may include inner wall
Or outer wall.Each room can also include floor 156 and ceiling 158.
One or more media devices be disposed in smart media environment 100 with provide be stored at local content source or
The media content transmitted from distant content source (for example, content hosting 114) streaming.Media device can be divided into two classes：Directly to
Spectators export the media output devices 106 of media content, and networking so that media content streaming is transmitted to media output devices
108 device for projecting 108.The example of media output devices 106 includes but not limited to that TV (TV) shows equipment and music
Device.The example of device for projecting 108 includes but not limited to set-top box (STB), DVD player and TV box.In example smart media
In environment 100, media output devices 106 are disposed in more than one position, and each media output devices 106 are coupled
To corresponding device for projecting 108 or including embedded projecting unit.Media output devices 106-1 includes being hardwired to DVD to broadcast
Put the television indicator of device or set-top box 108-1.Media output devices 106-2 includes intelligence TV equipment, intelligence TV equipment collection
At embedded projecting unit with streaming media content to be shown to its spectators.Media output devices 106-3 includes being coupled to
The conventional TV displays of TV box 108-3 (for example, Google TV or Apple TV products), and such TV box 108-3
The streaming media content that is received from media content Entrust Server 114 of transmission and offer are to the access of internet with defeated in media
Go out on equipment 106-3 and shows content Internet-based.
Other than media device 106 and 108, one or more electronic equipments 190 are disposed in smart media environment 100
In to collect the audio input of various media play functions for initiating media device.In some embodiments, these languages
Sound activation electronic equipment 190 (for example, equipment 1901-1,190-2 and 190-3) be disposed near media device, such as with throwing
In 106 identical room of jet device 108 and media output devices.Alternatively, in some embodiments, Speech-activated electronic
Equipment 190-4 is disposed in one or more intelligent home equipments rather than in the room of any media device.It is alternative
Ground, in some embodiments, Speech-activated electronic equipment 190 are disposed in the position for the electronic equipment that do not network.
Electronic equipment 190 include at least one or more microphone, loud speaker, processor and storage it is at least one for by
The memory for the program that processor executes.Loud speaker is configured as that electronic equipment 190 is allowed to set voice message transmission to electronics
Standby 190 the location of in smart media environment 100, to broadcast music, the state of reporting audio input processing, with electronics
The user session of equipment 190 provides the user that electronic equipment 190 is arrived in instruction.As the replacement of speech message, vision signal
It can be used for providing the feedback of the state about audio input process to the user of electronic equipment 190.When electronic equipment 190 is to pass
When mobile device (for example, mobile phone or tablet computer) of system, display screen is configured as display about audio input
The notice of the state of processing.
According to some embodiments, electronic equipment 190 is speech interface equipment, is taken by network connection with being projected by cloud
Business server 116 and/or voice secondary server 112 provide speech identifying function.For example, electronic equipment 190 includes intelligence
Loud speaker provides a user music and allows to exempt to regard to service (for example, Google assistant) with hands-free access voice assistant.It can
Selection of land, electronic equipment 190 be include microphone desk-top or laptop computer, tablet computer and mobile phone in one
It is a.Optionally, electronic equipment 190 is simple and inexpensive speech interface equipment.Simplicity in view of electronic equipment 190 and low
Cost, electronic equipment 190 includes full color light emitting diode (LED) array rather than full display screen, and is shown on full color LED
Visual pattern is to indicate the state of audio input process.
When the voice input from electronic equipment 190 be used to control media output devices 106 via device for projecting 108
When, electronic equipment 190 has effectively achieved the control that projection enables the new rank of media device.In specific example, electronics is set
Standby 190 include that there is the leisure of far field voice access to enjoy loud speaker and the speech interface equipment as Google assistant.Electricity
Sub- equipment 190 can be arranged in any room in smart media environment 100.When multiple electronic equipments 190 be distributed in it is multiple
When in room, they become to project audio receiver, are synchronized to provide the voice input from all these rooms.
Specifically, in some embodiments, electronic equipment 190 includes with the personal assistant clothes for being connected to voice activation
The WiFi loud speakers of the microphone of business (such as Google assistant).User can send out matchmaker by the microphone of electronic equipment 190
Body playing request, and ask personal assistant service on electronic equipment 190 itself or the media output devices 106 of another connection
Play media content.For example, user can be by saying WiFi loud speakers " OK Google play cat on my living-room TV
Video (OK Google, Play cat videos on my Living room TV.) " carrys out publication medium playing request.Then
Personal assistant service plays requested media content by using acquiescence or specified media application in requested equipment
To realize media play request.
User can also send out via the microphone of electronic equipment 190 about in the media played on the display device
The voice request of appearance.In some embodiments, when there is no remote control or user can be used in the second screen equipment, pass through language
The closed caption of the media content currently shown is initiated or deactivated to sound on the display device.Therefore, user can not be related to having
It is shown in the case of having any other equipment of physical interface by exempting to regard to open with hands-free voice activation electronic equipment 190
Closed caption in equipment, and this Speech-activated electronic equipment 190 meets to there is the federal of the user of hearing disabilities to assist
Functional requirement.
In some embodiments, user may wish to carry out current media session with them when they pass through house.
This needs personal assistant service that current media session is transmitted to the second device for projecting from the first device for projecting, which sets
It is standby to be not directly connected to the first device for projecting or do not know the presence of the first device for projecting.After media content transmission, coupling
To the second device for projecting 108 the second output equipment 106 continue out of music track or video clipping point of contact really play it is previous
It is coupled to the media content of the first output equipment 106 of the first device for projecting 108, wherein the broadcasting of media content is in the first output
It is abandoned in equipment 106.
In some embodiments, in addition to media device (such as output equipment 106 and device for projecting 108) and voice activation
Except electronic equipment 190, intelligent home equipment can also pass through the wall 154 of smart media environment 100, floor 156 or smallpox
The installation of plate 158, integrated and/or support (being widely referred to as Intelligence home environment in view of the presence of intelligent home equipment).Integrated intelligence
Energy household equipment includes that the more sensing networks of intelligence connect equipment, can be in intelligent home network and/or central server or cloud
Mutual Seamless integration- in computing system, to provide various useful wired home functions.In some embodiments, wired home
Equipment is disposed at the identical position of the Intelligence home environment 100 with device for projecting 108 and/or output equipment 106, and
Therefore it is located at neighbouring or known distance relative to device for projecting 108 and output equipment 106.
Intelligent home equipment in smart media environment 100 can include but is not limited to one or more more Sensor Networks of intelligence
The thermostat 122 of network connection, more sensing hazard detectors of one or more intelligent network connection, the more biographies of one or more intelligence
Feel the access road interface equipment 126 and 128 (hereinafter referred to as " (the smart doorbells of intelligent doorbell 126 of network connection
126) " with " intelligent door lock 128 (smart door locks 128) "), the report that connects of one or more more sensing networks of intelligence
The camera system 132 and one or more more Sensor Networks of intelligence of the more sensing network connections of alert system 130, one or more intelligence
The switch on wall 136 of network connection.In some embodiments, the intelligent home equipment in the smart media environment 100 of Fig. 1 includes
Multiple more sensing network connection equipment 138 (hereinafter referred to as " intelligent appliance 138 (smart appliances 138) ") of intelligence are all
As refrigerator, stove, oven, television set, washing machine, dryer, lamp, intercom system valve rod, garage door opener, flooring fan,
Ceiling fan, wall-hanging air conditioner, pool heater, irrigation system, security system, space heater, vehicle window alternating current unit, motor-driven pipeline
Ventilation opening etc..
Intelligent home equipment in smart media environment 100 can include additionally or alternatively it is one or more other
Take sensor (for example, touch screen, IR sensors, ambient light sensor and motion detector).In some embodiments, intelligence
Intelligent home equipment that can be in media environment 100 includes being determined based on the RFID tag positioned at the upper or embedded occupant of occupant
Radio frequency identification (RFID) reader (for example, in each room 152 or part of it) of occupancy.For example, RFID reader can
To be integrated into intelligent hazard detector 104.
In some embodiments, other than comprising sensing function, equipment 122,124,126,128,130,132,136
(it is referred to as " intelligent home equipment (the smart home devices) " or " 120 (the of intelligent home equipment with 138
Smart home devices 120) ") it can be with other intelligent home equipments, central server or cloud computing system and/or net
Other equipment (such as client device 104, device for projecting 108 and Speech-activated electronic equipment 190) shared data of network connection
Communication and information.Similarly, each in device for projecting 108 and Speech-activated electronic equipment 190 can also be with other projections
Equipment 108, Speech-activated electronic equipment 190, intelligent home equipment, central server or cloud computing system and/or network connection
Other equipment (for example, client device 104) shared data communication and information 140.Data communication can use various customizations
Or standard wireless protocol (for example, IEEE 802.15.4, Wi-Fi, ZigBee, 6LoWPAN, thread, Z-Wave, blue-tooth intelligence,
ISA100.11a, WirelessHART, MiWi etc.) and/or it is various customization or standard cable agreement (for example, Ethernet,
HomePlug etc.) or any other suitable communication protocol (be included in still undeveloped communication before the submission date of this document
Agreement) any one of execute.
In some embodiments, device for projecting 108, electronic equipment 190 and intelligent home equipment are used as wirelessly or non-wirelessly
Repeater.In some embodiments, the first device for projecting in device for projecting 108 is via wireless router and device for projecting
The second device for projecting in 108 and intelligent home equipment communication.Device for projecting 108, electronic equipment 190 and intelligent home equipment can
Further communicate with one another via connection (for example, network interface 160) and the network of such as internet 110.Device for projecting 108, electricity
Sub- equipment 190 and intelligent home equipment can be by internets 110 and intelligent server system 140 (in being referred to herein as
Entreat server system and/or cloud computing system) communication.Optionally, intelligent server system 140 can with device for projecting 108
Associated manufacturer, support entity or service provider and the media content shown to user are associated.
Therefore, intelligent server system 140 may include handling the audio input collected by Speech-activated electronic equipment
Voice secondary server 112, is created based on distribution the one or more content hostings 104 for providing shown media content
The cloud projection service server in the Virtual User domain of device end and the distributed apparatus terminal in holding Virtual User environment
Record device registry 118.The example of distributed apparatus terminal include but not limited to device for projecting 108, media output set
Standby 106, electronic equipment 190 and intelligent home equipment.In some embodiments, these distributed apparatus terminals are linked to virtually
User account (for example, Google user accounts) in user domain.
In some embodiments, network interface 160 includes general networks equipment (for example, router).The intelligent matchmaker of Fig. 1
Body environment 100 further includes hub device 180 direct or that network 110 is communicatively coupled to via network interface 160.Line concentration
Device equipment 180 is further communicably coupled to the more sensing networks of the above intelligence and connects equipment (for example, device for projecting 108, electronics are set
Standby 190, intelligent home equipment and client device 104).Each in the equipment of these network connections optionally uses at least
In smart media environment 100 it is available one or more radio circuit (such as ZigBee, Z-Wave, Insteon,
Bluetooth, Wi-Fi and other radio circuits) it is communicated with hub device 180.In some embodiments, hub is set
Standby 180 and can be via smart phone, household controller, above-knee with the equipment of hub device/be coupled to hub device
The application that is run on type computer, tablet computer, game console or similar electronic equipment is controlled and/or is interacted.One
In a little embodiments, the user of such controller application can check the shape of the network access device of hub device or coupling
Hub device is configured to interoperate with the equipment for being newly introduced into home network, debug new equipment and adjustment or check by state
The setting etc. of the equipment connected.
Fig. 2A is the throwing according to the wherein Speech-activated electronic equipment 190 and smart media environment 100 of some embodiments
The Example Operating Environment that jet device 108, client device 104 or server system 140 interact.Speech-activated electronic equipment 190
It is configured as receiving audio input from the environment close to Speech-activated electronic equipment 190.Optionally, electronic equipment 190 stores sound
Frequency inputs and at least partly in processing locality audio input.Optionally, electronic equipment 190 will be received via communication network 110
To audio input or the audio input of part processing be sent to voice secondary server 112 for being further processed.Projection
Equipment 108 is configured as obtaining media content or internet content from one or more content hostings 114 for being shown in coupling
It closes on the output equipment 106 of device for projecting 108.As described above, device for projecting 108 and Speech-activated electronic equipment 190 with
Family links each other in domain, and more specifically, associated with each other via the user account in user domain.The information of device for projecting 108
It is stored in association in device registry 118 with user account with the information of electronic equipment 190.
In some embodiments, device for projecting 108 and Speech-activated electronic equipment 190 do not include any display screen,
And it is necessarily dependent upon client device 104 and user interface is provided during debugging process.Specifically, client device 104 is pacified
Equipped with application, which enables user interface that adjustment is promoted to be arranged in the new device for projecting near client device 104
108 or new Speech-activated electronic equipment 190.User can initiate in the user interface of client device 104 to needing to adjust
The adaptation process of suitable new device for projecting 108 or electronic equipment 190.After receiving adjustment request, client device 104 is established
With the short range communications link of the new device for projecting 108 or electronic equipment 190 that need to adjust.Optionally, it is based on near-field communication
(NFC), bluetooth, low-power consumption bluetooth (BLE) etc. establish short range communications link.Client device 104 then will be with WLAN
(WLAN) associated radio configuration data transfer is to new device for projecting 108 or electronic equipment 190.Radio configuration data are at least wrapped
Wlan security code (that is, Service Set Identifier (SSID) password) is included, and with optionally including SSID, Internet protocol (IP)
Location, proxy configurations and gateway configuration.After receiving radio configuration data via short range communications link, new device for projecting 108
Or electronic equipment 190 decodes and restores radio configuration data, and WLAN is added based on radio configuration data.
Further user domain information is entered in the user interface shown on client device 104, and for will newly throw
Jet device 108 or electronic equipment 190 are linked to the account in user domain.Optionally, further user domain information is via short range communication
Link is transferred to new device for projecting 108 or electronic equipment 190 together with wireless communication data.Optionally, added in new equipment
After entering WLAN, further user domain information is transferred to new device for projecting 108 or electronic equipment 190 via WLAN.
Once device for projecting 108 and electronic equipment 190 have been adapted into user domain, device for projecting 108, output equipment
106 and its associated media playing activities can be controlled via two control paths (control path A and control path B).Root
According to control path A, device for projecting application or one or more media play applications on client device 104 are used for
Control device for projecting 108 and its associated media playing activities.Alternatively, according to control path B, 190 quilt of electronic equipment
For realizing device for projecting 108 and its associated media playing activities exempt to regard and hands-free control is (for example, in output equipment
The playback that media content plays on 106, and activation are currently displayed at the closed caption of the media content on output equipment 106).
Fig. 2 B are the example flow diagrams of controlling media play process 250, and control path B shown in A is controlled according to fig. 2
Device for projecting 108 and its relevant media playing activities.Assistant server (such as voice secondary server 112) is configured as propping up
Speech-activated electronic equipment 190 is held, control is with the interaction for searching for stack and according to the undressed voice collected by electronic equipment 190
Which media action input needs to execute to parse.Assistant server sends (202) to cloud projection service server 116 and asks,
Media action is converted to action script by cloud projection service server 116, and then the action script can be by target device for projecting
108 execute.There are two types of possible execution routes for action script.According to the first execution route A, it is in the response to assistant server
Middle return.This is " local path (local path.) ".If target device for projecting 108 is Speech-activated electronic equipment 190
Body, then action script is easy obtains from assistant server.Alternatively, according to the second execution route B, cloud projects service server
Action script is dispatched to equipment by 116 via cloud messaging service.This is long-range execution route.In some embodiments,
Two execution routes carry out parallel, and target device for projecting 108 ignores the action script of second arrival.unique_
Command_id is associated with each ExecuteCloudCastCommand.
In some embodiments, voice secondary server is carried out using CloudCastCommand
The remote procedure call (RPC) of executeCastCommand, as follows：
message CloudCastCommand{
Optional string unique_command_id=1；
Optional string source_device_id=2；
Optional stnng target_device_id=3；
Optional string app_id=4；
Optional string content_id=5；
Optional string content_anth_token=6；
}
message ExecuteCastCommandRequest{
Optional CloudCastCommand cast_command=1；
}
message ExecuteCastCommandResponse{
Optional CloudCastCommand cast_command=1；
Optional string cast_action_script=2；
}
Once being ordered, which is just maintained in by unique_ by cloud projection service server 116
In the permanently storing of command_id and target_device_id keyings.When for identical target device for projecting 108 or electricity
When sub- equipment 190 sends out another order or when/executionReport endpoints receive successfully/error condition when,
CloudCastCommand will be replaced or remove.Then cloud projection service server 116 is cleared up outmoded (in special time period
Inside not yet complete) order, and generate projection action script (Cast Action Script).Once generating projection acts foot
This, cloud projects service server 116 and returns to script, and if (source_device_id in RPC responses！=target_
Device_id) then Google cloud messenger services is used to send response.
In some embodiments, device for projecting 108 reports (204) its shape during and after executing projection action script
State is as follows：
message ReportExecutionStatusRequest{
enum StatusCode{
UNKNOWN=0；
SUCCESS=1；
ERROR=2；
QUEUED=3；
IN_PROGRESS=4；
}
Optional string device_id=1；
Optional string unique_command_id=2；
Optional StatusCode status_code=3；
Individual part in the // action script reported in the request.
Optional string last_action=4；
// include the equipment for customizing status data based on state code or error code.
// for example, being directed to " CAST::Customization error character will be arranged in EINJECTWRAPPED " error codes in this byte
String
Optional string custom_data=5；
// error code is the character string defined in go/castactionscript.
Optional string error_code=6；
}
message ExecutionReportResponse{
//TBD
}
In some embodiments, as long as its state changes, device for projecting 108 updates its state using status message.
In some embodiments, device for projecting 108 periodically send heartbeat with notify cloud projection service server 116 they deposit
, and last_action_time fields have been updated to the time (second) since the epoch by cloud projection service server 116.Cloud
Projection service server 116 optionally will execute status message via cloud messenger service and be sent to source device (such as voice activation
Electronic equipment 190).Speech-activated electronic equipment 190 then will be directed to TTS and S3 is called in playback.
Voice activation media play on media output devices
With reference to Fig. 2A, all it is adapted in device for projecting 108 and Speech-activated electronic equipment 190 and is linked to common user domain
Later, Speech-activated electronic equipment 190 is used as Voice User Interface and is not related to enabling media content streaming being transmitted to
And the device for projecting 108 of remote controler, client device 104 or other the second screen equipments exempt to regard and hands-free control.For example, with
Family can provide voice command, such as " Lady Gaga (Play Lady Gaga on Living be played on the loud speaker of parlor
Room speakers.)”.Lady Gaga music tracks or video clipping are streamed to and " parlor loud speaker (Living
Room speakers.) " associated device for projecting 108.It is not related to client device 104, is not related in client device yet
The application of any device for projecting or media play application loaded on 104.
Cloud projection service 116 is agency service, and Speech-activated electronic equipment is otherwise communicatively linked to device for projecting 108,
And device for projecting 108 can be projected in the case of any application on not being related to client device 104.Specifically, language
Sound message is recorded by electronic equipment 190, and speech message is configured as that the media on media output devices 106 is asked to be broadcast
It puts.Optionally, electronic equipment 190 in local part handles speech message.Optionally, electronic equipment 190 is via communication network
The speech message of speech message or part processing is sent to voice secondary server 112 for being further processed by 110.Cloud is thrown
It penetrates service server 116 and determines that speech message includes the first media play request, and the first media play request is included in matchmaker
The user voice command of media content is played on body output equipment 106 and 106 are specified to the user speech of media output devices.With
Family voice command also include at least need play the first media play application (for example, YouTube and Netflix) information and
Media content (such as Lady Gaga music).
Specified according to the voice to media output devices, the cloud projection service server 116 in device registry 118 includes
Device for projecting that is associated with electronic equipment 190 in the customer domain and being coupled to media output devices 106.Device for projecting 108
It is configured as executing one or more media plays and applies and played from one or more matchmakers for control media output devices 106
The media content that body content hosting 114 receives.Then, cloud projects service server 116 and sends the second media to device for projecting 108
Playing request comprising the media content that the information and needs of the first media play application play.It is projected by cloud once receiving
The information that service server 116 is sent, device for projecting 108 execute the first media play and apply and control media output devices 106
Play requested media content.
In some embodiments, specified to the user speech of media output devices 106 includes that destination media output is set
Standby description.Cloud projection service server 116 identifies according to the description to destination media output devices in registration table multiple
Destination media output devices in media output devices.In some embodiments, destination media output devices are retouched
State the brand (" Samsung TV (Samsung TV) ") or position (" my living-room TV including at least media output devices 106
(Samsung TV)”)。
Voice activation closed caption is shown
The accessible legal requirement electronic communication of the United States Federal and information technology (such as website, Email or network documentation)
It is addressable, and the user for being necessary for becoming deaf or hearing is bad provides closed caption option.With reference to figure 2A, in device for projecting
108 and Speech-activated electronic equipment 190 be all adapted and be linked to after common user domain, Speech-activated electronic equipment 190 can
It enables to exempt from depending on and hands-free controls the tool currently shown on media output devices 106 to be used as Voice User Interface
There is the closed caption of media content.Specifically, voice command is translated into and is opened for subtitle to be sent to cloud by speech recognition system
Project the recognizable message of service.Cloud projection service explains the message and sends commands to the media on device for projecting
It plays and applies (for example, YouTube).Media play application receives the order and subtitle track is presented based on the message.In this way, with
Family can open and close subtitle using voice on media output devices.This control that closed caption is shown is not related to appointing
What remote controler, client device 104 or other second screen equipment, is not also related to loading any on client device 104
Device for projecting is applied or media play application.Therefore, the voice activation control that closed caption is shown meets federal accessible requirement,
Especially suitable for deaf person or the user of dysaudia.
When user wants to initiate the display to the closed caption of the media content currently shown, user sends and is set by electronics
The speech message (for example, " opening closed caption (Turn on closed captioning.) ") of standby 190 record.Optionally,
Electronic equipment 190 is partly in processing locality speech message.Optionally, electronic equipment 190 handles speech message or part
Speech message is sent to voice secondary server 112 for further processing.Cloud projects service server 116 and determines speech message
It is that the first closed caption initiates request, and it includes initiating the user voice command of closed caption that the first closed caption, which initiates request,
To activate the user speech to showing equipment 106 of the media content of closed caption specified for it with playing.In some embodiment party
In formula, the speech message recorded is directly sent to cloud and projects service server 116 by electronic equipment 190.Cloud projection service clothes
Business device 116 parses speech message by the way that speech message is forwarded to voice secondary server 112 and identifies user voice command
It is specified with the user speech to destination media device to determine that speech message is that the first closed caption initiates request and from language
Sound secondary server 112 receives user voice command and is specified to the user speech of destination media device.
It is specified according to display equipment, cloud projection service server 116 identifies in device registry 118 in user domain
In device for projecting 108 associated with electronic equipment 190 and being coupled to specified display equipment 106.Device for projecting 108 by with
It is set to and executes media play application, the media content received from media content trustship is shown for controlling specified display equipment.
In some embodiments, electronic equipment 190 and device for projecting 108 are all associated with the user account of user domain.User account
Can be Google user accounts.
Then, the second closed caption is initiated the display equipment that request is sent to and specifies by cloud projection service server 116
The device for projecting of coupling.Once receiving the information sent by cloud projection service server 116, device for projecting 108 executes media
It plays application and opens the media content being currently displayed in specified display equipment 106 to control specified display equipment 106
Closed caption simultaneously initiates request display closed caption according to the second closed caption.In some embodiments, closed caption according to
Default hidden Subtitle Demonstration specification is shown in specified display equipment.
In some embodiments, it initiates to ask according to the first closed caption, cloud, which projects service server 116 and determines, to be hidden
The Display specification of subtitle.Second closed caption initiates the Display specification that request includes closed caption, and device for projecting is configured
It is applied with control display equipment according to Display specification display closed caption to execute media play.In addition, in some embodiments
In, the Display specification of closed caption includes font (such as Arial), font size (such as 12), font color (such as white)
At least one of with background colour (for example, black).In addition, in some embodiments, service server 116 is projected via cloud
Sending the Display specification of closed caption allows user by by self-defined voice command (such as " larger subtitle (larger
Captions) " or " background color is changed into blue (change the background color to blue) ") adjust
The format of its whole closed caption is sent to the closed caption initiation request of device for projecting 108 to update.In addition, closed caption is shown
This voice activation control allow to have any electronic equipment of microphone (for example, mobile phone) to initiate returning for media content
It puts and adjusts the closed caption in media display devices 106.
In some embodiments, electronic equipment, device for projecting and specified display equipment are arranged to closer to each other, but
It is located remotely from cloud projection service system 116, voice secondary server 112 and device registry 118.In some embodiments,
Two or more in cloud projection service system 116, voice secondary server 112 and device registry 118 are integrated in list
In a server.In some embodiments, cloud projection service system 116, voice secondary server 112 and device registry
118 are different from content hosting 114, and media content is supplied to device for projecting 108 for including specified by content hosting 114
It shows in equipment 106.
In some embodiments, specified to the user speech of media output devices 106 includes that destination media output is set
Standby description.Cloud projection service server 116 identifies multiple matchmakers according to the description of destination media output devices in registration table
Destination media output devices in body output equipment.In some embodiments, the description of destination media output devices
Including at least the brand (" Samsung TV (Samsung TV) ") or position (" my living-room TV (my of media output devices 106
Living Room TV)”)。
Voice activation media transmission between media output devices
Fig. 3 is set with client device 104, Speech-activated electronic according to the wherein device for projecting 108 of some embodiments
For 190 or another Example Operating Environment of the server system interaction of smart media environment 100.Smart media environment 100 includes
First device for projecting 108-1 and the first output equipment 106-1 for being coupled to the first device for projecting.Smart media environment 100 also wraps
Include the second device for projecting 108-2 and the second output equipment 106-2 for being coupled to the first device for projecting.Device for projecting 108-1 and
108-2 is alternatively located in and the same position (such as parlor) or two different locations (such as two in smart media environment 100
Room) in.Each 108-2 in device for projecting 108-1 and 108-2 is configured as obtaining media content from media trustship 114
Or internet content is for being shown on the output equipment 106 for being coupled to corresponding device for projecting 108-1 or 108-2.First He
Second device for projecting is all communicatively coupled to cloud projection service server 116 and content hosting 114.
Smart media environment 100 further includes being communicably coupled to cloud projection service server 116 and voice assisted server
112 one or more Speech-activated electronic equipment 190.In some embodiments, Speech-activated electronic equipment 190 is arranged
Independently of device for projecting 108 and output equipment 106.For example, as shown in Figure 1, electronic equipment 190-4 is arranged in no device for projecting
108 or output equipment 106 where room in.In some embodiments, the first electronic equipment 190-1 is arranged to close to
One device for projecting 108-1 and the first output equipment 106-1, for example, the first electronic equipment 190-1, the first device for projecting 108-1 and
First output equipment 106-1 is located in same room.Optionally, the second electronic equipment 190-2 is arranged independently of or close to
Two device for projecting 108-2 and the second output equipment 106-2.
When media content plays on the first output equipment 106-1, user can send language to any electronic equipment 190
Sound order is to ask to play the media content that be transmitted to the second output equipment 106-2.Voice command includes media play transmission
Request.In one case, user can be moved to the forward directions of destination locations in user to be located at the first device for projecting 108-1 attached
Close electronic equipment 190-1 sends out voice command.Alternatively, in another case, user can reach purpose in user
The backward electronic equipment 190-2 near the second equipment 108-2 of position sends out voice command.
Voice command is sent to cloud projection service server 116.Cloud projects service server 116 to the first device for projecting
108-1 sends media display information request to ask on being coupled to the first output equipment 106-1 of the first device for projecting 108-1
The instant media play information for the media content being currently played.Then first device for projecting 108-1 projects service clothes to cloud
Business device 116 return to requested instant broadcast information, include at least the first media play application information (for example,
YouTube), currently playing media content (such as " the Lady Gaga- national anthems-(Lady Gaga-National of Super Bowl 2016
Anthem-Super Bowl 2016) ") and time location related with media content is played.Second device for projecting 108-2 is right
Request is shown from the media that the cloud projection reception of service server 116 includes instant broadcast information, and play letter according to instant afterwards
Breath, executes the first media play application, and the second output equipment 106-2 of control plays media content from time location.
In specific example, when playing music playlist on the first output equipment 106-1, user says " mine
(Play on my living room speakers.) is played on the loud speaker of parlor ".First output equipment 106-1 stops playing
Currently playing song, and the song stopped restarting on the loud speaker of parlor.When song is completed, parlor loud speaker after
Next song on the music playlist previously played on the first output equipment 106-1 is put in continued broadcasting.In this way, when user exists
When being moved around in Intelligence home environment 100, the broadcasting of media content follows user by seamless, while only relating to limited user
Intervene (that is, providing voice command).This seamless delivery of media content is completed according to being operated below one or more：
Voice assistant services (for example, voice secondary server 112) and identifies that it is by media from an output equipment
(source) is transmitted to the user voice command of another output equipment (destination)；
Message transmission including user voice command is projected service server 116 by assistant's service to cloud；
Cloud projects service server 116, and then request source output equipment 106-1 provides the data transmitted needed for Media Stream
Block；
The content of data block depends on affiliate, but generally comprises current media content being played on, current matchmaker
The flow of the position and current media content that hold in vivo；
Optionally, the content of data block includes the container of current media content (for example, the played column belonging to media content
Table) position in playlist of information and current media content；
Cloud projects service server 116 and source device is notified to stop playing media content；
Then, cloud projects service server 116 in destination (that is, the identical reception run on the output equipment of source
Device application) on load receiver appropriate and apply (for example, media play application).
The data block is sent collectively to destination by cloud projection service server 116 together with to the instruction that receiver is applied
Device for projecting 108-2 is to restart the transmission of media content；And
Receiver application explains data block correspondingly to restart media content.
Specifically, in server end, by cloud projection service server 116 implement the broadcasting for showing media content from
The method that source media output devices are moved to destination media output devices.The cloud projection reception of service server 116 is set by electronics
The speech message that standby 190-1 or 190-2 is recorded, and determine that speech message includes media transmission request.As explained above, electronics
Equipment can be arranged to be located near the source device for projecting 108-1 at first position, and the destination for being located at the second place is thrown
Near jet device 108-2, or independently of both source and destination device for projecting.In some embodiments, electronic equipment
190, source device for projecting 108-1 and destination device for projecting 108-2 is projected with by cloud in the user domain that service server 116 manages
User account it is associated.User account can be Google user accounts.
Media transmission request in user voice command includes that media content being played on is transmitted to destination media
The user voice command of output equipment 190-2 and specified to the user speech of destination media output devices 190-2.In some realities
It applies in mode, after receiving the speech message by electronic equipment 190-1 or 190-2 record, cloud projects service server 116
Speech message is forwarded to voice assisted server 112, voice assisted server 112 parses speech message and identifies user speech
It orders and specified to the voice of destination media output devices, and user speech life is received from voice secondary server 112
It enables and specified to the voice of destination media output devices 106-2.
Cloud projection service server 116 obtains the instant of the media content being currently played from source device for projecting 108-1
Media play information.Instant broadcast information include at least the information of the first media play application, currently playing media content with
And with play the relevant time location of media content.When user asks media content being moved to destination output equipment 106-2
When, time location can be recorded.In some embodiments, cloud projection service server 116 identifies current just defeated in source media
Go out media content being played at equipment 106-1.Cloud projection service server 116 identified in device registry 118 with
Source device for projecting 108-1 associated with electronic equipment 190 and being coupled to source media output devices 106-1 in the domain of family.Then,
Cloud projects service server 116 and sends media information request to source device for projecting 108-1, to be connect from source device for projecting 108-1
Receive instant media play information.
Specified according to the voice to destination media output devices, cloud projects service server 116 in device registry 118
Middle identification is associated with electronic equipment in the customer domain and is coupled to the projection of the destination of destination media output devices 106-2
Equipment 108-2.Destination device for projecting 108-2 is configured as executing one or more media play-back applications, for controlling
Destination media output devices 106-2 plays the media content received from one or more media content trustships 114.In some realities
It applies in mode, specified to the user speech of destination media output devices 106-2 includes destination media output devices 106-2
It describes (for example, the brand of output equipment 106-2 and position).According to the description of destination media output devices 106-2, cloud projection
Service server 116 identifies the destination media output devices 106-2 in multiple media output devices in registration table 112.Cause
This, user needs not be provided to be identified with the accurate device of the record matching in device registry 112, and cloud projects service server
116 can determine destination media output devices 106-2 based on the description of destination media output devices 106-2.
After with obtaining instant broadcast information and identifying purpose device for projecting 108-2, cloud project service server 116 to
Device for projecting 108-2 transmissions in destination include the media play request of instant media play information, thus enable destination projection
Equipment 108-2 executes control destination media output devices 106-2 and is broadcast from the first media of time location broadcasting media content
Put application.In some embodiments, according to user voice command, cloud projects service server 116 also to be stopped asking by media
It is sent to source device for projecting 108-1, coupled source device for projecting is controlled to make source device for projecting 108-1 be able to carry out
108-1 is to abandon the first media play application of the broadcasting of the media content on the media output devices 106-1 of source.
The media conveying method extracts media stream leaving the required data of service, and it is directly transmitted with streaming
Service provider places, so that they can define the parameter needed for the stream that transmission is currently played (for example, Google is projected
Agreement).This makes the design of the present invention very flexible, to adapt to any kind of media affiliate or Media Stream.In addition, it
It also utilizes cloud infrastructure (to project and service) transmission message by cloud and coordinates the playback between source device and destination equipment.This
This case where being transmitted in no mutual any knowledge or these device for projecting in same wireless LAN is allowed to issue
It is raw.
Scalability, flexibility and Information Security are also realized in the media transmission that service server 116 is projected via cloud.It passes
The data block needed for media is sent especially loosely to define, to adapt to the quantity of content provider partners and the number of stream type
Amount.Stream may be single song, playlist, live TV stream, advertisement, automatic broadcasting video and many other content formats.It keeps
The flexibility of data block and the dependence of affiliate can allow single method to be suitable for all types of Media Streams.In addition,
Source and destination device for projecting is independently connected by allowing cloud to project service, does not need these equipment and is connected to each other, in identical
WLAN or each other have knowledge.In addition, CCS does not have disintermediation.It is sent out between the receiver application of source and destination ground
The data sent are opaque for cloud projection service server 116.This allows the confidential details of the media session about transmission
Leave the affiliate using cloud projection service for.
The physical features of Speech-activated electronic equipment
Fig. 4 A and Fig. 4 B are the front view 400 and rearview of the Speech-activated electronic equipment 190 according to some embodiments
420.Electronic equipment 190 is designed to many regions that are warm and tempting, and being suitable for family naturally.Electronic equipment 190
Array including one or more microphones 402 and full color LED 404.Full color LED 404 can be hidden in the top table of electronic equipment 190
Below face, and it is invisible to user when user does not light.In some embodiments, full color LED array 404 is physically arranged
Row are circlewise.In addition, the rear side of electronic equipment 190 optionally includes the power connector 408 for being configured to coupled to power supply.
In some embodiments, electronic equipment 190 is presented without the clean appearance of the visible button, and and electronic equipment
190 interaction is based on voice and touch gestures.Alternatively, in some embodiments, electronic equipment 190 includes limited quantity
Physical button (for example, the button 406 of side behind), and be based further in addition to voice with the interaction of electronic equipment 190
Except press lower button and touch gestures.
One or more speakers are arranged in electronic equipment 190.Fig. 4 C are the perspective views of Speech-activated electronic equipment 190
440, it illustrates the raising in open configuration included in the pedestal 410 of electronic equipment 190 according to some embodiments
Sound device 422.Fig. 4 D and Fig. 4 E are the side view 450 and expanded view 460 of Speech-activated electronic equipment 190 respectively, and it illustrates roots
According to the electronic building brick of some embodiments being included in.Electronic equipment 190 includes full color LED array 404, one or more
Microphone 402, loud speaker 422, double frequency-band WiFi 802.11ac radio, bluetooth LE radio, ambient light sensor, the ends USB
Mouth, processor and the memory for storing at least one program for being executed by processor.
In addition, in some embodiments, electronic equipment 190 further includes the top table for being configured as detection electronic equipment 190
The touch-sensing array 424 of touch event on face.Touch-sensing array 424 is arranged and is hidden in the top of electronic equipment 190
Lower face.In some embodiments, the top surface in the circuit board including through-hole array is arranged in touch-sensing array 424
On, and full color LED is arranged in the through-hole of circuit board.When circuit board is located at immediately below the top surface of electronic equipment 190, entirely
Color LED404 and touch-sensing array 424 are also all disposed within the underface of the top surface of electronic equipment 190.
Fig. 4 F (1)-Fig. 4 F (4) show the touch-sensing in Speech-activated electronic equipment 190 according to some embodiments
Four touch events detected on array 424.With reference to figure 4F (1) and Fig. 4 F (2), touch-sensing array 424 detects voice and swashs
Rotational slide on the top surface of electronic equipment 190 living.In response to detecting slid clockwise, Speech-activated electronic equipment 190
Increase the volume of its audio output, and in response to detecting that sliding counterclockwise, Speech-activated electronic equipment 190 reduce its audio
The volume of output.With reference to figure 4F (3), touch-sensing array 424 detects clicking on the top surface of Speech-activated electronic equipment 190
It touches.In response to detecting that the first tap touch, Speech-activated electronic equipment 190 implement the control operation of the first media (for example, broadcasting
Put specific media content), and in response to detecting that the second tap touch, Speech-activated electronic equipment 190 implement the second media
Control operation (for example, specific media content that pause is currently played).With reference to figure 4F (4), touch-sensing array 424 detects
Double-click on the top surface of Speech-activated electronic equipment 190 touches (for example, two continuous touches).Two continuous touches are separated
Time span is less than predetermined length.However, when they more than the duration of predetermined length to separate, two continuous touch quilts
It is considered to click touch twice.It being touched in response to detecting to double-click, Speech-activated electronic equipment 190 initiates hot word detecting state,
Wherein electronic equipment 190 is listened to and identifies one or more hot words (for example, predefined keyword).Know in electronic equipment 190
Do not go out before hot word, electronic equipment 190 does not send any sound to voice secondary server 112 or cloud projection service server 118
Frequency inputs.
In some embodiments, full color LED array 404 is configured as showing visual pattern collection according to LED design language
Close, instruction detect slid clockwise on the top surface of Speech-activated electronic equipment 190, slide counterclockwise, click or
It double-clicks.For example, full color LED array 404 can be with sequential illumination to track respectively as shown in Fig. 4 F (1) and Fig. 4 F (2) clockwise
Or sliding counterclockwise.It is explained about the speech processes state with electronic equipment 190 below with reference to Fig. 4 G and Fig. 4 H (1)-Fig. 4 H (8)
The more details of associated visual pattern.
Fig. 4 F (5) show the button 406 on the rear side of Speech-activated electronic equipment 190 according to some embodiments
On example user touch or pressing.Button 406, the microphone of electronic equipment 190 are touched or pressed in response to the first user
It is muted, and touches in response to second user or press button 406, the microphone of electronic equipment 190 is activated.
The LED design language of visual effect for Voice User Interface
In some embodiments, the simplicity and low cost of electronic equipment 190 are given, electronic equipment 190 includes panchromatic
Light emitting diode (LED) array rather than show screen entirely.The illumination of full color LED array is configured using LED design language, and
Enable the different visual patterns of the different phonetic processing state of instruction electronic equipment 190.LED design language includes color, pattern
With the grammer of special exercise, it is applied to the fixed set of all-colour LED.Element in language is combined with using electronic equipment
Specific equipment state is visually indicated during 190.In some embodiments, the illumination of full color LED is intended to clearly retouch
The passive of the electronic equipment 190 being painted in other important states is listened to and actively listens to state.The placement of full color LED meets electricity
The physical constraint of sub- equipment 190, and full color LED array can be used for being based on spy by third party original equipment manufacturer (OEM)
Determine the loud speaker of technology (for example, Google assistant) manufacture.
When based on particular technology in the loud speaker manufactured by third party OEM use full color LED array when, full color LED and
LED design language is configured as being suitble to the correspondence physical user interface of OEM loud speakers.In this case, OEM loud speakers are set
Standby state keeps identical, and the particular visual pattern for representing equipment state may be different (for example, the color of full color LED
May be different, but can be shown with similar animation effect).
In Speech-activated electronic equipment 190, when the audio input collected from its ambient enviroment of processing of electronic equipment 190 but
When not storing audio input or audio input being sent to any remote server, generation is passively listened to.On the contrary, working as electronic equipment
190 audio inputs collected from its ambient enviroment of storage and/or when sharing audio input with remote server, occur actively receipts
It listens.According to some embodiments of the application, feelings of the electronic equipment 190 in the privacy for the user that will not destroy electronic equipment 190
The audio input in its ambient enviroment is only passively listened under condition.
Fig. 4 G are according to the top view of the Speech-activated electronic equipment 190 of some embodiments, and Fig. 4 H show root
According to the six example visual patterns for being used to indicate speech processes state of some embodiments shown by full color LED array.
In some embodiments, electronic equipment 190 does not include any display screen, and compared with display screen entirely, full color LED 404
Simple and inexpensive visual user interface is provided.Full color LED can be hidden in below the top surface of electronic equipment, and ought not
It is invisible to user when being lit.With reference to Fig. 4 G and Fig. 4 H, in some embodiments, full color LED array 404 is physically arranged
In ring.For example, as shown in Fig. 4 H (6), full color LED array 404 can be with sequential illumination with tracing figure 4F (1) and Fig. 4 F respectively
(2) it is slided clockwise or counterclockwise shown in.
A kind of method is realized at electronic equipment 190 for visually indicating speech processes state.Electronic equipment 190 passes through
The audio input of the environment near electronic equipment is collected by one or more microphones 402, and handles audio input.Processing
Including one or more of the voice input for identifying and responding the user in environment.Electronic equipment 190 makes a reservation for from multiple
The state of processing is determined in the speech processes state of justice.For each in full color LED 404, the identification of electronic equipment 190 with
The associated corresponding predetermined LED illumination specification of identified speech processes state.When lighting regulations include that LED illumination continues
Between, one or more of pulsation rate, duty ratio, color sequences and brightness.In some embodiments, electronic equipment 190 is logical
At least one predetermined LED illumination specification (for example, colour sequential) of customization full color LED 404 is crossed according to one in multiple users
Identity determine that speech processes state is associated with one in multiple users, and identify the predetermined LED of full color LED 404
Lighting regulations.
In addition, in some embodiments, according to identified speech processes state, the color of full color LED includes color
Predetermined set.For example, referring to Fig. 4 H (2), Fig. 4 H (4) and Fig. 4 H (7)-(10), the predetermined set of color includes Google product
Board color, including blue, green, yellow and red and full color LED array are divided into four quadrants, each quadrant and Google
One kind of brand color is associated.
According to the LED illumination specification of full color LED identified, electronic equipment 190 synchronizes the illumination of full color LED array to carry
For the visual pattern of speech processes state determined by instruction.In some embodiments, the vision of speech processes state is indicated
Pattern includes multiple discrete LED illumination pixels.In some embodiments, visual pattern includes starting segment, cycle region
With termination segment.Cycle region continues time span associated with the LED illumination duration of full color LED and is configured as
Length with speech processes state.
In some embodiments, electronic equipment 190, which has, different is set by what LED design language indicated more than 20
Standby state (including multiple predefined speech processes states).Optionally, multiple predefined speech processes states include hot word
Detecting state one or more of listens to state, thinking cap and responsive state.
1. hot word detecting state and listening to state
In some embodiments, electronic equipment 190 is listened to and identifies one or more of hot word detecting state hot word
(for example, predefined keyword).Before electronic equipment 190 identifies hot word, electronic equipment 190 is not to audio ancillary service
Device 112 or cloud projection service server 118 send any audio input.When detecting hot word, when microphone record is further
The beginning of electronic equipment 190 operates in listening to state when being sent to the audio input that cloud is further processed.In listening to pattern
In, the audio input since predetermined time position (for example, two seconds before detecting hot word) is sent to voice auxiliary
Server 112 or cloud project service server 118, thus promote the seamless search for more natural talk formula flow.
Therefore, in some embodiments, it is to work as to detect one or more according to speech processes state determined by determination
The hot word detecting state occurred when a predetermined hot word, the array of all-colour LED, which is divided into, to be arranged alternately and is configured to sequential illumination
Multiple diode groups, and the diode in each in multiple diode groups is lighted in different colors.In addition, in some realities
It applies in mode, is when actively to receive the voice from environment defeated for electronic equipment according to speech processes state determined by determination
Enter and what is occurred when the voice received input is supplied to remote server listens to state, all all-colour LEDs are with solid color
It lights, and each full color LED is illuminated with different and variation brightness.
As shown in Fig. 4 H (1), (3) and (5), visual pattern can be configured as people associated with speech processes state
Class reaction (for example, breathing, flicker, blink and sliding) is consistent.For example, using the ground with strongest influence power of Google brand colors
One of side, the careful wake-up rotation after soft breathing animation indicate patient, earnestly listen attentively to hat in hand.These color sheets
Body allows people to associate brand sense and the embodiment of Google voice assistants.The dead angle of these elements and equipment is contrasted, to show
Go out gem-pure record and recording status.
2. thinking deeply pattern or operating mode
Specifically, in some embodiments, according to determine speech processes state be when electronic equipment handling from
The thinking cap occurred when the voice input that family receives, the RGB diodes of increasing number are in the LED illumination duration
First illumination period is lit, the second illumination period phase of the RGB diodes of fewer and fewer quantity after the first illumination period
Between be lit.This visual pattern reacts consistent with the mankind that people are thinking deeply.Optionally, microphone 402 is in thinking pattern
Middle closing.
With reference to Fig. 4 H (3), Fig. 4 H (5) and Fig. 4 H (6), use and progress bar and other kinds of number in visual pattern
Waiting signal most like movement indicates thinking pattern.In some embodiments, white with chase animation and be used together.Product
Board color does not deliberately use and other speech processes states is preferably distinguished contrast and highlighted to provide herein.
3. response modes or speaking mode
Alternatively, in some embodiments, according to determine speech processes state be when electronic equipment in response to from
The input of voice that family receives and responsive state that when broadcasting speech message occurs, the subset of full color LED is with different and change bright
The solid color of degree is lighted, and the brightness change of each subset of full color LED is associated with voice from the user input
Speech speed it is consistent.In some embodiments, speaking mode is the place that voice assistant shows its seal.With visual pattern
It uses color set (for example, Google brands color) so that full color LED visually indicates the closing to speech polling, i.e.,
The problem is answered.
Each equipment involved in smart media environment
Fig. 5 is to show to be applied to collect in smart media environment 100 as speech interface according to some embodiments
The block diagram of the exemplary electronic device 190 of user voice command.Electronic equipment 190 generally includes one or more processing units
(CPU) 502, one or more network interfaces 504, memory 506 and for interconnecting these components (sometimes referred to as chipset)
One or more communication bus 508.Electronic equipment 190 includes being convenient for one or more input equipments 510 input by user, all
Button 406, touch-sensing array as shown in Fig. 4 A- Fig. 4 H and one or more microphones 402.Electronic equipment 190 further includes
One or more output equipments 512 comprising one or more speakers 422 and full color LED array 404.
Memory 506 includes high-speed random access memory, and such as DRAM, SRAM, DDR RAM or other arbitrary accesses are solid
State memory devices；And nonvolatile memory is optionally included, such as one or more disk storage equipments, one or more
A optical disc memory apparatus, one or more flash memory devices or other one or more non-volatile solid state memory equipment.Storage
Device 506 optionally includes one or more storage devices far from one or more processing units 502.Memory 506 can replace
Nonvolatile memory in selection of land memory 506 includes non-transitorycomputer readable storage medium.In some embodiments
In, non-transitory computer-readable storage media storage following procedure, module and the data knot of memory 506 or memory 506
Structure or its subset or superset：
Operating system 516 comprising for handling various basic system services and executing the processes of hardware dependent tasks；
Network communication module 518, for via one or more network interfaces 504 (wired or wireless) and one or
Electronic equipment 190 is connected to other equipment by multiple networks 110 (internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc.)
(for example, server system 140, device for projecting 108, client device 104, intelligent home equipment 120 and other electronic equipments
190)；
Input/output control module, for via one or more input equipments 510 receive input, via one or
Multiple output equipments 512 enable the presentation of information at electronic equipment 190, including：
Zero speech processing module 522, for handling the audio input collected in the environment around electronic equipment 190 or language
Sound message, or prepare the audio input collected or speech message in voice secondary server 112 or cloud projection service clothes
It is engaged in handling at device 118；
Zero LED control module 524, for generating vision on full color LED 404 according to the equipment state of electronic equipment 190
Pattern；
Zero touch-sensing module 526, for the touch event on the top surface of sensing electronic equipment 190；And
The voice activated device data 530 of data associated with electronic equipment 190 are at least stored, including：
Zero speech ciphering equipment setting 532, for storing and the associated information of electronic equipment 190 itself, including common equipment
(for example, service layer, unit type, memory capacity, processing capacity, communication capacity etc.), user account in user domain are set
Information and Display specification associated with the one or more visual patterns shown by full color LED 536；And
Zero voice control data 534, for store audio signal related with the speech interface function of electronic equipment 190,
Speech message, response message and other data.
Specifically, Display specification associated with the one or more visual patterns shown by full color LED 536 includes and one
Each associated predetermined LED illumination specification in a or multiple visual patterns.For each in full color LED, illumination
Specification includes LED illumination duration associated with corresponding visual pattern, pulse rate, duty ratio, colour sequential and brightness
One or more of.Each visual pattern corresponds at least one speech processes state.
Each in element identified above can be stored in one or more of above-mentioned memory devices
In, and corresponding to the instruction set for executing above-mentioned function.Module identified above or program (that is, instruction set) are not required to
To be implemented is individual software program, process, module or data structure, and therefore each subset of these modules can be
It is combined in various embodiments or otherwise rearranges.In some embodiments, memory 506 is optionally stored
The subset of module and data structure identified above.In addition, the optionally stored add-on module not described above of memory 506 and
Data structure.
Fig. 6 is the display for showing the media content being applied in smart media environment 100 according to some embodiments
The block diagram of the example device for projecting 108 automatically controlled.Typically, device for projecting 108 includes one or more processing units (CPU)
602, one or more network interfaces 604, memory 606 and one for interconnecting these components (sometimes referred to as chipset)
Or multiple communication bus 608.
Memory 606 includes high-speed random access memory, and such as DRAM, SRAM, DDR RAM or other arbitrary accesses are solid
State memory devices；And nonvolatile memory is optionally included, such as one or more disk storage equipments, one or more
A optical disc memory apparatus, one or more flash memory devices or other one or more non-volatile solid state memory equipment.Storage
Device 606 optionally includes one or more storage devices far from one or more processing units 602.Memory 606 can replace
Nonvolatile memory in selection of land memory 606 includes non-transitorycomputer readable storage medium.In some embodiments
In, non-transitory computer-readable storage media storage following procedure, module and the data knot of memory 606 or memory 606
Structure or its subset or superset：
Operating system 616 comprising handle various basic system services and execute the process of hardware dependent tasks；
Network communication module 618, for via one or more network interfaces 604 (wired or wireless) and one or more
A network 110 (such as internet, other wide area networks, LAN, Metropolitan Area Network (MAN), cable television system, satellite TV system, IPTV
System etc.) device for projecting 108 is connected to other computers or system (for example, server system 140, intelligent home equipment 120
With client device 104)；
Content decoding module 620, for being decoded to the content signal received from one or more content sources 114,
And the content in decoded signal is output to the output display unit 106 for being coupled to device for projecting 108；
Automatic media display module 624 comprising one or more media plays apply 624, aobvious for controlling media
Show, such as so that media is output to output equipment according to the instant media play information received from cloud projection service server 116
106；And
Device for projecting data 626, at least storage data associated with automatically controlling of showing of media (for example, with
Automatic media output mode and follow-up mode), including：
Zero device for projecting setting 628, for storing information associated with the user account that device for projecting is applied, including account
Family access information, for equipment setting information (for example, service layer, unit type, memory capacity, processing capacity, communication capacity
Deng) and one or more of information for automatic media display control；
Zero media player applications setting 630, for storing the user account with one or more media player applications
Associated information, including the user preference of account access information, media content type, comment historical data and be used for automatic matchmaker
One or more of the information of body display control.
Each element identified above can be stored in one or more aforementioned memory equipment, and corresponded to and be used for
Execute the instruction set of above-mentioned function.Module identified above or program (that is, instruction set) need not be implemented as individually soft
Part program, process, module or data structure, and therefore each subset of these modules can be in various embodiments by group
It closes or otherwise rearranges.In some embodiments, the optionally stored module sum number identified above of memory 606
According to the subset of structure.In addition, memory 606 optionally stored add-on module and data structure not described above.
Fig. 7 is the example of service in the server system 140 shown according to the smart media environment 100 of some embodiments
The block diagram of device.Example server is one in cloud projection service server 116.Server 140 generally includes one or more
Processing unit (CPU) 702, one or more network interface 704, memory 706 and for interconnecting these components (sometimes referred to as cores
Piece group) one or more communication bus 708.Server 140 may include being set convenient for one or more inputs input by user
Standby 710, such as keyboard, mouse, voice command input unit or microphone, touch-screen display, touch sensitive tablet, gesture-capture
Camera or other load buttons or control.In addition, server 140 can use microphone and speech recognition or camera and gesture
It identifies to supplement or replace keyboard.In some embodiments, server 140 includes for example being printed on electronic equipment for capturing
On figure series code image one or more cameras, scanner or optical sensor unit.Server 140 can also wrap
Include one or more output equipments 712, can presentation user interface and display content, including one or more speakers and/
Or one or more visual displays.
Memory 706 includes high-speed random access memory, such as DRAM, SRAM, DDR RAM or other arbitrary accesses are consolidated
State memory devices；And nonvolatile memory is optionally included, such as one or more disk storage equipments, one or more
A optical disc memory apparatus, one or more flash memory devices or other one or more non-volatile solid state memory equipment.Storage
Device 706 optionally includes one or more storage devices far from one or more processing units 702.Memory 706 can replace
Nonvolatile memory in selection of land memory 706 includes non-transitorycomputer readable storage medium.In some embodiments
In, non-transitory computer-readable storage media storage following procedure, module and the data knot of memory 706 or memory 706
Structure or its subset or superset：
Operating system 716 comprising handle various basic system services and execute the process of hardware dependent tasks；
Network communication module 718, for via one or more network interfaces 704 (wired or wireless) and one or
Server system 140 is connected to other and set by multiple networks 110 (internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc.)
Standby (for example, the various servers, device for projecting 108 in server system 140 and intelligent home equipment 120)；
Subscriber interface module 720, for enabling the presentation of information at client device 104 (for example, answering for rendering
With graphical user circle of 826-830, widget, its website and webpage, and/or game, audio and/or video content, text etc.
Face)；
Command execution module 721 for being executed in server end is (for example, game, social networking application, wired home
Using and/or other applications based on web or non-web, for controlling client device 104, device for projecting 108, electronic equipment
190 and intelligent home equipment 120 and check the data that are captured by such equipment), including one of the following or multiple：
Zero device for projecting applies 722, is performed to provide for equipment offer, equipment control and and device for projecting
The server side functionality of 108 associated user account management；
Zero one or more media player applications 724, are performed to provide for associated with corresponding source of media
Media are shown and the server side functionality of nusrmgr.cpl；
Zero one or more intelligent home equipments apply 726, are performed to provide for corresponding intelligent home equipment
120 equipment provides, equipment controls, data processing sum number is it is investigated that the server side functionality seen；And
Zero voice assistance application 728 is performed so that the speech processes of the speech message received from electronic equipment 190 are arranged
Or directly processing speech message is to extract the specified of user voice command and device for projecting 108 or another electronic equipment 190；
And
Server system data 730, which is at least stored, to be automatically controlled with what media were shown (for example, exporting mould in automatic media
In formula and follow-up mode) associated data, including one of the following or multiple：
Zero client device setting 732, for storing associated with client device 104 information (including common equipment
Setting (for example, service layer, unit type, memory capacity, processing capacity, communication capacity etc.)) and show for automatic media
The information of control；
Zero device for projecting setting 734 is wrapped for store the associated information of user account for applying 722 with device for projecting
Account access information, the information being arranged for equipment are included (for example, service layer, unit type, memory capacity, processing capacity, communication
Ability etc.) and one or more of information for automatic media display control；
Zero media player applications setting 736, for storing user's account with one or more media player applications 724
The associated information in family, including the user preference of account access information, media content type, comment historical data and for from
One or more of the information of dynamic media display control；
Zero intelligent home equipment setting 738, for store the associated letter of user account for applying 726 with wired home
Breath, including account access information, for one or more intelligent home equipments 120 information (for example, service layer, unit type,
One or more of memory capacity, processing capacity, communication capacity etc.)；And
Zero voice auxiliary data 740 is wrapped for storing information associated with the user account of voice assistance application 728
Include account access information, for one or more electronic equipments 190 information (for example, service layer, unit type, memory capacity,
One or more of processing capacity, communication capacity etc.).
When the non-transitory that server 140 includes cloud projection service server 116, memory 706 or memory 706 calculates
When machine readable storage medium storing program for executing, following procedure, module and data structure or its subset or superset are stored：
Facility registration module 750, for managing the device registry 118 for being coupled to cloud and projecting service server 116；
Cloud projection applies 760, is used for the user voice command identified in speech message to be relayed to project in cloud
One or more of device for projecting 180, electronic equipment 190 and intelligent home equipment 120 for being coupled in the domain of family；With
Status reporting module 770, for being maintained in the device for projecting 180, the electronic equipment that are coupled in cloud projection user domain
190 and intelligent home equipment 120 state.
Each of the above element can be stored in one or more aforementioned memory equipment, and corresponding to for executing
State the instruction set of function.Module identified above or program (that is, instruction set) need not be implemented as individual software journey
Sequence, process, module or data structure, and therefore each subset of these modules can be combined in various embodiments or
Otherwise rearrange.In some embodiments, the optionally stored module identified above of memory 706 and data knot
The subset of structure.In addition, memory 706 optionally stored add-on module and data structure not described above.
Fig. 8 is the automatic control for showing the media being applied in smart media environment 100 according to some embodiments and showing
The block diagram of the example client end equipment 104 of system.The example of client device include but not limited to mobile phone, tablet computer and
Wearable personal device.Client device 104 generally includes one or more processing units (CPU) 802, one or more networks
Interface 804, memory 806 and one or more communication bus 808 for interconnecting these components (sometimes referred to as chipset).
Client device 104 includes convenient for one or more input equipments 810 input by user, and such as keyboard, mouse, voice command are defeated
Enter unit or microphone, touch-screen display, touch sensitive tablet, gesture-capture camera or other load buttons or control.This
Outside, some client devices 104 supplement using microphone and speech recognition or camera and gesture identification or replace keyboard.One
In a little embodiments, client device 104 includes for capturing the figure for for example printing the series code of figure on an electronic device
One or more cameras, scanner or the optical sensor unit of picture.Client device 104 further includes one or more output equipments
812, allow presentation user interface and display content, including one or more speakers and/or one or more visual displays
Device.Optionally, client device 104 includes the location detecting apparatus 814 of the position for determining client device 104, such as
GPS (HA Global Positioning Satellite) or other geographical location receivers.
Memory 806 includes high-speed random access memory, such as DRAM, SRAM, DDR RAM or other arbitrary accesses are consolidated
State memory devices；And nonvolatile memory is optionally included, such as one or more disk storage equipments, one or more
A optical disc memory apparatus, one or more flash memory devices or other one or more non-volatile solid state memory equipment.Storage
Device 806 optionally includes one or more storage devices far from one or more processing units 802.Memory 806 can replace
Nonvolatile memory in selection of land memory 806 includes non-transitorycomputer readable storage medium.In some embodiments
In, non-transitory computer-readable storage media storage following procedure, module and the data knot of memory 806 or memory 806
Structure or its subset or superset：
Operating system 816 comprising for handling various basic system services and executing the processes of hardware dependent tasks；
Network communication module 818, for via one or more network interfaces 804 (wired or wireless) and one or more
Client device 104 is connected to other equipment by a network 110 (such as internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc.)
(for example, server system 140, device for projecting 108, electronic equipment 190, intelligent home equipment 120 and other client devices
104)；
Subscriber interface module 820, for via one or more output equipments 812 (for example, display, loud speaker etc.)
At client device 104 enable information presentation (for example, for rendering apply 826-830, widget, its website and webpage,
And/or the graphic user interface of game, audio and/or video content, text etc.)；
Input processing module 822, it is defeated for detecting one or more users from one or more input equipments 810
Enter or interacts and explain the input or interaction that detect；
Web browser module 824, for navigating, asking (for example, passing through HTTP) and show its website and webpage,
Including being used for login and device for projecting 108, electronic equipment 190, media application or intelligent home equipment 120, if with user's account
Family is associated, then controls device for projecting 108, electronic equipment 190 or intelligent home equipment 120, and editor and checks and user
The associated setting of account and data；
One or more application (such as game, social networking application, wired home for being executed by client device
Using and/or other applications based on web or non-web, for controlling device for projecting 108, electronic equipment 190 and/or intelligent family
Front yard equipment 120 and check the data captured by such equipment), including one of the following or multiple：
Zero device for projecting applies 826, is performed to provide client functionality for associated with device for projecting 108
Equipment provides, equipment controls and user account management；
Zero voice activated device applies 827, is performed to provide client functionality for related to electronic equipment 190
The equipment of connection provides, equipment controls and user account management；
Zero one or more media player applications 828, are performed to provide for associated with corresponding source of media
Media are shown and the client functionality of nusrmgr.cpl；And
Zero one or more intelligent home equipments apply 830, are performed to provide corresponding intelligent home equipment
120 equipment supply, equipment controls, data processing sum number is it is investigated that the client functionality seen；And
At least storage automatically controls (for example, in automatic media output mode or follow-up mode) phase with what media were shown
The client data 832 of associated data comprising：
Zero client device setting 834, is used for storage and the associated information of client device 104 itself, including common
Equipment setting (for example, service layer, unit type, memory capacity, processing capacity, communication capacity etc.) and be used for automatic media
The information of display control；
Zero device for projecting setting 836 is wrapped for store the associated information of user account for applying 826 with device for projecting
Account access information is included, the information for equipment setting is (for example, service layer, unit type, memory capacity, processing capacity, communication
Ability, etc.) and for automatic media display control information；
Zero media player applications setting 838, for storing user's account with one or more media player applications 828
The associated information in family, including the user preference of account access information, media content type, comment historical data and for from
One or more of the information of dynamic media display control；
Zero intelligent home equipment setting 840, for store the associated letter of user account for applying 830 with wired home
Breath, including account access information, for intelligent home equipment setting information (for example, service layer, unit type, memory capacity,
Processing capacity, communication capacity etc.)；And
Zero voice activated device setting 842, it is associated using 827 user account with voice activated device for storing
Information, including account access information, for electronic equipment setting information (for example, service layer, unit type, memory capacity, place
Reason ability, communication capacity etc.).
In some embodiments, device for projecting applies 827, media player applications using 826, voice activated device
828 and intelligent home equipment cause to show corresponding use on the output equipment 812 of client device using each in 830
Family interface 104.In some embodiments, it is answered using 827, media player using 826, voice activated device with device for projecting
Single cloud is linked to using the user account of 830 associated users project service account with 828 and intelligent home equipment.User
Services account information can be projected using cloud to log on to all device for projecting using 826, voice activated device using 827, matchmaker
Body player application 828 and intelligent home equipment apply 830.In some embodiments, memory 806 or memory 806
Non-transitory computer-readable storage media stores cloud projection and applies 844, is performed to provide and is linked to identical cloud and throw
The device for projecting 108, intelligent home equipment 120 and electronic equipment for penetrating services accounts (for example, Google user accounts) are associated
Function control and user account management client functionality 190.
Each in element identified above can be stored in one or more aforementioned memory equipment, and corresponding
In the instruction set for executing above-mentioned function.Module identified above or program (that is, instruction set) need not be implemented as
Individual software program, process, module or data structure, and therefore each subset of these modules can be in various embodiment party
It is combined in formula or otherwise rearranges.In some embodiments, memory 806 is optionally stored identified above
The subset of module and data structure.In addition, memory 806 optionally stored add-on module and data structure not described above.
Fig. 9 is the example intelligent home equipment 120 in the smart media environment 100 shown according to some embodiment modes
Block diagram.In general, intelligent home equipment 120 includes one or more processing units (CPU) 902, one or more network interfaces
904, memory 906 and one or more communication bus 908 for interconnecting these components (sometimes referred to as chipset).Storage
Device 906 includes high-speed random access memory, and such as DRAM, SRAM, DDR RAM or other random access solid state memories are set
It is standby；And nonvolatile memory is optionally included, such as one or more disk storage equipments, one or more optical disc storages
Equipment, one or more flash memory devices or other one or more non-volatile solid state memory equipment.Memory 906 is optionally
Include one or more storage devices far from one or more processing units 902.Memory 906 or alternatively memory
Nonvolatile memory in 906 includes non-transitorycomputer readable storage medium.In some embodiments, memory
906 or the non-transitory computer-readable storage media storage following procedure of memory 906, module and data structure or its subset
Or superset：
Operating system 916 comprising for handling various basic system services and for executing intelligent home equipment 120
Hardware dependent tasks process；
Network communication module 918, for via one or more network interfaces 904 (wired or wireless) and one or
Intelligent home equipment 120 is connected to other by multiple networks 110 (internet, other wide area networks, LAN, Metropolitan Area Network (MAN) etc.)
Computer or system (such as server system 140, client device 104, device for projecting 108, electronic equipment 190 and other intelligence
Energy household equipment 120)；
Intelligent home equipment module 922, for enable intelligent home equipment 120 realize its specified function (for example,
When intelligent home equipment 120 includes camera 132, for capturing and generating multimedia data stream and make the multimedia data stream
It is sent to client device 104 or server system 140 for continuous feeding or with short paroxysm).
At least the intelligent home equipment data 924 of 926 associated data are arranged with equipment for storage.
In some embodiments, intelligent home equipment 120 is controlled by voice.Specifically, cloud projects service server
116 receive the speech message that electronic equipments 190 record, and determine that the speech message includes smart machine control data (for example, putting
Big or diminution camera closes false alarm and inquires the temperature measured from intelligent thermostat).Smart machine control data includes control
The user voice command of intelligent home equipment 120 processed and specified to the user speech of intelligent home equipment.According to wired home
The voice of equipment is specified, cloud projection service server 116 identified in device registry 118 in the customer domain with electronic equipment phase
Associated intelligent home equipment 120.Cloud projects service server 116 and then sends another equipment to intelligent home equipment 1290
Control data enables the intelligent home equipment module 922 of intelligent home equipment 120 be controlled according to user speech instruction whereby
Intelligent home equipment 120.
Each in element identified above can be stored in one or more aforementioned memory equipment, and corresponding
In the instruction set for executing above-mentioned function.Module identified above or program (that is, instruction set) need not be implemented as
Individual software program, process, module or data structure, and therefore each subset of these modules can be in various embodiment party
It is combined in formula or otherwise rearranges.In some embodiments, memory 906 is optionally stored identified above
The subset of module and data structure.In addition, the optionally stored add-on module being not described above of memory 906 and data knot
Structure.
Voice-based LED is shown and media control method in smart media environment
Figure 10 is the flow chart for the method 1000 for showing the visually instruction speech processes state according to some embodiments.
Method 1000 with full color LED array, one or more microphone, loud speaker, processor and at least one program of storage with
For being realized at the electronic equipment 190 of the memory of processor execution.Electronic equipment 190 is received via one or more microphones 402
Collect the audio input (1002) of the environment near electronic equipment 190, and handles (1004) audio input.The processing is in language
It is realized at sound processing module 522, and includes one or more in the voice input of the user of identification and response in environment
It is a.Then electronic equipment 190 determines the state of (1006) processing from multiple predefined speech processes states.For full color LED
In each, electronic equipment 190 identify (1008) with determination the associated corresponding predetermined LED illumination of speech processes state
Specification, and corresponding lighting regulations include (1010) LED illumination duration, pulse rate, duty ratio, color sequences and bright
One or more of degree.According to the LED illumination specification of full color LED identified, (specifically, LED is controlled electronic equipment 190
Module 524) so that the illumination of full color LED array is synchronized, to provide the visual pattern for indicating identified speech processes state.More than
With reference to Fig. 4 A- Fig. 4 H and Fig. 5 it has been explained that more details about method 1000.
Method 1000 is optionally by being stored in non-transitory computer-readable storage media and being set by Speech-activated electronic
Instruction that standby 190 one or more processors execute manages.Each operation can correspond to be stored in meter shown in Figure 10
Instruction in calculation machine memory or computer readable storage medium (for example, memory 506 of the electronic equipment 190 in Fig. 5).Meter
Calculation machine readable storage medium storing program for executing may include magnetically or optically disk storage device, the solid-state memory device of such as flash memory or other are non-easily
The property lost memory devices.The computer-readable instruction being stored on computer readable storage medium may include one of the following
Or it is multiple：Source code, assembler language code, object code or other instruction formats explained by one or more processors.Side
Some operations in method 1000 can be combined and/or the sequence of some operations can be changed.
Figure 11 is the side that the closed caption of display media content is initiated by voice shown according to some embodiments
The flow chart of method 1100.The realization at server system (for example, cloud projects service server 116) of method 1100, the server
System includes the storage of at least one program (for example, cloud projection applies 760) of processor and storage for being executed by processor
Device.Server system receives (1102) speech message for being recorded by electronic equipment 190, and determines that (1104) speech message is the
One closed caption initiates request.First closed caption initiate request include (1106) initiate closed caption user voice command with
And closed caption is played by the media content being activated to showing that the user speech of equipment 106 is specified.According to showing equipment
It is specified, server system identified in device registry 118 (1108) it is associated with electronic equipment 190 in the customer domain and
It is coupled to the device for projecting 108 of specified display equipment 106.It is to execute media play to answer that device for projecting 108, which is configured (1110),
With showing the media content received from media content trustship for controlling specified display equipment.Then, server system (tool
Body, 760) cloud projection is applied asks to device for projecting transmission (1112) the second closed captions initiation for being coupled to given display device
It asks, to make device for projecting be able to carry out media play application, control given display device opening is currently displayed at specified aobvious
Show the closed caption of the media content in equipment, and request is initiated according to the second closed caption and shows closed caption.Above with reference to
Fig. 2A, Fig. 2 B and Fig. 5-Fig. 7 are it has been explained that more details about method 1100.
Figure 12 be show the speech play by the media content on media output devices according to some embodiments come
The flow chart of the method 1200 of initiation.The realization at server system (for example, cloud projects service server 116) of method 1200,
The server system includes processor and stores the memory of at least one program executed by processor.Server system receives
(1202) speech message recorded by electronic equipment, and determine that (1204) speech message includes the first media play request.
First media play request includes that (1206) play the user voice command of media content and to media on media output devices
The user speech of output equipment 106 is specified, and user voice command includes at least the information of the first media play application and needs
The media content to be played.Specified according to the voice to media output devices, server system identifies in device registry 118
(1208) device for projecting 108 that is associated with electronic equipment 190 in the customer domain and being coupled to media output devices 106.It throws
Jet device 108 is configured as (1210) and executes one or more media play applications, is played for controlling media output devices 106
The media content received from one or more media content trustships.Then, (specifically, cloud projection is using 760) for server system
Include the second media play of the information that the first media play is applied and the media content that needs play to the transmission of device for projecting 108
It asks (1212), device for projecting 108 is thus made to be able to carry out the first media that control media output devices 106 play media content
Play application.Above with reference to Fig. 2A, Fig. 2 B and Fig. 5-Fig. 7 it has been explained that more details about method 1200.
Figure 13 is to show that the broadcasting by media content according to some embodiments is moved from source media output devices to play
To the flow chart of the method 1300 of destination media output devices.Method 1200 is in server system (for example, cloud projection service clothes
Be engaged in device 116) at realize, which includes processor and the storage of at least one program that storage is executed by processor
Device.
Server system receives the speech message that (1302) are recorded by electronic equipment 190, and determines (1304) voice
Message includes media transmission request.Media content being played on is transmitted to destination matchmaker by media transmission request including (1306)
The user voice command of body output equipment and specified to the user speech of destination media output devices.Server system is from source
Device for projecting (for example, device for projecting 108-1 of Fig. 3) obtains the instant media play information for the media content being currently played
(1308).Instant broadcast information includes the information of (1310) at least first media play application, the current media content just played
And time location related with media content is played.
Specified according to the voice to destination media output devices, server system identifies in device registry 118
(1312) associated with electronic equipment 190 in the customer domain and be coupled to destination media output devices (for example, Fig. 3's is defeated
Go out equipment 106-2) destination device for projecting (for example, device for projecting 108-2 of Fig. 3).Destination device for projecting is configured as
(1314) one or more media play applications are executed, media output devices are played from one or more matchmakers for control purposes
The media content that body content hosting receives.Then, (760) specifically, cloud projection is applied sets server system to destination projection
Preparation send the media play request (1316) including instant media play information, and destination device for projecting is thus made to be able to carry out
One media play application, control destination media output devices play media content from time location.Above by reference to Fig. 3 and figure
5- Fig. 7 is it has been explained that more details about method 1300.
Method 1100,1200 and 1300 is optionally by being stored in non-transitory computer-readable storage media and being thrown by cloud
The instruction of the one or more processors execution of service server 116 is penetrated to manage.Each operation shown in Figure 12-Figure 14
It can correspond to be stored in computer storage or computer readable storage medium (for example, the storage of the server system in Fig. 7
Device 706) in instruction.Computer readable storage medium may include magnetically or optically disk storage device, the solid-state storage of such as flash memory
Device equipment or other non-volatile memory devices.The computer-readable instruction being stored on computer readable storage medium can
To include one of the following or multiple：Source code, assembler language code, object code are explained by one or more processors
Other instruction formats.Some operations in each in method 1100,1200 and 1300 can be combined and/or some behaviour
The sequence of work can be changed.
The term used in various described embodiments described here is only used for the mesh of description particular implementation
, and be not intended to be limited to.As used in the description of various described embodiments and appended claims, unless on
It clearly indicates additionally below, otherwise singulative " one ", "one" and "the" are intended to also include plural form.It will also be appreciated that
Be, term as used herein "and/or" refer to and include one or more related Listed Items any and all possibility
Combination.It will be further appreciated that when used in this manual, the terms "include", "comprise", "comprising" and/or " packet
Containing " indicate the feature, entirety, step, operations, elements, and/or components, but be not excluded for other one or more features, entirety,
Step, operation, component, assembly unit and/or a combination thereof presence or addition.
As used herein, term " if " be optionally interpreted to mean depending on context " when " or " it
Afterwards " or " in response to determination " or " in response to detection " or " according to determination ".Similarly, phrase " if it is determined that " or " if [detection
To the condition or event] " it is optionally construed to mean " to determine " depending on context or " in response to determination " or " detected
To [situation or event] " or " in response to detecting [situation or event] " or " according to determination [situation or thing
Part] be detected ".
It should be understood that " smart media environment " can refer to the intelligent environment for the family of such as one family dwelling,
But the range of this introduction is without being limited thereto.This introduction is also applied for (but not limited to) duplex room, united villa, multiple-unit apartment
Building, hotel, retail shop, office building, industrial building and more common any living space or working space.
Although should also be appreciated that terms user, customer, setter, house-owner, holder, guest, tenant, landlord, repair
Personnel etc. can be used for referring to the one or more people to take action in the contexts of some particular cases described here, but these
Bibliography does not limit the range of this introduction for being carrying out such one or more people acted.Thus, for example,
In the case of single household home dwelling, user, client, buyer, setter, subscriber and house-owner term can usually refer to identical
People because the supervisor of family be typically carry out purchase decision, purchase of equipment, installation and configuration equipment and unit user it
One.However, in the case that such as landlord's tenant environment other, client may be buy the unit landlord, setter may
It is that local apartment is responsible for, the first user may be tenant, and second user may be landlord again in terms of distant control function.
Importantly, although the specific advantages that the identity of the people of execution action may be provided with one or more embodiments have close pass
System, but such identity is not necessarily to be construed as being that those are specific with these specific identities by the scope limitation of this introduction
Personal subsequent description.
Although various attached drawings show multiple logical stages with particular order, the stage for not depending on sequence can be by again
It sorts and other stages can be combined or decompose.Although specifically reciting some rearrangements or other groupings,
Other will be obvious to those of ordinary skill in the art, therefore sequence presented herein and grouping are not
Detailed alternative list.Moreover, it should be appreciated that these stages can with hardware, firmware, software or any combination thereof come
It realizes.
For purposes of explanation, the description of front is described by reference to specific implementation mode.However, saying above
Bright property discussion is not exhaustive or the scope of the claims is limited to exact form disclosed.In view of above-mentioned introduction,
Many modifications and variations are possible.It is to best explain claim and its practical application to select these embodiments
Basic principle, to enable others skilled in the art most preferably using with being suitable for each of expected special-purpose
The embodiment of kind modification.
It should be noted that being also disclosed in the annex submitted together with the application about the above embodiment and being substituted real
Apply the more details of mode.In annex, OOBE refers to the experience (out-of-box experience) of out-of-the-box.
Claims (13)
1. a kind of method for initiating the display to the closed caption of media content by voice, including：
In the server system for including processor and the memory for storing at least one program for being executed by the processor
Place：
Receive the speech message of electronic equipment record；
Determine that the speech message is that the first closed caption initiates request, wherein it includes hair that first closed caption, which initiates request,
Play the user voice command of closed caption and to playing closed caption by the display equipment for the media content being activated
User speech is specified；
It is specified according to the display equipment, identification is associated with the electronic equipment in the customer domain and is coupled to described
The device for projecting of specified display equipment, wherein the device for projecting is configured as executing media play using for control institute
It states specified display equipment and shows the media content received from media content trustship；And
The second closed caption, which is sent, to the device for projecting for being coupled to the specified display equipment initiates request, so that
The device for projecting is able to carry out the media play application, and the media play is applied to be initiated according to second closed caption
The request control specified display equipment opens the described of the media content being currently displayed in the specified display equipment
Closed caption and show the closed caption.
2. according to the method described in claim 1, further including：
It initiates to ask according to first closed caption, determines the Display specification of the closed caption, wherein described second hides
Subtitle initiates the Display specification that request includes the closed caption, and the device for projecting is configured as executing the matchmaker
Body plays application and shows the closed caption according to the Display specification to control the display equipment.
3. according to the method described in claim 2, the Display specification of the wherein described closed caption includes that font, font are big
At least one of small, font color and background color.
4. method according to any preceding claims, wherein the closed caption is shown according to the CC Display specifications of acquiescence
Show in the specified display equipment.
5. method according to any preceding claims, wherein the electronic equipment and the device for projecting all with user's account
Family is associated.
6. according to the method described in claim 5, the wherein described user account is Google user accounts.
7. method according to any preceding claims, wherein the server system is different from carrying the media content
The device for projecting is supplied for the content hosting that is shown in the specified display equipment.
8. method according to any preceding claims, wherein the server system is in close proximity to one another far from being arranged to
The electronic equipment, the device for projecting and the specified display equipment.
9. method according to any preceding claims includes wherein specified to the user speech of the display equipment
The description of the specified display equipment, and the device for projecting is identified in device registry, and the method further includes：
The finger in multiple display equipment is identified in the registration table according to the description of the specified display equipment
Fixed display equipment.
10. according to the method described in claim 9, the description of the wherein described specified display equipment includes at least the finger
The brand of fixed display equipment or position.
11. method according to any preceding claims, further includes：
The speech message is forwarded to Speech processing services device, the Speech processing services device parses the speech message and knows
The not described user voice command and specified to the user speech of destination media device；And
The user voice command is received from the Speech processing services device and to the user of the destination media device
Voice is specified.
12. a kind of server system, including：
One or more processors；And
It is wherein stored with the memory of instruction, described instruction makes the processor when being executed by one or more of processors
Execute method according to any one of claim 1 to 11.
13. a kind of non-transitory computer-readable storage media of at least one program of storage, at least one program by with
It sets at least one processor execution by server system, at least one program includes being wanted according to right for executing
Ask the instruction of the method described in any one of 1 to 11.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202210299423.2A CN114758655A (en) | 2016-05-13 | 2017-05-11 | Voice controlled closed captioning display |
Applications Claiming Priority (15)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662336566P | 2016-05-13 | 2016-05-13 | |
US201662336551P | 2016-05-13 | 2016-05-13 | |
US201662336565P | 2016-05-13 | 2016-05-13 | |
US201662336569P | 2016-05-13 | 2016-05-13 | |
US62/336,565 | 2016-05-13 | ||
US62/336,569 | 2016-05-13 | ||
US62/336,551 | 2016-05-13 | ||
US62/336,566 | 2016-05-13 | ||
US15/592,120 | 2017-05-10 | ||
US15/592,128 US10332516B2 (en) | 2016-05-10 | 2017-05-10 | Media transfer among media output devices |
US15/592,120 US10304450B2 (en) | 2016-05-10 | 2017-05-10 | LED design language for visual affordance of voice user interfaces |
US15/592,126 | 2017-05-10 | ||
US15/592,126 US10235997B2 (en) | 2016-05-10 | 2017-05-10 | Voice-controlled closed caption display |
US15/592,128 | 2017-05-10 | ||
PCT/US2017/032262 WO2017197186A1 (en) | 2016-05-13 | 2017-05-11 | Voice-controlled closed caption display |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210299423.2A Division CN114758655A (en) | 2016-05-13 | 2017-05-11 | Voice controlled closed captioning display |
Publications (2)
Publication Number | Publication Date |
---|---|
CN108604254A true CN108604254A (en) | 2018-09-28 |
CN108604254B CN108604254B (en) | 2022-04-12 |
Family
ID=60295267
Family Applications (6)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780011386.8A Active CN108604254B (en) | 2016-05-13 | 2017-05-11 | Voice controlled closed captioning display |
CN202110178882.0A Pending CN112947683A (en) | 2016-05-13 | 2017-05-11 | Media delivery between media output devices |
CN201780011381.5A Active CN108604181B (en) | 2016-05-13 | 2017-05-11 | Media delivery between media output devices |
CN202210299423.2A Pending CN114758655A (en) | 2016-05-13 | 2017-05-11 | Voice controlled closed captioning display |
CN201780011357.1A Pending CN108604180A (en) | 2016-05-13 | 2017-05-11 | The LED design language of visual effect for Voice User Interface |
CN201780009235.9A Active CN108604178B (en) | 2016-05-13 | 2017-05-12 | Personalized and contextualized audio presentations |
Family Applications After (5)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202110178882.0A Pending CN112947683A (en) | 2016-05-13 | 2017-05-11 | Media delivery between media output devices |
CN201780011381.5A Active CN108604181B (en) | 2016-05-13 | 2017-05-11 | Media delivery between media output devices |
CN202210299423.2A Pending CN114758655A (en) | 2016-05-13 | 2017-05-11 | Voice controlled closed captioning display |
CN201780011357.1A Pending CN108604180A (en) | 2016-05-13 | 2017-05-11 | The LED design language of visual effect for Voice User Interface |
CN201780009235.9A Active CN108604178B (en) | 2016-05-13 | 2017-05-12 | Personalized and contextualized audio presentations |
Country Status (5)
Country | Link |
---|---|
US (2) | US10402450B2 (en) |
EP (5) | EP3757753A1 (en) |
JP (2) | JP6797938B2 (en) |
KR (2) | KR102177786B1 (en) |
CN (6) | CN108604254B (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113365124A (en) * | 2020-03-06 | 2021-09-07 | 海信视像科技股份有限公司 | Display device and display method |
Families Citing this family (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10515637B1 (en) * | 2017-09-19 | 2019-12-24 | Amazon Technologies, Inc. | Dynamic speech processing |
US11526518B2 (en) | 2017-09-22 | 2022-12-13 | Amazon Technologies, Inc. | Data reporting system and method |
US20190095444A1 (en) * | 2017-09-22 | 2019-03-28 | Amazon Technologies, Inc. | Voice driven analytics |
JP6943192B2 (en) * | 2018-01-24 | 2021-09-29 | 沖電気工業株式会社 | Home appliances and location search system |
WO2019188393A1 (en) * | 2018-03-29 | 2019-10-03 | ソニー株式会社 | Information processing device, information processing method, transmission device and transmission method |
JP7004834B2 (en) * | 2018-05-07 | 2022-01-21 | グーグル エルエルシー | Synchronization of access control between computing devices |
US11085777B2 (en) * | 2018-07-27 | 2021-08-10 | Adobe Inc. | Generating digital event sequences utilizing a dynamic user preference interface to modify recommendation model reward functions |
US11231975B2 (en) * | 2018-09-29 | 2022-01-25 | Apple Inc. | Devices, methods, and user interfaces for providing audio notifications |
US20200127988A1 (en) * | 2018-10-19 | 2020-04-23 | Apple Inc. | Media intercom over a secure device to device communication channel |
US10867603B2 (en) * | 2018-10-24 | 2020-12-15 | Sony Corporation | Audio-video reproduction device setup using interview-based voice control |
US10878805B2 (en) * | 2018-12-06 | 2020-12-29 | Microsoft Technology Licensing, Llc | Expediting interaction with a digital assistant by predicting user responses |
FR3093840B1 (en) | 2019-03-14 | 2021-02-19 | Psa Automobiles Sa | Method and device for assisting the use of a motor vehicle |
US10990939B2 (en) * | 2019-04-15 | 2021-04-27 | Advanced New Technologies Co., Ltd. | Method and device for voice broadcast |
US10867608B1 (en) * | 2019-05-31 | 2020-12-15 | Apple Inc. | Multi-user configuration |
CN110501988B (en) * | 2019-09-25 | 2020-06-09 | 北京金茂绿建科技有限公司 | Method and device for realizing integration control of Internet of things equipment |
JP2021091182A (en) * | 2019-12-12 | 2021-06-17 | コニカミノルタ株式会社 | Image processing device and control method |
US11322150B2 (en) * | 2020-01-28 | 2022-05-03 | Amazon Technologies, Inc. | Generating event output |
US11036466B1 (en) | 2020-02-28 | 2021-06-15 | Facebook, Inc. | Social media custom audio program |
WO2021183147A1 (en) | 2020-03-13 | 2021-09-16 | Google Llc | Network-connected television devices with knowledge-based media content recommendations and unified user interfaces |
CN113711617B (en) * | 2020-03-13 | 2024-04-02 | 谷歌有限责任公司 | Method and device for projecting media content in networking television device |
CN112735419A (en) * | 2021-01-28 | 2021-04-30 | 东莞维升电子制品有限公司 | Intelligent voice wake-up control method and control device thereof |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102064985A (en) * | 2010-11-24 | 2011-05-18 | 深圳市同洲电子股份有限公司 | Digital and intelligent remote control system and method for household electrical appliances on basis of interactive TV application |
CN102196207A (en) * | 2011-05-12 | 2011-09-21 | 深圳市子栋科技有限公司 | Method, device and system for controlling television by using voice |
WO2014064531A1 (en) * | 2012-10-22 | 2014-05-01 | Spotify Ab | Systems and methods for pre-fetching media content |
Family Cites Families (157)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5659665A (en) | 1994-12-08 | 1997-08-19 | Lucent Technologies Inc. | Method and apparatus for including speech recognition capabilities in a computer system |
US5774859A (en) | 1995-01-03 | 1998-06-30 | Scientific-Atlanta, Inc. | Information system having a speech interface |
US5760754A (en) | 1995-12-04 | 1998-06-02 | Motorola, Inc. | Light pipe assembly and electrical device using same |
US6195641B1 (en) | 1998-03-27 | 2001-02-27 | International Business Machines Corp. | Network universal spoken language vocabulary |
US7050977B1 (en) | 1999-11-12 | 2006-05-23 | Phoenix Solutions, Inc. | Speech-enabled server for internet website and method |
US6397186B1 (en) | 1999-12-22 | 2002-05-28 | Ambush Interactive, Inc. | Hands-free, voice-operated remote control transmitter |
US6681380B1 (en) * | 2000-02-15 | 2004-01-20 | International Business Machines Corporation | Aggregating constraints and/or preferences using an inference engine and enhanced scripting language |
GB2372864B (en) * | 2001-02-28 | 2005-09-07 | Vox Generation Ltd | Spoken language interface |
US7426505B2 (en) * | 2001-03-07 | 2008-09-16 | International Business Machines Corporation | Method for identifying word patterns in text |
US7302634B2 (en) * | 2001-03-14 | 2007-11-27 | Microsoft Corporation | Schema-based services for identity-based data access |
US20030120744A1 (en) * | 2001-12-20 | 2003-06-26 | Gordon Kessler | Method and apparatus for providing downlaoded audio data |
US7493259B2 (en) * | 2002-01-04 | 2009-02-17 | Siebel Systems, Inc. | Method for accessing data via voice |
US7260538B2 (en) | 2002-01-08 | 2007-08-21 | Promptu Systems Corporation | Method and apparatus for voice control of a television control device |
US20040001095A1 (en) | 2002-07-01 | 2004-01-01 | Todd Marques | Method and apparatus for universal device management |
JP2004102415A (en) * | 2002-09-05 | 2004-04-02 | Toshiba Corp | Data transmission device and method and onboard electronic equipment |
US20060276230A1 (en) * | 2002-10-01 | 2006-12-07 | Mcconnell Christopher F | System and method for wireless audio communication with a computer |
US7911358B2 (en) * | 2002-10-08 | 2011-03-22 | Johnson Controls Technology Company | System and method for enrollment of a remotely controlled device in a trainable transmitter |
JP4292789B2 (en) | 2002-11-20 | 2009-07-08 | 日本電気株式会社 | Browser function expansion method |
US7925754B2 (en) * | 2003-11-21 | 2011-04-12 | Microsoft Corporation | Method and computer program product to provide synch notifications to client devices |
US7660715B1 (en) * | 2004-01-12 | 2010-02-09 | Avaya Inc. | Transparent monitoring and intervention to improve automatic adaptation of speech models |
US20050164681A1 (en) * | 2004-01-22 | 2005-07-28 | Jenkins William W. | Voice message storage in a push-to-talk communication system |
US20050212684A1 (en) | 2004-03-23 | 2005-09-29 | Flora Huang | Indicating apparatus combined with flash |
US20060075429A1 (en) * | 2004-04-30 | 2006-04-06 | Vulcan Inc. | Voice control of television-related information |
TWM260059U (en) * | 2004-07-08 | 2005-03-21 | Blueexpert Technology Corp | Computer input device having bluetooth handsfree handset |
JP2006286275A (en) * | 2005-03-31 | 2006-10-19 | Koizumi Sangyo Corp | Control apparatus for lighting fixture |
ATE550756T1 (en) | 2005-08-04 | 2012-04-15 | Nuance Communications Inc | VOICE DIALOGUE SYSTEM |
US8104054B2 (en) * | 2005-09-01 | 2012-01-24 | At&T Intellectual Property I, L.P. | Methods, systems, and devices for bandwidth conservation |
US7996228B2 (en) | 2005-12-22 | 2011-08-09 | Microsoft Corporation | Voice initiated network operations |
US8516087B2 (en) | 2006-02-14 | 2013-08-20 | At&T Intellectual Property I, L.P. | Home automation system and method |
US7721313B2 (en) | 2006-06-30 | 2010-05-18 | Microsoft Corporation | Multi-DVR node communication |
US20080010652A1 (en) | 2006-07-07 | 2008-01-10 | General Instrument Corporation | Association of Network Terminals to a Common Account |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8073697B2 (en) * | 2006-09-12 | 2011-12-06 | International Business Machines Corporation | Establishing a multimodal personality for a multimodal application |
US8056070B2 (en) | 2007-01-10 | 2011-11-08 | Goller Michael D | System and method for modifying and updating a speech recognition program |
US20080180572A1 (en) | 2007-01-29 | 2008-07-31 | Microsoft Corporation | Enabling access to closed captioning data present in a broadcast stream |
JP4315986B2 (en) * | 2007-02-08 | 2009-08-19 | 富士通株式会社 | Electronic device with light emitting diode |
US8219406B2 (en) * | 2007-03-15 | 2012-07-10 | Microsoft Corporation | Speech-centric multimodal user interface design in mobile technology |
US8150699B2 (en) | 2007-05-17 | 2012-04-03 | Redstart Systems, Inc. | Systems and methods of a structured grammar for a speech recognition command system |
US8538757B2 (en) | 2007-05-17 | 2013-09-17 | Redstart Systems, Inc. | System and method of a list commands utility for a speech recognition command system |
CN101309390B (en) * | 2007-05-17 | 2012-05-23 | 华为技术有限公司 | Visual communication system, apparatus and subtitle displaying method |
US8160426B2 (en) | 2007-10-12 | 2012-04-17 | Rovi Guides, Inc. | Storage management of a recording device in a multi-user system |
US8521766B1 (en) * | 2007-11-12 | 2013-08-27 | W Leo Hoarty | Systems and methods for providing information discovery and retrieval |
US8543622B2 (en) | 2007-12-07 | 2013-09-24 | Patrick Giblin | Method and system for meta-tagging media content and distribution |
US8385536B2 (en) | 2008-01-09 | 2013-02-26 | Verizon Patent And Licensing Inc. | Automatic telephone number favorites list |
US9135809B2 (en) | 2008-06-20 | 2015-09-15 | At&T Intellectual Property I, Lp | Voice enabled remote control for a set-top box |
US8898568B2 (en) * | 2008-09-09 | 2014-11-25 | Apple Inc. | Audio user interface |
US8244531B2 (en) * | 2008-09-28 | 2012-08-14 | Avaya Inc. | Method of retaining a media stream without its private audio content |
JP5334178B2 (en) | 2009-01-21 | 2013-11-06 | クラリオン株式会社 | Speech recognition apparatus and data update method |
US8423353B2 (en) | 2009-03-25 | 2013-04-16 | Microsoft Corporation | Sharable distributed dictionary for applications |
US20100265397A1 (en) | 2009-04-20 | 2010-10-21 | Tandberg Television, Inc. | Systems and methods for providing dynamically determined closed caption translations for vod content |
US9858925B2 (en) * | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US9197736B2 (en) * | 2009-12-31 | 2015-11-24 | Digimarc Corporation | Intuitive computing methods and systems |
US20120253822A1 (en) | 2009-12-11 | 2012-10-04 | Thomas Barton Schalk | Systems and Methods for Managing Prompts for a Connected Vehicle |
US20110161085A1 (en) * | 2009-12-31 | 2011-06-30 | Nokia Corporation | Method and apparatus for audio summary of activity for user |
US9401099B2 (en) * | 2010-05-11 | 2016-07-26 | AI Squared | Dedicated on-screen closed caption display |
US8750687B2 (en) | 2010-06-16 | 2014-06-10 | Verizon Patent And Licensing Inc. | Method and apparatus for managing digital video recorders |
US9633656B2 (en) | 2010-07-27 | 2017-04-25 | Sony Corporation | Device registration process from second display |
US8473289B2 (en) * | 2010-08-06 | 2013-06-25 | Google Inc. | Disambiguating input based on context |
US20120096497A1 (en) | 2010-10-14 | 2012-04-19 | Sony Corporation | Recording television content |
TW201224966A (en) * | 2010-12-03 | 2012-06-16 | Accton Technology Corp | Interactive media player system and method thereof |
US10382509B2 (en) * | 2011-01-28 | 2019-08-13 | Amazon Technologies, Inc. | Audio-based application architecture |
US20120226981A1 (en) | 2011-03-02 | 2012-09-06 | Microsoft Corporation | Controlling electronic devices in a multimedia system through a natural user interface |
CN102148031A (en) | 2011-04-01 | 2011-08-10 | 无锡大核科技有限公司 | Voice recognition and interaction system and method |
US20120260192A1 (en) * | 2011-04-11 | 2012-10-11 | Detweiler Sean D | Automated browser mode based on user and access point |
WO2013012107A1 (en) | 2011-07-19 | 2013-01-24 | 엘지전자 주식회사 | Electronic device and method for controlling same |
US20130046773A1 (en) | 2011-08-18 | 2013-02-21 | General Instrument Corporation | Method and apparatus for user-based tagging of media content |
CN102289374B (en) | 2011-08-31 | 2017-06-30 | 南京中兴新软件有限责任公司 | A kind of method and device for building multi-platform software running environment |
US9495331B2 (en) * | 2011-09-19 | 2016-11-15 | Personetics Technologies Ltd. | Advanced system and method for automated-context-aware-dialog with human users |
US8762156B2 (en) | 2011-09-28 | 2014-06-24 | Apple Inc. | Speech recognition repair using contextual information |
US8340975B1 (en) | 2011-10-04 | 2012-12-25 | Theodore Alfred Rosenberger | Interactive speech recognition device and system for hands-free building control |
EP2801016A1 (en) | 2011-10-11 | 2014-11-12 | Serge Media Inc. | System and methods for content-search carousel for mobile-computing devices |
US9326088B2 (en) | 2011-10-21 | 2016-04-26 | GM Global Technology Operations LLC | Mobile voice platform architecture with remote service interfaces |
US9847083B2 (en) | 2011-11-17 | 2017-12-19 | Universal Electronics Inc. | System and method for voice actuated configuration of a controlling device |
US8954330B2 (en) * | 2011-11-28 | 2015-02-10 | Microsoft Corporation | Context-aware interaction system using a semantic model |
US9152376B2 (en) | 2011-12-01 | 2015-10-06 | At&T Intellectual Property I, L.P. | System and method for continuous multimodal speech and gesture interaction |
US8793136B2 (en) * | 2012-02-17 | 2014-07-29 | Lg Electronics Inc. | Method and apparatus for smart voice recognition |
US9836545B2 (en) * | 2012-04-27 | 2017-12-05 | Yahoo Holdings, Inc. | Systems and methods for personalized generalized content recommendations |
CN102685579B (en) * | 2012-05-02 | 2015-03-25 | 合一网络技术(北京)有限公司 | Method for realizing media sharing and control among devices in local network |
US9230556B2 (en) * | 2012-06-05 | 2016-01-05 | Apple Inc. | Voice instructions during navigation |
US20130332159A1 (en) | 2012-06-08 | 2013-12-12 | Apple Inc. | Using fan throttling to enhance dictation accuracy |
US9679330B2 (en) * | 2012-06-10 | 2017-06-13 | Apple Inc. | Interface for enhanced continuity of browsing experience |
KR20130140423A (en) * | 2012-06-14 | 2013-12-24 | 삼성전자주식회사 | Display apparatus, interactive server and method for providing response information |
US20130339859A1 (en) | 2012-06-15 | 2013-12-19 | Muzik LLC | Interactive networked headphones |
US9195383B2 (en) | 2012-06-29 | 2015-11-24 | Spotify Ab | Systems and methods for multi-path control signals for media presentation devices |
US10620797B2 (en) * | 2012-06-29 | 2020-04-14 | Spotify Ab | Systems and methods for multi-context media control and playback |
US9786294B1 (en) | 2012-07-30 | 2017-10-10 | Amazon Technologies, Inc. | Visual indication of an operational state |
US9779757B1 (en) | 2012-07-30 | 2017-10-03 | Amazon Technologies, Inc. | Visual indication of an operational state |
US9106957B2 (en) * | 2012-08-16 | 2015-08-11 | Nuance Communications, Inc. | Method and apparatus for searching data sources for entertainment systems |
US9424840B1 (en) * | 2012-08-31 | 2016-08-23 | Amazon Technologies, Inc. | Speech recognition platforms |
US9576574B2 (en) * | 2012-09-10 | 2017-02-21 | Apple Inc. | Context-sensitive handling of interruptions by intelligent digital assistant |
JP5986468B2 (en) | 2012-09-25 | 2016-09-06 | 富士通テン株式会社 | Display control apparatus, display system, and display control method |
US9043210B1 (en) * | 2012-10-02 | 2015-05-26 | Voice Security Systems, Inc. | Biometric voice command and control switching device and method of use |
US9230560B2 (en) | 2012-10-08 | 2016-01-05 | Nant Holdings Ip, Llc | Smart home automation systems and methods |
PL401346A1 (en) * | 2012-10-25 | 2014-04-28 | Ivona Software Spółka Z Ograniczoną Odpowiedzialnością | Generation of customized audio programs from textual content |
US9337674B2 (en) | 2012-11-02 | 2016-05-10 | Chen-Source Inc. | Desktop charger |
US9704486B2 (en) | 2012-12-11 | 2017-07-11 | Amazon Technologies, Inc. | Speech recognition power management |
US9672822B2 (en) * | 2013-02-22 | 2017-06-06 | Next It Corporation | Interaction with a portion of a content item through a virtual assistant |
US9292832B2 (en) | 2013-02-25 | 2016-03-22 | Qualcomm Incorporated | Collaborative intelligence and decision-making in an IoT device group |
US9361885B2 (en) | 2013-03-12 | 2016-06-07 | Nuance Communications, Inc. | Methods and apparatus for detecting a voice command |
US9304736B1 (en) | 2013-04-18 | 2016-04-05 | Amazon Technologies, Inc. | Voice controlled assistant with non-verbal code entry |
US10445115B2 (en) | 2013-04-18 | 2019-10-15 | Verint Americas Inc. | Virtual assistant focused user interfaces |
US9116619B2 (en) | 2013-05-10 | 2015-08-25 | Seagate Technology Llc | Displaying storage device status conditions using multi-color light emitting diode |
US9811087B2 (en) * | 2013-05-15 | 2017-11-07 | Deere & Company | Method for controlling a vehicle and a vehicle guidance system |
US9843623B2 (en) * | 2013-05-28 | 2017-12-12 | Qualcomm Incorporated | Systems and methods for selecting media items |
WO2014197336A1 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US20140365887A1 (en) * | 2013-06-10 | 2014-12-11 | Kirk Robert CAMERON | Interactive platform generating multimedia from user input |
US9324322B1 (en) | 2013-06-18 | 2016-04-26 | Amazon Technologies, Inc. | Automatic volume attenuation for speech enabled devices |
US9554632B2 (en) | 2013-06-21 | 2017-01-31 | Logitech Europe S.A. | Portable device case and accessories |
US9997160B2 (en) | 2013-07-01 | 2018-06-12 | Toyota Motor Engineering & Manufacturing North America, Inc. | Systems and methods for dynamic download of embedded voice components |
CN103474068B (en) | 2013-08-19 | 2016-08-10 | 科大讯飞股份有限公司 | Realize method, equipment and system that voice command controls |
US9431004B2 (en) * | 2013-09-05 | 2016-08-30 | International Business Machines Corporation | Variable-depth audio presentation of textual information |
KR20150029974A (en) | 2013-09-11 | 2015-03-19 | 엘지전자 주식회사 | Display device and method for controlling the same |
CN103501382B (en) | 2013-09-17 | 2015-06-24 | 小米科技有限责任公司 | Voice service providing method, device and terminal |
US9240182B2 (en) | 2013-09-17 | 2016-01-19 | Qualcomm Incorporated | Method and apparatus for adjusting detection threshold for activating voice assistant function |
US9443527B1 (en) | 2013-09-27 | 2016-09-13 | Amazon Technologies, Inc. | Speech recognition capability generation and control |
CA2926463A1 (en) | 2013-10-07 | 2015-04-16 | Google Inc. | Smart-home hazard detector providing useful follow up communications to detection events |
US9484025B2 (en) | 2013-10-15 | 2016-11-01 | Toyota Jidosha Kabushiki Kaisha | Configuring dynamic custom vocabulary for personalized speech recognition |
US9706007B2 (en) * | 2013-10-17 | 2017-07-11 | Blue Syntax Consulting LLC | System and method for querying disparate data sources in real time |
US9698999B2 (en) | 2013-12-02 | 2017-07-04 | Amazon Technologies, Inc. | Natural language control of secondary device |
US9900177B2 (en) | 2013-12-11 | 2018-02-20 | Echostar Technologies International Corporation | Maintaining up-to-date home automation models |
US9804820B2 (en) | 2013-12-16 | 2017-10-31 | Nuance Communications, Inc. | Systems and methods for providing a virtual assistant |
US9721570B1 (en) * | 2013-12-17 | 2017-08-01 | Amazon Technologies, Inc. | Outcome-oriented dialogs on a speech recognition platform |
US10248856B2 (en) | 2014-01-14 | 2019-04-02 | Toyota Motor Engineering & Manufacturing North America, Inc. | Smart necklace with stereo vision and onboard processing |
US9430186B2 (en) * | 2014-03-17 | 2016-08-30 | Google Inc | Visual indication of a recognized voice-initiated action |
US10031721B2 (en) | 2014-05-15 | 2018-07-24 | Tyco Safety Products Canada Ltd. | System and method for processing control commands in a voice interactive system |
EP3158691A4 (en) * | 2014-06-06 | 2018-03-28 | Obschestvo S Ogranichennoy Otvetstvennostiyu "Speactoit" | Proactive environment-based chat information system |
US10440499B2 (en) | 2014-06-16 | 2019-10-08 | Comcast Cable Communications, Llc | User location and identity awareness |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
CN104135697A (en) * | 2014-07-31 | 2014-11-05 | 上海素控控制技术有限公司 | Bluetooth-based smart resonance loudspeaker box and control method thereof |
US9767794B2 (en) * | 2014-08-11 | 2017-09-19 | Nuance Communications, Inc. | Dialog flow management in hierarchical task dialogs |
KR101579292B1 (en) | 2014-08-29 | 2015-12-21 | 서울대학교 산학협력단 | Apparatus and method for universal control using speech recognition |
US10235996B2 (en) | 2014-10-01 | 2019-03-19 | XBrain, Inc. | Voice and connection platform |
EP3823342B1 (en) * | 2014-10-30 | 2023-12-13 | DZS Inc. | Method and apparatus for providing performance and usage information for a wireless local area network |
WO2016066760A1 (en) * | 2014-10-31 | 2016-05-06 | Piksel, Inc | Personalised channel |
CN104506944B (en) | 2014-11-12 | 2018-09-21 | 科大讯飞股份有限公司 | Interactive voice householder method and system based on tv scene and voice assistant |
US20170329766A1 (en) * | 2014-12-09 | 2017-11-16 | Sony Corporation | Information processing apparatus, control method, and program |
US9811312B2 (en) | 2014-12-22 | 2017-11-07 | Intel Corporation | Connected device voice command support |
US10284618B2 (en) * | 2015-04-28 | 2019-05-07 | Apple Inc. | Dynamic media content |
US10038757B2 (en) * | 2015-04-29 | 2018-07-31 | Microsoft Technology Licensing, Llc | Providing personalized greetings on a digital assistant |
US9766596B2 (en) | 2015-07-08 | 2017-09-19 | Google Inc. | Wake up to a cast alarm or an alarm plus content prompt |
CN105163298B (en) * | 2015-08-24 | 2019-01-15 | 努比亚技术有限公司 | A kind of communication means and terminal |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10671665B2 (en) * | 2015-09-25 | 2020-06-02 | Oath Inc. | Personalized audio introduction and summary of result sets for users |
JP6532021B2 (en) | 2015-09-29 | 2019-06-19 | 本田技研工業株式会社 | Speech processing apparatus and speech processing method |
EP3151583B1 (en) | 2015-09-30 | 2022-02-02 | Apple Inc. | Earbud case with receptacle connector for earbuds |
US10102201B2 (en) | 2015-11-30 | 2018-10-16 | Soundhound, Inc. | Natural language module store |
US10026401B1 (en) | 2015-12-28 | 2018-07-17 | Amazon Technologies, Inc. | Naming devices via voice commands |
US20170221322A1 (en) | 2016-02-01 | 2017-08-03 | Brian M. Ignomirello | System and method of multimodal status indication |
US10623518B2 (en) * | 2016-02-04 | 2020-04-14 | Spotify Ab | System and method for ordering media content for shuffled playback based on user preference |
US9858927B2 (en) | 2016-02-12 | 2018-01-02 | Amazon Technologies, Inc | Processing spoken commands to control distributed audio outputs |
US20170259121A1 (en) * | 2016-03-08 | 2017-09-14 | Your Trainer Inc. | Science engine operative to select workout segments responsive to user-supplied information about their physical state |
US20170262537A1 (en) * | 2016-03-14 | 2017-09-14 | Amazon Technologies, Inc. | Audio scripts for various content |
CN114357128A (en) | 2016-04-18 | 2022-04-15 | 谷歌有限责任公司 | Automated assistant invocation of appropriate agents |
US9990002B2 (en) | 2016-05-25 | 2018-06-05 | Lg Electronics Inc. | Sound output apparatus and hub for communication network |
WO2017203366A1 (en) | 2016-05-27 | 2017-11-30 | Mobile Synergy 26 International Limited | Multifunctional connection systems for various devices and methods of use thereof |
US10832684B2 (en) * | 2016-08-31 | 2020-11-10 | Microsoft Technology Licensing, Llc | Personalization of experiences with digital assistants in communal settings through voice and query processing |
US11085777B2 (en) * | 2018-07-27 | 2021-08-10 | Adobe Inc. | Generating digital event sequences utilizing a dynamic user preference interface to modify recommendation model reward functions |
-
2017
- 2017-05-11 CN CN201780011386.8A patent/CN108604254B/en active Active
- 2017-05-11 US US15/593,236 patent/US10402450B2/en active Active
- 2017-05-11 EP EP20191991.7A patent/EP3757753A1/en active Pending
- 2017-05-11 EP EP17725446.3A patent/EP3455720B1/en active Active
- 2017-05-11 KR KR1020187033465A patent/KR102177786B1/en active IP Right Grant
- 2017-05-11 CN CN202110178882.0A patent/CN112947683A/en active Pending
- 2017-05-11 EP EP17726055.1A patent/EP3455747B1/en active Active
- 2017-05-11 KR KR1020187036139A patent/KR102114003B1/en active IP Right Grant
- 2017-05-11 JP JP2018559837A patent/JP6797938B2/en active Active
- 2017-05-11 CN CN201780011381.5A patent/CN108604181B/en active Active
- 2017-05-11 CN CN202210299423.2A patent/CN114758655A/en active Pending
- 2017-05-11 CN CN201780011357.1A patent/CN108604180A/en active Pending
- 2017-05-11 EP EP17725447.1A patent/EP3455721B1/en active Active
- 2017-05-12 CN CN201780009235.9A patent/CN108604178B/en active Active
- 2017-05-12 EP EP17727769.6A patent/EP3455722A1/en not_active Withdrawn
-
2019
- 2019-09-03 US US16/558,907 patent/US11860933B2/en active Active
-
2020
- 2020-11-18 JP JP2020191833A patent/JP7293180B2/en active Active
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102064985A (en) * | 2010-11-24 | 2011-05-18 | 深圳市同洲电子股份有限公司 | Digital and intelligent remote control system and method for household electrical appliances on basis of interactive TV application |
CN102196207A (en) * | 2011-05-12 | 2011-09-21 | 深圳市子栋科技有限公司 | Method, device and system for controlling television by using voice |
WO2014064531A1 (en) * | 2012-10-22 | 2014-05-01 | Spotify Ab | Systems and methods for pre-fetching media content |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113365124A (en) * | 2020-03-06 | 2021-09-07 | 海信视像科技股份有限公司 | Display device and display method |
CN113365124B (en) * | 2020-03-06 | 2023-01-20 | 海信视像科技股份有限公司 | Display device and display method |
Also Published As
Publication number | Publication date |
---|---|
KR20190006975A (en) | 2019-01-21 |
US11860933B2 (en) | 2024-01-02 |
EP3455720A1 (en) | 2019-03-20 |
EP3455721A1 (en) | 2019-03-20 |
JP7293180B2 (en) | 2023-06-19 |
CN108604254B (en) | 2022-04-12 |
CN108604181A (en) | 2018-09-28 |
EP3757753A1 (en) | 2020-12-30 |
EP3455747B1 (en) | 2021-07-28 |
US20170329848A1 (en) | 2017-11-16 |
US20190391998A1 (en) | 2019-12-26 |
CN108604180A (en) | 2018-09-28 |
US10402450B2 (en) | 2019-09-03 |
EP3455721B1 (en) | 2020-09-16 |
EP3455720B1 (en) | 2023-12-27 |
CN108604178A (en) | 2018-09-28 |
JP2021052403A (en) | 2021-04-01 |
CN108604178B (en) | 2021-09-03 |
EP3455747A1 (en) | 2019-03-20 |
KR20190014515A (en) | 2019-02-12 |
CN114758655A (en) | 2022-07-15 |
JP6797938B2 (en) | 2020-12-09 |
CN112947683A (en) | 2021-06-11 |
CN108604181B (en) | 2021-03-09 |
EP3455722A1 (en) | 2019-03-20 |
KR102114003B1 (en) | 2020-05-25 |
JP2019526177A (en) | 2019-09-12 |
KR102177786B1 (en) | 2020-11-12 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108604254A (en) | The closed caption of voice control is shown | |
US10861461B2 (en) | LED design language for visual affordance of voice user interfaces | |
CN209400877U (en) | Electronic equipment, speech interface equipment and electronic system | |
CN208444593U (en) | The loudspeaker apparatus of shell with vertical orientation | |
EP3535753B1 (en) | Focus session at a voice interface device | |
CN109791762A (en) | The noise of speech interface equipment reduces | |
WO2017197184A1 (en) | Led design language for visual affordance of voice user interfaces | |
CN108268235A (en) | Proactive notification is perceived for the dialogue of speech interface equipment | |
CN108141637A (en) | For the method and system that media is controlled to show in smart media display environment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |