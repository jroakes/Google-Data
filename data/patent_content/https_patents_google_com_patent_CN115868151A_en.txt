CN115868151A - Virtual remote control of a second device, e.g. a TV, on a first device - Google Patents
Virtual remote control of a second device, e.g. a TV, on a first device Download PDFInfo
- Publication number
- CN115868151A CN115868151A CN202180047030.6A CN202180047030A CN115868151A CN 115868151 A CN115868151 A CN 115868151A CN 202180047030 A CN202180047030 A CN 202180047030A CN 115868151 A CN115868151 A CN 115868151A
- Authority
- CN
- China
- Prior art keywords
- computing device
- communication channel
- query
- target
- source
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000004891 communication Methods 0.000 claims abstract description 113
- 230000006854 communication Effects 0.000 claims abstract description 113
- 230000004044 response Effects 0.000 claims abstract description 66
- 238000012545 processing Methods 0.000 claims description 127
- 238000000034 method Methods 0.000 claims description 89
- 230000000153 supplemental effect Effects 0.000 claims description 63
- 230000008569 process Effects 0.000 claims description 32
- 230000006870 function Effects 0.000 claims description 28
- 230000007175 bidirectional communication Effects 0.000 claims 1
- 230000005236 sound signal Effects 0.000 description 39
- 230000009471 action Effects 0.000 description 37
- 230000000694 effects Effects 0.000 description 14
- 230000000875 corresponding effect Effects 0.000 description 12
- 238000004590 computer program Methods 0.000 description 11
- 230000001276 controlling effect Effects 0.000 description 9
- 230000000007 visual effect Effects 0.000 description 9
- 230000003993 interaction Effects 0.000 description 8
- 230000005540 biological transmission Effects 0.000 description 6
- 230000002085 persistent effect Effects 0.000 description 5
- 230000000903 blocking effect Effects 0.000 description 4
- 230000008859 change Effects 0.000 description 4
- 230000000670 limiting effect Effects 0.000 description 4
- 238000003058 natural language processing Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000009877 rendering Methods 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 3
- 230000033001 locomotion Effects 0.000 description 3
- 230000006855 networking Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000013515 script Methods 0.000 description 3
- 230000027455 binding Effects 0.000 description 2
- 238000009739 binding Methods 0.000 description 2
- 238000006243 chemical reaction Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 238000007726 management method Methods 0.000 description 2
- 230000000877 morphologic effect Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 210000000707 wrist Anatomy 0.000 description 2
- IRLPACMLTUPBCL-KQYNXXCUSA-N 5'-adenylyl sulfate Chemical compound C1=NC=2C(N)=NC=NC=2N1[C@@H]1O[C@H](COP(O)(=O)OS(O)(=O)=O)[C@@H](O)[C@H]1O IRLPACMLTUPBCL-KQYNXXCUSA-N 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 238000013475 authorization Methods 0.000 description 1
- 239000011449 brick Substances 0.000 description 1
- 230000003139 buffering effect Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000008878 coupling Effects 0.000 description 1
- 238000010168 coupling process Methods 0.000 description 1
- 238000005859 coupling reaction Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000005672 electromagnetic field Effects 0.000 description 1
- 230000005670 electromagnetic radiation Effects 0.000 description 1
- 238000001914 filtration Methods 0.000 description 1
- 238000013467 fragmentation Methods 0.000 description 1
- 238000006062 fragmentation reaction Methods 0.000 description 1
- 239000000446 fuel Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 239000004570 mortar (masonry) Substances 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 238000013468 resource allocation Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 238000012384 transportation and delivery Methods 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42204—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor
- H04N21/42206—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor characterized by hardware details
- H04N21/4222—Remote control device emulator integrated into a non-television apparatus, e.g. a PDA, media center or smart toy
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42204—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L12/00—Data switching networks
- H04L12/28—Data switching networks characterised by path configuration, e.g. LAN [Local Area Networks] or WAN [Wide Area Networks]
- H04L12/2803—Home automation networks
- H04L12/2807—Exchanging configuration information on appliance services in a home automation network
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/2866—Architectures; Arrangements
- H04L67/30—Profiles
- H04L67/303—Terminal profiles
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/16—Implementation or adaptation of Internet protocol [IP], of transmission control protocol [TCP] or of user datagram protocol [UDP]
- H04L69/161—Implementation details of TCP/IP or UDP/IP stack architecture; Specification of modified or new header fields
- H04L69/162—Implementation details of TCP/IP or UDP/IP stack architecture; Specification of modified or new header fields involving adaptations of sockets based mechanisms
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/4104—Peripherals receiving signals from specially adapted client devices
- H04N21/4126—The peripheral being portable, e.g. PDAs or mobile phones
- H04N21/41265—The peripheral being portable, e.g. PDAs or mobile phones having a remote control device for bidirectional communication between the remote control device and client device
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42203—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS] sound input device, e.g. microphone
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/42204—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor
- H04N21/42206—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor characterized by hardware details
- H04N21/42225—User interfaces specially adapted for controlling a client device through a remote control device; Remote control devices therefor characterized by hardware details characterized by types of remote control, e.g. universal remote control
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/436—Interfacing a local distribution network, e.g. communicating with another STB or one or more peripheral devices inside the home
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/436—Interfacing a local distribution network, e.g. communicating with another STB or one or more peripheral devices inside the home
- H04N21/43615—Interfacing a Home Network, e.g. for connecting the client to a plurality of peripherals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/442—Monitoring of processes or resources, e.g. detecting the failure of a recording device, monitoring the downstream bandwidth, the number of times a movie has been viewed, the storage space available from the internal hard disk
- H04N21/44213—Monitoring of end-user related data
- H04N21/44222—Analytics of user selections, e.g. selection of programs or purchase activity
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/442—Monitoring of processes or resources, e.g. detecting the failure of a recording device, monitoring the downstream bandwidth, the number of times a movie has been viewed, the storage space available from the internal hard disk
- H04N21/44227—Monitoring of local network, e.g. connection or bandwidth variations; Detecting new devices in the local network
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04Q—SELECTING
- H04Q9/00—Arrangements in telecontrol or telemetry systems for selectively calling a substation from a main station, in which substation desired apparatus is selected for applying a control signal thereto or for obtaining measured values therefrom
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W12/00—Security arrangements; Authentication; Protecting privacy or anonymity
- H04W12/50—Secure pairing of devices
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W76/00—Connection management
- H04W76/10—Connection setup
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/18—Multiprotocol handlers, e.g. single devices capable of handling multiple protocols
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/24—Negotiation of communication capabilities
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04Q—SELECTING
- H04Q2209/00—Arrangements in telecontrol or telemetry systems
- H04Q2209/40—Arrangements in telecontrol or telemetry systems using a wireless architecture
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W8/00—Network data management
- H04W8/005—Discovery of network devices, e.g. terminals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W8/00—Network data management
- H04W8/22—Processing or transfer of terminal data, e.g. status or physical capabilities
- H04W8/24—Transfer of terminal data
Abstract
Virtual remote control between digital assistant devices is provided. The first computing device detects the second computing device and determines capabilities of the second computing device. The first computing device generates a prompt indicating that the first computing device is capable of controlling the second computing device. In response to the prompt, the first computing device receives an instruction to control the second computing device. The first computing device establishes a communication channel with the second computing device. The first computing device invokes a virtual controller on the first computing device. The virtual controller forwards a query received by the first computing device to the second computing device via the communication channel to control the second computing device.
Description
Background
A computing device, such as a television, may receive instructions from a remote control dedicated to the computing device. However, the computing device or remote control may have limited functionality or limited interfaces.
Disclosure of Invention
The present disclosure is generally directed to virtual remote control between digital assistant devices. One physical location may contain or include multiple computing devices. Computing devices may run different programs to form a fragmented, silo-like (silo) environment in which it is challenging or impossible for one computing device to communicate with another computing device. For example, each computing device may have different types of hardware and operating systems that are customized or configured for the particular computing device. Furthermore, tracking or determining which computing device has which capabilities can be challenging, resulting in a poor user experience or inability to perform certain functions efficiently due to lack of device capabilities. For example, a smart television or monitor equipped with a digital assistant may be controlled by a physical remote control. However, controlling a smart television or monitor may be challenging if the remote control is unavailable, lost, or inoperative. Furthermore, smart televisions or monitors may have limited interfaces or be configured with poor user interfaces.
The system and method of the present technology provides virtual remote control between digital assistant devices. In an illustrative example, a wearable computing device, such as a smart watch, may be brought within close proximity of a smart television computing device that is broadcasting a presence signal. The wearable computing device may detect the presence signal of the smart television and then determine the capabilities of the smart television. The smart watch may display an icon indicating the capability for controlling the smart television from the smart watch. In response to an input from a user, the smart watch may perform a handshake process to establish a communication channel with the smart television and invoke a virtual controller on the smart watch. The smart watch may then receive a voice query or other input from the user of the smart watch, which may be forwarded to the smart television for processing. The smart tv may receive the query from the smart watch and process the query as if the query was received directly by the smart tv, thereby providing seamless virtual control of the smart tv via the smart watch. When the smart watch is paired with the smart television, the smart watch may display a persistent indicator on the smart watch to indicate the active communication channel and the virtual controller.
At least one aspect is directed to a system for dynamic remote control of a device. The system may include a first computing device having one or more processors and memory. A first computing device may detect a second computing device that is within range of the first computing device. The first computing device may determine capabilities of the second computing device in response to detecting the second computing device. The first computing device may generate a prompt indicating that the first computing device is capable of controlling the second computing device based on the capabilities of the second computing device. The first computing device may receive an instruction to control the second computing device in response to the prompt. The first computing device may establish a communication channel with the second computing device in response to the instruction. The first computing device may invoke a virtual controller on the first computing device. The virtual controller may forward a query received by the first computing device to the second computing device via the communication channel to control the second computing device.
At least one aspect is directed to a method of dynamic remote control of a device. The method may be performed by a first computing device having one or more processors and memory. The method may include a first computing device detecting a second computing device that is within range of the first computing device. The method may include the first computing device determining capabilities of the second computing device in response to detecting the second computing device. The method may include the first computing device generating a prompt indicating that the first computing device is capable of controlling the second computing device based on capabilities of the second computing device. The method may include the first computing device receiving an instruction to control the second computing device in response to the prompt. The method may include the first computing device establishing a communication channel with the second computing device in response to the instruction. The method may include the first computing device invoking a virtual controller on the first computing device. The virtual controller may forward a query received by the first computing device to the second computing device via the communication channel to control the second computing device.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings provide an illustration and a further understanding of the various aspects and embodiments, and are incorporated in and constitute a part of this specification.
Drawings
The drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the drawings:
FIG. 1 is an illustration of an example system for virtual remote control between digital assistant devices, according to an embodiment;
FIG. 2 is an illustration of example operation of a system for virtual remote control between digital assistant devices, according to an embodiment;
FIG. 3 is an illustration of an example method of virtual remote control between digital assistant devices according to an embodiment;
fig. 4 is a block diagram illustrating an architecture of a computer system that may be used to implement elements of the systems and methods described and illustrated herein, including, for example, the systems depicted in fig. 1 and 2 and the method depicted in fig. 3.
Detailed Description
Following are more detailed descriptions of various concepts related to methods, apparatus, and systems for virtual remote control between digital assistant devices and embodiments thereof. The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways.
The present solution is generally directed to virtual remote control between digital assistant devices. For example, digital assistants are available in more than 10 billion computing devices around the world. However, these computing devices contain a variety of hardware and software, resulting in significant fragmentation between devices and an inability to efficiently communicate between devices to perform functions that the devices are capable of performing. Thus, the present technology may provide a virtual controller designed, constructed, and operated to seamlessly control other devices (e.g., televisions, smart displays, automobiles, or Internet of Things (IoT) devices) with a user's personal device (e.g., mobile device, phone, watch, speaker, or headset) in a manner that provides the same user interface and user experience as if the user had physically interacted directly with the target device with its physical remote controls, touch, or other default interface.
Providing cross-device control to perform various action sequences or key user tours can be challenging. There may be a predefined limited set of key user itineraries configured for cross device control. The target device may be configured to perform a set of actions, while a source device other than the target device may only perform a small subset of these actions on the target device. For example, the target device may be able to complete the following query: showing me an action movie; showing me today's weather; showing me photos to me; playing the movie _ name; pause/resume; my TV is turned on. However, when a query is first received by a different device (e.g., a source device) and then the source device attempts to control the target device, the target device may only be able to complete a subset of the query. For example, the source device may not be able to perform the following functions on the target device: show me an action movie on my TV; show me today weather on my TV; and show me my photos on my TV. Furthermore, such cross-device control may require additional or excessive input to function properly. For example, the user may be required to enter the name of the target device along with each query. This may require not only the user to provide additional input, but also the source computing device or other computing device to perform additional natural language processing or other processing to parse the query and identify the name of the target device. Furthermore, since the crossover devices and target devices are built as silos, they may have duplicate and redundant stacks that perform the same or similar functions, unnecessarily complicating the stacks and accumulating technical debt without providing or detracting from a good user experience.
Thus, the systems and methods of the present technology may provide a virtual controller that may provide seamless cross-device control. The virtual controller of the present technical solution can provide the same user experience on the target device regardless of which device is the source device on which the virtual controller is invoked. By providing an active user interface to guide a user to access other target devices without requiring the user to learn a new user interface, the virtual controller can be easily used. The virtual controller of the present solution may utilize a single unified horizontal stack that allows the same experience across multiple digital surfaces and target devices.
Fig. 1 illustrates an example system 100 for virtual remote control between digital assistant devices, according to an embodiment. The system 100 may include a content selection infrastructure. System 100 may include a data processing system 102. The data processing system 102 may communicate with one or more of the source computing device 112, the target computing device 132, or the supplemental digital content provider device 144 via the network 105. The network 105 may include a computer network, such as the internet, a local area network, a wide area network, a metropolitan area network, or other area network, an intranet, a satellite network, and other communication networks (such as voice or data mobile telephone networks). The network 105 may be used to access information resources, such as web pages, web sites, domain names, or uniform resource locators, that may be provided, output, presented, or displayed on at least one of the source computing device 112 or the target computing device 132. The source computing device 112 may include, for example, a laptop, a desktop, a tablet, a digital assistant device, a smart phone, a mobile telecommunication device, a portable computer, a smart watch, a headset, or a speaker. The target computing device 132 may include, for example, a television, a smart display, an automotive unit, or a networking device (e.g., an internet of things (IoT) device). For example, via the network 105, a user of the source computing device 112 or the target computing device 132 may access information or data provided by the supplemental digital content provider device 144. In some cases, the source computing device 112 or the target computing device 132 may or may not include a display; for example, a computing device may include a limited type of user interface, such as a microphone and a speaker. In some cases, the primary user interface of the computing device 112 or 132 may be a microphone and speaker, or a voice interface. In some cases, the computing device 112 includes a display device 122 coupled to the source computing device 112, and the primary user interface of the computing device 112 may utilize the display device 122. The target computing device 132 may include one or more output interfaces, such as a display device or speakers.
The source computing device 112 may refer to a computing device being used by or owned by a user. The source computing device 112 may be a wearable device or a mobile device. The source computing device 112 may be with the user as the user travels or moves from one location to another. The source computing device 112 may be a stationary device or a device that remains in the same location for some period of time. Target computing device 132 may refer to a computing device or client device located at a private location, such as a residence, cell house, home, apartment, condominium. The target computing device 132 may be located in a public place such as a hotel, office, restaurant, retail store, mall, or park.
The source computing device 112 may be referred to as a crossbar device. Source computing device 112 may refer to a device with which a user interacts to control target computing device 132. The target computing device 132 may refer to a device being controlled by the source computing device 112.
The network 105 may include or constitute a display network, e.g., a subset of information resources available on the internet, that is associated with a content placement or search engine results system, or that is eligible to include third party digital components as part of a digital component placement activity. The data processing system 102 may use the network 105 to access information resources, such as web pages, websites, domain names, or uniform resource locators that may be provided, output, rendered, or displayed by the source client computing device 112 or the target computing device 132. For example, via the network 105, a user of the source computing device 112 or the target computing device 132 may access information or data provided by the supplemental digital content provider device 144.
The network 105 may be any type or form of network and may include any of the following: a point-to-point Network, a broadcast Network, a wide area Network, a local area Network, a telecommunications Network, a data communication Network, a computer Network, an ATM (Asynchronous Transfer Mode) Network, a SONET (Synchronous Optical Network) Network, an SDH (Synchronous Digital Hierarchy) Network, a wireless Network, and a wired Network. The network 105 may include a wireless link, such as an infrared channel or satellite band. The topology of the network 105 may include a bus, star, or ring network topology. The network may comprise a mobile telephone network using any one or more protocols for communication between mobile devices, including advanced mobile phone protocol (AMPS), time Division Multiple Access (TDMA), code Division Multiple Access (CDMA), global system for mobile communications (GSM), general Packet Radio Service (GPRS), or Universal Mobile Telecommunications System (UMTS). Different types of data may be transmitted via different protocols, or the same type of data may be transmitted via different protocols.
The servers in the machine farm may be stored in a high density rack system along with associated storage systems and located in an enterprise data center. For example, merging servers in this manner may improve system manageability, data security, physical security, and system performance of the system by locating the servers and high performance storage systems on a local high performance network. The centralization of all or some of the data processing system 102 components, including servers and storage systems, and coupling them with advanced system management tools allows for more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage.
The supplemental digital content provider device 144 may provide audio-based digital components for display by the source computing device 112 or the target computing device 132 as audio output digital components. The digital component may be referred to as a sponsored digital component because it is provided by a third party sponsor. The digital component may include offers for goods or services, such as voice-based messages: "do you want me to reserve a taxi for you? "for example, the supplemental digital content provider device 144 may include a memory for storing a series of audio digital components that may be provided in response to a voice-based query. The supplemental digital content provider device 144 may also provide audio-based digital components (or other digital components) to the data processing system 102, where these digital components may be stored in a data repository of the data processing system 102. The data processing system 102 can select the audio digital component and provide (or instruct the supplemental digital content provider device 144 to provide) the audio digital component to the source computing device 112. The audio-based digital component may be audio only, or may be combined with text, image, or video data.
The data processing system 102 may include a content placement system having at least one computing resource or server. The data processing system 102 may include, interface with, or communicate with at least one interface 104. The data processing system 102 may include, interface with, or communicate with at least one natural language processor 106 (or natural language processor component). The interface 104 or the natural language processor 106 may form or be referred to as a server digital assistant component. The data processing system 102 may include, interface with, or communicate with at least one server digital assistant 108 (or server digital assistant component). The server digital assistant 108 can communicate or interface with one or more voice-based interfaces or various digital assistant devices or surfaces to provide data or receive data or perform other functions. The data processing system 102 may include at least one content selector 110 (or content selector component).
The data processing system 102, the interface 104, the NLP106, or the content selector 110 may each include at least one processing unit or other logic device, such as a programmable logic array engine, or module configured to communicate with a data repository or database of the data processing system 102. The interface 104, NLP106, or content selector 110 may be a separate component, a single component, or part of the data processing system 102. System 100 and its components, such as data processing system 102, may include hardware elements, such as one or more processors, logic devices, or circuits.
The data processing system 102 may obtain anonymous computer network activity information associated with a plurality of source computing devices 112 (or computing devices or digital assistant devices) or target computing devices 132. The user of the source computing device 112 or mobile computing device may affirmatively authorize the data processing system 102 to obtain network activity information corresponding to the source computing device 112 or mobile computing device. For example, the data processing system 102 may prompt the user of the source computing device 112 for consent to obtain one or more types of network activity information. The source computing device 112 may include a mobile computing device, such as a smartphone, tablet, smart watch, or wearable device. The identity of the user of the source computing device 112 may remain anonymous, and the source computing device 112 may be associated with a unique identifier (e.g., a unique identifier of the user or computing device provided by the data processing system or the user of the computing device). The data processing system 102 can associate each observation with a corresponding unique identifier.
The data processing system 102 may include an interface 104 (or interface component) that the interface 104 is designed, configured, constructed, or operated to receive and transmit information using, for example, data packets. The interface 104 may receive and transmit information using one or more protocols, such as a network protocol. The interface 104 may include a hardware interface, a software interface, a wired interface, or a wireless interface. The interface 104 may facilitate converting or formatting data from one format to another. For example, the interface 104 may include an application programming interface that includes definitions for communication between various components (such as software components). The interface 104 may communicate with one or more of the source computing device 112, the supplemental digital content provider device 144, or the target computing device 132 via the network 105.
The data processing system 102 may interface with an application, script, or program (such as an app) installed at the source computing device 112 or the target computing device 132 to communicate input audio signals to the interface 104 of the data processing system 102 and to drive components of the source client computing device 112 or the target computing device 132 to render output visual or audio signals. The data processing system 102 may receive data packets or other signals that include or identify audio input signals.
The data processing system 102 may include a Natural Language Processor (NLP) 106. For example, the data processing system 102 may execute or run the NLP106 to parse a received input audio signal or query. For example, the NLP106 may provide for interaction between a human and a computer. The NLP106 may be configured with techniques for understanding natural language and allowing the data processing system 102 to derive meaning from human or natural language input. The NLP106 may include or be configured with machine learning based techniques, such as statistical machine learning. The NLP106 may parse the input audio signal using a decision tree, statistical model, or probabilistic model. NLP106 may perform functions such as, for example, named entity recognition (e.g., determining which items in text map to appropriate names (such as people or places) and what type (such as people, places, or organizations) each such name is given a text stream), natural language generation (e.g., converting information or semantic intent from a computer database into intelligible human language), natural language understanding (e.g., converting text into a more formal representation such as a first-order logical structure that a computer module can manipulate), machine translation (e.g., automatically translating text from one human language to another), morphological segmentation (e.g., separating words into individual morphemes and identifying classes of morphemes, which may be challenging based on the morphological or structural complexity of the words of the language under consideration), question answering (e.g., determining answers to human language questions, which questions may be specific or open), semantic processing (e.g., processing may occur after identifying words and encoding their meanings in order to associate the identified words with other words having similar meanings).
The NLP106 may convert the audio input signal into recognized text by comparing the input signal to a stored set of representative audio waveforms and selecting the closest match. The set of audio waveforms may be stored in a data repository or other database accessible to the data processing system 102. The representative waveform is generated across a larger set of users, which can then be augmented with speech samples from the users. After the audio signal is converted to recognized text, the NLP106 matches the associated text-to-word with actions that the data processing system 102 can provide (e.g., via cross-user training or through manual specification). Aspects or functions of the NLP106 may be performed by the data processing system 102, the source computing device 112, or the target computing device 132. For example, the NLP component may execute on the source computing device 112 or the target computing device 132 to perform aspects of converting the input audio signal to text and sending the text via data packets to the data processing system 102 for further natural language processing.
The audio input signal may be detected by a sensor or transducer (e.g., a microphone) of the source client computing device 112. Via the transducer, audio driver, or other component, the source computing device 112 can provide an audio input signal to the data processing system 102 (e.g., via the network 105), where it can be received (e.g., by the interface 104) and provided to the NLP106 or stored in a data repository.
The data processing system 102 may receive, via the interface 104, a data packet including an input audio signal detected by a microphone of the source computing device 112 or a microphone of the target computing device 132. The data processing system 102 may receive data packets generated based on input audio signals detected by the microphone. The data packets may be filtered or unfiltered. The data packet may be a digitized version of the detected input audio signal. The data packet may include text generated by the source computing device 112 or the target computing device 132 based on the detected input audio signal. For example, the source digital assistant 114 of the source computing device 112 or the target digital assistant 142 of the target computing device 132 may process the detected input audio signal and send data packets to the server digital assistant 108 for further processing or to perform actions based on the processed input audio signal.
The data processing system 102 may include a server digital assistant 108. Server digital assistant 108 and NLP106 can be a single component, or server digital assistant 108 can include one or more components or functions of NLP 106. The server digital assistant 108 can interface with the NLP 106. The data processing system 102 (e.g., server digital assistant 108) may process the data packets to perform actions or otherwise respond to voice input. In some cases, the data processing system 102 may identify the acoustic marker from the input audio signal. The data processing system 102 can identify an electronic account corresponding to the acoustic token based on a lookup in a data repository (e.g., a query database). In response to the identification of the electronic account, the data processing system 102 can establish the session and the account used in the session. An account may include a profile with one or more policies. The data processing system 102 may parse the input audio signal to identify the request and the trigger keyword corresponding to the request.
The NLP106 may obtain an input audio signal. In response to the digital assistant detecting the trigger keyword, the NLP106 of the data processing system 102 can receive a data packet with a speech input or an input audio signal. The trigger keyword may be a wake signal or a hotword that instructs the source computing device 112 to convert subsequent audio input into text and send the text to the data processing system 102 for further processing.
Upon receiving the input audio signal, the NLP106 may identify at least one request or at least one keyword corresponding to the request. The request may indicate an intent or subject matter of the input audio signal. The keywords may indicate the type of action that is likely to be taken. For example, the NLP106 may parse the input audio signal to identify at least one request for leaving home in the evening to attend dinner and movies. The trigger key may comprise at least one word, phrase, root or partial word, or derivative indicating an action to be taken. For example, the trigger keyword "go" or "to go to" from the input audio signal may indicate a need for transmission. In this example, the input audio signal (or the identified request) does not directly represent an intent for transmission, however the trigger keyword indicates that the transmission is an auxiliary action to the at least one other action indicated by the request. In another example, the voice input may include a search query, such as "find work near me".
The NLP106 may parse the input audio signal to identify, determine, retrieve, or otherwise obtain the request and one or more keywords associated with the request. For example, the NLP106 may apply semantic processing techniques to the input audio signal to identify keywords or requests. The NLP106 may apply semantic processing techniques to the input audio signal to identify keywords or phrases that include one or more keywords, such as a first keyword and a second keyword. For example, the input audio signal may comprise the sentence "i want to purchase an audio reading". The NLP106 may apply semantic processing techniques or other natural language processing techniques to the data packets comprising sentences to identify keywords or phrases "want to purchase" and "voiced books". The NLP106 can further identify multiple keywords, such as purchases and audiobooks. For example, the NLP106 may determine that the phrase includes a first keyword and a second keyword.
The NLP106 may filter the input audio signal to identify the trigger keyword. For example, a data packet carrying an input audio signal may include "It would be a linear be I complex get means that is one that is complex help me to the airport" in which case the NLP106 may filter out one or more of the following words: "it", "would", "be", "good", "if", "I", "could", "get", "someone", "that", "could", or "help". By filtering out these words, the NLP106 may more accurately and reliably identify a trigger keyword, such as "go to the airport", and determine that this is a request for taxi or carpool service.
In some cases, the NLP106 may determine that the data packet carrying the input audio signal includes one or more requests. For example, the input audio signal may include the sentence "show me an action movie". The NLP106 may determine that this is a request to play an action movie. The server digital assistant 108 can send a request for content to the content selector 110 based on the input audio signal. The server digital assistant 108 can send a request for supplemental or sponsored content from a third-party content provider. The content selector 110 can perform a content selection process to select supplemental content items or sponsored content items based on actions in the voice query. The content item may be a sponsored or supplemental digital component object. The content items may be provided by third-party content providers, such as supplemental digital content provider device 144. The supplemental content items may include advertisements for goods or services. In response to receiving a request for content from the server digital assistant 108, the content selector 110 can select a content item using content selection criteria.
The server digital assistant 108 can receive supplemental or sponsored content items from the content selector 110. The server digital assistant 108 can receive the content item in response to the request. The server digital assistant 108 can receive the content item from the content selector 110 and present the content item via audio output or visual output. The server digital assistant 108 can provide the content item for presentation via the target computing device 132 or the source computing device 112 communicatively coupled with the target computing device 132.
The data processing system 102 can include a content selector 110, the content selector 110 designed, constructed, or operated to select a supplemental content item (or sponsored content item or digital component object). To select sponsored content items or digital components, the content selector 110 may use the generated content selection criteria to select matching sponsored content items based on a broad match, an exact match, or a phrase match. For example, the content selector 110 may analyze, parse, or otherwise process the subject matter of the candidate sponsored content item to determine whether the subject matter of the candidate sponsored content item corresponds to the subject matter (e.g., action or intent) of the keyword or phrase of the content selection criteria. The content selector 110 may use image processing techniques, character recognition techniques, natural language processing techniques, or database lookups to identify, analyze, or recognize speech, audio, words, characters, text, symbols, or images of candidate digital components. The candidate sponsored content item may include metadata indicating the subject matter of the candidate digital component, in which case the content selector 110 may process the metadata to determine whether the subject matter of the candidate digital component corresponds to the input audio signal. The content campaign provided by the supplemental digital content provider device 144 may include content selection criteria that the data processing system 102 may match with criteria indicated in the second profile tier or the first profile tier.
The supplemental digital content provider may provide additional indicators when establishing a content campaign that includes digital components. The supplemental digital content provider device 144 may provide information at the content activity or content group level that the content selector 110 may identify by performing a lookup using information about candidate digital components. For example, the candidate digital components may include unique identifiers that may map to content groups, content campaigns, or content providers.
In response to the request, the content selector 110 may select a digital component object associated with the supplemental digital content provider device 144. The supplemental digital content may be provided by a supplemental digital content provider. The supplemental digital content may correspond to a service type different from the service type of the action data structure (e.g., a taxi service and a food delivery service). Computing device 112 or 132 may interact with the supplemental digital content. The computing device 112 or 132 may receive an audio response to the digital component. The computing device 112 or 132 may receive an indication to select a hyperlink or other button associated with the digital component object that causes or allows the computing device 112 or 132 to identify the supplemental digital content provider device 144, request a service from the supplemental digital content provider device 144, instruct the supplemental digital content provider device 144 to perform the service, send information to the supplemental digital content provider device 144, or otherwise query the supplemental digital content provider device 144.
The supplemental digital content provider device 144 may establish an electronic content campaign. An electronic content campaign may refer to one or more groups of content that correspond to a common topic. The content campaign may include a hierarchical data structure including content groups, digital component data objects, and content selection criteria provided by the content provider. The content selection criteria provided by the content provider device 144 may include a content type, such as a digital assistant content type, a search content type, a streaming video content type, a streamlined audio content type, or a contextual content type. To create a content campaign, the supplemental digital content provider device 144 may specify a value for an activity level parameter of the content campaign. The activity level parameters may include, for example, the name of the activity, a preferred content network for placing the digital component object, the resource value to be used for the content activity, the start and end dates of the content activity, the duration of the content activity, the schedule for placement of the digital component object, the language, the geographic location, the type of computing device on which the digital component object is provided. In some cases, an impression may refer to when a digital component object is fetched from its source (e.g., the data processing system 102 or the supplemental digital content provider device 144) and is countable. In some cases, robot activity may be filtered and excluded as an impression due to the possibility of click fraud. Thus, in some cases, an impression may refer to a measure of responses from the web server to page requests from the browser, filtered from the robot activity and error codes, and recorded at a point as close as possible to the opportunity to render the digital component object for display on the source computing device 112 or the target computing device 132. In some cases, an impression may refer to a visible or audible impression; for example, the digital component object is at least partially (e.g., 20%, 30%, 40%, 50%, 60%, 70%, or more) visible on a display device of the computing device or audible via a speaker of the source computing device 112 or the target computing device 132. Clicking or selecting may refer to user interaction with a digital component object, such as a voice response to an audible impression, a mouse click, a touch interaction, a gesture, a shake, an audio interaction, or a keyboard click. Conversion may refer to a user taking a desired action with respect to a digital component object; for example, purchasing a product or service, completing a survey, visiting a brick and mortar store corresponding to a digital component, or completing an electronic transaction.
The supplemental digital content provider device 144 may also establish one or more content groups for the content campaign. The content group includes one or more digital component objects and corresponding content selection criteria, such as keywords, words, phrases, geographic locations, computing device type, time of day, interests, topics, or verticals. Content groups under the same content campaign may share the same campaign level parameters, but may have customized specifications for particular content group level parameters, such as keywords, negative keywords (e.g., blocking placement of digital components if negative keywords are present on the primary content), bids for keywords, or parameters associated with bids or content campaigns.
To create a new content group, the content provider may provide a value for a content group level parameter for the content group. Content group level parameters include, for example, content group name or content group subject, and bids for different content placement opportunities (e.g., automatic placement or managed placement) or results (e.g., clicks, impressions, or conversions). The content group name or content group topic may be one or more terms that the supplemental digital content provider device 144 may use to capture the topic or topic that the digital component object of the content group will select for display. For example, an auto dealer may create a different content set for each brand of vehicle that it sells, and may also create a different content set for each model of vehicle that it sells. Examples of content group themes that may be used by an automobile dealer include, for example, "sports a", "sports B", "car C", "truck C", "mixed C", or "mixed D". For example, an example content campaign topic may be "mix" and include content groups of both "type C mix" and "type D mix".
The supplemental digital content provider device 144 may provide one or more keywords and digital component objects to each content group. The keywords may include words related to the product or service, associated with or identified by the digital component object. The keywords may include one or more words or phrases. For example, an automobile dealer may include "sports car," "V-6 Engine," "four wheel drive," "Fuel efficiency," as keywords for a content group or content campaign. In some cases, the content provider may specify negative keywords to avoid, prevent, block, or disable content placement on certain words or keywords. The content provider may specify a match type, such as an exact match, a phrase match, or a broad match, for selecting the digital component object.
The supplemental digital content provider device 144 may provide one or more keywords for use by the data processing system 102 in selecting the digital component object provided by the supplemental digital content provider device 144. The supplemental digital content provider device 144 may identify one or more keywords to bid on and further provide bid amounts for the various keywords. The supplemental digital content provider device 144 may provide additional content selection criteria that are used by the data processing system 102 to select digital component objects. The plurality of supplemental digital content provider devices 144 may bid on the same or different keywords and the data processing system 102 may run a content selection process or an advertisement auction in response to receiving an indication of a keyword of an electronic message.
The supplemental digital content provider device 144 may provide one or more digital component objects for selection by the data processing system 102. The data processing system 102 (e.g., via the content selector 110) may select digital component objects that match resource allocation, content scheduling, top bids, keywords, and other selection criteria specified for the content group when content placement opportunities become available. Different types of digital component objects may be included in the content set, such as a voice digital component, an audio digital component, a text digital component, an image digital component, a video digital component, a multimedia digital component, a digital component link, or an assistant application component. The digital component object (or digital component, supplemental content item, or sponsored content item) may include, for example, a content item, an online document, audio, an image, a video, multimedia content, sponsored content, or an assistant application. When a digital component is selected, the data processing system 102 can send the digital component object for rendering (rendering) on the source computing device 112 or the target computing device 132 or a display device thereof. Rendering may include displaying the digital component on a display device, executing an application such as a chat robot or a conversation robot, or playing the digital component via a speaker of the source computing device 112 or the target computing device 132. The data processing system 102 may provide instructions for rendering the digital component object to the target computing device 132. The data processing system 102 may instruct the target computing device 132 to generate an audio signal or sound wave.
In response to the request, the content selector 110 may perform a real-time content selection process. Real-time content selection may refer to or include performing content selection in response to a request. Real-time may refer to or include selecting content within 0.2 seconds, 0.3 seconds, 0.4 seconds, 0.5 seconds, 0.6 seconds, or 1 second of receiving the request. Real-time may refer to selecting content in response to receiving an input audio signal from the target computing device 132.
The content selector 110 may identify a plurality of candidate supplemental content items. The content selector 110 may determine a score or rank for each of a plurality of candidate supplemental content items in order to select the highest ranked supplemental content item to provide to the computing device.
The system 100 may include, interface with, or otherwise communicate with a target computing device 132. The target computing device 132 may include a target digital assistant 142. The target digital assistant 142 may interface with the server digital assistant 108. The target digital assistant 142 may include one or more components or functions of the server digital assistant 108. The target digital assistant 142 may, for example, receive an input voice query and complete the query or request by performing an action. For example, the target computing device 132 may be a television or smart display. The target digital assistant 142 may receive an input audio signal with a query or request as follows: "turn up volume", "turn down volume", "change channel", "play action movie", "pause", "resume", or "turn on TV". When an input query is received, the target digital assistant 142 may interface with one or more of the controller 138 or the query processor 134 to parse the query and perform a corresponding action. The target digital assistant 142 may include one or more components or functions of the query processor 134.
The target computing device 132 may include a controller 138, the controller 138 designed, configured, and operated to control functions or aspects of the target computing device 132. The controller 138 may provide remote control of the target computing device 132. The controller 138 may receive signals or instructions from hardware, a physical remote control configured for use with the target computing device 132. The controller 138 may establish a communication channel with the virtual controller 120 of the source computing device 112 and receive a query from the virtual controller 120.
The controller 138 may include a gRPC service that receives input via the network 105 (such as a local WIFI network). The controller 138 may receive the query or interaction and forward the query or interaction to the query processor 134 for further processing or completion. Query processor 134 may parse the query, determine the action to perform, and then perform the action. For example, query processor 134 may interface with platform 136 to control what is displayed on target computing device 132. Query processor 134 may interface with data processing system 102 to determine what action to take. The query processor 134 may interface with the target digital assistant 142 to perform actions.
The query processor 134 may include one or more components or functions of the target digital assistant 142 or NLP106 to parse and complete the query. The query processor 134 may receive a query from the controller 138, the platform 136, or the target digital assistant 142.
The target computing device 132 may include a signal broadcaster 140, the signal broadcaster 140 being designed, constructed and operative to broadcast wireless signals. Signal broadcaster 140 may broadcast wireless signals using one or more wireless protocols. Signal broadcaster 140 may broadcast the signal using a short-range wireless protocol, such as bluetooth. The signal broadcaster 140 may broadcast the signal using a near field communication protocol. Signal broadcaster 140 may broadcast signals using a WIFI protocol. The signal broadcaster 140 may broadcast the signal using ZigBee or other wireless protocols.
The signal broadcaster 140 may broadcast signals in response to trigger conditions or events. For example, the signal broadcaster 140 may broadcast the signal in response to detecting motion or proximity to the target computing device 132. For example, the target computing device 132 may include or be coupled to a proximity sensor. Proximity sensors can detect the presence of nearby objects without any physical contact using electromagnetic fields or beams of electromagnetic radiation (e.g., infrared) and identify changes in the field or return signal. In response to detecting a change in the presence or return, the proximity sensor may provide an indication to the signal broadcaster 140 to broadcast the signal.
The signal broadcaster 140 may broadcast a signal with information. The information may identify the target computing device 132 and capabilities of the target computing device 132. The information may identify a name (e.g., identifier) or type of the target computing device 132. For example, the name of the target computing device 132 may be "living room TV," "bedroom TV," "study TV," "meeting room a display," or other name or label established by an administrator, user, or owner of the target computing device 132. The information may include or indicate capabilities of the target computing device 132. Capabilities may refer to what types of functions the target computing device 132 is capable of performing. Capabilities may refer to what type of device the target computing device 132 is. For example, the capability may be a smart display, a smart television, an automotive unit, a networking device, or an IoT device. The capabilities may indicate functions such as multimedia streaming, movie services, music services, appliance functions, or car functions. The signal broadcaster 140 can broadcast additional information that can facilitate the source computing device 112 in identifying the target computing device 132.
The system 100 may include, interface with, or communicate with a source computing device 112. The source computing device 112 may include or refer to a phone, watch, wearable device, speaker, headset, or other type of device. The source computing device 112 may include a source digital assistant 114. The source digital assistant 114 may include one or more components or functions of the server digital assistant 108 or the target digital assistant 142 or the NLP 106. Source digital assistant 114 can interface or communicate with server digital assistant 108 to complete or perform actions in response to voice-based queries received or detected by source computing device 112.
The source computing device 112 or the target computing device 132 may include, interface with, or otherwise have access to one or more of a sensor, a microphone, a speaker, a display device, a transducer, or an audio driver. For example, source computing device 112 may include a display device 122 and a microphone 124. The display device 122 may include, for example, a light indicator, a Light Emitting Diode (LED), an Organic Light Emitting Diode (OLED), or other visual indicators configured to provide visual or optical output. In some cases, the target computing device 132 may include a display device, such as a television display. The sensors may include, for example, ambient light sensors, proximity sensors, temperature sensors, accelerometers, gyroscopes, motion detectors, GPS sensors, location sensors, microphones, or touch sensors. The transducer may comprise a speaker or a microphone. The audio driver may provide a software interface to the hardware converter. The audio driver may execute audio files or other instructions to control the transducer to generate corresponding sound or sound waves.
The sensor may receive or detect an input audio signal (e.g., a voice input). A digital assistant (e.g., source digital assistant 114 or target digital assistant 142) may be coupled to the audio driver, transducer, and sensor. The digital assistant may filter the input audio signal to create a filtered input audio signal (e.g., by removing certain frequencies or suppressing noise). The digital assistant may convert the filtered input audio signal into data packets (e.g., using a software or hardware digital-to-analog converter). In some cases, the digital assistant may convert the unfiltered input audio signal into data packets and send the data packets to the data processing system 102. The digital assistant can send the data packets to a data processing system 102 that includes one or more processors and memory that execute natural language processor components. One or more components of the source computing device 112 or the target computing device 132 may each include at least one processing unit or other logic device, such as a programmable logic array engine, component, or module. The system 100 and its components may comprise hardware elements, such as one or more processors, logic devices, or circuits.
The source computing device 112 or the target computing device 132 may include, interface with, or otherwise communicate with at least one application. The application may be installed on the source computing device 112 or the target computing device 132. The applications may be downloaded from an online application marketplace, such as the online marketplace provided by the data processing system 102. The applications may include native applications installed on the operating system of the source computing device 112 or the target computing device 132 by the manufacturer of the source computing device 112 or the target computing device 132. The applications may include any type of application capable of providing a resource or service. For example, the application may be a daily workout application, an order application, a taxi-taking application, a weather application, a document processing application, a navigation application, a messaging application, a telephony application, a streaming media application, a social networking application, a calendar application, a camera application, a ticketing application, an e-commerce application, a banking application, a financial services application, or the like.
The source computing device 112 may include a data repository 126. The data repository 126 may include one or more local or distributed databases. The data repository 126 may include a computer data store or memory and may store one or more profiles 128 or UI templates 130. The profile 128 may include a user profile, preferences, credentials, tokens, or other information that facilitates performing a function or completing a query or response. The UI template 130 may include visual or graphical user interface elements that may be used to generate prompts for display on the display device 122.
The source computing device 112 may include a source digital assistant 114. The source computing device 112 may include a digital assistant 114, the digital assistant 114 designed, constructed, and operative to receive a voice query or other audio input detected by a sensor of the source computing device 112, determine an intent or action based on the voice input, and cause a corresponding action to be completed. The digital assistant 114 may include one or more components or functions of the NLP106 or interface with the NLP 106. For example, the source digital assistant 114 may interface or communicate with the NLP106 of the data processing system 102 to parse or process voice input or audio input. In some cases, the source digital assistant 114 may be configured to process or parse the speech input without interfacing or communicating with the NLP106 of the data processing system 102. For example, source computing device 112 may perform digital assistant functions without communicating with data processing system 102 via network 105. The source computing device 112 may receive the voice query, parse the voice query, identify an action, and invoke an application on the source computing device 112 to complete the action without communicating with the data processing system 102 via the network 105.
The source computing device 112 may include a presence Application Programming Interface (API) 116, the presence API116 being designed, constructed and operated to detect the target computing device 132. The presence API116 may detect or sense signals broadcast by the signal broadcaster 140 of the target computing device 132. For example, the source computing device 112 may be referred to as a first computing device, and the target computing device 132 may be referred to as a second computing device.
The presence API116 may be configured to listen to signals broadcast by the signal broadcaster 140 on a continuous basis, a periodic basis, based on time intervals, or in response to events, conditions, or triggers. For example, the presence API116 may wake up or open a communication port or search signal every 0.5 seconds, 1 second, 2 seconds, or other time interval. In another example, the presence API116 may listen for broadcast signals in response to gestures or actions made by the user or wearer of the source computing device 112. For example, the source computing device 112 may be a watch, and the user may lift their wrist wearing the watch or put their wrist down in a manner that causes or triggers the presence API116 to wake up or listen to the broadcast signal.
The presence API116 may detect signals broadcast by the signal broadcaster 140. In response to detecting the signal, the presence API116 may determine that the source computing device 112 is within range of the target computing device 132. The presence API116 may determine that the source computing device 112 is within a desired range, a threshold range, or another predetermined range of the target computing device 132. The presence API116 may determine a range or distance between the source computing device 112 and the target computing device based on various techniques. For example, the presence API116 may determine a distance or range from the target computing device 132 based on the signal strength broadcast by the signal broadcaster 140. The stronger the signal strength received or detected by the source computing device 112, the closer the source computing device 112 may be to the target computing device 132. The presence API116 may determine that the source computing device 112 is within range of the target computing device 132 if the signal strength detected at the source computing device 112 satisfies a strength threshold (e.g., amplitude or power). In some cases, the source computing device 112 and the target computing device 132 may exchange signals to calculate the distance between the two devices, such as based on the amount of time it takes for a signal to travel from one device to the other (e.g., by comparing timestamps associated with the transmission and reception of the signal). In some cases, the presence API116 may determine that the source computing device 112 is within range of the target computing device 132 based on the type of wireless protocol being used and whether the source computing device 112 is capable of interpreting the signal. For example, if the wireless protocol used by the signal broadcaster 140 to transmit signals is a near field communication protocol and the source computing device 112 is capable of processing or interpreting the signals, the source computing device 112 may determine that the target computing device 132 is within a satisfactory range.
The presence API116 may be configured with or access one or more communication ports or sensors of the source computing device 112 to identify signals broadcast by the signal broadcaster 140. The presence API116 may parse, process, or otherwise analyze the signal. The presence API116 may parse or process the signal to identify the target computing device 132. For example, the presence API116 may parse the signal to determine an identifier of the target computing device 132, such as a name of the target computing device 132. The signal may include one or more data packets having a header and a payload. The payload may include information about the target computing device 132 broadcasting the signal. The signal broadcaster 140 may be configured to embed or include information in the broadcasted signal. The presence API116 may analyze the signal to determine one or more of a name of the target computing device 132, a type of the target computing device 132, or capabilities of the target computing device 132.
The presence API116 may determine to generate a prompt for display on the source computing device 112. The cues may include one or more of visual cues, tactile cues, or audio cues. The prompt may include a display icon that accompanies one or more of an audio prompt (e.g., a beep or voice output) or tactile feedback. Presence API116 may select the type of hint based on information in the signal broadcast by signal broadcaster 140. The presence API116 may access the UI template 130 data structure to identify a design or template for the prompt and then generate the prompt based on the template. The UI template 130 may include different icons for different types of devices or capabilities. For example, if the target computing device 132 has a target digital assistant 142, the UI template 130 may include a microphone icon with a label or metadata indicating use of the microphone icon of the target computing device 132 with digital assistant capabilities.
For example, the presence API116 may determine the capabilities of the target computing device 132. Capabilities may include, for example, the target computing device 132 being a television that may play streaming media content from one or more streaming services, may provide weather, may have access to the data processing system 102, is configured with a target digital assistant 142, a controller 138, a query processor 134, including speakers, or other capability information. The presence API116 may determine whether to generate a prompt based on information in the signal or otherwise obtained from the target computing device 132. For example, a signal broadcast by the signal broadcaster 140 may indicate the capabilities of the target computing device 132 by including information in the signal indicating that the target computing device 132 includes one or more of the controller 138, the target digital assistant 142, or the query processor 134. The presence API116 may determine to generate the prompt in response to determining that the target computing device 132 includes one or more of the controller 138, the target digital assistant 142, or the query processor 134. For example, in response to determining that the target computing device 132 includes the controller 138 and the query processor 134, the presence API116 may determine to generate an icon with a prompt for a request asking the user whether to connect or pair with the target computing device 132.
The presence API116 may generate a prompt indicating that the source computing device 112 is able to control the target computing device 132 based on the capabilities of the target computing device 132. If the target computing device 132 includes the controller 138 and the query processor 134, the presence API116 may determine that the source computing device 112 may invoke the virtual controller 120, which may forward the query to the controller 138, which may be processed by the query processor 134 as if the query were detected by the microphones of the target digital assistant 142 and the target computing device 132.
In response to the prompt, the source computing device 112 may receive instructions for controlling the target computing device 132. The instructions may be received via a user interface of the source computing device 112. The instructions may be received via user interaction with the prompt. The user may interact with the prompt via a touch interface of the source computing device 112. The user may interact with the prompt via voice input. The user may interact with the prompt via a gesture. For example, the prompt may be a button or other user interface element that the user may select or click using the touch interface of the source computing device 112.
If the user does not interact with the reminder, or the source computing device 112 does not receive instructions to control the target computing device 132, the source computing device 112 may determine to remove or hide the reminder. For example, the source computing device 112 may display the prompt for a duration of time, and if the user does not provide instructions for controlling the target computing device 132 within the duration or time interval, the source computing device 112 may determine that the user is not interested in controlling the target computing device 132 and remove the icon accordingly. By removing the icon in the event that the user does not want to control the target computing device 132, the source computing device 112 may reduce resource consumption, such as battery or processor consumption.
The source computing device 112 may include a pairing component 118, the pairing component 118 designed, constructed and operated to pair with the target computing device 132. The pairing component 118 can perform a handshake procedure to pair with the target computing device 132. The pairing component 118 may pair with the target computing device 132 in response to receiving an instruction to control the target computing device 132, which may be received in response to a prompt generated by the presence API 116. The pairing component 118 can establish a communication channel with the target computing device 132 in response to the instruction.
Pairing component 118 can establish a communication channel using one or more techniques. The pairing component 118 can establish a communication channel with one or more components of the target computing device 132. The pairing component 118 can establish a communication channel with the controller 138 of the target computing device 132. For example, the controller 138 may provide the websocket protocol. The websocket protocol may refer to or include a communication protocol configured to provide a full-duplex communication channel through a Transmission Control Protocol (TCP). The websocket protocol may be located at layer 7 of the Open Systems Interconnection (OSI) model. The controller 138 may be configured to communicate with the source computing device 112 using the websocket protocol and communication ports, such as the HTTP ports 443 and 80. The websocket protocol may facilitate real-time data transfer.
In some cases, the controller 138 may be configured to use a remote procedure call, such as a gRPC remote procedure call. The gRPC may refer to a source remote procedure call that may communicate using HTTP/2 for transport and protocol buffering as an interface description language. The gRPC may provide authentication, bi-directional flow and flow control, blocking or non-blocking bindings, cancellation, and timeouts. The controller 138 using the gRPC protocol can generate cross-platform client and server bindings for different programming languages. Thus, the pairing component 118 can establish the communication channel as a two-way full duplex communication layer over the wireless protocol.
The pairing component 118 can perform a handshaking procedure with the controller 138 to establish a communication channel. During the handshaking process, the pairing component 118 may provide authentication credentials, such as a username, password, security token, digital certificate, or other authentication information. The controller 138 may use the credentials to establish a connection with the data processing system 102 as a proxy. For example, the target digital assistant 142 may use the authentication credentials to establish a connection with the server digital assistant 108 on behalf of the user of the source computing device 112, thereby converting the target digital assistant 142 into the source digital assistant 114 of the source computing device 112. By doing so, the target digital assistant 142 can respond to and complete the query from the user of the source computing device 112. For example, a target digital assistant 142 using authentication credentials may respond to a query such as "show me my photos" by accessing user photos stored in a cloud storage system in the data processing system 102 and presenting the photos via the target computing device 132.
The target computing device 132 may use the credentials to load the profile 128 of the user of the source computing device 112 in order to customize the target digital assistant 142 or configure the target digital assistant 142 to respond to queries from the user. The profile 128 information may include any profile information established by the user of the source computing device 112 including, for example, music information, media stream information, photos, preferences, application preferences, settings, or contact information.
After the source computing device 112 has paired with the target computing device 132 or has established a communication channel with the target computing device 132, the source computing device 112 may generate and present an icon on the display device 122 indicating the activated communication channel or communication session.
The source computing device 112 may maintain an icon on the display device 122 indicating the active session. The icon may be a persistent icon that remains visible on the display device 122 as long as the communication session is active. The icons may be overlaid on other graphical user interface elements that may be presented on the display device 122. The source computing device 112 may maintain an icon on the display device 122 that indicates that the virtual controller 120 is active to forward the query via the communication channel.
In response to establishing the communication channel or session, the source computing device 112 or the pairing component 118 can invoke the virtual controller 120 on the source computing device 112 to forward a query received by the source computing device 112 to the target computing device 132 via the communication channel to control the target computing device 132. The source computing device 112 may include a virtual controller 120, the virtual controller 120 designed, constructed and operated to forward a query to the controller 138 of the target computing device 132 via a communication session or channel established by the pairing component 118. The virtual controller 120 may receive or detect a query from a user of the source computing device 112. The query may be a voice input query detected by the microphone 124.
In some cases, the source digital assistant 114 may detect the voice input and parse the voice input. The source digital assistant 114 may parse the voice input to identify a request or query. The source digital assistant 114 may query the virtual controller 120 to determine whether an active communication session exists with the target computing device 132. Source digital assistant 114 may determine the capabilities of target computing device 132 (e.g., based on signals previously broadcast by signal broadcaster 140). The source digital assistant 114 may determine that the target computing device 132 is capable of completing the request or query. Source digital assistant 114 may forward the voice input query to virtual controller 120 and instruct virtual controller 120 to forward the query to controller 138. Upon receiving a query from the virtual controller 120, the controller 138 may forward the query to the query processor 134 of the target computing device 132. The query processor 134 may process the query as if the query was detected by a component of the target computing device 132. Thus, the target computing device 132 can seamlessly process the speech input detected by the source computing device 112 as if the target computing device 132 itself originally detected the query.
In some cases, when there is an active communication session between the virtual controller 120 and the controller 138, the virtual controller 120 may automatically or by default forward all queries received by the source computing device 112 to the controller 138. In some cases, virtual controller 120 may provide a prompt asking the user whether to forward the query in response to detecting the query. In some cases, the virtual controller 120 may generate a prompt with a timer that allows the user to stop or block forwarding of queries to the controller 138, but by default. Thus, the virtual controller 120 can provide seamless forwarding of queries during active communication sessions, and the query processor 134 can seamlessly process the query as if it was originally detected or received by the target computing device 132 itself. The target computing device 132 may receive the query forwarded from the source computing device 112 and process the query to control the functionality of the target computing device 132.
The source computing device 112 may detect audio input including the query via the microphone 124. The source computing device 112 may forward the audio input to the target computing device 132 via the communication channel. When there is an active communication session with the target computing device 132, the source computing device 112 may determine to automatically forward all queries. The target computing device 132 may receive the audio input, parse the audio input to identify the query, and process the query to control the functions of the target computing device 132. For example, the query may be to play an action movie, play a particular movie, display a photo of the user, or provide weather. The query processor 134 may receive the input audio, parse the input audio to identify the query, and complete the query based on the profile information 128 received or accessed by the target computing device 132 in response to the authentication credentials received during the handshake process. In some cases, to complete the query, the query processor 134 may use the profile information to communicate with the data processing system 102. The data processing system 102 may provide, for example, a photograph to the target computing device 132 for presentation.
The data processing system 102 may select and provide content items from the supplemental digital content provider 144. The data processing system 102 can generate a request for a supplemental content item. For example, the data processing system 102 may receive a request for content from the target computing device 132. The request for content may be for photos, weather, travel information, car pool requests, streaming multimedia, or other requests. The data processing system 102 may receive the request from the query processor 134 or the target digital assistant 142. The target computing device 132 may generate and provide the request for content in response to receiving a query forwarded from the virtual controller 120 during the active communication session.
The data processing system 102 may generate a second request or a new request for a supplemental content item. The data processing system 102 may generate a new request in response to receiving a request from the target computing device 132. For example, in addition to providing content requested by the target computing device 132 (e.g., a photo of the user), the data processing system 102 may generate a new supplemental request for supplemental content, such as an advertising digital object. The data processing system 102 may provide a new supplemental request to the content selector 110. The content selector 110 may perform the real-time content selection profile using information associated with the original request received from the target computing device 132 or profile information associated with the profile 128 of the source computing device 112. In response to the second request for supplemental digital content, the content selector 110 can select a supplemental content item that is associated with the source computing device 112 in response to the second request and based on profile information associated with the source computing device 112 in response to the communication channel between the source computing device 112 and the target computing device 132 being active. The data processing system 102 may provide a supplemental content item, different from the requested content, for display on the target computing device 132. Accordingly, the target computing device 132 may present content (e.g., weather information) and supplemental content items (e.g., advertisements) in response to the request forwarded by the virtual controller 120. The supplemental content items may be related to the original request, or profile information or other information.
The communication session or channel between the source computing device 112 and the target computing device 132 may be terminated or disconnected. Terminating or disconnecting a communication channel or session may refer to interrupting the channel, ending the communication session, blocking further communication, preventing further communication, or otherwise ceasing data communication between the source computing device 112 and the target computing device 132. However, in some cases, ending the communication session may not stop or prevent the signal broadcaster 140 from broadcasting a signal that can be received or detected by the presence API 116. For example, ending the communication session may stop forwarding the query from the source computing device 112 to the target computing device 132.
One or more of the source computing device 112, the target computing device 132, or the data processing system 102 may terminate or end the communication channel. The communication channel may be terminated in response to a request to terminate the communication channel. The communication channel may be terminated based on or in response to an event, condition, or trigger. For example, the communication channel may be terminated based on a time interval or duration (e.g., 10 minutes, 15 minutes, 20 minutes, 30 minutes, or 1 hour). The communication channel may terminate when the source computing device 112 is far from the target computing device 132 or is no longer within range of the target computing device 132. For example, if the source computing device 112 is no longer able to detect a signal broadcast by the signal broadcaster 140 with satisfactory signal strength (or amplitude or power), the source computing device 112 may terminate the communication channel. The communication channel may be terminated after the query is completed. For example, the target computing device 132 may send a signal to the source computing device 112 that the query has been completed, and the source computing device 112 may determine to disconnect from the target computing device 132. In some cases, the source computing device 112 may remain connected to the target computing device 132 or maintain a persistent communication channel until the user provides instructions to terminate the communication channel, or the source computing device 112 moves out of range of the target computing device 132. In some cases, the target computing device 132 or the source computing device 112 may determine to terminate the communication channel in response to an idle timeout condition. For example, if the virtual controller 120 does not forward any queries to the controller 138 within a time interval (e.g., 10 minutes, 15 minutes, 20 minutes, 30 minutes, 1 hour, or other time interval), the source computing device 112 or the target computing device 132 may determine that the communication channel has timed out. Terminating a communication channel without use may reduce security breaches and resource utilization (e.g., processor, memory, or battery utilization).
Upon termination of the communication channel or the communication channel otherwise times out, the target computing device 132 may remove any authentication credentials or profile information received from the source computing device 112. The target computing device 132 may erase, scrub, or delete any profile information or other information received from the source computing device 112 during the communication channel. Target computing device 132 may further erase, scrub or delete any information received from data processing system 102 to complete the query received from source computing device 112. For example, in response to a usage query for displaying photos, the target computing device 132 may delete photos that may have been received from the data processing system 102. Thus, the target computing device 132 may remove any user information received in accordance with establishing the communication channel or received after the communication channel is established.
Fig. 2 is an illustration of example operation of a system for dynamic remote control of a device, according to an embodiment. The operations 200 may be performed by one or more systems or components depicted in fig. 1, including, for example, the source computing device 112, the target computing device 132, or the data processing system 102. The source computing device 112 may be a mobile device, such as a smartphone. The target computing device 132 may be a smart display or a television.
At 202, the source computing device 112 may be in an initial state. The initial state may refer to a state in which the source computing device 112 is not proximate to the target computing device 132 or is not within range of the target computing device 132. During the initial state, there may be no communication channel between the source computing device 112 and the target computing device 132. During the initial state 202, the source computing device 112 may not be receiving signals broadcast by the signal broadcaster of the target computing device 132. For example, during the initial state 202, the source computing device 112 may be remote from the target computing device 132, such as in a different room, floor, or building.
The source computing device 112 may move towards the target computing device 132. For example, a user of the source computing device 112 may bring the source computing device 112 close to the target computing device 132, or within range of a signal being broadcast by the target computing device 132. At 204, the source computing device 112 may detect the signal broadcast by the target computing device 132. For example, the source computing device 112 may detect the signal via a presence API. In response to detecting the signal and corresponding information associated with the signal, the source computing device 112 may generate a prompt 206. The prompt may indicate that the source computing device 112 may be used to control the target computing device 132. The prompt may be, for example, "control your TV," where TV may refer to a television or target computing device 132.
The user of the source computing device 112 may respond to the prompt 206 by selecting the prompt or otherwise providing input. At 208, the input may cause the source computing device 112 to establish a communication channel. For example, the user may click on the prompt 206, and in response to the selection, the source computing device 112 may perform a handshake procedure with the target computing device 132 to establish the communication channel 208.
When the communication channel is established, and in some cases, the source computing device 112 may generate and present a persistent icon 210, the persistent icon 210 indicating that the source computing device 112 has an active communication session with the target computing device 132. The icon may be a microphone if the target computing device 132 has the capability to receive voice input and process the voice input. In some cases, the source computing device 112 may provide a suggestion 212 regarding the type of functionality that the target computing device 132 may perform. The suggestions 212 may be generated based on the capabilities or functionality of the target computing device 132. The source computing device 112 may generate suggestions based on UI templates stored in a memory of the source computing device 112. The source computing device 112 may receive the suggestion from the target computing device 132 or the data processing system 102.
At 214, the source computing device 112 may receive a query from a microphone of the source computing device 112. The query may be "show me my photos". At 216, the source computing device 112 may determine to forward the query to the target computing device 132. For example, the source computing device 112 may include a virtual controller that may detect an incoming query and an active communication session, and then determine a controller that forwards the query to the target computing device 132.
The target computing device 132 may receive the query forwarded from the source computing device 112. Due to the established communication channel, the target computing device 132 may process the query as if the target computing device 132 received a voice query directly from the user rather than from the virtual controller. The target computing device 132 may utilize the display of the photograph 220 associated with the account or profile of the source computing device 112 to change the default or current display 218 (e.g., a screen saver, television program, or other media currently being presented on the television).
FIG. 3 is an illustration of an example method of dynamic remote control of a device, according to an embodiment. The method 300 may be performed by one or more systems or components depicted in fig. 1, including, for example, a data processing system, a source computing device, or a target computing device. At 302, a source device may detect the presence of a target device. The source device may include a mobile computing device (such as a smartphone or smart watch) or other computing device. The source device may detect presence by detecting a signal transmitted or broadcast by the target device. The target device may include, for example, a smart television or display, or speakers, or a networked device, such as an IoT device (e.g., smart appliance, speakers, or automobile).
At 304, the source device may determine capabilities of the target device. The source device may determine that the target device includes a controller to which the query may be forwarded by the source device. The source device may determine that the target device is configured with a digital assistant. The source device may determine the function based on information present in the broadcast signal or using other techniques, such as performing a lookup or web search using a device identifier or type.
At 306, the source device may determine whether the target device is compatible with the source device. The source device may determine a compatibility match based on the conditions or specifications of the source device, the compatibility match allowing the source device to forward the query to the target device for completion. For example, if the target device includes a controller configured to receive a query from a virtual controller of the source device, the source device may determine that the target device is compatible with the source device. In some cases, compatibility matching may refer to or be based on the version of the software application or program installed on the source or target device. In some cases, compatibility matching may be based on user preferences. For example, a profile stored on the source device may include settings or preferences for the type of target device with which the communication session is established. The user may indicate in the profile that only a communication session with the smart display or smart television is to be established. Accordingly, if the target device is a smart display or smart television, the source device may determine that a compatibility match exists at 306. If there is no compatibility match (e.g., the device type does not match the settings or the target device does not have a controller or digital assistant compatible with the source device), the method may proceed to block 308 to block further presence detection of the target device.
However, if the source device determines at 306 that the target device is compatible with the source device, the source device may proceed to 310 to generate and provide a prompt. The source device may provide the hints based on the compatibility or capabilities of the target device. For example, if the target device includes a digital assistant or query processor that can process voice queries, the prompt may include a microphone. The prompt may require user authorization to establish a communication channel or communication session with the target device. The source device may present the prompt via a display of the source device. In some cases, the cues may include or be associated with audio cues, visual cues, tactile cues, or other types of cues to gain the attention of the user.
At 312, the source device may receive an instruction to control the target device. The instructions may be received via user input from the source device and in response to the prompt. The instruction may be a touch input or a voice input indicating that it is desired to control the target device. Controlling the target device may include, for example, playing video or music on the target device, accessing information via the target device, playing photos on the target device, or otherwise using a function or capability of the target device. If the user does not enter an instruction to control the device or provide an indication to not control the target device, the method may proceed to 314 to block the establishment of the communication channel. In some cases, the source device may modify the prompt. The modification cue may refer to or include a removal cue, a hide cue, or a change cue. For example, if the user indicates that the communication channel is not to be established, or no input is provided within a predetermined amount of time (e.g., 1 minute, 2 minutes, 3 minutes, 4 minutes, etc.) after the prompt is generated, the source device may reduce or lighten the prompt or move it to the background on the display.
However, if the user indicates establishment of a communication channel with the target device in response to the prompt, the method may proceed to 316 to establish a communication channel with the target device. Establishing the communication channel may include performing a handshake procedure, sharing authentication credentials, or sharing a profile. The communication channel may be a gRPC session or other type of protocol.
At 318, the source device may invoke the virtual controller. The source device may invoke or configure the virtual controller in response to the communication session established with the target device. The virtual controller may act as a proxy input interface for the target device. For example, the virtual controller may detect a voice input or query received by the source device and then forward the query to the target device. When forwarded queries are received, the target device may process the queries as if they were received directly from the user at the target device, thereby providing seamless virtual control of the target device via the source device.
Fig. 4 is a block diagram of an example computer system 400. The computer system or computing device 400 may include or be used to implement the system 100 or components thereof, such as the data processing system 102, the source computing device 112, or the target computing device 132. The data processing system 102, the source computing device 112, or the target computing device 132 may include an intelligent personal assistant or a voice-based digital assistant. Computing system 400 includes a bus 405 or other communication component for communicating information, and a processor 410 or processing circuit coupled with bus 405 for processing information. Computing system 400 may also include one or more processors 410 or processing circuits coupled to the bus for processing information. Computing system 400 also includes main memory 415, such as a Random Access Memory (RAM) or other dynamic storage device, coupled to bus 405 for storing information and instructions to be executed by processor 410. The main memory 415 may be or include a data repository. Main memory 415 also may be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 410. Computing system 400 may also include a Read Only Memory (ROM) 420 or other static storage device coupled to bus 405 for storing static information and instructions for processor 410. A storage device 425, such as a solid state device, magnetic disk or optical disk, may be coupled to bus 405 for persistently storing information and instructions. The storage device 425 may include or be part of a data repository.
The processes, systems, and methods described herein may be implemented by computing system 400 in response to processor 410 executing an arrangement of instructions contained in main memory 415. Such instructions may be read into main memory 415 from another computer-readable medium, such as storage device 425. Execution of the arrangement of instructions contained in main memory 415 causes the computing system 400 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 415. Hardwired circuitry may be used in place of or in combination with software instructions and the systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
Although an example computing system is depicted in FIG. 4, the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
For situations in which the systems discussed herein collect personal information about a user or may make use of personal information, the user may be provided with an opportunity to control whether programs or features may collect personal information (e.g., information about the user's social network, social actions or activities, the user's preferences, or the user's location), or whether or how to receive content from a content server or other data processing system that may be more relevant to the user. Furthermore, certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed when the parameters are generated. For example, the identity of the user may be anonymized such that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized to a location (such as a city, zip code, or state level) at which location information is obtained such that a particular location of the user cannot be determined. Thus, the user can control how the content server collects and uses information about him or her.
The subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs (e.g., one or more circuits of computer program instructions) encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, program instructions may be encoded on an artificially generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage media may be or be embodied in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification may be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The words "data processing system," "computing device," "component," or "data processing apparatus" encompass various devices, apparatuses, and machines for processing data, including for example, programmable processors, computers, systems on a chip, or multiples or combinations of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures. For example, the presence API116, the pairing component 118, the virtual controller 120, or other components may include or share one or more data processing apparatuses, systems, computing devices, or processors.
A computer program (also known as a program, software application, app, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of data processing system 102) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include Local Area Networks (LANs) and Wide Area Networks (WANs), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system, such as system 100 or system 400, may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network, such as network 105. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server sends data (e.g., data packets representing digital components) to the client device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., the result of the user interaction) may be received at the server from the client device (e.g., received by the data processing system 102 from the source computing device 112 or the supplemental digital content provider device 144).
Although operations are depicted in the drawings in a particular order, such operations need not be performed in the particular order shown or in sequential order, and all illustrated operations need not be performed. The actions described herein may be performed in a different order.
The separation of various system components need not be separated in all embodiments, and the described program components may be included in a single hardware or software product. For example, the natural language processor 106 and the interface 104 may be a single component, app, or program, or a logical device having one or more processing circuits, or part of one or more servers of the data processing system 102.
Having now described some illustrative embodiments, it is to be understood that the foregoing is illustrative and not limiting, having been provided by way of example. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, those acts and those elements may be combined in other ways to accomplish the same objectives. Acts, elements and features discussed in connection with one embodiment are not intended to be excluded from a similar role in other embodiment or embodiments.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by," and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional embodiments that consist of the items specifically listed thereafter. In one embodiment, the systems and methods described herein include one, each of more than one combination, or all of the described elements, acts, or components.
Any reference herein to embodiments or elements or acts of the systems and methods in the singular may also include embodiments comprising a plurality of these elements, and any reference herein to any plural of any embodiments or elements or acts may also include embodiments comprising only a single element. References in the singular or plural form are not intended to limit the presently disclosed systems or methods, their components, acts or elements to a single or multiple configurations. References to any action or element based on any information, action, or element may include embodiments in which the action or element is based, at least in part, on any information, action, or element.
Any embodiment disclosed herein may be combined with any other embodiment or examples, and references to "an embodiment," "some embodiments," "one embodiment," etc. are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment or example. Such phrases are not necessarily referring to the same embodiment. Any embodiment may be combined with any other embodiment, whether inclusive or exclusive, in any manner consistent with aspects and embodiments disclosed herein.
References to "or" may be construed as inclusive such that any term described using "or" may indicate any single, more than one, or all of the described terms. A reference to at least one of a combined list of words may be interpreted as an inclusive "or" indicating any of a single, more than one, and all described words. For example, a reference to "at least one of a and B" may include a alone, B alone, and both a and B. Such references used in conjunction with "including" or other disclosed terms may include additional items.
Where technical features in the drawings, detailed description or any claim are followed by reference signs, the reference signs have been included to increase the intelligibility of the drawings, detailed description, and claims. Accordingly, the presence or absence of a reference label has no limiting effect on the scope of any claim element.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. The foregoing embodiments are illustrative, and not limiting of the described systems and methods. The scope of the systems and methods described herein is, therefore, indicated by the appended claims rather than by the foregoing description, and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.
Claims (20)
1. A system for remote control of a device, comprising:
a first computing device comprising one or more processors and memory, the first computing device to:
detecting a second computing device within range of the first computing device;
determining capabilities of the second computing device in response to detecting the second computing device;
generating, based on capabilities of the second computing device, a prompt indicating that the first computing device is capable of controlling the second computing device;
receiving instructions for controlling the second computing device in response to the prompt;
establishing a communication channel with the second computing device in response to the instruction; and
invoking a virtual controller on the first computing device to forward a query received by the first computing device to the second computing device via the communication channel to control the second computing device.
2. The system of claim 1, wherein the second computing device broadcasts information about the second computing device, including the first computing device to:
receiving information broadcast by the second computing device; and
determining capabilities of the second computing device based on the information.
3. The system of claim 1 or 2, comprising:
the first computing device is to perform a handshake process with the second computing device using one or more security credentials to establish the communication channel.
4. The system of any preceding claim, comprising:
the first computing device is to maintain an icon on a display device of the first computing device indicating that the virtual controller is active to forward a query via the communication channel.
5. The system of any preceding claim, comprising:
the second computing device is to access a profile associated with the first computing device in response to establishment of the communication channel.
6. The system of any preceding claim, comprising:
the first computing device is to disconnect from the second computing device and terminate the communication channel, wherein the second computing device removes any profile information received from the first computing device after establishment of the communication channel.
7. The system of any preceding claim, comprising:
the first computing device is to establish the communication channel as a bi-directional communication layer via a wireless protocol.
8. The system of any preceding claim, comprising:
the first computing device is configured to establish the communication channel with the second computing device using a websocket protocol.
9. The system of any preceding claim, comprising:
the second computing device is to receive the query forwarded from the first computing device and process the query to control a function of the second computing device.
10. The system of any preceding claim, comprising:
the first computing device to detect, via a microphone of the first computing device, an audio input comprising a query and forward the audio input to the second computing device via the communication channel; and
the second computing device is to receive the audio input, parse the audio input to identify the query, and process the query to control a function of the second computing device.
11. The system of any preceding claim, comprising:
the second computing device to send a request for content to a data processing system remote from the first computing device and the second computing device; and
the data processing system is configured to:
generating, in response to the request for content, a second request for supplemental digital content that is different from the requested content;
in response to the second request for supplemental digital content, selecting a supplemental content item that is associated with the first computing device in response to the second request and based on profile information associated with the first computing device in response to a communication channel between the first computing device and the second computing device being active; and
providing, for display on the second computing device, the supplemental content item that is different from the requested content.
12. A method for remote control of a device, comprising:
detecting, by a first computing device comprising one or more processors and memory, a second computing device that is within range of the first computing device;
determining, by the first computing device, capabilities of the second computing device in response to detecting the second computing device;
generating, by the first computing device, a prompt indicating that the first computing device is capable of controlling the second computing device based on the capabilities of the second computing device;
receiving, by the first computing device, an instruction to control the second computing device in response to the prompt;
establishing, by the first computing device, a communication channel with the second computing device in response to the instruction; and
invoking, by the first computing device, a virtual controller on the first computing device to forward a query received by the first computing device to the second computing device via the communication channel to control the second computing device.
13. The method of claim 12, wherein the second computing device broadcasts information about the second computing device, comprising:
receiving, by the first computing device, information broadcast by the second computing device; and
determining, by the first computing device, capabilities of the second computing device based on the information.
14. The method according to claim 12 or 13, comprising:
performing, by the first computing device, a handshake process with the second computing device using one or more security credentials to establish the communication channel.
15. The method of any of claims 12 to 14, comprising:
maintaining, by the first computing device, an icon on a display device of the first computing device, the icon indicating that the virtual controller is active to forward a query via the communication channel.
16. The method according to any one of claims 12 to 15, comprising:
accessing, by the second computing device, a profile associated with the first computing device in response to the establishment of the communication channel.
17. The method of any of claims 12 to 16, comprising:
disconnecting, by the first computing device, from the second computing device and terminating the communication channel, wherein the second computing device removes any profile information received from the first computing device after establishment of the communication channel.
18. The method according to any one of claims 12 to 17, comprising:
establishing, by the first computing device, the communication channel as a two-way communication layer via a wireless protocol.
19. The method of any of claims 12 to 18, comprising:
establishing, by the first computing device, a communication channel with the second computing device using a websocket protocol.
20. The method of any of claims 12 to 19, comprising:
receiving, by the second computing device, the query forwarded from the first computing device, and processing the query to control a function of the second computing device.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/040336 WO2023277928A1 (en) | 2021-07-02 | 2021-07-02 | Virtual remote control on first device to control second device, eg tv |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115868151A true CN115868151A (en) | 2023-03-28 |
Family
ID=77155877
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180047030.6A Pending CN115868151A (en) | 2021-07-02 | 2021-07-02 | Virtual remote control of a second device, e.g. a TV, on a first device |
Country Status (4)
Country | Link |
---|---|
EP (1) | EP4150893A1 (en) |
KR (1) | KR20240023435A (en) |
CN (1) | CN115868151A (en) |
WO (1) | WO2023277928A1 (en) |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2013012107A1 (en) * | 2011-07-19 | 2013-01-24 | 엘지전자 주식회사 | Electronic device and method for controlling same |
US10469474B2 (en) * | 2017-05-23 | 2019-11-05 | Google Llc | Mobile assisted television sign in using discovery and launch protocol |
-
2021
- 2021-07-02 KR KR1020247001862A patent/KR20240023435A/en unknown
- 2021-07-02 CN CN202180047030.6A patent/CN115868151A/en active Pending
- 2021-07-02 WO PCT/US2021/040336 patent/WO2023277928A1/en active Application Filing
- 2021-07-02 EP EP21749020.0A patent/EP4150893A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20240023435A (en) | 2024-02-21 |
EP4150893A1 (en) | 2023-03-22 |
WO2023277928A1 (en) | 2023-01-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11949733B2 (en) | Audio-based data structure generation | |
JP7139295B2 (en) | System and method for multimodal transmission of packetized data | |
KR20180091707A (en) | Modulation of Packetized Audio Signal | |
CN112967716A (en) | Feedback controller for data transmission | |
US11893993B2 (en) | Interfacing with applications via dynamically updating natural language processing | |
US11514896B2 (en) | Interfacing with applications via dynamically updating natural language processing | |
CN115210692A (en) | Interface and mode selection for digital motion execution | |
CN110637300B (en) | Delayed two-factor authentication in a networking environment | |
KR20210097842A (en) | Establishment of audio-based network sessions with non-registered resources | |
CN110692040A (en) | Activating remote devices in a network system | |
CN115868151A (en) | Virtual remote control of a second device, e.g. a TV, on a first device | |
US11798555B2 (en) | Detection of duplicate packetized data for selective transmission into one of a plurality of a user's devices | |
US20230267928A1 (en) | Indexing Application Actions for Voice-Based Execution |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |