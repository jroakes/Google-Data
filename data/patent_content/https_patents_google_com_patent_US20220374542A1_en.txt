US20220374542A1 - Scalable and Differentially Private Distributed Aggregation - Google Patents
Scalable and Differentially Private Distributed Aggregation Download PDFInfo
- Publication number
- US20220374542A1 US20220374542A1 US17/620,438 US202017620438A US2022374542A1 US 20220374542 A1 US20220374542 A1 US 20220374542A1 US 202017620438 A US202017620438 A US 202017620438A US 2022374542 A1 US2022374542 A1 US 2022374542A1
- Authority
- US
- United States
- Prior art keywords
- value
- private
- messages
- message
- values
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000002776 aggregation Effects 0.000 title claims abstract description 33
- 238000004220 aggregation Methods 0.000 title claims abstract description 33
- 238000000034 method Methods 0.000 claims abstract description 95
- 238000005070 sampling Methods 0.000 claims description 12
- 230000004931 aggregating effect Effects 0.000 claims description 11
- 238000013139 quantization Methods 0.000 claims description 2
- 230000008569 process Effects 0.000 abstract description 53
- 238000004891 communication Methods 0.000 description 17
- 238000012545 processing Methods 0.000 description 15
- 238000009826 distribution Methods 0.000 description 10
- 239000013598 vector Substances 0.000 description 8
- 238000004458 analytical method Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 6
- 230000006870 function Effects 0.000 description 5
- 230000011218 segmentation Effects 0.000 description 5
- 230000000007 visual effect Effects 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 238000012935 Averaging Methods 0.000 description 3
- 230000004075 alteration Effects 0.000 description 3
- 230000001143 conditioned effect Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000012549 training Methods 0.000 description 3
- 238000013519 translation Methods 0.000 description 3
- 238000007792 addition Methods 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000003709 image segmentation Methods 0.000 description 2
- 230000001939 inductive effect Effects 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 238000012800 visualization Methods 0.000 description 2
- 239000000654 additive Substances 0.000 description 1
- 230000000996 additive effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000000295 complement effect Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 238000013479 data entry Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- PCHJSUWPFVWCPO-UHFFFAOYSA-N gold Chemical compound [Au] PCHJSUWPFVWCPO-UHFFFAOYSA-N 0.000 description 1
- 230000006698 induction Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 238000003860 storage Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6245—Protecting personal data, e.g. for financial or medical purposes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6245—Protecting personal data, e.g. for financial or medical purposes
- G06F21/6254—Protecting personal data, e.g. for financial or medical purposes by anonymising data, e.g. decorrelating personal data from the owner's identification
Definitions
- the present disclosure relates generally to distributed aggregation. More particularly, the present disclosure relates to scalable and differentially private distributed aggregation, for example, in the shuffled model.
- Federated learning promises to make machine learning feasible on distributed, private datasets by implementing gradient descent using secure aggregation methods. The idea is to compute a global weight update without revealing the contributions of individual users.
- a more scalable and robust primitive for privacy-preserving protocols is shuffling of user data, so as to hide the origin of each data item.
- Highly scalable and secure protocols for shuffling, so-called mixnets have been proposed as a primitive for privacy-preserving analytics in the Encode-Shuffle-Analyze framework by Bittau et al.
- One example aspect is directed to a computer-implemented method to enable privacy-preserving aggregation of private data.
- the method includes obtaining, by one or more computing devices, private data comprising a private value.
- the method includes producing, by the one or more computing devices, a plurality of messages that respectively comprise a plurality of message values, wherein a total sum of the plurality of message values approximates the private value, and wherein at least one of the plurality of message values is randomly selected.
- the method includes providing, by the one or more computing devices, the plurality of messages for aggregation with a plurality of additional messages respectively generated for a plurality of additional private values.
- producing, by the one or more computing devices, the plurality of messages that respectively comprise the plurality of message values comprises: for each of one or more first iterations associated with one or more first messages of the plurality of messages: uniformly and randomly sampling, by the one or more computing devices, one of a plurality of available values to serve as the message value for such first message; and for a final iteration associated with a final message of the plurality of messages: determining, by the one or more computing devices, an intermediate sum of the message values of the one or more first messages; and selecting, by the one or more computing devices, a final value to serve as the message value for the final message such that the total sum of the intermediate sum and the final value approximates the private value.
- the plurality of available values comprises a set of integers extending from zero to a sample control parameter value minus one; and selecting, by the one or more computing devices, the final value to serve as the message value for the final message comprises setting, by the one or more computing devices, the final value equal to the private value minus the intermediate sum modulo the sample control parameter value.
- a number of the one or more first iterations is controlled by a message control parameter value.
- the private value comprises a scaled private value produced by scaling an unscaled private value; and obtaining, by one or more computing devices, the private data comprising the private value comprises scaling, by the one or more computing devices, the unscaled private value by a scaling control parameter value to obtain the scaled private value.
- the private value comprises a normalized private value produced by normalizing a raw private value; and obtaining, by one or more computing devices, the private data comprising the private value comprises normalizing, by the one or more computing devices, the raw private value according to an expected maximum private value.
- the method includes scaling, by the one or more computing devices, the normalized private value by a scaling control parameter value to obtain a scaled private value.
- the private value comprises a noised private value produced by adding noise to a raw private value; and obtaining, by one or more computing devices, the private data comprising the private value comprises pre-randomizing, by the one or more computing devices, the raw private value according to a shared noise probability to obtain the noised private value.
- one or more of the sampling control parameter value, the scaling control parameter value, and the message control parameter value comprises a user-specified hyperparameter or a learned hyperparameter.
- one or more of the sampling control parameter value, the scaling control parameter value, and the message control parameter value is greater than or equal to four.
- providing, by the one or more computing devices, the plurality of messages for aggregation comprises transmitting, by the one or more computing devices, the plurality of messages to a shuffler model configured to shuffle the plurality of messages with the plurality of additional messages.
- the one or more computing devices consist of a user device.
- the private value comprises one or more of: an update value for a parameter of a machine-learned model; a heavy hitter value; an entropy value; a quantization value; or a support size value.
- the method includes encrypting, by the one or more computing devices, at least one of the plurality of messages.
- the method includes encrypting, by the one or more computing devices, at least one of the plurality of messages with a public key associated with a shuffler model configured to shuffle the plurality of messages.
- Another example aspect is directed to a computing system configured to perform any portion of the computer-implemented method described herein.
- Another example aspect is directed to one or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause the one or more processors to perform any portion of the computer-implemented method described herein.
- Another example aspect is directed to a computing system comprising: one or more processors; and one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising: obtaining a plurality of multisets of messages, wherein a plurality of private values are respectively associated with the plurality of multisets of messages, each multiset of messages comprising two or more messages that respectively contain two or more message values that sum to approximate the private value associated with such multiset of messages, and wherein at least one of the two or more message values comprises a random value; and aggregating the message values for the plurality of multisets of messages to obtain an aggregate sum that approximates a sum of the plurality of private values.
- the plurality of multisets of messages have been respectively generated by a plurality of different devices, and wherein the messages have been shuffled and are randomly distributed amongst each other without regard to which of the plurality of different devices generated each message.
- aggregating the message values for the plurality of multisets of messages comprises: determining a sum of the message values modulo a sampling control parameter value.
- aggregating the message values for the plurality of multisets of messages comprises further comprises: downscaling the sum of the message values modulo the sampling control parameter value by a scaling control parameter value.
- aggregating the message values for the plurality of multisets of messages comprises: determining an intermediate value that equals a sum of the message values modulo a sampling control parameter value; and performing the following return logic: if the intermediate value is greater than two times a number of the private values times a scaling control parameter value: returning zero; else if the intermediate value is greater than the number of the private values times the scaling control parameter value: returning the number of the private values; and else: returning the intermediate value divided by the scaling control parameter value.
- aggregating the message values for the plurality of multisets of messages comprises determining, based on the aggregate sum that approximates the sum of the plurality of private values, an average value that approximates an average of the private values.
- Another example aspect is directed to a computer-implemented method comprising performing any of the operations described herein.
- Another example aspect is directed to one or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause the one or more processors to perform any of the operations described herein.
- FIG. 1 depicts a block diagram of an example computing system according to example embodiments of the present disclosure.
- FIG. 2 depicts a block diagram of an example encoding and analysis process for secure multi-party aggregation according to example embodiments of the present disclosure.
- FIG. 3 depicts a block diagram of an example algorithm for encoding private data according to example embodiments of the present disclosure.
- FIG. 4 depicts a block diagram of an example algorithm for analyzing encoded data according to example embodiments of the present disclosure.
- the present disclosure is directed to scalable and differentially private distributed aggregation, for example, in the shuffled model.
- the present disclosure proposes a simple and more efficient protocol for aggregation in the shuffled model, where communication as well as error increases only polylogarithmically in the number of users n.
- the proposed technique is a conceptual “invisibility cloak” that makes users' data almost indistinguishable from random noise while introducing zero distortion on the sum.
- an encoding process performed by a computing device can include obtaining private data that includes a private value.
- the computing device can produce a plurality of messages that respectively comprise a plurality of message values, where a total sum of the plurality of message values approximates the private value, and where at least one of the plurality of message values is randomly selected.
- the device can provide the plurality of messages for aggregation with a plurality of additional messages respectively generated for a plurality of additional private values.
- the messages can be transmitted to a shuffler model configured to shuffle the plurality of messages with the plurality of additional messages. Once all of the messages are shuffled, an analyzer can determine a sum of all private values from all users but cannot (with some guarantees) determine any single private value from any single user.
- example aspects of the present disclosure are directed to the problem of privately summing n numbers in the shuffled model recently defined by Cheu et al.
- the term aggregation is used for the sum operation.
- a protocol in the shuffled model is ( ⁇ , ⁇ )-differentially private if (R 1 (x 1 ), . . . , R n (x n )) is ( ⁇ , ⁇ )-differentially private, where probabilities are with respect to the random choices made in the algorithm E and the shuffler .
- the privacy claim is justified by the existence of highly scalable protocols for privately implementing the shuffling primitive.
- aspects of the present disclosure build upon a technique from protocols for secure multi-party aggregation: Ensure that individual numbers passed to the analyzer are fully random by adding random noise terms but coordinate the noise such that all noise terms cancel, and the sum remain the same as the sum of the original data.
- a new insight provided herein is that in the shuffled model the addition of zero-sum noise can be done without coordination between the users. Instead, each user individually can produce numbers y 1 , . . . , y m that are fully random except that they sum to x i , and pass them to the shuffler. This is visualized, for example, in FIG. 2 .
- the noise that is introduced acts as an invisibility cloak: The data is still there, possible to aggregate, but is almost impossible to gain any other information from.
- Algorithm 1 The details of one example encoder is given as Algorithm 1, which is presented in FIG. 3 .
- N, k, and m For parameters N, k, and m to be specified later it converts each input x i to a set of random message values ⁇ y i , . . . , y m ⁇ whose sum, up to scaling and rounding, equals x i .
- E N,k,m (x i ) When the output of all encoders E N,k,m (x i ) is composed with a shuffler this directly gives differential privacy with respect to sum-preserving changes of data (where the sum is considered after rounding).
- the notation Uniform(R) is used to denote a value uniformly sampled from a finite set R.
- the protocol can be combined with a pre-randomizer that adds noise to each x i with some probability.
- Algorithm 2 One example analyzer is given as Algorithm 2, which is presented in FIG. 4 . It computes z as the sum of the inputs (received from the shuffler) modulo N, which by definition of the encoder is guaranteed to equal the sum ⁇ i ⁇ x i k ⁇ of scaled, rounded inputs. If x 1 , . . . , x n ⁇ [0,1] this sum will be in [0, nk] and z /k will be within n/k of the true sum ⁇ i x i . In the setting where a pre-randomizer adds noise to some inputs, however, it may be the case that z ⁇ [0, nk] in which case the analyzer can round to the nearest feasible output sum, 0 or n.
- One example application of the techniques described herein in a machine learning context is gradient descent-based federated learning to learn a machine-learned model.
- the idea is to avoid collecting user data, and instead compute weight updates in a distributed manner by sending model parameters to users, locally running stochastic gradient descent on private data, and aggregating model updates over all users.
- Using a secure aggregation protocol guards against information leakage from the update of a single user, since the server only learns the aggregated model update.
- federated learning is one useful application, many other applications exist as well.
- the proposed techniques can easily be applied to other problems such as: finding heavy hitters (e.g., finding the most commonly typed words into a virtual keyboard); entropy estimation; quantile estimation; support size estimation; and/or other problems.
- the proposed algorithms can be used for aggregation in conjunction with any context that demonstrates the linearity property.
- linear sketches form the basis of efficient algorithms for a variety of estimation tasks including norms, entropy, support size, quantiles, and heavy hitters.
- the systems and methods of the present disclosure provide a number of technical effects and benefits, including, as one example, reducing communication costs associated with secure aggregation of private data.
- current practical secure aggregation protocols such as that of Bonawitz et al. have user computation cost O(n 2 ) and total communication complexity O(n 2 ), where n is the number of users. This limits the number of users that can participate in the secure aggregation protocol.
- communication per user as well as error increases only polylogarithmically in n.
- secure aggregation can be performed with reduced communication costs. Reduced communication costs can conserve computing resources such as processor usage, memory usage, network bandwidth, and the like.
- the proposed techniques can enable improved privacy.
- the privacy analysis for many existing secure aggregation protocols assumes of an “honest but curious” server that does not deviate from the protocol, so some level of trust in the secure aggregation server is required.
- protocols based on shuffling operate with much weaker assumptions on the server.
- total work and communication of the proposed new protocol scales near-linearly with the number of users.
- the communication can be reduced by amortizing the cost of the m ⁇ 1 random messages over sufficiently long input vectors. For instance, if one wishes to aggregate vectors of length d, instead of having each user sending d*(m ⁇ 1) random values to the shuffler, each user can simply send (1) d random seeds that can be expanded into a pseudorandom vector of length d*(m ⁇ 1), as well as (2) the true data vector with the random vectors subtracted. Thus, the communication overhead would be just a constant factor (e.g., amortized over long vectors).
- one of the bottlenecks of current shuffling-based schemes is the encryption overhead incurred for messages sent from the users to the shuffler.
- One advantage of the proposed method is that, although there are m messages sent by each user to the shuffler, privacy can still be maintained via encryption of just one of these messages while sending the remaining messages in plaintext. This is because, by design of the protocol, any subset of m ⁇ 1 messages is random and independent. Thus, reduced amounts of encryption may need to be performed, thereby conserving computing resources such as processor usage, memory usage, and the like.
- the present disclosure shows that a trade-off between privacy and scalability is not necessary—it is possible to avoid the n ⁇ (1) factor in both the error bound and the amount of communication per user.
- the precise results obtained depend on the notion of “neighboring dataset” in the definition of differential privacy.
- the present disclosure considers the standard notion of neighboring dataset in differential privacy, that the input of a single user is changed, and show:
- the present disclosure also considers a different notion similar to the gold standard of secure multi-party computation: Two datasets are considered neighboring if the their sums (taken after discretization) are identical. This notion turns out to allow much better privacy, even with zero noise in the final sum—the only error in the protocol comes from representing the terms of the sum in bounded precision.
- this disclosure considers its resilience towards untrusted users that may deviate from the protocol. While the shuffled model is vulnerable to such attacks in general, the privacy guarantees of the proposed protocol are robust even to a large fraction of colluding users.
- the private data can be usage metrics for a client device.
- a client device can be an Internet of Things device (AKA a “smart” device).
- Example usage metrics include hours of operation (e.g., per day), CPU usage, data usage, etc.
- the private data can be measurements of interaction of a user with various items of content such as photographs, webpages, documents, files, search results, or other items of content.
- a client device can be a user device such as, for example, a laptop, tablet, smartphone, or device that can be worn.
- This disclosure uses Uniform(R) to denote a value uniformly sampled from a finite set R, and denoted by S t the set of all permutations of ⁇ 0, t ⁇ 1 ⁇ .
- sets in this disclosure will be multisets. It will be convenient to work with indexed multisets whose elements are identified by indices in some set I.
- I 1 and I 2 we define the union of M 1 and M 2 as the function defined on I 1 ⁇ I 2 that maps i 1 ⁇ I 1 to M 1 (i 1 ) and i 2 ⁇ I 2 to M 2 (i 2 )
- Definition 1 Let be a randomized algorithm taking as input a dataset and let ⁇ 0 and ⁇ ⁇ (0,1) be given parameters. Then, is said to be ( ⁇ , ⁇ )-differentially private if for all neighboring datasets D 1 and D 2 and for all subsets S of the image of , it is the case that Pr[ (D 1 ) ⁇ S] ⁇ e ⁇ ⁇ Pr[ (D 2 ) ⁇ S]+ ⁇ , where the probability is over the randomness used by the algorithm .
- the algorithm that we want to show differentially private is the composition of the shuffler and the encoder algorithm run on user inputs.
- the outputs of encoders do not need to be differentially private.
- Z′(x) Z I (x), which is a sum of
- ⁇ 2 pairwise independent terms, each with expectation E[Z I (x)] 1/N.
- a union bound over all x ⁇ implies that with probability at least
- E (x 1 , x 2 ; y 1 , . . . , y m ⁇ 1 , y m+1 , . . . , y 2m ) the sequence obtained by the deterministic encoding for given values y 1 , . . . , y m ⁇ 1 , y m+1 , . . . , y 2m ⁇ in Algorithm.
- R S , A ⁇ T ⁇ S ( T ⁇ ⁇ a ⁇ A ⁇ a 1 , a 2 , ... , a m ⁇ ) .
- ⁇ 2 ⁇ m 2 N + 1 ⁇ 8 ⁇ m ⁇ N 2 ⁇ 2 ⁇ 2 2 ⁇ m ⁇ and ⁇ ⁇ > 6 ⁇ m 2 2 ⁇ m .
- Lemma 5 as a building block for analyzing differential privacy guarantees in the context of sum-preserving swaps, we can derive a differential privacy result with respect to general sum-preserving changes.
- N being the first odd integer larger than
- One example idea is to run Algorithm 1 after having each player add some noise to her input, with some fixed probability independently from the other players.
- the noise distribution can satisfy three properties: it should be supported on a finite interval, the logarithm of its probability mass function should have a small Lipschitz-constant (even under modular arithmetic) and its variance should be small.
- the following truncated version of the discrete Laplace distribution satisfies all three properties.
- N a positive odd integer and p ⁇ (0,1).
- the probability mass function of the truncated discrete Laplace distribution N,p is defined by
- Case ⁇ 1 0 ⁇ k ⁇ N - 1 2 ⁇ and - ( N - 1 ) 2 ⁇ k + t ⁇ - 1.
- Case ⁇ 2 0 ⁇ k ⁇ N - 1 2 ⁇ and ⁇ 0 ⁇ k + t ⁇ N - 1 2 .
- Case ⁇ 3 0 ⁇ k ⁇ N - 1 2 ⁇ and ⁇ N + 1 2 ⁇ k + t ⁇ N - 1.
- Case ⁇ 4 N + 1 2 ⁇ k ⁇ N - 1 ⁇ and ⁇ 1 ⁇ k + t ⁇ N - 1 2 .
- N be a positive odd integer and p ⁇ (0,1) a real number.
- w 1 , w 2 be two independent random variable sampled from the truncated discrete Laplace distribution N,p where N is any positive odd integer and p ⁇ (0,1) is any real number, and let
- x ⁇ 1 ⁇ x 1 ⁇ k ⁇ k
- x ⁇ 2 ⁇ x 2 ⁇ k ⁇ k
- ⁇ x ⁇ 1 ′ ⁇ x 1 ′ ⁇ k ⁇ k
- N be a positive odd integer and p ⁇ (0,1) and q ⁇ (0,1] be real numbers.
- b 1 , . . . , b n be iid random variables that are equal to 1 with probability q and to 0 otherwise, let w 1 , . . . , w n be iid random variables that are drawn from the truncated discrete Laplace distribution N,p independently of b 1 , . . . , b n , and let
- x ⁇ j ′ ⁇ x j ′ ⁇ k ⁇ k ,
- Pr [ E ⁇ ( x ⁇ 1 + z 1 , ... , x ⁇ j + z j , ... , x ⁇ n + z n ) ⁇ S ] Pr [ A ] ⁇ Pr [ E ⁇ ( x ⁇ 1 + z 1 , ... , x ⁇ j + z j , ... , x ⁇ n + z n ) ⁇ S ⁇ ⁇ " ⁇ [LeftBracketingBar]" A ] + Pr [ A _ ] ⁇ Pr [ E ⁇ ( x ⁇ 1 + z 1 , ... , x ⁇ j + z j , ... , x ⁇ n + z n ) ⁇ S ⁇ ⁇ " ⁇ [LeftBracketingBar]" A _ ] ⁇ Pr [ E ⁇ ( x ⁇ 1 + z 1 , ... , x ⁇ j + z
- (61) follows by averaging over all settings of z 1 , z 2 and invoking Lemma 1, and (62) follows from Lemma 10 and the fact that at least one of b 1 , b 2 is equal to 1.
- Algorithm 1 In Algorithm 1, each user communicates at most O(mlogN) bits which are sent via m messages. By Lemma 11, Algorithm 1 is ( ⁇ , ⁇ )-differentially private with respect to single-user changes if
- the error in our final estimate consists of two parts: the rounding error which is O(n/k) in the worst case, and the error due to the added folded Discrete Laplace noise whose average absolute value is at most
- N being the first odd integer larger than
- N be a positive odd integer and p ⁇ (0,1) and q ⁇ (0,1] be real numbers.
- C ⁇ [n] denote the subset of colluding users.
- b 1 , . . . , b n be iid random variables that are equal to 1 with probability q and to 0 otherwise, let w 1 , . . . , w n be iid random variables that are drawn from the folded discrete Laplace distribution N,p independently of b 1 , . . . , b n , and let
- tail probability term e ⁇ qn in (52) is replaced by the slightly larger quantity e ⁇ q(n ⁇
- FIG. 1 depicts an example computing system 100 that can be used to implement one example application of the methods and systems of the present disclosure in the federated learning context.
- Federated learning is provided as one example only, the proposed aggregation techniques can be applied to many other different problems/applications.
- the system 100 can be implemented using a client-server architecture that includes a server 110 that communicates with one or more client devices 130 and/or a shuffler 150 over a network.
- Each client device 130 can include one or more processor(s) 132 and a memory 134 .
- the one or more processor(s) 132 can include, for example, one or more central processing units (CPUs), graphics processing units (GPUs) dedicated to efficiently rendering images or performing other specialized calculations, and/or other processing devices.
- the memory 134 can include one or more computer-readable media and can store information accessible by the one or more processors 132 , including instructions 136 that can be executed by the one or more processors 132 and data 138 .
- the instructions 136 can include instructions for implementing a local updater configured to determine one or more local updates to a machine-learned model (e.g., a set of values descriptive of changes to the model parameters based on a set of locally stored training data).
- the local updater can perform one or more training techniques such as, for example, backwards propagation of errors to re-train or otherwise update the model based on the locally stored training data.
- the local updater can be included in an application or can be included in the operating system of the device 130 .
- the locally stored data 138 such as the local update can be considered private data.
- the local update is used only as one example of private data that can be securely aggregated. Any form of private data can be securely aggregated according to the described techniques.
- the instructions 136 can further include instructions for implementing an encoder to encode the private data such as the local update.
- the encoder can perform one or more of the encoding techniques described herein (e.g., the encoding Algorithm 1 shown in FIG. 3 ).
- the encoder can encode the private data (e.g., the local update) into a plurality of messages and the messages can be transmitted to a shuffler 150 .
- the client device 130 of FIG. 1 can include various input/output devices for providing and receiving information from a user, such as a touch screen, touch pad, data entry keys, speakers, and/or a microphone suitable for voice recognition.
- the client device 130 can also include a network interface used to communicate with one or more remote computing devices (e.g. server 110 ) over the network.
- the network interface can include any suitable components for interfacing with one more networks, including for example, transmitters, receivers, ports, controllers, antennas, or other suitable components.
- the shuffler 150 can receive a respective plurality of messages from each of the client devices 130 and can randomly shuffle them so that the messages are randomly distributed amongst each other without regard to which of the plurality of different devices 130 generated each message.
- multiple shufflers can be used (e.g., sequentially) to provide added layer(s) of privacy assurance.
- the system 100 also includes a server 110 , such as a web server.
- the server 110 can be implemented using any suitable computing device(s).
- the server 110 can have one or more processors 112 and one or more memory devices 114 .
- the server 110 can be implemented using one server device or a plurality of server devices. In implementations in which a plurality of devices are used, such plurality of devices can operate according to a parallel computing architecture, a sequential computing architecture, or a combination thereof.
- the server 110 can also include a network interface used to communicate with one or more client devices 130 over the network.
- the network interface can include any suitable components for interfacing with one more networks, including for example, transmitters, receivers, ports, controllers, antennas, or other suitable components.
- the one or more processors 112 can include any suitable processing device, such as a microprocessor, microcontroller, integrated circuit, logic device, or other suitable processing device.
- the one or more memory devices 114 can include one or more computer-readable media, including, but not limited to, non-transitory computer-readable media, RAM, ROM, hard drives, flash drives, or other memory devices.
- the one or more memory devices 114 can store information accessible by the one or more processors 112 , including computer-readable instructions 116 that can be executed by the one or more processors 112 .
- the instructions 116 can be any set of instructions that when executed by the one or more processors 112 , cause the one or more processors 112 to perform operations. For instance, the instructions 116 can be executed by the one or more processors 112 to implement a global updater 120 .
- the global updater 120 can be configured to update a global model based at least in part on a sum or average of local updates computed at the client devices 130 .
- the instructions 116 can further include instructions that cause the server 110 to implement an analyzer 122 .
- the analyzer 122 can determine the sum or average of local updates based on the shuffled messages.
- the analyzer 122 can perform any of the analysis techniques described herein, including Algorithm 2 shown in FIG. 4 .
- the one or more memory devices 114 can also store data 118 that can be retrieved, manipulated, created, or stored by the one or more processors 112 .
- the data 118 can include, for instance, local updates, global parameters, and other data.
- the data 118 can be stored in one or more databases.
- the one or more databases can be connected to the server 110 by a high bandwidth LAN or WAN, or can also be connected to server 110 through the network.
- the one or more databases can be split up so that they are located in multiple locales.
- the server 110 can exchange data with one or more client devices 130 and/or shuffler 150 over the network. Any number of client devices 130 can be connected to the server 110 and/or shuffler 150 over the network. Each of the client devices 130 can be any suitable type of computing device, such as a general purpose computer, special purpose computer, laptop, desktop, mobile device, navigation system, smartphone, tablet, wearable computing device, gaming console, a display with one or more processors, or other suitable computing device.
- the network can be any type of communications network, such as a local area network (e.g. intranet), wide area network (e.g. Internet), cellular network, or some combination thereof.
- the network can also include a direct connection between a client device 130 and the server 110 .
- communication between the server 110 and a client device 130 can be carried via network interface using any type of wired and/or wireless connection, using a variety of communication protocols (e.g. TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g. HTML, XML), and/or protection schemes (e.g. VPN, secure HTTP, SSL).
- machine-learned models described in this specification and/or generated using techniques described in this specification may be used in a variety of tasks, applications, and/or use cases.
- the input to the machine-learned model(s) of the present disclosure can be image data.
- the machine-learned model(s) can process the image data to generate an output.
- the machine-learned model(s) can process the image data to generate an image recognition output (e.g., a recognition of the image data, a latent embedding of the image data, an encoded representation of the image data, a hash of the image data, etc.).
- the machine-learned model(s) can process the image data to generate an image segmentation output.
- the machine-learned model(s) can process the image data to generate an image classification output.
- the machine-learned model(s) can process the image data to generate an image data modification output (e.g., an alteration of the image data, etc.).
- the machine-learned model(s) can process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.).
- the machine-learned model(s) can process the image data to generate an upscaled image data output.
- the machine-learned model(s) can process the image data to generate a prediction output.
- the input to the machine-learned model(s) of the present disclosure can be text or natural language data.
- the machine-learned model(s) can process the text or natural language data to generate an output.
- the machine-learned model(s) can process the natural language data to generate a language encoding output.
- the machine-learned model(s) can process the text or natural language data to generate a latent text embedding output.
- the machine-learned model(s) can process the text or natural language data to generate a translation output.
- the machine-learned model(s) can process the text or natural language data to generate a classification output.
- the machine-learned model(s) can process the text or natural language data to generate a textual segmentation output.
- the machine-learned model(s) can process the text or natural language data to generate a semantic intent output.
- the machine-learned model(s) can process the text or natural language data to generate an upscaled text or natural language output (e.g., text or natural language data that is higher quality than the input text or natural language, etc.).
- the machine-learned model(s) can process the text or natural language data to generate a prediction output.
- the input to the machine-learned model(s) of the present disclosure can be speech data.
- the machine-learned model(s) can process the speech data to generate an output.
- the machine-learned model(s) can process the speech data to generate a speech recognition output.
- the machine-learned model(s) can process the speech data to generate a speech translation output.
- the machine-learned model(s) can process the speech data to generate a latent embedding output.
- the machine-learned model(s) can process the speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.).
- an encoded speech output e.g., an encoded and/or compressed representation of the speech data, etc.
- the machine-learned model(s) can process the speech data to generate an upscaled speech output (e.g., speech data that is higher quality than the input speech data, etc.).
- the machine-learned model(s) can process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.).
- the machine-learned model(s) can process the speech data to generate a prediction output.
- the input to the machine-learned model(s) of the present disclosure can be latent encoding data (e.g., a latent space representation of an input, etc.).
- the machine-learned model(s) can process the latent encoding data to generate an output.
- the machine-learned model(s) can process the latent encoding data to generate a recognition output.
- the machine-learned model(s) can process the latent encoding data to generate a reconstruction output.
- the machine-learned model(s) can process the latent encoding data to generate a search output.
- the machine-learned model(s) can process the latent encoding data to generate a reclustering output.
- the machine-learned model(s) can process the latent encoding data to generate a prediction output.
- the input to the machine-learned model(s) of the present disclosure can be statistical data.
- the machine-learned model(s) can process the statistical data to generate an output.
- the machine-learned model(s) can process the statistical data to generate a recognition output.
- the machine-learned model(s) can process the statistical data to generate a prediction output.
- the machine-learned model(s) can process the statistical data to generate a classification output.
- the machine-learned model(s) can process the statistical data to generate a segmentation output.
- the machine-learned model(s) can process the statistical data to generate a segmentation output.
- the machine-learned model(s) can process the statistical data to generate a visualization output.
- the machine-learned model(s) can process the statistical data to generate a diagnostic output.
- the input to the machine-learned model(s) of the present disclosure can be sensor data.
- the machine-learned model(s) can process the sensor data to generate an output.
- the machine-learned model(s) can process the sensor data to generate a recognition output.
- the machine-learned model(s) can process the sensor data to generate a prediction output.
- the machine-learned model(s) can process the sensor data to generate a classification output.
- the machine-learned model(s) can process the sensor data to generate a segmentation output.
- the machine-learned model(s) can process the sensor data to generate a segmentation output.
- the machine-learned model(s) can process the sensor data to generate a visualization output.
- the machine-learned model(s) can process the sensor data to generate a diagnostic output.
- the machine-learned model(s) can process the sensor data to generate a detection output.
- the machine-learned model(s) can be configured to perform a task that includes encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding).
- the task may be audio compression task.
- the input may include audio data and the output may comprise compressed audio data.
- the input includes visual data (e.g. one or more image or videos), the output comprises compressed visual data, and the task is a visual data compression task.
- the task may comprise generating an embedding for input data (e.g. input audio or visual data).
- the input includes visual data and the task is a computer vision task.
- the input includes pixel data for one or more images and the task is an image processing task.
- the image processing task can be image classification, where the output is a set of scores, each score corresponding to a different object class and representing the likelihood that the one or more images depict an object belonging to the object class.
- the image processing task may be object detection, where the image processing output identifies one or more regions in the one or more images and, for each region, a likelihood that region depicts an object of interest.
- the image processing task can be image segmentation, where the image processing output defines, for each pixel in the one or more images, a respective likelihood for each category in a predetermined set of categories.
- the set of categories can be foreground and background.
- the set of categories can be object classes.
- the image processing task can be depth estimation, where the image processing output defines, for each pixel in the one or more images, a respective depth value.
- the image processing task can be motion estimation, where the network input includes multiple images, and the image processing output defines, for each pixel of one of the input images, a motion of the scene depicted at the pixel between the images in the network input.
- the input includes audio data representing a spoken utterance and the task is a speech recognition task.
- the output may comprise a text output which is mapped to the spoken utterance.
- the task comprises encrypting or decrypting input data.
- the task comprises a microprocessor performance task, such as branch prediction or memory address translation.
- the technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems.
- the inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components.
- processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
- Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
Abstract
An encoding process performed by a computing device (e.g., a user's private device) can include obtaining private data that includes a private value. According to an aspect of the present disclosure, the computing device can produce a plurality of messages that respectively comprise a plurality of message values, where a total sum of the plurality of message values approximates the private value, and where at least one of the plurality of message values is randomly selected. The device can provide the plurality of messages for aggregation with a plurality of additional messages respectively generated for a plurality of additional private values. For example, the messages can be transmitted to a shuffler model configured to shuffle the plurality of messages with the plurality of additional messages.
Description
- This application claims priority to and the benefit of U.S. Provisional Patent Application No. 62/863,197 filed Jun. 18, 2019. U.S. Provisional Patent Application No. 62/863,197 is hereby incorporated by reference in its entirety.
- The present disclosure relates generally to distributed aggregation. More particularly, the present disclosure relates to scalable and differentially private distributed aggregation, for example, in the shuffled model.
- Given a number of different private values that reside on a number of different devices, it may be desirable to compute a sum and/or mean of such private values. This basic problem is used as a subroutine in several learning and optimization tasks where data is distributed across several clients.
- However, it may also be desirable to compute such sum or mean in a privacy-preserving way, such that no device other than the client device has access to or the ability to compute the private value (with some guarantees). This process may be referred to as secure and distributed aggregation.
- One example scenario in which such secure and distributed aggregation is desirable is federated learning. Federated learning promises to make machine learning feasible on distributed, private datasets by implementing gradient descent using secure aggregation methods. The idea is to compute a global weight update without revealing the contributions of individual users.
- Current practical protocols for secure aggregation work in an “honest but curious” setting where a curious adversary observing all communication to and from the server cannot learn any private information assuming the server is honest and follows the protocol.
- A more scalable and robust primitive for privacy-preserving protocols is shuffling of user data, so as to hide the origin of each data item. Highly scalable and secure protocols for shuffling, so-called mixnets, have been proposed as a primitive for privacy-preserving analytics in the Encode-Shuffle-Analyze framework by Bittau et al.
- Recent papers by Cheu et al. and Balle et al. have formalized the “shuffled model” and suggested protocols for secure aggregation that achieve differential privacy guarantees. Their protocols come at a cost, though: Either the expected aggregation error or the amount of communication per user scales as a polynomial nΩ(1) in the number of users n.
- Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.
- One example aspect is directed to a computer-implemented method to enable privacy-preserving aggregation of private data. The method includes obtaining, by one or more computing devices, private data comprising a private value. The method includes producing, by the one or more computing devices, a plurality of messages that respectively comprise a plurality of message values, wherein a total sum of the plurality of message values approximates the private value, and wherein at least one of the plurality of message values is randomly selected. The method includes providing, by the one or more computing devices, the plurality of messages for aggregation with a plurality of additional messages respectively generated for a plurality of additional private values.
- In some implementations, producing, by the one or more computing devices, the plurality of messages that respectively comprise the plurality of message values comprises: for each of one or more first iterations associated with one or more first messages of the plurality of messages: uniformly and randomly sampling, by the one or more computing devices, one of a plurality of available values to serve as the message value for such first message; and for a final iteration associated with a final message of the plurality of messages: determining, by the one or more computing devices, an intermediate sum of the message values of the one or more first messages; and selecting, by the one or more computing devices, a final value to serve as the message value for the final message such that the total sum of the intermediate sum and the final value approximates the private value.
- In some implementations, the plurality of available values comprises a set of integers extending from zero to a sample control parameter value minus one; and selecting, by the one or more computing devices, the final value to serve as the message value for the final message comprises setting, by the one or more computing devices, the final value equal to the private value minus the intermediate sum modulo the sample control parameter value.
- In some implementations, a number of the one or more first iterations is controlled by a message control parameter value.
- In some implementations, the private value comprises a scaled private value produced by scaling an unscaled private value; and obtaining, by one or more computing devices, the private data comprising the private value comprises scaling, by the one or more computing devices, the unscaled private value by a scaling control parameter value to obtain the scaled private value.
- In some implementations, the private value comprises a normalized private value produced by normalizing a raw private value; and obtaining, by one or more computing devices, the private data comprising the private value comprises normalizing, by the one or more computing devices, the raw private value according to an expected maximum private value.
- In some implementations, the method includes scaling, by the one or more computing devices, the normalized private value by a scaling control parameter value to obtain a scaled private value.
- In some implementations, the private value comprises a noised private value produced by adding noise to a raw private value; and obtaining, by one or more computing devices, the private data comprising the private value comprises pre-randomizing, by the one or more computing devices, the raw private value according to a shared noise probability to obtain the noised private value.
- In some implementations, one or more of the sampling control parameter value, the scaling control parameter value, and the message control parameter value comprises a user-specified hyperparameter or a learned hyperparameter.
- In some implementations, one or more of the sampling control parameter value, the scaling control parameter value, and the message control parameter value is greater than or equal to four.
- In some implementations, providing, by the one or more computing devices, the plurality of messages for aggregation comprises transmitting, by the one or more computing devices, the plurality of messages to a shuffler model configured to shuffle the plurality of messages with the plurality of additional messages.
- In some implementations, the one or more computing devices consist of a user device.
- In some implementations, the private value comprises one or more of: an update value for a parameter of a machine-learned model; a heavy hitter value; an entropy value; a quantization value; or a support size value.
- In some implementations, the method includes encrypting, by the one or more computing devices, at least one of the plurality of messages.
- In some implementations, the method includes encrypting, by the one or more computing devices, at least one of the plurality of messages with a public key associated with a shuffler model configured to shuffle the plurality of messages.
- Another example aspect is directed to a computing system configured to perform any portion of the computer-implemented method described herein.
- Another example aspect is directed to one or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause the one or more processors to perform any portion of the computer-implemented method described herein.
- Another example aspect is directed to a computing system comprising: one or more processors; and one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising: obtaining a plurality of multisets of messages, wherein a plurality of private values are respectively associated with the plurality of multisets of messages, each multiset of messages comprising two or more messages that respectively contain two or more message values that sum to approximate the private value associated with such multiset of messages, and wherein at least one of the two or more message values comprises a random value; and aggregating the message values for the plurality of multisets of messages to obtain an aggregate sum that approximates a sum of the plurality of private values.
- In some implementations, the plurality of multisets of messages have been respectively generated by a plurality of different devices, and wherein the messages have been shuffled and are randomly distributed amongst each other without regard to which of the plurality of different devices generated each message.
- In some implementations, aggregating the message values for the plurality of multisets of messages comprises: determining a sum of the message values modulo a sampling control parameter value.
- In some implementations, aggregating the message values for the plurality of multisets of messages comprises further comprises: downscaling the sum of the message values modulo the sampling control parameter value by a scaling control parameter value.
- In some implementations, aggregating the message values for the plurality of multisets of messages comprises: determining an intermediate value that equals a sum of the message values modulo a sampling control parameter value; and performing the following return logic: if the intermediate value is greater than two times a number of the private values times a scaling control parameter value: returning zero; else if the intermediate value is greater than the number of the private values times the scaling control parameter value: returning the number of the private values; and else: returning the intermediate value divided by the scaling control parameter value.
- In some implementations, aggregating the message values for the plurality of multisets of messages comprises determining, based on the aggregate sum that approximates the sum of the plurality of private values, an average value that approximates an average of the private values.
- Another example aspect is directed to a computer-implemented method comprising performing any of the operations described herein.
- Another example aspect is directed to one or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause the one or more processors to perform any of the operations described herein.
- Other aspects of the present disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
- These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, serve to explain the related principles.
- U.S. Provisional Patent Application No. 62/863,197 describes example implementations of the proposed techniques in greater detail. U.S. Provisional Patent Application No. 62/863,197 is incorporated into and forms a part of this disclosure.
- Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:
-
FIG. 1 depicts a block diagram of an example computing system according to example embodiments of the present disclosure. -
FIG. 2 depicts a block diagram of an example encoding and analysis process for secure multi-party aggregation according to example embodiments of the present disclosure. -
FIG. 3 depicts a block diagram of an example algorithm for encoding private data according to example embodiments of the present disclosure. -
FIG. 4 depicts a block diagram of an example algorithm for analyzing encoded data according to example embodiments of the present disclosure. - Reference numerals that are repeated across plural figures are intended to identify the same features in various implementations.
- Generally, the present disclosure is directed to scalable and differentially private distributed aggregation, for example, in the shuffled model. In particular, the present disclosure proposes a simple and more efficient protocol for aggregation in the shuffled model, where communication as well as error increases only polylogarithmically in the number of users n. The proposed technique is a conceptual “invisibility cloak” that makes users' data almost indistinguishable from random noise while introducing zero distortion on the sum.
- Specifically, an encoding process performed by a computing device (e.g., a user's private device) can include obtaining private data that includes a private value. According to an aspect of the present disclosure, the computing device can produce a plurality of messages that respectively comprise a plurality of message values, where a total sum of the plurality of message values approximates the private value, and where at least one of the plurality of message values is randomly selected. The device can provide the plurality of messages for aggregation with a plurality of additional messages respectively generated for a plurality of additional private values. For example, the messages can be transmitted to a shuffler model configured to shuffle the plurality of messages with the plurality of additional messages. Once all of the messages are shuffled, an analyzer can determine a sum of all private values from all users but cannot (with some guarantees) determine any single private value from any single user.
- More particularly, example aspects of the present disclosure are directed to the problem of privately summing n numbers in the shuffled model recently defined by Cheu et al. For consistency with the literature, the term aggregation is used for the sum operation.
- Consider n users with private data values x1, . . . , xn∈ [0,1]. In the shuffled model, user i applies a randomized encoder algorithm E that maps xi to a multiset of m messages, E(xi)={yi,1, . . . , yi,m}⊆
- A protocol in the shuffled model is (ε, δ)-differentially private if
- Two protocols for aggregation in the shuffled model were recently suggested by Balle et al. and Cheu et al. These and all other previously known protocols have either communication or error that grows as nΩ(1). This is unavoidable for single-message protocols. However, aspects of the present disclosure show that such a trade-off is not necessary—it is possible to avoid the nΩ(1) factor in both the error bound and the amount of communication per user. The precise results obtained depend on the notion of “neighboring dataset” in the definition of differential privacy.
- In particular, aspects of the present disclosure build upon a technique from protocols for secure multi-party aggregation: Ensure that individual numbers passed to the analyzer are fully random by adding random noise terms but coordinate the noise such that all noise terms cancel, and the sum remain the same as the sum of the original data.
- A new insight provided herein is that in the shuffled model the addition of zero-sum noise can be done without coordination between the users. Instead, each user individually can produce numbers y1, . . . , ym that are fully random except that they sum to xi, and pass them to the shuffler. This is visualized, for example, in
FIG. 2 . Conceptually the noise that is introduced acts as an invisibility cloak: The data is still there, possible to aggregate, but is almost impossible to gain any other information from. - The details of one example encoder is given as
Algorithm 1, which is presented inFIG. 3 . For parameters N, k, and m to be specified later it converts each input xi to a set of random message values {yi, . . . , ym} whose sum, up to scaling and rounding, equals xi. When the output of all encoders EN,k,m(xi) is composed with a shuffler this directly gives differential privacy with respect to sum-preserving changes of data (where the sum is considered after rounding). InAlgorithm 1, the notation Uniform(R) is used to denote a value uniformly sampled from a finite set R. - In some implementations, to achieve differential privacy with respect to single-user changes the protocol can be combined with a pre-randomizer that adds noise to each xi with some probability.
- One example analyzer is given as
Algorithm 2, which is presented inFIG. 4 . It computes z as the sum of the inputs (received from the shuffler) modulo N, which by definition of the encoder is guaranteed to equal the sum Σi ┌xik┐ of scaled, rounded inputs. If x1, . . . , xn ∈ [0,1] this sum will be in [0, nk] andz /k will be within n/k of the true sum Σi xi. In the setting where a pre-randomizer adds noise to some inputs, however, it may be the case that z ∉[0, nk] in which case the analyzer can round to the nearest feasible output sum, 0 or n. - One example application of the techniques described herein in a machine learning context is gradient descent-based federated learning to learn a machine-learned model. The idea is to avoid collecting user data, and instead compute weight updates in a distributed manner by sending model parameters to users, locally running stochastic gradient descent on private data, and aggregating model updates over all users. Using a secure aggregation protocol guards against information leakage from the update of a single user, since the server only learns the aggregated model update.
- Although federated learning is one useful application, many other applications exist as well. For example, the proposed techniques can easily be applied to other problems such as: finding heavy hitters (e.g., finding the most commonly typed words into a virtual keyboard); entropy estimation; quantile estimation; support size estimation; and/or other problems. The proposed algorithms can be used for aggregation in conjunction with any context that demonstrates the linearity property.
- More particularly, although the proposed protocol is described with reference to performing sums/aggregation, it can alternatively be used to perform computation of any statistics that can be approximated by a linear sketch. The basic idea behind linear sketches is to compress a n-dimensional vector x by multiplying it by a suitable random m×n matrix A where m<<n. This gives a vector Ax of (much smaller) dimension m. Such linear sketches form the basis of efficient algorithms for a variety of estimation tasks including norms, entropy, support size, quantiles, and heavy hitters.
- The systems and methods of the present disclosure provide a number of technical effects and benefits, including, as one example, reducing communication costs associated with secure aggregation of private data. In particular, current practical secure aggregation protocols such as that of Bonawitz et al. have user computation cost O(n2) and total communication complexity O(n2), where n is the number of users. This limits the number of users that can participate in the secure aggregation protocol. In contrast, in the proposed techniques, communication per user as well as error increases only polylogarithmically in n. Thus, secure aggregation can be performed with reduced communication costs. Reduced communication costs can conserve computing resources such as processor usage, memory usage, network bandwidth, and the like.
- In addition, the proposed techniques can enable improved privacy. For example, the privacy analysis for many existing secure aggregation protocols assumes of an “honest but curious” server that does not deviate from the protocol, so some level of trust in the secure aggregation server is required. In contrast, protocols based on shuffling operate with much weaker assumptions on the server. In addition to this advantage, along with providing differential privacy guarantees, total work and communication of the proposed new protocol scales near-linearly with the number of users.
- Further, although differential privacy is established for a number m of messages per user that is logarithmic in the number of users, much smaller values of m can be used in practice. Under standard cryptographic assumptions, the communication can be reduced by amortizing the cost of the m−1 random messages over sufficiently long input vectors. For instance, if one wishes to aggregate vectors of length d, instead of having each user sending d*(m−1) random values to the shuffler, each user can simply send (1) d random seeds that can be expanded into a pseudorandom vector of length d*(m−1), as well as (2) the true data vector with the random vectors subtracted. Thus, the communication overhead would be just a constant factor (e.g., amortized over long vectors).
- In addition, one of the bottlenecks of current shuffling-based schemes is the encryption overhead incurred for messages sent from the users to the shuffler. One advantage of the proposed method is that, although there are m messages sent by each user to the shuffler, privacy can still be maintained via encryption of just one of these messages while sending the remaining messages in plaintext. This is because, by design of the protocol, any subset of m−1 messages is random and independent. Thus, reduced amounts of encryption may need to be performed, thereby conserving computing resources such as processor usage, memory usage, and the like.
- The present disclosure shows that a trade-off between privacy and scalability is not necessary—it is possible to avoid the nΩ(1) factor in both the error bound and the amount of communication per user. The precise results obtained depend on the notion of “neighboring dataset” in the definition of differential privacy. The present disclosure considers the standard notion of neighboring dataset in differential privacy, that the input of a single user is changed, and show:
-
Theorem 1. Let ε>0 and δ ∈ (0,1) be any real numbers. There exists a protocol in the shuffled model that is (ε, δ)-differentially private under single-user changes, has expected error -
- and where each encoder sends
-
- messages of
-
- bits.
- The present disclosure also considers a different notion similar to the gold standard of secure multi-party computation: Two datasets are considered neighboring if the their sums (taken after discretization) are identical. This notion turns out to allow much better privacy, even with zero noise in the final sum—the only error in the protocol comes from representing the terms of the sum in bounded precision.
-
Theorem 2. Let ε>0 and δ ∈ (0,1) be any real numbers and let -
- There exists a protocol in the shuffled model that is (ε, δ)-differentially private under sum-preserving changes, has worst-
case error 2−m, and where each encoder sends m messages of O(m) bits. - In addition to analyzing error and privacy of the proposed new protocol, this disclosure considers its resilience towards untrusted users that may deviate from the protocol. While the shuffled model is vulnerable to such attacks in general, the privacy guarantees of the proposed protocol are robust even to a large fraction of colluding users.
- Various applications, use cases, or other implementations of the technologies described herein are possible. In one example, the private data can be usage metrics for a client device. In one example, a client device can be an Internet of Things device (AKA a “smart” device). Example usage metrics include hours of operation (e.g., per day), CPU usage, data usage, etc. In another example, the private data can be measurements of interaction of a user with various items of content such as photographs, webpages, documents, files, search results, or other items of content. In one example, a client device can be a user device such as, for example, a laptop, tablet, smartphone, or device that can be worn.
- This section first considers privacy with respect to sum-preserving changes to the input, arguing that observing the output of the shuffler gives almost no information on the input, apart from the sum. The use of “we” and “our” in this section refers to concepts and implementations of the present disclosure.
- The proof strategy is to show privacy in the setting of two players and then argue that this implies privacy for n players, essentially because the two-player privacy holds regardless of the behavior of the other players. In the two-player case we first argue that with high probability the outputs of the encoders satisfy a smoothness condition saying that every potential input x1, x2 to the encoders corresponds to roughly the same number of divisions of the 2m shuffler outputs into sets of size m. Finally we argue that smoothness in conjunction with the 2m elements being unique implies privacy.
- This disclosure uses Uniform(R) to denote a value uniformly sampled from a finite set R, and denoted by St the set of all permutations of {0, t−1}. Unless stated otherwise, sets in this disclosure will be multisets. It will be convenient to work with indexed multisets whose elements are identified by indices in some set I. We can represent a multiset M⊆R with index set I as a function M:I→R. Multisets M1 and M2 with index sets I1 and I2 are considered identical if there exists a bijection π: I1→I2 such that M1(i)=M2(π(i)) for all i ∈ I1. For disjoint I1 and I2 we define the union of M1 and M2 as the function defined on I1∪I2 that maps i1∈ I1 to M1(i1) and i2 ∈ I2 to M2(i2)
- Differential Privacy and the Shuffled Model.
- This disclosure considers the established notion of differential privacy, formalizing that the output distribution does not differ much between a certain data set and any “neighboring” dataset.
-
Definition 1 Let - We consider two notions of “neighboring dataset”: 1) That the input of a single user is changed, but all other inputs are the same, and 2) That the sum of user inputs is preserved. In the latter case we consider the sum after rounding to the nearest lower multiple of 1/k, for a large integer parameter k, i.e., (x1, . . . , xn) ∈ [0,1]n is a neighbor of (x′1, . . . , x′n) ∈ [0,1]n if and only if Σi xik=Σi x′ik. (Alternatively, just assume that the input is discretized such that xik is integer.)
- In the shuffled model, the algorithm that we want to show differentially private is the composition of the shuffler and the encoder algorithm run on user inputs. In contrast to the local model of differential privacy, the outputs of encoders do not need to be differentially private. We refer to [5] for details.
- Common Lemmas
- Let
-
-
- We name the collection of multisets that are γ-smooth and contain 2m distinct elements:
-
- Given x1, x2 ∈ [0,1] such that x1k and x2k are integers, consider the multisets EN,k,m(x1)={y1, . . . , ym} and EN,k,m(x2)={ym+1, . . . , y2m}, and let E(x1, x2)={y1, . . . , y2m} be their multiset union. The multiset E(x1, x2) is a random variable due to the random choices made by the encoder algorithm.
-
-
-
-
- pairs yields an upper bound of 2 m2/N on the probability that there is at least one duplicate pair. Second, we bound the probability that E(x1, x2) is not γ-smooth. Let I1={1, . . . , m} and I2={m+1, . . . ,2m}. Then by definition of the encoder, XI
1 (E (x1, x2))=x1 and XI2 (E (x1, x2))=x2 withprobability 1. For each I E1 (x)=1x1 =x and ZI2 (x)=1x2 =x it will be helpful to disregard these fixed terms in Z(x). Thus we define Z′(x)= -
- The second equality uses that
-
- because it is a product of two independent, zero-mean random variables. The inequality holds because ZI(x) is an indicator function. By Chebychev's inequality over the random choices in the encoder, for any σ>0:
-
- For m≥4 we can bound
-
- as follows:
-
- Using this for upper and lower bounding μ in (1), and choosing σ=γ/3 we get:
-
-
-
-
- Conditioned on (2) we have:
-
- The final inequality uses the assumption that γ>6√{square root over (m)}/22m. A similar computation shows that conditioned on (2),
-
- Corollary 1 For m≥4, and m=3┌log N┐,
-
- Proof. We invoke
Lemma 1 with γ=N−1 and m=3 log N The probability bound is -
- Because log N≥3 and N≥6 this shows the stated bound.
-
-
Lemma 2 For any y* -
- and for any x1 and x2, it is the case that
-
- Proof of
Lemma 2. Using the fact that all the elements in y* are distinct, we have that -
- Example Analysis of Privacy Under Sum-Preserving Changes
-
Lemma 3 For any y* -
- and for all x1, x2, x′1, x′2 that are integer multiples of 1/k and that satisfy x1+x2=x′1+x′2, it is that case that
-
- Pr[E(x′1, x′2)=y*].
- Proof of
Lemma 3. We denote by Σi yi*:=Σi∈[2m] yi* the sum of all elements in the set y*. We define -
B y*,x1 :=NumberofsubsetsSof{1, . . . ,2m}ofsizemforwhich -
- We similarly define By*,x′
1 by replacing x1 in (3) by x′1. - Since y*
-
-
Lemma 2 implies that -
- Similarly, we love that
-
- Since y* is γ-smooth,
Definition 2 implies that -
- By Equations (46) and (5) and the assumption that x1+x2=x′1+x′2 (as well as the assumption that x1, x2, x′1, x′2 are all integer multiples of 1/k), we get that for every γ-smooth y* whose sum is not equal to x1+x2k, it is the case that
-
Pr[E(x 1 ,x 2)=y*]=Pr[E(x′ 1 ,x′ 2)=y*]=0, (7) - and for every γ-smooth y* whose sum is equal to x1+x2k, the ratio of Equations (46) and (5) along with (6) give that
-
-
-
Pr[E(x j1 ,x j2 )∈T]≤e ε ·Pr[E(x′ j1 ,x′ j2 )∈T]+δ (9) -
-
Pr[E(x 1 ,x 2 , . . . ,x n)∈S]≤e ε ·Pr[E(x 1′ ,x 2′ , . . . ,x n′)∈S]+δ. -
-
- Then, we observe that
-
- where (10) follows from (9) and the fact that xi=xi, for i=3,4, . . . , n. This completes the proof.
-
-
- Pr[E(x1′, x2′, . . . , xn,) ∈ S]+η, where
-
-
-
- where (11) and (12) follow from
Lemma 1 andLemma 3, respectively. The desired result now follows from a direct application of Lemma 4. - Using Lemma 5 as a building block for analyzing differential privacy guarantees in the context of sum-preserving swaps, we can derive a differential privacy result with respect to general sum-preserving changes.
- Lemma 6 Suppose x=(x1, x2, . . . , xn) and x′=(x1′, x2′, . . . , xn,) have coordinates that are integer multiples of 1/k satisfying x1+x2+ . . . +xn=x1′+x2′+ . . . +xn, and x′ can be obtained from x by a series of t sum-preserving swaps. Then, for any S, we have
-
- Proof of Lemma 6. We prove the lemma by induction on t. Note that the case t=1 holds by Lemma 5. Now, for the inductive step, suppose the lemma holds for t=r. We wish to show that it also holds for t=
r+ 1. Note that there exists some x″ ∈ -
- Moreover, by Lemma 4, we have that
-
Pr[E(x 1′ ,x 2′ , . . . ,x n′)∈S]≤βPr[E(x 1″ ,x 2″ , . . . ,x n″)∈S]+η. (14) - Combining (13) and (14), we note that
-
- which establishes the claim for t=
r+ 1. - As a consequence, we obtain the following main theorem establishing differential privacy of
Algorithm 1 with respect to sum-preserving changes in the shuffled model: We are now ready to proveTheorem 2. - Proof of
Theorem 2. InAlgorithm 1, each user communicates at most O(mlogN) bits which are sent via m messages. Note that if x=(x1, . . . , xn) and x′=(x1′, . . . , xn,) have coordinates that are integer multiples of 1/k satisfying x1+ . . . +xn=x1′+ . . . +xn′, then there is a sequence of t≤n−1 sum-preserving swaps that allows us to transform x into x′. Thus, Lemma 6 implies thatAlgorithm 1 is (ε, δ)-differentially private with respect to sum-preserving changes if -
- for any
-
- The error in our final estimate (which is due to rounding) is O(n/k) in the worst case. The theorem now follows by choosing
-
- and N being the first odd integer larger than
-
- Example Analysis of Privacy Under Single-User Changes
- One example idea is to run
Algorithm 1 after having each player add some noise to her input, with some fixed probability independently from the other players. The noise distribution can satisfy three properties: it should be supported on a finite interval, the logarithm of its probability mass function should have a small Lipschitz-constant (even under modular arithmetic) and its variance should be small. The following truncated version of the discrete Laplace distribution satisfies all three properties. -
-
- for every integer k in the range
-
- Lemma 7 (Log-Lipschitzness) Let N be a positive odd integer and p ∈ (0,1) a real number. Define the interval
-
- For all k ∈ {0, . . . , N−1} and all t ∈ I, it is the case that
-
- Proof of Lemma 7. We start by noting that (15) implies that
-
- We distinguish six cases depending on the values of k and k+t:
-
- In
Cases -
- which implies that |k mod I|=k and hence the denominator in (16) satisfies
-
p |k modI| =p k. (23) - Plugging (23) in (16), we get
-
- We now separately examine each of these three cases.
-
Case 1. - If (17) holds, then |(k+t)modI|=−k−t and the numerator in (24) becomes
-
p |(k+t)modI| =p −k−t. (25) - Plugging (25) in (24), we get
-
- Using the facts that k+t<0 and k≥0, and thus that t<0, we get that the quantity in (26) is at most p−|t| and at least p|t|.
-
Case 2. - If (18) holds, then |(k+t)modI|=k+t and the numerator in (24) becomes
-
p |(k+t)modI| =p k+t. (27) - Plugging (27) in (24), we get
-
-
Case 3. - If (19) holds, then |(k+t)modI|=N−k−t and the numerator in (24) becomes
-
p |(k+t)modI| =p N−k−t. (28) - Plugging (28) in (24), we get
-
- Using the fact that
-
- which, along with the fact that
-
- implies that t>0, we get that the quantity in (29) is at most p−|t| and at least p|t|.
- We now turn to Cases 4, 5 and 6. In these,
-
- which implies that |kmodI|=N−k and hence the denominator in (16) satisfies
-
p|kmodI|=pN−k. (30) - Plugging (30) in (16), we get
-
- We now separately examine each of these three cases.
- Case 4.
- If (20) holds, then |(k+t)modI|=k+t and the numerator in (31) becomes
-
p |(k+t)modI| =p k+t. (32) - Plugging (32) in (31), we get
-
- Using the facts that
-
- we deduce that t<0 and that the quantity in (33) is at most p−|t| and at least p|t|.
- Case 5.
- If (21) holds, then |(k+t)modI|=N−k−t and the numerator in (31) becomes
-
p |(k+t)modI| =p N−k−t. (34) - Plugging (34) in (31), we get
-
- Case 6.
- If (22) holds, then |(k+t)modI|=k+t−N and the numerator in (31) becomes
-
p |(k+t)modI| =p k+t−N. (35) - Plugging (35) (35), we get
-
- Using the facts that k<N and k+t≥N, we get that t<0 and that the quantity in (36) is at most p−|t| and at least p|t|.
-
-
- In order to prove Lemma 8, we will need the simple fact given in Lemma 9.
- Lemma 9 For any p ∈ [0,1), it is the case that
-
- Proof of Lemma 9. For every p E [0,1), we consider the geometric series f(p):=Σk=1 ∞ pk. Differentiating and multiplying by p, we get pf′(p)=Σk=1 ∞ kpk. Differentiating a second time and multiplying by p, we get
-
- Using the formula for a convergent geometric series, we have
-
- Plugging this expression in (37) and differentiating, we get
-
- Proof of Lemma 8. We have that
-
-
-
-
- Applying Lemma 9 in (40) and simplifying, we get that
-
- The next lemma will be used to show that the proposed algorithm is differentially private with respect to single-user changes.
-
-
-
- and for all x1, x2, x′1 ∈ [0,1), if we denote
-
- then
-
- and
- Proof of Lemma 10. As in Lemma 7, we define the interval
-
- We define
-
- We similarly define By*,x′
1 and By*,x2 by replacing x1 in (44) by x′1 and x2 respectively. - Proof of Inequality (41).
- By
Lemma 2, we have that -
- By
Lemma 2, we also have that -
- Since y* is γ-smooth,
Definition 2 implies that -
- Applying Lemma 7 with k=Σi∈[2m] y*i−└x′1k┘−└x2 k┘ and t=└x′1k┘−└x1 k┘ and using the fact that x1, x′1 ∈ [0,1) gives
-
- Dividing (46) by (47) and using (48) and (49), we get Inequality (41).
- Proof of Inequality (42).
- We note that similarly to (47) we have
-
- Dividing (50) by (51) and using (49), we get Inequality (42).
- Proof of Inequality (43).
- By averaging over z2 and applying Inequality (42) with {tilde over (x)}2 replaced by {tilde over (x)}2+z2 (for every fixed setting of z2), we get Inequality (43).
- Lemma 11 Let N be a positive odd integer and p ∈ (0,1) and q ∈ (0,1] be real numbers. Let b1, . . . , bn be iid random variables that are equal to 1 with probability q and to 0 otherwise, let w1, . . . , wn be iid random variables that are drawn from the truncated discrete Laplace distribution
-
- for all i ∈ [n]. Then, for all j ∈ [n], all x1, . . . , xj, . . . , xn, x′j ∈ [0,1), if we donate
-
- for all i ∈ [n] and
-
- then for all S, the following inequality holds
-
- for any
-
- and where the probabilities in (52) are over z1, . . . , zn and the internal randomness of E(·).
- Proof of Lemma 11. Let A denote the event that there exists at least one i ∈ [n] for which bi=1. Then,
-
Pr[A]=1−(1−q)n≥1−e −qn, (53) - where the last inequality follows from the fact that et≥1+t for any real number t. To prove (52), it suffices to show a similar inequality conditioned on the event A, i.e.,
-
- To see this, denote by Ā the complement of the event A and assume that (54) holds. Then,
-
- where (55) and (57) follow from (53), and where (56) follows from the assumption that (54) holds. We thus turn to the proof of (54). Note that it suffices to prove this inequality for any fixed setting of b1, . . . , bn satisfying the event A, i.e.,
-
- and (54) would follows from (58) by averaging. Henceforth, we fix a setting of b1, . . . , bn satisfying the event A. Without loss of generality, we assume that j=1. If bj=0, then the event A implies that there exists j2≠j such that bj
2 =1. Without loss of generality, we assume that j2=2. In order to show (58) for this setting of b1, . . . , bn, it suffices to show the same inequality where we also condition on any setting of w3, . . . , wn, i.e., -
- Applying Lemma 4 with j1=j=1 and j2=2 and with inputs {tilde over (x)}′3+Z3, . . . , {tilde over (x)}′n+zn for the non-selected players, we get that to prove (59), it suffices to show that for any set T, the following inequality holds
-
- We now prove (60):
-
- with
-
- and where (61) follows by averaging over all settings of z1, z2 and invoking
Lemma 1, and (62) follows from Lemma 10 and the fact that at least one of b1, b2 is equal to 1. - As a consequence, we obtain the following main theorem establishing differential privacy of
Algorithm 1 with respect to single-user changes in the shuffled model: We are now ready to prove Theorem 1.1. - Proof of
Theorem 1. InAlgorithm 1, each user communicates at most O(mlogN) bits which are sent via m messages. By Lemma 11,Algorithm 1 is (ε, δ)-differentially private with respect to single-user changes if -
- for any
-
- The error in our final estimate consists of two parts: the rounding error which is O(n/k) in the worst case, and the error due to the added folded Discrete Laplace noise whose average absolute value is at most
-
- (this follows from Lemma 8 along with the facts that the variance is additive for independent random variables, and that for any zero-mean random variable X, it is the case that E[|X|]≤√{square root over (Var[X]))}. The theorem now follows by choosing
-
- and N being the first odd integer larger than
-
- Example Resilience Against Colluding Users
- This section formalizes the resilience of
Algorithm 1 against a very large fraction of the users colluding with the server (thereby revealing their inputs and messages). - Lemma 12 (Resilient privacy under sum-preserving changes) Let C ⊆ [n] denote the subset of colluding users. Then, for all x1, . . . , . . . , xi, and x′1, . . . , . . . , x′n that are integer multiples of 1/k in the interval [0,1) and that satisfy Σj∉C xj=Σj∉C x′j and x′j=xj for all j ∈ C, and for all subsets S, the following inequality holds
-
- for
-
- and where the probabilities in (63) are over the internal randomness of E(·).
- Lemma 13 (Resilient privacy under single-user changes) Let N be a positive odd integer and p ∈ (0,1) and q ∈ (0,1] be real numbers. Let C ⊆ [n] denote the subset of colluding users. Let b1, . . . , bn be iid random variables that are equal to 1 with probability q and to 0 otherwise, let w1, . . . , wn be iid random variables that are drawn from the folded discrete Laplace distribution
-
- for all i ∈ [n]. If |C|≤0.9n, then for all j ∉ C, all x1, . . . , xj, . . . , xn, x′j ∈ [0,1) and all subsets S, if we denote
-
- then
-
- for any
-
- and where the probabilities in (64) are over z1, . . . , zn and the internal randomness of E(·).
- Proof of Lemma 12. We start by applying Lemma 4 in order to condition on the messages of all the colluding users. This allows us to reduce to the case where the messages of all users in C are fixed and where we would like to prove the differential privacy guarantee with respect to single-user changes on the inputs of the smaller subset [n]\C of (non-colluding) users. The rest of the proof follows along the same lines as the proof of Lemma 6 with any modification in the bounds.
- Proof of Lemma 13. We start by applying Lemma 4 in order to condition on the messages of all colluding users. This allows us to reduce to the case where the messages of all users in C are fixed and where we would like to prove differential privacy guarantees with respect to sum-preserving changes on the smaller subset [n]\C of (non-colluding) users. The rest of the proof follows along the same lines as the proof of Lemma 11. Note that that the tail probability term e−qn in (52) is replaced by the slightly larger quantity e−q(n−|C|) in (64) as the event A in the proof of Lemma 11 has now to be defined over the smaller set [n]\C of non-colluding users (and consequently the bounds in (53) and (57) are modified similarly).
- Example Devices and Systems
-
FIG. 1 depicts anexample computing system 100 that can be used to implement one example application of the methods and systems of the present disclosure in the federated learning context. Federated learning is provided as one example only, the proposed aggregation techniques can be applied to many other different problems/applications. Thesystem 100 can be implemented using a client-server architecture that includes aserver 110 that communicates with one ormore client devices 130 and/or ashuffler 150 over a network. - Each
client device 130 can include one or more processor(s) 132 and amemory 134. The one or more processor(s) 132 can include, for example, one or more central processing units (CPUs), graphics processing units (GPUs) dedicated to efficiently rendering images or performing other specialized calculations, and/or other processing devices. Thememory 134 can include one or more computer-readable media and can store information accessible by the one ormore processors 132, includinginstructions 136 that can be executed by the one ormore processors 132 anddata 138. - The
instructions 136 can include instructions for implementing a local updater configured to determine one or more local updates to a machine-learned model (e.g., a set of values descriptive of changes to the model parameters based on a set of locally stored training data). For example, the local updater can perform one or more training techniques such as, for example, backwards propagation of errors to re-train or otherwise update the model based on the locally stored training data. The local updater can be included in an application or can be included in the operating system of thedevice 130. - The locally stored
data 138 such as the local update can be considered private data. The local update is used only as one example of private data that can be securely aggregated. Any form of private data can be securely aggregated according to the described techniques. - The
instructions 136 can further include instructions for implementing an encoder to encode the private data such as the local update. For example, the encoder can perform one or more of the encoding techniques described herein (e.g., theencoding Algorithm 1 shown inFIG. 3 ). In particular, the encoder can encode the private data (e.g., the local update) into a plurality of messages and the messages can be transmitted to ashuffler 150. - The
client device 130 ofFIG. 1 can include various input/output devices for providing and receiving information from a user, such as a touch screen, touch pad, data entry keys, speakers, and/or a microphone suitable for voice recognition. - The
client device 130 can also include a network interface used to communicate with one or more remote computing devices (e.g. server 110) over the network. The network interface can include any suitable components for interfacing with one more networks, including for example, transmitters, receivers, ports, controllers, antennas, or other suitable components. - The
shuffler 150 can receive a respective plurality of messages from each of theclient devices 130 and can randomly shuffle them so that the messages are randomly distributed amongst each other without regard to which of the plurality ofdifferent devices 130 generated each message. In some implementations, multiple shufflers can be used (e.g., sequentially) to provide added layer(s) of privacy assurance. - The
system 100 also includes aserver 110, such as a web server. Theserver 110 can be implemented using any suitable computing device(s). Theserver 110 can have one ormore processors 112 and one ormore memory devices 114. Theserver 110 can be implemented using one server device or a plurality of server devices. In implementations in which a plurality of devices are used, such plurality of devices can operate according to a parallel computing architecture, a sequential computing architecture, or a combination thereof. - The
server 110 can also include a network interface used to communicate with one ormore client devices 130 over the network. The network interface can include any suitable components for interfacing with one more networks, including for example, transmitters, receivers, ports, controllers, antennas, or other suitable components. - The one or
more processors 112 can include any suitable processing device, such as a microprocessor, microcontroller, integrated circuit, logic device, or other suitable processing device. The one ormore memory devices 114 can include one or more computer-readable media, including, but not limited to, non-transitory computer-readable media, RAM, ROM, hard drives, flash drives, or other memory devices. The one ormore memory devices 114 can store information accessible by the one ormore processors 112, including computer-readable instructions 116 that can be executed by the one ormore processors 112. - The
instructions 116 can be any set of instructions that when executed by the one ormore processors 112, cause the one ormore processors 112 to perform operations. For instance, theinstructions 116 can be executed by the one ormore processors 112 to implement aglobal updater 120. Theglobal updater 120 can be configured to update a global model based at least in part on a sum or average of local updates computed at theclient devices 130. - The
instructions 116 can further include instructions that cause theserver 110 to implement ananalyzer 122. Theanalyzer 122 can determine the sum or average of local updates based on the shuffled messages. Theanalyzer 122 can perform any of the analysis techniques described herein, includingAlgorithm 2 shown inFIG. 4 . - As shown in
FIG. 1 , the one ormore memory devices 114 can also storedata 118 that can be retrieved, manipulated, created, or stored by the one ormore processors 112. Thedata 118 can include, for instance, local updates, global parameters, and other data. Thedata 118 can be stored in one or more databases. The one or more databases can be connected to theserver 110 by a high bandwidth LAN or WAN, or can also be connected toserver 110 through the network. The one or more databases can be split up so that they are located in multiple locales. - The
server 110 can exchange data with one ormore client devices 130 and/orshuffler 150 over the network. Any number ofclient devices 130 can be connected to theserver 110 and/orshuffler 150 over the network. Each of theclient devices 130 can be any suitable type of computing device, such as a general purpose computer, special purpose computer, laptop, desktop, mobile device, navigation system, smartphone, tablet, wearable computing device, gaming console, a display with one or more processors, or other suitable computing device. - The network can be any type of communications network, such as a local area network (e.g. intranet), wide area network (e.g. Internet), cellular network, or some combination thereof. The network can also include a direct connection between a
client device 130 and theserver 110. In general, communication between theserver 110 and aclient device 130 can be carried via network interface using any type of wired and/or wireless connection, using a variety of communication protocols (e.g. TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g. HTML, XML), and/or protection schemes (e.g. VPN, secure HTTP, SSL). - The machine-learned models described in this specification and/or generated using techniques described in this specification may be used in a variety of tasks, applications, and/or use cases.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be image data. The machine-learned model(s) can process the image data to generate an output. As an example, the machine-learned model(s) can process the image data to generate an image recognition output (e.g., a recognition of the image data, a latent embedding of the image data, an encoded representation of the image data, a hash of the image data, etc.). As another example, the machine-learned model(s) can process the image data to generate an image segmentation output. As another example, the machine-learned model(s) can process the image data to generate an image classification output. As another example, the machine-learned model(s) can process the image data to generate an image data modification output (e.g., an alteration of the image data, etc.). As another example, the machine-learned model(s) can process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine-learned model(s) can process the image data to generate an upscaled image data output. As another example, the machine-learned model(s) can process the image data to generate a prediction output.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be text or natural language data. The machine-learned model(s) can process the text or natural language data to generate an output. As an example, the machine-learned model(s) can process the natural language data to generate a language encoding output. As another example, the machine-learned model(s) can process the text or natural language data to generate a latent text embedding output. As another example, the machine-learned model(s) can process the text or natural language data to generate a translation output. As another example, the machine-learned model(s) can process the text or natural language data to generate a classification output. As another example, the machine-learned model(s) can process the text or natural language data to generate a textual segmentation output. As another example, the machine-learned model(s) can process the text or natural language data to generate a semantic intent output. As another example, the machine-learned model(s) can process the text or natural language data to generate an upscaled text or natural language output (e.g., text or natural language data that is higher quality than the input text or natural language, etc.). As another example, the machine-learned model(s) can process the text or natural language data to generate a prediction output.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be speech data. The machine-learned model(s) can process the speech data to generate an output. As an example, the machine-learned model(s) can process the speech data to generate a speech recognition output. As another example, the machine-learned model(s) can process the speech data to generate a speech translation output. As another example, the machine-learned model(s) can process the speech data to generate a latent embedding output. As another example, the machine-learned model(s) can process the speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.). As another example, the machine-learned model(s) can process the speech data to generate an upscaled speech output (e.g., speech data that is higher quality than the input speech data, etc.). As another example, the machine-learned model(s) can process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, the machine-learned model(s) can process the speech data to generate a prediction output.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be latent encoding data (e.g., a latent space representation of an input, etc.). The machine-learned model(s) can process the latent encoding data to generate an output. As an example, the machine-learned model(s) can process the latent encoding data to generate a recognition output. As another example, the machine-learned model(s) can process the latent encoding data to generate a reconstruction output. As another example, the machine-learned model(s) can process the latent encoding data to generate a search output. As another example, the machine-learned model(s) can process the latent encoding data to generate a reclustering output. As another example, the machine-learned model(s) can process the latent encoding data to generate a prediction output.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be statistical data. The machine-learned model(s) can process the statistical data to generate an output. As an example, the machine-learned model(s) can process the statistical data to generate a recognition output. As another example, the machine-learned model(s) can process the statistical data to generate a prediction output. As another example, the machine-learned model(s) can process the statistical data to generate a classification output. As another example, the machine-learned model(s) can process the statistical data to generate a segmentation output. As another example, the machine-learned model(s) can process the statistical data to generate a segmentation output. As another example, the machine-learned model(s) can process the statistical data to generate a visualization output. As another example, the machine-learned model(s) can process the statistical data to generate a diagnostic output.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be sensor data. The machine-learned model(s) can process the sensor data to generate an output. As an example, the machine-learned model(s) can process the sensor data to generate a recognition output. As another example, the machine-learned model(s) can process the sensor data to generate a prediction output. As another example, the machine-learned model(s) can process the sensor data to generate a classification output. As another example, the machine-learned model(s) can process the sensor data to generate a segmentation output. As another example, the machine-learned model(s) can process the sensor data to generate a segmentation output. As another example, the machine-learned model(s) can process the sensor data to generate a visualization output. As another example, the machine-learned model(s) can process the sensor data to generate a diagnostic output. As another example, the machine-learned model(s) can process the sensor data to generate a detection output.
- In some cases, the machine-learned model(s) can be configured to perform a task that includes encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). For example, the task may be audio compression task. The input may include audio data and the output may comprise compressed audio data. In another example, the input includes visual data (e.g. one or more image or videos), the output comprises compressed visual data, and the task is a visual data compression task. In another example, the task may comprise generating an embedding for input data (e.g. input audio or visual data).
- In some cases, the input includes visual data and the task is a computer vision task. In some cases, the input includes pixel data for one or more images and the task is an image processing task. For example, the image processing task can be image classification, where the output is a set of scores, each score corresponding to a different object class and representing the likelihood that the one or more images depict an object belonging to the object class. The image processing task may be object detection, where the image processing output identifies one or more regions in the one or more images and, for each region, a likelihood that region depicts an object of interest. As another example, the image processing task can be image segmentation, where the image processing output defines, for each pixel in the one or more images, a respective likelihood for each category in a predetermined set of categories. For example, the set of categories can be foreground and background. As another example, the set of categories can be object classes. As another example, the image processing task can be depth estimation, where the image processing output defines, for each pixel in the one or more images, a respective depth value. As another example, the image processing task can be motion estimation, where the network input includes multiple images, and the image processing output defines, for each pixel of one of the input images, a motion of the scene depicted at the pixel between the images in the network input.
- In some cases, the input includes audio data representing a spoken utterance and the task is a speech recognition task. The output may comprise a text output which is mapped to the spoken utterance. In some cases, the task comprises encrypting or decrypting input data. In some cases, the task comprises a microprocessor performance task, such as branch prediction or memory address translation.
- The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For instance, processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
- While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, upon attaining an understanding of the foregoing, can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present disclosure cover such alterations, variations, and equivalents.
Claims (25)
1. A computer-implemented method to enable privacy-preserving aggregation of private data, the method comprising:
obtaining, by one or more computing devices, private data comprising a private value;
producing, by the one or more computing devices, a plurality of messages that respectively comprise a plurality of message values, wherein a total sum of the plurality of message values approximates the private value, and wherein at least one of the plurality of message values is randomly selected; and
providing, by the one or more computing devices, the plurality of messages for aggregation with a plurality of additional messages respectively generated for a plurality of additional private values.
2. The computer-implemented method of claim 1 , wherein producing, by the one or more computing devices, the plurality of messages that respectively comprise the plurality of message values comprises:
for each of one or more first iterations associated with one or more first messages of the plurality of messages:
uniformly and randomly sampling, by the one or more computing devices, one of a plurality of available values to serve as the message value for such first message; and
for a final iteration associated with a final message of the plurality of messages:
determining, by the one or more computing devices, an intermediate sum of the message values of the one or more first messages; and
selecting, by the one or more computing devices, a final value to serve as the message value for the final message such that the total sum of the intermediate sum and the final value approximates the private value.
3. The computer-implemented method of claim 2 , wherein:
the plurality of available values comprises a set of integers extending from zero to a sample control parameter value minus one; and
selecting, by the one or more computing devices, the final value to serve as the message value for the final message comprises setting, by the one or more computing devices, the final value equal to the private value minus the intermediate sum modulo the sample control parameter value.
4. The computer-implemented method of claim 2 , wherein a number of the one or more first iterations is controlled by a message control parameter value.
5. The computer-implemented method of claim 1 , wherein:
the private value comprises a scaled private value produced by scaling an unsealed private value; and
obtaining, by one or more computing devices, the private data comprising the private value comprises scaling, by the one or more computing devices, the unsealed private value by a scaling control parameter value to obtain the scaled private value.
6. The computer-implemented method of claim 1 , wherein:
the private value comprises a normalized private value produced by normalizing a raw private value; and
obtaining, by one or more computing devices, the private data comprising the private value comprises normalizing, by the one or more computing devices, the raw private value according to an expected maximum private value.
7. The computer-implemented method of claim 6 , further comprising scaling, by the one or more computing devices, the normalized private value by a scaling control parameter value to obtain a scaled private value.
8. The computer-implemented method of claim 1 , wherein:
the private value comprises a noised private value produced by adding noise to a raw private value; and
obtaining, by one or more computing devices, the private data comprising the private value comprises pre-randomizing, by the one or more computing devices, the raw private value according to a shared noise probability to obtain the noised private value.
9. The computer-implemented method of claim 1 , wherein one or more of the sampling control parameter value, the scaling control parameter value, and the message control parameter value comprises a user-specified hyperparameter or a learned hyperparameter.
10. The computer-implemented method of claim 1 , wherein one or more of the sampling control parameter value, the scaling control parameter value, and the message control parameter value is greater than or equal to four.
11. The computer-implemented method of claim 1 wherein providing, by the one or more computing devices, the plurality of messages for aggregation comprises transmitting, by the one or more computing devices, the plurality of messages to a shuffler model configured to shuffle the plurality of messages with the plurality of additional messages.
12. The computer-implemented method of claim 1 , wherein the one or more computing devices consist of a user device.
13. The computer-implemented method of claim 1 , wherein the private value comprises one or more of:
an update value for a parameter of a machine-learned model;
a heavy hitter value;
an entropy value;
a quantization value; or
a support size value.
14. (canceled)
15. The computer-implemented method of claim 1 , further comprising:
encrypting, by the one or more computing devices, at least one of the plurality of messages with a public key associated with a shuffler model configured to shuffle the plurality of messages.
16. (canceled)
17. (canceled)
18. A computing system comprising:
one or more processors; and
one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:
obtaining a plurality of multisets of messages, wherein a plurality of private values are respectively associated with the plurality of multisets of messages, each multiset of messages comprising two or more messages that respectively contain two or more message values that sum to approximate the private value associated with such multiset of messages, and wherein at least one of the two or more message values comprises a random value; and
aggregating the message values for the plurality of multisets of messages to obtain an aggregate sum that approximates a sum of the plurality of private values.
19. The computing system of claim 18 , wherein the plurality of multisets of messages have been respectively generated by a plurality of different devices, and wherein the messages have been shuffled and are randomly distributed amongst each other without regard to which of the plurality of different devices generated each message.
20. The computing system of claim 18 , wherein aggregating the message values for the plurality of multisets of messages comprises:
determining a sum of the message values modulo a sampling control parameter value.
21. The computing system of claim 20 , wherein aggregating the message values for the plurality of multisets of messages comprises further comprises:
downscaling the sum of the message values modulo the sampling control parameter value by a scaling control parameter value.
22. The computing system of claim 18 , wherein aggregating the message values for the plurality of multisets of messages comprises:
determining an intermediate value that equals a sum of the message values modulo a sampling control parameter value; and
performing the following return logic:
if the intermediate value is greater than two times a number of the private values times a scaling control parameter value: returning zero;
else if the intermediate value is greater than the number of the private values times the scaling control parameter value: returning the number of the private values; and
else: returning the intermediate value divided by the scaling control parameter value.
23. The computing system of claim 18 , wherein aggregating the message values for the plurality of multisets of messages comprises determining, based on the aggregate sum that approximates the sum of the plurality of private values, an average value that approximates an average of the private values.
24. (canceled)
25. (canceled)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/620,438 US20220374542A1 (en) | 2019-06-18 | 2020-06-17 | Scalable and Differentially Private Distributed Aggregation |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962863197P | 2019-06-18 | 2019-06-18 | |
PCT/US2020/038109 WO2020257264A1 (en) | 2019-06-18 | 2020-06-17 | Scalable and differentially private distributed aggregation |
US17/620,438 US20220374542A1 (en) | 2019-06-18 | 2020-06-17 | Scalable and Differentially Private Distributed Aggregation |
Publications (1)
Publication Number | Publication Date |
---|---|
US20220374542A1 true US20220374542A1 (en) | 2022-11-24 |
Family
ID=71452791
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/620,438 Pending US20220374542A1 (en) | 2019-06-18 | 2020-06-17 | Scalable and Differentially Private Distributed Aggregation |
Country Status (2)
Country | Link |
---|---|
US (1) | US20220374542A1 (en) |
WO (1) | WO2020257264A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113221183B (en) * | 2021-06-11 | 2022-09-16 | 支付宝(杭州)信息技术有限公司 | Method, device and system for realizing privacy protection of multi-party collaborative update model |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160182170A1 (en) * | 2014-06-10 | 2016-06-23 | PB, Inc | System Architectures and Methods for Radiobeacon Data Sharing |
US20180039855A1 (en) * | 2016-08-08 | 2018-02-08 | Yahoo! Inc. | Quality of location-related content |
US20200372061A1 (en) * | 2019-05-23 | 2020-11-26 | Google Llc | Cross-platform content muting |
-
2020
- 2020-06-17 US US17/620,438 patent/US20220374542A1/en active Pending
- 2020-06-17 WO PCT/US2020/038109 patent/WO2020257264A1/en active Application Filing
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160182170A1 (en) * | 2014-06-10 | 2016-06-23 | PB, Inc | System Architectures and Methods for Radiobeacon Data Sharing |
US20180039855A1 (en) * | 2016-08-08 | 2018-02-08 | Yahoo! Inc. | Quality of location-related content |
US20200372061A1 (en) * | 2019-05-23 | 2020-11-26 | Google Llc | Cross-platform content muting |
Also Published As
Publication number | Publication date |
---|---|
WO2020257264A1 (en) | 2020-12-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11539515B2 (en) | High-precision privacy-preserving real-valued function evaluation | |
US11843687B2 (en) | Systems, devices, and processes for homomorphic encryption | |
Wagh et al. | Falcon: Honest-majority maliciously secure framework for private deep learning | |
Alhayani et al. | Manufacturing intelligent Corvus corone module for a secured two way image transmission under WSN | |
Hesamifard et al. | Cryptodl: Deep neural networks over encrypted data | |
Thapa et al. | Advancements of federated learning towards privacy preservation: from federated learning to split learning | |
Hesamifard et al. | Deep neural networks classification over encrypted data | |
Chen et al. | {SANNS}: Scaling up secure approximate {k-Nearest} neighbors search | |
Gheid et al. | Efficient and privacy-preserving k-means clustering for big data mining | |
Watson et al. | Piranha: A {GPU} platform for secure computation | |
Grivet Sébert et al. | SPEED: secure, PrivatE, and efficient deep learning | |
Beguier et al. | Safer: Sparse secure aggregation for federated learning | |
US20220374542A1 (en) | Scalable and Differentially Private Distributed Aggregation | |
US20210326757A1 (en) | Federated Learning with Only Positive Labels | |
Zhang et al. | A speech fully homomorphic encryption scheme for DGHV based on multithreading in cloud storage | |
Prest et al. | Quantum-Error-Mitigated Detectable Byzantine Agreement with Dynamical Decoupling for Distributed Quantum Computing | |
Leung et al. | Tight bounds on communication complexity of symmetric xor functions in one-way and smp models | |
Słomczyński et al. | Quantum dynamical entropy, chaotic unitaries and complex Hadamard matrices | |
Ghavamipour et al. | Federated Synthetic Data Generation with Stronger Security Guarantees | |
Walch et al. | Cryptotl: Private, efficient and secure transfer learning | |
Zhang et al. | Image encryption algorithm based on the Matryoshka transform and modular-inverse matrix | |
Xu et al. | FedG2L: a privacy-preserving federated learning scheme base on “G2L” against poisoning attack | |
Li et al. | PrivPy: Enabling scalable and general privacy-preserving machine learning | |
Kjamilji | Blockchain assisted secure feature selection, training and classifications in cloud and distributed edge IoT environments | |
Zhang et al. | An adaptive speech homomorphic encryption scheme based on energy in cloud storage |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE, LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GHAZI, BADIH;PAGH, RASMUS;VELINGKER, AMEYA;REEL/FRAME:058420/0019Effective date: 20190628 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |