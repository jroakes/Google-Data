EP3485361B1 - Pressure-based gesture typing for a graphical keyboard - Google Patents
Pressure-based gesture typing for a graphical keyboard Download PDFInfo
- Publication number
- EP3485361B1 EP3485361B1 EP17828476.6A EP17828476A EP3485361B1 EP 3485361 B1 EP3485361 B1 EP 3485361B1 EP 17828476 A EP17828476 A EP 17828476A EP 3485361 B1 EP3485361 B1 EP 3485361B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- keys
- gesture
- computing device
- sequence
- input
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims description 56
- 238000004891 communication Methods 0.000 description 57
- 230000015654 memory Effects 0.000 description 15
- 230000004044 response Effects 0.000 description 15
- 230000000007 visual effect Effects 0.000 description 15
- 230000003287 optical effect Effects 0.000 description 10
- 238000010586 diagram Methods 0.000 description 8
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 8
- 241000219109 Citrullus Species 0.000 description 6
- 235000012828 Citrullus lanatus var citroides Nutrition 0.000 description 6
- 230000009471 action Effects 0.000 description 6
- 230000006870 function Effects 0.000 description 6
- 230000001939 inductive effect Effects 0.000 description 6
- 230000001149 cognitive effect Effects 0.000 description 3
- 238000013500 data storage Methods 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 241000219112 Cucumis Species 0.000 description 2
- 235000015510 Cucumis melo subsp melo Nutrition 0.000 description 2
- FJJCIZWZNKZHII-UHFFFAOYSA-N [4,6-bis(cyanoamino)-1,3,5-triazin-2-yl]cyanamide Chemical compound N#CNC1=NC(NC#N)=NC(NC#N)=N1 FJJCIZWZNKZHII-UHFFFAOYSA-N 0.000 description 2
- 150000001875 compounds Chemical class 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 239000000835 fiber Substances 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 230000007774 longterm Effects 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 238000004806 packaging method and process Methods 0.000 description 2
- 230000008569 process Effects 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000012937 correction Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 238000011982 device technology Methods 0.000 description 1
- 230000005057 finger movement Effects 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 238000010079 rubber tapping Methods 0.000 description 1
- 230000011664 signaling Effects 0.000 description 1
- 239000010454 slate Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000001052 transient effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/02—Input arrangements using manually operated switches, e.g. using keyboards or dials
- G06F3/023—Arrangements for converting discrete items of information into a coded form, e.g. arrangements for interpreting keyboard generated codes as alphanumeric codes, operand codes or instruction codes
- G06F3/0233—Character input methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/02—Input arrangements using manually operated switches, e.g. using keyboards or dials
- G06F3/023—Arrangements for converting discrete items of information into a coded form, e.g. arrangements for interpreting keyboard generated codes as alphanumeric codes, operand codes or instruction codes
- G06F3/0233—Character input methods
- G06F3/0236—Character input methods using selection techniques to select from displayed items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
- G06F40/169—Annotation, e.g. comment data or footnotes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/274—Converting codes to words; Guess-ahead of partial word inputs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/12—Use of codes for handling textual entities
- G06F40/163—Handling of whitespace
Definitions
- Some computing devices provide a graphical keyboard for inputting text as part of a graphical user interface.
- a "gesture keyboard” is one example of a graphical keyboard that can enable a computing device to determine a string of text based on a continuous gesture input.
- the continuous gesture input may include a single, continuous gesture spanning locations of multiple keys of the gesture keyboard rather than multiple, individual tapgesture inputs entered at each of the individual locations of the multiple keys.
- a gesture keyboard may be preferable to users who prefer to swipe across-rather than merely tap on- individual keys of a graphical keyboard.
- Some computing devices have adopted a one-gesture-for-one-word model for gesture keyboards. That is, these computing devices map a single, continuous gesture to only a single word, and any interruption of the gesture (e.g., lifting a finger from the gesture keyboard) may be interpreted by the computing device as signaling the completion of the entered word.
- This approach may have certain drawbacks, however. For example, a gesture to select a relatively long word with many characters (e.g., "supermarket”) would require continuous contact with the gesture keyboard for the entirety of the lengthy word, potentially resulting in user fatigue and increased cognitive difficulty that can aggravate input errors, decrease input accuracy, and slow the speed of input using the gesture keyboard.
- US 2014/098023 A1 presents “incremental multi-touch gesture recognition.”
- United States patent application publication no. US 2010/241984 A1 presents “an apparatus, comprising a shorthand-aided rapid keyboarding enabled touchscreen” which is “configured to receive a user input.”
- United States patent application publication no. US 2010/238125 A1 presents “various methods for discontinuous shapewriting”.
- United States patent application publication no. US 2003/165801 A1 presents "a method of interpreting keypad input”.
- the disclosure is directed to a method that includes outputting, by a computing device and for display, a graphical keyboard comprising a plurality of keys.
- the method further includes receiving, by the computing device, an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys, and receiving, by the computing device, an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys.
- the method further includes, determining, by the computing device and based on pressure information associated with the indication of the second gesture, a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys, and outputting, by the computing device and for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- a computing device may include at least one processor, and at least one module operable by the at least one processor to output, for display, a graphical keyboard comprising a plurality of keys.
- the at least one module may be operable to receive an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys, and receive an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys.
- the module may be further operable to determine, based on pressure information associated with the indication of the second gesture, a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys, and output, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- a computer-readable storage medium is encoded with instructions that, when executed, cause at least one processor of a computing device to output, for display, a graphical keyboard comprising a plurality of keys.
- the instructions may cause the processor to receive an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys, and receive an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys.
- the instructions may further cause the processor to determine, based on pressure information associated with the indication of the second gesture, a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys, and output, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- this disclosure is directed to techniques for determining words based at least in part on an amount of pressure associated with a portion of a gesture input associated with one or more graphical keys of a gesture keyboard that is output by a computing device for display.
- An example computing device may initially output a gesture keyboard for display at a display device.
- the example computing device may detect a gesture made by an input object (e.g., a finger or stylus) at locations on a presence-sensitive device that correspond to locations of keys of the gesture keyboard that are displayed by the display device.
- an input object e.g., a finger or stylus
- the example computing device may determine a selection of one or more of the keys and then use this selection of keys and pressure information associated with the gesture input to determine one or more words from a lexicon that correspond to the gesture input. That is, unlike other computing devices that have adopted a one-gesture-for-one-word model (e.g., always mapping a single, continuous gesture spanning locations of multiple keys of the gesture keyboard to only a single word with an interruption of the gesture input denoting an end to the desired word), the example computing device may, before mapping a selection of keys to a word in a lexicon, determine whether the selection of keys corresponds to an input for entering either a single word or multiple words by using pressure information obtained about the gesture input (e.g., from the presence-sensitive input device).
- a one-gesture-for-one-word model e.g., always mapping a single, continuous gesture spanning locations of multiple keys of the gesture keyboard to only a single word with an interruption of the gesture input denoting an end to the desired word
- the example computing device may infer whether a single gesture is an input for a single word or for multiple words based on pressure information, regardless of whether the selection of keys includes a selection of a space bar key or other end-of-word identifier. For example, the example computing device may determine that a given gesture input is intended to input two words by identifying the start of the second of two words based on a determination that a middle portion of the gesture was entered with a "high" pressure (e.g., a pressure that exceeds a particular pressure threshold) as opposed to a "normal” or “low” pressure (e.g., a pressure that does not exceed the particular pressure threshold).
- a "high" pressure e.g., a pressure that exceeds a particular pressure threshold
- a "normal” or "low” pressure e.g., a pressure that does not exceed the particular pressure threshold
- the example computing device may use pressure information obtained about a gesture input to determine whether a single gesture is intended to enter a new word or to continue entry of a previous word determined from a previous gesture input. For example, the example computing device may determine that a new word is being entered when the computing device determines that a "high" pressure (e.g., a pressure that exceeds a particular pressure threshold) is associated with a beginning portion of a gesture as opposed to a "normal” or "low” pressure (e.g., pressure that does not exceed the particular pressure threshold), in which case the computing device may treat the gesture input as a continuation of a previous gesture.
- a "high" pressure e.g., a pressure that exceeds a particular pressure threshold
- the techniques described herein may enable a computing device to utilize pressure to determine whether a continuous gesture input at a gesture keyboard corresponds to a single or multiple word inputs in addition to discerning whether a gesture input is for entering the start of a new word or whether the gesture input is for continuing where a previous gesture left off in entering a previous word.
- Techniques of this disclosure may provide one or more advantages.
- the physical and cognitive difficulty of entering long words using single, continuous gestures can cause errors in text entry, which are exacerbated by requiring additional inputs and time to correct the text entries.
- techniques of this disclosure may improve the overall input speed and accuracy of the gesture keyboard by reducing the likelihood of erroneous text entry, particular for long words and may thus provide an improved user interface.
- FIG. 1 is a conceptual diagram illustrating an example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure.
- computing device 100 may be a mobile phone.
- computing device 100 may be a tablet computer, a personal digital assistant (PDA), a laptop computer, a portable gaming device, a portable media player, an e-book reader, a computerized watch, television platform, or another type of computing device.
- PDA personal digital assistant
- computing device 100 includes a user interface device (UID) 102.
- UID 102 of computing device 100 may function as an input device for computing device 100 and as an output device.
- UID 102 may be implemented using various technologies. For instance, UID 102 may function as an input device using a presence-sensitive input device, such as a resistive touchscreen, a surface acoustic wave touchscreen, a capacitive touchscreen, a projective capacitance touchscreen, a pressure sensitive screen, an acoustic pulse recognition touchscreen, or another presence-sensitive input device technology.
- a presence-sensitive input device such as a resistive touchscreen, a surface acoustic wave touchscreen, a capacitive touchscreen, a projective capacitance touchscreen, a pressure sensitive screen, an acoustic pulse recognition touchscreen, or another presence-sensitive input device technology.
- UID 102 may function as an output device using any one or more of a liquid crystal display (LCD), dot matrix display, light emitting diode (LED) display, organic lightemitting diode (OLED) display, e-ink, or similar monochrome or color display capable of outputting visible information to the user of computing device 100.
- LCD liquid crystal display
- LED light emitting diode
- OLED organic lightemitting diode
- e-ink e-ink
- monochrome or color display capable of outputting visible information to the user of computing device 100.
- UID 102 of computing device 100 may include a presence-sensitive screen (e.g., presence-sensitive display) that may receive tactile user input from a user of computing device 100.
- UID 102 may receive indications of the tactile user input by detecting one or more tap and/or non-tap gestures from a user of computing device 100 (e.g., the user touching or pointing to one or more locations of UID 102 with a finger or a stylus pen).
- the presence-sensitive screen of UID 102 may present output to a user.
- UID 102 may present the output as a user interface (e.g., user interface 104) which may be related to functionality provided by computing device 100.
- UID 102 may present various user interfaces of applications (e.g., an electronic message application, an Internet browser application, etc.) executing at computing device 100.
- applications e.g., an electronic message application, an Internet browser application, etc.
- a user of computing device 100 may interact with one or more of these applications to perform a function with computing device 100 through the respective user interface of each application.
- Computing device 100 may include user interface ("UI") module 120, and a keyboard module 122.
- UI user interface
- Modules 120 and 122 may perform operations described using software, hardware, firmware, or a mixture of both hardware, software, and firmware residing in and executing on computing device 100.
- Computing device 100 may execute modules 120 and 122 with multiple processors.
- Computing device 100 may execute modules 120 and 122 as a virtual machine executing on underlying hardware.
- UI module 120 of computing device 100 may receive from UID 102, one or more indications of user input detected at UID 102. Generally, each time UID 102 receives an indication of user input detected at a location of UID 102, UI module 120 may receive information about the user input from UID 102.
- UID 102 may virtually overlay a grid of coordinates onto UID 102.
- the grid may not be visibly displayed by UID 102.
- the grid may assign a coordinate that includes a horizontal component (X) and a vertical component (Y) to each location.
- UI module 120 may receive information from UID 102.
- the information may include one or more coordinate locations and associated times indicating to UI module 120 both, where UID 102 detects the gesture input at UID 102, and when UID 102 detects the gesture input.
- the information may further include pressure information indicating an amount of pressure or force applied by the user on the UID 102.
- UID 102 may determine the amount of pressure applied by a gesture input using capacitive, inductive, or other pressure sensing techniques.
- the pressure information may be in the form of numerical, quantified data points (e.g., "0.73 units of force") or discrete levels (e.g., "normal", "hard”).
- the pressure information may be represented as a depth component (Z) that are part of the grid coordinates (e.g., X, Y) described above.
- each touch event in the sequence may include data or components that represents parameters for characterizing a presence, an amount of pressure associated with, and/or movement (e.g., when, where, originating direction) of, input at UID 102.
- Each touch event in the sequence may include a location component (e.g., a x,y coordinate of a location at UID 102), a time component related to when UID 102 detected user input at the location, a pressure component indicating an amount of force detected by UID 102 when detecting the input at the location, and an action component related to whether the touch event corresponds to a lift up or a push down at the location.
- UI module 120 may determine one or more other characteristics of the user input and include information about these one or more characteristics within each touch event in the sequence of touch events. For example, UI module 120 may determine a start location of the user input, an end location of the user input, a density of a portion of the user input, a speed of a portion of the user input, a direction of a portion of the user input, and a curvature of a portion of the user input.
- One or more touch events in the sequence of touch events may include (in addition to a time, a location, and an action component as described above) a characteristic component that includes information about one or more characteristics of the user input (e.g., a density, a speed, etc.).
- UI module 120 may cause UID 102 to present user interface 104.
- User interface 104 includes graphical elements displayed at various locations of UID 102.
- FIG. 1 illustrates edit region 106A and gesture keyboard 106B of user interface 104.
- Gesture keyboard 106B is a graphical keyboard having selectable, graphical elements displayed as keys for typing text at edit region 106A.
- Edit region 106A may include graphical elements such as images, objects, hyperlinks, characters of text (e.g., character strings) etc., that computing device 100 generates in response to input associated with gesture keyboard 106B.
- edit region 106A is associated with a messaging application, a word processing application, an internet webpage browser application, or other text entry field of an application, operating system, or platform executing at computing device 100.
- edit region 106A represents a final destination of the letters that a user of computing device 100 is selecting using gesture keyboard 106B and is not an intermediary region associated with gesture keyboard 106B, such as word suggestion or autocorrect region that displays one or more complete word suggestions or auto-corrections.
- FIG. 1 shows the letters w-a-t-e-r-m-e-l-o-n within edit region 106A.
- the letters w- a-t-e-r-m-e-l-o-n make up a string of characters or candidate word 130.
- Candidate word 130 is delineated by dashed circles in the example of FIG. 1 , however UI device 102 may or may not output such dashed circles in some examples.
- Candidate word 130 may be a portion of a word and/or a word or portion of a word included in a dictionary.
- FIG. 1 also shows a sequence of paths 132 and 134 presented as dashed and solid line segments, respectively, spanning locations of keys of gesture keyboard 106B.
- UI module 120 may cause UID 102 to present paths 132, 134 based on information received from UI module 120 about the locations of a gesture detected by UID 102. In other words, UI module 120 may cause UID 102 to display path 132, 134 at UID 102 such that, when paths 132, 134 are presented at UID 102, paths 132, 134 is visible to a user. In some examples, rather than display only one current path at a time, UI module 120 may cause UID 102 to display path 134 at the same time as a (previous) path 132. Paths 132, 134 may be rendered differently to represent which path is a previous gesture input, e.g., with dashed or light-colored lines.
- a user of computing device 100 may cause UI module 120 to enter text in edit region 106A by providing input (e.g., tap and/or non-tap gestures) at locations of UID 102 that display the keys of gesture keyboard 106B.
- computing device 100 may output one or more characters, strings, or multi-string phrases within edit region 106A, such as candidate word 130.
- a word may generally be described as a string of one or more characters in a dictionary or lexicon (e.g., a set of strings with semantic meaning in a written or spoken language), a "word" may, in some examples, refer to any group of one or more characters.
- a word may be an out-of-vocabulary word or a string of characters not contained within a dictionary or lexicon but otherwise used in a written vocabulary to convey information from one person to another.
- a word may include a name, a place, slang, or any other out-of-vocabulary word or uniquely formatted strings, etc., that includes a first portion of one or more characters followed by a second portion of one or more characters.
- UI module 120 may act as an intermediary between various components of computing device 100 to make determinations based on input detected by UID 102 and generate output presented by UID 102. For instance, UI module 120 may receive, as an input from keyboard module 122 a representation of a keyboard layout of the keys included in gesture keyboard 106B. In some examples, UI module 120 may determine that the one or more location components in the sequence of touch events approximate input at gesture keyboard 106B (e.g., UI module 120 may determine the location of one or more of the touch events corresponds to an area of UID 102 that presents gesture keyboard 106B). UI module 120 may transmit, as output to keyboard module 122, the sequence of touch events along with locations where UID 102 presents each of the keys.
- UI module 120 may receive a candidate word (e.g., a string of text representing candidate word 130) that keyboard module 122 determined from the sequence of touch events.
- UI module 120 may update user interface 104 to include candidate word 130 received from keyboard module 122 within edit region 106A.
- UI module 120 may cause UID 102 to present the updated user interface 104 including candidate word 130 presented as graphical elements (e.g., characters) in edit region 106A.
- Keyboard module 122 of computing device 100 may transmit, as output to UI module 120 (for inclusion as gesture keyboard 106B of user interface 104) a keyboard layout including a plurality of keys related to one or more written languages (e.g., English, Spanish, French, etc.). Keyboard module 122 may assign one or more characters or operations to each key of the plurality of keys in the keyboard layout. For instance, keyboard module 122 may generate a QWERTY keyboard layout including keys that represent characters used in typing the English language. The QWERTY keyboard layout may also include keys that represent operations used in typing the English language (e.g., backspace, delete, spacebar, enter, etc.).
- Keyboard module 122 may receive data from UI module 120 that represents the sequence of touch events generated by UI module 120 as well as the locations of UID 102 where UID 102 presents each of the keys of gesture keyboard 106B. Keyboard module 122 may determine, based on the locations of the keys and the sequence of touch events, a selection of one or more keys. Keyboard module 122 may determine a character string based on the selection where each character in the character string corresponds to at least one key in the selection. Keyboard module 122 may send data indicating the character string to UI module 120 for inclusion as candidate word 130 in edit region 106A of user interface 104.
- Keyboard module 122 may rely on various models and algorithms (e.g., a spatial model, a language model, etc.) to determine whether or not a sequence of touch events represents a selection of one or more keys.
- keyboard module 122 may include spatial and language models to determine with a certain degree of certainty, which keys of gesture keyboard 106B are being selected and which character string is being entered with the selection.
- keyboard module 122 may compare the location components (e.g., coordinates) of one or more touch events in the sequence of touch events to respective locations of one or more keys of gesture keyboard 106B and generate a probability based on these comparisons that a selection of a key occurred. For example, keyboard module 122 may compare the location component of each touch event in the sequence of touch events to a key location of a particular key of gesture keyboard 106B. The location component of each touch event in the sequence may include one location of UID 102 and a key location (e.g., a centroid of a key) of a key in gesture keyboard 106B may include a different location of UID 102.
- location components e.g., coordinates
- Keyboard module 122 may determine a Euclidian distance between the two locations and generate a probability based on the Euclidian distance that the key was selected. Keyboard module 122 may correlate a higher probability to a key that shares a smaller Euclidian distance with one or more touch events than a key that shares a greater Euclidian distance with one or more touch events. Based on the spatial model probability associated with each key, keyboard module 122 may assemble the individual key selections with the highest spatial model probabilities into a time-ordered sequence of keys that keyboard module 122 may then determine represents a character string.
- keyboard module 122 may modify the output from the spatial model using the pressure components of one or more touch events in the sequence of touch events. Keyboard module 122 may compare the pressure component of each touch event in the sequence of touch events to a pressure threshold. Based on the comparison, keyboard module 122 may annotate the time-ordered sequence of keys with metadata indicating whether the sequence of keys included a "hard" pressure (e.g., exceeding the pressure threshold). For example, keyboard module 122 may analyze the pressure component of the one or more touch events associated with the start location of the user input to determine whether the touch input began with a "hard" pressure (e.g., exceeding the pressure threshold).
- keyboard module 122 may determine whether the input sequence of keys ended with a "hard" pressure (e.g., exceeding the pressure threshold) based on the pressure component in touch events associated with the end location of the user input. In some examples, rather than associate the pressure information with the gesture as a whole, keyboard module 122 may correlate the pressure information with specific keys in the sequence of keys. Keyboard module 122 may use both the pressure and location components of the touch events to associate pressure information with individual key presses, e.g., a hard 'W' press.
- keyboard module 122 may use pressure data to determine which gesture inputs on gesture keyboard 106B should be included in a same candidate word 130 or in separate candidate words. In other words, rather than assume each distinct gesture is a new candidate word, keyboard module 122 may continue to build a candidate word with each subsequent gesture on top of previous gestures. In such cases, keyboard module 122 incorporates the selection of keys from a current gesture with the selection of keys from prior gestures. Upon detecting a gesture contains a "hard" press, keyboard module 122 may complete its building of the candidate word, and begin a new word. By enabling computing device 100 to combine non-continuous gestures to form a word, keyboard module 122 may provide a way for a user of computing device 100 to enter text more accurately and/or with less effort.
- computing device 100 may output, for display, a graphical keyboard comprising a plurality of keys.
- keyboard module 122 may generate data that includes a representation of gesture keyboard 106B.
- UI module 120 may generate user interface 104 and include gesture keyboard 106B in user interface 104 based on the data representing gesture keyboard 106B.
- UI module 120 may send information to UID 102 that includes instructions for displaying user interface 104 at UID 102.
- UID 102 may receive the information and cause UID 102 to present user interface 104 including edit region 106A and gesture keyboard 106B.
- Gesture keyboard 106B may include a plurality of keys.
- Computing device 100 may receive an indication of a gesture selecting a first sequence of keys from the plurality of keys. For example, a user of computing device 100 may provide a continuous gesture input that traverses across path 132 beginning at a first location 136A and ending at a second location 136B of UID 102 such that the gesture performed from the first location to the second location is detected by UID 102 throughout the performance of the gesture.
- the gesture may traverse across a sequence of keys that includes the 'W' key, the 'A' key, the ⁇ S' key, the 'D' key, the 'R' key, the 'T' key, the 'R' key, the 'E' key, and the 'R' key.
- the gesture may begin with a greater amount of force (e.g., a hard force or force that exceeds a pressure threshold) (represented in FIG. 1 by concentric circles) applied at location 136A of UID 102 which is nearest to the location of the 'W' key and end with a lesser amount of force (e.g., a light force or force that does not exceed the pressure threshold) at location 136B proximate to the location of the 'R' key.
- a greater amount of force e.g., a hard force or force that exceeds a pressure threshold
- a lesser amount of force e.g., a light force or force that does not exceed the pressure threshold
- UI module 120 may generate a sequence of touch events based on the gesture.
- UI module 120 may cause UID 102 to present a graphical indication of path 132 based on information received from UI module 120 about the locations of the gesture detected by UID 102.
- Keyboard module 122 may rely on pressure information associated with the sequence of touch events generated by UI module 120 as well as a spatial model, language model, and/or other technique to determine which keys are most likely being selected. For example, keyboard module 122 may determine whether the received indication of the gesture satisfies a pressure threshold based on pressure information from touch events associated with the start location of the gesture. In the example shown in FIG.
- keyboard module 122 may determine that pressure information for the touch events having locations proximate to the "W" key (e.g., location 136A) and during the start of the gesture exceeds a pressure threshold. Based on the pressure information, keyboard module 122 may initiate a new word instead of combining the gesture with previous input. In other words, keyboard module 122 may complete the word prediction of previous inputs, insert a word delimiter (e.g., space), and begins word prediction anew on the received gesture. In some examples, keyboard module 122 may analyze the pressure information after receiving some but not all of the sequence of touch events associated with a gesture input. In such a way, keyboard module 122 may incrementally determine whether the received indication of the gesture satisfies a pressure threshold without having to wait until the end of the gesture.
- a word delimiter e.g., space
- computing device 100 may further receive an indication of a second gesture selecting a second sequence of keys from the plurality of keys.
- Keyboard module 122 may characterize the second gesture, which was received subsequent to the first gesture, as separate and distinct from the first gesture based on action components in the sequence of touch events associated with the first and second gestures. For example, keyboard module 122 may receive a touch event indicating a lift off action by an input object (e.g., user's finger) and a subsequent touch event indicating a push down action by the input object.
- an input object e.g., user's finger
- a user of computing device 100 may provide a gesture input that traverses across path 134, which traverses across a sequence of keys that includes the 'M' key, the 'E' key, the 'L' key, the 'O' key, and the 'N' key (i.e., M-N-J-H-G-F-E-R-T-Y-J-K-L-O-K-J-N).
- the second gesture may begin at location 138A of UID 102 (which, notably, does not include concentric circles in FIG. 1 to indicate that the gesture does not include the application of a hard force) which is nearest to the location of the 'M' key and ends at location 138B of UID 102 which is nearest to the location of the 'N' key.
- keyboard module 122 may determine whether the indication of the second gesture satisfies a pressure threshold based on pressure information from touch events associated with the start location of the second gesture. For example, keyboard module 122 may determine that pressure information for the touch events having locations proximate to the "M" key (e.g., location 138A) and during the start of the second gesture does not exceed the pressure threshold.
- a pressure threshold based on pressure information from touch events associated with the start location of the second gesture. For example, keyboard module 122 may determine that pressure information for the touch events having locations proximate to the "M" key (e.g., location 138A) and during the start of the second gesture does not exceed the pressure threshold.
- keyboard module 122 may perform word prediction to determine a candidate word 130 that includes a combination of the first sequence of keys as well as the second sequence of keys. For example, rather than outputting a candidate word "water" upon receipt of the second gesture, keyboard module 122 may combine the second sequence of keys which includes, among other keys, the 'M' key, the 'E' key, the 'L' key, the 'O' key, and the 'N' key, with the first sequence of keys which includes the 'W' key, the 'A' key, the 'D' key, the ⁇ R' key, the 'T' key, the ⁇ R' key, the 'E' key, and the ⁇ R' key in the determination of the candidate word "watermelon.”
- Keyboard module 122 may perform word prediction to determine a candidate word 130 using the combination of the first gesture and the second gesture even though the gestures are separate and non-continuous. Keyboard module 122 outputs candidate word 130 that includes the combination of the first sequence of keys and the second sequence of keys. For example, keyboard module 122 causes UID 102 to display the candidate word "watermelon" in edit region 106A
- keyboard module 122 may complete the word prediction based on the first gesture that includes the first sequence of keys (and not based on the second gesture having the second sequence of keys). In other words, keyboard module 122 refrains from outputting a candidate word that combines the first gesture and the second gesture, and instead outputs a candidate word "water” that include the selection of keys from the first gesture only. Keyboard module 122 may also initiate prediction of a new word based on the second gesture to determine the candidate word "melon.”
- a "hard” pressure e.g., exceeding a pressure threshold
- Techniques of the present disclosure may improve the accuracy with which a user can enter text into a computing device.
- a user may, instead of having to perform a single, continuous gesture for a word, perform multiple gestures that indicate the word.
- Such techniques are particularly helpful to users for entering long words containing many characters, such as compound words.
- the described technique allows the user to rest the input finger in the middle of gesturing a long word, which reduces user fatigue and the likelihood of errors.
- the present techniques further reduce the likelihood of input errors by decreasing the cognitive difficulty of entering long words by breaking the long word in chunks.
- techniques of this disclosure may improve the overall input speed and accuracy of the gesture keyboard by reducing the likelihood of erroneous text entry.
- FIG. 2 is a block diagram illustrating an example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure.
- Computing device 100 of FIG. 2 is described below within the context of FIG. 1 .
- FIG. 2 illustrates only one particular example of computing device 100, and many other examples of computing device 100 may be used in other instances and may include a subset of the components included in example computing device 100 or may include additional components not shown in FIG. 2 .
- computing device 100 includes user interface device 102 ("UID 102"), one or more processors 202, one or more input devices 204, one or more communication units 206, one or more output devices 208, and one or more storage devices 210.
- Storage devices 210 of computing device 100 also include UI module 120 and keyboard module 122.
- UI module 120 includes a pressure module 222.
- Keyboard module 122 includes a spatial-language model module 220 (“SLM module 220").
- Communication channels 212 may interconnect each of the components 102, 120, 122, 202, 204, 206, 208, 210, 220, and 222 for inter-component communications (physically, communicatively, and/or operatively).
- communication channels 212 may include a system bus, a network connection, an inter-process communication data structure, or any other method for communicating data.
- One or more input devices 204 of computing device 100 may receive input. Examples of input are tactile, audio, and video input.
- Input devices 204 of computing device 100 includes a presence-sensitive input device (e.g., a touch sensitive screen, a presence-sensitive display), mouse, keyboard, voice responsive system, video camera, microphone or any other type of device for detecting input from a human or machine.
- One or more output devices 208 of computing device 100 may generate output. Examples of output are tactile, audio, and video output.
- Output devices 208 of computing device 100 includes a presence-sensitive display, sound card, video graphics adapter card, speaker, cathode ray tube (CRT) monitor, liquid crystal display (LCD), or any other type of device for generating output to a human or machine.
- CTR cathode ray tube
- LCD liquid crystal display
- One or more communication units 206 of computing device 100 may communicate with external devices via one or more networks by transmitting and/or receiving network signals on the one or more networks.
- computing device 100 may use communication unit 206 to transmit and/or receive radio signals on a radio network such as a cellular radio network.
- communication units 206 may transmit and/or receive satellite signals on a satellite network such as a GPS network.
- Examples of communication unit 206 include a network interface card (e.g. such as an Ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device that can send and/or receive information.
- Other examples of communication units 206 may include Bluetooth ® , GPS, 3G, 4G, LTE, and Wi-Fi ® radios found in mobile devices as well as Universal Serial Bus (USB) controllers.
- USB Universal Serial Bus
- UID 102 of computing device 100 may include functionality of input devices 204 and/or output devices 208.
- UID 102 may be or may include a presence-sensitive input device.
- a presence-sensitive input device may detect an object at and/or near the presence-sensitive input device.
- a presence-sensitive input device may detect an object, such as a finger or stylus that is within two inches or less of the presence-sensitive input device.
- the presence-sensitive input device may determine a location (e.g., an [x,y] coordinate) of the presence-sensitive input device at which the object was detected.
- a presence-sensitive input device may detect an object six inches or less from the presence-sensitive input device and other ranges are also possible.
- the presence-sensitive input device may determine the location of the input device selected by a user's finger using capacitive, inductive, and/or optical recognition techniques.
- presence-sensitive input device provides output to a user using tactile, audio, or video stimuli as described with respect to output device 208.
- UID 102 presents a user interface (such as user interface 104 of FIG. 1 ) at UID 102.
- UID 102 While illustrated as an internal component of computing device 100, UID 102 also represents an external component that shares a data path with computing device 100 for transmitting and/or receiving input and output.
- UID 102 represents a built-in component of computing device 100 located within and physically connected to the external packaging of computing device 100 (e.g., a screen on a mobile phone).
- UID 102 represents an external component of computing device 100 located outside and physically separated from the packaging of computing device 100 (e.g., a monitor, a projector, etc. that shares a wired and/or wireless data path with a tablet computer).
- One or more storage devices 210 within computing device 100 may store information for processing during operation of computing device 100.
- storage device 210 is a temporary memory, meaning that a primary purpose of storage device 210 is not long-term storage.
- Storage devices 210 on computing device 100 may be configured for short-term storage of information as volatile memory and therefore not retain stored contents if powered off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art.
- RAM random access memories
- DRAM dynamic random access memories
- SRAM static random access memories
- Storage devices 210 also include one or more computer-readable storage media.
- Storage devices 210 may be configured to store larger amounts of information than volatile memory.
- Storage devices 210 may further be configured for long-term storage of information as non-volatile memory space and retain information after power on/off cycles. Examples of non-volatile memories include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.
- Storage devices 210 may store program instructions and/or data associated with UI module 120, keyboard module 122, pressure module 222, and SLM module 220.
- processors 202 may implement functionality and/or execute instructions within computing device 100.
- processors 202 on computing device 100 may receive and execute instructions stored by storage devices 210 that execute the functionality of UI module 120, keyboard module 122, pressure module 222, and SLM module 220. These instructions executed by processors 202 may cause computing device 100 to store information, within storage devices 210 during program execution.
- Processors 202 may execute instructions of modules 120, 122, 220, and 222 to cause UID 102 to display user interface 104 at UID 102. That is, modules 120, 122, 220, and 222 may be operable by processors 202 to perform various actions, including receiving an indication of a gesture at locations of UID 102 and causing UID 102 to present user interface 104 at UID 102.
- computing device 100 of FIG. 2 may output for display at UID 102 a graphical keyboard comprising a plurality of keys.
- keyboard module 122 may cause UI module 120 of computing device 100 to output a keyboard layout (e.g., an English language QWERTY keyboard, etc.) for display at UID 102.
- UI module 120 may receive data specifying the keyboard layout from keyboard module 122 over communication channels 212.
- UI module 120 may use the data to generate user interface 104 including edit region 106A and the plurality of keys of the keyboard layout from keyboard module 122 as gesture keyboard 106B.
- UI module 120 may transmit data over communication channels 212 to cause UID 102 to present user interface 104 at UID 102.
- UID 102 may receive the data from UI module 120 and cause UID 102 to present user interface 104.
- Computing device 100 may receive an indication of a first gesture that traverses a first group of keys of the plurality of keys and an indication of a pressure (e.g., force) associated with the first gesture.
- computing device 100 receives a first gesture input (e.g., from a user) that traverses across path 132 associated with the first group of keys and that includes an applied pressure applied to UID 102 during an initial portion of the gesture.
- the initial portion of the gesture input corresponds to location 136A of UID 102 which is nearest to the location of the 'W' key.
- the gesture input further includes a traversal of a path associated with a group of keys (e.g., the 'A' key, 'S' key, 'D' key, 'R' key, 'T' key, ⁇ R' key, 'E' key, and 'R' key again), ending at location 136B of UID 102 which is nearest to the location of the 'R' key.
- the last touch event in the sequence of touch events associated with the first gesture input may include an indication that the first gesture input has ended. For example, the touch event corresponding to the gesture input ending at location 136B proximate to the "R" key indicates that the continuous gesture input has ended.
- Computing device 100 may receive an indication of a second gesture that traverses a second group of keys of the plurality of keys, and which does not include an indication of a pressure. For example, computing device 100 receives a second gesture input (subsequent to the first gesture input) that traverses across path 134 and that does not include a pressure during an initial portion of the gesture.
- the initial portion of the gesture input begins at location 138A of UID 102 which is nearest to the location of the 'M' key, and continues to a traversal of a second group of keys (e.g., M-N-J-H-G-F-E-R-T-Y-J-K-L-O-K-J-N), ending at location 138B of UID 102 which is nearest to the location of the 'N' key.
- a second group of keys e.g., M-N-J-H-G-F-E-R-T-Y-J-K-L-O-K-J-N
- UI module 120 may generate sequences of touch events based on the first and second gestures. Pressure module 222 of UI module 120 may incorporate pressure information into the sequences of touch events based on the gestures. Pressure module 222 may modify the touch event, in the sequence of touch events, that corresponds to the initial portion of the first gesture. For example, pressure module 222 modifies the touch event associated with the traversal of the "W" key (i.e., during the initial portion of the gesture) to include pressure information.
- keyboard module 122 may determine a candidate word from a selection of keys determined from touch events of multiple gesture inputs, instead of only a single gesture input. Keyboard module 122 may use multiple sequences of touch events associated with multiple gesture inputs in determining a candidate word from the selection of keys. For example, keyboard module 122 may combine the selection of keys from the first gesture input and the second gesture input based on a pressure indication (or absence thereof) in the second sequence of touch events.
- keyboard module 122 uses the first sequence of touch events (and its associated selection of keys) separately from the second sequence of touch events (and its associated selection of keys) to determine a candidate word. Keyboard module 122 may continue to combine selections of keys from sequences of touch events from multiple gesture inputs until keyboard module 122 encounters a sequence of touch events from a new gesture input that indicates a pressure associated with the initial portion of the new gesture input. Put another way, keyboard module 122 assumes that each incoming selection of keys should be joined with the previous selection of keys until keyboard module 122 detects pressure information in the next gesture input.
- keyboard module 122 may be configured to join each new selection of keys with the previous selection of keys in response to the presence of pressure information in the new gesture input. For example, keyboard module 122 may determine a candidate word based on a selection of keys from a single continuous gesture input, and also determine a candidate word based on a selection of keys from multiple gesture inputs, each having a pressure indication associated with an initial portion of each subsequent gesture input.
- SLM module 220 of keyboard module 122 may determine the one or more candidate words from one or more lexicon data stores that have a highest probability of being the candidate words that a user may wish to enter by providing input at gesture keyboard 106B.
- the probability may indicate a frequency of use of each candidate word in a language context. That is, SLM module 220 may determine that one or more candidate words that have a greatest likelihood of being the word that a user may wish to enter at edit region 106A are the one or more candidate words that appear most often during an instance of written and/or spoken communication using a particular language.
- Keyboard module 122 may utilize a language module of SLM module 220 to refine the selection of keys and/or to determine at least one candidate word from the selection of keys determined from the locations in multiple gestures.
- SLM module 220 of keyboard module 122 may perform a look up within one or more lexicon data stores to identify one or more candidate words stored at the one or more lexicon data stores that begin with the letter "w " associated with the first key in the first sequence, end with the letter "n” associated with the last key in the second sequence, and that may or may not include any of the letters W-AS-D-R-T-R-E-R-M-N-J-H-G-F-E-R-T-Y-J-K-L-O-K-J-N that correspond to each of the keys in the sequence in-between the first and last keys.
- SLM module 220 may identify the candidate words watermelon, watermelons, and watermen as some example candidate words in the one or more lexicon data stores that begin and end with the letters w-n.
- Keyboard module 122 may rely on a language model and/or access a lexicon of computing device 100 to autocorrect (e.g., spellcheck) a character string generated from a sequence of key selections before and/or after outputting the character string to UI module 120 for inclusion within edit region 106A of user interface 104.
- a lexicon of computing device 100 may include a list of words within a written language vocabulary.
- keyboard module 122 may perform a lookup in the lexicon of a character string generated from a selection of keys to identify one or more candidate words that include at least some or all of the characters of the character string generated based on the selection of keys.
- keyboard module 122 may assign a Jaccard similarity coefficient to the one or more candidate words to determine a degree of similarity between a character string inputted by a user and a word in the lexicon.
- a Jaccard similarity coefficient also known as a Jaccard index, represents a measurement of similarity between two sample sets (e.g., a character string and a word in a dictionary). Based on the comparison, keyboard module 122 may generate a Jaccard similarity coefficient for one or more words in the lexicon.
- Each candidate word may include, as a prefix, an alternative arrangement of some or all of the characters in the character string.
- each candidate word may include as the first letters of the word, the letters of the character string determined from the selection of keys.
- keyboard module 122 may determine that a candidate word of the lexicon with a greatest Jaccard similarity coefficient to w-a-t-e is water.
- Keyboard module 122 may output the autocorrected character string water to UI module 120 for inclusion in edit region 106A rather than the actual character string w-a-t-e indicated by the selection of keys.
- Keyboard module 122 may cause UI module 120 to present candidate word 130, determined from the combined selection of keys, at edit region 106A.
- keyboard module 122 may send information via communication channel 212 that causes UI module 120 to cause UID 102 to present candidate word 130 as a string of text at edit region 106A.
- keyboard module 122 may cause UI module 120 to graphically annotate candidate word 130 to indicate which portions of candidate word are determined from separate gesture inputs. For example, to present the candidate word "watermelon" determined from multiple gesture inputs as described above, UI module 120 may highlight the portion "water” with a first color, and highlight the portion "melon" with a second color different from the first color.
- keyboard module 122 may iteratively update the candidate word based on each subsequent gesture input and cause UI module 120 to update the candidate word displayed in edit region 106A. For example, in a first iteration, keyboard module 122 may determine a character string "water” based on the first gesture input corresponding to path 132 and cause UI module 120 to display the character string "water” in edit region 106A. Then, in a second, subsequent iteration, keyboard module 122 may determine an updated candidate word "watermelon" based on the second gesture input non-continuous with the first gesture input, corresponding to path 134, and based in conjunction with the first (previous) gesture input. Keyboard module 122 may cause UI module 120 to update the edit region 106A to display the updated candidate word.
- computing device 100 may allow for using multiple gesture inputs (that are non-continuous to each other) to input a word. As a result, computing device 100 may reduce input errors caused by long words or compound words and improve the accuracy with which a user can type a word using gesture keyboard 106B. As such, computing device 100 may receive fewer re-entry inputs from a user to enter text using gesture keyboard, and by receiving fewer re-entry inputs, computing device 100 may perform fewer operations and as such consume less electrical power than other computing devices.
- FIG. 3 is a block diagram illustrating another example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more techniques of the present disclosure.
- the example computing device outputs graphical content for display at a remote device.
- Graphical content generally, may include any visual information that may be output for display, such as text, images, a group of moving images, etc.
- the example shown in FIG. 3 includes a computing device 300, presence-sensitive display 301, communication unit 310, projector 320, projector screen 322, tablet device 326, and visual display device 330. Although shown for purposes of example in FIGS.
- a computing device such as computing device 100 and/or computing device 300, may generally refer to any component or system that includes a processor or other suitable computing environment for executing software instructions and, for example, need not include a presence-sensitive display.
- computing device 300 may be a processor that includes functionality as described with respect to processors 202 in FIG. 2 .
- computing device 300 may be operatively coupled to presence-sensitive display 301 by a communication channel 303A, which may be a system bus or other suitable connection.
- Computing device 300 may also be operatively coupled to communication unit 310, further described below, by a communication channel 303B, which may also be a system bus or other suitable connection.
- a communication channel 303B may also be a system bus or other suitable connection.
- computing device 300 may be operatively coupled to presence-sensitive display 301 and communication unit 310 by any number of one or more communication channels.
- computing device 300 may be a portable or mobile device such as mobile phones (including smart phones), laptop computers, etc.
- computing device 100 may be desktop computers, tablet computers, smart television platforms, cameras, personal digital assistants (PDAs), servers, mainframes, etc.
- PDAs personal digital assistants
- Presence-sensitive display 301 may include display device 303 and presence-sensitive input device 305.
- Display device 303 may, for example, receive data from computing device 300 and display the graphical content.
- presence-sensitive input device 302 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at presence-sensitive display 301 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input to computing device 300 using communication channel 303A.
- presence-sensitive input device 302 may be physically positioned on top of display device 303 such that, when a user positions an input unit over a graphical element displayed by display device 303, the location at which presence-sensitive input device 305 corresponds to the location of display device 303 at which the graphical element is displayed.
- computing device 300 may also include and/or be operatively coupled with communication unit 310.
- Communication unit 310 may include functionality of communication unit 206 as described in FIG. 2 .
- Examples of communication unit 310 may include a network interface card, an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information.
- Other examples of such communication units may include Bluetooth, 3G, 4G, LTE, and Wi-Fi radios, Universal Serial Bus (USB) interfaces, etc.
- Computing device 300 may also include and/or be operatively coupled with one or more other devices, e.g., input devices, output devices, memory, storage devices, etc. that are not shown in FIG. 3 for purposes of brevity and illustration.
- FIG. 3 also illustrates a projector 320 and projector screen 322.
- projection devices may include electronic whiteboards, holographic display devices, and any other suitable devices for displaying graphical content.
- Projector 320 and projector screen 322 may include one or more communication units that enable the respective devices to communicate with computing device 300. In some examples, the one or more communication units may enable communication between projector 320 and projector screen 322.
- Projector 320 may receive data from computing device 300 that includes graphical content. Projector 320, in response to receiving the data, may project the graphical content onto projector screen 322.
- projector 320 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen using optical recognition or other suitable techniques and send indications of such user input using one or more communication units to computing device 300.
- user inputs e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.
- Projector screen 322 may include a presence-sensitive display 324.
- Presence-sensitive display 324 may include a subset of functionality or all of the functionality of UI device 102 as described in this disclosure. In some examples, presence-sensitive display 324 may include additional functionality.
- Projector screen 322 e.g., an electronic whiteboard
- Projector screen 322 may receive data from computing device 300 and display the graphical content.
- presence-sensitive display 324 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen 322 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input using one or more communication units to computing device 300.
- FIG. 3 also illustrates tablet device 326 and visual display device 330.
- Tablet device 326 and visual display device 330 may each include computing and connectivity capabilities. Examples of tablet device 326 may include e-reader devices, convertible notebook devices, hybrid slate devices, etc. Examples of visual display device 330 may include televisions, computer monitors, etc.
- tablet device 326 may include a presence-sensitive display 328.
- Visual display device 330 may include a presence-sensitive display 332. Presence-sensitive displays 328, 332 may include a subset of functionality or all of the functionality of UI device 102 as described in this disclosure. In some examples, presence-sensitive displays 328, 332 may include additional functionality.
- presence-sensitive display 332 may receive data from computing device 300 and display the graphical content.
- presence-sensitive display 332 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen using capacitive, inductive, and/or optical recognition techniques and send indications of such user input using one or more communication units to computing device 300.
- user inputs e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.
- Presence-sensitive displays 328, 332 may include pressure-sensitive functionality that enables presence-sensitive display to determine pressure or force information corresponding to a user input object (e.g., finger) contacting presence-sensitive display 328, 332.
- presence-sensitive display 332 may be configured to detect an amount of force applied to presence-sensitive display 332 by a user input object, using capacitive, inductive, or other pressure sensing techniques.
- computing device 300 may output graphical content for display at presence-sensitive display 301 that is coupled to computing device 300 by a system bus or other suitable communication channel.
- Computing device 300 may also output graphical content for display at one or more remote devices, such as projector 320, projector screen 322, tablet device 326, and visual display device 330.
- computing device 300 may execute one or more instructions to generate and/or modify graphical content in accordance with techniques of the present disclosure.
- Computing device 300 may output the data that includes the graphical content to a communication unit of computing device 300, such as communication unit 310.
- Communication unit 310 may send the data to one or more of the remote devices, such as projector 320, projector screen 322, tablet device 326, and/or visual display device 330.
- computing device 300 may output the graphical content for display at one or more of the remote devices.
- one or more of the remote devices may output the graphical content at a presence-sensitive display that is included in and/or operatively coupled to the respective remote devices.
- computing device 300 may not output graphical content at presence-sensitive display 301 that is operatively coupled to computing device 300.
- computing device 300 may output graphical content for display at both a presence-sensitive display 301 that is coupled to computing device 300 by communication channel 303A, and at one or more remote devices.
- the graphical content may be displayed substantially contemporaneously at each respective device. For instance, some delay may be introduced by the communication latency to send the data that includes the graphical content to the remote device.
- graphical content generated by computing device 300 and output for display at presence-sensitive display 301 may be different than graphical content display output for display at one or more remote devices.
- Computing device 300 may send and receive data using any suitable communication techniques.
- computing device 300 may be operatively coupled to external network 304 using network link 312A.
- Each of the remote devices illustrated in FIG. 3 may be operatively coupled to network external network 304 by one of respective network links 312B, 312C, and 312D.
- External network 304 may include network hubs, network switches, network routers, etc., that are operatively inter-coupled thereby providing for the exchange of information between computing device 300 and the remote devices illustrated in FIG. 3 .
- network links 312A-312D may be Ethernet, ATM or other network connections. Such connections may be wireless and/or wired connections.
- computing device 300 may be operatively coupled to one or more of the remote devices included in FIG. 3 using direct device communication 318.
- Direct device communication 318 may include communications through which computing device 300 sends and receives data directly with a remote device, using wired or wireless communication. That is, in some examples of direct device communication 318, data sent by computing device 300 may not be forwarded by one or more additional devices before being received at the remote device, and vice-versa. Examples of direct device communication 318 may include Bluetooth, Near-Field Communication, Universal Serial Bus, Wi-Fi, infrared, etc.
- One or more of the remote devices illustrated in FIG. 3 may be operatively coupled with computing device 300 by communication links 316A-316D.
- communication links 312A-312D may be connections using Bluetooth, Near-Field Communication, Universal Serial Bus, infrared, etc. Such connections may be wireless and/or wired connections.
- computing device 300 may be operatively coupled to visual display device 330 using external network 304.
- Computing device 300 may output a gesture keyboard for display at presence-sensitive display 332.
- computing device 300 may send data that includes a representation of gesture keyboard 306B to communication unit 310.
- Communication unit 310 may send the data that includes the representation of gesture keyboard 306B to visual display device 330 using external network 304.
- Visual display device 330 in response to receiving the data using external network 304, may cause presence-sensitive display 332 to output gesture keyboard 306B comprising a plurality of keys.
- visual display device 330 may send an indication of the gesture to computing device 300 using external network 304.
- Communication unit 310 may receive the indication of the gesture, and send the indication to computing device 300.
- visual display device 330 may send a second indication of the gesture to computing device 300 using external network 304.
- computing device 300 may determine a selection of the keys of graphical keyboard 106B. For example, computing device 100 may determine a combined selection of keys based on the indications of the first and second gestures received via communication unit 110. Computing device 300 may determine a candidate word based on the combined selection of keys from multiple gesture inputs that are non-continuous with each other (i.e., distinct from each other).
- Computing device 300 may output data indicative of pressure information along a path traversed by the first gesture to visual display device 330 using communication unit 310 and external network 314 to cause visual display device to output a graphical indication of the pressure information for display at presence-sensitive display 332.
- display device 330 may cause presence-sensitive display 332 to present a line segment having a bold line weight as a graphical indication of a "forceful" gesture (e.g., having pressure that exceeds a particular pressure threshold), and a standard line weight as a graphical indication of a "normal” gesture (e.g., having pressure that does not exceed a particular pressure threshold).
- display device 330 may cause presence-sensitive display 332 to present a line segment of varying line weight (or color, or other varying graphical qualities, etc.) such that only certain portions of the line segment having a bold line weight to indicate a "forceful" input (e.g., having pressure that exceeds a particular pressure threshold) and other portions of the line segment having a standard line weight to indicate a "normal" gesture (e.g., having pressure that does not exceeds a particular pressure threshold).
- a "forceful" input e.g., having pressure that exceeds a particular pressure threshold
- normal e.g., having pressure that does not exceeds a particular pressure threshold
- display device 330 may cause presence-sensitive display 332 to present one or more circles at an initial portion of a gesture as a graphical indication of gesture having a "forceful” (e.g., having pressure that exceeds a particular pressure threshold) initial contact with the presence-sensitive device.
- a "forceful" e.g., having pressure that exceeds a particular pressure threshold
- FIG. 4 is a conceptual diagram illustrating example graphical user interface for inputting words using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure.
- FIG. 4 is described below in the context of computing device 100 (described above) from FIG. 1 and FIG. 2 .
- FIG. 4 illustrates that computing device 100 may output graphical user interface 400 for display (e.g., at UID 102).
- Graphical user interface 400 includes a gesture keyboard which has a plurality of keys.
- Computing device 100 receives an indication of a gesture selecting a first sequence of keys from the plurality of keys.
- a user of computing device 100 may provide a gesture input that traverses across a path 402 (similar to gesture input on path 132 shown in FIG. 1 ) which includes a "forceful" input (e.g., having pressure that exceeds a certain pressure threshold) at a first location 406.
- computing device 100 further receives an indication of a second gesture selecting a second sequence of keys from the plurality of keys.
- the user of computing device 100 may provide a gesture input that begins at a location 408 and traverse across path 404 to select a second sequence of keys that includes the "I" key, the "U” key, the "Y” key, and the "T” key (i.e., I-U-Y-T).
- the gesture input associated with path 404 includes a "forceful" input (e.g., having pressure that exceeds a certain pressure threshold) during an initial portion of the gesture input, i.e., proximate to location 408.
- UI module 120 may generate a sequence of touch events based on the gesture.
- UI module 120 may cause UID 102 to present a graphical indication of path 404 based on information received from UI module 120 about the locations of the gesture detected by UID 102.
- Keyboard module 122 may rely on pressure information associated with the sequence of touch events generated by UI module 120 as well as a spatial model, language model, and/or other technique to determine which keys are most likely being selected. For example, keyboard module 122 may determine that pressure information for the touch events having locations proximate to the "I" key (e.g., location 408) and during the start of the gesture exceeds a pressure threshold.
- keyboard module 122 may initiate a new word instead of combining the gesture with previous input (such as in the example shown in FIG. 1 ).
- keyboard module 122 may complete the word prediction of the previous input to yield the candidate word "water”, insert a word delimiter (e.g., space), and perform word prediction on the second gesture.
- FIG. 4 shows the letters "w-a-t-e-r” and "i-t” within the edit region of user interface 400. The letters "w-a-t-e-r" and "i-t” make up a string of characters or candidate word 430.
- FIG. 5 is a flowchart illustrating an example operation 500 of the computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure.
- the process of FIG. 5 may be performed by one or more processors of a computing device, such as computing device 100 illustrated in FIG. 1 and FIG. 2 .
- FIG. 5 is described below within the context of computing device 100 of FIG. 1 and FIG. 2 .
- FIG. 5 illustrates that computing device 100 outputs a gesture keyboard comprising a plurality of keys (502).
- UI module 120 of computing device 100 may cause UID 102 to present graphical user interface 104 including a gesture keyboard having a plurality of keys.
- Computing device 100 receives an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys (504). For example, a user of computing device 100 may draw two strokes for the character strings “grand” and “mother” on the graphical keyboard. While the user provides a first gesture at UID 102 to input the word "grand", UI module 120 and keyboard module 122 may receive data from UID 102 about the input as an indication of a first gesture selecting a first sequence of keys including the ⁇ G-key>, the ⁇ R-key>, the ⁇ A-key>, the ⁇ N-key>, and the ⁇ D-key>, among others.
- Computing device 100 receives an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys (506). For example, while the user provides a second gesture at UID 102 to input the word "mother", UI module 120 and keyboard module 122 may receive data from UID 102 about the input as an indication of a second gesture selecting a second sequence of keys including the ⁇ M-key>, the ⁇ O-key>, the ⁇ T-key>, the ⁇ H-key>, the ⁇ E-key>, and the ⁇ R-key> among others.
- Computing device 100 may determine whether the indication of the second gestures satisfies a pressure threshold (508).
- keyboard module 122 determines, based on the indication of the second gesture, an initial portion of the second gesture that corresponds to an initial contact at a pressure-sensitive input device at which the second gesture was detected.
- Keyboard module 122 determines whether the amount of pressure associated with the initial portion of the second gesture satisfies the pressure threshold.
- keyboard module 122 determines whether the amount of pressure made during the user's initial contact with UID 102, i.e., associated with the beginning of the input ("mother") proximate to the location of the ⁇ M-key>, satisfies the pressure threshold.
- computing device 100 may determine a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys (510).
- keyboard module 122 determines a candidate word "grandmother" which includes the combination of the first sequence selecting the ⁇ G-key>, the ⁇ R-key>, the ⁇ A-key>, the ⁇ N-key>, and the ⁇ D-key>) and the second sequence selecting the ⁇ M-key>, the ⁇ O-key>, the ⁇ T-key>, the ⁇ H-key>, the ⁇ E-key>, and the ⁇ R-key>, among others, based on the data received from UID 102.
- Computing device 100 outputs, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys (512).
- UI module 120 may cause UID 102 to present candidate word 130 (e.g., "grandmother") within an edit region of user interface 104.
- computing device 100 may determine a candidate word that includes the first sequence of one or more keys from the plurality of keys and not the second sequence of one or more keys from the plurality of keys (514). Computing device 100 may refrain from outputting, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys (516).
- keyboard module 122 determines the candidate word "grand" based on the first sequence of keys, and which does not include the second sequence of keys selecting the ⁇ M-key>, the ⁇ O-key>, the ⁇ T-key>, the ⁇ H-key>, the ⁇ E-key>, and the ⁇ R-key>, among others.
- Computing device 100 may output, for display, the candidate word that includes the first sequence of one or more keys from the plurality of keys (518).
- keyboard module 122 may cause UI module 120 to output the candidate word "grand" within an edit region of user interface 104.
- computing device 100 may determine a second candidate word that includes the second sequence of one or more keys from the plurality of keys.
- Computing device 100 may further, after outputting, for display, the second candidate word that includes the first sequence of one or more keys from the plurality of keys, output, for display, the second candidate word that includes the second sequence of one or more keys from the plurality of keys.
- keyboard module 122 may cause UI module 120 to output the candidate word "mother” after determining and displaying the candidate word "grand.”
- computing device 100 may output for display character strings for "grand" and for "mother” as separate words based on the determination that the second gesture does not satisfy the pressure threshold.
- Certain aspects of the present disclosure describe an input technique that is based on pressure information associated with an indication of a gesture input, i.e., a determination of whether the pressure information associated with the indication of the gesture input satisfies a pressure threshold.
- some embodiments determine a candidate word that includes a combination of a first sequence of keys and a second sequence of keys. It should be recognized that alternative embodiments may allow for other uses of the pressure information.
- the computing device determines a candidate word that includes a combination of the first sequence of keys and the second sequence of keys.
- computing device 100 may be configured to use pressure information associated with touch input on a graphical keyboard to enter input indicating a word delimiter, such as a whitespace character (sometimes referred to as a "space" character), or other punctuation.
- a word delimiter such as a whitespace character (sometimes referred to as a "space" character)
- whitespace is a common character found within many text sequences. For example, assuming the average length of English language words without a whitespace is 4 letters, which means a user can enter a whitespace character every four characters.
- embodiments of the present disclosure provide a technique for entering whitespace input, improving input efficiency and input speed, for both touch typing and gesture typing.
- FIG. 6 is a flowchart illustrating another example operation 600 of the computing device, in accordance with one or more aspects of the present disclosure.
- the process of FIG. 6 may be performed by one or more processors of a computing device, such as computing device 100 illustrated in FIG. 1 and FIG. 2 .
- FIG. 6 is described below within the context of computing device 100 of FIG. 1 and FIG. 2 .
- computing device 100 may be configured to insert word delimiter characters based on gesture input on a graphical keyboard. Similar to the examples described above, the exertion of a forceful pressure (e.g., exceeding a particular threshold) during a gesture input may in some examples indicate an end to a current word, and that subsequent gesture input (e.g., finger movement) is inputting a subsequent word, without requiring the user to select a word delimiter key (e.g., space bar) between word inputs.
- the described technique may therefore allow a user to continuously enter multiple words without moving the input object (e.g., finger) to the space bar, or lifting the input up at the end of a word.
- UI module 120 and keyboard module 122 may receive data from UID 102 about the input as indication of a gesture selecting a sequence of keys including the ⁇ T-key>, the ⁇ H-key>, the ⁇ A-key>, the ⁇ N-key>, and the ⁇ K-key>.
- Keyboard module 122 may determine that pressure information included in the gesture indication indicates that an exertion of force on UID 102 while an input object (e.g., finger) is proximate to the ⁇ K-key>, and in response, determine the candidate word based on the selection of keys up until the ⁇ K-key>. Keyboard module 122 may proceed to output, for display, the candidate word "thank", along with a word delimiter (e.g., whitespace) to an edit region of user interface 104 and treat subsequent input (e.g., to select the ⁇ Y-key>, the ⁇ O-key>, and the ⁇ U-key> as input for determining and displaying a subsequent word "you" in the edit region of user interface 104. For example, consider the operations of FIG. 6 .
- a word delimiter e.g., whitespace
- FIG. 6 illustrates that computing device 100 may output a graphical keyboard comprising a plurality of keys (602).
- UI module 120 of computing device 100 may cause UID 102 to present a graphical user interface 104 that includes the graphical keyboard.
- Computing device 100 may receive an indication of a first touch input selecting one or more keys of the plurality of keys of the graphical keyboard (604). For example, the user of computing device 100 may tap a sequence of keys for the words “thank” on the graphical keyboard, with a "forceful” pressure (e.g., exceeding a pressure threshold) placed on the "K" key, the last key of the word. While the user provides the touch input at UID 102 to input the word "thank,” UI module 120 and keyboard module 122 may receive data from UID 102 about the input as an indication of touch input selecting the keys of the graphical keyboard, including the ⁇ T-key>, the ⁇ H-key>, the ⁇ A-key>, the ⁇ N-key>, and the ⁇ K-key>.
- Computing device 100 may determine whether the received indication of the first touch input selecting one or more keys of the plurality of keys of the graphical keyboard satisfies a pressure threshold (606). For example, keyboard module 122 determines whether the amount of pressure associated with a touch event containing key-press event satisfies the pressure threshold.
- computing device 100 may determine a candidate letter based on the selected key (612). For example, in response to determining that the touch events associated with key-press of any of the keys including ⁇ T-key>, the ⁇ H-key>, the ⁇ A-key>, and the ⁇ N-key>, do not satisfy the pressure threshold, keyboard module 122 may determine the candidate letters "t", "h", "a", and "n", respectively.
- Computing device 100 may output, for display, the candidate letter (614).
- keyboard module 122 may cause UI module 120 to output the candidate letter "t" within an edit region of user interface 104.
- computing device 100 may determine a candidate letter and a word delimiter based on the received indication (608). For example, in response to determining that the touch event associated with a key-press of the ⁇ K-key> exceeds the pressure threshold, keyboard module 122 determines the candidate letter "k" and a word delimiter are the user input.
- the word delimiter may be a whitespace character (e.g., space, tab, new line, or other whitespace character), although other characters including punctuation (such as a period or comma) may be used.
- Computing device 100 may output the determined candidate letter and the word delimiter (610).
- keyboard module 122 may cause UI module 120 to output the character "k" concatenated with a whitespace character within an edit region of user interface 104.
- keyboard module 122 may generate a graphical indication to signal to the user that the whitespace character has been automatically generated and inserted into the edit region of user interface 104.
- the user does not need to separately entire a whitespace character to separate the words "thank" and "you” by tapping a spacebar key on the graphical keyboard, and can instead proceed directly to providing touch input on the first letter of the next word, i.e., the ⁇ Y-key>.
- the described techniques may offer technical solutions for both touch typing and gesture typing forms of keyboard input.
- the keyboard module 122 does not require a user to need to enter the space bar keypress, saving the finger travel distance, and improving the input speed.
- the described keyboard module may allow a user to enter multiple words without traveling finger to space bar or lifting finger at the end of each word.
- Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol.
- computer-readable media generally may correspond to (1) tangible computer-readable storage media, which is non-transitory or (2) a communication medium such as a signal or carrier wave.
- Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure.
- a computer program product may include a computer-readable medium.
- such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer.
- any connection is properly termed a computer-readable medium.
- a computer-readable medium For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium.
- DSL digital subscriber line
- Disk and disc includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.
- processors such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry.
- DSPs digital signal processors
- ASICs application specific integrated circuits
- FPGAs field programmable logic arrays
- processors may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described.
- the functionality described may be provided within dedicated hardware and/or software modules. Also, the techniques could be fully implemented in one or more circuits or logic elements.
- the techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set).
- IC integrated circuit
- a set of ICs e.g., a chip set.
- Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.
Description
- Some computing devices provide a graphical keyboard for inputting text as part of a graphical user interface. A "gesture keyboard" is one example of a graphical keyboard that can enable a computing device to determine a string of text based on a continuous gesture input. The continuous gesture input may include a single, continuous gesture spanning locations of multiple keys of the gesture keyboard rather than multiple, individual tapgesture inputs entered at each of the individual locations of the multiple keys. A gesture keyboard may be preferable to users who prefer to swipe across-rather than merely tap on- individual keys of a graphical keyboard.
- Some computing devices have adopted a one-gesture-for-one-word model for gesture keyboards. That is, these computing devices map a single, continuous gesture to only a single word, and any interruption of the gesture (e.g., lifting a finger from the gesture keyboard) may be interpreted by the computing device as signaling the completion of the entered word. This approach may have certain drawbacks, however. For example, a gesture to select a relatively long word with many characters (e.g., "supermarket") would require continuous contact with the gesture keyboard for the entirety of the lengthy word, potentially resulting in user fatigue and increased cognitive difficulty that can aggravate input errors, decrease input accuracy, and slow the speed of input using the gesture keyboard. United States patent application publication no.
US 2014/098023 A1 presents "incremental multi-touch gesture recognition." United States patent application publication no.US 2010/241984 A1 presents "an apparatus, comprising a shorthand-aided rapid keyboarding enabled touchscreen" which is "configured to receive a user input." United States patent application publication no.US 2010/238125 A1 presents "various methods for discontinuous shapewriting". United States patent application publication no.US 2003/165801 A1 presents "a method of interpreting keypad input". - In one example, the disclosure is directed to a method that includes outputting, by a computing device and for display, a graphical keyboard comprising a plurality of keys. The method further includes receiving, by the computing device, an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys, and receiving, by the computing device, an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys. The method further includes, determining, by the computing device and based on pressure information associated with the indication of the second gesture, a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys, and outputting, by the computing device and for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- In another example, a computing device may include at least one processor, and at least one module operable by the at least one processor to output, for display, a graphical keyboard comprising a plurality of keys. The at least one module may be operable to receive an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys, and receive an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys. The module may be further operable to determine, based on pressure information associated with the indication of the second gesture, a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys, and output, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- In another example, a computer-readable storage medium is encoded with instructions that, when executed, cause at least one processor of a computing device to output, for display, a graphical keyboard comprising a plurality of keys. The instructions may cause the processor to receive an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys, and receive an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys. The instructions may further cause the processor to determine, based on pressure information associated with the indication of the second gesture, a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys, and output, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- The details of one or more examples are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.
-
-
FIG. 1 is a conceptual diagram illustrating an example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure. -
FIG. 2 is a block diagram illustrating an example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure. -
FIG. 3 is a block diagram illustrating another example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more techniques of the present disclosure. -
FIG. 4 is a conceptual diagram illustrating example graphical user interface for inputting words using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure. -
FIG. 5 is a flowchart illustrating an example operation of the computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure. -
FIG. 6 is a flowchart illustrating another example operation of the computing device, in accordance with one or more aspects of the present disclosure. - In general, this disclosure is directed to techniques for determining words based at least in part on an amount of pressure associated with a portion of a gesture input associated with one or more graphical keys of a gesture keyboard that is output by a computing device for display. An example computing device may initially output a gesture keyboard for display at a display device. The example computing device may detect a gesture made by an input object (e.g., a finger or stylus) at locations on a presence-sensitive device that correspond to locations of keys of the gesture keyboard that are displayed by the display device.
- Based on the detected gesture, the example computing device may determine a selection of one or more of the keys and then use this selection of keys and pressure information associated with the gesture input to determine one or more words from a lexicon that correspond to the gesture input. That is, unlike other computing devices that have adopted a one-gesture-for-one-word model (e.g., always mapping a single, continuous gesture spanning locations of multiple keys of the gesture keyboard to only a single word with an interruption of the gesture input denoting an end to the desired word), the example computing device may, before mapping a selection of keys to a word in a lexicon, determine whether the selection of keys corresponds to an input for entering either a single word or multiple words by using pressure information obtained about the gesture input (e.g., from the presence-sensitive input device).
- In some instances, the example computing device may infer whether a single gesture is an input for a single word or for multiple words based on pressure information, regardless of whether the selection of keys includes a selection of a space bar key or other end-of-word identifier. For example, the example computing device may determine that a given gesture input is intended to input two words by identifying the start of the second of two words based on a determination that a middle portion of the gesture was entered with a "high" pressure (e.g., a pressure that exceeds a particular pressure threshold) as opposed to a "normal" or "low" pressure (e.g., a pressure that does not exceed the particular pressure threshold).
- In some instances, the example computing device may use pressure information obtained about a gesture input to determine whether a single gesture is intended to enter a new word or to continue entry of a previous word determined from a previous gesture input. For example, the example computing device may determine that a new word is being entered when the computing device determines that a "high" pressure (e.g., a pressure that exceeds a particular pressure threshold) is associated with a beginning portion of a gesture as opposed to a "normal" or "low" pressure (e.g., pressure that does not exceed the particular pressure threshold), in which case the computing device may treat the gesture input as a continuation of a previous gesture.
- Therefore, the techniques described herein may enable a computing device to utilize pressure to determine whether a continuous gesture input at a gesture keyboard corresponds to a single or multiple word inputs in addition to discerning whether a gesture input is for entering the start of a new word or whether the gesture input is for continuing where a previous gesture left off in entering a previous word. Techniques of this disclosure may provide one or more advantages. The physical and cognitive difficulty of entering long words using single, continuous gestures can cause errors in text entry, which are exacerbated by requiring additional inputs and time to correct the text entries. As such, techniques of this disclosure may improve the overall input speed and accuracy of the gesture keyboard by reducing the likelihood of erroneous text entry, particular for long words and may thus provide an improved user interface.
-
FIG. 1 is a conceptual diagram illustrating an example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure. In the example ofFIG. 1 ,computing device 100 may be a mobile phone. However, in other examples,computing device 100 may be a tablet computer, a personal digital assistant (PDA), a laptop computer, a portable gaming device, a portable media player, an e-book reader, a computerized watch, television platform, or another type of computing device. - As shown in
FIG. 1 ,computing device 100 includes a user interface device (UID) 102. UID 102 ofcomputing device 100 may function as an input device forcomputing device 100 and as an output device. UID 102 may be implemented using various technologies. For instance, UID 102 may function as an input device using a presence-sensitive input device, such as a resistive touchscreen, a surface acoustic wave touchscreen, a capacitive touchscreen, a projective capacitance touchscreen, a pressure sensitive screen, an acoustic pulse recognition touchscreen, or another presence-sensitive input device technology. UID 102 may function as an output device using any one or more of a liquid crystal display (LCD), dot matrix display, light emitting diode (LED) display, organic lightemitting diode (OLED) display, e-ink, or similar monochrome or color display capable of outputting visible information to the user ofcomputing device 100. - UID 102 of
computing device 100 may include a presence-sensitive screen (e.g., presence-sensitive display) that may receive tactile user input from a user ofcomputing device 100. UID 102 may receive indications of the tactile user input by detecting one or more tap and/or non-tap gestures from a user of computing device 100 (e.g., the user touching or pointing to one or more locations ofUID 102 with a finger or a stylus pen). The presence-sensitive screen ofUID 102 may present output to a user. UID 102 may present the output as a user interface (e.g., user interface 104) which may be related to functionality provided bycomputing device 100. For example, UID 102 may present various user interfaces of applications (e.g., an electronic message application, an Internet browser application, etc.) executing atcomputing device 100. A user ofcomputing device 100 may interact with one or more of these applications to perform a function withcomputing device 100 through the respective user interface of each application. -
Computing device 100 may include user interface ("UI")module 120, and akeyboard module 122.Modules computing device 100.Computing device 100 may executemodules Computing device 100 may executemodules -
UI module 120 ofcomputing device 100 may receive fromUID 102, one or more indications of user input detected atUID 102. Generally, eachtime UID 102 receives an indication of user input detected at a location ofUID 102,UI module 120 may receive information about the user input fromUID 102. - For example,
UID 102 may virtually overlay a grid of coordinates ontoUID 102. The grid may not be visibly displayed byUID 102. The grid may assign a coordinate that includes a horizontal component (X) and a vertical component (Y) to each location. Eachtime UID 102 detects a gesture input,UI module 120 may receive information fromUID 102. The information may include one or more coordinate locations and associated times indicating toUI module 120 both, whereUID 102 detects the gesture input atUID 102, and whenUID 102 detects the gesture input. The information may further include pressure information indicating an amount of pressure or force applied by the user on theUID 102. For example,UID 102 may determine the amount of pressure applied by a gesture input using capacitive, inductive, or other pressure sensing techniques. The pressure information may be in the form of numerical, quantified data points (e.g., "0.73 units of force") or discrete levels (e.g., "normal", "hard"). In some implementations, the pressure information may be represented as a depth component (Z) that are part of the grid coordinates (e.g., X, Y) described above. -
UI module 120 may assemble the information received fromUID 102 into a time-ordered sequence of touch events. For example, each touch event in the sequence may include data or components that represents parameters for characterizing a presence, an amount of pressure associated with, and/or movement (e.g., when, where, originating direction) of, input atUID 102. Each touch event in the sequence may include a location component (e.g., a x,y coordinate of a location at UID 102), a time component related to whenUID 102 detected user input at the location, a pressure component indicating an amount of force detected byUID 102 when detecting the input at the location, and an action component related to whether the touch event corresponds to a lift up or a push down at the location. - In addition to time, location, and pressure,
UI module 120 may determine one or more other characteristics of the user input and include information about these one or more characteristics within each touch event in the sequence of touch events. For example,UI module 120 may determine a start location of the user input, an end location of the user input, a density of a portion of the user input, a speed of a portion of the user input, a direction of a portion of the user input, and a curvature of a portion of the user input. One or more touch events in the sequence of touch events may include (in addition to a time, a location, and an action component as described above) a characteristic component that includes information about one or more characteristics of the user input (e.g., a density, a speed, etc.). -
UI module 120 may causeUID 102 to presentuser interface 104.User interface 104 includes graphical elements displayed at various locations ofUID 102.FIG. 1 illustratesedit region 106A andgesture keyboard 106B ofuser interface 104.Gesture keyboard 106B is a graphical keyboard having selectable, graphical elements displayed as keys for typing text atedit region 106A.Edit region 106A may include graphical elements such as images, objects, hyperlinks, characters of text (e.g., character strings) etc., thatcomputing device 100 generates in response to input associated withgesture keyboard 106B. In some examples, editregion 106A is associated with a messaging application, a word processing application, an internet webpage browser application, or other text entry field of an application, operating system, or platform executing atcomputing device 100. In other words, editregion 106A represents a final destination of the letters that a user ofcomputing device 100 is selecting usinggesture keyboard 106B and is not an intermediary region associated withgesture keyboard 106B, such as word suggestion or autocorrect region that displays one or more complete word suggestions or auto-corrections. -
FIG. 1 shows the letters w-a-t-e-r-m-e-l-o-n withinedit region 106A. The letters w-a-t-e-r-m-e-l-o-n make up a string of characters orcandidate word 130.Candidate word 130 is delineated by dashed circles in the example ofFIG. 1 , howeverUI device 102 may or may not output such dashed circles in some examples.Candidate word 130 may be a portion of a word and/or a word or portion of a word included in a dictionary. -
FIG. 1 also shows a sequence ofpaths gesture keyboard 106B.UI module 120 may causeUID 102 to presentpaths UI module 120 about the locations of a gesture detected byUID 102. In other words,UI module 120 may causeUID 102 to displaypath UID 102 such that, whenpaths UID 102,paths UI module 120 may causeUID 102 to displaypath 134 at the same time as a (previous)path 132.Paths - A user of
computing device 100 may causeUI module 120 to enter text inedit region 106A by providing input (e.g., tap and/or non-tap gestures) at locations ofUID 102 that display the keys ofgesture keyboard 106B. In response to user input such as this,computing device 100 may output one or more characters, strings, or multi-string phrases withinedit region 106A, such ascandidate word 130. In some examples, a word may generally be described as a string of one or more characters in a dictionary or lexicon (e.g., a set of strings with semantic meaning in a written or spoken language), a "word" may, in some examples, refer to any group of one or more characters. For example, a word may be an out-of-vocabulary word or a string of characters not contained within a dictionary or lexicon but otherwise used in a written vocabulary to convey information from one person to another. For instance, a word may include a name, a place, slang, or any other out-of-vocabulary word or uniquely formatted strings, etc., that includes a first portion of one or more characters followed by a second portion of one or more characters. -
UI module 120 may act as an intermediary between various components ofcomputing device 100 to make determinations based on input detected byUID 102 and generate output presented byUID 102. For instance,UI module 120 may receive, as an input from keyboard module 122 a representation of a keyboard layout of the keys included ingesture keyboard 106B. In some examples,UI module 120 may determine that the one or more location components in the sequence of touch events approximate input atgesture keyboard 106B (e.g.,UI module 120 may determine the location of one or more of the touch events corresponds to an area ofUID 102 that presentsgesture keyboard 106B).UI module 120 may transmit, as output tokeyboard module 122, the sequence of touch events along with locations whereUID 102 presents each of the keys. - In response to transmitting touch events and locations of keys to
keyboard module 122,UI module 120 may receive a candidate word (e.g., a string of text representing candidate word 130) thatkeyboard module 122 determined from the sequence of touch events.UI module 120 may updateuser interface 104 to includecandidate word 130 received fromkeyboard module 122 withinedit region 106A.UI module 120 may causeUID 102 to present the updateduser interface 104 includingcandidate word 130 presented as graphical elements (e.g., characters) inedit region 106A. -
Keyboard module 122 ofcomputing device 100 may transmit, as output to UI module 120 (for inclusion asgesture keyboard 106B of user interface 104) a keyboard layout including a plurality of keys related to one or more written languages (e.g., English, Spanish, French, etc.).Keyboard module 122 may assign one or more characters or operations to each key of the plurality of keys in the keyboard layout. For instance,keyboard module 122 may generate a QWERTY keyboard layout including keys that represent characters used in typing the English language. The QWERTY keyboard layout may also include keys that represent operations used in typing the English language (e.g., backspace, delete, spacebar, enter, etc.). -
Keyboard module 122 may receive data fromUI module 120 that represents the sequence of touch events generated byUI module 120 as well as the locations ofUID 102 whereUID 102 presents each of the keys ofgesture keyboard 106B.Keyboard module 122 may determine, based on the locations of the keys and the sequence of touch events, a selection of one or more keys.Keyboard module 122 may determine a character string based on the selection where each character in the character string corresponds to at least one key in the selection.Keyboard module 122 may send data indicating the character string toUI module 120 for inclusion ascandidate word 130 inedit region 106A ofuser interface 104. -
Keyboard module 122 may rely on various models and algorithms (e.g., a spatial model, a language model, etc.) to determine whether or not a sequence of touch events represents a selection of one or more keys. For example,keyboard module 122 may include spatial and language models to determine with a certain degree of certainty, which keys ofgesture keyboard 106B are being selected and which character string is being entered with the selection. - Using such a spatial model,
keyboard module 122 may compare the location components (e.g., coordinates) of one or more touch events in the sequence of touch events to respective locations of one or more keys ofgesture keyboard 106B and generate a probability based on these comparisons that a selection of a key occurred. For example,keyboard module 122 may compare the location component of each touch event in the sequence of touch events to a key location of a particular key ofgesture keyboard 106B. The location component of each touch event in the sequence may include one location ofUID 102 and a key location (e.g., a centroid of a key) of a key ingesture keyboard 106B may include a different location ofUID 102.Keyboard module 122 may determine a Euclidian distance between the two locations and generate a probability based on the Euclidian distance that the key was selected.Keyboard module 122 may correlate a higher probability to a key that shares a smaller Euclidian distance with one or more touch events than a key that shares a greater Euclidian distance with one or more touch events. Based on the spatial model probability associated with each key,keyboard module 122 may assemble the individual key selections with the highest spatial model probabilities into a time-ordered sequence of keys thatkeyboard module 122 may then determine represents a character string. - Using a pressure model,
keyboard module 122 may modify the output from the spatial model using the pressure components of one or more touch events in the sequence of touch events.Keyboard module 122 may compare the pressure component of each touch event in the sequence of touch events to a pressure threshold. Based on the comparison,keyboard module 122 may annotate the time-ordered sequence of keys with metadata indicating whether the sequence of keys included a "hard" pressure (e.g., exceeding the pressure threshold). For example,keyboard module 122 may analyze the pressure component of the one or more touch events associated with the start location of the user input to determine whether the touch input began with a "hard" pressure (e.g., exceeding the pressure threshold). Similarly,keyboard module 122 may determine whether the input sequence of keys ended with a "hard" pressure (e.g., exceeding the pressure threshold) based on the pressure component in touch events associated with the end location of the user input. In some examples, rather than associate the pressure information with the gesture as a whole,keyboard module 122 may correlate the pressure information with specific keys in the sequence of keys.Keyboard module 122 may use both the pressure and location components of the touch events to associate pressure information with individual key presses, e.g., a hard 'W' press. - In one example,
keyboard module 122 may use pressure data to determine which gesture inputs ongesture keyboard 106B should be included in asame candidate word 130 or in separate candidate words. In other words, rather than assume each distinct gesture is a new candidate word,keyboard module 122 may continue to build a candidate word with each subsequent gesture on top of previous gestures. In such cases,keyboard module 122 incorporates the selection of keys from a current gesture with the selection of keys from prior gestures. Upon detecting a gesture contains a "hard" press,keyboard module 122 may complete its building of the candidate word, and begin a new word. By enablingcomputing device 100 to combine non-continuous gestures to form a word,keyboard module 122 may provide a way for a user ofcomputing device 100 to enter text more accurately and/or with less effort. - The techniques are now further described in detail with reference to
FIG. 1 . In the example ofFIG. 1 ,computing device 100 may output, for display, a graphical keyboard comprising a plurality of keys. For example,keyboard module 122 may generate data that includes a representation ofgesture keyboard 106B.UI module 120 may generateuser interface 104 and includegesture keyboard 106B inuser interface 104 based on the data representinggesture keyboard 106B.UI module 120 may send information toUID 102 that includes instructions for displayinguser interface 104 atUID 102.UID 102 may receive the information and causeUID 102 to presentuser interface 104 includingedit region 106A andgesture keyboard 106B.Gesture keyboard 106B may include a plurality of keys. -
Computing device 100 may receive an indication of a gesture selecting a first sequence of keys from the plurality of keys. For example, a user ofcomputing device 100 may provide a continuous gesture input that traverses acrosspath 132 beginning at afirst location 136A and ending at asecond location 136B ofUID 102 such that the gesture performed from the first location to the second location is detected byUID 102 throughout the performance of the gesture. In the example shown inFIG. 1 , the gesture may traverse across a sequence of keys that includes the 'W' key, the 'A' key, the `S' key, the 'D' key, the 'R' key, the 'T' key, the 'R' key, the 'E' key, and the 'R' key. The gesture may begin with a greater amount of force (e.g., a hard force or force that exceeds a pressure threshold) (represented inFIG. 1 by concentric circles) applied atlocation 136A ofUID 102 which is nearest to the location of the 'W' key and end with a lesser amount of force (e.g., a light force or force that does not exceed the pressure threshold) atlocation 136B proximate to the location of the 'R' key. -
UI module 120 may generate a sequence of touch events based on the gesture.UI module 120 may causeUID 102 to present a graphical indication ofpath 132 based on information received fromUI module 120 about the locations of the gesture detected byUID 102.Keyboard module 122 may rely on pressure information associated with the sequence of touch events generated byUI module 120 as well as a spatial model, language model, and/or other technique to determine which keys are most likely being selected. For example,keyboard module 122 may determine whether the received indication of the gesture satisfies a pressure threshold based on pressure information from touch events associated with the start location of the gesture. In the example shown inFIG. 1 ,keyboard module 122 may determine that pressure information for the touch events having locations proximate to the "W" key (e.g.,location 136A) and during the start of the gesture exceeds a pressure threshold. Based on the pressure information,keyboard module 122 may initiate a new word instead of combining the gesture with previous input. In other words,keyboard module 122 may complete the word prediction of previous inputs, insert a word delimiter (e.g., space), and begins word prediction anew on the received gesture. In some examples,keyboard module 122 may analyze the pressure information after receiving some but not all of the sequence of touch events associated with a gesture input. In such a way,keyboard module 122 may incrementally determine whether the received indication of the gesture satisfies a pressure threshold without having to wait until the end of the gesture. - Subsequent to the first gesture,
computing device 100 may further receive an indication of a second gesture selecting a second sequence of keys from the plurality of keys.Keyboard module 122 may characterize the second gesture, which was received subsequent to the first gesture, as separate and distinct from the first gesture based on action components in the sequence of touch events associated with the first and second gestures. For example,keyboard module 122 may receive a touch event indicating a lift off action by an input object (e.g., user's finger) and a subsequent touch event indicating a push down action by the input object. In the example shown inFIG. 1 , a user ofcomputing device 100 may provide a gesture input that traverses acrosspath 134, which traverses across a sequence of keys that includes the 'M' key, the 'E' key, the 'L' key, the 'O' key, and the 'N' key (i.e., M-N-J-H-G-F-E-R-T-Y-J-K-L-O-K-J-N). The second gesture may begin atlocation 138A of UID 102 (which, notably, does not include concentric circles inFIG. 1 to indicate that the gesture does not include the application of a hard force) which is nearest to the location of the 'M' key and ends atlocation 138B ofUID 102 which is nearest to the location of the 'N' key. - Similar to the operations associated with the first gesture,
keyboard module 122 may determine whether the indication of the second gesture satisfies a pressure threshold based on pressure information from touch events associated with the start location of the second gesture. For example,keyboard module 122 may determine that pressure information for the touch events having locations proximate to the "M" key (e.g.,location 138A) and during the start of the second gesture does not exceed the pressure threshold. - Responsive to determining that the indication of the second gesture does not satisfy a certain pressure threshold,
keyboard module 122 may perform word prediction to determine acandidate word 130 that includes a combination of the first sequence of keys as well as the second sequence of keys. For example, rather than outputting a candidate word "water" upon receipt of the second gesture,keyboard module 122 may combine the second sequence of keys which includes, among other keys, the 'M' key, the 'E' key, the 'L' key, the 'O' key, and the 'N' key, with the first sequence of keys which includes the 'W' key, the 'A' key, the 'D' key, the `R' key, the 'T' key, the `R' key, the 'E' key, and the `R' key in the determination of the candidate word "watermelon." -
Keyboard module 122 may perform word prediction to determine acandidate word 130 using the combination of the first gesture and the second gesture even though the gestures are separate and non-continuous.Keyboard module 122outputs candidate word 130 that includes the combination of the first sequence of keys and the second sequence of keys. For example,keyboard module 122 causesUID 102 to display the candidate word "watermelon" inedit region 106A - For sake of explanation, assume that the second gesture did include a "hard" pressure (e.g., exceeding a pressure threshold) applied during the beginning of the gesture and proximate to the "M" key at
location 138A. Responsive to determining that the indication of the second gesture did in fact satisfy the pressure threshold,keyboard module 122 may complete the word prediction based on the first gesture that includes the first sequence of keys (and not based on the second gesture having the second sequence of keys). In other words,keyboard module 122 refrains from outputting a candidate word that combines the first gesture and the second gesture, and instead outputs a candidate word "water" that include the selection of keys from the first gesture only.Keyboard module 122 may also initiate prediction of a new word based on the second gesture to determine the candidate word "melon." - Techniques of the present disclosure may improve the accuracy with which a user can enter text into a computing device. Using techniques of this disclosure, a user may, instead of having to perform a single, continuous gesture for a word, perform multiple gestures that indicate the word. Such techniques are particularly helpful to users for entering long words containing many characters, such as compound words. By enabling the computing device to determine input words based on multiple gesture inputs and corresponding pressure information, the described technique allows the user to rest the input finger in the middle of gesturing a long word, which reduces user fatigue and the likelihood of errors. The present techniques further reduce the likelihood of input errors by decreasing the cognitive difficulty of entering long words by breaking the long word in chunks. As such, techniques of this disclosure may improve the overall input speed and accuracy of the gesture keyboard by reducing the likelihood of erroneous text entry.
-
FIG. 2 is a block diagram illustrating an example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure.Computing device 100 ofFIG. 2 is described below within the context ofFIG. 1 .FIG. 2 illustrates only one particular example ofcomputing device 100, and many other examples ofcomputing device 100 may be used in other instances and may include a subset of the components included inexample computing device 100 or may include additional components not shown inFIG. 2 . - As shown in the example of
FIG. 2 ,computing device 100 includes user interface device 102 ("UID 102"), one ormore processors 202, one ormore input devices 204, one ormore communication units 206, one ormore output devices 208, and one ormore storage devices 210.Storage devices 210 ofcomputing device 100 also includeUI module 120 andkeyboard module 122.UI module 120 includes apressure module 222.Keyboard module 122 includes a spatial-language model module 220 ("SLM module 220").Communication channels 212 may interconnect each of thecomponents communication channels 212 may include a system bus, a network connection, an inter-process communication data structure, or any other method for communicating data. - One or
more input devices 204 ofcomputing device 100 may receive input. Examples of input are tactile, audio, and video input.Input devices 204 ofcomputing device 100, in one example, includes a presence-sensitive input device (e.g., a touch sensitive screen, a presence-sensitive display), mouse, keyboard, voice responsive system, video camera, microphone or any other type of device for detecting input from a human or machine. - One or
more output devices 208 ofcomputing device 100 may generate output. Examples of output are tactile, audio, and video output.Output devices 208 ofcomputing device 100, in one example, includes a presence-sensitive display, sound card, video graphics adapter card, speaker, cathode ray tube (CRT) monitor, liquid crystal display (LCD), or any other type of device for generating output to a human or machine. - One or
more communication units 206 ofcomputing device 100 may communicate with external devices via one or more networks by transmitting and/or receiving network signals on the one or more networks. For example,computing device 100 may usecommunication unit 206 to transmit and/or receive radio signals on a radio network such as a cellular radio network. Likewise,communication units 206 may transmit and/or receive satellite signals on a satellite network such as a GPS network. Examples ofcommunication unit 206 include a network interface card (e.g. such as an Ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device that can send and/or receive information. Other examples ofcommunication units 206 may include Bluetooth®, GPS, 3G, 4G, LTE, and Wi-Fi® radios found in mobile devices as well as Universal Serial Bus (USB) controllers. - In some examples,
UID 102 ofcomputing device 100 may include functionality ofinput devices 204 and/oroutput devices 208. In the example ofFIG. 2 ,UID 102 may be or may include a presence-sensitive input device. In some examples, a presence-sensitive input device may detect an object at and/or near the presence-sensitive input device. As one example range, a presence-sensitive input device may detect an object, such as a finger or stylus that is within two inches or less of the presence-sensitive input device. The presence-sensitive input device may determine a location (e.g., an [x,y] coordinate) of the presence-sensitive input device at which the object was detected. In another example range, a presence-sensitive input device may detect an object six inches or less from the presence-sensitive input device and other ranges are also possible. The presence-sensitive input device may determine the location of the input device selected by a user's finger using capacitive, inductive, and/or optical recognition techniques. In some examples, presence-sensitive input device provides output to a user using tactile, audio, or video stimuli as described with respect tooutput device 208. In the example ofFIG. 2 ,UID 102 presents a user interface (such asuser interface 104 ofFIG. 1 ) atUID 102. - While illustrated as an internal component of
computing device 100,UID 102 also represents an external component that shares a data path withcomputing device 100 for transmitting and/or receiving input and output. For instance, in one example,UID 102 represents a built-in component ofcomputing device 100 located within and physically connected to the external packaging of computing device 100 (e.g., a screen on a mobile phone). In another example,UID 102 represents an external component ofcomputing device 100 located outside and physically separated from the packaging of computing device 100 (e.g., a monitor, a projector, etc. that shares a wired and/or wireless data path with a tablet computer). - One or
more storage devices 210 withincomputing device 100 may store information for processing during operation ofcomputing device 100. In some examples,storage device 210 is a temporary memory, meaning that a primary purpose ofstorage device 210 is not long-term storage.Storage devices 210 oncomputing device 100 may be configured for short-term storage of information as volatile memory and therefore not retain stored contents if powered off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art. -
Storage devices 210, in some examples, also include one or more computer-readable storage media.Storage devices 210 may be configured to store larger amounts of information than volatile memory.Storage devices 210 may further be configured for long-term storage of information as non-volatile memory space and retain information after power on/off cycles. Examples of non-volatile memories include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.Storage devices 210 may store program instructions and/or data associated withUI module 120,keyboard module 122,pressure module 222, andSLM module 220. - One or
more processors 202 may implement functionality and/or execute instructions withincomputing device 100. For example,processors 202 oncomputing device 100 may receive and execute instructions stored bystorage devices 210 that execute the functionality ofUI module 120,keyboard module 122,pressure module 222, andSLM module 220. These instructions executed byprocessors 202 may causecomputing device 100 to store information, withinstorage devices 210 during program execution.Processors 202 may execute instructions ofmodules UID 102 to displayuser interface 104 atUID 102. That is,modules processors 202 to perform various actions, including receiving an indication of a gesture at locations ofUID 102 and causingUID 102 to presentuser interface 104 atUID 102. - In accordance with aspects of this
disclosure computing device 100 ofFIG. 2 may output for display at UID 102 a graphical keyboard comprising a plurality of keys. For example, during operational use ofcomputing device 100,keyboard module 122 may causeUI module 120 ofcomputing device 100 to output a keyboard layout (e.g., an English language QWERTY keyboard, etc.) for display atUID 102.UI module 120 may receive data specifying the keyboard layout fromkeyboard module 122 overcommunication channels 212.UI module 120 may use the data to generateuser interface 104 includingedit region 106A and the plurality of keys of the keyboard layout fromkeyboard module 122 asgesture keyboard 106B.UI module 120 may transmit data overcommunication channels 212 to causeUID 102 to presentuser interface 104 atUID 102.UID 102 may receive the data fromUI module 120 and causeUID 102 to presentuser interface 104. -
Computing device 100 may receive an indication of a first gesture that traverses a first group of keys of the plurality of keys and an indication of a pressure (e.g., force) associated with the first gesture. For example,computing device 100 receives a first gesture input (e.g., from a user) that traverses acrosspath 132 associated with the first group of keys and that includes an applied pressure applied toUID 102 during an initial portion of the gesture. In this example, the initial portion of the gesture input corresponds tolocation 136A ofUID 102 which is nearest to the location of the 'W' key. The gesture input further includes a traversal of a path associated with a group of keys (e.g., the 'A' key, 'S' key, 'D' key, 'R' key, 'T' key, `R' key, 'E' key, and 'R' key again), ending atlocation 136B ofUID 102 which is nearest to the location of the 'R' key. The last touch event in the sequence of touch events associated with the first gesture input may include an indication that the first gesture input has ended. For example, the touch event corresponding to the gesture input ending atlocation 136B proximate to the "R" key indicates that the continuous gesture input has ended. -
Computing device 100 may receive an indication of a second gesture that traverses a second group of keys of the plurality of keys, and which does not include an indication of a pressure. For example,computing device 100 receives a second gesture input (subsequent to the first gesture input) that traverses acrosspath 134 and that does not include a pressure during an initial portion of the gesture. In this example, the initial portion of the gesture input begins atlocation 138A ofUID 102 which is nearest to the location of the 'M' key, and continues to a traversal of a second group of keys (e.g., M-N-J-H-G-F-E-R-T-Y-J-K-L-O-K-J-N), ending atlocation 138B ofUID 102 which is nearest to the location of the 'N' key. -
UI module 120 may generate sequences of touch events based on the first and second gestures.Pressure module 222 ofUI module 120 may incorporate pressure information into the sequences of touch events based on the gestures.Pressure module 222 may modify the touch event, in the sequence of touch events, that corresponds to the initial portion of the first gesture. For example,pressure module 222 modifies the touch event associated with the traversal of the "W" key (i.e., during the initial portion of the gesture) to include pressure information. - Using the pressure information,
keyboard module 122 may determine a candidate word from a selection of keys determined from touch events of multiple gesture inputs, instead of only a single gesture input.Keyboard module 122 may use multiple sequences of touch events associated with multiple gesture inputs in determining a candidate word from the selection of keys. For example,keyboard module 122 may combine the selection of keys from the first gesture input and the second gesture input based on a pressure indication (or absence thereof) in the second sequence of touch events. In some cases, in response tokeyboard module 122 determining that the second sequence of touch events indicates a pressure has been applied toUID 102 during an initial portion of the second gesture input,keyboard module 122 uses the first sequence of touch events (and its associated selection of keys) separately from the second sequence of touch events (and its associated selection of keys) to determine a candidate word.Keyboard module 122 may continue to combine selections of keys from sequences of touch events from multiple gesture inputs untilkeyboard module 122 encounters a sequence of touch events from a new gesture input that indicates a pressure associated with the initial portion of the new gesture input. Put another way,keyboard module 122 assumes that each incoming selection of keys should be joined with the previous selection of keys untilkeyboard module 122 detects pressure information in the next gesture input. - Alternatively,
keyboard module 122 may be configured to join each new selection of keys with the previous selection of keys in response to the presence of pressure information in the new gesture input. For example,keyboard module 122 may determine a candidate word based on a selection of keys from a single continuous gesture input, and also determine a candidate word based on a selection of keys from multiple gesture inputs, each having a pressure indication associated with an initial portion of each subsequent gesture input. -
SLM module 220 ofkeyboard module 122 may determine the one or more candidate words from one or more lexicon data stores that have a highest probability of being the candidate words that a user may wish to enter by providing input atgesture keyboard 106B. The probability may indicate a frequency of use of each candidate word in a language context. That is,SLM module 220 may determine that one or more candidate words that have a greatest likelihood of being the word that a user may wish to enter atedit region 106A are the one or more candidate words that appear most often during an instance of written and/or spoken communication using a particular language.Keyboard module 122 may utilize a language module ofSLM module 220 to refine the selection of keys and/or to determine at least one candidate word from the selection of keys determined from the locations in multiple gestures. For example,SLM module 220 ofkeyboard module 122 may perform a look up within one or more lexicon data stores to identify one or more candidate words stored at the one or more lexicon data stores that begin with the letter "w " associated with the first key in the first sequence, end with the letter "n" associated with the last key in the second sequence, and that may or may not include any of the letters W-AS-D-R-T-R-E-R-M-N-J-H-G-F-E-R-T-Y-J-K-L-O-K-J-N that correspond to each of the keys in the sequence in-between the first and last keys.SLM module 220 may identify the candidate words watermelon, watermelons, and watermen as some example candidate words in the one or more lexicon data stores that begin and end with the letters w-n. -
Keyboard module 122 may rely on a language model and/or access a lexicon ofcomputing device 100 to autocorrect (e.g., spellcheck) a character string generated from a sequence of key selections before and/or after outputting the character string toUI module 120 for inclusion withinedit region 106A ofuser interface 104. A lexicon ofcomputing device 100 may include a list of words within a written language vocabulary. Using a language model,keyboard module 122 may perform a lookup in the lexicon of a character string generated from a selection of keys to identify one or more candidate words that include at least some or all of the characters of the character string generated based on the selection of keys. - In some examples,
keyboard module 122 may assign a Jaccard similarity coefficient to the one or more candidate words to determine a degree of similarity between a character string inputted by a user and a word in the lexicon. In general, a Jaccard similarity coefficient, also known as a Jaccard index, represents a measurement of similarity between two sample sets (e.g., a character string and a word in a dictionary). Based on the comparison,keyboard module 122 may generate a Jaccard similarity coefficient for one or more words in the lexicon. Each candidate word may include, as a prefix, an alternative arrangement of some or all of the characters in the character string. In other words, each candidate word may include as the first letters of the word, the letters of the character string determined from the selection of keys. For example, based on a selection of w-a-t-e,keyboard module 122 may determine that a candidate word of the lexicon with a greatest Jaccard similarity coefficient to w-a-t-e is water.Keyboard module 122 may output the autocorrected character string water toUI module 120 for inclusion inedit region 106A rather than the actual character string w-a-t-e indicated by the selection of keys. -
Keyboard module 122 may causeUI module 120 to presentcandidate word 130, determined from the combined selection of keys, atedit region 106A. For example,keyboard module 122 may send information viacommunication channel 212 that causesUI module 120 to causeUID 102 to presentcandidate word 130 as a string of text atedit region 106A. In some examples,keyboard module 122 may causeUI module 120 to graphically annotatecandidate word 130 to indicate which portions of candidate word are determined from separate gesture inputs. For example, to present the candidate word "watermelon" determined from multiple gesture inputs as described above,UI module 120 may highlight the portion "water" with a first color, and highlight the portion "melon" with a second color different from the first color. - In one embodiment,
keyboard module 122 may iteratively update the candidate word based on each subsequent gesture input and causeUI module 120 to update the candidate word displayed inedit region 106A. For example, in a first iteration,keyboard module 122 may determine a character string "water" based on the first gesture input corresponding topath 132 and causeUI module 120 to display the character string "water" inedit region 106A. Then, in a second, subsequent iteration,keyboard module 122 may determine an updated candidate word "watermelon" based on the second gesture input non-continuous with the first gesture input, corresponding topath 134, and based in conjunction with the first (previous) gesture input.Keyboard module 122 may causeUI module 120 to update theedit region 106A to display the updated candidate word. - In this way, unlike a typical gesture keyboard that adopts a one-gesture for one-word model,
computing device 100 may allow for using multiple gesture inputs (that are non-continuous to each other) to input a word. As a result,computing device 100 may reduce input errors caused by long words or compound words and improve the accuracy with which a user can type a word usinggesture keyboard 106B. As such,computing device 100 may receive fewer re-entry inputs from a user to enter text using gesture keyboard, and by receiving fewer re-entry inputs,computing device 100 may perform fewer operations and as such consume less electrical power than other computing devices. -
FIG. 3 is a block diagram illustrating another example computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more techniques of the present disclosure. The example computing device outputs graphical content for display at a remote device. Graphical content, generally, may include any visual information that may be output for display, such as text, images, a group of moving images, etc. The example shown inFIG. 3 includes acomputing device 300, presence-sensitive display 301,communication unit 310,projector 320,projector screen 322,tablet device 326, andvisual display device 330. Although shown for purposes of example inFIGS. 1 and2 as a stand-alone computing device 100, a computing device, such ascomputing device 100 and/orcomputing device 300, may generally refer to any component or system that includes a processor or other suitable computing environment for executing software instructions and, for example, need not include a presence-sensitive display. - As shown in the example of
FIG. 3 ,computing device 300 may be a processor that includes functionality as described with respect toprocessors 202 inFIG. 2 . In such examples,computing device 300 may be operatively coupled to presence-sensitive display 301 by acommunication channel 303A, which may be a system bus or other suitable connection.Computing device 300 may also be operatively coupled tocommunication unit 310, further described below, by acommunication channel 303B, which may also be a system bus or other suitable connection. Although shown separately as an example inFIG. 3 ,computing device 300 may be operatively coupled to presence-sensitive display 301 andcommunication unit 310 by any number of one or more communication channels. - In other examples, such as illustrated previously by computing
devices 100 inFIGS. 1-2 ,computing device 300 may be a portable or mobile device such as mobile phones (including smart phones), laptop computers, etc. In some examples,computing device 100 may be desktop computers, tablet computers, smart television platforms, cameras, personal digital assistants (PDAs), servers, mainframes, etc. - Presence-
sensitive display 301, likeUID 102 as shown inFIG. 1 , may includedisplay device 303 and presence-sensitive input device 305.Display device 303 may, for example, receive data fromcomputing device 300 and display the graphical content. In some examples, presence-sensitive input device 302 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at presence-sensitive display 301 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input tocomputing device 300 usingcommunication channel 303A. In some examples, presence-sensitive input device 302 may be physically positioned on top ofdisplay device 303 such that, when a user positions an input unit over a graphical element displayed bydisplay device 303, the location at which presence-sensitive input device 305 corresponds to the location ofdisplay device 303 at which the graphical element is displayed. - As shown in
FIG. 3 ,computing device 300 may also include and/or be operatively coupled withcommunication unit 310.Communication unit 310 may include functionality ofcommunication unit 206 as described inFIG. 2 . Examples ofcommunication unit 310 may include a network interface card, an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information. Other examples of such communication units may include Bluetooth, 3G, 4G, LTE, and Wi-Fi radios, Universal Serial Bus (USB) interfaces, etc.Computing device 300 may also include and/or be operatively coupled with one or more other devices, e.g., input devices, output devices, memory, storage devices, etc. that are not shown inFIG. 3 for purposes of brevity and illustration. -
FIG. 3 also illustrates aprojector 320 andprojector screen 322. Other such examples of projection devices may include electronic whiteboards, holographic display devices, and any other suitable devices for displaying graphical content.Projector 320 andprojector screen 322 may include one or more communication units that enable the respective devices to communicate withcomputing device 300. In some examples, the one or more communication units may enable communication betweenprojector 320 andprojector screen 322.Projector 320 may receive data fromcomputing device 300 that includes graphical content.Projector 320, in response to receiving the data, may project the graphical content ontoprojector screen 322. In some examples,projector 320 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen using optical recognition or other suitable techniques and send indications of such user input using one or more communication units tocomputing device 300. -
Projector screen 322, in some examples, may include a presence-sensitive display 324. Presence-sensitive display 324 may include a subset of functionality or all of the functionality ofUI device 102 as described in this disclosure. In some examples, presence-sensitive display 324 may include additional functionality. Projector screen 322 (e.g., an electronic whiteboard), may receive data fromcomputing device 300 and display the graphical content. In some examples, presence-sensitive display 324 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) atprojector screen 322 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input using one or more communication units tocomputing device 300. -
FIG. 3 also illustratestablet device 326 andvisual display device 330.Tablet device 326 andvisual display device 330 may each include computing and connectivity capabilities. Examples oftablet device 326 may include e-reader devices, convertible notebook devices, hybrid slate devices, etc. Examples ofvisual display device 330 may include televisions, computer monitors, etc. As shown inFIG. 3 ,tablet device 326 may include a presence-sensitive display 328.Visual display device 330 may include a presence-sensitive display 332. Presence-sensitive displays UI device 102 as described in this disclosure. In some examples, presence-sensitive displays sensitive display 332, for example, may receive data fromcomputing device 300 and display the graphical content. In some examples, presence-sensitive display 332 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures, etc.) at projector screen using capacitive, inductive, and/or optical recognition techniques and send indications of such user input using one or more communication units tocomputing device 300. - Presence-
sensitive displays sensitive display sensitive display 332 may be configured to detect an amount of force applied to presence-sensitive display 332 by a user input object, using capacitive, inductive, or other pressure sensing techniques. - As described above, in some examples,
computing device 300 may output graphical content for display at presence-sensitive display 301 that is coupled tocomputing device 300 by a system bus or other suitable communication channel.Computing device 300 may also output graphical content for display at one or more remote devices, such asprojector 320,projector screen 322,tablet device 326, andvisual display device 330. For instance,computing device 300 may execute one or more instructions to generate and/or modify graphical content in accordance with techniques of the present disclosure.Computing device 300 may output the data that includes the graphical content to a communication unit ofcomputing device 300, such ascommunication unit 310.Communication unit 310 may send the data to one or more of the remote devices, such asprojector 320,projector screen 322,tablet device 326, and/orvisual display device 330. In this way,computing device 300 may output the graphical content for display at one or more of the remote devices. In some examples, one or more of the remote devices may output the graphical content at a presence-sensitive display that is included in and/or operatively coupled to the respective remote devices. - In some examples,
computing device 300 may not output graphical content at presence-sensitive display 301 that is operatively coupled tocomputing device 300. In other examples,computing device 300 may output graphical content for display at both a presence-sensitive display 301 that is coupled tocomputing device 300 bycommunication channel 303A, and at one or more remote devices. In such examples, the graphical content may be displayed substantially contemporaneously at each respective device. For instance, some delay may be introduced by the communication latency to send the data that includes the graphical content to the remote device. In some examples, graphical content generated by computingdevice 300 and output for display at presence-sensitive display 301 may be different than graphical content display output for display at one or more remote devices. -
Computing device 300 may send and receive data using any suitable communication techniques. For example,computing device 300 may be operatively coupled to external network 304 usingnetwork link 312A. Each of the remote devices illustrated inFIG. 3 may be operatively coupled to network external network 304 by one ofrespective network links computing device 300 and the remote devices illustrated inFIG. 3 . In some examples, network links 312A-312D may be Ethernet, ATM or other network connections. Such connections may be wireless and/or wired connections. - In some examples,
computing device 300 may be operatively coupled to one or more of the remote devices included inFIG. 3 usingdirect device communication 318.Direct device communication 318 may include communications through whichcomputing device 300 sends and receives data directly with a remote device, using wired or wireless communication. That is, in some examples ofdirect device communication 318, data sent by computingdevice 300 may not be forwarded by one or more additional devices before being received at the remote device, and vice-versa. Examples ofdirect device communication 318 may include Bluetooth, Near-Field Communication, Universal Serial Bus, Wi-Fi, infrared, etc. One or more of the remote devices illustrated inFIG. 3 may be operatively coupled withcomputing device 300 bycommunication links 316A-316D. In some examples,communication links 312A-312D may be connections using Bluetooth, Near-Field Communication, Universal Serial Bus, infrared, etc. Such connections may be wireless and/or wired connections. - In accordance with techniques of the disclosure,
computing device 300 may be operatively coupled tovisual display device 330 using external network 304.Computing device 300 may output a gesture keyboard for display at presence-sensitive display 332. For instance,computing device 300 may send data that includes a representation of gesture keyboard 306B tocommunication unit 310.Communication unit 310 may send the data that includes the representation of gesture keyboard 306B tovisual display device 330 using external network 304.Visual display device 330, in response to receiving the data using external network 304, may cause presence-sensitive display 332 to output gesture keyboard 306B comprising a plurality of keys. - In response to a user performing first gesture at presence-
sensitive display 332 to select a first group of keys of the keyboard, e.g., a "forceful" (e.g., using pressure that exceeds a particular pressure threshold) <W-Key> followed by the <A-key>, the <S-key>, the <D-key>, the <R-key>, the <T-key>, the <R-key>, the <E-key>, and the <R-key>,visual display device 330 may send an indication of the gesture tocomputing device 300 using external network 304.Communication unit 310 may receive the indication of the gesture, and send the indication tocomputing device 300. Similarly, in response to a user performing a second gesture at presence-sensitive display 332 to select a second group of keys of the keyboard, e.g., a "normal" (e.g., using pressure that does not exceed a particular pressure threshold) <M-key>, followed by a <N-key>, <J-key>, <H-key>, etc., ending in the <N-key>,visual display device 330 may send a second indication of the gesture tocomputing device 300 using external network 304. - After receiving the indication of the gestures,
computing device 300 may determine a selection of the keys ofgraphical keyboard 106B. For example,computing device 100 may determine a combined selection of keys based on the indications of the first and second gestures received via communication unit 110.Computing device 300 may determine a candidate word based on the combined selection of keys from multiple gesture inputs that are non-continuous with each other (i.e., distinct from each other). -
Computing device 300 may output data indicative of pressure information along a path traversed by the first gesture tovisual display device 330 usingcommunication unit 310 andexternal network 314 to cause visual display device to output a graphical indication of the pressure information for display at presence-sensitive display 332. For example,display device 330 may cause presence-sensitive display 332 to present a line segment having a bold line weight as a graphical indication of a "forceful" gesture (e.g., having pressure that exceeds a particular pressure threshold), and a standard line weight as a graphical indication of a "normal" gesture (e.g., having pressure that does not exceed a particular pressure threshold). In another example,display device 330 may cause presence-sensitive display 332 to present a line segment of varying line weight (or color, or other varying graphical qualities, etc.) such that only certain portions of the line segment having a bold line weight to indicate a "forceful" input (e.g., having pressure that exceeds a particular pressure threshold) and other portions of the line segment having a standard line weight to indicate a "normal" gesture (e.g., having pressure that does not exceeds a particular pressure threshold). In other examples,display device 330 may cause presence-sensitive display 332 to present one or more circles at an initial portion of a gesture as a graphical indication of gesture having a "forceful" (e.g., having pressure that exceeds a particular pressure threshold) initial contact with the presence-sensitive device. -
FIG. 4 is a conceptual diagram illustrating example graphical user interface for inputting words using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure.FIG. 4 is described below in the context of computing device 100 (described above) fromFIG. 1 andFIG. 2 . -
FIG. 4 illustrates thatcomputing device 100 may outputgraphical user interface 400 for display (e.g., at UID 102).Graphical user interface 400 includes a gesture keyboard which has a plurality of keys.Computing device 100 receives an indication of a gesture selecting a first sequence of keys from the plurality of keys. For example, a user ofcomputing device 100 may provide a gesture input that traverses across a path 402 (similar to gesture input onpath 132 shown inFIG. 1 ) which includes a "forceful" input (e.g., having pressure that exceeds a certain pressure threshold) at afirst location 406. - Subsequent to the first gesture,
computing device 100 further receives an indication of a second gesture selecting a second sequence of keys from the plurality of keys. For example, the user ofcomputing device 100 may provide a gesture input that begins at alocation 408 and traverse acrosspath 404 to select a second sequence of keys that includes the "I" key, the "U" key, the "Y" key, and the "T" key (i.e., I-U-Y-T). However, in contrast to the example shown inFIG. 1 , the gesture input associated withpath 404 includes a "forceful" input (e.g., having pressure that exceeds a certain pressure threshold) during an initial portion of the gesture input, i.e., proximate tolocation 408. - As previously described,
UI module 120 may generate a sequence of touch events based on the gesture.UI module 120 may causeUID 102 to present a graphical indication ofpath 404 based on information received fromUI module 120 about the locations of the gesture detected byUID 102.Keyboard module 122 may rely on pressure information associated with the sequence of touch events generated byUI module 120 as well as a spatial model, language model, and/or other technique to determine which keys are most likely being selected. For example,keyboard module 122 may determine that pressure information for the touch events having locations proximate to the "I" key (e.g., location 408) and during the start of the gesture exceeds a pressure threshold. Based on the pressure information,keyboard module 122 may initiate a new word instead of combining the gesture with previous input (such as in the example shown inFIG. 1 ). In other words,keyboard module 122 may complete the word prediction of the previous input to yield the candidate word "water", insert a word delimiter (e.g., space), and perform word prediction on the second gesture.FIG. 4 shows the letters "w-a-t-e-r" and "i-t" within the edit region ofuser interface 400. The letters "w-a-t-e-r" and "i-t" make up a string of characters orcandidate word 430. -
FIG. 5 is a flowchart illustrating anexample operation 500 of the computing device configured to determine text using pressure information associated with a gesture input at a gesture keyboard, in accordance with one or more aspects of the present disclosure. The process ofFIG. 5 may be performed by one or more processors of a computing device, such ascomputing device 100 illustrated inFIG. 1 andFIG. 2 . For purposes of illustration only,FIG. 5 is described below within the context ofcomputing device 100 ofFIG. 1 andFIG. 2 . -
FIG. 5 illustrates thatcomputing device 100 outputs a gesture keyboard comprising a plurality of keys (502). For example,UI module 120 ofcomputing device 100 may causeUID 102 to presentgraphical user interface 104 including a gesture keyboard having a plurality of keys. -
Computing device 100 receives an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys (504). For example, a user ofcomputing device 100 may draw two strokes for the character strings "grand" and "mother" on the graphical keyboard. While the user provides a first gesture atUID 102 to input the word "grand",UI module 120 andkeyboard module 122 may receive data fromUID 102 about the input as an indication of a first gesture selecting a first sequence of keys including the <G-key>, the <R-key>, the <A-key>, the <N-key>, and the <D-key>, among others. -
Computing device 100 receives an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys (506). For example, while the user provides a second gesture atUID 102 to input the word "mother",UI module 120 andkeyboard module 122 may receive data fromUID 102 about the input as an indication of a second gesture selecting a second sequence of keys including the <M-key>, the <O-key>, the <T-key>, the <H-key>, the <E-key>, and the <R-key> among others. -
Computing device 100 may determine whether the indication of the second gestures satisfies a pressure threshold (508). In some examples,keyboard module 122 determines, based on the indication of the second gesture, an initial portion of the second gesture that corresponds to an initial contact at a pressure-sensitive input device at which the second gesture was detected.Keyboard module 122 determines whether the amount of pressure associated with the initial portion of the second gesture satisfies the pressure threshold. For example, in response to the user providing a gesture atUID 102 to input the word "mother",keyboard module 122 determines whether the amount of pressure made during the user's initial contact withUID 102, i.e., associated with the beginning of the input ("mother") proximate to the location of the <M-key>, satisfies the pressure threshold. - Responsive to determining that the pressure information satisfies the pressure threshold,
computing device 100 may determine a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys (510). For example,keyboard module 122 determines a candidate word "grandmother" which includes the combination of the first sequence selecting the <G-key>, the <R-key>, the <A-key>, the <N-key>, and the <D-key>) and the second sequence selecting the <M-key>, the <O-key>, the <T-key>, the <H-key>, the <E-key>, and the <R-key>, among others, based on the data received fromUID 102. -
Computing device 100 outputs, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys (512). For example,UI module 120 may causeUID 102 to present candidate word 130 (e.g., "grandmother") within an edit region ofuser interface 104. - Responsive to determining that the pressure information associated with the indication of the second gesture does not satisfy the pressure threshold,
computing device 100 may determine a candidate word that includes the first sequence of one or more keys from the plurality of keys and not the second sequence of one or more keys from the plurality of keys (514).Computing device 100 may refrain from outputting, for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys (516). For example,keyboard module 122 determines the candidate word "grand" based on the first sequence of keys, and which does not include the second sequence of keys selecting the <M-key>, the <O-key>, the <T-key>, the <H-key>, the <E-key>, and the <R-key>, among others.Computing device 100 may output, for display, the candidate word that includes the first sequence of one or more keys from the plurality of keys (518). For example,keyboard module 122 may causeUI module 120 to output the candidate word "grand" within an edit region ofuser interface 104. - In some examples, further responsive to determining that the pressure information associated with the indication of the second gesture does not satisfy the pressure threshold,
computing device 100 may determine a second candidate word that includes the second sequence of one or more keys from the plurality of keys.Computing device 100 may further, after outputting, for display, the second candidate word that includes the first sequence of one or more keys from the plurality of keys, output, for display, the second candidate word that includes the second sequence of one or more keys from the plurality of keys. For example,keyboard module 122 may causeUI module 120 to output the candidate word "mother" after determining and displaying the candidate word "grand." As such, in this example,computing device 100 may output for display character strings for "grand" and for "mother" as separate words based on the determination that the second gesture does not satisfy the pressure threshold. - Certain aspects of the present disclosure, such as in the description of
FIG. 5 , describe an input technique that is based on pressure information associated with an indication of a gesture input, i.e., a determination of whether the pressure information associated with the indication of the gesture input satisfies a pressure threshold. In response to determining that the pressure information associated with a second gesture input satisfies the pressure threshold, some embodiments determine a candidate word that includes a combination of a first sequence of keys and a second sequence of keys. It should be recognized that alternative embodiments may allow for other uses of the pressure information. In other embodiments, in response to determining that the pressure information does not satisfy the pressure threshold, the computing device determines a candidate word that includes a combination of the first sequence of keys and the second sequence of keys. - In another embodiment of the present disclosure,
computing device 100 may be configured to use pressure information associated with touch input on a graphical keyboard to enter input indicating a word delimiter, such as a whitespace character (sometimes referred to as a "space" character), or other punctuation. In some languages, whitespace is a common character found within many text sequences. For example, assuming the average length of English language words without a whitespace is 4 letters, which means a user can enter a whitespace character every four characters. As such, embodiments of the present disclosure provide a technique for entering whitespace input, improving input efficiency and input speed, for both touch typing and gesture typing. -
FIG. 6 is a flowchart illustrating anotherexample operation 600 of the computing device, in accordance with one or more aspects of the present disclosure. The process ofFIG. 6 may be performed by one or more processors of a computing device, such ascomputing device 100 illustrated inFIG. 1 andFIG. 2 . For purposes of illustration only,FIG. 6 is described below within the context ofcomputing device 100 ofFIG. 1 andFIG. 2 . - In another embodiment of the present disclosure,
computing device 100 may be configured to insert word delimiter characters based on gesture input on a graphical keyboard. Similar to the examples described above, the exertion of a forceful pressure (e.g., exceeding a particular threshold) during a gesture input may in some examples indicate an end to a current word, and that subsequent gesture input (e.g., finger movement) is inputting a subsequent word, without requiring the user to select a word delimiter key (e.g., space bar) between word inputs. The described technique may therefore allow a user to continuously enter multiple words without moving the input object (e.g., finger) to the space bar, or lifting the input up at the end of a word. For example, to enter "thank you" with gesture typing, the user traces the following 8 letters "thankyou" on the keyboard and applies a "high" pressure (e.g., exceeding a pressure threshold) at the end of the first word, which is the letter "k". In some examples, while the user provides a gesture atUID 102 to input the word "thank",UI module 120 andkeyboard module 122 may receive data fromUID 102 about the input as indication of a gesture selecting a sequence of keys including the <T-key>, the <H-key>, the <A-key>, the <N-key>, and the <K-key>.Keyboard module 122 may determine that pressure information included in the gesture indication indicates that an exertion of force onUID 102 while an input object (e.g., finger) is proximate to the <K-key>, and in response, determine the candidate word based on the selection of keys up until the <K-key>.Keyboard module 122 may proceed to output, for display, the candidate word "thank", along with a word delimiter (e.g., whitespace) to an edit region ofuser interface 104 and treat subsequent input (e.g., to select the <Y-key>, the <O-key>, and the <U-key> as input for determining and displaying a subsequent word "you" in the edit region ofuser interface 104. For example, consider the operations ofFIG. 6 . -
FIG. 6 illustrates thatcomputing device 100 may output a graphical keyboard comprising a plurality of keys (602). For example,UI module 120 ofcomputing device 100 may causeUID 102 to present agraphical user interface 104 that includes the graphical keyboard. -
Computing device 100 may receive an indication of a first touch input selecting one or more keys of the plurality of keys of the graphical keyboard (604). For example, the user ofcomputing device 100 may tap a sequence of keys for the words "thank" on the graphical keyboard, with a "forceful" pressure (e.g., exceeding a pressure threshold) placed on the "K" key, the last key of the word. While the user provides the touch input atUID 102 to input the word "thank,"UI module 120 andkeyboard module 122 may receive data fromUID 102 about the input as an indication of touch input selecting the keys of the graphical keyboard, including the <T-key>, the <H-key>, the <A-key>, the <N-key>, and the <K-key>. -
Computing device 100 may determine whether the received indication of the first touch input selecting one or more keys of the plurality of keys of the graphical keyboard satisfies a pressure threshold (606). For example,keyboard module 122 determines whether the amount of pressure associated with a touch event containing key-press event satisfies the pressure threshold. - Responsive to determining that the indication of the first touch input does not satisfy the pressure threshold,
computing device 100 may determine a candidate letter based on the selected key (612). For example, in response to determining that the touch events associated with key-press of any of the keys including <T-key>, the <H-key>, the <A-key>, and the <N-key>, do not satisfy the pressure threshold,keyboard module 122 may determine the candidate letters "t", "h", "a", and "n", respectively. -
Computing device 100 may output, for display, the candidate letter (614). For example,keyboard module 122 may causeUI module 120 to output the candidate letter "t" within an edit region ofuser interface 104. - Responsive to determining that the indication of the first touch input does satisfy the pressure threshold,
computing device 100 may determine a candidate letter and a word delimiter based on the received indication (608). For example, in response to determining that the touch event associated with a key-press of the <K-key> exceeds the pressure threshold,keyboard module 122 determines the candidate letter "k" and a word delimiter are the user input. In some examples, the word delimiter may be a whitespace character (e.g., space, tab, new line, or other whitespace character), although other characters including punctuation (such as a period or comma) may be used. -
Computing device 100 may output the determined candidate letter and the word delimiter (610). For example,keyboard module 122 may causeUI module 120 to output the character "k" concatenated with a whitespace character within an edit region ofuser interface 104. In some examples,keyboard module 122 may generate a graphical indication to signal to the user that the whitespace character has been automatically generated and inserted into the edit region ofuser interface 104. As a result of the described technique, the user does not need to separately entire a whitespace character to separate the words "thank" and "you" by tapping a spacebar key on the graphical keyboard, and can instead proceed directly to providing touch input on the first letter of the next word, i.e., the <Y-key>. - The described techniques may offer technical solutions for both touch typing and gesture typing forms of keyboard input. For touch typing, the
keyboard module 122 does not require a user to need to enter the space bar keypress, saving the finger travel distance, and improving the input speed. For gesture typing, the described keyboard module may allow a user to enter multiple words without traveling finger to space bar or lifting finger at the end of each word. - In one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over, as one or more instructions or code, a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol. In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media, which is non-transitory or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure. A computer program product may include a computer-readable medium.
- By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transient media, but are instead directed to nontransient, tangible storage media. Disk and disc, as used, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.
- Instructions may be executed by one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term "processor," as used may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described. In addition, in some aspects, the functionality described may be provided within dedicated hardware and/or software modules. Also, the techniques could be fully implemented in one or more circuits or logic elements.
- The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.
- Various examples have been described. These and other examples are within the scope of the following claims.
Claims (11)
- A method comprising:outputting (502), by a computing device and for display, a graphical keyboard comprising a plurality of keys;receiving (504), by the computing device, an indication of a first gesture selecting a first sequence of one or more keys from the plurality of keys;receiving (506), by the computing device, an indication of a second gesture selecting a second sequence of one or more keys from the plurality of keys; anddetermining (510), by the computing device and based on pressure information associated with the indication of the second gesture, a candidate word that includes a combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys; andoutputting (512), by the computing device and for display, the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- The method of claim 1, wherein determining the candidate word further comprises:determining whether the pressure information associated with the indication of the second gesture satisfies a pressure threshold; andresponsive to determining that the pressure information satisfies the pressure threshold, determining the candidate word that includes the combination of the first sequence of one or more keys from the plurality of keys and the second sequence of one or more keys from the plurality of keys.
- The method of claim 2, further comprising:
responsive to determining that the pressure information does not satisfy the pressure threshold:determining, by the computing device, a candidate word that includes the first sequence of one or more keys from the plurality of keys and not the second sequence of one or more keys from the plurality of keys; andoutputting, by the computing device and for display, the candidate word that includes the first sequence of one or more keys from the plurality of keys. - The method of claim 3, wherein the determined candidate word comprises a first candidate word, the method further comprising:
further responsive to determining that the pressure information does not satisfy the pressure threshold:determining, by the computing device, a second candidate word that includes the second sequence of one or more keys from the plurality of keys; andafter outputting, for display, the first candidate word that includes the first sequence of one or more keys from the plurality of keys, outputting, by the computing device, for display, the second candidate word that includes the second sequence of one or more keys from the plurality of keys. - The method of any one of claims 1-4, further comprising:determining, by the computing device, based on the indication of the second gesture, an initial portion of the second gesture that corresponds to an initial contact at a pressure-sensitive input device at which the second gesture was detected, the pressure-sensitive input device being operatively coupled to the computing device; anddetermining that an amount of pressure associated with the initial portion of the second gesture does not satisfy a pressure threshold.
- The method of any one of claims 1-5, wherein receiving the indication of the second gesture selecting the second sequence of one or more keys from the plurality of keys further comprises:
receiving, by the computing device, a plurality of touch events corresponding to the second sequence of the one or more keys from the plurality of keys, wherein at least one of the touch events includes the pressure information. - The method of any one of claims 1-6, further comprising:
outputting, by the computing device and for display, a graphical annotation to the candidate word indicating that the candidate word was determined based on multiple gesture inputs. - The method of any one of claims 1-7, wherein:
the second gesture is received subsequent to the first gesture and is distinct from the first gesture. - A computing device (100; 300) comprising at least one processor (202) configured to perform any one of the methods of claims 1-8.
- A system comprising means for performing any one of the methods of claims 1-8.
- A computer-readable storage medium (210) encoded with instructions that, when executed, cause at least one processor (202) of a computing device (100; 300) to perform any one of the methods of claims 1-8.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/210,401 US20180018086A1 (en) | 2016-07-14 | 2016-07-14 | Pressure-based gesture typing for a graphical keyboard |
PCT/US2017/041968 WO2018013832A2 (en) | 2016-07-14 | 2017-07-13 | Pressure-based gesture typing for a graphical keyboard |
Publications (3)
Publication Number | Publication Date |
---|---|
EP3485361A2 EP3485361A2 (en) | 2019-05-22 |
EP3485361A4 EP3485361A4 (en) | 2019-11-13 |
EP3485361B1 true EP3485361B1 (en) | 2023-11-22 |
Family
ID=60940589
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP17828476.6A Active EP3485361B1 (en) | 2016-07-14 | 2017-07-13 | Pressure-based gesture typing for a graphical keyboard |
Country Status (4)
Country | Link |
---|---|
US (1) | US20180018086A1 (en) |
EP (1) | EP3485361B1 (en) |
CN (1) | CN109478122A (en) |
WO (1) | WO2018013832A2 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180121000A1 (en) * | 2016-10-27 | 2018-05-03 | Microsoft Technology Licensing, Llc | Using pressure to direct user input |
US11061556B2 (en) * | 2018-01-12 | 2021-07-13 | Microsoft Technology Licensing, Llc | Computer device having variable display output based on user input with variable time and/or pressure patterns |
CN108429810B (en) * | 2018-03-15 | 2021-06-25 | 广东小天才科技有限公司 | Remote control method and system based on family education equipment |
US11554322B2 (en) | 2019-04-26 | 2023-01-17 | Sony Interactive Entertainment LLC | Game controller with touchpad input |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030165801A1 (en) * | 2002-03-01 | 2003-09-04 | Levy David H. | Fast typing system and method |
US20100238125A1 (en) * | 2009-03-20 | 2010-09-23 | Nokia Corporation | Method, Apparatus, and Computer Program Product For Discontinuous Shapewriting |
Family Cites Families (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7250938B2 (en) * | 2004-01-06 | 2007-07-31 | Lenovo (Singapore) Pte. Ltd. | System and method for improved user input on personal computing devices |
US20060055669A1 (en) * | 2004-09-13 | 2006-03-16 | Mita Das | Fluent user interface for text entry on touch-sensitive display |
JP2010521022A (en) * | 2007-02-23 | 2010-06-17 | ティーピーアイ カンパニー リミテッド | Virtual keyboard input system using a pointing device used in digital equipment |
US20100241984A1 (en) * | 2009-03-21 | 2010-09-23 | Nokia Corporation | Method and apparatus for displaying the non alphanumeric character based on a user input |
US8266528B1 (en) * | 2010-06-24 | 2012-09-11 | Google Inc. | Spelling suggestions based on an input sequence including accidental “delete” |
US8918734B2 (en) * | 2010-07-28 | 2014-12-23 | Nuance Communications, Inc. | Reduced keyboard with prediction solutions when input is a partial sliding trajectory |
GB201200643D0 (en) * | 2012-01-16 | 2012-02-29 | Touchtype Ltd | System and method for inputting text |
DE102010054859A1 (en) * | 2010-12-17 | 2012-06-21 | Rohde & Schwarz Gmbh & Co. Kg | System with gesture recognition unit |
US8782549B2 (en) * | 2012-10-05 | 2014-07-15 | Google Inc. | Incremental feature-based gesture-keyboard decoding |
US9021380B2 (en) * | 2012-10-05 | 2015-04-28 | Google Inc. | Incremental multi-touch gesture recognition |
US8843845B2 (en) * | 2012-10-16 | 2014-09-23 | Google Inc. | Multi-gesture text input prediction |
US8887103B1 (en) * | 2013-04-22 | 2014-11-11 | Google Inc. | Dynamically-positioned character string suggestions for gesture typing |
US9081500B2 (en) * | 2013-05-03 | 2015-07-14 | Google Inc. | Alternative hypothesis error correction for gesture typing |
US9176668B2 (en) * | 2013-10-24 | 2015-11-03 | Fleksy, Inc. | User interface for text input and virtual keyboard manipulation |
CN104714741A (en) * | 2013-12-11 | 2015-06-17 | 北京三星通信技术研究有限公司 | Method and device for touch operation |
US9940016B2 (en) * | 2014-09-13 | 2018-04-10 | Microsoft Technology Licensing, Llc | Disambiguation of keyboard input |
US9996258B2 (en) * | 2015-03-12 | 2018-06-12 | Google Llc | Suggestion selection during continuous gesture input |
US9760560B2 (en) * | 2015-03-19 | 2017-09-12 | Nuance Communications, Inc. | Correction of previous words and other user text input errors |
CN105100492A (en) * | 2015-08-11 | 2015-11-25 | 努比亚技术有限公司 | Pressure feedback system and pressure feedback method |
US9554273B1 (en) * | 2015-09-04 | 2017-01-24 | International Business Machines Corporation | User identification on a touchscreen device |
-
2016
- 2016-07-14 US US15/210,401 patent/US20180018086A1/en not_active Abandoned
-
2017
- 2017-07-13 WO PCT/US2017/041968 patent/WO2018013832A2/en unknown
- 2017-07-13 CN CN201780043269.XA patent/CN109478122A/en active Pending
- 2017-07-13 EP EP17828476.6A patent/EP3485361B1/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030165801A1 (en) * | 2002-03-01 | 2003-09-04 | Levy David H. | Fast typing system and method |
US20100238125A1 (en) * | 2009-03-20 | 2010-09-23 | Nokia Corporation | Method, Apparatus, and Computer Program Product For Discontinuous Shapewriting |
Also Published As
Publication number | Publication date |
---|---|
EP3485361A4 (en) | 2019-11-13 |
CN109478122A (en) | 2019-03-15 |
WO2018013832A3 (en) | 2018-02-22 |
WO2018013832A2 (en) | 2018-01-18 |
EP3485361A2 (en) | 2019-05-22 |
US20180018086A1 (en) | 2018-01-18 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108700951B (en) | Iconic symbol search within a graphical keyboard | |
US20200050661A1 (en) | Incremental Multi-Word Recognition | |
US9122376B1 (en) | System for improving autocompletion of text input | |
US10140017B2 (en) | Graphical keyboard application with integrated search | |
US20150160855A1 (en) | Multiple character input with a single selection | |
US10073536B2 (en) | Virtual keyboard input for international languages | |
US10095405B2 (en) | Gesture keyboard input of non-dictionary character strings | |
KR101484583B1 (en) | Gesture keyboard input of non-dictionary character strings using substitute scoring | |
EP3485361B1 (en) | Pressure-based gesture typing for a graphical keyboard | |
US20170336969A1 (en) | Predicting next letters and displaying them within keys of a graphical keyboard | |
WO2017181355A1 (en) | Automatic translations by keyboard | |
US10146764B2 (en) | Dynamic key mapping of a graphical keyboard | |
EP3241105B1 (en) | Suggestion selection during continuous gesture input | |
US9298276B1 (en) | Word prediction for numbers and symbols | |
WO2018053695A1 (en) | Pressure-based selection of additional characters | |
US9952763B1 (en) | Alternative gesture mapping for a graphical keyboard |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20190104 |
|
AK | Designated contracting states |
Kind code of ref document: A2Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
A4 | Supplementary search report drawn up and despatched |
Effective date: 20191014 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06F 3/0488 20130101AFI20191008BHEP |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
17Q | First examination report despatched |
Effective date: 20210125 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R079Ref document number: 602017076811Country of ref document: DEFree format text: PREVIOUS MAIN CLASS: G06F0003048800Ipc: G06F0003048860Ref country code: DERef legal event code: R079 |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06F 3/023 20060101ALI20230516BHEPIpc: G06F 3/0482 20130101ALI20230516BHEPIpc: G06F 3/04883 20220101ALI20230516BHEPIpc: G06F 3/04886 20220101AFI20230516BHEP |
|
INTG | Intention to grant announced |
Effective date: 20230619 |
|
RIN1 | Information on inventor provided before grant (corrected) |
Inventor name: BI, XIAOJUN |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20231017 |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602017076811Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20231122 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240223 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240322 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1634434Country of ref document: ATKind code of ref document: TEffective date: 20231122 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240322Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240223Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240222Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20231122Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20240322 |