US9263052B1 - Simultaneous estimation of fundamental frequency, voicing state, and glottal closure instant - Google Patents
Simultaneous estimation of fundamental frequency, voicing state, and glottal closure instant Download PDFInfo
- Publication number
- US9263052B1 US9263052B1 US13/750,000 US201313750000A US9263052B1 US 9263052 B1 US9263052 B1 US 9263052B1 US 201313750000 A US201313750000 A US 201313750000A US 9263052 B1 US9263052 B1 US 9263052B1
- Authority
- US
- United States
- Prior art keywords
- candidate
- gci
- determining
- speech
- speech signal
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/04—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using predictive techniques
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/90—Pitch determination of speech signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/03—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters
- G10L25/12—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters the extracted parameters being prediction coefficients
Definitions
- a goal of speech analysis is to determine characteristics of a speech signal that may be related to physiological properties of speech production. Such characteristics may have application in processes or operations involving speech synthesis, speech recognition, and speech encoding, possibly among others.
- Various technologies including computers, network servers, telephones, and personal digital assistants (PDAs), can be employed to implement a speech analysis system, or one or more components of such a system.
- Communication networks may in turn provide communication paths and links between some or all of such devices, supporting speech analysis system capabilities, and services that may utilize speech analysis system capabilities.
- an example embodiment presented herein provides, a method comprising: receiving, by a system including one or more processors, a speech signal comprising a first temporal sequence of speech-signal samples, each speech-signal sample having a sample time; processing the received speech signal with the one or more processors to determine (i) a second temporal sequence of candidate glottal closure instants (GCIs), each candidate GCI corresponding to a respective sample time in the first temporal sequence, (ii) for each respective candidate GCI of the second temporal sequence, a respective set of candidate fundamental frequencies (F0s) of the speech signal at the respective sample time corresponding to the respective candidate GCI, and (iii) for each respective candidate GCI of the second temporal sequence, a metric of voicing degree of the speech signal at the respective sample time corresponding to the respective candidate GCI; for each respective candidate GCI of the second temporal sequence, determining an objective function for each respective candidate F0 of the respective set, wherein the objective function comprises a respective hypothesis of a concurrence of
- an example embodiment presented herein provides, a system comprising: one or more processors; memory; and machine-readable instructions stored in the memory, that upon execution by the one or more processors cause the system to carry out operations comprising: receiving a speech signal comprising a first temporal sequence of speech-signal samples, wherein each speech-signal sample has a sample time, processing the received speech signal to determine (i) a second temporal sequence of candidate glottal closure instants (GCIs), wherein each candidate GCI corresponds to a respective sample time in the first temporal sequence, (ii) for each respective candidate GCI of the second temporal sequence, a respective set of candidate fundamental frequencies (F0s) of the speech signal at the respective sample time corresponding to the respective candidate GCI, and (iii) for each respective candidate GCI of the second temporal sequence, a metric of voicing degree of the speech signal at the respective sample time corresponding to the respective candidate GCI, for each respective candidate GCI of the second temporal sequence, determining an objective function for each respective
- an article of manufacture including a computer-readable storage medium, having stored thereon program instructions that, upon execution by one or more processors of a system, cause the system to perform operations comprising: receiving a speech signal comprising a first temporal sequence of speech-signal samples, each speech-signal sample having a sample time; processing the received speech signal to determine (i) a second temporal sequence of candidate glottal closure instants (GCIs), wherein each candidate GCI corresponds to a respective sample time in the first temporal sequence, (ii) for each respective candidate GCI of the second temporal sequence, a respective set of candidate fundamental frequencies (F0s) of the speech signal at the respective sample time corresponding to the respective candidate GCI, and (iii) for each respective candidate GCI of the second temporal sequence, a metric of voicing degree of the speech signal at the respective sample time corresponding to the respective candidate GCI; for each respective candidate GCI of the second temporal sequence, determining an objective function for each respective candidate F0 of the respective set, wherein
- FIG. 1 is a flowchart illustrating an example method in accordance with an example embodiment.
- FIG. 2 is a block diagram of an example network and computing architecture, in accordance with an example embodiment.
- FIG. 3A is a block diagram of a server device, in accordance with an example embodiment.
- FIG. 3B depicts a cloud-based server system, in accordance with an example embodiment.
- FIG. 4 depicts a block diagram of a client device, in accordance with an example embodiment.
- FIG. 5 depicts an example speech signal, an estimate of glottal flow corresponding to the example speech signal, and a time derivative of the estimated glottal flow, in accordance with an example embodiment.
- FIG. 6 illustrates the example speech signal as measured in speech-signal samples, and linear predictive code residuals of the speech signal, as measured at sample times, in accordance with an example embodiment.
- FIG. 7 is a schematic depiction of an example lattice of hypotheses of concurrent glottal closure instants, F0s, and voicing states, in accordance with an example embodiment.
- FIG. 8 depicts a block diagram of a speech synthesis system, in accordance with an example embodiment.
- FIG. 9 is a conceptual illustration of unit concatenation employing information from glottal closure instants and F0s, in accordance with an example embodiment.
- the physiology of speech production involves a dynamic mechanical process of airflow from the lungs, through the vocal tract, and ultimately out of the mouth through the lips.
- the airflow may be modulated by physical adjustments at various points in the vocal tract and at various times during the flow, resulting, for example, in temporally-varying resonant frequencies and amplitudes that combine to shape the air flow into speech.
- the physical-mechanical processes of the vocal tract are well studied and understood, the ability to accurately and reliably identify signatures of certain physical speech-production characteristics in a speech signal remains a challenge.
- the need for automatic, reliable estimates of speech-production characteristics from speech signals can have wide-ranging practical applicability in areas including speech synthesis, narrow-band speech encoding, and medical diagnostics, to name a few.
- the oscillation varies the degree of the glottal opening, which then modulates the volume of air passing through the glottis and results in periodic airflow modulation that serves as excitation for the vocal tract during what is referred to as “voiced speech.”
- the periodicity of voiced speech is characterized by a relatively abrupt closure of the glottis followed by a more gradual opening, a subsequent abrupt closure, and so on.
- Each moment in time when the glottis closes is called the “glottal closure instant” or “GCI,” and marks the start of a “closed glottis cycle.”
- F0 fundamental frequency
- F0 may be related to frequencies present in the spectrum of a speech signal, in practice it tends to be a nonlinear function of a speech signal's spectral and temporal energy distribution.
- automatic analytical determination of F0 from a speech signal can be a challenging task.
- the term “pitch” is sometimes used in reference to F0.
- F0 may be defined operationally, pitch may be more properly described in terms of listener perception of tonal agreement of pure sinusoid with a complex speech signal, and its determination may therefore be at least partially subjective.
- the term “pitch tracking” as used in the vernacular may be considered as encompassing some technical imprecision when applied to determination of F0.
- Unvoiced speech When airflow is forced through the vocal tract with sufficient velocity to generate significant turbulence, the result can be “unvoiced speech.”
- Voiced speech and unvoiced speech represent two ends of a range of voicing classification or degree (sometimes referred to as “voicedness”) that characterizes relative proportions of periodic and turbulent airflow, as well as whether voicing is trending from unvoiced to voiced (“onset”) or voiced to unvoiced (“offset”).
- onset unvoiced to voiced
- offset voiced to unvoiced
- accurate and reliable estimates of F0, GCIs, and voicing state may be obtained by simultaneous determination of all three quantities from a speech signal. More particularly, a speech signal may be processed to determine candidate GCIs and candidate F0s. Candidate GCIs may be paired with candidate F0 in hypotheses of concurrency, which may also include further hypotheses of voicing state. The hypotheses may also include one or more quality scores that can connect the hypotheses to the observed data of the speech signal, and support determination of “cost” of each hypothesis. By applying dynamic programming to a set of hypotheses, a least-cost path connecting the “best” hypotheses may be determined in a form of optimization, from which accurate and reliable estimates of GCIs, F0, and voicing state may then be obtained.
- the procedures for processing a speech signal to determine candidate GCIs and F0s, constructing and scoring the hypotheses, applying dynamic programming, and deriving the estimates, along with other ancillary and/or supporting procedures can be implemented in the form of machine-readable instructions (e.g., computer code) by one or more processors of a speech analysis system, or other type of processor-base system.
- the speech signal could be in the form of digitized samples at discrete sample times of an input sample stream, and the determined GCIs, F0s, and voicing state could be used in one or more applications, and/or stored in data file on machine-readable storage medium (e.g., magnetic, optical, or solid state disk, flash memory, etc.).
- applications that used the determined GCIs, F0s, and/or voicing state could include speech synthesis, voice encoding, and medical diagnostics.
- a speech analysis system may include one or more processors, one or more forms of memory, one or more input devices/interfaces, one or more output devices/interfaces, and machine-readable instructions that when executed by the one or more processors cause the speech synthesis system to carry out the various functions and tasks described herein.
- the functions and tasks may form a basis for simultaneous estimation of glottal closure instant (GCI), fundamental frequency (F0), and voicing state of a speech signal.
- GCI glottal closure instant
- F0 fundamental frequency
- voicing state of a speech signal An example of method for generating such an estimate is described in the current section.
- FIG. 1 is a flowchart illustrating an example method in accordance with example embodiments.
- a system having one or more processors receives a speech signal including a first temporal sequence of speech-signal samples.
- Each speech-signal sample is at a respective sample time in the first temporal sequence.
- each speech-signal sample may be a digitized measurement of a speech waveform.
- each may be referred to as a “digital sample.”
- the source of the speech waveform could be a real-time waveform, such as produced by a microphone (or other audio input device) in response to a real-time utterance spoken by a user.
- the source could be a prerecorded waveform supplied as input to the system.
- the system processes the received speech signal to determine a second temporal sequence of candidate glottal closure instants (GCIs).
- Each candidate GCI corresponds to (e.g., marks or is identified with) a respective sample time in the first temporal sequence.
- Processing of the received speech signal may also determine a respective set of candidate fundamental frequencies (F0s) for each candidate GCI of the second temporal sequence.
- processing of the received speech signal may also determine a metric of voicing degree of the speech signal at a sample time corresponding to each respective candidate GCI. That is, for each candidate GCI, a respective set of candidate F0s and a metric of voicing degree are also determined from the speech signal.
- each candidate GCI of the second temporal sequence a respective objective function is determined for each respective candidate F0 of the respective set F0 candidates.
- Each objective function includes a respective hypothesis of a concurrence of all three of the respective candidate GCI, the respective candidate F0, and a voicing state of the speech signal, and each respective hypothesis includes a GCI-period score for a correspondence between the respective candidate F0 and a subsequent candidate GCI of the second temporal sequence. More particularly, each hypothesis may be considered as a postulation that a given candidate GCI marks an actual (true) GCI, that a given candidate F0 at the time marked by the GCI is an actual F0, and that the speech signal is described by a particular voicing state at the time marked by the GCI.
- the period between successive GCIs can be related to F0, one measure of the hypothesis can be based on how well a candidate F0 corresponds to the period between successive candidate GCIs. As described below, the GCI-period score is a way to quantify this correspondence.
- a cost is determined for each respective hypothesis.
- the cost for each hypothesis is based, at least in part, on both the GCI-period score and the metric of voicing degree.
- a sequence of hypotheses corresponding to a least-cost path through the candidate GCIs is determined.
- the sequence of hypotheses includes at most one hypothesis associated with each candidate GCI. That is, each candidate GCI of the second temporal sequence is represented at most just once in the sequence of hypotheses that corresponds to the least-cost path.
- a given candidate GCI may be associated with more than one hypothesis by virtue of multiple candidate F0s associated with the given candidate GCI, only one of the possibly multiple hypotheses associated with the given candidate GCI may be included in the sequence of hypotheses that corresponds to the least-cost path.
- the procedure backtracks through the least-cost path to determine a cost-optimal set of GCIs of the received speech signal. Part of this determination may also include determination of at least one cost-optimal F0 for at least one GCI of the cost-optimal set. More particularly, a set of cost-optimal F0s may also be determined that corresponds in whole or in part to the cost-optimal set of GCIs.
- processing the received speech signal to determine the second temporal sequence of GCIs could correspond to determining linear predictive code (LPC) residuals of the speech signal at each respective sample time in the first temporal sequence, normalizing the LPC residuals (or a function of the LPC residuals as described below), and then identifying sub-sequences of consecutive values of the normalized LPC residuals that both meet a set of pulse-shape criteria and have at least one peak magnitude normalized LPC residual value that exceeds a LPC residual threshold.
- a respective GCI-quality score could be determined for each identified sub-sequence based on the respective peak magnitude normalized LPC residual value and on a respective pulse shape relative to the pulse-shape criteria.
- the sample time of the peak magnitude normalized LPC residual of each identified sub-sequence could be used to mark an associated candidate GCI, and the respective GCI-quality score of each identified sub-sequence could be associated with the corresponding candidate GCI.
- the normalized LPC residuals could be determined by normalizing the LPC residuals by a temporally local root-mean-square (RMS) measure of at least a subset of the LPC residuals. For instance, each given LPC residual (i.e., at a given sample time) could be normalized by an RMS measure over a Hann window of samples centered on the given LPC residual. Other local RMS measures could be determined as well.
- RMS root-mean-square
- the LPC residuals could be subject to a form of conditioning prior to the normalization described above. More particularly, the LPC residuals could first be polarity-corrected, whereby a mean value of the LPC residuals is subtracted from the LPC residuals to yield mean-shifted LPC residuals, and then a separate RMS calculated for positive and negative values. If the negative values yield the highest RMS, this may indicate a likely presence of GCIs, since they may be expected to be characterized by negative LPC residuals. In this case, the LPC residual values can be left unchanged. If, instead, the positive RMS is greater than the negative RMS, this may indicate that the positive components of the LPC residuals are more peaky.
- the LPC residuals may be sign-inverted (polarity reversed).
- the normalized LPC residuals may then be determined from the polarity-corrected LPC residuals. More generally, the normalized LPC residuals may be considered as being determined from a function of the LPC residuals.
- the function could be polarity correction, although other functions, including an identity function or a null function (e.g., a function that leaves the LPC residuals unchanged) may be applied as well.
- processing the received speech signal to determine the respective set of candidate F0s of the speech signal could correspond to determining a linear combination of the first temporal sequence and of the LPC residuals, then determining a normalized cross-correlation function (NCCF) of the linear combination.
- NCCF normalized cross-correlation function
- a separate NCCF computation could be centered at the respective sample time of each respective candidate GCI and carried out within a time window corresponding to a range of F0 values from a minimum F0 value to a maximum F0 value.
- peak NCCF values, or local maxima, that exceed a NCCF threshold value could be identified, and a lag time of each maximum could be associated with one of the candidate F0s for the respective candidate GCI.
- the inverse of the time difference between the respective candidate GCI and the lag time associated with any given one of the NCCF maxima could be considered the candidate F0 associated with the given NCCF peak.
- processing the received speech signal to determine the metric of voicing degree of the speech signal could correspond to subdividing the first temporal sequence into sequential frames of speech-sample signals, each of the sequential frames having a respective frame time, and then determining a band-limited RMS value of speech-sample signals within each of the sequential frames.
- a respective voicing indicator value, a respective voicing onset indicator value, and a respective voicing offset indicator value could each be determined based on the determined band-limited RMS value of each of the sequential frames.
- the metric of voicing degree could be taken to correspond to the three determined indicators. Since each sequential frame and its band-limited RMS value may correspond to multiple consecutive sample times, the metric of voicing degree associated with a given candidate GCI could be identified as a frame time closest to the respective sample time corresponding to the candidate GCI.
- determining the objective function for each respective candidate F0 of the respective set could correspond to constructing a hypothesis of a concurrence of the respective candidate GCI and the respective candidate F0, for each respective candidate F0 of the respective set.
- a GCI-period score could be determined for each constructed hypothesis.
- Each hypothesis could be further extended by a postulation that the speech signal is in a voiced state at the respective sample time of the candidate GCI.
- a postulation that the speech signal is instead in an unvoiced state at the sample time of the candidate GCI could be made for at least one of the hypotheses.
- determining the GCI-period score could correspond to determining a respective time period based on an inverse of the respective candidate F0, and determining a predicted GCI corresponding to the respective candidate F0 by adding the respective time period to the respective sample time corresponding to the respective candidate GCI. That is, the next predicted GCI following the respective candidate GCI could be estimated as one F0 time period after the candidate GCI (where the F0 time period is just the inverse of F0). Then the GCI-period score could be determined based on a temporal proximity of the predicted GCI to the subsequent candidate GCI of the second temporal sequence. Thus, the GCI-period score could be interpreted as a temporal proximity score.
- determining the cost for each respective hypothesis could be achieved by determining a respective NCCF-peak score for the respective candidate F0 based on the peak NCCF value associated with the respective candidate F0, and then merging the GCI-period score, the metric of voicing degree of the speech signal at the respective sample time corresponding to the respective candidate GCI, the respective GCI-quality score, and the respective NCCF-peak score. If the respective candidate GCI is not the first candidate GCI of the second temporal sequence, a temporally prior candidate GCI could be determined based on a prior candidate F0 associated with the temporally prior candidate GCI. Similarly, if the respective candidate GCI is not the last candidate GCI of the second temporal sequence, a temporally subsequent candidate GCI could be determined based on the respective candidate F0.
- the determination of the sequence of hypotheses corresponding to the least-cost path through the candidate GCIs could be made by determining a directed graph of all connections between candidate GCIs that traverse each candidate GCI at most once. More particularly, each connection could correspond to a respective period between a temporally-earlier candidate GCI and a temporally-later candidate GCI, where the respective period corresponds to an inverse of the candidate F0 of a given one of the hypotheses of the temporally-earlier candidate GCI.
- the inverse of each of possibly multiple F0s when added to the sample time of the given candidate GCI would yield a possible connection to a subsequent candidate GCI.
- Each respective path through the candidate GCIs would include one such connection between any particular pair of a temporally-earlier candidate GCI and a temporally-later candidate GCI, and the graphic sum of all such connections would correspond to the respective path. For each such path, a cumulative cost could be determined, and the path with the smallest cumulative cost could be selected as the least-cost path.
- a determination that the best hypothesis for a given candidate GCI corresponds to an unvoiced state could indicate that the candidate GCI is not a true GCI.
- the connection between a prior voiced GCI and the given candidate GCI could represent a transition from a voiced to an unvoiced state.
- the connection between the given candidate GCI and the next voice GCI could represent a transition from an unvoiced to a voiced state.
- determining the sequence of hypotheses corresponding to the least-cost path through the candidate GCIs in a manner as described above could be achieved by applying dynamic programming to the directed graph of connections between the sequence of hypotheses corresponding to the least-cost path through the candidate GCIs.
- backtracking through the least-cost path to determine the cost-optimal set of GCIs of the received speech signal could correspond to identifying all candidate GCIs traversed by the selected determined path.
- the cost-optimal set of GCIs could be used to facilitate and/or enhance concatenation-based speech synthesis, a speech synthesis technique based on concatenation of stored speech units.
- speech units used in concatenation could be phonemes.
- phonemes are speech segments that generally correspond to the smallest units of speech that are distinguishable from each other. There are, for example, approximately 40-50 phonemes in spoken English. Spoken words (or other segments of speech) can be constructed from appropriate sequences of subsets of phonemes.
- phonemes can be stored as small segments of audio data (e.g., in digitized form), each with an identifying phoneme label, and other ancillary information, such as context, time duration, etc.
- a sequence of phonemes may be determined that corresponds to a speech utterance being synthesized.
- GCIs, F0s, and voicing state associated with the stored phonemes, concatenation of phonemes (or other speech units) determined during synthesis can be achieved accurately. More particularly, GCIs can be used to determine temporal connection points between successive phonemes, thereby making the transition between concatenated phonemes sound like naturally produced speech.
- F0 and voicing state may facilitate more accurate determination of speech units to include in the concatenation.
- the received speech signal (e.g., at step 102 ) could be processed into phonetic units, such as phonemes.
- the received speech signal could be processed using a speech recognition system (or an implementation of a speech recognition technique).
- Each of the phonetic units could include a sub-sequence of the first temporal sequence of speech-signal samples, together with an identifying label (e.g., a phoneme label).
- the sample times of each phonetic speech unit could then be marked with one or more GCIs from the cost-optimal set, and each marked phonetic speech unit could be stored in a speech-synthesis database for later use in concatenation-based synthesis.
- Each stored speech unit could also include one or more cost-optimal F0s corresponding to the GCIs, as well as voicing state. It will be appreciated that later use of the marked phonetic speech units could include using them to concatenate (e.g., synthesize) utterances and/or phrases other than the received speech signal from which the units were derived.
- the cost-optimal set of GCIs could be used to facilitate and/or enhance narrow-band speech encoding. More specifically, the received speech signal could be processed to derive parameters for driving a narrow-band speech encoder (e.g. vocoder). The derived parameters and at least one GCI of the cost-optimal set to the narrow-band speech encoder could then be provided to the speech encoder to encode the received speech signal.
- a narrow-band speech encoder e.g. vocoder
- a further application of the cost-optimal set of GCIs, F0s and voicing state could be in medical diagnostics of speech production. More particularly, medical-diagnostic data corresponding to measurements of glottal function of a source of the speech signal during physiological production of the speech signal could be obtained in coordination with determination of the cost-optimal set of GCIs, F0s and voicing state of the speech signal. Comparison of the measurements of glottal function with one or more GCIs could then aid and/or enhance medical diagnosis and/or study based on the measurements.
- FIG. 1 is meant to illustrate a method in accordance with example embodiments. As such, various steps could be altered or modified, the ordering of certain steps could be changed, and additional steps could be added, while still achieving the overall desired operation.
- devices could be implemented using so-called “thin clients” and “cloud-based” server devices, as well as other types of client and server devices.
- client devices such as mobile phones and tablet computers
- client services are able to communicate, via a network such as the Internet, with the server devices.
- applications that operate on the client devices may also have a persistent, server-based component. Nonetheless, it should be noted that at least some of the methods, processes, and techniques disclosed herein may be able to operate entirely on a client device or a server device.
- This section describes general system and device architectures for such client devices and server devices.
- the methods, devices, and systems presented in the subsequent sections may operate under different paradigms as well.
- the embodiments of this section are merely examples of how these methods, devices, and systems can be enabled.
- FIG. 2 is a simplified block diagram of a communication system 200 , in which various embodiments described herein can be employed.
- Communication system 200 includes client devices 202 , 204 , and 206 , which represent a desktop personal computer (PC), a tablet computer, and a mobile phone, respectively.
- Client devices could also include wearable computing devices, such as head-mounted displays and/or augmented reality displays, for example.
- Each of these client devices may be able to communicate with other devices (including with each other) via a network 208 through the use of wireline connections (designated by solid lines) and/or wireless connections (designated by dashed lines).
- Network 208 may be, for example, the Internet, or some other form of public or private Internet Protocol (IP) network.
- IP Internet Protocol
- client devices 202 , 204 , and 206 may communicate using packet-switching technologies. Nonetheless, network 208 may also incorporate at least some circuit-switching technologies, and client devices 202 , 204 , and 206 may communicate via circuit switching alternatively or in addition to packet switching.
- a server device 210 may also communicate via network 208 .
- server device 210 may communicate with client devices 202 , 204 , and 206 according to one or more network protocols and/or application-level protocols to facilitate the use of network-based or cloud-based computing on these client devices.
- Server device 210 may include integrated data storage (e.g., memory, disk drives, etc.) and may also be able to access a separate server data storage 212 .
- Communication between server device 210 and server data storage 212 may be direct, via network 208 , or both direct and via network 208 as illustrated in FIG. 2 .
- Server data storage 212 may store application data that is used to facilitate the operations of applications performed by client devices 202 , 204 , and 206 and server device 210 .
- communication system 200 may include any number of each of these components.
- communication system 200 may comprise millions of client devices, thousands of server devices and/or thousands of server data storages.
- client devices may take on forms other than those in FIG. 2 .
- FIG. 3A is a block diagram of a server device in accordance with an example embodiment.
- server device 300 shown in FIG. 3A can be configured to perform one or more functions of server device 210 and/or server data storage 212 .
- Server device 300 may include a user interface 302 , a communication interface 304 , processor 306 , and data storage 308 , all of which may be linked together via a system bus, network, or other connection mechanism 314 .
- User interface 302 may comprise user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, and/or other similar devices, now known or later developed.
- User interface 302 may also comprise user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, now known or later developed.
- user interface 302 may be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices, now known or later developed.
- user interface 302 may include software, circuitry, or another form of logic that can transmit data to and/or receive data from external user input/output devices.
- Communication interface 304 may include one or more wireless interfaces and/or wireline interfaces that are configurable to communicate via a network, such as network 208 shown in FIG. 2 .
- the wireless interfaces may include one or more wireless transceivers, such as a BLUETOOTH® transceiver, a Wifi transceiver perhaps operating in accordance with an IEEE 802.11 standard (e.g., 802.11b, 802.11g, 802.11n), a WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard, a Long-Term Evolution (LTE) transceiver perhaps operating in accordance with a 3rd Generation Partnership Project (3GPP) standard, and/or other types of wireless transceivers configurable to communicate via local-area or wide-area wireless networks.
- a BLUETOOTH® transceiver e.g., 802.11b, 802.11g, 802.11n
- WiMAX transceiver perhaps operating in accordance with an IEEE 802.16 standard
- the wireline interfaces may include one or more wireline transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link or other physical connection to a wireline device or network.
- wireline transceivers such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link or other physical connection to a wireline device or network.
- USB Universal Serial Bus
- communication interface 304 may be configured to provide reliable, secured, and/or authenticated communications.
- information for ensuring reliable communications e.g., guaranteed message delivery
- a message header and/or footer e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as cyclic redundancy check (CRC) and/or parity check values.
- CRC cyclic redundancy check
- Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, the data encryption standard (DES), the advanced encryption standard (AES), the Rivest, Shamir, and Adleman (RSA) algorithm, the Diffie-Hellman algorithm, and/or the Digital Signature Algorithm (DSA).
- DES data encryption standard
- AES advanced encryption standard
- RSA Rivest, Shamir, and Adleman
- Diffie-Hellman algorithm Diffie-Hellman algorithm
- DSA Digital Signature Algorithm
- Other cryptographic protocols and/or algorithms may be used instead of or in addition to those listed herein to secure (and then decrypt/decode) communications.
- Processor 306 may include one or more general purpose processors (e.g., microprocessors) and/or one or more special purpose processors (e.g., digital signal processors (DSPs), graphical processing units (GPUs), floating point processing units (FPUs), network processors, or application specific integrated circuits (ASICs)).
- DSPs digital signal processors
- GPUs graphical processing units
- FPUs floating point processing units
- ASICs application specific integrated circuits
- Processor 306 may be configured to execute computer-readable program instructions 310 that are contained in data storage 308 , and/or other instructions, to carry out various functions described herein.
- Data storage 308 may include one or more non-transitory computer-readable storage media that can be read or accessed by processor 306 .
- the one or more computer-readable storage media may include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with processor 306 .
- data storage 308 may be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments, data storage 308 may be implemented using two or more physical devices.
- Data storage 308 may also include program data 312 that can be used by processor 306 to carry out functions described herein.
- data storage 308 may include, or have access to, additional data storage components or devices (e.g., cluster data storages described below).
- server device 210 and server data storage device 212 may store applications and application data at one or more locales accessible via network 208 . These locales may be data centers containing numerous servers and storage devices. The exact physical location, connectivity, and configuration of server device 210 and server data storage device 212 may be unknown and/or unimportant to client devices. Accordingly, server device 210 and server data storage device 212 may be referred to as “cloud-based” devices that are housed at various remote locations. One possible advantage of such “cloud-based” computing is to offload processing and data storage from client devices, thereby simplifying the design and requirements of these client devices.
- server device 210 and server data storage device 212 may be a single computing device residing in a single data center. In other embodiments, server device 210 and server data storage device 212 may include multiple computing devices in a data center, or even multiple computing devices in multiple data centers, where the data centers are located in diverse geographic locations. For example, FIG. 2 depicts each of server device 210 and server data storage device 212 potentially residing in a different physical location.
- FIG. 3B depicts an example of a cloud-based server cluster.
- functions of server device 210 and server data storage device 212 may be distributed among three server clusters 320 A, 320 B, and 320 C.
- Server cluster 320 A may include one or more server devices 300 A, cluster data storage 322 A, and cluster routers 324 A connected by a local cluster network 326 A.
- server cluster 320 B may include one or more server devices 300 B, cluster data storage 322 B, and cluster routers 324 B connected by a local cluster network 326 B.
- server cluster 320 C may include one or more server devices 300 C, cluster data storage 322 C, and cluster routers 324 C connected by a local cluster network 326 C.
- Server clusters 320 A, 320 B, and 320 C may communicate with network 308 via communication links 328 A, 328 B, and 328 C, respectively.
- each of the server clusters 320 A, 320 B, and 320 C may have an equal number of server devices, an equal number of cluster data storages, and an equal number of cluster routers. In other embodiments, however, some or all of the server clusters 320 A, 320 B, and 320 C may have different numbers of server devices, different numbers of cluster data storages, and/or different numbers of cluster routers. The number of server devices, cluster data storages, and cluster routers in each server cluster may depend on the computing task(s) and/or applications assigned to each server cluster.
- server devices 300 A can be configured to perform various computing tasks of a server, such as server device 210 . In one embodiment, these computing tasks can be distributed among one or more of server devices 300 A.
- Server devices 300 B and 300 C in server clusters 320 B and 320 C may be configured the same or similarly to server devices 300 A in server cluster 320 A.
- server devices 300 A, 300 B, and 300 C each may be configured to perform different functions.
- server devices 300 A may be configured to perform one or more functions of server device 210
- server devices 300 B and server device 300 C may be configured to perform functions of one or more other server devices.
- the functions of server data storage device 212 can be dedicated to a single server cluster, or spread across multiple server clusters.
- Cluster data storages 322 A, 322 B, and 322 C of the server clusters 320 A, 320 B, and 320 C, respectively, may be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective server devices, may also be configured to manage backup or redundant copies of the data stored in cluster data storages to protect against disk drive failures or other types of failures that prevent one or more server devices from accessing one or more cluster data storages.
- server device 210 and server data storage device 212 can be distributed across server clusters 320 A, 320 B, and 320 C
- various active portions and/or backup/redundant portions of these components can be distributed across cluster data storages 322 A, 322 B, and 322 C.
- some cluster data storages 322 A, 322 B, and 322 C may be configured to store backup versions of data stored in other cluster data storages 322 A, 322 B, and 322 C.
- Cluster routers 324 A, 324 B, and 324 C in server clusters 320 A, 320 B, and 320 C, respectively, may include networking equipment configured to provide internal and external communications for the server clusters.
- cluster routers 324 A in server cluster 320 A may include one or more packet-switching and/or routing devices configured to provide (i) network communications between server devices 300 A and cluster data storage 322 A via cluster network 326 A, and/or (ii) network communications between the server cluster 320 A and other devices via communication link 328 A to network 308 .
- Cluster routers 324 B and 324 C may include network equipment similar to cluster routers 324 A, and cluster routers 324 B and 324 C may perform networking functions for server clusters 320 B and 320 C that cluster routers 324 A perform for server cluster 320 A.
- the configuration of cluster routers 324 A, 324 B, and 324 C can be based at least in part on the data communication requirements of the server devices and cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers 324 A, 324 B, and 324 C, the latency and throughput of the local cluster networks 326 A, 326 B, 326 C, the latency, throughput, and cost of the wide area network connections 328 A, 328 B, and 328 C, and/or other factors that may contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design goals of the system architecture.
- FIG. 4 is a simplified block diagram showing some of the components of an example client device 400 .
- client device 400 may be or include a “plain old telephone system” (POTS) telephone, a cellular mobile telephone, a still camera, a video camera, a fax machine, an answering machine, a computer (such as a desktop, notebook, or tablet computer), a personal digital assistant (PDA), a wearable computing device, a home automation component, a digital video recorder (DVR), a digital TV, a remote control, or some other type of device equipped with one or more wireless or wired communication interfaces.
- POTS plain old telephone system
- PDA personal digital assistant
- DVR digital video recorder
- client device 400 may include a communication interface 402 , a user interface 404 , a processor 406 , and data storage 408 , all of which may be communicatively linked together by a system bus, network, or other connection mechanism 410 .
- Communication interface 402 functions to allow client device 400 to communicate, using analog or digital modulation, with other devices, access networks, and/or transport networks.
- communication interface 402 may facilitate circuit-switched and/or packet-switched communication, such as POTS communication and/or IP or other packetized communication.
- communication interface 402 may include a chipset and antenna arranged for wireless communication with a radio access network or an access point.
- communication interface 402 may take the form of a wireline interface, such as an Ethernet, Token Ring, or USB port.
- Communication interface 402 may also take the form of a wireless interface, such as a Wifi, BLUETOOTH®, global positioning system (GPS), or wide-area wireless interface (e.g., WiMAX or LTE).
- communication interface 402 may comprise multiple physical communication interfaces (e.g., a Wifi interface, a BLUETOOTH® interface, and a wide-area wireless interface).
- User interface 404 may function to allow client device 400 to interact with a human or non-human user, such as to receive input from a user and to provide output to the user.
- user interface 404 may include input components such as a keypad, keyboard, touch-sensitive or presence-sensitive panel, computer mouse, trackball, joystick, microphone, still camera and/or video camera.
- User interface 404 may also include one or more output components such as a display screen (which, for example, may be combined with a touch-sensitive panel), CRT, LCD, LED, a display using DLP technology, printer, light bulb, and/or other similar devices, now known or later developed.
- User interface 404 may also be configured to generate audible output(s), via a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices, now known or later developed.
- user interface 404 may include software, circuitry, or another form of logic that can transmit data to and/or receive data from external user input/output devices.
- client device 400 may support remote access from another device, via communication interface 402 or via another physical interface (not shown).
- Processor 406 may comprise one or more general purpose processors (e.g., microprocessors) and/or one or more special purpose processors (e.g., DSPs, GPUs, FPUs, network processors, or ASICs).
- Data storage 408 may include one or more volatile and/or non-volatile storage components, such as magnetic, optical, flash, or organic storage, and may be integrated in whole or in part with processor 406 .
- Data storage 408 may include removable and/or non-removable components.
- processor 406 may be capable of executing program instructions 418 (e.g., compiled or non-compiled program logic and/or machine code) stored in data storage 408 to carry out the various functions described herein. Therefore, data storage 408 may include a non-transitory computer-readable medium, having stored thereon program instructions that, upon execution by client device 400 , cause client device 400 to carry out any of the methods, processes, or functions disclosed in this specification and/or the accompanying drawings. The execution of program instructions 418 by processor 406 may result in processor 406 using data 412 .
- program instructions 418 e.g., compiled or non-compiled program logic and/or machine code
- program instructions 418 may include an operating system 422 (e.g., an operating system kernel, device driver(s), and/or other modules) and one or more application programs 420 (e.g., address book, email, web browsing, social networking, and/or gaming applications) installed on client device 400 .
- data 412 may include operating system data 416 and application data 414 .
- Operating system data 416 may be accessible primarily to operating system 422
- application data 414 may be accessible primarily to one or more of application programs 420 .
- Application data 414 may be arranged in a file system that is visible to or hidden from a user of client device 400 .
- Application programs 420 may communicate with operating system 412 through one or more application programming interfaces (APIs). These APIs may facilitate, for instance, application programs 420 reading and/or writing application data 414 , transmitting or receiving information via communication interface 402 , receiving or displaying information on user interface 404 , and so on.
- APIs application programming interfaces
- application programs 420 may be referred to as “apps” for short. Additionally, application programs 420 may be downloadable to client device 400 through one or more online application stores or application markets. However, application programs can also be installed on client device 400 in other ways, such as via a web browser or through a physical interface (e.g., a USB port) on client device 400 .
- FIG. 5 illustrates a relation between a speech signal 502 , the corresponding glottal volume velocity U(t) 504 , and the time derivative U′(t) 506 .
- a time duration of approximately 0.025 seconds applies to all three plots in FIG. 5 .
- the speech signal corresponds to production of the phoneme /a/.
- the glottal volume velocity U(t) 504 shows periodic amplitude variations that rise gradually, corresponding to the opening of the glottis, and that drop sharply, corresponding the rapid closing of the glottis.
- the derivative of glottal volume velocity U′(t) 506 shows relatively gradual positive rises corresponding to the gradual increases in glottal volume velocity U(t) 504 , and sharp negative peaks corresponding to glottal closures. This trend and pulse shape in the derivative of glottal volume velocity suggests that times corresponding to the negative peaks can be associated with GCIs of the speech signal.
- the digital samples could be obtained by digitally sampling an analog speech signal at the discrete sample times, for example using a digital signal processor.
- the speech signal could correspond to a spoken utterance, such as a phoneme, a word, a phrase, a sentence, or another segment of speech.
- the time between successive samples is the sampling period, and its inverse is the sample frequency or sampling rate.
- Typical sampling rates for speech signals may range from 8 kHz (kilo samples per second) to 22.05 kHz, although other sampling rates could be used.
- FIG. 6 illustrates a digital speech signal 602 .
- Each sample is represented by a vertical line with dot marking a positive or negative amplitude relative to a horizontal line at zero amplitude.
- the digital speech signal could correspond to a digitally sampled version of speech signal 502 in FIG. 5 , for example.
- the sampling rate shown is evidently much smaller than 8 kHz. This should not be viewed as a limitation with respect to the example embodiments described herein.
- LPC linear predictive code
- a digital speech signal s(i) such as the digital speech signal 602
- the derivative of the volume velocity may be approximated by computing linear predictive code (LPC) residuals of the sequence of speech-signal samples.
- LPC residuals of a digital speech signal s(i) may be carried out according well-known LPC residual analytical techniques, as implemented in machine-language instructions (e.g., computer code) executable by one or more processors, for example.
- LPC residual computation 603 may be applied to the digital speech signal 602 to generate LPC residual samples 604 .
- N ⁇ 1 could correspond to a discrete digital approximation of the time derivative U′(t) 506 in FIG. 5 , for example. Note that the s(i) and r(i) are phase-aligned and have the same sampling times t i .
- LPC residuals of a digital speech signal may provide a basis for initial identification of GCIs in the digital speech signal. More particularly, analysis of LPC residuals can be used to identify negative peaks that may mark time instants in the digital speech signal that correspond to GCIs in the production of the original speech signal that is represented in the digital samples of the digital speech signal. As described below, determining that what appears to be a GCI in the LPC residuals actually or likely corresponds to a true GCI may involve further analysis of the data in accordance with principles and techniques of example embodiments herein.
- initial identification of GCIs from LPC residuals may be considered “candidate” GCIs.
- candidate GCIs in the LPC residual samples 604 are circled and their times indicated by vertical arrows.
- longer speech signal e.g., multiple phonemes, a word, a sentence, etc.
- the illustrative description of just three is not limiting with respect to example embodiments discussed herein.
- the fundamental frequency F0 of the produced speech is related to the spectral and temporal energy distribution of the speech, but generally not as a linear combination of frequency components.
- the somewhat descriptive property referred to as “pitch” may be defined in terms of listener perception, and does not necessarily recommend a convenient or rigorous analytical approach to determining F0 from a digital speech signal.
- F0 may be related in the inverse to the period between GCIs, suggesting that F0 values and GCIs in a digital speech signal may be determined together through a form of optimization.
- “candidate” F0s determined from a digital speech signal may be computationally linked with candidate GCIs from the signal within the framework of an optimization problem, whereby solving the optimization problem may yield optimal determinations of both the GCIs and F0s.
- the voicing state of the produced speech may be introduced to help discriminate among optimization paths of the framework.
- the voicing state can be related to the relative proportions of periodic and turbulent airflow in speech production, it is possible to analytically connect voicing state to both candidate GCIs and F0s, and thereby provide an additional basis for their evaluation within the optimization context. For example, during unvoiced speech, evidence of periodicity between candidate GCIs might be expected to be lacking. Similarly, correlations between candidate GCIs and candidate F0s might also be weaker for unvoiced than for voiced speech.
- the optimization problem may be constructed analytically as a collection of hypotheses, each of which hypothesizes a concurrence of a candidate GCI, a candidate F0, and a voicing state, and which further includes a computational basis for determining an associated cost.
- the collection may be constructed to represent possible GCIs, F0s, and voicing states present in a digital voice signal, and each hypothesis can thus be considered as marking a particular sample time of the digital speech signal.
- each hypothesis may have a “link” to a temporally prior and/or a temporally later hypothesis of the collection, whereby each of one or more sequences of links may represent a respective path through the collection of hypotheses.
- the cost of each hypothesis provides a quantitative evaluation of the hypothesized concurrence, and accounts for a quantitative evaluation of links to temporally prior and/or temporally later hypotheses.
- a least-cost path may be determined, from which an optimal set GCIs and F0s may be derived.
- the analytical framework and techniques outlined above may be described operationally in terms of an algorithm that can be implemented as machine-language instructions (e.g., computer code) executable by one or more processors, for example.
- Such an algorithm for simultaneous estimation of GCI, F0, and voicing state of a speech signal in accordance with example embodiments is described below.
- s(i) could correspond to a spoken utterance, such as a word, a phrase, or sentence, that has a duration of one or more seconds (e.g., 1-10 seconds). More generally, the algorithm may benefit when one or more portions of an utterance can provide context for other portions. This could correspond to utterances that may be expected to include tens of true GCIs, for example. However, the algorithm does not necessarily require this to be the case, and other forms of shorter utterances are possible as well, such as phonemes or triphones, for example.
- the algorithm can be described as having six phases.
- the signal s(i) is obtained in a form described above.
- the signal could be obtained from real-time speech production, or from a prerecorded speech signal, for example.
- the second phase corresponds to preliminary processing of the signal, which can included all-pass filtering of s(i) to correct possible phase distortion introduced, for example, during acquisition by a microphone or during recording.
- the second phase can additionally or alternatively include high-pass filtering of s(i) to remove possible low-frequency rumble and DC (direct current) distortion.
- these filtering actions do not alter s(i) in a manner necessarily required by, or disruptive to, the subsequent phases of the algorithm. As such, they may be considered optional, their necessity and/or desirability being determined by the nature and quality of s(i) as received or obtained.
- Techniques for all-pass and high-pass filtering of digitized signal such as s(i) are generally known, and not discussed further herein.
- the speech signal is processed to determine the candidate GCIs, candidate F0s, and metrics of the degree of voicing at times corresponding to the candidate GCIs.
- a lattice of hypotheses is created in preparation for solving an optimization problem for simultaneously optimizing GCI, F0, and voicing state.
- dynamic programming is used to solve the optimization problem by determining a least-cost path through the lattice.
- an optimal set of GCIs, as well as F0s and voicing state are determined by backtracking through the least-cost path.
- LPC residuals are computed from s(i) according to known computational methods.
- the polarity of r(i) may then be adjusted to reflect the relative levels of positive and negative excursions present. More specifically, an overall mean can be subtracted from r(i), and a separate RMS computed for positive and negative values. If the RMS of the positive values exceeds that of the negative values, r(i) may be inverted in place to yield a polarity-corrected version of r(i).
- a Hann window of 20 milliseconds (ms) may be appropriate, although other window sizes are possible as well.
- the normalized, polarity-corrected LPC residuals, nr(i) provide a basis for determining candidate GCI pulses, as described below.
- the mixture signal may be used as a basis for a search for candidate F0s, as described below.
- the feature frames may be arranged in an overlapping fashion, each having a duration w ff and an interval from on to the next of w b ⁇ f s .
- f b could be 500 Hz and w ff could be 25 ms, although other values are possible as well.
- Each feature frame could be identified with a frame time; for example frame times could be the times at the center of each feature frame. In the example above, these would correspond to times at 12.5 ms, 14.5 ms, 16.5 ms, and so on. Other frame-time definitions could be used as well.
- the Hann window could correspond to the duration of each feature frame w ff . For the above example this would correspond to a width of 25 ms, although other values could be used.
- b RMS (j) tends to be well correlated with the presence and amplitude of voicing in a speech signal, such as s(i).
- a voicing indicator, p v (j) can be determined as a pseudo-probability corresponding to “voicedness” of the speech represented in the j th feature frame, where voicedness falls in a range from completely unvoiced speech to completely voiced speech.
- a voicing onset indicator, p von (j) can be determined as a pseudo-probability corresponding to a likelihood that the j th feature frame corresponds to an onset of voicing, where onset corresponds to a transition from unvoiced to voiced speech.
- a voicing offset indicator, p voff (j) can be determined as a pseudo-probability corresponding to a likelihood that the j th feature frame corresponds to an offset of voicing, where offset corresponds to a transition from voiced to unvoiced speech.
- the voicing indicator can be computed as:
- p v ⁇ ( j ) max ⁇ ⁇ 0 , [ b RMS ⁇ ( j ) - floor ⁇ ( min ⁇ ( b RMS ) ) ] range ⁇ , [ 1 ]
- the constant c floor 20.0, although other values could be used.
- b RMS (j) At voicing onset, b RMS (j) will generally tend to be increasing.
- c s is a slope factor
- i off is an index offset corresponding to an offset between frames used to sense the slope of b RMS (j).
- c s 30.0, although other values could be used.
- p voff (j) max ⁇ 0,min[1.0, ⁇ b ] ⁇ .
- the candidate GCIs may be determined from the normalized, polarity-corrected LPC residuals, nr(i), by applying a set of criteria relating to peak values and pulse shape, where pulse shape can be evaluated by comparing neighboring samples of nr(i). More specifically, GCIs may be expected to be pulses with high amplitude compared to a background, and skewed in pulse shape such that they descend more slowly than they rise. As defined above the values of nr(i) may be considered as measuring standard deviations estimated locally in r(i).
- the frequency constant c f could have a value of 0.0004, although other values could be used as well. All samples of nr(i) with values that meet these criteria may be considered a respective candidate GCI.
- Each identified candidate GCI will have respective goodness scores for value, prominence, and skew, determined according to the ranking definitions above.
- a respective data structure e.g., organized storage
- Each respective data structure may be used to record the information listed in Table 1.
- the window duration is set so as to include enough signal samples to yield reasonable correlation estimates, while helping limit possible negative effects of including more than one GCI in the window.
- the NCCF for all the gc(k) can thus be computed as a two-dimensional array cc(k, l), where, for each gc(k), there are l 2 ⁇ l 1 +1 NCCF values.
- N c int(f s ⁇ w dur ) samples spanning the time window.
- the NCCF may be expressed analytically as:
- each such value may mark a time period with respect to the sample time of gc(k) that corresponds to the inverse of a candidate F0.
- the possible inverse F0 candidates indexed in d(k,m) may then be related to candidate F0s by how well they correlate with possible periods between successive candidate GCIs.
- the subset of candidate GCIs against which the NCCF for a given k is compared is related to a range of expected F0s.
- the subset of candidate GCIs corresponds to all those for which ik+l 1 ⁇ ikn ⁇ ik+l 2 .
- the inverse of this interval can therefore be taken to correspond to a possible F0 at the sample time indexed by ik.
- each respective candidate GCI and each corresponding candidate F0 in this manner can be viewed as completing the third phase of the algorithm and beginning the fourth phase. More particularly, each of the determinations that complete the third phase can also be taken as forming a respective hypothesis of a concurrency of the respective candidate GCI and each of the corresponding candidate F0s.
- a lattice of alternative hypotheses is constructed based on each respective hypothesis of concurrency of a respective candidate GCI and each respective corresponding candidate F0.
- Each hypothesis is further extended to include a postulation of a voicing state.
- Each hypothesis may also include a cost based on one or more quality scores and/or cost functions, as described below.
- the lattice can be considered as having two dimensions.
- One dimension is epoch (time), along which each hypothesized candidate GCI is located in temporal order.
- the other dimension is F0, along which the hypothesized candidate F0s associated with each hypothesized candidate GCI are located.
- the hypothesized candidate GCIs may not necessarily all have the same number of hypothesized candidate F0s.
- all but one of the GCI-F0 hypotheses includes a postulation of voiced speech.
- One additional GCI-F0 hypothesis at each epoch includes a postulation of unvoiced speech.
- the lattice thus sets up an optimization problem for simultaneously optimizing GCI, F0, and voicing state of a speech signal.
- each hypothesis also includes one or more measures, scores, or rankings of the hypothesized quantities. These may be used to determine a local cost for each hypothesis, which may be applied during optimization.
- Each hypothesis also includes “links” to temporally different hypotheses, where the links can be thought of as representing possible segments of progression across the temporal dimension of the lattice, in correspondence with the voice-production dynamics in the speech signal. Different paths across the lattice may be constructed from different sequences of connected inter-hypothesis links.
- a cost for each given path may be determined base on the costs of the hypotheses traversed by the given path, and the costs associated with the links in the given path. Determination of the path with the least cost, which may be considered optimal, occurs during the fifth phase of the algorithm (described later). Construction of the lattice in the fourth phase of the algorithm involves determining the various hypotheses, their associated local costs, and identification of their links.
- a local cost c local may be determined for each voiced-speech hypothesis based on the GCI-quality scores q(k) and q(kn) of gc(k) and gc(kn), the NCCF peak value ccvn(k), the duration of the period implied by ⁇ ik , a score for temporal proximity between gc(kn) and the inverse of F0, and the metric of degree of voicing (p v , p von , p voff ).
- NCCF peak or local maximum
- ccvn(k) The definition of the NCCF peak (or local maximum) ccvn(k) has been given above.
- the parameter a peak is a constant, which, by way of example, could be 1.0, although other values could be used.
- a GCI-period score c GCI-period may be determined for each hypothesis as follows.
- the candidate GCIs included in each hypothesis corresponds to a respective gc(k), and a gc(kn) satisfying ik+l 1 ⁇ ikn ⁇ ik+l 2 for the respective gc(k).
- the GCI-period score could be defined as:
- w period is a weighting constant.
- An example value of w period 1.0 could be used, although other values are possible as well.
- Other quantitative definitions of c GCI-period are also possible.
- measures of hypothesis quality for integer multiples of a true glottal period may tend to have similar values.
- the period cost may help favor shorter periods, and thereby increase the likelihood of identifying a true period.
- the local cost c local given by equation [10] may be seen to have components that depend on both residual peak quality and the value of the NCCF at the hypothesize F0 period.
- organized storage may be created for each hypothesis that includes a postulation of voiced speech.
- the organize storage could be a data structure, for example.
- additional storage e.g., a data structure
- Table 2 An example of the organization of each voiced hypothesis data structure is illustrated in Table 2.
- each hypothesis data structure that links the respective hypothesis of the data structure with possible past and future GCI peaks. More particularly, a link is added that identifies the next (future) GCI peak to which the hypothesis may connect by virtue of the hypothesized GCI period.
- One or more links may also be added that identify all previous (past) GCI peaks that may connect to the GCI peak of respective hypothesis by virtue of the hypothesized GCI periods associated with those previous (past) GCI peaks.
- the links may take the form of pointers, for example.
- the local cost c local of all the voiced hypotheses at each given epoch are compared, from which the voiced hypothesis with the lowest cost at each given epoch may be identified.
- the voiced hypothesis with the lowest cost is then used as sort of template for an unvoiced hypothesis at the given epoch. More particularly, a data structure (or other form of organized storage) for an unvoiced hypothesis may be created. An example of the organization of an unvoiced hypothesis data structure is illustrated in Table 3.
- the local cost for unvoiced speech, c U-local may differ form that for voiced speech.
- the weighting factor w uv may be set to 0.9, although other values could be used.
- jk again identifies the temporally closest feature frame to ik, and may thereby be used to associate voicing metric (p v , p von , p voff ) determined for the jk th feature frame with gc(k).
- the reward r is as defined above for c voice .
- the dynamic programming of the fifth phase of the algorithm may be carried in order to determine a least-cost path through the lattice.
- a general outline of this procedure is described below. A detailed description is omitted here, since techniques of dynamic programming are generally known.
- Each hypothesis at a given epoch may have an identified link (e.g., pointer) to one subsequent GCI peak at a subsequent epoch, and one or more earlier GCI peaks back to one or more earlier epochs.
- a combined hypothesis-link cost for every link back to an earlier epoch may be determined.
- the combined hypothesis-link cost for a given link may include a contribution from the local cost (c local or c U-local ) and a transition-cost contribution corresponding to a transition from the earlier epoch to which the given link connects. Since each link back to an earlier epoch is a link to a hypothesis at that earlier epoch, the link may be considered to entail a transition between the voicing state of the hypothesis at the earlier epoch and the voicing state of the hypothesis at the given epoch.
- transitions Four types of transitions may be considered: voiced ⁇ voiced, voiced ⁇ unvoiced, unvoiced ⁇ voiced, and unvoiced ⁇ unvoiced. As described below, the cost of each link may differ based on the type of transition, as well as scores and rankings of the hypothesis at the given epoch.
- the hypothesis-link cost with the least cost may be used to respectively identify a “favored” backward link for each hypothesis at a given epoch and the hypothesis-link cost for that favored backward link.
- favored backward links for all hypotheses of the lattice may be arranged in one or more connected sequences that correspond to one or more paths through the lattice, each path traversing a given epoch just once.
- Each path may have a path cost that depends on the connected links in the path.
- the path with the least cost from among the one or more paths may then be considered an optimal path that identifies a best estimate of a temporal sequence of GCIs, F0, and voicing states represented in the original speech signal.
- the four types of transition costs may be determined based on parameters of the hypotheses connected by the links. More particularly, the transition costs for a voiced ⁇ voiced link, an unvoiced ⁇ voiced link, a voiced ⁇ unvoiced link, and an unvoiced ⁇ unvoiced link may be respectively given as:
- the least-cost path through the hypotheses of the lattice may be traversed backward in order to identify an optimal set of GCIs from the sequence of hypotheses connected by way of the least-cost path.
- This backtracking procedure is carried out as part of the sixth phase of the algorithm.
- the GCIs identified by backtracking across the least-cost path may be considered as a best estimate of true GCIs that occur during production of the original speech signal.
- the inverse of the identified GCI at each given epoch may be taken as an estimate of the true F0 at that given epoch.
- Each F0 estimate may be further refined by reference to a closest matching NCCF peak from among the NCCF peaks associated with the GCI at each given epoch.
- the voicing states (voiced or unvoiced) of the hypotheses connected by way of the least-cost path may be considered as best estimates of the true voicing states at the epochs of the optimal GCIs.
- the example algorithm may be seen as simultaneously estimating GCIs, F0s, and voicing states of a speech signal.
- FIG. 7 A conceptual illustration of the lattice and example connections between hypotheses at different epochs is shown in FIG. 7 .
- four epochs, 702 , 704 , 706 , and 708 are depicted along the horizontal direction.
- the epoch 702 is marked by a candidate GCI labeled “Candidate-GCI(1),” where the index “1” indicates that this is the first candidate GCI of an example sequence of candidate GCIs.
- epoch 704 is marked by a candidate GCI labeled “Candidate-GCI(2),” and epoch 706 is marked by a candidate GCI labeled “Candidate-GCI(3).”
- the epoch 708 is marked by a candidate GCI labeled “Candidate-GCI(L),” where the index L indicates the last candidate GCI of the example sequence.
- a set of hypotheses, 702 - 1 , 702 - 2 , 702 - 3 , 702 - 4 , and 702 - m 1 is constructed at the epoch 702 , and depicted along the vertical direction in the figure.
- Each hypothesis includes a concurrency of the candidate GCI at the epoch 702 , a candidate F0, a voicing state, and a local cost.
- the hypothesis 702 - 1 labeled “Hypothesis (1,1),” includes a concurrency of Candidate-GCI(1), F0(1,1), Voiced State, and Cost(1,1).
- the hypothesis 702 - 2 labeled “Hypothesis (1,2),” includes a concurrency of Candidate-GCI(1), F0(1,2), Voiced State, and Cost(1,2)
- the hypothesis 702 - 3 labeled “Hypothesis (1,3),” includes a concurrency of Candidate-GCI(1), F0(1,3), Voiced State, and Cost(1,3)
- the hypothesis 702 - 4 labeled “Hypothesis (1,4),” includes a concurrency of Candidate-GCI(1), F0(1,4), Voiced State, and Cost(1,4).
- the last hypothesis of the set, 702 - m 1 corresponds to an unvoiced state, and includes a concurrency of Candidate-GCI(1), F0(1,m 1 ), Unvoiced State, and Cost(1,m 1 ).
- hypotheses at the epochs 702 , 704 , 706 , and 708 are similar explanations, except that the first index of each hypothesis identifies the index of the epoch.
- the hypothesis 704 - 1 labeled “Hypothesis (2,1),” includes a concurrency of Candidate-GCI(2), F0(2,1), Voiced State, and Cost(2,1), and so on.
- the last index at each epoch in this illustration is labeled m 1 , m 2 , m 3 , and m L , respectively. Each of these could be different, although not necessarily.
- curved arrows represent links or connections between hypotheses at different epochs, along what may be considered for purposes of illustration a least-cost path.
- the link 703 is shown as connecting Hypothesis(1,3) at the epoch 702 with Hypothesis (2,2) at the epoch 704 .
- the link 703 corresponds to a voiced ⁇ voiced transition.
- the link 705 is shown as connecting Hypothesis(2,2) at the epoch 704 with Hypothesis (3,m 3 ) at the epoch 706 .
- the link 705 corresponds to a voiced ⁇ unvoiced transition.
- the link 707 is shown as connecting Hypothesis(3,m 3 ) at the epoch 706 with Hypothesis (L,1) at the epoch 708 .
- the link 707 corresponds to an unvoiced ⁇ voiced transition.
- the ellipses in the link 707 suggest that there could be other transitions between the epoch 706 and 708 corresponding to possible additional epochs, omitted from the figure for the sake of brevity.
- an optimal set of GCIs, F0s, and voicing state could be identified by backtracking across the connected hypotheses in the lattice.
- the apparent connection of successive candidate GCIs in FIG. 7 may therefore considered as illustrative and not necessarily requiring inclusion of the candidate GCI at every epoch of the lattice.
- An example of an application of simultaneously estimated GCIs, F0s, and voicing states of a speech signal in accordance with example embodiments may be illustrated in the context of speech synthesis. More particularly, in concatenation-based speech synthesis, short segments of prerecorded speech are concatenated to generate a desired utterance of synthesized speech.
- the prerecorded segments may be stored in a speech database, and each may include a respective phonetic label that identifies its phonetic content.
- Speech unit Each speech segment and its phonetic label, possibly as well as other, ancillary information, is referred to as a “speech unit.”
- the collection of speech units in the database may be viewed a sort of toolkit of recorded speech elements that may be analytically “mixed and matched” in order to construct synthesized speech corresponding to specified input, such as a text string.
- a concatenation-based synthesis system may operate by translating input text into a sequence of phonetic labels, possibly including contextual (or other) information, which can be used to identify and select, by one or another set of criteria, a sequence of speech units from the speech database.
- the recorded speech segments from the selected speech units can then concatenated into a synthesized waveform, and the waveform played out as the synthesized speech corresponding to the input text string.
- the process of selecting speech units may be made more reliable by the inclusion of F0 and voicing state among the ancillary information in each speech unit of the database.
- concatenating speech segments of the selected units so as to generate natural sounding speech may be significantly aided by inclusion (or identification) of GCIs of the speech segments in the speech units of the database.
- FIG. 8 depicts a block diagram of an example speech synthesis system 800 in which an example embodiment of speech synthesis using simultaneously determined GCIs, F0s, and voicing state could be applied.
- FIG. 8 also shows selected example inputs, outputs, and intermediate products of example operation.
- the functional components of the speech synthesis system 800 include a speech database 802 , a unit selection module 804 , a text analysis module 806 , and a concatenative speech generation module (speech synthesizer) 808 .
- speech synthesizer concatenative speech generation module
- the machine-language instructions could be stored in one or another form of a tangible, non-transitory computer-readable medium (or other article of manufacture), such as magnetic or optical disk, or the like, and made available to processing elements of the system as part of a manufacturing procedure, configuration procedure, and/or execution start-up procedure, for example.
- a tangible, non-transitory computer-readable medium such as magnetic or optical disk, or the like
- a speech synthesis system such as system 800 may be prepared for run-time operation with run-time input (e.g., run-time text strings) by populating the database with speech units, and tuning or “training” the unit selection procedure to do a good job of unit selection (where “good” may be defined by one or more specific measures, for example).
- run-time input e.g., run-time text strings
- tuning or “training” the unit selection procedure to do a good job of unit selection (where “good” may be defined by one or more specific measures, for example).
- speech recitations may be recorded by a human who follows (e.g., reads) textual scripts.
- the speech recitations may be digitized and recorded as collections (e.g., data files) of digital samples.
- a computer-readable pronunciation dictionary may be used to automatically convert each textual script into an equivalent (or corresponding) sequence of phonetic units (e.g., phonemes), each having a unit label.
- Speech recognition technology may then be used to automatically align the phonetic units with the corresponding recorded digital speech recitation (or portion thereof, for example).
- boundaries between the phonetic units of the sequence may be identified as a sequence of time marks across the sequence of digital samples that make up the recorded recitation.
- the time marks may then serve to delineate labeled sub-segments of the digital sequence that correspond to respective, labeled phonetic units.
- the labeled sub-segments may be referred to as “source units,” and the recorded recitation as “source speech.”
- each speech unit may then be generated and stored in the speech database.
- Each speech unit may include time marks that delineate the associated source unit.
- each speech unit may be associated with a unit of recorded speech by virtue of an identified sub-segment of the recorded source speech.
- each speech unit may not necessarily include an actual copy of the digital samples of the associated source unit, but rather two (or possibly more) time marks that delineate a sub-sequence of recorded digital samples of the source speech.
- the source speech may also serve as input to other forms of analysis, including simultaneously determination of GCIs, F0s, and voicing state in a manner described above. In particular, such determinations may be used help refine identification of phone boundaries. Additional analysis may be used to determine energy (e.g., loudness) of the source speech, as well as various spectral measures that may be further used later to help match unit boundaries at run-time. Context information, such as word identity, syllable position, phrase position, etc., may also be determined. Some or all of the above information (and possibly other information about the source speech as well) may be included in the speech units derived for the recorded source speech, along with the time marks described above.
- each speech unit may include GCIs, F0s, and voicing state identification specific to the speech unit.
- the above process may be carried out for multiple speech recitations.
- the larger the number the larger the speech database (e.g., speech database 802 ), and the larger the body of speech units available during run-time synthesis.
- a run-time text string 801 may be input to the text analysis module 806 during run-time speech synthesis.
- the text analysis module 806 analyzes the run-time text string 801 and thereby generates a target unit specification 803 , which represents the speech that should be synthesized.
- the target unit specification 803 may include most or all of the attributes that can be inferred from the text, possibly including some features or combinations of features that might not identically exist in the speech database 802 .
- the target unit specification 803 is then input to the unit selection module 804 , which performs run-time unit selection 805 to identify and select units from the speech database 802 that represent a determination of speech units from which speech corresponding to the run-time text string 801 may be synthesized.
- the speech units selected in this manner form run-time predicted speech units 807 output by the unit selection module 804 .
- a matrix can be constructed in which the columns, corresponding to the target unit specification 803 , contain labels of exact or approximately matching phonetic unit labels in the database. Dynamic programming across this matrix of variable length columns may be applied to find a lowest cost (best match) path.
- Target costs in this search can be feature-based differences between prospective, target speech units from the database 802 and the target unit specification 803 . Transition costs may be computed from features including F0 and spectrum-shape measured at endpoints of the prospective speech units that would be joined (i.e., concatenated). voicing state may also be used in unit selection by examining context information that may be associated with the target unit specification 803 . Finally, backtracking may be carried out to extract the “best” sequence of speech units, which corresponds to the run-time predicted speech units 807 in the illustration in FIG. 8 .
- unit selection techniques could include statistical modeling base on hidden Markov models (HMMs), machine learning, for example using neural networks (NNs), and hybrid techniques using both HMMs and NNs.
- HMMs hidden Markov models
- NNs neural networks
- hybrid techniques using both HMMs and NNs.
- the unit selection module 804 may be trained or tuned to generate reliable and/or accurate results based on known inputs.
- the run-time predicted speech units 807 are next input to the concatenative speech generation module (speech synthesizer) signal generation module 808 , which may then synthesize a run-time waveform 809 .
- the run-time waveform 809 may thereby be a concatenation of speech segments of the run-time predicted speech units 807 that can be played out by an audio output device, for example.
- the quality or naturalness of the sound of run-time waveform 809 can depend, at least in part, on how well connection points of adjacent speech segments of the concatenated sequence match and fit together.
- the quality of the segment-to-segment connections can be improved by aligning the connection points at GCIs of the segments.
- the concatenative speech generation module (speech synthesizer) signal generation module 808 may apply the GCIs and F0s of the run-time predicted speech units 807 to facilitate high-quality, concatenation-based speech synthesis.
- FIG. 9 is a conceptual illustration of unit concatenation employing GCIs.
- a sequence 901 of run-time predicted speech units is input to a concatenative speech generation module (speech synthesizer) 904 .
- the sequence 901 includes speech units 901 - 1 , 901 - 2 , 901 - 3 , 901 - 4 , 901 - 5 , and 901 - 6 , each of which is depicted by a cartoon-like rendering of a segment of a digitized speech.
- the particular forms of the signals in the speech units are illustrative, and do not necessarily depict actual speech signals.
- Each speech unit includes two GCIs labeled “a” and “b” and marked by respective vertical arrows.
- speech unit 901 - 1 includes GCIs a 1 and b 1 ; speech unit 901 - 2 includes GCIs a 2 and b 2 ; speech unit 901 - 3 includes GCIs a 3 and b 3 ; speech unit 901 - 4 includes GCIs a 4 and b 4 ; speech unit 901 - 5 includes GCIs a 5 and b 5 ; and speech unit 901 - 6 includes GCIs a 6 and b 6 .
- There could be other GCIs associated with the speech unit but only two are shown for each for the sake of brevity.
- a unit concatenation module 906 in the speech generation module 904 generates an unaligned concatenated sequence 903 from the input sequence 901 .
- Unaligned connection points of the unaligned concatenated sequence 903 are shown with circles, and positions of the unaligned GCIs at each unaligned connection point are labeled and marked with vertical arrows.
- the unaligned connection point between speech units 901 - 1 and 901 - 2 is marked by two vertical arrows corresponding to GCIs b 1 and a 2 . Similar pairs of GCIs of adjacent speech units are also shown. If the unaligned concatenated sequence 903 were played out as is, there might be unnatural sounding artifacts, such as “clicks,” or acoustic gaps, due to the unaligned connection points.
- the unaligned concatenated sequence 903 is next input to a GCI-F0 alignment module 908 , which generates an aligned concatenated sequence 905 .
- Alignment in this conceptual illustration corresponds to temporal alignment of successive speech units GCI boundaries. For example, speech units 901 - 1 and 901 - 2 are aligned so that GCI b 1 and GCI a 2 align at a common sample time.
- speech units 901 - 2 and 901 - 3 are aligned so that GCI b 2 and GCI a 3 align at a common sample time; speech units 901 - 3 and 901 - 4 are aligned so that GCI b 3 and GCI a 4 align at a common sample time; speech units 901 - 4 and 901 - 5 are aligned so that GCI b 4 and GCI a 5 align at a common sample time; and speech units 901 - 5 and 901 - 6 are aligned so that GCI b 5 and GCI a 6 align at a common sample time.
- the resulting aligned concatenated sequence 905 may then be output as the run-time waveform 907 . Because of the alignment possible using accurate GCIs, the run-time waveform 907 may sound like natural speech when played out.
Abstract
Description
where min(bRMS) and max(bRMS) are the minimum and maximum values, respectively, of bRMS(j) determined over the entire range of s(i),
range=max{1.0,max[b RMS]−floor[min(b RMS)]}, [2]
and
floor[min(b RMS)]=max[c floor,min(b RMS)], [3]
By way of example, the constant cfloor=20.0, although other values could be used.
p von(j)=max{0,min[1.0,Δb]}, [4]
where
cs is a slope factor, and ioff is an index offset corresponding to an offset between frames used to sense the slope of bRMS(j). By way of example, cs=30.0, although other values could be used. The index offset may be computed as ioff=max[1, int(coff×fb)], where “int” is the integer function, and coff is an offset constant. A value of coff=0.02 could be used, although other values are possible as well.
p voff(j)=max{0,min[1.0,−Δb]}. [6]
nr(i)<−1.0,
[nr(i−1)>nr(i)] and [nr(i)≦nr(i+1)],
[nr(i)<nr(i−p)] and [nr(i)<nr(i+p)],
where p=int(cf×fs). The frequency constant cf could have a value of 0.0004, although other values could be used as well. All samples of nr(i) with values that meet these criteria may be considered a respective candidate GCI.
q val =q 1 ×nr(i),
q prom =q 2 ×[q 3×(nr(i+p)+nr(i−p)−nr(i))],
q skew =q 4 ×[nr(i+q 5)−nr(i−q 5)],
where example values of the constants are q1=−0.1, q2=0.3, q3, =0.5, q4=0.1, and q5=int(q6×fs), with q6=0.00015. It will be appreciated that different values could be used as well for any one or more of these constants.
TABLE 1 |
Candidate GCI Data Structure: one per candidate GCI |
gc(k) |
q(k) = qval + qprom + qskew |
frame index jk locating temporally closest frame, for associating voicing |
metric with gc(k) |
sample residual index ik corresponding to index in nr(i) where gc(k) |
was identified |
storage for normalized cross-correlation function |
storage for pointer to previous and following candidate GCIs to be |
considered as actual glottal period endpoints |
The parameter q(k) in Table 1 corresponds to a GCI-quality score, and can be seen to include components of peak value as well as pulse shape. The normalized cross-correlation function (NCCF) is discussed below.
where e0 and e1 are given by:
e 0=Σi=i
e l=Σi=i
The calculation may be carried out for each of the L candidate GCIs gc(k), k=, . . . , L−1, ultimately populating cc(k=0, . . . , L−1; l=l1, . . . , l2) with NCCF values.
c local =[a peak −ccvn(k)]+c GCI-period +c voice +q GCI-peak +c period +r. [10]
Some of the quantities in clocal have been described above, others are explained below.
where wperiod is a weighting constant. An example value of wperiod=1.0 could be used, although other values are possible as well. Other quantitative definitions of cGCI-period are also possible.
TABLE 2 |
GCI-F0-Voiced Hypothesis Data Structure: one per voiced hypothesis |
vs = 1, the hypothesized voicing state (1 |
GCI period = Δik, the hypothesized period |
F0 period = lm=n, lag of NCCF peak closest to GCI period |
clocal, local cost (as described) |
start_peak = k |
end_peak = kn |
csum = 0.0, cumulative cost tallied during dynamic programming (initialized |
to zero) |
best_previous_candidate = −1, for backpointers during dynamic |
programming (initialized) |
The voicing state vs=1 corresponds to the hypothesis that the speech is voiced. Other parameters are used during dynamic programming, as described below.
TABLE 3 |
GCI-F0-Unvoiced Hypothesis Data Structure: one per GCI epoch |
vs = 0, the hypothesized voicing state (0 |
GCI period = Δik, the hypothesized period |
F0 period = lm=n, lag of NCCF peak closest to GCI period |
cU-local, local cost for unvoiced speech (as described below) |
start_peak = k |
end_peak = kn |
csum = 0.0, cumulative cost tallied during dynamic programming (initialized |
to zero) |
best_previous_candidate = −1, for backpointers during dynamic |
programming (initialized) |
c U-local =w uv ×ccvn(k)+c pv +c uv +q GCI-peak +r. [12]
Some of the quantities in cU-local have been described above in connection with clocal, others are explained below.
where Δik=ikn−ik, as described above, and Δik-1=ik−ik−1 corresponds to the period between the GCI at the given epoch and the GCI at the previous epoch from which the transition occurs. The constant wF0-trans=1.8 could be used, for example, and the constant wv-trans=1.4 could be used, for example.
Claims (19)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/750,000 US9263052B1 (en) | 2013-01-25 | 2013-01-25 | Simultaneous estimation of fundamental frequency, voicing state, and glottal closure instant |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/750,000 US9263052B1 (en) | 2013-01-25 | 2013-01-25 | Simultaneous estimation of fundamental frequency, voicing state, and glottal closure instant |
Publications (1)
Publication Number | Publication Date |
---|---|
US9263052B1 true US9263052B1 (en) | 2016-02-16 |
Family
ID=55275485
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/750,000 Active 2034-04-20 US9263052B1 (en) | 2013-01-25 | 2013-01-25 | Simultaneous estimation of fundamental frequency, voicing state, and glottal closure instant |
Country Status (1)
Country | Link |
---|---|
US (1) | US9263052B1 (en) |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108133713A (en) * | 2017-11-27 | 2018-06-08 | 苏州大学 | A kind of method that sound channel area is estimated in the case where glottis closes phase |
CN111899716A (en) * | 2020-08-03 | 2020-11-06 | 北京帝派智能科技有限公司 | Speech synthesis method and system |
US20210314307A1 (en) * | 2018-09-20 | 2021-10-07 | Sony Semiconductor Solutions Corporation | Transmitting device and transmitting method, and receiving device and receiving method |
US11443761B2 (en) | 2018-09-01 | 2022-09-13 | Indian Institute Of Technology Bombay | Real-time pitch tracking by detection of glottal excitation epochs in speech signal using Hilbert envelope |
US20230056987A1 (en) * | 2021-08-19 | 2023-02-23 | Digital Asset Capital, Inc. | Semantic map generation using hierarchical clause structure |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5138661A (en) * | 1990-11-13 | 1992-08-11 | General Electric Company | Linear predictive codeword excited speech synthesizer |
US6073093A (en) * | 1998-10-14 | 2000-06-06 | Lockheed Martin Corp. | Combined residual and analysis-by-synthesis pitch-dependent gain estimation for linear predictive coders |
US20040059568A1 (en) * | 2002-08-02 | 2004-03-25 | David Talkin | Method and apparatus for smoothing fundamental frequency discontinuities across synthesized speech segments |
US20120265534A1 (en) * | 2009-09-04 | 2012-10-18 | Svox Ag | Speech Enhancement Techniques on the Power Spectrum |
-
2013
- 2013-01-25 US US13/750,000 patent/US9263052B1/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5138661A (en) * | 1990-11-13 | 1992-08-11 | General Electric Company | Linear predictive codeword excited speech synthesizer |
US6073093A (en) * | 1998-10-14 | 2000-06-06 | Lockheed Martin Corp. | Combined residual and analysis-by-synthesis pitch-dependent gain estimation for linear predictive coders |
US20040059568A1 (en) * | 2002-08-02 | 2004-03-25 | David Talkin | Method and apparatus for smoothing fundamental frequency discontinuities across synthesized speech segments |
US20120265534A1 (en) * | 2009-09-04 | 2012-10-18 | Svox Ag | Speech Enhancement Techniques on the Power Spectrum |
Non-Patent Citations (11)
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108133713A (en) * | 2017-11-27 | 2018-06-08 | 苏州大学 | A kind of method that sound channel area is estimated in the case where glottis closes phase |
CN108133713B (en) * | 2017-11-27 | 2020-10-02 | 苏州大学 | Method for estimating sound channel area under glottic closed phase |
US11443761B2 (en) | 2018-09-01 | 2022-09-13 | Indian Institute Of Technology Bombay | Real-time pitch tracking by detection of glottal excitation epochs in speech signal using Hilbert envelope |
US20210314307A1 (en) * | 2018-09-20 | 2021-10-07 | Sony Semiconductor Solutions Corporation | Transmitting device and transmitting method, and receiving device and receiving method |
US11528260B2 (en) * | 2018-09-20 | 2022-12-13 | Sony Semiconductor Solutions Corporation | Transmitting device and transmitting method, and receiving device and receiving method |
CN111899716A (en) * | 2020-08-03 | 2020-11-06 | 北京帝派智能科技有限公司 | Speech synthesis method and system |
US20230056987A1 (en) * | 2021-08-19 | 2023-02-23 | Digital Asset Capital, Inc. | Semantic map generation using hierarchical clause structure |
US20230075341A1 (en) * | 2021-08-19 | 2023-03-09 | Digital Asset Capital, Inc. | Semantic map generation employing lattice path decoding |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11749414B2 (en) | Selecting speech features for building models for detecting medical conditions | |
US10878823B2 (en) | Voiceprint recognition method, device, terminal apparatus and storage medium | |
US9542927B2 (en) | Method and system for building text-to-speech voice from diverse recordings | |
US8484022B1 (en) | Adaptive auto-encoders | |
US8527276B1 (en) | Speech synthesis using deep neural networks | |
US9536525B2 (en) | Speaker indexing device and speaker indexing method | |
US9183830B2 (en) | Method and system for non-parametric voice conversion | |
US9311915B2 (en) | Context-based speech recognition | |
US9240184B1 (en) | Frame-level combination of deep neural network and gaussian mixture models | |
CN104934029B (en) | Speech recognition system and method based on pitch synchronous frequency spectrum parameter | |
Vestman et al. | Speaker recognition from whispered speech: A tutorial survey and an application of time-varying linear prediction | |
US9263052B1 (en) | Simultaneous estimation of fundamental frequency, voicing state, and glottal closure instant | |
US20110123965A1 (en) | Speech Processing and Learning | |
JP4515054B2 (en) | Method for speech recognition and method for decoding speech signals | |
CN112750445B (en) | Voice conversion method, device and system and storage medium | |
US11322133B2 (en) | Expressive text-to-speech utilizing contextual word-level style tokens | |
Middag et al. | Robust automatic intelligibility assessment techniques evaluated on speakers treated for head and neck cancer | |
US9263033B2 (en) | Utterance selection for automated speech recognizer training | |
WO2021012495A1 (en) | Method and device for verifying speech recognition result, computer apparatus, and medium | |
US10818308B1 (en) | Speech characteristic recognition and conversion | |
US20210121124A1 (en) | Classification machine of speech/lingual pathologies | |
US9484045B2 (en) | System and method for automatic prediction of speech suitability for statistical modeling | |
Adi et al. | Automatic measurement of vowel duration via structured prediction | |
JP5949634B2 (en) | Speech synthesis system and speech synthesis method | |
US20180268815A1 (en) | Quality feedback on user-recorded keywords for automatic speech recognition systems |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:TALKIN, DAVID;REEL/FRAME:029693/0283Effective date: 20130124 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |