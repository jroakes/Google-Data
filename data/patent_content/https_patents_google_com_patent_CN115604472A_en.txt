CN115604472A - Method and apparatus for coding blocks of video data - Google Patents
Method and apparatus for coding blocks of video data Download PDFInfo
- Publication number
- CN115604472A CN115604472A CN202211126406.5A CN202211126406A CN115604472A CN 115604472 A CN115604472 A CN 115604472A CN 202211126406 A CN202211126406 A CN 202211126406A CN 115604472 A CN115604472 A CN 115604472A
- Authority
- CN
- China
- Prior art keywords
- intra
- prediction mode
- prediction
- block
- class
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 title claims abstract description 268
- 230000015654 memory Effects 0.000 claims description 56
- 238000013507 mapping Methods 0.000 claims description 31
- 230000004044 response Effects 0.000 claims description 5
- 239000000758 substrate Substances 0.000 claims 1
- 230000008569 process Effects 0.000 description 202
- 238000009826 distribution Methods 0.000 description 78
- 238000010586 diagram Methods 0.000 description 48
- 238000005192 partition Methods 0.000 description 18
- 238000001914 filtration Methods 0.000 description 14
- 230000002441 reversible effect Effects 0.000 description 14
- 230000002829 reductive effect Effects 0.000 description 12
- 238000004891 communication Methods 0.000 description 11
- 230000006835 compression Effects 0.000 description 11
- 238000007906 compression Methods 0.000 description 11
- 230000002093 peripheral effect Effects 0.000 description 11
- 230000006870 function Effects 0.000 description 10
- 238000012545 processing Methods 0.000 description 10
- 238000013139 quantization Methods 0.000 description 10
- 230000009467 reduction Effects 0.000 description 10
- 238000010845 search algorithm Methods 0.000 description 6
- 230000009466 transformation Effects 0.000 description 6
- 239000013598 vector Substances 0.000 description 6
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 5
- 230000006978 adaptation Effects 0.000 description 5
- 230000008859 change Effects 0.000 description 4
- 102100029272 5-demethoxyubiquinone hydroxylase, mitochondrial Human genes 0.000 description 3
- 101100494773 Caenorhabditis elegans ctl-2 gene Proteins 0.000 description 3
- 102100035959 Cationic amino acid transporter 2 Human genes 0.000 description 3
- 102100021391 Cationic amino acid transporter 3 Human genes 0.000 description 3
- 102100021392 Cationic amino acid transporter 4 Human genes 0.000 description 3
- 101710195194 Cationic amino acid transporter 4 Proteins 0.000 description 3
- 101100112369 Fasciola hepatica Cat-1 gene Proteins 0.000 description 3
- 101000770593 Homo sapiens 5-demethoxyubiquinone hydroxylase, mitochondrial Proteins 0.000 description 3
- 241000023320 Luma <angiosperm> Species 0.000 description 3
- 101100005271 Neurospora crassa (strain ATCC 24698 / 74-OR23-1A / CBS 708.71 / DSM 1257 / FGSC 987) cat-1 gene Proteins 0.000 description 3
- 108091006231 SLC7A2 Proteins 0.000 description 3
- 108091006230 SLC7A3 Proteins 0.000 description 3
- 230000000670 limiting effect Effects 0.000 description 3
- OSWPMRLSEDHDFF-UHFFFAOYSA-N methyl salicylate Chemical compound COC(=O)C1=CC=CC=C1O OSWPMRLSEDHDFF-UHFFFAOYSA-N 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 108091026890 Coding region Proteins 0.000 description 2
- 230000003044 adaptive effect Effects 0.000 description 2
- 230000001174 ascending effect Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000010790 dilution Methods 0.000 description 2
- 239000012895 dilution Substances 0.000 description 2
- 238000009499 grossing Methods 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000002156 mixing Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000002123 temporal effect Effects 0.000 description 2
- 230000002411 adverse Effects 0.000 description 1
- 230000004075 alteration Effects 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 230000000903 blocking effect Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 230000006837 decompression Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 230000036961 partial effect Effects 0.000 description 1
- 230000000750 progressive effect Effects 0.000 description 1
- 239000013074 reference sample Substances 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000126 substance Substances 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000001131 transforming effect Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/18—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a set of transform coefficients
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/124—Quantisation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/129—Scanning of coding units, e.g. zig-zag scan of transform coefficients or flexible macroblock ordering [FMO]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/13—Adaptive entropy coding, e.g. adaptive variable length coding [AVLC] or context adaptive binary arithmetic coding [CABAC]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/157—Assigned coding mode, i.e. the coding mode being predefined or preselected to be further used for selection of another element or parameter
- H04N19/159—Prediction type, e.g. intra-frame, inter-frame or bidirectional frame prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/184—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being bits, e.g. of the compressed video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/44—Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder
- H04N19/45—Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder performing compensation of the inverse transform mismatch, e.g. Inverse Discrete Cosine Transform [IDCT] mismatch
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/60—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/90—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using coding techniques not provided for in groups H04N19/10-H04N19/85, e.g. fractals
- H04N19/91—Entropy coding, e.g. variable length coding [VLC] or arithmetic coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/90—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using coding techniques not provided for in groups H04N19/10-H04N19/85, e.g. fractals
- H04N19/93—Run-length coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/12—Selection from among a plurality of transforms or standards, e.g. selection between discrete cosine transform [DCT] and sub-band transform or selection between H.263 and H.264
- H04N19/122—Selection of transform size, e.g. 8x8 or 2x4x8 DCT; Selection of sub-band transforms of varying structure or type
Abstract
Methods and apparatus for coding blocks of video data. Coding the current block may include: decoding a scanning position corresponding to an end of a block; decoding bits representing absolute values of respective non-zero transform coefficients of the current block and one or more additional bits associated with respective picture levels starting from a most significant bit associated with a minimum picture level, the bits having a least significant bit indicating whether the absolute value is greater than a maximum picture level of the level map, each additional bit indicating whether the absolute value is at least equal to the respective picture level; and decoding a coefficient residual map, each residual coefficient of the coefficient residual map corresponding to a respective non-zero transform coefficient of the current block, the respective non-zero transform coefficient having an absolute value exceeding a maximum map level. Other techniques for coding a current block are described.
Description
Description of the cases
The application belongs to divisional application of Chinese invention patent application 201880036331.7, which has application date of 2018, 7, month and 16.
Technical Field
The present invention relates to a method and apparatus for coding a block of video data.
Background
A digital video stream may represent video using a sequence of frames or still images. Digital video may be used for a variety of applications including, for example, video conferencing, high definition video entertainment, video advertising, or sharing of user-generated video. Digital video streams may contain large amounts of data and consume a large amount of computing or communication resources of a computing device for processing, transmission, or storage of video data. Various methods have been proposed for reducing the amount of data in a video stream, including compression and other encoding techniques.
Disclosure of Invention
An aspect of an embodiment of the present disclosure is a method for intra-coding a current block using an intra prediction mode. The method includes determining a first intra prediction mode of a left neighbor block of the current block; and determining a second intra prediction mode of an upper neighbor block of the current block. On a condition that the first intra prediction mode and the second intra prediction mode are the same mode, the method determines a probability distribution of an intra prediction mode for coding the current block using the same mode. The method determines a probability distribution for coding the intra-prediction mode using the other of the first intra-prediction mode and the second intra-prediction mode on a condition that the first intra-prediction mode and the second intra-prediction mode are not the same mode and only one of the left intra-prediction mode and the upper second intra-prediction mode is a smooth intra-prediction mode. Otherwise, the method determines a probability distribution for coding the intra prediction mode using any one of the first intra prediction mode and the second intra prediction mode. The method also includes coding the intra-prediction mode using the probability distribution.
Another aspect is an apparatus for coding a current block using an intra prediction mode that includes a memory and a processor. The memory includes instructions executable by the processor to determine a first intra-prediction mode of a left-neighboring block of the current block; determining a second intra prediction mode of an upper neighbor block of the current block; determining a probability distribution of an intra prediction mode for coding the current block using the same mode on the condition that the first intra prediction mode and the second intra prediction mode are the same mode; on a condition that the first intra-prediction mode and the second intra-prediction mode are not the same mode and only one of a left intra-prediction mode and an upper intra-prediction mode is a smooth intra-prediction mode, determining a probability distribution for coding the intra-prediction mode using the other of the first intra-prediction mode and the second intra-prediction mode; or otherwise, determining a probability distribution for coding the intra prediction mode using any one of the first intra prediction mode and the second intra prediction mode. The instructions may also include instructions to code the intra-prediction mode using the probability distribution.
One aspect of an embodiment of the present disclosure is a method of coding a transform block using a level map. The method includes coding scan locations corresponding to an end of a block in a forward scan order; coding a positional non-zero map indicating the current block including non-zero transform coefficients in a reverse scan order; coding lower-range level maps in reverse scan order, each lower-range level map having a respective map level up to a maximum map level, the lower-range level maps indicating which absolute values of the non-zero transform coefficients are equal to the respective map level and which absolute values of the non-zero transform coefficients are greater than the respective map level; and coding a coefficient residual map, each residual coefficient of the coefficient residual map corresponding to a respective non-zero transform coefficient of the current block having an absolute value greater than the maximum map level.
Another aspect is an apparatus for coding a transform block using a level map that includes a memory and a processor. The memory includes instructions executable by the processor to code a scan location corresponding to an end of a block; coding bits representing absolute values of respective non-zero transform coefficients of the current block and one or more additional bits associated with respective graph levels starting from a most significant bit associated with a minimum graph level, the bits having a least significant bit indicating whether the absolute values are greater than a maximum graph level of a level graph, each additional bit indicating whether the absolute values are at least equal to the respective graph level; and coding a coefficient residual map, each residual coefficient of the coefficient residual map corresponding to a respective non-zero transform coefficient of the current block having an absolute value exceeding the maximum map level.
Another aspect is a method of decoding a transform block using a level map. The method includes decoding scan positions corresponding to an end of a block in an order of reverse scan; decoding bits representing absolute values of respective non-zero transform coefficients of the current block and one or more additional bits associated with respective graph levels starting from a most significant bit associated with a minimum graph level, the bits having a least significant bit indicating whether the absolute values are greater than a maximum graph level of a level graph, each additional bit indicating whether the absolute values are at least equal to the respective graph level; and decoding a coefficient residual map, each residual coefficient of the coefficient residual map corresponding to a respective non-zero transform coefficient of the current block having an absolute value exceeding the maximum map level.
These and other aspects of the disclosure are disclosed in the following detailed description of the embodiments, the appended claims and the accompanying drawings.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views.
Fig. 1 is a schematic diagram of a video encoding and decoding system.
Fig. 2 is a block diagram of an example of a computing device that may implement a transmitting station or a receiving station.
Fig. 3 is a schematic diagram of a video stream to be encoded and subsequently decoded.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present disclosure.
Fig. 6 is a flow diagram of a process for encoding a transform block in an encoded video bitstream using a level map, according to an embodiment of the disclosure.
Fig. 7 is a schematic diagram illustrating stages of transform coefficient coding using a level map according to an embodiment of the present disclosure.
Fig. 8 is a schematic diagram of a previously coded neighbor in a non-zero graph according to an embodiment of the present disclosure.
Fig. 9 is a flow diagram of a process for coding a transform block using a level map, according to an embodiment of the disclosure.
Fig. 10A and 10B are schematic diagrams of examples of templates for determining coding context according to embodiments of the present disclosure.
Fig. 11 is a schematic diagram of a coefficient token tree used to entropy code transform blocks, according to an embodiment of the present disclosure.
Fig. 12 is a flow diagram of a process for alphacoding a transform block using coefficients including a head token and a tail token, according to an embodiment of the present disclosure.
Fig. 13 is a schematic diagram of an example of a probability map according to an embodiment of the present disclosure.
Fig. 14A is a schematic diagram of directional intra prediction modes according to an embodiment of the present disclosure.
Fig. 14B is a schematic diagram of an example of an intra prediction mode according to an embodiment of the present disclosure.
Fig. 14C is a schematic diagram of an example of a smooth prediction mode according to an embodiment of the present disclosure.
FIG. 15 is a flowchart of a process for intra-coding a current block, according to an embodiment of the present disclosure.
Fig. 16 is a schematic diagram of an example of a region for determining context according to an embodiment of the present disclosure.
Fig. 17 is a flow diagram of a process for decoding a transform block using a level map, according to an embodiment of the disclosure.
FIG. 18 is a flowchart of a process for coding a current block using an intra prediction mode according to an embodiment of the present disclosure.
FIG. 19 is a flowchart of a process for coding a current block using an intra prediction mode according to another embodiment of the present disclosure.
Fig. 20 is a schematic diagram of context model indexing, according to an embodiment of the present disclosure.
Detailed Description
As described above, compression schemes associated with coded video streams may include dividing an image into blocks; and generating the output bitstream using one or more techniques for limiting information included in the digital video output bitstream (i.e., the encoded bitstream). The received bitstream may be decoded to recreate the block and source images from the limited information. Encoding a video stream or a portion thereof (e.g., a frame or a block) may include using temporal or spatial similarities in the video stream to improve coding efficiency. For example, a current block of a video stream may be encoded based on identifying differences (residuals) between previously coded pixel values and those in the current block, or between combinations of previously coded pixel values and those in the current block.
Encoding using spatial similarity may be referred to as intra prediction. Intra-prediction attempts to predict the pixel values of a block of a video frame using pixels that are peripheral to the block, i.e., pixels that are in the same frame as the block but outside the block. The prediction block resulting from intra prediction is referred to herein as an intra predictor. Intra prediction may be performed along the direction of prediction, where each direction may correspond to an intra prediction mode. The intra prediction mode may be signaled by the encoder to the decoder.
Encoding using temporal similarity may be referred to as inter prediction. The inter prediction block predicts pixel values of a block using one or more blocks that may be shifted from one or more temporally adjacent frames (i.e., reference frames). Temporally adjacent frames are frames that occur earlier in time in the video stream than the frame of the block being encoded. The prediction block resulting from inter prediction is referred to herein as an inter predictor.
Inter prediction is performed using the motion vector. The motion vector used to generate the prediction block refers to a frame other than the current frame, i.e., a reference frame. The reference frame may be located before or after the current frame in the video stream sequence. Some codecs use up to eight reference frames that can be stored in a frame buffer. The motion vector may reference (i.e., use) one of the reference frames of the frame buffer. In this way, one or more reference frames may be used to code the current frame.
As described above, a current block of a video stream may be encoded based on identifying differences (residuals) between previously coded pixel values and pixel values in the current block. In this way, only the residual and the parameters used to generate the residual need to be added to the encoded bitstream. The residual may be encoded using a lossy quantization step.
The residual block may be in the pixel domain. The residual block may be transformed into the frequency domain, resulting in a transformed block of transform coefficients. The transform coefficients may be quantized, thereby generating a quantized transform block of quantized transform coefficients. The quantized coefficients may be entropy encoded and added to the encoded bitstream. The decoder may receive the encoded bitstream, entropy decode the quantized transform coefficients to reconstruct the original video frame.
Entropy coding is a technique for "lossless" coding that relies on a probabilistic model that models the distribution of values that occur in an encoded video bitstream. Entropy coding can reduce the number of bits required to represent video data to near a theoretical minimum by using a probabilistic model based on measured or estimated value distributions. In practice, the actual reduction in the number of bits required to represent the video data may be a function of the accuracy of the probability model, the number of bits used to perform the coding, and the computational accuracy of the fixed-point algorithm used to perform the coding.
In an encoded video bitstream, many bits are used in one of two cases: content prediction (e.g., inter mode/motion vector coding, intra prediction mode coding, etc.) or residual coding (e.g., transform coefficients).
With respect to content prediction, a bit in the bitstream may include, for a block, an intra prediction mode used to encode the block. The intra-prediction mode may be coded (encoded by the encoder and decoded by the decoder) using entropy coding. In this way, a context is determined for the intra-prediction mode, and a probability model corresponding to the context used to code the intra-prediction mode is used for coding.
The encoder may use techniques to reduce the amount of bits spent on coefficient coding. For example, a coefficient token tree (which may also be referred to as a binary token tree) specifies a range of values and a forward adaptive probability for each branch in the token tree. The token base value is subtracted from the value to be coded to form a residual, and the block is then coded with a fixed probability. Similar schemes with minor variations (including backward adaptation) are also possible. The adaptation technique may alter the probability model as the video stream is encoded to accommodate changes in data characteristics. In any case, the decoder is informed of (or has available) the probability model that was used to encode the entropy coded video bitstream in order to decode the video bitstream.
As described above, entropy coding of a sequence of symbols is typically achieved by using a probability model to determine the probability p of the sequence and then using binary arithmetic coding to map the sequence to binary codewords at the encoder and decode the sequence from the binary codewords at the decoder. The length (i.e., the number of bits) of the codeword is given by-log (p). The efficiency of entropy coding may be directly related to the probability model. In this document, unless otherwise stated, logarithmic representation logarithmic functions are based on two (2).
The model used herein may be lossless (entropy) coding or may be a parameter in lossless (entropy) coding. The model may be any parameter or method that affects the probability estimation of entropy coding.
The purpose of context modeling is to obtain a probability distribution for a subsequent entropy coding engine, such as arithmetic coding, huffman coding, and other variable length to variable length coding engines. To obtain good compression performance, a large number of contexts may be required. For example, some video coding systems may include hundreds or even thousands of contexts that are only used for transform coefficient coding. Each context may correspond to a probability distribution.
The probability distribution may be learned by the decoder and/or included in the header of the frame to be decoded.
Learning may refer to the entropy coding engine of the decoder may adapt the probability distribution of the context model (i.e., the probability model) based on the decoded frame. For example, a decoder may have an initial probability distribution available that the decoder (e.g., an entropy coding engine of the decoder) may continuously update as the decoder decodes other frames. The updating of the probability model may ensure that the initial probability distribution is updated to reflect the actual distribution in the decoded frame.
Including the probability distribution in the header may instruct the decoder to use the included probability distribution to decode the next frame, given the corresponding context. A cost (in bits) is associated with including each probability distribution in the header. For example, in a coding system that includes 3000 contexts and uses 8 bits to encode the probability distribution (coded to integer values between 1 and 255), 24,000 bits are added to the encoded bitstream. These bits are overhead bits. Techniques may be used to reduce the number of overhead bits. For example, a probability distribution for some but not all contexts may be included. For example, a prediction scheme may also be used to reduce overhead bits. Even with these overhead reduction techniques, the overhead will not be zero.
As described above, the residual of the video block is transformed into a transform block of transform coefficients. The transform block is in the frequency domain and one or more transform blocks may be generated for the video block. The transform coefficients are quantized and entropy coded into an encoded video bitstream. The decoder reconstructs the block using the encoded transform coefficients and the reference frame. Entropy coding transform coefficients involves selecting a context model (also referred to as a probabilistic context model or probability model) that provides an estimate of the conditional probability of the binary symbol used to code the binarized transform coefficient.
Embodiments of the present disclosure may result in a reduction in the number of contexts used to code different aspects of content prediction and/or residual coding. For example, the number of contexts used to code intra prediction modes may be reduced. Embodiments of the present disclosure may reduce the number of probability values associated with a context. As such, embodiments of the present disclosure may have reduced computational and storage complexity without adversely affecting compression performance.
Detailed entropy coding is first described herein with reference to a system incorporating the present teachings.
Fig. 1 is a schematic diagram of a video encoding and decoding system 100. Transmitting station 102 may be, for example, a computer having an internal configuration such as the hardware depicted in fig. 2. However, other suitable implementations of transmitting station 102 are possible. For example, the processing of transmitting station 102 may be distributed among multiple devices.
In one example, the receiving station 106 may be a computer having an internal configuration such as the hardware described in fig. 2. However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 may be distributed among multiple devices.
Other implementations of the video encoding and decoding system 100 are possible. For example, embodiments may omit network 104. In another embodiment, the video stream may be encoded and then stored for later transmission to the receiving station 106 or any other device having memory. In one implementation, the receiving station 106 receives an encoded video stream (e.g., via the network 104, a computer bus, and/or some communication path) and stores the video stream for later decoding. In an exemplary embodiment, the real-time transport protocol (RTP) is used to transmit encoded video over the network 104. In another embodiment, transport protocols other than RTP may be used, such as the video streaming protocol of the Hypertext transfer protocol (HTTP).
When used in a videoconferencing system, for example, transmitter station 102 and/or receiving station 106 may include the ability to encode and decode video streams as described below. For example, receiving stations 106 may be video conference participants that receive encoded video bitstreams from a video conference server (e.g., transmitting station 102) for decoding and viewing, and further encode and send their own video bitstreams to the video conference server for decoding and viewing by other participants.
Fig. 2 is a block diagram of an example of a computing device 200 that may implement a transmitting station or a receiving station. For example, computing device 200 may implement one or both of transmitting station 102 and receiving station 106 of fig. 1. Computing device 200 may be in the form of a computing system including multiple computing devices, or in the form of a single computing device, such as a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and so forth.
The CPU 202 in the computing device 200 may be a central processing unit. Alternatively, the CPU 202 may be any other type of device or devices now existing or later developed that is capable of manipulating or processing information. Although the disclosed embodiments may be implemented with a single processor as shown, such as CPU 202, speed and efficiency advantages may be realized using more than one processor.
The memory 204 in the computing device 200 may be a Read Only Memory (ROM) device or a Random Access Memory (RAM) device in one implementation. Any other suitable type of storage device may be used as memory 204. The memory 204 may include code and data 206 that are accessed by the CPU 202 using the bus 212. The memory 204 may further include an operating system 208 and application programs 210, the application programs 210 including at least one program that allows the CPU 202 to perform the methods described herein. For example, the application programs 210 may include applications 1 through N, which further include a video coding application that performs the methods described herein. Computing device 200 may also include secondary storage 214, which may be, for example, a memory card for use with a mobile computing device. Because video communication sessions may contain a large amount of information, they may be stored in whole or in part in secondary storage 214 and loaded into memory 204 for processing as needed.
Although fig. 2 illustrates the CPU 202 and memory 204 of the computing device 200 as integrated into a single unit, other configurations may be used. The operations of CPU 202 may be distributed across multiple machines (each machine having one or more processors) that may be coupled directly or across a local area network or other network. Memory 204 may be distributed across multiple machines, such as a network-based memory or memory among multiple machines that perform operations for computing device 200. Although illustrated here as a single bus, the bus 212 of the computing device 200 may be comprised of multiple buses. Further, secondary storage 214 may be directly coupled to other components of computing device 200, or may be accessed via a network, and may comprise a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards. Thus, computing device 200 may be implemented in a wide variety of configurations.
Fig. 3 is a diagram of an example of a video stream 300 to be encoded and then decoded. The video stream 300 includes a video sequence 302. At the next level, the video sequence 302 includes a plurality of adjacent frames 304. Although three frames are illustrated as adjacent frames 304, video sequence 302 may include any number of adjacent frames 304. The adjacent frames 304 may then be further subdivided into individual frames, such as frame 306. At the next level, the frame 306 may be divided into a series of slices 308 or planes. For example, the segment 308 may be a subset of frames that allow, for example, parallel processing. The segment 308 may also be a subset of a frame that separates the video data into individual colors. For example, a frame 306 of color video data may include a luminance plane and two chrominance planes. The segments 308 may be sampled at different resolutions.
Regardless of whether frame 306 is divided into segments 308, frame 306 may be further subdivided into blocks 310, and blocks 310 may contain data corresponding to, for example, 16x16 pixels in frame 306. The block 310 may also be arranged to include data from one or more segments 308 of pixel data. The block 310 may also be any other suitable size, such as 4 × 4 pixels, 8 × 8 pixels, 16 × 8 pixels, 8 × 16 pixels, 16 × 16 pixels, or larger.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. As described above, encoder 400 may be implemented in transmitting station 102, such as by providing a computer software program stored in a memory, such as memory 204. The computer software program may include machine instructions that, when executed by a processor, such as CPU 202, cause transmitting station 102 to encode video data in the manner described herein. Encoder 400 may also be implemented as dedicated hardware included in, for example, transmitting station 102. The encoder 400 has the following stages for performing various functions in the forward path (shown by the solid connecting lines) to generate an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy coding stage 408. The encoder 400 may also include a reconstruction path (shown by dotted connecting lines) to reconstruct the frame used to encode future blocks. In fig. 4, the encoder 400 has the following stages for performing various functions in the reconstruction path: a dequantization stage 410, an inverse transform stage 412, a reconstruction stage 414, and a loop filter stage 416. Other structural variations of encoder 400 may be used to encode video stream 300.
When the video stream 300 is presented for encoding, the frames 306 may be processed in units of blocks. At the intra/inter prediction stage 402, a block may be coded using intra prediction (also referred to as intra prediction) or inter prediction (also referred to as inter prediction), or a combination of both. In any case, a prediction block may be formed. In the case of intra prediction, all or part of the prediction block may be formed from samples in the current frame that have been previously coded and reconstructed. In the case of inter prediction, all or part of the prediction block may be formed from samples in one or more previously constructed reference frames determined using motion vectors.
Next, still referring to fig. 4, the predicted block may be subtracted from the current block at the intra/inter prediction stage 402 to generate a residual block (also referred to as a residual). The transform stage 404 transforms the residual into, for example, transform coefficients in the frequency domain using a block-based transform. Such block-based transforms include, for example, discrete Cosine Transform (DCT) and Asymmetric Discrete Sine Transform (ADST). Other block-based transforms are also possible. Furthermore, a combination of different transforms may be applied to a single residual. In one example of applying a transform, the DCT transforms the residual block to the frequency domain, where the transform coefficient values are based on spatial frequency. The lowest frequency (DC) coefficients are in the upper left corner of the matrix and the highest frequency coefficients are in the lower right corner of the matrix. It is noted that the size of the prediction block, and thus the residual block, may be different from the size of the transform block. For example, a prediction block may be divided into smaller blocks to which separate transforms are applied.
The quantization stage 406 converts the transform coefficients into discrete quantum values, referred to as quantized transform coefficients, using quantizer values or quantization stages. For example, the transform coefficients may be divided by the quantizer value and truncated. The quantized transform coefficients are then entropy encoded by the entropy encoding stage 408. Entropy coding may be performed using any number of techniques including tokens and binary trees. The entropy coded coefficients are then output to the compressed bitstream 420, along with other information used to decode the block (which may include, for example, the type of prediction used, the transform type, the motion vectors, and the quantizer values). The information for decoding the block may be entropy coded into a block, frame, slice, and/or partial header within the compressed bitstream 420. The compressed bitstream 420 may also be referred to as an encoded video stream or an encoded video bitstream, and these terms may be used interchangeably herein.
The reconstruction path in fig. 4 (shown by the dashed connecting lines) may be used to ensure that the encoder 400 and decoder 500 (described below) use the same reference frames and blocks to decode the compressed bitstream 420. The reconstruction path performs functions similar to those occurring during the decoding process (discussed in more detail below), including dequantizing the quantized transform coefficients at a dequantization stage 410, and inverse transforming the dequantized transform coefficients at an inverse transform stage 412 to generate a block of derived residuals (also referred to as derived residuals). At the reconstruction stage 414, the prediction block predicted at the intra/inter prediction stage 402 may be added to the derived residual to create a reconstructed block. A loop filtering stage 416 may be applied to the reconstructed block to reduce distortion such as block artifacts.
Other variations of the encoder 400 may be used to encode the compressed bitstream 420. For example, the non-transform based encoder 400 may quantize the residual signal directly without the transform stage 404 for certain blocks or frames. In another embodiment, the encoder 400 may have the quantization stage 406 and the dequantization stage 410 combined into a single stage.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. The decoder 500 may be implemented in the receiving station 106, for example, by providing a computer software program stored in the memory 204. The computer software program may include machine instructions that, when executed by a processor such as CPU 202, cause receiving station 106 to decode video data in a manner described below. Decoder 500 may also be implemented in hardware included in, for example, transmitting station 102 or receiving station 106.
Similar to the reconstruction path of the encoder 400 discussed above, the decoder 500 in one example includes the following stages that perform various functions to generate the output video stream 516 from the compressed bitstream 420: an entropy decoding stage 502, a dequantization stage 504, an inverse transform stage 506, an intra/inter prediction stage 508, a reconstruction stage 510, a loop filtering stage 512, and a post filtering stage 514. Other structural variations of the decoder 500 may be used to decode the compressed bitstream 420.
When the compressed bitstream 420 is presented for decoding, data elements within the compressed bitstream 420 may be decoded by the entropy decoding stage 502 to generate a set of quantized transform coefficients. Dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by quantizer values), and inverse transform stage 506 inverse transforms the dequantized transform coefficients using the selected transform type to generate derivative residuals that may be the same as the derivative residuals created by inverse transform stage 412 in encoder 400. Using the header information decoded from the compressed bitstream 420, the decoder 500 may use the intra/inter prediction stage 508 to create the same prediction block as was created in the encoder 400, e.g., at the intra/inter prediction stage 402. At the reconstruction stage 510, the prediction block may be added to the derived residual to create a reconstructed block. The loop filtering stage 512 may be applied to the reconstructed block to reduce blocking artifacts. Other filtering may be applied to the reconstructed block. In an example, a post-filtering stage 514 may be applied to the reconstructed block to reduce block distortion, and the result is output as an output video stream 516. The output video stream 516 may also be referred to as a decoded video stream, and these terms may be used interchangeably herein. Other variations of the decoder 500 may be used to decode the compressed bitstream 420. For example, the decoder 500 may generate the output video stream 516 without the post-filtering stage 514.
Other variations of the decoder 500 may be used to decode the compressed bitstream 420. For example, the decoder 500 may produce the output video stream 516 without the post-filtering stage 514. In some embodiments of the decoder 500, a post-filtering stage 514 is used after the loop filtering stage 512. The loop filtering stage 512 may include an optional deblocking filtering stage. Additionally or alternatively, encoder 400 includes an optional deblock filtering stage in loop filtering stage 416.
Some codecs may use the level map to code the transform block (i.e., either encoded by the encoder or decoded by the decoder). That is, some codecs may use a level map to code the transform coefficients of a transform block. In level map coding, a transform block is decomposed into multiple level maps, such that the level maps decompose (i.e., reduce) the coding of each transform coefficient value into a series of binary decisions corresponding to magnitude levels (i.e., map levels). The decomposition may be accomplished by multiple runs. Thus, the transform coefficients of a transform block are decomposed into a series of levels of binary sum residuals according to the following equation:
wherein the content of the first and second substances,
residue[r][c]＝absolute(coefficient[r][c])-T-1
in the above equation, coefficient [ r ]][c]Is the transform coefficient of the transform block at position (row = r, column = c), T is the maximum graph level, level k Is a level map corresponding to map level k, residual is a coefficient residual map, and sign is a sign map of transform coefficients. These terms are further described below with reference to fig. 7. The same equation may be used, such as by a decoder, from the encoded level k And the transformation coefficients of the transformation block are recombined in the graph, the residual graph residual and the sign graph sign.
The zeroth run may be used to determine a non-zero map (also referred to as a level 0 map) that indicates which transform coefficients of the transform block are zero and which are non-zero. A level map (i.e., level 1 map, level 2 map, level T map) corresponding to run 1 to a maximum (i.e., threshold) level T is generated in ascending order from level 1 to maximum map level T. The level map for level k (referred to as level k map) indicates which transform coefficients of the transform block have absolute values greater than or equal to k. The level map is a binary map. The final run generates a coefficient residual map. If the transform block contains transform coefficient values above the maximum picture level T, the coefficient residual map indicates how much these coefficients are larger than the maximum picture level T (i.e., the residual).
When generating (i.e., coding) a level k map, only the location (r, c) (i.e., level) corresponding to the location (r, c) of the level (k-1) map need be processed k-1 [r][c]= 1) -determine that the other positions of level (k-1) are less than k, so they need not be processed for the level k map. This reduces processing complexity and reduces the amount of binary coding operations.
Since the level map contains binary values, the upper and left neighbours of the value to be encoded are binary values. A context model based on the binary values of any number of previously coded neighbors may be determined. The context model can leverage information from all of these neighbors. The previously coded neighbors may be neighbors in the same level map or in a previous level map (e.g., an immediately preceding level map). The immediately preceding graph of the level-k (e.g., level-2) graph is a level (k-1) (e.g., level-1) graph. The context according to the present disclosure may be less complex, resulting in an efficient model for coding the level graph.
When encoding a level k graph, a fully coded level (k-1) graph and a partially coded level k graph may be used as context information for context modeling. Embodiments of the present invention may reduce the cardinality of the reference sample set by coding one coefficient value at a time before moving to the next transform coefficient, as compared to transform coefficient coding for other video systems. This is because, as further described herein, the information from the level (k-1) graph and the partially coded level k graph is binary information. The binary information enables the use of complex spatial neighborhood templates for context modeling binary information. Such spatially neighboring templates may better capture the statistical properties of the transform block, especially with larger transform block sizes.
Fig. 6 is a flow diagram of a process 600 for encoding a transform block in an encoded video bitstream using a level map, according to an embodiment of the disclosure. Process 600 may be implemented in an encoder, such as encoder 400. The encoded video bitstream may be the compressed bitstream 420 of fig. 4.
For example, process 600 may be implemented as a software program executable by a computing device, such as transmitting station 102. The software programs may include machine-readable instructions that may be stored in a memory, such as memory 204 or secondary storage 214, and executed by a processor, such as CPU 202, to cause a computing device to perform process 600. In at least some implementations, the process 600 may be performed in whole or in part by the entropy encoding stage 408 of the encoder 400.
The process 600 is now explained with reference to fig. 7. Fig. 7 is a schematic diagram illustrating stages of transform coefficient coding using a level map according to an embodiment of the present disclosure. More specifically, fig. 7 shows the levels as a coefficient coding tree 700. Fig. 7 includes a zig-zag forward scan order 702, a transform block 704, a non-zero map 706, a level 1 map 707, a level 2 map 709, an end of block (EOB) map 726, a sign map 732, and a coefficient residual map 734. The scanning order is also referred to as the scanning direction.
At 602, a non-zero map is encoded. The non-zero map indicates the location of a transform block that contains non-zero transform coefficients. The non-zero map may also be referred to as a level 0 map.
In an embodiment, no zero transform coefficients beyond (i.e., after) the last non-zero transform coefficient based on the scan order of the transform block are indicated in the non-zero map. For example, using the zig-zag forward scan order 702 to scan the transform block 704, the last non-zero transform coefficient 708 corresponding to scan order position 11 is the last indicated transform coefficient at the last non-zero coefficient 710 in the non-zero map 706. For the transform coefficients corresponding to scan positions 12-15 of the zig-zag forward scan order 702, there is no indication value in the non-zero map 706.
At 604, the process 600 encodes the corresponding lower range level map. Each lower range map has a map level up to the highest map level. The lower range level map indicates which values of the non-zero transform coefficients are equal to the map level of the lower range map and which values of the non-zero transform coefficients are greater than the map level.
For each picture level k, the lower range level picture level is encoded up to the maximum picture level T k . Each lower range level map indicates which values in the transform block are equal to the map level of the lower range level map and which values in the transform block are greater than the map level. Thus, process 600, which uses multiple runs (i.e., each run corresponding to a level k =1,2...., T), decomposes the coding of transform coefficients into a series of binary decisions, each corresponding to a magnitude level. The binary decision of the coefficients at the rows and columns (r, c) in the transform block of level k can be defined by:
level k [r][c]=1 if absolute (coefficient [ r)][c])>k
=0 if absolute (coeffient [ r ] [ c ]) is ≦ k
For example, for k =1 (i.e., for level 1 fig. 707), the process 600 determines, for each transform coefficient of the transform block 704, whether the absolute value of the transform coefficient is greater than k (i.e., 1) or less than or equal to k. For transform coefficient 720 (i.e., at r =0, c = 0), because the absolute value of-7 (i.e., | 7| = 7) is greater than 1, process 600 sets the corresponding value 722 of level 1 map 707 to 1. For the last non-zero transform coefficient 708 (i.e., at r =2, c = 2), the process 600 sets the corresponding value 716 of the level 1 map 707 to 0 because the absolute value of-1 (i.e., | 1| = 1) is equal to k (i.e., 1). The last non-zero transform coefficient (e.g., last non-zero transform coefficient 708) in the transform block may be referred to as the highest AC coefficient.
In an embodiment, to generate a lower level graph, the process 600 may scan back a previous level graph starting from the last 1 value of the previous level graph. For a level k map, the previous level map is a level (k-1) map corresponding to the previous map level (k-1). That is, for k =2, the previous level map is a level 1 map. For k =1, the previous level map is a level 0 map (i.e., a non-zero map). For level 1 graph 707, the scan of non-zero graph 706 starts with the last non-zero coefficient 710. For level 2 fig. 709, the scan of level 1 fig. 707 begins with the last non-zero coefficient 724. In generating the level k map, process 600 need only process transform coefficients corresponding to the 1 values in level (k-1). Process 600 does not need to process transform coefficients corresponding to non-1 values because those values have been determined to be equal to k-1 (i.e., zero values for the level (k-1) map) or less than k-1 (i.e., null values for the level (k-1) map).
In an embodiment, the maximum graph level T may be fixed. For example, the maximum picture level T may be provided as a configuration to the process 600, may be hard coded in a program implementing the process 600, or may be set statistically or adaptively based on previously coded transform blocks or other blocks of the encoded video bitstream. Alternatively, the maximum graph level T is determined by the process 600. That is, process 600 may test different values of the maximum map level T (i.e., T =1,2,3, 4.. And.) and determine which value provides the best compression performance. A decoder such as decoder 500 of fig. 5 that results in optimal compression can decode and use the value of the maximum picture level T in the video bitstream. The maximum map level T has been determined to be 2 or 3 compared to other values of the maximum map level T to provide acceptable compression.
At 606, process 600 encodes the coefficient residual map. Each residual coefficient of the coefficient residual map corresponds to a respective (i.e., co-located) non-zero transform coefficient of the transform block having an absolute value exceeding the maximum map level. The residual coefficient of the transform coefficient at the position (r, c) of the transform block may be calculated using equation (1):
residue[r][c]＝absolute(coefficient[r][c])-T-1 (1)
fig. 7 shows a coefficient residual map 734. In the example of fig. 7, the maximum graph level T is equal to two (2). Thus, the coefficient residual map 734 contains the residuals of the transform coefficients of the transform block 704 whose absolute values are greater than 2. The residual coefficient is the extent to which the absolute value of the transform coefficient exceeds the maximum map level T. The absolute values of the two values of the transform block 704, i.e., the transform coefficient 720 (i.e., | 7| =7>, 2) and the transform coefficient 739 (i.e., |4| =4>, 2), are greater than the value of the maximum graph level T (i.e., 2). Coefficient residual map 734 includes residual 736 and residual 738, respectively. Using equation (1), residual 736 is set to 5 (i.e., absolute (-7) -3= 4), and residual 738 is set to 1 (i.e., absolute (4) -3= 1).
The residual coefficients of the coefficient residual map 734 may be encoded in the encoded video bitstream using binary coding. A statistical probability distribution of the residual coefficients of the fitted coefficient residual map may be used. The probability distribution may be a geometric distribution, a laplacian distribution, a pareto distribution, or any other distribution.
Encoding residual coefficients in an encoded video bitstream provides several benefits, for example, on video coding systems that encode transform coefficients. Since each residual coefficient is smaller in magnitude than its corresponding transform coefficient, fewer bits are required to encode the residual coefficients. In addition, since the residual coefficients to be encoded (e.g., 2 in the coefficient residual map 734 of fig. 7) are less than the non-zero transform coefficients (e.g., 7 in the transform block 704 of fig. 7), additional compression may result.
In an embodiment of process 600, the symbol map may also be encoded. The sign indicates which transform coefficients in the transform block have positive values and which transform coefficients have negative values. Transform coefficients that are indicated as zero in the symbol map are not required. The symbol diagram 732 of fig. 7 shows an example of a symbol diagram of the transform block 704. In the symbol diagram, negative transform coefficients are indicated by-1, while positive transform coefficients are indicated by 1. In some embodiments, the sign of a positive coefficient may be indicated with 0 and the sign of a negative coefficient with 1.
In an implementation of process 600, encoding a non-zero map at 602 may further include: an end-of-block map of the transform block is generated, and the non-zero map and the end-of-block map are interleaved in the encoded video bitstream.
The end-of-block map indicates whether a non-zero transform coefficient of the transform block is the last non-zero coefficient with respect to a given scan order. If the non-zero coefficient is not the last non-zero coefficient in the transform block, it may be indicated in the end-of-block map with a binary value of 0 (zero). On the other hand, if the non-zero coefficient is the last non-zero coefficient in the transform block, it may be indicated by a binary value of 1 (one) in the end-of-block map.
For example, since transform coefficient 720 of transform block 704 is followed by another non-zero transform coefficient (e.g., transform coefficient-1 corresponding to scan position 2), transform coefficient 720 is not the last non-zero transform coefficient, so it is indicated with an end-of-block value 728 of zero. On the other hand, since the transform coefficient corresponding to scan position 11 (i.e., the last non-zero transform coefficient 708) is the last non-zero coefficient of transform block 704, it is indicated by an end-of-block value 730 of 1 (one).
In an embodiment, encoding the non-zero map at 602 may also include determining a coding context for the value of the non-zero map (i.e., the value to be coded). The coding context of the value to be coded at the current position (r, c) may be based on previously coded non-zero neighboring values of the value to be coded in the non-zero map. The coding context may also be based on the location of the value to be coded within the non-zero graph.
As described above, the context information may be determined based on the number of non-zero previously coded neighbors of the current location and may be calculated using the following sum.
non _ zero _ map _ sum (r, c) = Σ (r ', c'), [ n (r, c) ] nb (r, c) nz _ map (r ', c') (2) in equation (2), non _ zero _ map _ sum (r, c) is the number of non-zero previously coded neighbors of the non-zero block to be coded value at position (r, c), nb (r, c) is the set of previously coded neighbors of the value to be coded at position (r, c) of the non-zero map, and nz _ map (r ', c') is the value at position (r ', c') in the non-zero map. Equation (1) is further explained with reference to fig. 8.
Fig. 8 is a schematic diagram of a previously coded neighbor in a non-zero graph 800 in accordance with implementations of the present disclosure. Fig. 8 includes a value to be encoded, a current value 802, an unavailable context neighbor 806 (i.e., a neighboring value for which context information is not available), and a coded context neighbor, such as coded context neighbor 808. Ten coding context neighbors 804 are shown. Which values are contained in the neighbor set depends on the scanning order. For example, using the zig-zag forward scan order 702 of fig. 7, the set of neighbors shown in fig. 8 includes coded context neighbors 808, which include neighbors above and to the left of the current value 802. For the current value 802, non zero map problem sum (2, 2) =5. This value (i.e., 5) may be used as context information for determining a probability model for coding the current values 802 of the non-zero graph 800.
As described above, the coding context may also be based on the position of the value to be coded within the non-zero map (or equivalently, within the transform block). The locations of the transform blocks may be grouped into context groups. For example, four context groups may be set: a first group corresponding to the DC coefficients (i.e., r =0 and c = 0), a second group corresponding to the top row except for the AC coefficients (i.e., r =0 and c > 0), a third group corresponding to the leftmost column except for the AC coefficients (i.e., r >0 and c = 0), and a fourth group corresponding to all the other coefficients (i.e., r >0 and c > 0). Thus, the current value 802 corresponds to the fourth context group.
In an embodiment, encoding the non-zero map at 602 may further include determining a coding context for each value of the end-of-block map. The process 600 may determine a context model for a value to be encoded of an end-of-block map based on a location of the value to be encoded relative to frequency information of a transform block. That is, the position of the transform coefficient in the transform block may be used as a context for determining a context model for encoding a corresponding (i.e., co-located) value to be encoded of the block end map. The transform block may be divided into regions such that each region corresponds to a context. The division may be based on the following principle: the probability that the end of block is located at the DC position of the transform block is very low but the probability increases further from the DC coefficient.
In some implementations, the lower range level map may be a binary map having dimensions corresponding to the dimensions of the transform block and a map level k as described above. The position of the lower range level map may be set to one (1) when the corresponding value in the previous level map (i.e., level map k-1 as described below) is one (1) and the corresponding transform coefficient is greater than map level k of the lower range level map. When the corresponding value in the previous level map has a value of 1 and the corresponding transform coefficient is equal to the map level k of the lower range level map, the position of the lower range level map may be set to a value of 0. The position of the lower range level map may have no value when the corresponding value in the previous level map has a value of 0.
In an embodiment of process 600, encoding the lower range level map at 604 may further include determining a level map coding context for the values of the lower range level map based on a scan order of the lower range level map. As described above, encoding the values of the lower range level map k corresponds to encoding the binary value, i.e., whether the corresponding (i.e., co-located) transform coefficient of the transform block is equal to or higher than k. The encoding of binary values results in simple contexts. Thus, a plurality of adjacent values of a value may be used as context for a context model for determining said value.
As also described above, the scanning of the lower range level map may be performed in a backward (also referred to as reverse) scan order. Thus, when encoding a value, the adjacent values to the lower and right of the value to be encoded will have been encoded (e.g., if the scan order is a zig-zag forward scan order or 702 of fig. 7). Thus, the first neighbor value in the lower range level graph (e.g., the neighbor values below and to the right) may be used as context. In addition, the second adjacent values (e.g., top and left adjacent values) in the immediately preceding level (k-1) graph may also be used as context. For k >2, the previous level map of the lower range level k map is a lower range level (k-1) map; and the previous level map of the level 1 map is a non-zero map.
As described above, coding transform coefficients is a multi-pass process. In the first pass, a non-zero map 706 describing the positions of non-zero coefficients in the transform block is coded in forward scan order. In subsequent passes, the values of the non-zero coefficients in reverse scan order (i.e., from the position of the highest AC coefficient to the position of the DC coefficient) are coded. The coding of non-zero graph 706 may be implemented using the following steps:
1. initializing i =0, where i represents the scan position and i =0 corresponds to the DC position (e.g., transform coefficients 720).
2. A binary non-zero flag nz [ i ] is coded that indicates whether the quantized transform coefficient at scan position i is zero. For example, when the quantized transform coefficient is zero (i.e., the value in the non-zero map 706 at scan position i is zero), a zero value (nz [ i ] = 0) may be coded; otherwise (the value in the non-zero map 706 at the scanning position i is 1), a 1 value (nz [ i ] = 1) is coded. In another example, when the quantized transform coefficients are not zero, a zero value (nz [ i ] = 0) may be coded; otherwise, the 1 value (nz [ i ] = 1) is coded.
3. If nz [ i ] indicates that the transform coefficients at scan position i are not zero (e.g., nz [ i ] = 1), a binary flag indicating whether all coefficients at scan positions above i are all zero is coded. That is, when the 1 value of non-zero map 706 is coded, then the value at the same scan position in end of block map 726 is coded.
4. Set i to the next scanning position (i = i + 1).
5. Steps 2-4 are repeated until EOB is satisfied (i.e., until end-of-block value 730 is coded).
6. For all j > EOB, nz [ j ] =0 is set. That is, all transform coefficients after the end-of-block value 730 are set to 0.
During a quantization process, such as described with reference to quantization stage 406 of fig. 4, a rate-distortion optimized quantization (RDOQ) process determines (e.g., computes, selects, etc.) respective quantized transform coefficients for the transform coefficients of the transform block according to a rate-distortion cost for each quantized transform coefficient.
For example, RDOQ may initially provide a quantized transform coefficient Q (x) in response to receiving the transform coefficient value x. The quantized transform coefficients Q (x) may be obtained by first minimizing distortion (e.g., loss of video quality). However, when RDOQ considers the rate (e.g., the number of bits) at which the quantized transform coefficient Q (x) is coded in addition to distortion, RDOQ may obtain another quantized transform coefficient Q' (x) that provides a better overall rate-distortion cost. This process may continue until an optimal quantized transform coefficient is obtained for the transform coefficient value x. In this way, the quantized coefficient values of the transform coefficients may change during the coding process of the transform coefficients and/or the transform blocks comprising the transform coefficients.
As described above, coding non-zero graph 706 uses a forward scan order, and coding subsequent level graphs uses a reverse scan. Thus, since the first pass and the second (or subsequent) pass coding use different scan orders: one forward and one backward, it is therefore difficult to estimate the rate cost of changing the transform coefficient values.
More specifically, in the first pass where the scan order is forward (from the DC coefficient to the highest AC coefficient), a change to the quantized coefficient value at scan position i affects the rate cost of coding the coefficients at scan position j following scan position i (i.e., j > i); and in a second pass, where the scan order is backwards (from the highest AC coefficient to the DC coefficient), the change to the quantized coefficient at scan position i affects the rate cost of coding the coefficient at scan position j '(i.e., j' < i) before scan position i.
Thus, in order to estimate the cost of coding the coefficient at the scanning position i, information from the transform coefficient at the scanning position j > i and the transform coefficient at the scanning position j' < i is required, thereby generating the bidirectional correlation. This bi-directional correlation may significantly complicate the RDOQ process.
To avoid bi-directional correlation, rather than interleaving EOB indications (i.e., end-of-block values of end-of-block diagram 726) after non-zero values of non-zero map 706, embodiments in accordance with the present disclosure may first code the EOB symbols and then process non-zero map 706 in reverse scan order. In this way, the reverse scan order can be used for all passes of the coding of the transform block using the level map. By using the reverse scan order in all passes, only information from the transform coefficient at scan position j (i.e., j > i) after the scan position of the current transform coefficient i is needed to estimate the rate cost of coding the coefficient at scan position i. In this way, complexity is reduced, resulting in a more efficient implementation of RDOQ. Thus, the use of a level map coded transform block may be implemented using the following steps:
1. the EOB is coded.
2. For all j > EOB, nz [ j ] =0 is set, and nz [ EOB ] =1 is set. If EOB <1, the process is terminated.
3. Initialization i = EOB-1.
4. Coding indicates whether the quantized transform coefficient at the scanning position i is zero (nz [ i ] = 0) or nz [ i ] which is not zero (nz [ i ] = 1).
5. Set i = i-1.
6. Repeating the steps 3-5 until i = -1.
In the above steps, the EOB is described with reference to fig. 6 to 7. That is, the EOB indicates the position of the last non-zero coefficient of the transform block. However, other semantics of EOB are possible. For example, in an embodiment, the EOB may indicate the position immediately after the last non-zero coefficient of a transform block. As such, and with reference to fig. 7 for purposes of illustration, the eob will indicate a scan location 12 (rather than the scan location 11 described with reference to fig. 6-7).
When the EOB indicates a position immediately after the last non-zero coefficient, the above steps may be given by:
1. the EOB is coded.
2. For all j ≧ EOB, nz [ j ] =0 is set, and nz [ EOB-1] =1 is set. If EOB ≦ 1, the process terminates.
3. Initialization i = EOB-2.
4. Coding indicates whether the quantized transform coefficient at the scanning position i is zero (nz [ i ] = 0) or nz [ i ] which is not zero (nz [ i ] = 1).
5. Set i = i-1.
6. Repeat steps 3-5 until i = -1.
Fig. 9 is a flow diagram of a process 900 for coding a transform block using a level map, according to an embodiment of the disclosure. Process 900 may be implemented by an encoder, such as encoder 400 of fig. 4. When implemented by an encoder, coding refers to encoding in an encoded bitstream, such as compressed bitstream 420 of fig. 4. For example, the process 900 may be performed in whole or in part by the entropy encoding stage 408 of the encoder 400. Process 900 may be performed by a decoder, such as decoder 500 of fig. 5. When implemented by a decoder, coding refers to decoding from an encoded bitstream, such as the compressed bitstream 420 of fig. 5. For example, the process 900 may be performed in whole or in part by the entropy decoding stage 502 of the decoder 500, and the encoded video bitstream may be the compressed bitstream 420 of fig. 5.
For example, embodiments of process 900 may be performed by storing instructions in a memory, such as memory 204 of receiving station 106, for execution by a processor, such as CPU 202.
At 902, process 900 codes an end-of-block indicator for a transform block. In an embodiment, the scan location of the EOB may be coded. For example, and referring to fig. 7, the scan location 11 corresponding to the last non-zero transform coefficient 708 may be coded. In another example, scan position 12 is coded, which scan position 12 corresponds to the coefficient following the last non-zero transform coefficient 708 in the forward scan order. In an embodiment, the end-of-block indicator may be coded using a context model.
At 904, process 900 codes the non-zero map in reverse scan order, starting with the last non-zero coefficient of the transform block. The non-zero map indicates which transform coefficients of the transform block have zero values and which transform coefficients of the transform block have non-zero values. Process 900 codes a non-zero map similar to non-zero map 706 of fig. 7. Process 900 codes a binary value indicating whether the quantized transform coefficients in scan order are zero or non-zero. For example, for scan position i, if the quantized transform coefficient at scan position i is zero, process 900 may code zero; otherwise, 1 is coded.
At 906, the process 900 codes the respective lower range level graph with respective graph levels up to a maximum graph level T. A lower range level map with a map level indicates which transform coefficient absolute values of a transform block are equal to the respective map level and which transform coefficient absolute values of the transform block are greater than the respective map level. When implemented by a decoder, process 900 decodes values from the encoded video bitstream to reconstruct the lower range level k map encoded as described with reference to 604 of process 600.
For example, to reconstruct a level 1 graph, the process 900 traverses back starting from the highest non-zero transform coefficient to determine which transform coefficients are equal to 1 and which are greater than 1. That is, using the reconstructed non-zero map of non-zero map 706 of FIG. 7 and starting with the last non-zero coefficient 710 and traversing back to value 740, process 900 reconstructs level 1 map 707 of FIG. 7. For each 1 value of the reconstructed non-zero map, the process 900 decodes the values from the encoded video bitstream and reconstructs the level 1 map 707. The values decoded by process 900 are zero and one (1) values.
To reconstruct the level 2 graph, process 900 uses the same procedure as used to generate the level 1 graph, except that instead of traversing the reconstructed non-zero graph, process 900 uses the reconstructed level 1 graph. The process 900 repeats all steps until a maximum number of graph levels of level graphs are reconstructed.
In an embodiment, the maximum graph level T may be provided to the process 900 via configuration. In another embodiment, the maximum picture level T is signaled by the encoder in the encoded video bitstream. Thus, the process 900 decodes the maximum picture level T from the encoded video bitstream.
At 908, the process 900 codes the coefficient residual map. Each residual coefficient of the coefficient residual map corresponds to a respective transform coefficient of the transform block having an absolute value exceeding the maximum map level. When implemented by a decoder, the process 900 reconstructs, for example, the coefficient residual map 734 of fig. 7. For each one (1) value of the level T map, process 900 decodes the corresponding residual value from the encoded bitstream to reconstruct the coefficient residual map 734 of fig. 7.
In some embodiments, process 900 may include, at 910, coding a graphical diagram. The graphical diagram may be a graphical diagram such as described with reference to the graphical diagram 732 of fig. 7. The sign indicates which transform coefficients of a transform block have positive values and which transform coefficients have negative values.
For a transform block of size nxn, in the worst case, nxn-1 bins (bins) (binary symbols) may need to be context coded to determine the EOB position. For example, when N =32, in the worst case, a total of 1023 bins may be context coded (i.e., coded using a context model) to determine the EOB location.
Some embodiments may use the set of scan locations to reduce the number of context coding bins needed to code the EOB location. Thus, coding the scan positions corresponding to the end of block positions may include coding an index of a scan position group including the scan positions, and coding an offset within the scan position group, the offset corresponding to the position of the scan position in the scan position group.
In an embodiment of the level map, the value EOB =0 may be reserved to indicate that all transform coefficients of the block are zero. That is, when EOB =0, the block is an all-zero block.
In an example of such an embodiment, the scanning positions may be divided (e.g., grouped) into 11 scanning position groups: 1,2,[3,4],[5-8],[9-16],[17-32],[33-64],[65-128],[129-256],[257-512],[513-1024]. That is, the group with index 0 contains only scan position 1; the group with index 1 contains only scan position 2; the set with index 4 includes scan locations 9-16, and so on. For example, assuming that the scan position 50 corresponding to the EOB is to be coded, the index 6 of the scan position group [33-64] containing the scan position 50 is coded, and the offset 17 (i.e., 50-33= 17) within the scan position group [33-64] is coded.
In an embodiment, the index of the group of coded scan locations corresponding to the end of a block is context coded (i.e., coded using a context model, using arithmetic coding), and in bypass mode, the offsets within the group of scan locations may be coded. Bypass mode (which may also be referred to as literal mode) refers to the fact that the value to be coded is not coded using a context model. For example, the bypass mode may be used when offset values within the range are equally possible. To code offset 17, five (5) bits are required. Thus, in this example, the number of context coding bins for the EOB is at most 10, corresponding to the group index {0, 1.., 10}.
In an embodiment, the index of the group of coded scan locations that corresponds to the end of the block is context coded (i.e., coded using a context model) and at least some of the most significant bits of the offsets within the group of scan locations are also context coded. That is, the offset value may be considered to include a prefix bit (i.e., most significant bit) and a suffix bit (i.e., least significant bit). The prefix bits may be context coded and the suffix bits may be coded using bypass mode. Any number of bits of the offset value may be considered the most significant bit. For example, offset value 17 corresponds to binary string 10001. If the first 2 bits are considered to be the most significant bits, then bit 10 is context coded and then bit 001 is bypass coded (i.e., coded using bypass mode). If the first 3 bits are considered to be the most significant bits, then bit 100 is context coded and bit 01 is bypass coded.
The scanning positions may be grouped into scanning position groups in a variety of ways. In an embodiment, and as shown in the above groups, each group may comprise a power of 2 of the number of scan locations. The power of 2 may be the group of index scan positions minus 1 (i.e., index-1). For example, the set of scan positions at index 5 (i.e., set [17-32]]) Comprises 2 5-1 (＝2 4 = 16) scanning positions. Thus, only (inx-1) bits are needed to code the offset within the group of scan locations having the index inx. For example, one (1) bit is required to pair the set with index 2 (set [3,4]]) Two (2) bits are required to code the set with index 3 (sets [9-16]]) The offset in (b) is coded, etc.
In an embodiment, the number of scanning positions in each scanning position group may be limited to a maximum predetermined number (i.e., an upper limit). For example, the group size may be limited to no more than 16. Thus, the above group can be modified as follows: 1,2, [3,4], [5-8], [9-16], [17-32], [33-48], [49-64], \ 8230; [1009-1024]. Thus, the coding offset needs not exceed 4 bits. To code the EOB locations of 50 using the modified group, the index 7 corresponding to the scan location group [49-64] may be coded using arithmetic coding with a context model. Offset 1 (= 50-49) may be coded by using 4 bits (i.e., 0001) in bypass mode.
In some embodiments, the value EOB =0 is not reserved to indicate that all transform coefficients of the block are zero. In such an embodiment, the set of scanning positions may start at 0 and end at 1023, such as 0,1, [2,3], [4-7], [8-15], [16-31], [32-63], [64-127], [128-255], [256-511], [512-1023].
As described above, the values of non-zero map 706 comprise binary values. The binary value indicates whether the transform coefficient at a given position of the transform block is zero or non-zero. Thus, the value of the non-zero map may be considered a non-zero flag. The binary values enable the use of complex spatially adjacent templates for context modeling. Such a spatially neighboring template may better capture the statistical properties of the transform blocks, especially those with larger transform block sizes. Thus, when determining the context for selecting the context model, the coding of the non-zero flags (and thus the transform coefficients) can be improved by exploiting the information of neighboring non-zero flags.
The template captures the coding history of non-zero tokens (i.e., the values of the non-zero map) that were coded prior to the current non-zero token. The template may define (e.g., specify, select, set, or in any way define) for a current scan position a scan position that will be used to determine a non-zero graph value for a context that is used to code the current value. Equivalently, the template may be defined in terms of cartesian coordinates within a non-zero graph that are used to determine non-zero values of the context.
10A-10B are diagrams of an example 1000 of a template for determining coding context according to embodiments of the present disclosure. In fig. 10A to 10B, the values shaded with the pattern 1004 (i.e., circles representing values of a non-zero map) are values to be coded; and the values shaded with pattern 1002 are the values for which context information is available because these values are coded before the value 1032 to be coded (e.g., they are context neighbors). In the example of fig. 10A-10B, the value to be coded 1032 depicts the current value of the non-zero map to be coded. The example of fig. 10A-10B is a non-limiting example. Templates having other shapes and/or sizes are also possible.
In an example, the number of non-zero values corresponding to the template position may be used as context for coding the current value. For example, a value corresponding to the template position may be added, and the sum may be used as a context. In some cases, a context neighbor may not be available, such as if the context location is outside the boundary of the block. In an example, the unavailable value may be assumed to be zero (0). In another example, the unavailable value may be assumed to be one (1).
Codecs may use a variety of transform types. For example, the transform type may be the transform type used by transform stage 404 of fig. 4 to generate the transform block. For example, the transform type (i.e., inverse transform type) may be the transform type to be used by the dequantization stage 504 of fig. 5. The available transform types may include one-dimensional discrete cosine transform (1D DCT) or an approximation thereof, one-dimensional discrete sine transform DST (1D DST) or an approximation thereof, two-dimensional DCT (2D DCT) or an approximation thereof, two-dimensional DST (2D DST) or an approximation thereof, and identity transform. Other transform types may also be used. In an example, a one-dimensional transform (1D DCT or 1D DST) may be applied in one dimension (e.g., row or column), while an identity transform is applied in another dimension.
In case a 1D transform (e.g. 1D DCT, 1D DST) is used (e.g. 1D DCT is applied to the columns (or corresponding rows) of the transform block), the quantized coefficients may be coded by using a progressive (i.e. raster) scan order or a column-wise scan order. In case of using a 2D transform (e.g., 2D DCT), the quantized coefficients may be coded using different scanning orders. As described above, different templates may be used to derive contexts for coding non-zero flags of a non-zero map based on the type of transform used. As such, in an embodiment, the template may be selected based on the transform type used to generate the transform block. As indicated above, examples of transform types include: a 1D DCT applied to a row (or column) and an identity transform applied to a column (or row); 1D DST applied to row (or column) and identity transformation applied to column (or row); 1D DCT applied to row (or column) and 1D DST applied to column (or row); 2D DCT; and 2D DST. Other combinations of transforms may include transform types.
As described above with reference to fig. 9, the non-zero map may be coded in reverse scan order, starting from the last non-zero coefficient of the transform block (i.e., starting from the highest AC transform coefficient). As such, the coded history of the current value of the non-zero map (i.e., the current non-zero flag) includes values to the right and below the current value in a two-dimensional non-zero map, such as non-zero map 706.
When the 1D vertical transform type is applied, the value to be encoded 1032 is more relevant to the vertical neighbor value than the horizontal neighbor value. In this way, the template 1010 may be used where a 1D transform (e.g., 1D DCT) is applied to the columns. Assuming that the value 1032 to be coded is at position (x, y) of the non-zero map, the template 1010 includes values at seven positions (x +1, y), (x +2, y), (x +3, y), (x +4, y), (x, y + 1), (x +1, y + 1), and (x +1, y + 2).
When the 1D horizontal transform type is applied, the value to be encoded 1032 is more relevant to the horizontal neighbor value than the vertical neighbor value. As such, the template 1020 may be used in the case of applying a 1D transform (e.g., 1D DCT) to the lines. Assuming that the value 1032 to be coded is at position (x, y) of the non-zero map, then template 1020 includes values at seven positions (x +1, y), (x, y + 1), (x +1, y + 1), (x, y + 2), (x +1, y + 2), (x, y + 3), and (x, y + 4).
The template 1030 may be used when applying 2D transform types (e.g., 2D DCT, 2D DST). Assuming that the value 1032 to be coded is at position (x, y) of the non-zero map, then template 1030 includes values at seven positions (x +1, y), (x +2, y), (x, y + 1), (x +1, y + 1), (x +2, y + 1), (x, y + 2), and (x +1, y + 2).
In some examples of templates, non-zero values scanned immediately prior to the value to be coded 1032 are not included in the template. That is, if the value to be coded 1032 is at scan position i, then a non-zero value at scan position (i-1) is not included in the template. Even if the non-zero map is coded in the reverse scan order, the scan order of the scan order may depend on the transform type. The scan order of the reverse scan order is the order of accessing the non-zero values of the non-zero map from the highest AC value to the DC value. In an example, when a 1D horizontal transform type is used, a vertical scan order may be used. Thus, the scan order is performed in a column-by-column order (e.g., from bottom to top). In an example, when a 1D vertical transform type is used, a horizontal scan order may be used. Thus, the scanning order is performed in a row-by-row order (e.g., from right to left).
The template 1060 is an example of a template that may be used when using a 2D transform (e.g., 2D DCT, 2D DST). Assuming that the value 1032 to be coded is at position (x, y) of the non-zero map, the template 1040 includes values at seven positions (x +1, y), (x +2, y), (x +3, y), (x, y + 1), (x +1, y + 1), (x, y + 2), and (x, y + 3).
Each of the templates 1010-1060 includes seven (7) positions. However, the template may include more or fewer locations and/or may have other shapes. For example, template 1070 is another example of a template that may be used when using a 2D transformation type. Template 1070 includes eight (8) positions. Assuming that the value 1032 to be coded is at position (x, y) of the non-zero map, the template 1040 includes values at eight positions (x +1, y), (x +2, y), (x +3, y), (x, y + 1), (x +1, y + 1), (x, y + 2), (x +1, y + 2), and (x, y + 3).
In other examples, a template may include five (5) positions. For example, template 1088 of FIG. 10B, which includes locations (x +1, y), (x, y + 1), (x +2, y), (x, y + 2), (x +1, y + 1), may be used for the 2D transform type. For example, templates 1080, 1082, 1083 may be used for vertical 1D transform types. Template 1080 includes locations (x +1, y), (x +2, y), (x +3, y), (x +4, y), and (x, y + 1). Template 1082 includes locations (x +1, y), (x +2, y), (x +3, y), (x +4, y), and (x +1, y + 1). Template 1083 includes positions (x, y + 1), (x, y + 2), (x, y + 3), (x +1, y + 1). For example, templates 1084, 1086, and 1087 may be used for the horizontal 1D transform type. Template 1084 contains positions (x +1, y), (x +2, y), (x +3, y), (x, y + 1), (x +1, y + 1). Template 1086 includes locations (x, y + 1), (x, y + 2), (x, y + 3), (x, y + 4), and (x +1, y + 1). Template 1087 includes locations (x, y + 1), (x, y + 2), (x, y + 3), (x, y + 4), and (x +1, y). In some embodiments, positions (x +1, y) and (x, y + 1) may be replaced by positions (x +1, y + 2) and (x +2, y + 1), respectively.
In some embodiments, the coding level k map may also use different templates (e.g., as described above) depending on the transform type, where k >0. For example, as described above, one template may be used for the 2D transform type, one template may be used for the vertical 1D transform type, and another template may be used for the horizontal 1D transform type.
In some implementations, the context for coding the non-zero flags may depend on the location of the non-zero flags (i.e., the scan location or block location). Thus, in some embodiments, transformation type dependent templates as described above may be combined with the location of non-zero flags to determine context. To avoid using too much context, which may lead to a so-called context dilution problem, a location may be classified as a region. For example, the classification may depend on the transform type.
Fig. 16 is a schematic diagram of an example 1600 of a region for determining context, according to an embodiment of the present disclosure. Fig. 16 includes a vertical transform block 1602 (i.e., a TX _ CLASS _ VERT-like transform block), a horizontal transform block 1604 (i.e., a TX _ CLASS _ HORIZ-like transform block), and a 2D transform block 1606 (i.e., a TX _ CLASS _ 2D-like transform block). The transform class corresponds to a transform type and a direction. The CLASS TX _ CLASS _ VERT is called vertical transform CLASS. The CLASS TX _ CLASS _ HORIZ is called horizontal transform CLASS. The CLASS TX _ CLASS _2D is called a two-dimensional transform CLASS.
The vertical transform block 1602 is a transform block generated using a 1D vertical transform type as described above. As such, the TX _ CLASS _ VERT CLASS includes 1D transform types (e.g., DCT, DST, ADST, or approximations thereof) applied to the columns (i.e., in the vertical direction). The horizontal transform block 1604 is a transform block generated using the 1D horizontal transform type as described above. As such, the TX _ CLASS _ HORIZ CLASS includes a type of 1D transform (e.g., DCT, DST, ADST, or an approximation thereof) applied to the row (i.e., in the horizontal direction). The 2D transform block 1606 is a transform block generated using a 2D transform type as described above. Thus, the TX _ CLASS _2D CLASS includes all the remaining transform types applied to the rows and columns.
The TX _ CLASS _ VERT-like transform block can be divided into R V (>0) A plurality of regions such that each region comprises one or more rows. The TX _ CLASS _ HORIZ-like transform block can be divided into R H (>0) And each region thus comprises one or more columns. TX _ CLASS _ 2D-like transform blocks can be divided into R 2D (>0) Each region containing one or more diagonals.
The transform block may be divided into regions in a variety of ways. In the examples, R V ＝3,R H =3 and R 2D =4, and the region classification is as follows:
for TX _ CLASS _ HORIZ-like transform blocks, the first region (i.e., region 1616) consists of the leftmost columns (col = 0), the second region (i.e., region 1618) consists of the second leftmost columns (col = 1), and the third region (i.e., region 1620) consists of the remaining columns.
For TX _ CLASS _ VERT-like transform blocks, the first region (i.e., region 1610) consists of the uppermost row (row = 0), the second region (i.e., region 1612) consists of the second uppermost row (row = 1), and the third region (i.e., region 1614) consists of the remaining rows.
For TX _ CLASS _ 2D-like transform blocks, the first region (i.e., region 1622) consists of a first anti-diagonal (row + col = 0), the second region (i.e., region 1624) consists of a second anti-diagonal (row + col = 1), the third region (i.e., region 1626) consists of third and fourth anti-diagonals (row + col =2 or 3), and the fourth region (not shown) consists of the remaining anti-diagonals.
The region and transform CLASS (e.g., TX _ CLASS _ VERT, TX _ CLASS _ HORIZ, TX _ CLASS _ 2D) combination may correspond to a set of contexts. In embodiments where a context may be retrieved in a list (e.g., table) of available contexts using an offset (corresponding to the context), each set may be distinguished (e.g., identified) by the offset of the set. In some embodiments, some transform classes may map to the same offset. Mapping transform classes to the same offset refers to transform classes that map to the same offset sharing context.
To derive a context for coding transform coefficients at a location (x, y) of a transform block, where each transform class maps to a different offset (i.e., transform classes do not share an offset), the context (i.e., ctx) may be derived as follows:
in the above example, the offset for TX _ CLASS _2D is zero (0), the offset for TX _ CLASS _ VERT is 16, and the offset for TX_CLASS _HORIZis 31. In this example, for a TX _ CLASS _2D like transform block, the first region (i.e. only the DC coefficients) has one context and each of the remaining regions has five (5) contexts. For a transform block like TX _ CLASS _ VERT or TX _ CLASS _ HORIZ, there are five (5) contexts per region. Further, in the above example, the count (e.g., sum) is calculated by using a size 7 template that depends on the transform type class as described above with reference to FIG. 10A. Thus, (counts + 1) > >1 is a number between 0 and 4 ("> >1" right shifted by 1 bit (counts + 1)).
In fig. 6 to 10, coding of a transform block (e.g., transform coefficients of the transform block) using a level map is described. However, other codecs may code transform coefficients using a coefficient token tree and/or using an alphabet of coefficient tokens that may be organized as a coefficient token tree.
In an example, to derive a context for coding transform coefficients at location (x, y) of a transform block, where TX _ CLASS _ VERT and TX _ CLASS _ HORIZ map to the same offset, the context (i.e., ctx) may be derived as follows:
mapping some transform classes to the same offset may reduce the number of contexts. Mapping transform classes (e.g., transform classes TX _ CLASS _ VERT and TX _ CLASS _ HORIZ) to the same offset may also result in improved coding performance in at least some cases (e.g., depending on the characteristics of the coded video sequence). Generally, blending (e.g., blending) statistics may negatively impact compression performance when the statistics are different. However, since the contexts given by the classes TX _ CLASS _ VERT and TX _ CLASS _ HORIZ may be statistically similar, combining the contexts of these transform classes may have a positive impact on compression performance by reducing the impact of the so-called context dilution problem.
As described above with reference to fig. 7, in fig. 7, the level maps are coded sequentially. That is, non-zero map 706 is coded, then level 1 map is coded, then level 2 map is coded, and then coefficient residual map 734 is coded. However, in some embodiments, different coding structures may be used.
As described above with reference to fig. 10A-10B, depending on the transform type, the coding level k map may also use different templates (e.g., templates as described above), where k >0. That is, as described above, the template may be used to determine a context for coding whether the coefficient at (x, y) is greater than 1 (e.g., using a level 1 graph, such as the corresponding values of level 1 graph 707 of fig. 7) or greater than 2 (e.g., using a level 2 graph, such as the corresponding values of level 2 graph 709 of fig. 7).
In this way, each coefficient may be coded until whether the coefficient is greater than the maximum graph level. In the example of fig. 7, the maximum graph level is 2. In this way, each coefficient may be coded until it is greater than 2 (using the corresponding value of the level map) before coding of the next coefficient. In an embodiment, a coefficient value greater than 2 (i.e., a coefficient value greater than the maximum map level) may be represented by a value of 3 (i.e., maximum map level + 1). As such, coding a coefficient "until whether the coefficient is greater than a maximum map level (e.g., 2)" may refer to coding a value of 0,1, 2., (maximum map level + 1) (e.g., 3) which corresponds to coefficients having values equal to 0, equal to 1, equal to 2, and greater than 2, respectively and in the case of a maximum map level of 2.
Fig. 17 is a flow diagram of a process 1700 for decoding a transform block using a level map, according to an embodiment of the disclosure. Unlike process 900, which codes level graphs sequentially (i.e., codes each graph before coding the next graph), process 1700 uses templates and codes each non-zero coefficient with a maximum graph level of 2 (i.e., T = 2), whether the coefficient is 0,1,2 or greater than 2 (represented by the value 3). That is, the process 1700 codes the coefficients before proceeding to the next coefficient in the scan order. The process 1700 may include blocks similar to those of the process 900. Descriptions of similar blocks (e.g., 902, 908, and 910) are omitted. Some embodiments of process 1700 may include block 910 before block 908.
For example, implementation of process 1700 may be performed by instructions stored in a memory, such as memory 204 of receiving station 106, for execution by a processor, such as CPU 202.
At 1702, the process 1700 determines whether there are more non-zero coefficients to code. If so, process 1700 proceeds to 1704 to code the current quantized transform coefficient at (x, y); otherwise, process 1700 proceeds to 908. At 1704, the process 1700 selects a template for coding the current quantized transform coefficients. As used in this disclosure, "selecting" means identifying, constructing, determining, specifying, generating, or otherwise selecting in any manner. The template may be the template described with reference to fig. 10A to 10B. In an example, the same template is used for coding all coefficients of a transform block. Thus, a template may be selected once for the process 1700 and may be executed prior to block 1702.
At 1706, the process 1700 uses the template to determine a context. The process 1700 can use the template to determine context in a variety of ways. Each template position corresponds to a value (e.g., 0, 1.., T + 1). Combinations of values may be used to determine context. For example, a sum of values may be used. For example, a weighted sum may be used to determine context. The weight assigned to the location may be set based on the distance to the "origin", i.e., the location (x, y) of the current transform coefficient that determines the context. Examples of distances include a scan position distance (e.g., the difference between the scan position of the current coefficient and the position of the template) or a cartesian distance. However, other ways of setting the weights may be used. In yet another example, a non-linear function may be used. For example, a maximum or minimum value in the template may be used to determine the context. In yet another example, a combination of the sum and the maximum value may be used to determine the context. Other methods and/or values, or combinations of methods and/or values, may be used to determine the context from the template.
It is now presented that the sum (i.e., addition) of the values at the template locations is used to determine the context. It should be understood that the following may be used with any method for determining context using a template.
The process 1700 may sum (sum) values corresponding to the positions of the templates. When using a template to derive context for coding coefficients, each position of the template may have one of the values 0,1,2, or 3 (i.e., when T = 2). Thus, if the template includes N positions, the maximum sum is 3 × N. For example, if a template including 5 locations is selected at 1704, the maximum sum may be 15 (= 3 × 5); if a template containing 7 positions is selected, the maximum sum may be 21 (= 3 × 7).
In an embodiment, where the level map described with reference to fig. 7 is used to determine values for the position of a template, these values for the same position in the non-zero map and the level k map may be added or counted. For example, assuming that the position of the last non-zero transform coefficient 708 is the position of the template, the value of the template at that position may be determined as the sum of the values at 712 (i.e., 1) and 716 (i.e., 0) of non-zero map 706 and level 1 map 707. Thus, the value is 1. In addition, assuming that the position of transform coefficient 739 is the position of the template, the value of the template at the position is determined as the sum of the values at the corresponding positions in non-zero map 706, level 1 map 707, and level 2 map 709. Thus, the value is 3. In this way, the context index may be determined using a sum of values corresponding to the positions of the templates, where each value of a template is determined by summing the individual values of at least some of the level maps.
In another embodiment, a level map is not generated that includes a null map and is as described with reference to FIG. 7. Instead, a single graph level may be used to indicate whether a transform coefficient is one of 0, 1. Thus, a scan position i, level [ i ] = {0, 1., T +1}, is given. A single map may include values for all transform coefficients of a transform block. Alternatively, a single map may include coefficient values up to the end-of-block coefficient. That is, a single level may comprise the value of each transform coefficient up to and including the last non-zero coefficient of the transform block.
In this way, the sum for determining the context may be generated by adding the values of the individual graph levels. For example, assume that the template includes a scan location l 1 ,l 2 ,l 3 ,l 4 And l 5 Corresponding to 5 positions, thenThe sum is determined as sum = level [ l 1]]+level[l2]+level[l3]+level[l4]+level[l5]。
Using this sum, the process 1700 can determine a context index (i.e., ctx). The context index may be determined using an operation similar to the operation described above ((counts + 1) > > 1). However, instead of using "count", the "sum" is used. Thus, process 1700 uses ((sum + 1) > > 1) to determine the context index.
The context index ctx may be out of range. For example, assuming that the sum is 15, the transform CLASS is TX _ CLASS _2D, and (x, y) is (1, 2), the context index is ((sum + 1)) > >1+6= (15 + 1) > > 1) +6=14. However, using the example above, the number of available contexts for TX _ CLASS _2D is 11. Thus, the context index is out of range. Equivalently, the sum of the context indices that result in out of range can itself be considered out of range. If the context index is out of range, the process 1700 may set the context index to a predetermined number. Thus, process 1700 may use a formula such as min (((sum + 1) > > 1), predefined number) to determine the context index. Thus, the value ((sum + 1) > > 1) is at the upper limit of the predetermined number. In an example, the predetermined number may be 4. In an example, the predetermined number may depend on the type of transform class. The predetermined number may be selected in any other way.
The context index ctx may be used to select a context for coding the current transform coefficient. At 1708, the process 1700 codes the coefficients using the context.
An embodiment of the process 1700 for using a single graph level for coding transform coefficients of a transform block may be summarized using the following process:
steps 1-10 encode bits representing the absolute values of the respective non-zero transform coefficients of the block. Steps 1-10 are repeated for each transform coefficient until the last non-zero coefficient of the transform block. In other words, the bit representing the absolute value of the non-zero transform coefficient of the current block is coded before the bit representing the absolute value of the next non-zero transform coefficient in the coded sequence. In this example, the coding sequence is a reverse scan order, but the coding sequence may be a (e.g., forward) scan order.
For the transform coefficient at scan position i, steps 2-9 encode until whether the transform coefficient of the transform block is greater than the maximum map level of the level map. That is, the step encodes bits representing the absolute value of each non-zero transform coefficient, wherein the bits include a least significant bit indicating whether the absolute value is greater than a maximum graph level of the level graph, and one or more additional bits associated with the respective graph level, each additional bit indicating whether the absolute value is at least equal to the respective graph level, starting with a most significant bit associated with a minimum graph level.
For example, assuming a maximum graph level T =2 and i < eob-1, if level [ i ] =3, then steps 2-9 code bit 111; if level [ i ] =0, then steps 2-9 code bit 0; if level [ i ] =2, then steps 2-9 code bit 110. Steps 11-14 code the sign bit of the non-zero transform coefficient (e.g., the value of the sign map 732 of fig. 7) until the last non-zero coefficient. For each non-zero coefficient greater than the maximum level map T (i.e., level [ i ] > T), steps 15-17 code the residual of the transform coefficient. The values of the residuals may be described with reference to a coefficient residual map 734. The coded sign bit and residual may be reversed.
Using the coding structure described in fig. 17 for coding transform coefficients (i.e., quantizing transform coefficients), better throughput may be obtained compared to the coding structure described with reference to fig. 9. Better throughput may mean that the transform block may be decoded faster than process 900 using the coding structure of process 1700.
Fig. 11 is a schematic diagram of a coefficient token tree 1100 that may be used for entropy coding transform blocks, according to an embodiment of the present disclosure. Coefficient token tree 1100 is referred to as a binary tree because at each node of the tree, one of two branches must be taken (i.e., traversed). Coefficient token tree 1100 (which may also be referred to as a binary token tree) specifies a range of values (e.g., sizes) of transform coefficients to be coded, and has a forward adaptive probability for each branch in the token tree. The token contribution is subtracted from the value to be coded to form a residual, and the block is then coded with a fixed probability. Similar schemes with minor variations (including backward adaptability) are also possible.
Using the coefficient token tree 1100, a binary string of coefficients is generated for the quantized coefficients of a quantized transform block (e.g., transform block 704 of fig. 7). Each binary digit is coded. Here, "coding bits" may mean outputting or generating bits in a codeword representing the transform coefficient to be encoded. Similarly, a "decoded bit" may represent a bit that reads the codeword (such as from an encoded bitstream) corresponding to the quantized transform coefficient being decoded, such that the bit corresponds to the branch traversed in the coefficient token tree. The bits are entropy coded using the context.
In an example, quantized coefficients in an nxn block (e.g., transform block 704) are organized into a 1D (one-dimensional) array (here, array u) in a prescribed scan order (e.g., scan order 702 of fig. 7). N may be 4, 8, 16, 32 or any other value. The quantized coefficients at the ith position of the 1D array may be referred to as u [ i ], where i =0, \8230;, N × N-1.
In an embodiment, the end of block (EOB) may indicate the position of the last non-zero coefficient. However, in other embodiments, and in the subsequent description of FIG. 11, unless otherwise noted, EOB represents the starting position of the last run of zeros in u [ i ], \8230;, u [ N-1 ].
In case u N-1 is not zero, EOB may be set to N. That is, EOB may be set to the value N × N if the last coefficient of the 1D array u is not zero. The value of each of u [ i ] is a quantized transform coefficient. The quantized transform coefficients of the 1D array u may also be referred to herein simply as "coefficients" or "transform coefficients". The coefficient at position i =0 corresponds to a DC coefficient. In this example eob equals 12 because there are no non-zero coefficients after the zero coefficient at position 12 of the 1D array u.
To encode and decode the coefficients u [ i ], \8230, u [ N × N-1], a token t [ i ] is generated at each position i < = eob for i =0 to N × N-1. For i < eob, the token t [ i ] may indicate the size and/or size range of the corresponding quantized transform coefficient at u [ i ]. The TOKEN for the quantized transform coefficients at EOB may be EOB TOKEN, which is a TOKEN indicating that the 1D array u does not contain non-zero coefficients after (including) the EOB position. That is, t [ EOB ] = EOB _ TOKEN indicates the EOB position of the current block. Table I provides a list of examples of TOKEN values and their corresponding names in addition to EOB TOKEN, according to embodiments of the present disclosure.
TABLE 1
In an example, the quantized coefficient values are taken as signed 12-bit integers. To represent the quantized coefficient values, the range of 12-bit signed values may be divided into 11 TOKENs (TOKENs 0-10 in Table I) plus an end-of-block TOKEN (EOB _ TOKEN). To generate tokens representing quantized coefficient values, the coefficient token tree 1100 may be traversed. The results of traversing the tree (i.e., the bit string) may then be encoded by an encoder into a bitstream (such as bitstream 420 of fig. 4) as described with reference to entropy encoding stage 408 of fig. 4.
The base value of a token is defined as the minimum number within its range. For example, token 1120 has a base value of 19. Entropy coding identifies a token for each quantized coefficient, and if a token represents a range, a residual may be formed by subtracting a base value from the quantized coefficient. For example, a quantized transform coefficient having a value of 20 may be represented by including a token 1120 and a residual value of 1 (i.e., 20 minus 19) in the encoded video bitstream to allow the decoder to reconstruct the original quantized transform coefficient. The end of block token (i.e., token 1102) signals that there are no additional non-zero quantized coefficients in the transformed block data.
In another example of transform coefficient coding, tokens available for coding transform coefficients may be divided into token groups. The available tokens may be organized as described with reference to the coefficient token tree 700. In an example, tokens are divided into two token sets: a set of head tokens and a set of tail tokens. The tokens in the set of head tokens are referred to herein as head tokens. The tokens in the tail token set are referred to herein as tail tokens. It may be logical to divide into token groups. That is, for example, a token may be considered part of a group even though data representing the token in the group may not be stored.
When coding the DC transform coefficients, the header TOKEN set includes TOKENs BLOCK _ Z _ TOKEN, ZERO _ TOKEN, ONE _ NEOB, ONE _ EOB, TWO _ PLUS _ NEOB, and TWO _ PLUS _ EOB. That is, when coding the DC coefficient, the set of head tokens may include six (6) tokens. The DC coefficient corresponds to the first scan position in the forward scan order. BLOCK _ Z _ TOKEN indicates that there are no non-zero coefficients in the transform BLOCK. The value of BLOCK _ Z _ TOKEN may be 255.
When coding transform coefficients other than DC transform coefficients, the set of header tokens includes tokens: ZERO _ TOKEN, ONE _ NEOB, ONE _ EOB, TWO _ NEOB, and TWO _ EOB. That is, when coding coefficients other than the DC coefficient, the set of head tokens includes five (5) tokens. The coefficients other than the DC coefficient are coefficients that do not correspond to the first scanning position in the forward scanning order.
The TOKEN ZERO TOKEN (which may have a value of 0 in an example) may indicate that the transform coefficient coded with ZERO TOKEN has a value of 0. The token ONE EOB (which may be 1 in the example) indicates that the current transform coefficient has a value of 1 and is immediately followed by EOB. That is, the current transform coefficient is the last non-zero coefficient of the transform block. The token ONE _ NEOB (which may have a value of 2 in an example) may indicate that the current transform coefficient has a value of 1 and is not the last non-zero transform coefficient of the transform block.
The token TWO _ PLUS _ EOB (which may have a value of 3 in an example) may indicate that the current transform coefficient has a value greater than TWO (2), and is followed by EOB. That is, the current transform coefficient is the last non-zero coefficient of the transform block. The token TWO PLUS NEOB (which may have a value of 4 in an example) may indicate that the current transform coefficient has a value greater than TWO (2) and is not the last non-zero transform coefficient of the transform block. If a TWO _ PLUS _ EOB token or a TWO _ PLUS _ NEOB token is coded, the tokens in the tail token set will also be coded.
The tail token set includes tokens: TWO _ TOKEN, THEE _ TOKEN, FOUR _ TOKEN, DCT _ VAL _ CAT1, DCT _ VAL _ CAT2, DCT _ VAL _ CAT3, DCT _ VAL _ CAT4, DCT _ VAL _ CAT5, and DCT _ VAL _ CAT6. The tail token set includes nine (9) tokens. Tokens in the tail token set are used only when the TWO EOB or TWO NEOB in the head token set is coded. As mentioned elsewhere, coding refers to encoding by an encoder or decoding by a decoder.
The codec may use the same information to derive the context used to code the token, whether it is a head or tail token. In an example, the information for determining a context includes: transform size, plane type, reference type, band position, and coefficient context.
The transform size is the least squares transform size covering the transform used to generate the transform block being coded. In an example, the transform size may be one of the values { TX _4x4, TX _8x8, TX _16x16, TX _32x32} corresponding to square transform sizes 4x4, 8x8, 16x16, and 32x32, respectively. For example, if a transform of size 8x16 is used to generate the transform block, the transform size used to determine the coding context is the value TX _16x16. As another example, if a transform of size 8X4 is used to generate the transform block, the transform size used to determine the coding context is the value TX _8X8.
The plane type indicates whether the current transform block is a luminance block or a chrominance block. Thus, the PLANE TYPE may have values of { PLANE _ TYPE _ Y, PLANE _ TYPE _ UV }, where PLANE _ TYPE _ Y corresponds to transform blocks of luma blocks, and PLANE _ TYPE _ UV corresponds to transform blocks of chroma blocks (U or V chroma blocks).
The reference type may have one of the values 0, 1. The reference type indicates whether a source block generating the transform block is intra prediction. If the block is intra predicted, the reference type may be zero (0); otherwise the reference type is one (1).
The band position may have one of the values 0,1,2,3,4, 5. The scan position of the current coefficient is mapped to one of the frequency bands 0,1,2,3,4, 5. The band positions constitute a mapping from the scanning position to the band positions. For example, scan positions 0-4 may be mapped to band position 0, scan positions 5-9 may be mapped to band position 1, scan positions 10-16 may be mapped to band position 2, and so on. Thus, if the coefficient at the scanning position 12 is encoded, the band position is 2. Other mappings are also possible.
The coefficient context may have one of the values 0,1,2,3,4, 5. In an example, the sum of the combination of the left and up-neighbor transform coefficients of the current coefficient may be mapped to one of the values 0,1,2,3,4, 5.
Thus, the total number of possible contexts is 576 (= 4 transform size 2 reference type 6 waveplate position 6 coefficient context). Each context provides a probability distribution for coding coefficient tokens. For example, the probability distribution used to code one of the nine (9) tail tokens includes nine (9) probabilities. However, since the sum of the probabilities in the probability distribution is equal to 1, the context only needs to provide eight (8) probabilities, and a ninth probability can be derived. Thus, given N (e.g., 9) symbols (e.g., tokens) of letters, N-1 (e.g., 9-1= 8) total free numbers in the probability are required. That is, for example, N-1 probability values need to be stored and retrieved.
For the above set of head tokens and tail tokens, the total free number is given by: the free number of head tokens when the DC coefficient is uncoded + the free number of head tokens when the DC coefficient is coded + the free number of tail tokens. The total free number is 7008 (2304 +96+ 4608):
1. the free number of the head token when the DC coefficient is not coded is =576 × 4=2304. Four (4) is the number of tokens in the set of head tokens minus 1 (i.e., 5-1).
2. The free number of the head token when the DC coefficient is coded = 4x2x1x6 = 96. 96 corresponds to 4 transform sizes times 2 plane types times 2 reference types times 1 band position times 6 coefficient contexts. When coding the DC coefficient, the number of possible band positions is one (1), since the position of the DC coefficient is known/fixed in the transform block.
3. The free number of the tail token is =576 × 8=4608. Eight (8) is the number of tokens in the tail token set minus 1 (i.e., 9-1).
If each probability is stored by using N bits, where N is an integer, a total of 7008 x N bits are needed to store the probability required for coefficient coding. In the case where N is 8, 54K bits of memory are required; in the case where N is 16, 108 kbits are required.
Embodiments according to the present disclosure may reduce the amount of storage required to store a probability distribution for coding transform coefficients by using a different context for coding a head token than for a tail token.
For example, although the coded head token may use the context information described above, fewer band positions may be used to determine the context of the tail token. For example, instead of using six (6) band positions as described above, three (3) band positions {0,1,2} may be used for the tail token. As described above, although the head token may be coded without coding the tail token, the tail token is coded only when the TWO _ NEOB or TWO _ EOB head token is coded. Thus, if the band position of the corresponding head token is greater than 2, the band position of the tail token may be considered as a band value of 2. The probability distribution of the tail tokens may be insensitive to greater than or equal to two given coefficient contexts.
In another example, although the coded head token may use the above-described context information and values, fewer transform types may be used to determine the context of the tail token, since the probability distribution of the tail token appears insensitive to large transform types that give coefficient context and band position-for example, instead of using four (4) transform types as described above, three (3) transform types (e.g., { TX _4x4, TX _8x8, tx16x16_above } may be used for the tail token. If the transform type of the corresponding head token is one of TX _16X16 and TX _32X32, the value TX _16x16 _abovemay be used for the tail token.
Three band transform types are used for the tail token, and the number of contexts used to code the tail token is 288 (= 4 × 2 × 3 × 6) =4 transform sizes × 2 plane types × 2 reference types × 3 band positions × 6 coefficient contexts), so that the contexts used to code the tail token are reduced by 50%. Therefore, the total free number of the coded tail token is 288 × 8=2304.
In addition, using three transform types for the tail token, the number of contexts used to code the tail token is further reduced to 216 (= 3 × 2 × 3 × 6=3 transform sizes × 2 flat types:2reference types × 3 band positions × 6 coefficient contexts), thereby reducing the context of the coded tail token by an additional 12.5%. Thus, the total number of free of the coded tail token is 216 × 8=1728.
By reducing the number of free for coding the end token to 1728, the total number of free (i.e., the number of stored probabilities) can be reduced to 4128 (= 2304+96+ 1728), constituting a 41% reduction in the stored probabilities (= (7008-4128)/7008).
Fig. 12 is a flow diagram of a process 1200 for coding a transform block using coefficient letters including a head token and a tail token, according to an embodiment of the disclosure. Process 1200 may be implemented by an encoder, such as encoder 400 of fig. 4. When implemented by an encoder, coding means coding in an encoded bitstream, such as compressed bitstream 420 of fig. 4. For example, the process 1200 may be performed in whole or in part by the entropy encoding stage 408 of the encoder 400. Process 1200 may be performed by a decoder, such as decoder 500 of fig. 5. When implemented by a decoder, coding means decoding from an encoded bitstream, such as the compressed bitstream 420 of fig. 5. For example, the process 1200 may be performed in whole or in part by the entropy decoding stage 502 of the decoder 500, and the encoded video bitstream may be the compressed bitstream 420 of fig. 5.
For example, embodiments of process 1200 may be performed by instructions stored in a memory, such as memory 204 of receiving station 106, for execution by a processor, such as CPU 202.
At 1202, process 1200 determines a header token for coding transform coefficients of a transform block. As used in this disclosure, "determining" means selecting, constructing, identifying, specifying, generating, or otherwise determining in any manner. For example, process 1200 may determine tokens by traversing a coefficient token tree (such as coefficient token tree 700 of fig. 7). In an example, the process 1200 may determine that the head token is a TWO _ PLUS _ NEOB or token TWO _ PLUS _ EOB.
At 1204, the process 1200 determines a tail token for coding the transform coefficients. For example, as described above, 1100 may determine that the tail token is one of the tokens in the tail token set described above. For example, process 1200 may determine tokens by traversing a token tree using the same coefficients as 1202.
At 1206, process 1200 determines a header context for coding the header token. As described above, the header context may be selected using one or more of a transform size, a plane type, a reference type, a band position, and a coefficient context. Other information may be used to determine the header context. In an example, and as described above, the process 1200 may determine that the header context is one of the 576 contexts described above.
At 1208, the process 1200 determines a tail context for coding the tail token. As described above, the tail context may be selected using one or more of transform size, plane type, reference type, band position, and coefficient context. Other information may be used to determine the header context. The tail context is selected from a set of contexts that includes less context than the set of contexts from which the head context was selected.
At 1210, process 1200 codes transform coefficients using a head token and a tail token.
In an embodiment, a current block (e.g., a transform block) is coded using a scan order, and scan positions of the scan order are mapped to first band positions having a first base and second band positions having a second base that is less than the first base. In an example, the first band position may be {0,1,2,3,4,5}, which is a set of cardinality 6. In an example, the second band position may be {0,1,2}, which is a radix-3 set. The first band position may be used to select a header context. The second band position may be used to select a tail context.
In an embodiment, a transform-sized head set is used to select a head context, the head set having a first cardinality, and a transform-sized tail set is used to select a tail context, the tail set having a second cardinality less than the first cardinality. In an example, the set of transform sized headers may be { TX _4x4, TX _8x8, TX _16x16, TX _32x32}, with a radix of 4. In an example, the tail set of transform sizes may be { TX _4x4, TX _8x8, TX _16x16_above }, which has a cardinality of 3.
As described above, the codec may use backward adaptation or backward update for entropy coding. As such, at the beginning of coding (i.e., encoding and decoding) a video frame (also referred to as an image) or a portion of the frame (e.g., a tile), some or all of the probabilities that will be used in entropy coding the current frame and/or tile may be initialized. The probability distribution comprises a binary probability distribution.
For example, a probability distribution for coding of an intra prediction mode may be initialized. For example, a probability distribution for coding a non-zero graph (such as non-zero graph 706 of FIG. 7) can be initialized. For example, a probability distribution for coding the indices of the scan position groups may be initialized. For example, when the bypass mode is not used for coding, a probability distribution for coding the valid bits of the offset within the group of scan locations may be initialized. For example, the probability distribution for coding the end of block of the non-zero map, as described above with reference to fig. 9, may be initialized.
The binary probability distribution for coding binary symbols (i.e., 0 and 1 symbols) can be described and stored by using N-bit values. In a non-limiting example, and where N =8, a probability value of 1 may indicate a probability that symbol 0 has 1/256, while 128 may indicate that symbol 0 and symbol 1 have the same probability of 128/256=0.5.
Embodiments according to the present disclosure may reduce the complexity (i.e., the number of bits) of storing and describing the initial probability values. In some cases, such as backward adaptation, an accurate probability value is typically not needed since the probabilities are updated as the frames and/or tiles are coded. That is, when coding a frame and/or tile, the adaptation scheme adapts the initial probability based on statistical information of the frame and/or tile. Thus, using fewer bits to store the initial probability values does not necessarily result in any worse or better coding performance.
Embodiments according to the present disclosure may map an N-bit (e.g., N = 8) probability value to a smaller M-bit representation, where M < N is an integer. The probability map may be such that the worst-case penalty associated with the map is a very small value. The probability mapping may be a non-uniform mapping, which maps a probability space [0,1 (if the probability is expressed as a real number) or [1,255 (if the probability is expressed as an integer) to a representative value that may be stored in fewer bits than the initial probability value given the initial probability of the probability distribution.
Fig. 13 is a schematic diagram of an example of a probability map according to an embodiment of the present disclosure. Probability map 1310 maps 1,255 to 31 different values, which may be stored using 5 bits. The row 1312 representing the set of 5/256,6/256,7/256 probabilities in the row 1312 corresponding to the included LEFT LIMIT of 5 and the excluded RIGHTL IMIT of 8 (i.e., partition [5, 8)) is mapped to a single PROBABILITY of 6/256 (as shown in the column REPRESENT PROBABILITY 256). Row 1314 indicates that if the initial probability falls within the partition [40, 52), then the representative probability value 45 is used.
When a probability p is mapped to another probability q, it may result in degraded coding performance. The column labeled LOSS describes the worst case percentage LOSS due to using this mapping. For example, the worst case penalty associated with rows 1312 and 1314 is 0.00367. The penalty associated with row 1312 may be described as follows: using the representative probability 6, rather than any of the exact probabilities 5,6,7, a maximum of 0.00367 additional bits may be used.
The loss is given by equation (3):
Loss＝D(p||q)/H(p) (3)
in equation (3), D (p | | q) is the Kullback-Leibler (KL) divergence defined between distributions q and p on the same letter (e.g., binary symbols 0 and 1), and H (p) represents the entropy of the distribution (p, 1-p).
The KL divergence represents the relative entropy between the two distributions (p, 1-p) and (q, 1-q). The KL divergence is given by equation (4):
entropy Η (p) can be given by equation (5):
H(p)＝-p logp-(1-p)log(1-p) (5)
given an initial probability distribution, the probability map can be determined in a number of ways. In an example, the probability map may be determined using an exhaustive search algorithm that finds partition compositions that meet a loss threshold for each partition or the entire map (i.e., the left and right limits of the mapped partitions). That is, given an initial probability distribution and a loss threshold as inputs, the search algorithm will find a suitable partition of [1,255] such that the representative probability of the partition minimizes the cost of the given partition.
Minimizing the cost of a given partition means that the penalty associated with that partition cannot be greater than the input penalty threshold. For example, the input penalty threshold for generating probability map 1310 is 0.6%, and the search algorithm determines a partition [40, 52) with a penalty of 0.367% (i.e., row 1314) whose penalty is less than the penalty threshold.
The search algorithm may proceed by finding the first partition that meets the loss threshold before continuing to find another partition. The search algorithm may start searching from above (e.g., starting from the partition containing LEFT LIMIT of 1), from below (i.e., starting from the partition ending with RIGHT LIMIT of 256), or from some other partition.
Using the search algorithm described above, and using different loss thresholds, a probability map 1320 may be obtained such that [1,255] is mapped to 43 different values. In this example, the loss is always below 0.2%. Probability map 1320 may be stored using 6 bits. The higher the number of partitions in the probability map, the smaller the loss.
In some embodiments, the encoder encodes an initial probability distribution in an encoded bitstream, such as in a header of a frame, and the decoder decodes the initial probability distribution from the encoded bitstream. Using probability mapping as described above, the number of bits required to transmit the initial distribution can be reduced. In some cases, such as when the frame is significantly compressed, the reduction in magnitude is significant.
The probability map may be used to signal probability values in a backward update of the probabilities, or to describe initial probability values to use when starting to code (encode or decode) a picture (i.e., frame) or tiles of a frame.
As described above, the current block may be predicted using intra prediction. The intra prediction mode uses pixels around the predicted current block. Pixels outside the current block are pixels outside the current block. Many different intra prediction modes may be used. Some intra-prediction modes use a single value for all pixels within a prediction block generated using at least one peripheral pixel. The other intra prediction modes (which may be referred to as directional intra prediction modes) may each have a corresponding prediction angle. Other types of intra prediction modes may also be used.
The encoder may select the intra-prediction mode as part of a rate-distortion loop. In short, various intra-prediction modes may be tested to determine which type of prediction will have the lowest distortion for a given rate or number of bits to be transmitted in the encoded video bitstream (including overhead bits included in the bitstream indicating the type of prediction used).
In an example, the following 13 intra prediction modes may be used: DC _ PRED, V _ PRED, H _ PRED, D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED, SMOOTH _ V _ PRED, and SMOOTH _ H _ PRED. One of the 13 intra prediction modes may be used to predict the luma block.
Fig. 14A is a schematic diagram 1400 of directional intra prediction modes according to an embodiment of the present disclosure. Fig. 14A shows the grouping of directional intra-prediction modes into three regions, namely regions 1402, 1404, and 1406 labeled region 1, region 2, and region 3, respectively. The intra prediction mode of one of the regions 1402, 1404, 1406 may be used to generate a prediction block of a size consistent with the current block. Region 1402 is characterized by an intra prediction mode having a prediction angle between 0 degrees and 90 degrees but not equal to 0 degrees and 90 degrees. Thus, the prediction mode with an angle in region 1 may be referred to as the northeast intra prediction mode. Region 1404 is characterized by intra prediction modes with prediction angles between 90 degrees and 180 degrees but not equal to 90 degrees and 180 degrees. Thus, the prediction mode with an angle in region 2 may be referred to as the northwest intra prediction mode. Region 1406 is characterized by intra prediction modes having prediction angles between 90 degrees and 180 degrees but not equal to 90 degrees and 180 degrees. Thus, the prediction mode having an angle in the area 3 may be referred to as a southwestern intra prediction mode. In general, the prediction modes of regions 1404-106 may be referred to as directional intra prediction modes. Thus, the prediction modes D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, and D63_ PRED are directional intra prediction modes.
Fig. 14A also shows a first pixel 1408 in a row above the current block and a second pixel 1405 in a column to the left of the current block. The first pixel 1408 and the second pixel 1405 may be used to generate a prediction block. In some implementations, the directional intra-prediction mode in region 1 (i.e., an intra-prediction mode with a prediction angle between 0 and 90 degrees) uses the first pixel 1408, but may not use the second pixel 1405 to generate the prediction block; for example, directional prediction (i.e., an intra prediction mode having a prediction angle between 90 ° and 180 °) in the region 2 uses the first pixel 1408 and the second pixel 1405 to generate a prediction block; and directional prediction in region 3 (i.e., intra prediction mode with prediction angle between 180 ° and 270 °) uses the second pixel 1405, but may not use the first pixel 1408 to generate a prediction block.
Fig. 14B is a schematic diagram of an example of an intra prediction mode according to an embodiment of the present disclosure. In the example of fig. 14B, pixels a-M may be the first pixel 1408 of fig. 14A or may be a subset thereof. The pixels I-M may be the second pixel 1405 of fig. 14A or a subset thereof.
The intra prediction mode 1410 illustrates a V _ PRED intra prediction mode, which is generally referred to as a vertical intra prediction mode. In this mode, the prediction block pixel in the first column is set to the value of the peripheral pixel a; setting a prediction block pixel of a second column to a value of pixel B; setting a prediction block pixel in the third column to the value of pixel C; and the prediction block pixel in the fourth column is set to the value of pixel D.
The intra prediction mode 1420 shows the H _ PRED intra prediction mode, which is generally referred to as a horizontal intra prediction mode. In this mode, the prediction block pixels in the first row are set to the values of the peripheral pixels I; setting the prediction block pixels in the second row to the value of pixel J; setting a prediction block pixel in the third row to the value of pixel K; and the predicted block pixel in the fourth row is set to the value of pixel L.
The intra prediction mode 1430 illustrates a D117_ PRED intra prediction mode, which is referred to as a D117_ PRED intra prediction mode because the direction of the arrow is at an angle of approximately 117 ° from horizontal, and peripheral pixels will propagate in the direction of the arrow to generate a prediction block forming a diagonal line. That is, in D117_ PRED, the predicted angle is 117 °. The intra prediction mode 1430 is a region 2 (i.e., northwest) intra prediction mode.
The intra prediction mode 1440 shows a D63_ PRED intra prediction mode, which corresponds to a prediction angle of 63 °. The intra prediction mode 1440 is a region 3 (i.e., southwest) intra prediction mode. The intra prediction mode 1450 shows a D153_ PRED intra prediction mode, which corresponds to a prediction angle of 153 °. The intra prediction mode 1450 is an area 2 (i.e., northwest) intra prediction mode. The intra prediction mode 1460 shows a D135_ PRED intra prediction mode, which corresponds to a prediction angle of 135 °. Intra-prediction mode 1460 is a region 1 (i.e., northeast) intra-prediction mode.
The prediction modes D45_ PRED and D207_ PRED (not shown) correspond to the prediction angles 45 ° (area 1 or northeast intra-prediction mode) and 207 ° (area 3 or southwest intra-prediction mode), respectively. DC _ PRED corresponds to a prediction mode in which all prediction block pixels are set to a single value as a combination of peripheral pixels a-M.
In the PAETH _ PRED intra prediction mode, the prediction value of a pixel is determined as follows: 1) Calculating the base value as a combination of some peripheral pixels, and 2) regarding one of the peripheral pixels that is closest to the base value as a predicted pixel. As an example, the PAETH _ PRED intra prediction mode is shown using pixels 1412 (at position x =1, y = 2). In some examples of combinations of peripheral pixels, the base value may be calculated as base = B + K-M. That is, the base value is equal to: the value of the left peripheral pixel in the same row as the pixel to be predicted + the value of the upper peripheral pixel in the same column as the pixel-the value of the pixel in the upper left corner.
Fig. 14C is a schematic diagram of an example of a smooth prediction mode according to an embodiment of the present disclosure. In fig. 14C, the prediction mode 1470 shows a SMOOTH _ V intra prediction mode, the prediction mode 1480 shows a SMOOTH _ H intra prediction mode, and the prediction mode 1490 shows a SMOOTH _ PRED intra prediction mode.
In the SMOOTH _ V intra prediction mode (i.e., prediction mode 1470), the prediction pixel of the lowermost row (i.e., row 1472) of the prediction block (i.e., prediction block 1407) is estimated using the value of the last pixel (i.e., pixel 1474 having the value of BL) in the left adjacent column. Thus, each pixel in row 1472 may have the same value BL. Each pixel P in the prediction block 1407 may be predicted i,j (i.e., one of the remaining pixels) is calculated as a pixel T i (e.g., with pixel P) i,j The same column of pixels in the first pixel 1408 of fig. 14A) and the value BL. The weight may depend on the pixel P i,j Distance from the uppermost and lowermost rows of the prediction block 1407. Thus, the weights may be equivalent to quadratic interpolation in the vertical direction.
In the SMOOTH _ H intra prediction mode (i.e., the prediction mode 1480), the predicted pixels of the rightmost column (i.e., the column 1482) of the prediction block (i.e., the prediction block 1407) are estimated using the value of the last pixel (i.e., the pixel 1484 having the TR value) in the uppermost adjacent row. Thus, each pixel in column 1482 may have the same value TR. Each pixel P in the prediction block 1407 can be predicted i,j (i.e., one of the remaining pixels) is calculated as pixel L j (e.g., with pixel P) i,j A pixel in the second pixel 1405 of fig. 14A on the same line) and the value TR. The weight may depend on the pixel P i,j Distance from the left and right columns of the prediction block 1407. Thus, the weights may be equivalent to a quadratic interpolation in the horizontal direction.
In the SMOOTH _ PRED intra prediction mode (i.e., prediction mode 1490), the predicted pixel of the bottom row (i.e., row 1492) of the prediction block is estimated using the value of the last pixel (pixel 1496 with BL value) in the left adjacent column, and the right-most column (i.e., column 1494) of the prediction block is estimated using the value of the last pixel (i.e., pixel 1498 with value TR) in the upper adjacent row) The predicted pixel of (2). The pixel 1499 at the intersection of the row 1492 and the column 1494 may be set to the average of the BL and TR values. The remaining pixels of the prediction block are calculated as a scaled weighted sum. For example, the prediction pixel (i.e., P) at position (i, j) of the prediction block may be predicted i,j ) Is calculated as pixel L j ,TR,T i And a scaled weighted sum of the values of the BL. Pixel L j Is a pixel in the left column and on the same row as the predicted pixel. The pixel R is a pixel provided by SMOOTH _ H. Pixel T i Are pixels in the upper row and in the same column as the predicted pixels. Pixel B is the pixel provided by SMOOTH _ V. The weights may be equivalent to quadratic interpolation in the horizontal and vertical directions.
The intra prediction mode selected by the encoder may be transmitted to the decoder in a bitstream. The intra-prediction mode may be entropy coded (encoded by an encoder and/or decoded by a decoder) using a context model.
Some codecs use the intra prediction modes of the left and upper neighboring blocks as context for coding the intra prediction mode of the current block. Using FIG. 14 as an example, the left neighboring block may be a block containing pixels I-L, and the upper neighboring block may be a block containing pixels A-D.
The codec may include a probability distribution for each combination of intra prediction modes used by the left neighbor block and the upper neighbor block. Thus, using the above 13 prediction modes, 169 probability distributions (corresponding to 13 × 13=169 contexts) can be stored. To retrieve the probability distribution, the codec may perform a process comprising the steps of:
cdf_prob kf_y_mode_cdf[13][13][13]；
left＝left neighbor intra mode(or DC_PRED if unavailable)
above＝above neighbor intra mode(or DC_PRED if
unavailable)
prob_table＝kf_y_mode_cdf[1eft][above]；
for ease of reference, the above process is referred to as a ctx combining technique. In the ctx combining technique, left stores the intra prediction mode of the left neighbor block. If the intra prediction mode of the left neighbor block is not available, the DC-PRED intra prediction mode may be assumed. Above stores the intra prediction mode of the upper neighbor block. If the intra prediction mode of the upper neighbor block is not available, the DC-PRED intra prediction mode may be assumed.
An intra-prediction mode may not be available because, for example, the current block is located at an edge of the frame (e.g., the uppermost and/or leftmost block), or an inter-prediction neighbor block (left or upper). Intra prediction modes may not be available for other reasons. The probability distribution (i.e., prob _ table) may be retrieved from a three-dimensional array (i.e., kf _ y _ mode _ cdf) that includes probability distributions for all combinations of left and upper intra prediction modes. In an example, the first dimension corresponds to a left neighbor intra prediction mode (e.g., 13 values), the second dimension corresponds to an upper neighbor intra prediction mode (e.g., 13 values), and the third dimension corresponds to values of a probability distribution.
As described above, each probability distribution includes 13 probability values (i.e., probability values for each prediction mode). Thus, there are 12 degrees of freedom (i.e., each context stores only 12 of the 13 probability values). In some embodiments, the 13 th entry may also be stored as a constant (e.g., 0).
The number of stored probability distributions and/or probability values for each probability distribution may be reduced by reducing the number of contexts used to code the intra-prediction mode.
In an example, the number of contexts may be reduced when some intra prediction modes exhibit certain characteristics. For example, based on the left intra-prediction mode and/or the upper intra-prediction mode, the probability of some intra-prediction modes may not change significantly. Such intra prediction modes may be equally probable regardless of the left and upper intra prediction modes. In an example, SMOOTH _ PRED, SMOOTH _ V _ PRED, and SMOOTH _ H _ PRED prediction modes may exhibit such characteristics.
Embodiments according to the present disclosure may reduce the number of contexts available for coding an intra prediction mode of a current block using a process including the steps of:
for ease of reference, the above procedure is referred to as the first-ctx reduction technique. Using the first-ctx reduction technique, the number of contexts can be reduced to 26. In this way, the number of contexts for intra prediction modes used to code (encode/decode) luma blocks can be reduced by approximately 85%. In step 4-4a above, if the left neighbor block and the upper neighbor block use the same intra prediction mode, the probability distribution is retrieved using the left intra prediction mode. This explains the 13 probability distributions. Steps 5-6 can be summarized as: if one of the left neighbor or the upper neighbor uses a SMOOTH prediction mode (i.e., one of SMOOTH _ PRED, SMOOTH _ V _ PRED, or SMOOTH _ H _ PRED), the other intra prediction mode is used to retrieve the probability distribution. These steps explain the other 13 probability distributions.
In an embodiment, these steps may be used to determine a context for coding an intra prediction mode for a block of a key frame (also referred to as a golden frame). In a key frame, all blocks are predicted using intra prediction.
FIG. 15 is a flow diagram of a process 1500 for intra coding a current block, according to an embodiment of the present disclosure. Process 1500 may be implemented by an encoder, such as encoder 400 of fig. 4. When implemented by an encoder, coding means encoding in an encoded bitstream, such as compressed bitstream 420 of fig. 4. For example, the process 1500 may be performed in whole or in part by the entropy encoding stage 408 of the encoder 400. Process 1500 may be performed by a decoder, such as decoder 500 of fig. 5. When implemented by a decoder, coding means decoding from an encoded bitstream, such as the compressed bitstream 420 of fig. 5. For example, the process 1500 may be performed in whole or in part by the entropy decoding stage 502 of the decoder 500, and the encoded video bitstream may be the compressed bitstream 420 of fig. 5.
For example, embodiments of process 1500 may be performed by instructions stored in a memory, such as memory 204 of receiving station 106, for execution by a processor, such as CPU 202.
At 1502, the process 1500 codes the current block using intra prediction mode. At 1504, the process 1500 determines a left intra mode (i.e., intra prediction mode) of the left neighbor block. In an embodiment, if left intra mode is not available, left intra mode may be assumed to be the default left intra prediction mode. The default left intra prediction mode may be one of the available intra prediction modes. For example, the default left intra prediction mode may be DC _ PRED mode.
At 1506, the process 1500 determines an upper intra mode (i.e., intra prediction mode) of the upper neighbor block. In an embodiment, if the upper intra mode is not available, the upper intra mode may be assumed as a default upper intra prediction mode. The default intra prediction mode may be one of the available intra prediction modes. For example, the default intra prediction mode may be the DC _ PRED mode.
At 1510, if the left intra mode and the upper intra mode are the same, process 1500 proceeds to 1512; otherwise, process 1500 proceeds to 1516. At 1512, process 1500 uses one of the left intra mode or the upper intra mode (both being the same) to determine a probability distribution for coding the intra prediction mode. In this way, on the condition that the left intra mode and the upper intra mode are equal to the same mode, the same mode is used to determine a probability distribution for coding the intra prediction mode. As used in this disclosure, "determining" refers to selecting, constructing, identifying, specifying, generating, or otherwise determining in any manner. For example, the probability distribution may be retrieved from a memory.
Some available intra-prediction modes may be classified as smooth intra-prediction modes. In an embodiment, the SMOOTH intra prediction mode may be one of SMOOTH _ PRED, SMOOTH _ H _ PRED, and SMOOTH _ V _ PRED. The process 1500 uses the other of the left intra mode and the upper intra mode to determine a probability distribution for coding the intra prediction mode on a condition that at least one of the left intra mode or the upper intra mode is the smooth intra prediction mode. Accordingly, at 1516, the process 1500 determines whether one, but not both, of the left intra mode and the up intra mode is a smooth intra prediction mode. If so, process 1500 proceeds to 1518 and determines a probability distribution for coding the intra-prediction mode using one of the left intra-mode and the upper intra-mode that is not the smooth intra-prediction mode; otherwise, process 1500 proceeds to 1520 to determine a probability distribution using either of the left intra mode and the upper intra mode. In an embodiment, the left intra prediction mode is used at 1520. From 1518 or 1520, process 1500 proceeds to 1514. At 1514, the process 1500 codes the intra-prediction mode using the probability distribution.
The number of contexts used to code the current intra prediction mode may be reduced by dividing the available intra prediction modes into classes. Each class may include one or more intra-prediction modes. The number of symbols coded for the intra prediction mode may depend on the class of intra prediction modes.
In an example, the above-mentioned 13 intra prediction modes may be divided into classes shown in table II.
TABLE II
The first symbol may indicate an intra prediction class of the intra prediction mode. Since there are six (6) intra prediction classes in table II, a total of 36 contexts are possible using the ctx combining technique to determine the probability distribution for coding the first symbol. These 36 contexts correspond to the six (6) possible classes of the left neighbor block and the six (6) possible classes of the upper neighbor class, respectively. Each context includes five (5) probability values. Using the first-ctx reduction technique, only 12 (= 6x 2) contexts are needed to code the first symbol.
If the intra prediction mode is in the SMOOTH class, a ctx combining technique is used to determine the probability distribution for coding the second symbol, for a total of nine (9) contexts. The nine (9) contexts correspond to the three (3) possible smooth intra prediction modes of the left neighbor block and the three (3) possible smooth intra prediction modes of the upper neighbor class. Using the first-ctx reduction technique, only one (1) context is needed. Each context includes 2 probability values. The SMOOTH class may include intra prediction modes such as the SMOOTH intra prediction modes described above. In other words, each SMOOTH intra prediction mode may be mapped to a SMOOTH class.
If the intra prediction mode is in the class DIRECTIONAL, a ctx combining technique is used to determine the probability distribution for coding the second symbol, for a total of possibly 36 contexts. The 36 contexts correspond to six (6) possible intra prediction modes in the DIRECTIONAL class of the left neighbor block and six (6) possible intra prediction modes in the DIRECTIONAL class of the upper neighbor class. Using the first-ctx reduction technique, only 12 contexts are needed. Each context provides five (5) probability values. As described above, the directionnal class may include an intra prediction mode as the DIRECTIONAL intra prediction mode. In other words, each DIRECTIONAL intra prediction mode may be mapped to a DIRECTIONAL class.
FIG. 18 is a flowchart of a process 1800 for coding a current block using intra prediction mode, according to an embodiment of the disclosure. In encoding a current block using intra-prediction, an encoder may encode an intra-prediction mode that is used to encode the current block into an encoded bitstream (such as compressed bitstream 420 of fig. 4). When decoding a current block using intra prediction, a decoder may decode an intra prediction mode to use for decoding the current block from an encoded bitstream (e.g., the compressed bitstream 420 of fig. 5).
At 1802, the process 1800 determines a first class of intra prediction for a first intra prediction mode used to decode a first neighboring block to the current block. At 1804, the process 1800 determines a second intra-prediction class for decoding a second intra-prediction mode of a second neighboring block to the current block. The first neighboring block and the second neighboring block may be an upper neighboring block and a left neighboring block of the current block, respectively, and vice versa. Referring to fig. 14B, as an example, in case of processing a frame including a current block in a raster scan order, the upper block may be a block including some pixels a-H, and the left block may be a block including some pixels I-L.
The first intra-prediction mode and the second intra-prediction mode are selected from a list of available intra-prediction modes. In an embodiment, the first intra prediction mode and the second intra prediction mode are each selected from a set comprising DC _ PRED, V _ PRED, H _ PRED, D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED, SMOOTH _ V, and SMOOTH _ H _ PRED and pass _ PRED intra prediction modes described above. More, fewer, or other intra prediction modes may be available.
Table II maps 13 available intra prediction modes to six classes. The directional classes may include D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED intra prediction modes. The smoothing classes may include SMOOTH _ PRED, SMOOTH _ V _ PRED, and SMOOTH _ H _ PRED intra prediction modes.
At 1806, process 1800 codes the intra-prediction mode using the first intra-prediction class and the second intra-prediction class.
In an embodiment, coding the intra prediction mode using the first intra prediction class and the second intra prediction class may include: a first symbol of a class indicating an intra prediction mode is coded, and a second symbol indicating the intra prediction mode is coded on a condition that the intra prediction mode is one of a smooth intra prediction mode or a directional intra prediction mode.
The first symbol may be a symbol of an alphabet such that each symbol of the alphabet indicates one of the intra-prediction classes. Thus, one symbol represents DC class, a second symbol represents horizontal class, a third symbol represents vertical class, and so on.
When implemented by an encoder, the process 1800 may determine that the intra prediction mode for coding the current block is the SMOOTH intra prediction mode by determining that the intra prediction mode is one of SMOOTH _ PRED, SMOOTH _ V _ PRED, and SMOOTH _ H _ PRED intra prediction modes. Alternatively, if the class of intra prediction modes is a smooth class, the process 1800 may determine that the intra prediction mode is smooth intra prediction. Similarly, the process 1800 may determine that the intra-prediction mode is the directional intra-prediction mode by determining that the intra-prediction mode is one of D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED intra-prediction modes. Alternatively, if the class of intra-prediction modes is a directional class, process 1800 may determine that the intra-prediction mode is directional intra-prediction. Alternatively, process 1800 may determine that the intra-prediction mode is one of a smooth intra-prediction mode or a directional intra-prediction mode using the first symbol.
When implemented by a decoder, process 1800 may use the first symbol to determine that the intra-prediction mode is one of a smooth intra-prediction mode or a directional intra-prediction mode. That is, after decoding the first symbol, the process 1800 may use the first symbol to determine that the intra-prediction mode is one of a smooth intra-prediction mode or a directional intra-prediction mode.
In an embodiment, only one context model may be used to code any smooth intra prediction mode. As such, if the intra-prediction mode is a smooth intra-prediction mode, the process 1800 selects a single available context model to code the second symbol. When implemented by a decoder, process 1800 may use the first symbol to determine that the intra-prediction mode is a smooth intra-prediction mode to determine the class of intra-prediction modes. In another embodiment, more than one context model may be used to code the smoothed intra-prediction mode. In this way, process 1800 codes another symbol to select a context model.
In an embodiment, only two context models may be used to code the directional intra prediction mode: a first context model and a second context model. As such, coding the second symbol indicating the intra-prediction mode may include, when the intra-prediction mode is the directional intra-prediction mode, if the first intra-prediction class and the second intra-prediction class are the same class (i.e., the same), the process 1800 selecting the first context model to code the second symbol; if the first intra-prediction class and the second intra-prediction class are different classes (i.e., not the same), the process 1800 selects a second context model to code the second symbol.
FIG. 19 is a flowchart of a process 1900 for coding a current block using an intra prediction mode according to another embodiment of the present disclosure. The process 1900 uses the classes of intra prediction modes of neighboring blocks of the current block to determine a context for coding the intra prediction mode used to code the current block.
At 1902, the process 1900 determines a first intra-prediction class for decoding a first intra-prediction mode of a first neighboring block to the current block. At 1904, the process 1900 determines a second intra prediction class for a second intra prediction mode used to decode a second neighboring block to the current block. The first neighboring block and the second neighboring block may be as described above with reference to fig. 18.
The available intra prediction modes may be mapped to classes. The mapping shown in table III may be used. However, other mappings are possible. For example, some of the classes in table III may be combined to reduce the number of classes. In another example, some classes in table III having more than one intra-prediction mode may be split into multiple classes to produce a finer mapping. As such, and using the mapping of table III, the first intra prediction mode and the second intra prediction mode may be mapped to a class selected from the set of classes comprising 8 classes (i.e., classes 0-7 of table III).
The intra prediction classes of table III are ordinal values. That is, the classes may be ordered from lowest to highest. In the mapping of table III, of the available intra prediction modes, class 0 includes DC prediction mode, class 1 includes vertical prediction mode, class 2 includes horizontal prediction mode, class 3 includes northeast intra prediction mode, class 4 includes northwest intra prediction mode, class 5 includes southwest intra prediction mode, class 6 includes smooth intra prediction mode, and class 7 includes pei prediction mode.
TABLE III
At 1906, the process 1900 may select a context model using the first intra-prediction class and the second intra-prediction class. The context model may be used to code an intra prediction mode used to code the current block. The context model may be selected from a list of available context models. In an embodiment, process 1900 uses a mapping that utilizes a first intra-prediction class and a second intra-prediction class to select a context model (e.g., a context model index). The mapping may be a symmetric mapping. That is, given first _ class, second _ class, and mapping f, f (first _ class, second _ class) = f (second _ class, first _ class) = context _ model _ index.
In an embodiment, selecting a context model using the first intra-prediction class and the second intra-prediction class may comprise selecting an index into a list of available context models using the first intra-prediction class and selecting a context model to decode the intra-prediction mode using the second intra-prediction class as an offset from the index in the list of available context models.
In an embodiment, the first intra-prediction class is greater than or equal to the second intra-prediction class. In other words, the larger of the first intra-prediction class and the second intra-prediction class may be used as an index to the list of available context models, while the smaller of the first intra-prediction class and the second intra-prediction class is used as an offset.
The get _ intra _ ctx function described below is an example of an implementation that may be used with process 1900 to select a context for coding an intra prediction mode for coding a current block. The function get _ intra _ ctxis is described using table III.
The function get _ intra _ ctx receives as input the intra prediction modes of the upper and left blocks. As described above, one or both of the received intra-prediction modes may be a default intra-prediction mode. The function get _ intra _ ctx returns the index ctx of the context model used to code the intra prediction mode of the current block. In case the available intra prediction modes and classes are as described with reference to table III, 36 context models may be used.
At step 2, the class of the upper neighboring block class _ above may be determined based on, for example, the mapping of table III. Thus, class _ above can be any value 0-7. At step 3, the class of the left neighboring block class _ left may be determined based on, for example, the mapping of Table III. Thus, class _ left can be any value 0-7. At step 4, variable low is set to the lesser of the values of class _ above and class _ left. At step 5, variable high is set to the larger of the values of class _ above and class _ left. When class _ above and class _ left are equal, the variables high and low are set to the same value. The values high and low may be one of the values 0-7, respectively.
The list of available context models may be organized such that for a given class value, the context models corresponding to that class value and all lower or equal class values are listed in turn. For example, if the high value is 5, context models corresponding to (high, low) combinations (5,0), (5,1), (5,2), (5,3), (5,4) and (5,5) are sequentially listed in the available context list. The available context models can be further organized in ascending order of high-class values. That is, prior to the context model of class N +1, the context model corresponding to class N (for N = 0.. 6) is listed. Thus, in step 1, the array ctx _ idx describes the starting position of the context model within the list of available context models, corresponding to the high intra prediction class. For a given high intra prediction class i, for i =0., 7,ctx _idx ], = (i · (i + 1))/2. For example, if the high value is 5, all possible context models of high =5 can be found starting from position (5 × 5+ 1))/2 =15 in the list of available context models.
In step 6, the high value is used as an index to find the starting index of the context model of the class corresponding to the high value, and the low value is used as an offset from the index. As such, step 6 may be used to implement step 1906 of FIG. 19. The above steps basically implement the lower triangular arrangement of the context model as described with reference to fig. 20.
FIG. 20 is a schematic diagram 2000 of context model indexing, according to an embodiment of the present disclosure. The first column 2006 corresponds to the array ctx _ idx. For example, the diagram 2000 shows that the context model corresponding to class high =6 starts from index 21 (i.e., ctx _ idx [6] = 21). Index 2002 (value 8) is the index of the context model to be used when high =3 and low =2 (i.e., ctx = ctx _ idx [3] +2=6+2= 8). Index 2004 (value 25)) is the index of the context model to be used when high =6 and low =4 (i.e., ctx = ctx _ idx [6] +4=21+4= 25).
For simplicity of explanation, processes 600, 1000, 1200, 1500, 1700, 1800, and 1900 are each illustrated and described as a series of blocks, steps, or operations. However, blocks, steps or operations in accordance with the present disclosure may occur in various orders and/or concurrently. In addition, other steps or operations not presented and described herein may be used. Moreover, not all illustrated steps or operations may be required to implement a methodology in accordance with the disclosed subject matter.
The above encoding and decoding aspects illustrate some encoding and decoding techniques. However, it should be understood that encoding and decoding (as those terms are used in the claims) may refer to compression, decompression, transformation, or any other processing or alteration of data. Further, variations and combinations of the above are possible.
For example, the teachings herein also describe a method for coding a current block, the method comprising: determining a first intra prediction mode of a left neighbor block of a current block; determining a second intra prediction mode of an upper neighbor block of the current block, in response to determining that the first intra prediction mode and the second intra prediction mode are the same mode, determining a probability distribution for coding the intra prediction mode of the current block using the same mode, and coding the intra prediction mode using the probability distribution.
Another example of the present teachings includes a method for coding a current block, the method comprising: determining a first intra prediction mode of a left neighbor block of a current block; a second intra prediction mode of an upper neighbor block of the current block is determined. In response to determining that the first intra-prediction mode and the second intra-prediction mode are not the same mode and in response to determining that only one of the first intra-prediction mode and the second intra-prediction mode is the smooth intra-prediction mode, determining a probability distribution for coding an intra-prediction mode of the current block using the other of the first intra-prediction mode and the second intra-prediction mode, and coding the intra-prediction mode using the probability distribution.
The word "example" or "embodiment" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" or "embodiment" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "embodiment" is intended to present concepts in a concrete fashion. As used in this application, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless specified otherwise, or clear from context, "X includes a or B" is intended to mean any of the natural inclusive permutations. That is, if X includes A; x comprises B; or X includes A and B, then "X includes A or B" is satisfied under any of the foregoing circumstances. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form. Furthermore, unless described as such, the use of the term "embodiment" or "one implementation" throughout is not intended to refer to the same embodiment or implementation.
Implementations of transmitting station 102 and/or receiving station 106 (as well as algorithms, methods, instructions, etc. stored thereon and/or executed thereby, including by encoder 400 and decoder 500) may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, a microcontroller, a server, a microprocessor, a digital signal processor, or any other suitable circuitry. In the claims, the term "processor" should be understood to encompass any of the foregoing hardware, alone or in combination. The terms "signal" and "data" are used interchangeably. Furthermore, portions of transmitting station 102 and receiving station 106 need not necessarily be implemented in the same manner.
Further, in an aspect, transmitting station 102 or receiving station 106 may be implemented, for example, using a general purpose computer or a general purpose processor having a computer program that, when executed, performs any of the respective methods, algorithms, and/or instructions described herein. Additionally or alternatively, for example, a special purpose computer/processor may be used, which may contain other hardware for performing any of the methods, algorithms, or instructions described herein.
Transmitting station 102 and receiving station 106 may be implemented, for example, on computers in a videoconferencing system. Alternatively, transmitting station 102 may be implemented on a server, and receiving station 106 may be implemented on a device separate from the server, such as a handheld communication device. In this case, transmitting station 102 may encode the content into an encoded video signal using encoder 400 and transmit the encoded video signal to a communication device. The communication device may then decode the encoded video signal using the decoder 500. Alternatively, the communication device may decode content stored locally on the communication device, e.g., content not transmitted by transmitting station 102. Other embodiments of transmitting station 102 and receiving station 106 are available. For example, the receiving station 106 may be a generally stationary personal computer rather than a portable communication device, and/or a device that includes the encoder 400 may also include the decoder 500.
Furthermore, all or portions of embodiments of the present disclosure may take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium. A computer-usable or computer-readable medium may be any apparatus that can, for example, tangibly embody, store, communicate, or transport the program for use by or in connection with any processor. The medium may be, for example, an electrical, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media are also available.
Further embodiments are summarized in the following examples:
example 1: an apparatus for coding a current block, comprising: a memory; and a processor, the memory including instructions executable by the processor to: determining a first intra-prediction class for a first intra-prediction mode used to decode a first neighboring block of the current block; determining a second intra-prediction class for a second intra-prediction mode used to decode a second neighboring block to the current block; the intra-prediction mode is coded using the first intra-prediction class and the second intra-prediction class.
Example 2: the apparatus of example 1, wherein the first neighboring block is above the current block and the second neighboring block is to the left of the current block in raster scan order.
Example 3: the apparatus of example 1 or 2, wherein the first intra prediction mode, the second intra prediction mode, and the intra prediction mode are each selected from a set including DC _ PRED, V _ PRED, H _ PRED, D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED, SMOOTH _ V _ PRED, SMOOTH _ H _ PRED, and pass _ PRED intra prediction modes.
Example 4 (11): the apparatus of any one of examples 1 to 3, wherein the first intra-prediction mode and the second intra-prediction mode are each selected from a set comprising a DC class, a horizontal class, a vertical class, a smooth class, a Perth (Paeth) class, and an oriented class.
Example 5: the apparatus of example 4, wherein the directional class includes D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED intra prediction modes.
Example 6: the apparatus of example 4, wherein the smoothing classes comprise SMOOTH _ PRED, SMOOTH _ V _ PRED, and SMOOTH _ H _ PRED intra prediction modes.
Example 7: the apparatus of any of examples 1-6, wherein the instructions to code the intra-prediction mode using the first intra-prediction class and the second intra-prediction class comprise instructions to: encoding a first symbol indicating a class indicating an intra prediction mode; code a second symbol indicating an intra-prediction mode on a condition that the intra-prediction mode is one of a smooth intra-prediction mode or a directional intra-prediction mode.
Example 8: the apparatus of example 7, wherein the instructions to code the second symbol indicating the intra prediction mode comprise instructions to: on a condition that the intra-prediction mode is a smooth intra-prediction mode, a single available context model for coding the second symbol is selected.
Example 9: the apparatus of example 7, wherein the instructions to code the second symbol indicating the intra prediction mode comprise instructions to: the method further includes selecting a first context model for coding the second symbol on a condition that the intra-prediction mode is a directional intra-prediction mode and on a condition that the first intra-prediction class and the second intra-prediction class are of a same class, and selecting a second context model for coding the second symbol on a condition that the first intra-prediction class and the second intra-prediction class are of a different class.
Example 10: a method for coding a current block, comprising: determining a first intra-prediction class for a first intra-prediction mode used to decode a first neighboring block of the current block; determining a second intra-prediction class for a second intra-prediction mode used to decode a second neighboring block to the current block; the first intra-prediction class and the second intra-prediction class are used to select a context model for coding an intra-prediction mode for the current block, the context model being selected from a list of available context models.
Example 11: the method of example 10, wherein the first class of intra prediction and the second class of intra prediction are sequence values, wherein the first class of intra prediction is greater than or equal to the second class of intra prediction, wherein the context model used to code the intra prediction mode is selected from a list of available context models using the first class of intra prediction and the second class of intra prediction, comprising: selecting an index to a list of available context models using a first intra-prediction class; the second intra-prediction class is used as an offset from an index in the list of available context models to select a context model for decoding the intra-prediction mode.
Example 12: the method of example 10 or 11, wherein the first intra-prediction mode and the second intra-prediction mode are selected from a set of classes comprising a northeast intra-prediction class, a northwest intra-prediction class, and a southwest intra-prediction class.
The above-described embodiments, embodiments and aspects have been described in order to facilitate understanding of the present disclosure, and do not limit the present disclosure. On the contrary, the disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent arrangements as is permitted under the law.
Claims (20)
1. A method for coding a current block using an intra prediction mode, comprising:
a mapping from available intra-prediction modes to intra-prediction classes is defined,
wherein the first number of intra-prediction classes is less than the second number of available intra-prediction modes,
wherein each class is an ordinal value, an
Wherein each available intra-prediction mode is uniquely mapped to one of the intra-prediction classes;
determining a first class of intra prediction for a first intra prediction mode used to decode a first neighboring block to the current block using the mapping;
determining a second intra-prediction class for a second intra-prediction mode used to decode a second neighboring block of the current block using the mapping;
using the first intra-prediction class and the second intra-prediction class as indices to a list of available context models to select a context model for coding the intra-prediction mode; and
coding the intra-prediction mode using the context model.
2. The method of claim 1, wherein coding the intra-prediction mode using the context model comprises:
encoding the intra-prediction mode in a compressed bitstream using the context model.
3. The method of claim 1, wherein coding the intra-prediction mode using the context model comprises:
decoding the intra-prediction mode from a compressed bitstream using the context model.
4. The method as set forth in claim 1, wherein,
wherein the first neighboring block of the current block is a block above the current block, an
Wherein the second neighboring block to the current block is to the left of the current block.
5. The method of claim 1, wherein the first and second intra prediction modes are respectively selected from a set including DC _ PRED, V _ PRED, H _ PRED, D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED, SMOOTH _ V _ PRED, SMOOTH _ H _ PRED, and pass _ PRED intra prediction modes.
6. The method of claim 1, wherein using the mapping to determine the first intra-prediction class of the first intra-prediction mode for decoding the first neighboring block to the current block comprises:
select a default first intra-prediction mode for the first intra-prediction mode if the first neighboring block is unavailable.
7. The method of claim 6, wherein the default first intra prediction mode is a DC _ PRED intra prediction mode.
8. An apparatus for decoding a current block using an intra prediction mode, comprising:
a memory; and
a processor configured to execute instructions stored in the memory to:
determining a first intra-prediction class of a first intra-prediction mode for decoding a first neighboring block of the current block, wherein the first intra-prediction class is not a mode for intra-predicting the current block;
determining a second intra-prediction class of a second intra-prediction mode for decoding a second neighboring block of the current block, wherein the second intra-prediction class is not a mode for intra-predicting the current block; and
selecting a context model for coding the intra-prediction mode using the first intra-prediction class and the second intra-prediction class, wherein the context model is selected from a list of available context models.
9. The apparatus of claim 8, wherein selecting the context model for coding the intra-prediction mode using the first intra-prediction class and the second intra-prediction class comprises:
using the first intra-prediction class and the second intra-prediction class as indices to a list of available context models.
10. The apparatus as set forth in claim 9, wherein,
wherein the first intra-prediction class and the second intra-prediction class are ordinal values, an
Wherein using the first intra-prediction class as a first index to the list of available context models and the second intra-prediction class as a second index to the list of available context models results in selecting the same context model as using the second intra-prediction class as the first index to the list of available context models and the first intra-prediction class as the second index to the list of available context models.
11. The apparatus of claim 8, wherein the context model comprises a probability value such that an initial value of the probability value is represented using N bits and an updated value of the probability value is represented using M bits, where N < M.
12. The apparatus of claim 11, wherein the instructions further comprise instructions to:
decoding the initial values of the probability values from a compressed bitstream.
13. The apparatus of claim 11, wherein the instructions further comprise instructions to:
setting the first intra-prediction mode to a default first intra-prediction mode in response to determining that the first neighboring block of the current block is unavailable.
14. An apparatus for coding a current block using an intra prediction mode, the apparatus configured to:
including a mapping from available intra-prediction modes to classes of intra-prediction, wherein a first number of the classes of intra-prediction is less than a second number of the available intra-prediction modes, wherein each class is an ordinal value, and wherein each available intra-prediction mode is uniquely mapped to one of the classes of intra-prediction;
determining a first intra-prediction class of a first intra-prediction mode for decoding a first neighboring block of the current block using the mapping;
determining a second intra-prediction class for a second intra-prediction mode used to decode a second neighboring block of the current block using the mapping;
using the first intra-prediction class and the second intra-prediction class as indices to a list of available context models to select a context model for coding the intra-prediction mode; and
coding the intra-prediction mode using the context model.
15. The apparatus of claim 14, wherein coding the intra-prediction mode using the context model comprises:
encoding the intra-prediction mode in a compressed bitstream using the context model.
16. The apparatus of claim 14, wherein coding the intra-prediction mode using the context model comprises:
decoding the intra-prediction mode from a compressed bitstream using the context model.
17. The apparatus of claim 14, wherein the first and second electrodes are disposed on opposite sides of the substrate,
wherein the first neighboring block of the current block is a block above the current block, an
Wherein the second neighboring block of the current block is to the left of the current block.
18. The apparatus of claim 14, wherein the first and second intra prediction modes are respectively selected from a set comprising DC _ PRED, V _ PRED, H _ PRED, D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED, SMOOTH _ V _ PRED, SMOOTH _ H _ PRED, and pass _ PRED intra prediction modes.
19. The apparatus of claim 14, wherein determining the first class of intra-prediction of the first intra-prediction mode for decoding the first neighboring block to the current block using the mapping comprises:
select a default first intra-prediction mode for the first intra-prediction mode if the first neighboring block is unavailable.
20. The apparatus of claim 19, wherein the default first intra prediction mode is a DC PRED intra prediction mode.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202211126406.5A CN115604472B (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
Applications Claiming Priority (9)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762575716P | 2017-10-23 | 2017-10-23 | |
US62/575,716 | 2017-10-23 | ||
US15/798,495 | 2017-10-31 | ||
US15/798,495 US10484695B2 (en) | 2017-10-23 | 2017-10-31 | Refined entropy coding for level maps |
US15/819,651 | 2017-11-21 | ||
US15/819,651 US10440369B2 (en) | 2017-10-23 | 2017-11-21 | Context modeling for intra-prediction modes |
CN201880036331.7A CN110679148B (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
CN202211126406.5A CN115604472B (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
PCT/US2018/042352 WO2019083583A1 (en) | 2017-10-23 | 2018-07-16 | Methods and apparatuses for coding a block of video data |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880036331.7A Division CN110679148B (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
Publications (2)
Publication Number | Publication Date |
---|---|
CN115604472A true CN115604472A (en) | 2023-01-13 |
CN115604472B CN115604472B (en) | 2024-05-07 |
Family
ID=66170234
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202211126389.5A Pending CN115604471A (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
CN202211127567.6A Pending CN115604473A (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
CN201880036331.7A Active CN110679148B (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
CN202211126406.5A Active CN115604472B (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
Family Applications Before (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202211126389.5A Pending CN115604471A (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
CN202211127567.6A Pending CN115604473A (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
CN201880036331.7A Active CN110679148B (en) | 2017-10-23 | 2018-07-16 | Method and apparatus for coding blocks of video data |
Country Status (4)
Country | Link |
---|---|
US (6) | US10484695B2 (en) |
EP (1) | EP3701720A1 (en) |
CN (4) | CN115604471A (en) |
WO (1) | WO2019083583A1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2521828A (en) * | 2013-12-23 | 2015-07-08 | Sony Corp | Data encoding and decoding |
US10484695B2 (en) | 2017-10-23 | 2019-11-19 | Google Llc | Refined entropy coding for level maps |
CN109788290A (en) * | 2017-11-13 | 2019-05-21 | 慧荣科技股份有限公司 | Image processor and the lossless image compressing method for utilizing intra prediction |
US10869060B2 (en) * | 2018-01-30 | 2020-12-15 | Google Llc | Efficient context model computation design in transform coefficient coding |
EP3777156A1 (en) * | 2018-03-29 | 2021-02-17 | FRAUNHOFER-GESELLSCHAFT zur Förderung der angewandten Forschung e.V. | Transform coefficient block coding |
US10645381B2 (en) * | 2018-04-30 | 2020-05-05 | Google Llc | Intra-prediction for smooth blocks in image/video |
US11113846B2 (en) | 2018-08-31 | 2021-09-07 | Hulu, LLC | Coefficient context modeling in video coding |
CN113228645A (en) * | 2018-12-28 | 2021-08-06 | 韩国电子通信研究院 | Image encoding/decoding method and apparatus, and recording medium storing bit stream |
US11212555B2 (en) * | 2019-06-19 | 2021-12-28 | Tencent America LLC | Method of reducing context models for entropy coding of transform coefficient significant flag |
US11553208B2 (en) * | 2019-11-27 | 2023-01-10 | Tencent America LLC | Method and system of video coding using a subset of available intra prediction modes for multiple reference lines |
KR20230004797A (en) * | 2020-05-01 | 2023-01-06 | 베이징 바이트댄스 네트워크 테크놀로지 컴퍼니, 리미티드 | Entropy Coding for Partition Syntax |
EP4169256A1 (en) * | 2020-06-23 | 2023-04-26 | SSIMWAVE Inc. | Scaling factor detection for compressed images and videos |
CN111787324B (en) * | 2020-06-29 | 2021-05-04 | 北京大学 | Method for rate distortion optimization quantization, encoding method and device |
US11368694B1 (en) * | 2021-01-26 | 2022-06-21 | Meta Platforms, Inc. | Architecture for rate estimation in video coding |
WO2023104144A1 (en) * | 2021-12-09 | 2023-06-15 | Mediatek Inc. | Entropy coding transform coefficient signs |
WO2024026332A1 (en) * | 2022-07-28 | 2024-02-01 | Apple Inc. | Coefficient-based transform and mode signaling |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2013074327A (en) * | 2011-09-26 | 2013-04-22 | Jvc Kenwood Corp | Image decoder, image decoding method and image decoding program |
CN104272736A (en) * | 2013-01-16 | 2015-01-07 | 黑莓有限公司 | Context determination for entropy coding of run-length encoded transform coefficients |
US20160373741A1 (en) * | 2015-06-18 | 2016-12-22 | Qualcomm Incorporated | Intra prediction and intra mode coding |
CN107046642A (en) * | 2011-11-07 | 2017-08-15 | 华为技术有限公司 | Video Decoder and video encoder with enhancing CABAC decodings |
CN110679148A (en) * | 2017-10-23 | 2020-01-10 | 谷歌有限责任公司 | Method and apparatus for coding blocks of video data |
Family Cites Families (59)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2205704B (en) | 1987-04-01 | 1991-06-19 | Univ Essex | Reduced bandwidth video transmission based on two-component model |
US6157676A (en) | 1997-07-31 | 2000-12-05 | Victor Company Of Japan | Digital video signal inter-block interpolative predictive encoding/decoding apparatus and method providing high efficiency of encoding |
EP0905651A3 (en) * | 1997-09-29 | 2000-02-23 | Canon Kabushiki Kaisha | Image processing apparatus and method |
US6282322B1 (en) | 1998-12-03 | 2001-08-28 | Philips Electronics North America Corp. | System and method for compressing and decompressing images |
US6680974B1 (en) | 1999-12-02 | 2004-01-20 | Lucent Technologies Inc. | Methods and apparatus for context selection of block transform coefficients |
DE50305419D1 (en) | 2002-05-02 | 2006-11-30 | Fraunhofer Ges Forschung | Encoding and decoding of transform coefficients in image or video encoders |
US7289674B2 (en) | 2002-06-11 | 2007-10-30 | Nokia Corporation | Spatial prediction based intra coding |
JP3679083B2 (en) | 2002-10-08 | 2005-08-03 | 株式会社エヌ・ティ・ティ・ドコモ | Image encoding method, image decoding method, image encoding device, image decoding device, image encoding program, image decoding program |
US7660475B2 (en) | 2004-12-22 | 2010-02-09 | Ntt Docomo, Inc. | Method and apparatus for coding positions of coefficients |
KR100750128B1 (en) | 2005-09-06 | 2007-08-21 | 삼성전자주식회사 | Method and apparatus for intra prediction of video |
US7822117B1 (en) * | 2005-09-30 | 2010-10-26 | Ambarella, Inc. | Video encoder rate control using bit usage measuring and allocating by macroblock classes |
CN100488254C (en) * | 2005-11-30 | 2009-05-13 | 联合信源数字音视频技术（北京）有限公司 | Entropy coding method and decoding method based on text |
US7778472B2 (en) | 2006-03-27 | 2010-08-17 | Qualcomm Incorporated | Methods and systems for significance coefficient coding in video compression |
US7995649B2 (en) * | 2006-04-07 | 2011-08-09 | Microsoft Corporation | Quantization adjustment based on texture level |
US7486211B2 (en) | 2007-04-13 | 2009-02-03 | Apple Inc. | Method and system for entropy coding |
TWI388218B (en) | 2007-10-30 | 2013-03-01 | Nippon Telegraph & Telephone | Image encoding method and decoding method, programs therefor, and storage media for storing the programs |
KR101375668B1 (en) | 2008-03-17 | 2014-03-18 | 삼성전자주식회사 | Method and apparatus for encoding transformed coefficients and method and apparatus for decoding transformed coefficients |
NO330107B1 (en) * | 2009-07-09 | 2011-02-21 | Tandberg Telecom As | Computer implemented early dropping |
DK3457689T3 (en) | 2010-05-25 | 2020-10-12 | Lg Electronics Inc | NEW PLANAR PREDICTION MODE |
US9215470B2 (en) | 2010-07-09 | 2015-12-15 | Qualcomm Incorporated | Signaling selected directional transform for video coding |
US9025661B2 (en) | 2010-10-01 | 2015-05-05 | Qualcomm Incorporated | Indicating intra-prediction mode selection for video coding |
US9497472B2 (en) | 2010-11-16 | 2016-11-15 | Qualcomm Incorporated | Parallel context calculation in video coding |
CN103283237B (en) | 2010-12-22 | 2017-03-22 | Lg电子株式会社 | Intra prediction method and apparatus using the method |
KR101739579B1 (en) | 2011-01-04 | 2017-05-24 | 에스케이 텔레콤주식회사 | Video Encoding/Decoding Method and Apparatus Using Unit-level Parallel Intra Prediction |
US8913662B2 (en) | 2011-01-06 | 2014-12-16 | Qualcomm Incorporated | Indicating intra-prediction mode selection for video coding using CABAC |
KR101444667B1 (en) | 2011-01-15 | 2014-09-30 | 에스케이 텔레콤주식회사 | Video Coding Method and Apparatus Using Bi-Direction Intra Prediction |
US9420294B2 (en) | 2011-02-23 | 2016-08-16 | Lg Electronics Inc. | Intra-prediction method using filtering, and apparatus using the method |
US9106913B2 (en) | 2011-03-08 | 2015-08-11 | Qualcomm Incorporated | Coding of transform coefficients for video coding |
US9338449B2 (en) | 2011-03-08 | 2016-05-10 | Qualcomm Incorporated | Harmonized scan order for coding transform coefficients in video coding |
CA2839560C (en) * | 2011-06-16 | 2016-10-04 | Fraunhofer-Gesellschaft Zur Forderung Der Angewandten Forschung E.V. | Entropy coding of motion vector differences |
EP2727355A1 (en) * | 2011-06-29 | 2014-05-07 | Motorola Mobility LLC | Methods and system for using a scan coding pattern during intra coding |
CN107371154B (en) * | 2011-09-29 | 2021-06-01 | 北京三星通信技术研究有限公司 | Method for realizing anonymous reporting of MDT (minimization of drive test) measurement |
US10645398B2 (en) | 2011-10-25 | 2020-05-05 | Texas Instruments Incorporated | Sample-based angular intra-prediction in video coding |
WO2013067435A1 (en) | 2011-11-04 | 2013-05-10 | Huawei Technologies Co., Ltd. | Differential pulse code modulation intra prediction for high efficiency video coding |
EP2775713B1 (en) * | 2011-11-04 | 2021-06-16 | Sharp Kabushiki Kaisha | Arithmetic decoding device, image decoding device, arithmetic coding device, image coding device, and arithmetic decoding method |
US9451287B2 (en) * | 2011-11-08 | 2016-09-20 | Qualcomm Incorporated | Context reduction for context adaptive binary arithmetic coding |
JP6134651B2 (en) * | 2011-12-28 | 2017-05-24 | シャープ株式会社 | Arithmetic decoding apparatus, arithmetic encoding apparatus, and arithmetic decoding method |
US20130182772A1 (en) * | 2012-01-13 | 2013-07-18 | Qualcomm Incorporated | Determining contexts for coding transform coefficient data in video coding |
US9191670B2 (en) | 2012-01-17 | 2015-11-17 | Qualcomm Incorporated | Throughput improvement for CABAC coefficient level coding |
US9363510B2 (en) * | 2012-03-02 | 2016-06-07 | Qualcomm Incorporated | Scan-based sliding window in context derivation for transform coefficient coding |
CN103369315B (en) * | 2012-04-06 | 2016-08-24 | 华为技术有限公司 | The coding of the intra prediction mode of chromatic component, coding/decoding method, equipment and system |
US11025922B2 (en) * | 2012-06-13 | 2021-06-01 | Texas Instruments Incorporated | Inverse transformation using pruning for video coding |
US20130336386A1 (en) * | 2012-06-18 | 2013-12-19 | Qualcomm Incorporated | Sample adaptive offset (sao) coding |
US9538175B2 (en) * | 2012-09-26 | 2017-01-03 | Qualcomm Incorporated | Context derivation for context-adaptive, multi-level significance coding |
US9544597B1 (en) | 2013-02-11 | 2017-01-10 | Google Inc. | Hybrid transform in video encoding and decoding |
US9215464B2 (en) * | 2013-09-19 | 2015-12-15 | Blackberry Limited | Coding position data for the last non-zero transform coefficient in a coefficient group |
US9813737B2 (en) * | 2013-09-19 | 2017-11-07 | Blackberry Limited | Transposing a block of transform coefficients, based upon an intra-prediction mode |
US9392288B2 (en) | 2013-10-17 | 2016-07-12 | Google Inc. | Video coding using scatter-based scan tables |
US9179151B2 (en) | 2013-10-18 | 2015-11-03 | Google Inc. | Spatial proximity context entropy coding |
US10097833B2 (en) * | 2014-12-26 | 2018-10-09 | Intel Corporation | Method and system of entropy coding using look-up table based probability updating for video coding |
KR20170116043A (en) * | 2015-02-12 | 2017-10-18 | 엘지전자 주식회사 | Method and apparatus for processing video signals using graph-based transforms |
WO2016195453A1 (en) | 2015-06-05 | 2016-12-08 | 한양대학교 산학협력단 | Image encoding and decoding method and image decoding device |
EP3379832A4 (en) * | 2015-11-22 | 2019-04-17 | LG Electronics Inc. -1- | Method and apparatus for entropy encoding and decoding video signal |
US10397600B1 (en) * | 2016-01-29 | 2019-08-27 | Google Llc | Dynamic reference motion vector coding mode |
US20180199062A1 (en) | 2017-01-11 | 2018-07-12 | Qualcomm Incorporated | Intra prediction techniques for video coding |
US10992939B2 (en) | 2017-10-23 | 2021-04-27 | Google Llc | Directional intra-prediction coding |
WO2018212582A1 (en) | 2017-05-18 | 2018-11-22 | 에스케이텔레콤 주식회사 | Intra prediction encoding or decoding method and device |
US10127913B1 (en) * | 2017-07-07 | 2018-11-13 | Sif Codec Llc | Method of encoding of data stream, method of decoding of data stream, and devices for implementation of said methods |
US10645381B2 (en) | 2018-04-30 | 2020-05-05 | Google Llc | Intra-prediction for smooth blocks in image/video |
-
2017
- 2017-10-31 US US15/798,495 patent/US10484695B2/en active Active
- 2017-11-21 US US15/819,651 patent/US10440369B2/en active Active
-
2018
- 2018-07-16 WO PCT/US2018/042352 patent/WO2019083583A1/en unknown
- 2018-07-16 CN CN202211126389.5A patent/CN115604471A/en active Pending
- 2018-07-16 CN CN202211127567.6A patent/CN115604473A/en active Pending
- 2018-07-16 CN CN201880036331.7A patent/CN110679148B/en active Active
- 2018-07-16 EP EP18755961.2A patent/EP3701720A1/en active Pending
- 2018-07-16 CN CN202211126406.5A patent/CN115604472B/en active Active
-
2019
- 2019-09-24 US US16/580,226 patent/US10834410B2/en active Active
- 2019-10-22 US US16/659,666 patent/US10893280B2/en active Active
-
2020
- 2020-03-09 US US16/812,539 patent/US11178409B2/en active Active
- 2020-12-15 US US17/121,820 patent/US11477462B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2013074327A (en) * | 2011-09-26 | 2013-04-22 | Jvc Kenwood Corp | Image decoder, image decoding method and image decoding program |
CN107046642A (en) * | 2011-11-07 | 2017-08-15 | 华为技术有限公司 | Video Decoder and video encoder with enhancing CABAC decodings |
CN104272736A (en) * | 2013-01-16 | 2015-01-07 | 黑莓有限公司 | Context determination for entropy coding of run-length encoded transform coefficients |
US20160373741A1 (en) * | 2015-06-18 | 2016-12-22 | Qualcomm Incorporated | Intra prediction and intra mode coding |
CN110679148A (en) * | 2017-10-23 | 2020-01-10 | 谷歌有限责任公司 | Method and apparatus for coding blocks of video data |
Also Published As
Publication number | Publication date |
---|---|
US20210099720A1 (en) | 2021-04-01 |
EP3701720A1 (en) | 2020-09-02 |
CN115604471A (en) | 2023-01-13 |
US20200021820A1 (en) | 2020-01-16 |
CN115604472B (en) | 2024-05-07 |
US11477462B2 (en) | 2022-10-18 |
US10484695B2 (en) | 2019-11-19 |
US10834410B2 (en) | 2020-11-10 |
US10893280B2 (en) | 2021-01-12 |
US20190124340A1 (en) | 2019-04-25 |
WO2019083583A1 (en) | 2019-05-02 |
US20200213599A1 (en) | 2020-07-02 |
CN110679148A (en) | 2020-01-10 |
US10440369B2 (en) | 2019-10-08 |
US20200053367A1 (en) | 2020-02-13 |
CN115604473A (en) | 2023-01-13 |
US20190124342A1 (en) | 2019-04-25 |
CN110679148B (en) | 2022-10-04 |
US11178409B2 (en) | 2021-11-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110679148B (en) | Method and apparatus for coding blocks of video data | |
CN110710217B (en) | Method and apparatus for coding last significant coefficient flag | |
KR102314801B1 (en) | Selective Blending for Entropy Coding in Video Compression | |
US10735767B2 (en) | Transform coefficient coding using level maps | |
CN110800299B (en) | Scan order adaptation for entropy coding blocks of image data | |
CN114143559B (en) | Efficient context model calculation design in transform coefficient coding | |
US20220377376A1 (en) | Efficient context model computation design in transform coefficient coding | |
CN110710219B (en) | Method and apparatus for context derivation for coefficient coding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant |