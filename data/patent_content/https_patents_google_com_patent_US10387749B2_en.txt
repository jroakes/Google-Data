US10387749B2 - Distance metric learning using proxies - Google Patents
Distance metric learning using proxies Download PDFInfo
- Publication number
- US10387749B2 US10387749B2 US15/710,377 US201715710377A US10387749B2 US 10387749 B2 US10387749 B2 US 10387749B2 US 201715710377 A US201715710377 A US 201715710377A US 10387749 B2 US10387749 B2 US 10387749B2
- Authority
- US
- United States
- Prior art keywords
- proxies
- proxy
- negative
- computing devices
- loss function
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000012549 training Methods 0.000 claims abstract description 82
- 238000000034 method Methods 0.000 claims abstract description 68
- 230000006870 function Effects 0.000 claims abstract description 64
- 238000013528 artificial neural network Methods 0.000 claims description 11
- 239000011159 matrix material Substances 0.000 claims description 7
- 230000004044 response Effects 0.000 claims description 3
- 238000002474 experimental method Methods 0.000 abstract description 10
- 238000009472 formulation Methods 0.000 description 14
- 239000000203 mixture Substances 0.000 description 14
- 238000003909 pattern recognition Methods 0.000 description 12
- 238000005070 sampling Methods 0.000 description 10
- 230000008901 benefit Effects 0.000 description 8
- 230000001537 neural effect Effects 0.000 description 8
- 239000013598 vector Substances 0.000 description 8
- 238000010586 diagram Methods 0.000 description 7
- 238000005457 optimization Methods 0.000 description 7
- 230000010365 information processing Effects 0.000 description 6
- 238000013459 approach Methods 0.000 description 5
- 238000004422 calculation algorithm Methods 0.000 description 5
- 238000011156 evaluation Methods 0.000 description 5
- 230000003068 static effect Effects 0.000 description 5
- 238000012360 testing method Methods 0.000 description 4
- 238000004458 analytical method Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 238000005065 mining Methods 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 230000004075 alteration Effects 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 238000010606 normalization Methods 0.000 description 2
- 238000007792 addition Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 238000003064 k means clustering Methods 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000008450 motivation Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000012545 processing Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 241000894007 species Species 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G06K9/6215—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/44—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components
- G06V10/443—Local feature extraction by analysis of parts of the pattern, e.g. by detecting edges, contours, loops, corners, strokes or intersections; Connectivity analysis, e.g. of connected components by matching or filtering
- G06V10/449—Biologically inspired filters, e.g. difference of Gaussians [DoG] or Gabor filters
- G06V10/451—Biologically inspired filters, e.g. difference of Gaussians [DoG] or Gabor filters with interaction between the filter responses, e.g. cortical complex cells
- G06V10/454—Integrating the filters into a hierarchical structure, e.g. convolutional neural networks [CNN]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/217—Validation; Performance evaluation; Active pattern learning techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/22—Matching criteria, e.g. proximity measures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/28—Determining representative reference patterns, e.g. by averaging or distorting; Generating dictionaries
-
- G06K9/4628—
-
- G06K9/6232—
-
- G06K9/6255—
-
- G06K9/6256—
-
- G06K9/6262—
-
- G06K9/66—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/74—Image or video pattern matching; Proximity measures in feature spaces
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/7715—Feature extraction, e.g. by transforming the feature space, e.g. multi-dimensional scaling [MDS]; Mappings, e.g. subspace methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/772—Determining representative reference patterns, e.g. averaging or distorting patterns; Generating dictionaries
Definitions
- the present disclosure relates generally to machine learning. More particularly, the present disclosure relates to distance metric learning using proxies.
- DML Distance metric learning
- contrastive loss which is defined for a pair of either similar or dissimilar data points.
- triplet loss is defined by a triplet of data points: an anchor point, a similar data point, and one or more dissimilar data points.
- the goal in a triplet loss is to learn a distance in which the anchor point is closer to the similar point than to the dissimilar one.
- the computer system includes a machine-learned distance model configured to receive input data points and, in response, provide respective embeddings for the input data points within an embedding space. A distance between a pair of embeddings provided for a pair of the input data points is indicative of a similarity between the pair of the input data points.
- the computer system includes one or more processors and one or more non-transitory computer readable media that collectively store instructions that, when executed by the one or more processors cause the computer system to perform operations.
- the operations include accessing a training dataset that includes a plurality of data points to obtain an anchor data point.
- the operations include inputting the anchor data point into the machine-learned distance model.
- the operations include receiving a first embedding provided for the anchor data point by the machine-learned distance model.
- the operations include evaluating a loss function that compares the first embedding to a positive proxy and one or more negative proxies. Each of the positive proxy and the one or more negative proxies serve as a proxy for two or more data points included in the training dataset.
- the operations include adjusting one or more parameters of the machine-learned distance model based at least in part on the loss function.
- the method includes accessing, by one or more computing devices, a training dataset that includes a plurality of data points to obtain an anchor data point.
- the method includes inputting, by the one or more computing devices, the anchor data point into a machine-learned distance model.
- the method includes receiving, by the one or more computing devices, a first embedding provided for the anchor data point by the machine-learned distance model.
- the method includes evaluating, by the one or more computing devices, a loss function that compares the first embedding to one or more of: a positive proxy and one or more negative proxies.
- One or more of the positive proxy and the one or more negative proxies serve as a proxy for two or more data points included in the training dataset.
- the method includes adjusting, by the one or more computing devices, one or more parameters of the machine-learned distance model based at least in part on the loss function.
- Another aspect of the present disclosure is directed to one or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause the one or more processors to perform operations.
- the operations include accessing a training dataset that includes a plurality of data points to obtain an anchor data point.
- the operations include inputting the anchor data point into a machine-learned distance model.
- the operations include receiving a first embedding provided for the anchor data point by the machine-learned distance model.
- the operations include evaluating a loss function that compares the first embedding to one or more of: a positive proxy and one or more negative proxies.
- One or more of the positive proxy and the one or more negative proxies serve as a proxy for two or more data points included in the training dataset.
- the operations include adjusting one or more parameters of the machine-learned distance model based at least in part on the loss function.
- FIG. 1 depicts a graph of example Recall@ 1 results as a function of training step on the Cars196 dataset according to example embodiments of the present disclosure.
- FIG. 2A depicts a graphical diagram of example triplets according to example embodiments of the present disclosure.
- FIG. 2B depicts a graphical diagram of example triplets formed using proxies according to example embodiments of the present disclosure.
- FIG. 3 depicts example retrieval results on a set of images from the Cars196 using a distance model trained by an example proxy-based training technique according to example embodiments of the present disclosure.
- FIG. 4 depicts a graph of Recall@ 1 results on the Stanford Product dataset according to example embodiments of the present disclosure.
- FIG. 5 depicts example retrieval results on a randomly selected set of images from the Stanford Product dataset using a distance model trained by an example proxy-based training technique according to example embodiments of the present disclosure.
- FIG. 6 depicts a graph of example Recall@ 1 results as a function of ratio of proxies to semantic labels according to example embodiments of the present disclosure.
- FIG. 7 depicts a graph of example Recall@ 1 results for dynamic assignment on the Cars196 dataset as a function of proxy-to-semantic-label ratio according to example embodiments of the present disclosure.
- FIG. 8 depicts a block diagram of an example computing system to perform distance metric learning using proxies according to example embodiments of the present disclosure.
- FIG. 9 depicts a flow chart diagram of an example method to perform distance metric learning using proxies according to example embodiments of the present disclosure.
- FIG. 10 depicts a flow chart diagram of an example method to perform distance metric learning using proxies according to example embodiments of the present disclosure.
- a machine-learned distance model can be trained in a proxy space using a loss function that compares an embedding provided for an anchor data point of a training dataset to a positive proxy and/or one or more negative proxies.
- the positive proxy and/or the one or more negative proxies can serve as a proxy for two or more data points included in the training dataset.
- each proxy can approximate a number of data points in the training dataset, enabling faster convergence.
- the loss function can compare a first distance between the embedding provided for the anchor data point and the positive proxy to one or more second distances between the embedding and the one or more negative proxies.
- the loss function can include a constraint that the first distance be less than each of the one or more second distances.
- the proxy-based loss functions provided by the present disclosure can in some ways be similar to certain existing triplet loss formulations, but can replace the use of actual positive and negative data points explicitly sampled from the dataset with positive and negative proxies that serve as a proxy for multiple of such data points.
- the proxies of the proxy space can themselves be learned parameters, such that the proxies and the model are trained jointly.
- the proxies can be contained in a proxy matrix that is viewed as part of the model structure itself or is otherwise jointly trained with the model.
- the systems and methods of the present disclosure are not required to select or otherwise identify informative triplets from the dataset at all, but instead can simply compare a given anchor data point to one or more learned proxies.
- the present disclosure provides the technical effect and benefit faster convergence (e.g., reduced training time) without sacrificing the ultimate accuracy of the model.
- the present disclosure provides example experiments which demonstrate that, in addition to the convergence benefits, models trained using the proxy-based scheme of the present disclosure achieve a new state of the art on several popular training datasets.
- the present disclosure also provides the technical effect and benefit of improved and higher performance machine-learned distance models. These models can be used to provide improved services for a number of different applications, including, for example, image retrieval, near duplicate detection, clustering, and zero-shot learning.
- the systems and methods of the present disclosure address the problem of distance metric learning (DML), which can, in some instances, be defined as learning a distance consistent with a notion of semantic similarity.
- DML distance metric learning
- Traditionally for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship: an anchor point x is similar to a set of one or more positive points Y, and dissimilar to a set of one or more negative points Z, and a loss defined over these distances is minimized.
- Example existing formulations for distance metric learning include contrastive loss or triplet loss, which encode a notion of similar and dissimilar datapoints as described above.
- the present disclosure addresses this challenge and proposes to re-define triplet-based losses over a different space of points, which are referred to herein as proxies.
- This space approximates the training set of data points. For example, for each data point in the original space, there can be a proxy point close to it.
- the present disclosure proposes to optimize the triplet loss over triplets within a proxy space, where each triplet within the proxy space includes an anchor data point and similar and dissimilar proxy points, which can be learned as well.
- These proxies can approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss.
- the proxy space is small enough so that triplets from the original dataset are not required to be selected or sampled at all, but instead the loss can be explicitly written over all (or most) of the triplets involving proxies. As a result, this re-defined loss is easier to optimize, and it trains faster.
- the proxies are learned as part of the model parameters.
- the proxy-based approach provided by the present disclosure can compare full sets of examples. Both the embeddings and the proxies can be trained end-to-end (indeed, in some implementations, the proxies are part of the network architecture), without, in at least some implementations, requiring interruption of training to re-compute the cluster centers, or class indices.
- the proxy-based loss proposed by the present disclosure is also empirically better behaved.
- the present disclosure shows that the proxy-based loss is an upper bound to triplet loss and that, empirically, the bound tightness improves as training converges, which justifies the use of proxy-based loss to optimize the original loss.
- the present disclosure demonstrates that the resulting distance metric learning problem has several desirable properties.
- the obtained metric performs well in the zero-shot scenario, improving state of the art, as demonstrated on three widely used datasets for this problem (CUB200, Cars196 and Stanford Products).
- the learning problem formulated over proxies exhibits empirically faster convergence than other metric learning approaches.
- example experiments described herein demonstrate that the proxy-loss scheme of the present disclosure improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.
- FIG. 1 depicts an example results graph of Recall@ 1 as a function of training step on the Cars196 dataset.
- an example proxy-based scheme of the present disclosure referred to as Proxy-NCA converges about three times as fast compared with certain existing baseline methods, while also resulting in higher Recall@ 1 values.
- aspects of the present disclosure address the problem of learning a distance d(x, y; ⁇ ) between two data points x and y.
- ⁇ are the parameters of the network.
- the DML task is to learn a distance respecting the similarity relationships encoded in D: d ( x,y ; ⁇ ) ⁇ d ( x,z ; ⁇ ) f or all( x,y,z ) ⁇ D (1)
- NCA loss tries to make x closer to y than to any element in a set Z using exponential weighting:
- Neural networks can be trained using a form of stochastic gradient descent, where at each optimization step a stochastic loss is formulated by sampling a subset of the training set D, called a batch.
- n.
- a triplet (x, y, z) is selected such that x and y have the same label while x and z do not.
- a triplet (x, y, z) is selected such that x and y have the same label while x and z do not.
- points are distributed evenly between k classes.
- each batch typically samples a very small subset of all possible triplets, i.e., in the order of O(b 3 ).
- O((n/b) 3 ) steps in order to see all triplets in the training one would have to go over O((n/b) 3 ) steps, while in the case of classification or regression the needed number of steps is O(n/b).
- n is typically in the order of hundreds of thousands, while b is between a few tens to about a hundred, which leads to n/b being in the tens of thousands.
- the convergence rate of the optimization procedure is highly dependent on being able to see useful triplets, e.g., triplets which give a large loss value as motivated by F. Schroff et al. Facenet: A unified embedding for face recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. These authors propose to sample triplets within the data points present in the current batch. However, the problem of sampling from the whole set of triplets D remains particularly challenging as the number of triplets is so overwhelmingly large.
- the present disclosure proposes to learn a small set of data points P with
- P it is desirable for P to approximate the set of all data points. That is, for each x there is one element in P which is close to x w.r.t. the distance metric d.
- a proxy for x Such an element is referred to herein as a proxy for x:
- the present disclosure proposes to use these proxies to express the ranking loss. Further, because the proxy set is smaller than the original training data, the number of triplets is significantly reduced.
- FIGS. 2A-B depict simplified graphical diagrams of example triplets.
- instances i.e., data points
- stars instances associated with a first semantic concept
- stars instances associated with a second semantic concept
- proxies which are represented by the larger circle and larger star
- proxies serve as a concise representation for each semantic concept, one that fits in memory.
- 8 comparisons are sufficient.
- the reformulation of the loss would implicitly encourage the desired distance relationship in the original training data.
- the ranking loss is scale invariant in x.
- re-scaling affects the distances between the embeddings and proxies.
- the ranking loss is difficult to optimize, particularly with gradient based methods. Many losses, such as NCA loss (S. Ro Stamm et al. Neighbourhood component analysis. Adv. Neural Inf. Process. Syst. (NIPS), 2004), Hinge triplet loss (K. Q. Weinberger et al. Distance metric learning for large margin nearest neighbor classification. Advances in neural information processing systems, 18:1473, 2006), N-pairs loss (K. Sohn. Improved deep metric learning with multi-class n-pair loss objective. In D. D. Lee et al., editors, Advances in Neural Information Processing Systems 29, pages 1857-1865. Curran Associates, Inc., 2016), etc. are merely surrogates for the ranking loss.
- NCA loss S. Ro Stamm et al. Neighbourhood component analysis. Adv. Neural Inf. Process. Syst. (NIPS), 2004
- Hinge triplet loss K. Q. Weinberger et al. Distance metric learning for large margin nearest neighbor classification
- the proxy approximation can be used to bound the popular NCA loss for distance metric learning.
- the proxy techniques described herein are broadly applicable to other loss formulations as well, including, for example, various other types of triplet-based methods (e.g., Hinge triplet loss) or other forms of similarity style supervision. Application of the proxy-based technique to these other loss formulations is within the scope of the present disclosure.
- This section provides an example explanation of how to use the introduced proxies to train a distance based on the NCA formulation. It is desirable to minimize the total loss, defined as a sum over triplets (x, y, Z) (see Eq. (1)). Instead, however, the upper bound is minimized, defined as a sum over triplets over an anchor and two proxies (x, p(y), p(Z)) (see Eq. (7)).
- This optimization can be performed by gradient descent, one example of which is outlined below in Algorithm 1.
- a triplet of a data point and at least two proxies is sampled, which can be defined by a triplet (x, y, z) in the original training data.
- each triplet defined over proxies upper bounds all triplets (x, y′, z′) whose positive y′ and negative z′ data points have the same proxies as y and z respectively.
- the proxies can all be held in memory, and sampling from them is simple. In practice, when an anchor point is encountered in the batch, one can use its positive proxy as y, and all negative proxies as Z to formulate triplets that cover all points in the data. Back propagation can be performed through both points and proxies, and training does not need to be paused to re-calculate the proxies at any time.
- the model can be trained with the property that all proxies have the same norm N P and all embeddings have the norm N X .
- Empirically such a model performs at least as well as without this constraint, and it makes applicable the tighter bounds discussed in Section 2.3. While the equal norm property can be incorporated the model during training, for the example experiments described herein, the model was simply trained with the desired loss, and all proxies and embeddings were re-scaled to the unit sphere (note that the transformed proxies are typically only used for analyzing the effectiveness of the bounds, but are not used during inference).
- the proxies need to be assigned for the positive and negative data points. Two example assignment procedures are described below.
- This scheme can be referred to as static proxy assignment as it is defined by the semantic label and does not change during the execution of the algorithm.
- a point x can be assigned to the closest proxy, as defined in Eq. (5).
- This scheme can be referred to as dynamic proxy assignment. See Section 5 for evaluation with the two proxy assignment methods.
- the proxy approximation error depends to a degree on the number of proxies
- the number of proxies is equal to the number of data points, and the approximation error is zero.
- the number of terms in the bound is in O(n
- the number of proxies varies from a few hundreds to a few thousands, while the number of data points is in the tens/hundreds of thousands.
- Proxy loss bounds For the following example discussion it is assumed that the norms of proxies and data points are constant
- N p and
- N x , and it is denoted that
- NCA loss (see Eq. (4)) is proxy bounded: ⁇ circumflex over (L) ⁇ NCA ( x,y,Z ) ⁇ ⁇ circumflex over (L) ⁇ NCA ( x,p y ,p z )+(1 ⁇ )log(
- ⁇ circumflex over (L) ⁇ NCA is defined as L NCA with normalized data points and
- Curran Associates, Inc., 2016 are using multiple random crops during test time, but for fair comparison with the other methods, and following the procedure in Hyun Oh Song et al. Learnable Structured Clustering Framework for Deep Metric Learning.
- the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ), 2017, the example implementations described below use only a center crop during test time.
- the RMSprop optimizer was used with the margin multiplier constant ⁇ decayed at a rate of 0.94.
- NMI Normalized Mutual Information measure
- ⁇ i contains the instances assigned to the i'th cluster.
- Let ⁇ c 1 , c 2 , . . . , c m ⁇ be the ground truth classes, where c j contains the instances from class j.
- NMI ⁇ ( ⁇ , C ) 2 ⁇ I ⁇ ( ⁇ , C ) H ⁇ ( ⁇ ) + H ⁇ ( C ) . ( 8 )
- NMI is invariant to label permutation which can be a desirable property for the evaluation.
- the Proxy-based method described herein is compared with 4 state-of-the-art deep metric learning approaches: Triplet Learning with semi-hard negative mining (Schroff et al. FaceNet: A Unified Embedding for Face Recognition and Clustering. The IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ), 2015), Lifted Structured Embedding (Oh Song, Hyun et al. Deep metric learning via lifted structured feature embedding. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016), the N-Pairs deep metric loss (Sohn, Kihyuk. Improved Deep Metric Learning with Multi-class N-pair Loss Objective. In D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R.
- the Cars196 dataset (Krause et al. 3d object representations for fine-grained categorization. Proceedings of the IEEE International Conference on Computer Vision Workshops , pages 554-561, 2013) is a fine-grained car category dataset containing 16,185 images of 196 car models. Classes are at the level of make-model-year, for example, Mazda-3-2011. In the experiments the dataset was split such that 50% of the classes are used for training, and 50% are used for evaluation. Table 1 shows recall-at-k and NMI scores for all methods on the Cars196 dataset. Proxy-NCA has a 15 percentage points (26% relative) improvement in recall@ 1 from previous state-of-the-art, and a 6% point gain in NMI. FIG. 3 shows example retrieval results on the test set of the Cars196 dataset.
- FIG. 3 Retrieval results on a set of images from the Cars196 dataset using our proposed proxy-based training method. Left column contains query images. The results are ranked by distance.
- the Stanford product dataset contains 120,053 images of 22,634 products downloaded from eBay.com. For training, 59,5511 out of 11,318 classes are used, and 11,316 classes (60,502 images) are held out for testing. This dataset is more challenging as each product has only about 5 images, and at first seems well suited for tuple-sampling approaches, and less so for the proxy formulation. Note that holding in memory 11,318 float proxies of dimension 64 takes less than 3 Mb. FIG. 4 shows recall-at-1 results on this dataset. Proxy-NCA has over a 6% gap from previous state of the art. Proxy-NCA compares favorably on clustering as well, with a score of 90.6.
- FIG. 4 Recall@ 1 results on the Stanford Product Dataset. Proxy-NCA has a 6% point gap with previous SOTA.
- FIG. 5 shows example retrieval results on images from the Stanford Product dataset. Interestingly, the embeddings show a high degree of rotation invariance.
- FIG. 5 Retrieval results on a randomly selected set of images from the Stanford Product dataset. Left column contains query images. The results are ranked by distance. Note the rotation invariance exhibited by the embedding.
- the Caltech-UCSD Birds-200-2011 dataset contains 11,788 images of birds from 200 classes of fine-grained bird species. The first 100 classes were used as training data for the metric learning methods, and the remaining 100 classes were used for evaluation. Table 2 compares the proxy-NCA with the baseline methods. Birds are notoriously hard to classify, as the inner-class variation is quite large when compared to the intra-class variation. This is apparent when observing the results in the table. All methods perform less well than in the other datasets. Proxy-NCA improves on SOTA for recall at 1-2 and on the clustering metric.
- FIG. 1 compares the training speed of all methods on the Cars196 dataset. Proxy-NCA trains much faster than other metric learning methods, and converges about three times as fast.
- FIG. 6 shows the results of an experiment in which the ratio of labels to proxies was varied on the Cars196 dataset. The static proxy assignment method was varied to randomly pre-assign semantic labels to proxies. If the number of proxies is smaller than the number of labels, multiple labels are assigned to the same proxy. So in effect each semantic label has influence on a fraction of a proxy. Note that when proxy-per-class 0.5 Proxy-NCA has better performance than previous methods.
- FIG. 6 Recall@ 1 results as a function of ratio of proxies to semantic labels. When allowed 0.5 proxies per label or more, Proxy-NCA compares favorably with previous state of the art.
- the assignment of triplets is based on the use of a semantic concept—two images of a dog need to be more similar than an image of a dog and an image of a cat. These cases are easily handled by the static proxy assignment, which was covered in the experiments above. In some cases however, there are no semantic concepts to be used, and a dynamic proxy assignment is needed. In this section results using this assignment scheme are provided.
- FIG. 7 shows recall scores for the Cars196 dataset using the dynamic assignment. The optimization becomes harder to solve, specifically due to the non-differentiable argmin term in Eq. (5). However, it is interesting to note that first, a budget of 0.5 proxies per semantic concept is again enough to improve on state of the art, and one does see some benefit of expanding the proxy budget beyond the number of semantic concepts.
- FIG. 7 Recall@ 1 results for dynamic assignment on the Cars196 dataset as a function of proxy-to-semantic-label ratio. More proxies allow for better fitting of the underlying data, but one needs to be careful to avoid over-fitting.
- the present disclosure demonstrates the effectiveness of using proxies for the task of deep metric learning.
- proxies which can be saved in memory and trained using back-propagation, training time can be reduced, and the resulting models can achieve a new state of the art.
- the present disclosure presents two proxy assignment schemes—a static one, which can be used when semantic label information is available, and a dynamic one which can be used when the only supervision comes in the form of similar and dissimilar triplets.
- the present disclosure shows that a loss defined using proxies, upper bounds the original, instance-based loss. If the proxies and instances have constant norms, it is shown that a well optimized proxy-based model does not change the ordinal relationship between pairs of instances.
- Proxy-NCA loss produces a loss very similar to the standard cross-entropy loss used in classification.
- this formulation is arrived at from a different direction: in most instances, the systems and methods of the present disclosure are not interested in the actual classifier and indeed discard the proxies once the model has been trained. Instead, the proxies are auxiliary variables, enabling more effective optimization of the embedding model parameters.
- the formulations provided herein not only surpass the state of the art in zero-shot learning, but also offer an explanation to the effectiveness of the standard trick of training a classifier, and using its penultimate layer's output as the embedding.
- the dot product of a unit normalized data points ⁇ circumflex over (x) ⁇ and ⁇ can be upper bounded by the dot product of unit normalized point ⁇ circumflex over (x) ⁇ and proxy ⁇ circumflex over (p) ⁇ y using the Cauchy inequality as follows: ⁇ circumflex over (x) ⁇ T ( ⁇ circumflex over (z) ⁇ circumflex over (p) ⁇ z ) ⁇
- FIG. 8 depicts an example computing system 102 that can implement the present disclosure.
- the computing system 102 can include one or more physical computing devices.
- the one or more physical computing devices can be any type of computing device, including a server computing device, a personal computer (e.g., desktop or laptop), a mobile computing device (e.g., smartphone or tablet), an embedded computing device, or other forms of computing devices, or combinations thereof.
- the computing device(s) can operate sequentially and/or in parallel. In some implementations, the computing device(s) can implement various distributed computing techniques.
- the computing system includes one or more processors 112 and a memory 114 .
- the one or more processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 114 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 114 can store data 116 and instructions 118 which are executed by the processor(s) 112 to cause the computing system 102 to perform operations.
- the computing system 102 can further include a machine-learned distance model 120 .
- the machine-learned distance model 120 can be or have been trained to provide, for a pair of data points, a distance between such two data points.
- the distance can be descriptive of a similarity or relatedness between the two data points, where a larger distance indicates less similarity.
- the distance model 120 can receive input data point or instance (e.g., an image) and, in response, provide an embedding within an embedding space.
- the embedding can be provided at a final layer of the model 120 or a close to final, but not final layer of the model 120 (e.g., a penultimate layer).
- the embedding provided by the model 120 for one data point or instance can be compared to an embedding provided by the model 120 for another data point or instance to determine a measure of similarity (e.g., a distance) between the two data points or instances.
- a Euclidian distance between the two embeddings can be indicative of an amount of similarity (e.g., smaller distances indicate more similarity).
- the machine-learned distance model 120 can be or include a neural network (e.g., deep neural network).
- Neural networks can include feed-forward neural networks, recurrent neural networks, convolutional neural networks, and/or other forms of neural networks.
- the machine-learned distance model 120 can be or include other types of machine-learned models.
- the machine-learned distance model 120 can include or have associated therewith a proxy matrix or other data structure that includes a number of proxies (e.g., proxy vectors).
- the proxy matrix can be viewed as parameters of the model 120 itself or can otherwise be jointly trained with the model 120 .
- the computing system 102 can further include a model trainer 122 .
- the model trainer 122 can train the machine-learned model 120 using various training or learning techniques, such as, for example, backwards propagation of errors, stochastic gradient descent, etc.
- the model trainer 122 can perform a number of generalization techniques (e.g., weight decays, dropouts, etc.) to improve the generalization capability of the models being trained.
- the model trainer 122 can train a machine-learned distance model 120 based on a set of training data 126 .
- the training dataset 126 can include instances that are labelled (e.g., have one or more labels associated therewith).
- the labels can correspond to classes or semantic concepts.
- the training dataset 126 can include instances that are unlabeled (e.g., do not have one or more labels associated therewith).
- each instance in the training dataset 126 can be or include an image.
- the model trainer 122 can include computer logic utilized to provide desired functionality.
- the model trainer 122 can be implemented in hardware, firmware, and/or software controlling a general purpose processor.
- the model trainer 122 includes program files stored on a storage device, loaded into a memory and executed by one or more processors.
- the model trainer 122 includes one or more sets of computer-executable instructions that are stored in a tangible computer-readable storage medium such as RAM hard disk or optical or magnetic media.
- the computing system 102 can also include a network interface 124 used to communicate with one or more systems or devices, including systems or devices that are remotely located from the computing system 102 .
- the network interface 124 can include any number of components to provide networked communications (e.g., transceivers, antennas, controllers, cards, etc.).
- FIGS. 9 and 10 depict flow chart diagrams of example methods 900 and 1000 to perform distance metric learning using proxies according to example embodiments of the present disclosure.
- method 900 includes static proxy assignment while method 1000 includes dynamic proxy assignment. While methods 900 and 1000 are discussed with respect to a single training example, it should be understood that they can be performed on a batch of training examples.
- a computing system can initialize a number of proxies.
- the number of proxies can be equal to a number of labels or semantic classes associated with a training dataset.
- the number of proxies can be at least one-half the number of different labels.
- any number of proxies can be used.
- the proxies can be initialized at 902 with random values.
- the computing system can assign each data point included in a training dataset to one of the number of proxies. For example, in some implementations, each data point can be assigned to a respective nearest proxy. In some implementations, each data point can have a label or semantic class associated therewith and the data point can be assigned to a proxy that is associated with such label or semantic class.
- the computing system can access the training dataset to obtain an anchor data point.
- the anchor data point can be randomly selected from the training dataset or according to an ordering or ranking.
- the computing system can input the anchor data point into a machine-learned distance model.
- the machine-learned distance model can be a deep neural network.
- the computing system can receive a first embedding provided for the anchor data point by the machine-learned distance model.
- the embedding can be within a machine-learned embedding dimensional space.
- the embedding can be provided at a final layer of the machine-learned distance model or at a close to final but not final layer of the machine-learned distance model.
- the computing system can evaluate a loss function that compares the first embedding to a positive proxy and/or one or more negative proxies.
- a loss function that compares the first embedding to a positive proxy and/or one or more negative proxies.
- One or more of the positive proxy and the one or more negative proxies can serve as a proxy for two or more data points included in the training dataset.
- the loss function can be a triplet-based loss function (e.g., triplet hinge function loss, NCA, etc.).
- the loss function can compare a first distance between the first embedding and the positive proxy to one or more second distances between the first embedding and the one or more negative proxies.
- the loss function can compare the first distance to a plurality of second distances respectively between the first embedding and a plurality of different negative proxies (e.g., all negative proxies).
- the loss function can include a constraint that the first distance is less than each of the one or more second distances (e.g., all of the second distances).
- the anchor data point can be associated with a first label; the positive proxy can serve as a proxy for all data points included in the training dataset that are associated with the first label; and the one or more negative proxies can serve as a proxy for all data points included in the training dataset that are associated with at least one second label that is different than the first label.
- a plurality of negative proxies can respectively serve as proxies for all other labels included in the training dataset.
- the computing system can adjust one or more parameters of the machine-learned model based at least in part on the loss function. For example, one or more parameters of the machine-learned model can be adjusted to reduce the loss function (e.g., in an attempt to optimize the loss function).
- the loss function can be backpropagated through the distance model. In some implementations, the loss function can also be backpropagated through a proxy matrix that holds the values of the proxies (e.g., as proxy embedding vectors).
- method 900 returns to 906 to obtain an additional anchor data point.
- the machine-learned distance model can be iteratively trained using a number (e.g., thousands) of anchor data points. Since proxies are used, the number of training iterations required to converge over the training dataset is significantly reduced. After training is complete, the machine-learned distance model can be employed to perform a number of different tasks, such as, for example, assisting in performance of a similarity search (e.g., an image similarity search).
- a similarity search e.g., an image similarity search
- a computing system can initialize a number of proxies. Any number of proxies can be used. In some implementations, the proxies can be initialized at 1002 with random values.
- the computing system can assign each data point included in a training dataset to one of the number of proxies. For example, in some implementations, each data point can be assigned to a respective nearest proxy. As another example, the data points can be randomly assigned to the proxies.
- the computing system can access the training dataset to obtain an anchor data point.
- the anchor data point can be randomly selected from the training dataset or according to an ordering or ranking.
- the computing system can input the anchor data point into a machine-learned distance model.
- the machine-learned distance model can be a deep neural network.
- the computing system can receive a first embedding provided for the anchor data point by the machine-learned distance model.
- the embedding can be within a machine-learned embedding dimensional space.
- the embedding can be provided at a final layer of the machine-learned distance model or at a close to final but not final layer of the machine-learned distance model.
- the computing system can evaluate a loss function that compares the first embedding to a positive proxy and/or one or more negative proxies.
- a loss function that compares the first embedding to a positive proxy and/or one or more negative proxies.
- One or more of the positive proxy and the one or more negative proxies can serve as a proxy for two or more data points included in the training dataset.
- the loss function can be a triplet-based loss function (e.g., triplet hinge function loss, NCA, etc.).
- the loss function can compare a first distance between the first embedding and the positive proxy to one or more second distances between the first embedding and the one or more negative proxies.
- the loss function can compare the first distance to a plurality of second distances respectively between the first embedding and a plurality of different negative proxies (e.g., all negative proxies).
- the loss function can include a constraint that the first distance is less than each of the one or more second distances (e.g., all of the second distances).
- the computing system can adjust one or more parameters of the machine-learned model based at least in part on the loss function. For example, one or more parameters of the machine-learned model can be adjusted to reduce the loss function (e.g., in an attempt to optimize the loss function).
- the loss function can be backpropagated through the distance model. In some implementations, the loss function can also be backpropagated through a proxy matrix that holds the values of the proxies (e.g., as proxy embedding vectors).
- the computing system re-assigns each data point in the training dataset to a respective one of the number of proxies. For example, at 1016 , the computing system can re-assigned each data point in the training dataset to a respective nearest proxy of the number of proxies.
- method 1000 returns to 1006 to obtain an additional anchor data point.
- the machine-learned distance model can be iteratively trained using a number (e.g., thousands) of anchor data points. Since proxies are used, the number of training iterations required to converge over the training dataset is significantly reduced. After training is complete, the machine-learned distance model can be employed to perform a number of different tasks, such as, for example, assisting in performance of a similarity search (e.g., an image similarity search).
- a similarity search e.g., an image similarity search
- the technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems.
- the inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components.
- processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
- Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
- FIGS. 9 and 10 respectively depict steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement.
- the various steps of the methods 900 and 1000 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
Abstract
Description
d(x,y;θ)≤d(x,z;θ)f or all(x,y,z)∈D (1)
L Ranking(x,y,z)=H(d(x,y)−d(x,z)) (2)
where H is the Heaviside step function. Unfortunately, this loss is not amenable directly to optimization using stochastic gradient descent as its gradient is zero everywhere. As a result, one might resort to surrogate losses such as Neighborhood Component Analysis (NCA) (See, S. Roweis et al. Neighbourhood component analysis. Adv. Neural Inf. Process. Syst. (NIPS), 2004) or margin-based triplet loss (See, K. Q. Weinberger et al. Distance metric learning for large margin nearest neighbor classification. Advances in neural information processing systems, 18:1473, 2006; and F. Schroff et al. Facenet: A unified embedding for face recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015). For example, triplet loss can use a hinge function to create a fixed margin between the anchor-positive difference, and the anchor-negative difference:
L triplet(x,y,z)=[d(x,y)+M−d(x,z)]+ (3)
where M is the margin, and [+] is the hinge function.
The proxy approximation error is denoted by the worst approximation among all data points
|{d(x,y)−d(x,z)}−{d(x,p(y))−d(x,p(z))}|≤2ε
E[L Ranking(x;y,z)]≤E[L Ranking(x;p(y),p(z))]+Pr[|d(x,p(y)−d(x,p(z)|≤2ε]
|
1: | Randomly initialize all values in θ including proxy vectors. |
2: | for i = 1 . . . T do |
3: | Sample triplet (x, y, Z) from D |
4: | Formulate proxy triplet (x, p(y), p(Z)) |
5: |
|
6: | θ ← θ − λ∂θl |
7: | end for |
L(x,y,z)≤αL(x,p(y),p(z))+δ
for constant α and δ, then the following bound holds for the total loss:
where nx,p
Then the following bounds of the original losses by their proxy versions are:
{circumflex over (L)} NCA(x,y,Z)≤α{circumflex over (L)} NCA(x,p y ,p z)+(1−α)log(|Z|)+2√{square root over (2ε)}
where {circumflex over (L)}NCA is defined as LNCA with normalized data points and |Z| is the number of negative points used in the triplet.
{circumflex over (L)} triplet(x,y,z)≤αL triplet(x,p y ,p z)+(1−α)M+2√{square root over (ε)}
where {circumflex over (L)}triplet is defined as Ltriplet with normalized data points.
TABLE 1 |
Retrieval and Clustering Performance on the Cars196 dataset. Bold |
indicates best results. |
R@1 | R@2 | R@4 | R@8 | NMI | ||
Triplet Semihard | 51.54 | 63.78 | 73.52 | 81.41 | 53.35 | ||
Lifted Struct | 52.98 | 66.70 | 76.01 | 84.27 | 56.88 | ||
Npairs | 53.90 | 66.76 | 77.75 | 86.35 | 57.79 | ||
Proxy-Triplet | 55.90 | 67.99 | 74.04 | 77.95 | 54.44 | ||
Struct Clust | 58.11 | 70.64 | 80.27 | 87.81 | 59.04 | ||
Proxy-NCA | 73.22 | 82.42 | 86.36 | 88.68 | 64.90 | ||
TABLE 2 |
Retrieval and Clustering Performance on the CUB200 dataset. |
R@1 | R@2 | R@4 | R@8 | NMI | ||
Triplet Semihard | 42.59 | 55.03 | 66.44 | 77.23 | 55.38 | ||
Lifted Struct | 43.57 | 56.55 | 68.59 | 79.63 | 56.50 | ||
Npairs | 45.37 | 58.41 | 69.51 | 79.49 | 57.24 | ||
Struct Clust | 48.18 | 61.44 | 71.83 | 81.92 | 59.23 | ||
Proxy NCA | 49.21 | 61.90 | 67.90 | 72.40 | 59.53 | ||
{circumflex over (x)} T({circumflex over (z)}−{circumflex over (p)} z)≤|{circumflex over (x)}∥{circumflex over (x)}|{circumflex over (z)}−{circumflex over (p)} z|≤√{square root over (ε)} (9)
Hence:
{circumflex over (x)} T {circumflex over (z)}≤{circumflex over (x)} T {circumflex over (p)} z +√{square root over (ε)} (10)
−{circumflex over (x)} T ŷ≤−{circumflex over (x)} T {circumflex over (p)} y +√{square root over (ε)} (11)
and under the assumption that β<1, the following version of the Hoelder inequality defined for positive real numbers αi can be applied:
to upper bound the sum of exponential terms:
for
The propositions follows from Eq. (13) and Eq. (14).
|{circumflex over (x)}−ŷ| 2 −|{circumflex over (x)}−{circumflex over (z)}| 2 +M=−2{circumflex over (x)} T ŷ+2{circumflex over (x)} T {circumflex over (z)}+M≤−2{circumflex over (x)} T {circumflex over (p)} y+2{circumflex over (x)} T {circumflex over (p)} z+2√{square root over (ε)}+M
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/710,377 US10387749B2 (en) | 2017-08-30 | 2017-09-20 | Distance metric learning using proxies |
PCT/US2018/032538 WO2019045802A1 (en) | 2017-08-30 | 2018-05-14 | Distance metric learning using proxies |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/690,426 US20190065957A1 (en) | 2017-08-30 | 2017-08-30 | Distance Metric Learning Using Proxies |
US15/710,377 US10387749B2 (en) | 2017-08-30 | 2017-09-20 | Distance metric learning using proxies |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/690,426 Continuation-In-Part US20190065957A1 (en) | 2017-08-30 | 2017-08-30 | Distance Metric Learning Using Proxies |
Publications (2)
Publication Number | Publication Date |
---|---|
US20190065899A1 US20190065899A1 (en) | 2019-02-28 |
US10387749B2 true US10387749B2 (en) | 2019-08-20 |
Family
ID=62599691
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/710,377 Active US10387749B2 (en) | 2017-08-30 | 2017-09-20 | Distance metric learning using proxies |
Country Status (2)
Country | Link |
---|---|
US (1) | US10387749B2 (en) |
WO (1) | WO2019045802A1 (en) |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB201809604D0 (en) * | 2018-06-12 | 2018-07-25 | Tom Tom Global Content B V | Generative adversarial networks for image segmentation |
US10878297B2 (en) * | 2018-08-29 | 2020-12-29 | International Business Machines Corporation | System and method for a visual recognition and/or detection of a potentially unbounded set of categories with limited examples per category and restricted query scope |
US20200311554A1 (en) * | 2019-03-27 | 2020-10-01 | International Business Machines Corporation | Permutation-invariant optimization metrics for neural networks |
CN110321451B (en) * | 2019-04-25 | 2022-08-05 | 吉林大学 | Image retrieval algorithm based on distribution entropy gain loss function |
CN111797893B (en) * | 2020-05-26 | 2021-09-14 | 华为技术有限公司 | Neural network training method, image classification system and related equipment |
CN112068866B (en) * | 2020-09-29 | 2022-07-19 | 支付宝(杭州)信息技术有限公司 | Method and device for updating business model |
CN113191503B (en) * | 2021-05-20 | 2023-06-09 | 清华大学深圳国际研究生院 | Decentralized distributed learning method and system for non-shared data |
CN115795355B (en) * | 2023-02-10 | 2023-09-12 | 中国科学院自动化研究所 | Classification model training method, device and equipment |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110243381A1 (en) * | 2010-02-05 | 2011-10-06 | Rochester Institute Of Technology | Methods for tracking objects using random projections, distance learning and a hybrid template library and apparatuses thereof |
US20120089551A1 (en) * | 2010-10-08 | 2012-04-12 | International Business Machines Corporation | System and method for composite distance metric leveraging multiple expert judgments |
US20130144818A1 (en) * | 2011-12-06 | 2013-06-06 | The Trustees Of Columbia University In The City Of New York | Network information methods devices and systems |
US20130250181A1 (en) * | 2010-12-29 | 2013-09-26 | Thomson Licensing | Method for face registration |
US20140279755A1 (en) * | 2013-03-15 | 2014-09-18 | Sony Corporation | Manifold-aware ranking kernel for information retrieval |
US20150186793A1 (en) * | 2013-12-27 | 2015-07-02 | Google Inc. | System and method for distance learning with efficient retrieval |
US20170228641A1 (en) * | 2016-02-04 | 2017-08-10 | Nec Laboratories America, Inc. | Distance metric learning with n-pair loss |
-
2017
- 2017-09-20 US US15/710,377 patent/US10387749B2/en active Active
-
2018
- 2018-05-14 WO PCT/US2018/032538 patent/WO2019045802A1/en active Application Filing
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110243381A1 (en) * | 2010-02-05 | 2011-10-06 | Rochester Institute Of Technology | Methods for tracking objects using random projections, distance learning and a hybrid template library and apparatuses thereof |
US20120089551A1 (en) * | 2010-10-08 | 2012-04-12 | International Business Machines Corporation | System and method for composite distance metric leveraging multiple expert judgments |
US20130250181A1 (en) * | 2010-12-29 | 2013-09-26 | Thomson Licensing | Method for face registration |
US20130144818A1 (en) * | 2011-12-06 | 2013-06-06 | The Trustees Of Columbia University In The City Of New York | Network information methods devices and systems |
US20140279755A1 (en) * | 2013-03-15 | 2014-09-18 | Sony Corporation | Manifold-aware ranking kernel for information retrieval |
US20150186793A1 (en) * | 2013-12-27 | 2015-07-02 | Google Inc. | System and method for distance learning with efficient retrieval |
US20170228641A1 (en) * | 2016-02-04 | 2017-08-10 | Nec Laboratories America, Inc. | Distance metric learning with n-pair loss |
Non-Patent Citations (21)
Title |
---|
Abadi et al., Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems, Mar. 14, 2016, 19 pages. |
Chopra et al., "Learning a Similarity Metric Discrimininatively, with Application to Face Verification", In Computer Vision and Pattern Recognition, Institute of Electrical and Electronics Engineers Conference, vol. 1, Jun. 20-25, 2005, San Diego, California, 8 pages. |
Hadsell et al., "Dimensionality Reduction by Learning an Invariant Mapping", Computer Vision and Pattern Recognition, Institute of Electrical and Electronics Engineers Conference, vol. 2, Jun. 17-23, 2006, New York, New York, 8 pages. |
Hershey et al., "Deep Clustering: Discriminative Embeddings for Segmentation and Seperation", In Acoustics, Speech, and Signal Processing, Institute of Electrical and Electronics Engineers Conference, Mar. 20-25, 2016, Shanghai, China, 10 pages. |
International Search Report with Written Opinion for PCT/US2018/032538 dated Jul. 26, 2018, 9 pages. |
Ioffe et al., "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", Proceedings of the 32nd International Conference on Machine Learning, Jul. 6-11, 2015, Lille, France, 9 pages. |
Krause et al., "3d Object Representations for Fine-Grained Categorization", In Proceedings of the Institute of Electrical and Electronics Engineers Conference on Computer Vision Workshops, Dec. 1-8, 2013, Sydney, Australia, pp. 554-561. |
Manning et al., "Introduction to Information Retrieval", Cambridge University Press, New York, New York, May 27, 2008, 504 pages. |
Movshovitz-Attias et al., "No Fuss Distance Metric Learning using Proxies", arXiv:1703.07464v3, Aug. 1, 2017, 9 pages. |
Oh Song et al., "Deep Metric Learning via Lifted Structured Feature Embedding", In Proceedings of the Institute of Electrical and Electronics Engineers Conference on Computer Vision and Pattern Recognition, Jun. 27-30, 2016, Seattle, Washington, pp. 4,004-4,012. |
Rippel et al., "Metric Learning with Adaptive Density Discrimination", arXiv preprint arXiv:1511.05939, 2015, 15 pages. |
Roweis et al., "Neighbourhood Components Analysis", In Advances in Neural Information Processing Systems, Dec. 2004, Vancouver, B.C., Canada, 8 pages. |
Russakovsky et al., "Imagenet Large Scale Visual Recognition Challenge", International Journal of Computer Vision, vol. 115, No. 3, Dec. 1, 2015, 43 pages. |
Schroff et al., "Facenet: A Unified Embedding for Face Recognition and Clustering", In Proceedings of the Institute of Electrical and Electronics Engineers Conference on Computer Vision and Pattern Recognition, Jun. 7-12, 2015, Boston, Massachusetts, pp. 815-823. |
Schultz et al., "Learning a Distance Metric From Relative Comparisons", In Advances in Neural Information and Processing Systems, vol. 1, Dec. 5-10, 2013, Lake Tahoe, Nevada, 8 pages. |
Sohn, "Improved Deep Metric Learning with Multi-Class N-Pair Loss Objective", In Advances in Neural Information and Processing Systems, Dec. 5-10, 2016, Barcelona, Spain, 9 pages. |
Song et al., "Learnable Structured Clustering Framework for Deep Metric Learning", Institute of Electrical and Electronics Engineers Conference on Computer Vision and Pattern Recognition, Jul. 21-26, 2017, Hawaii, 9 pages. |
Szegedy et al., "Going Deeper with Convolutions", In Proceedings of the Institute of Electrical and Electronics Engineers Conference on Computer Vision and Pattern Recognition, Jun. 7-12, 2015, Boston, Massachusetts, 9 pages. |
Wah et al., "The Caltech-UCSD Birds-200-2011 Dataset", Computation & Neural Systems Technical Report, California Institute of Technology, 2011, Pasadena, California, 8 pages. |
Weinberger et al., "Distance Metric Learning for Large Margin Nearest Neighbor Classification", In Advances in Neural Information Processing Systems, Dec. 4, 2006, Vancouver, B.C., Canada, 8 pages. |
Zheng et al., "Improving the Robustness of Deep Neural Networks via Stability Training", In Proceedings of the Institute of Electrical and Electronics Engineers Conference on Computer Vision and Pattern Recognition, Jun. 27-30, 2016, Seattle, Washington, pp. 4,480-4,488. |
Also Published As
Publication number | Publication date |
---|---|
WO2019045802A1 (en) | 2019-03-07 |
US20190065899A1 (en) | 2019-02-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20190065957A1 (en) | Distance Metric Learning Using Proxies | |
US10387749B2 (en) | Distance metric learning using proxies | |
JP7470476B2 (en) | Integration of models with different target classes using distillation | |
He et al. | Neural factorization machines for sparse predictive analytics | |
US20190258925A1 (en) | Performing attribute-aware based tasks via an attention-controlled neural network | |
US10510021B1 (en) | Systems and methods for evaluating a loss function or a gradient of a loss function via dual decomposition | |
US20200334520A1 (en) | Multi-task machine learning architectures and training procedures | |
Titsias et al. | Spike and slab variational inference for multi-task and multiple kernel learning | |
US20190303535A1 (en) | Interpretable bio-medical link prediction using deep neural representation | |
US10824674B2 (en) | Label propagation in graphs | |
CN116261731A (en) | Relation learning method and system based on multi-hop attention-seeking neural network | |
De Mathelin et al. | Adversarial weighting for domain adaptation in regression | |
Fathony et al. | Adversarial surrogate losses for ordinal regression | |
WO2021238279A1 (en) | Data classification method, and classifier training method and system | |
US20220366260A1 (en) | Kernelized Classifiers in Neural Networks | |
Zhang et al. | Learning from few samples with memory network | |
Kumagai et al. | Combinatorial clustering based on an externally-defined one-hot constraint | |
US20220147758A1 (en) | Computer-readable recording medium storing inference program and method of inferring | |
Shen et al. | StructBoost: Boosting methods for predicting structured output variables | |
EP3166022A1 (en) | Method and apparatus for image search using sparsifying analysis operators | |
Burkhart et al. | Deep low-density separation for semi-supervised classification | |
US7933449B2 (en) | Pattern recognition method | |
Liu et al. | Evolutionary voting-based extreme learning machines | |
Büyüktaş et al. | More learning with less labeling for face recognition | |
Horn et al. | Predicting pairwise relations with neural similarity encoders |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MOYSHOVITZ-ATTIAS, YAIR;LEUNG, KING HONG;SINGH, SAURABH;AND OTHERS;SIGNING DATES FROM 20180103 TO 20180105;REEL/FRAME:044564/0207 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |