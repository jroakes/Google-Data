EP2810018A1 - Apparatus and method for acquiring underwater images - Google Patents
Apparatus and method for acquiring underwater imagesInfo
- Publication number
- EP2810018A1 EP2810018A1 EP13743981.6A EP13743981A EP2810018A1 EP 2810018 A1 EP2810018 A1 EP 2810018A1 EP 13743981 A EP13743981 A EP 13743981A EP 2810018 A1 EP2810018 A1 EP 2810018A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- image
- water
- watercraft
- images
- capture
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Withdrawn
Links
Classifications
-
- G—PHYSICS
- G03—PHOTOGRAPHY; CINEMATOGRAPHY; ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ELECTROGRAPHY; HOLOGRAPHY
- G03B—APPARATUS OR ARRANGEMENTS FOR TAKING PHOTOGRAPHS OR FOR PROJECTING OR VIEWING THEM; APPARATUS OR ARRANGEMENTS EMPLOYING ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ACCESSORIES THEREFOR
- G03B17/00—Details of cameras or camera bodies; Accessories therefor
- G03B17/02—Bodies
- G03B17/08—Waterproof bodies or housings
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/698—Control of cameras or camera modules for achieving an enlarged field of view, e.g. panoramic image capture
-
- G—PHYSICS
- G03—PHOTOGRAPHY; CINEMATOGRAPHY; ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ELECTROGRAPHY; HOLOGRAPHY
- G03B—APPARATUS OR ARRANGEMENTS FOR TAKING PHOTOGRAPHS OR FOR PROJECTING OR VIEWING THEM; APPARATUS OR ARRANGEMENTS EMPLOYING ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ACCESSORIES THEREFOR
- G03B37/00—Panoramic or wide-screen photography; Photographing extended surfaces, e.g. for surveying; Photographing internal surfaces, e.g. of pipe
- G03B37/04—Panoramic or wide-screen photography; Photographing extended surfaces, e.g. for surveying; Photographing internal surfaces, e.g. of pipe with cameras or projectors providing touching or overlapping fields of view
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/50—Constructional details
- H04N23/51—Housings
-
- G—PHYSICS
- G03—PHOTOGRAPHY; CINEMATOGRAPHY; ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ELECTROGRAPHY; HOLOGRAPHY
- G03B—APPARATUS OR ARRANGEMENTS FOR TAKING PHOTOGRAPHS OR FOR PROJECTING OR VIEWING THEM; APPARATUS OR ARRANGEMENTS EMPLOYING ANALOGOUS TECHNIQUES USING WAVES OTHER THAN OPTICAL WAVES; ACCESSORIES THEREFOR
- G03B15/00—Special procedures for taking photographs; Apparatus therefor
- G03B15/006—Apparatus mounted on flying objects
Definitions
- the present invention relates generally to image-acquisition and, more specifically, to techniques for acquiring underwater images.
- mapping systems often include images of geographic areas offering different views. For example, some mapping systems provide to users images from a satellite view, images from an airplane view, and images from a street-level view or eye-level view. The images from these different views are often associated with data indicating the direction, or orientation, of the view and data indicating the geographic location depicted in the view or from which the view was captured. Based on this location and orientation data, mapping systems often stitch the images together to form larger views than depicted in individual images. The mapping systems also often present the images in a sequence or other arrangement based on the orientation and location data in response to user requests to view a different orientation or location from one currently being depicted by the mapping system. By entering a series of such requests, users may navigate along a path through a geographic area by viewing a sequence of images along that path and changing which orientation is depicted, thereby simulating the experience of a person traveling that path and looking around the geographic area.
- mapping systems do not include an underwater view in which images of underwater features are depicted with adequate quality. Capturing such images is often difficult because bubbles in the water interfere with image capture, underwater lighting is often insufficient, and labor costs associated with imaging a given geographic area are often relatively high because limited viewing distances underwater often require a camera person to raster a camera over an imaged geographic area a relatively high number of times in order to capture a comprehensive set of images of the area. Further, it is often difficult to obtain orientation and geographic location data indicating the orientation and location from which images are captured while capturing underwater images because of movement induced by the water, e.g., by waves, and attenuation of signals for determining location underwater.
- the present invention includes a camera for capturing underwater panoramic views from a plurality of image locations along a path and data indicative of orientations and geographic locations of the camera at the image locations.
- the camera of this aspect includes a water-proof housing and an image-capture module disposed at least partially in the water-proof housing and configured to capture a plurality of underwater images of visible light from a plurality of image locations along a path.
- the camera of this aspect also includes a location sensor configured to obtain geographic locations of the plurality of image locations along the path, an orientation sensor configured to sense orientations of the image-capture module at the plurality of image locations along the path, and memory communicatively coupled to the image-capture module, and the location sensor, and the orientation sensor.
- Some aspects include an image-acquisition apparatus that includes an array of two or more water-craft each operable to transport a human operator, each water-craft mechanically coupled to another water-craft in the array of water-craft, and each water-craft having an image- capture module, the image capture module being operable to capture underwater images based on visible light and being operable to capture orientation data indicative of a spatial orientation of at least part of the image-capture module during image capture.
- Each image-capture module of this aspect includes one or more windows, at least part of which is disposed at a depth under a water-line of the corresponding water-craft.
- each of the present image-capture modules includes two or more cameras operable to sense visible light, each camera having an optical axis that intersects at least one of the one or more windows, each optical axis being at an oblique angle to a plane defined by the water-line of the water-craft, and the two or more cameras being approximately radially symmetrically disposed about a normal to the horizontal plane, and an orientation sensor having zero degrees of freedom relative to the two or more cameras.
- At least one of the image-capture modules of the present aspect includes a geographic-position sensor operable to capture location data indicative of a geographic location of the array of water craft, the geographic-position sensor comprising an antenna disposed at least partially above the water- line of the water-craft; a power-supply operable to provide electrical power to at least one of the image-capture modules; and a one or more processors coupled to a tangible-machine readable memory storing instructions that when executed by one or more of the one or more processors cause each of a plurality of images captured by each image-capture module to be correlated to data from the orientation sensor indicative of orientations of the cameras at the time the image was captured and data from the geographic-position sensor indicative of the geographic location of the array of water-craft at the time the image was captured.
- Some aspects include a method of imaging an underwater geographic area.
- the method of this aspect includes propelling a watercraft through water, capturing, with one or more cameras coupled to the watercraft, images of underwater features in field of view spanning an approximately 360-degree azimuth; sensing a geographic location of the watercraft; sensing an orientation of one or more of the one or more cameras; and storing the captured images, the sensed geographic location, and the sensed orientation in memory.
- Figure 1 illustrates an example of a image-capture module attached to an example of a watercraft
- Figure 2 is a partially-exploded view of the image-capture module of Figure 1;
- Figure 3 illustrates an example of an array of image-capture modules and watercraft;
- Figures 4A and 4B illustrate an examples of a bubble shield coupled to a watercraft;
- Figure 5 illustrates another example of an array of watercraft;
- Figure 6 illustrates an example of a watercraft having a light diffuser;
- Figure 7 is a block diagram illustrating features of an image-capture module
- Figure 8 is an example of a method of capturing images within an image-capture module
- Figure 9 is an example of a mapping system
- Figure 10 is an example of a computer system.
- Figure 1 illustrates an embodiment of a water-borne imaging system 9 having a watercraft 10 coupled to an image-capture module 12.
- the image-capture module 12 may include a plurality of cameras, an orientation sensor, and a location sensor that cooperate to capture a plurality of images in different orientations at different locations along a path and associate with those images data indicative of the orientation and location at which the images were captured.
- the images may be presented within a mapping system as an underwater view, and a user interface of the mapping system may receive user commands to navigate along an underwater path, which may cause the mapping system to present the underwater images to the user in a sequence or other arrangement based on the orientation and location data.
- the underwater images may be stitched to one another based on the orientation data and the location data such that a user may view a panorama from a given position or a view extending between images captured at different positions.
- some embodiments described below may include bubble shields that are expected to reduce the number of bubbles appearing in images and light diffusers expected to improve lighting conditions underwater.
- some embodiments may include an array of watercraft operable to image an underwater area from different positions along a plurality of different paths that are traversed generally concurrently, thereby potentially reducing labor costs.
- the watercraft 10 is a canoe.
- the illustrated watercraft 10 is operable to carry one, two, or more (or fewer) human beings, and is configured to be propelled by a human being by paddling.
- Other embodiments may include larger or smaller watercraft, and in some embodiments, the watercraft may be powered, for example with an outboard or an onboard electric or combustion engine or a sail.
- Some embodiments may include a separate watercraft, e.g., a separate powered watercraft, configured to tow or push the other watercraft described herein.
- some embodiments may include a watercraft configured to operate below the surface of the water rather than both above and below (spanning) the surface of the water.
- a human-powered watercraft such as a canoe, may facilitate efforts to image environmentally or historically sensitive underwater areas in which other propulsion systems are prohibited or are disfavored due to noise or other reasons. Further, a relatively small watercraft, such as a canoe, is expected to disturb the water less, cast a smaller shadow, and be operable to image shallower regions, each of which is expected to improve the number and quality of images captured with the image-capture module 12. Other embodiments, however, may not offer some or all of these benefits, and some embodiments may offer other benefits, as described in greater detail below.
- the image-capture module 12 may include a plurality of cameras oriented in a plurality of different directions and oriented at an oblique angle relative to each of (or at least one of) the horizon, a waterline of the watercraft 10, or a vertical axis 14 that is perpendicular to the waterline of the watercraft 12 and the horizon. Consequently, in some embodiments, the image-capture module may be operable to capture images in a variety of different directions, for example by capturing images in predetermined directions relative to the watercraft 10 simultaneously while the watercraft 10 is at a given orientation and location.
- the image-capture module 12 may include orientation and geographic location sensors operable to determine (or otherwise obtain data indicative of) the location and orientation of the image-capture module 12, for example as the watercraft 10 moves along a path through the water and as the image-capture module 12 changes orientation due to the watercraft 10 rocking back and forth, for instance when the watercraft 10 encounters disturbances on the water, such as waves.
- the image-capture module 12 may be integrally formed with the watercraft 10 or may be formed as a separate component mounted to the watercraft 10.
- the watercraft 10 may be formed from fiberglass, aluminum, or other materials, and a housing of the image-capture module 12, such as the housing described below with reference to Figure 2, may be formed as part of a hull 16 (or other structure) of the watercraft 10.
- the image-capture module 12 may be a removable component, which may be removably mounted to the watercraft 10, for example mounted straddling a keel 18 of the hull 16.
- the image-capture module 12 may be centered over the keel 18, or in other embodiments, the image-capture module 12 may be cantilevered over a side 20 of the watercraft 10, for example secured to an outrigger.
- a watercraft may be modified by cutting a hole through the hull 16, and portions or all of the illustrated image-capture module 12 may be placed through the hole to position them below the surface of the water.
- the image-capture module 12 may be mounted to the watercraft 10 with a variety of techniques.
- the image-capture module 12 may include flanges that are bolted to the watercraft 10 through holes through (or recesses in) the hull 16.
- the image-capture module 12 may be secured to the watercraft 10 without disturbing the structure of the watercraft 10, for example by straps 22 extending around the sides 20.
- the straps 22 may be rigid or flexible material and may envelope the watercraft 10 or may attach to hooks that clasp the sides 20.
- a top surface of the image-capture module 12 may include a resilient material, such as an elastomeric (e.g., rubber) resilient flange, which is expected to prevent the image-capture module 12 from damaging the hull 16 of the watercraft 10 and vice versa.
- a removable image-capture module 12 is expected to reduce shipping costs for imaging campaigns in which underwater areas are imaged in geographically distributed locations, as different watercraft 10 may be purchased or rented in the different locations, and the image-capture module 12 may be shipped from location to location without incurring the cost of transferring the watercraft 10.
- the watercraft 10 may be permanently secured to the image-capture module 12.
- Figure 2 illustrates additional details of the image-capture module 12.
- the image-capture module 12 includes a housing 24 and a plurality of cameras 26.
- the image-capture module 12 may also include other features described below with reference to Figure 7, such as location sensors and orientation sensors.
- the illustrated housing 24 includes a plurality of windows 28 and lights 30.
- the housing 24 may be generally waterproof or water resistant in some embodiments and may be made of a variety of materials, including composite materials, such as fiberglass, carbon fiber, Kevlar; or metal, such as aluminum or stainless steel; or may be made of plastic, for example vacuum formed or rotocast polypropylene or polyethylene.
- the illustrated housing 24 does not include a top, but in other embodiments, the housing 24 may include a top, which may seal the interior of the housing 24.
- the housing 24 may be made from a generallyu opaque material and may be generally dark inside, e.g., the joints may be sealed and interior lights omitted or suppressed, such that veiling glare on the windows 28 is mitigated.
- a top edge 32 extending around a top perimeter of the housing 24 may include a resilient material, such as a gasket, for example a rubber gasket, which may establish contact with the hull 16 of the watercraft 10.
- the housing 24 includes four, generally radially symmetric faces each having a window 28 and a pair of lights 30.
- Other embodiments may include other numbers of faces, curved non-planar surfaces, or other shaped housings. Further, other embodiments may include more or fewer of lights 30 or windows 28.
- Some embodiments may include a single window in the field of view of each of the cameras 26, such as a flat window that is generally parallel to a waterline of the watercraft 10.
- substantially all of the housing 24 may be made of a generally transparent material, such as acrylic glass or glass. Further, some embodiments may not include lights 30, which is not to suggest that any other feature described herein may not also be omitted in some embodiments.
- the windows 28 may be a generally transparent material, such as glass or acrylic glass and may generally align with a field of view of one of the respective cameras 26, for example a field of view of one of the cameras 26 may lie within (e.g., entirely within) the corresponding window 28.
- the windows 28 include an anti-reflective coating on a surface of the windows that mitigates veiling glare in the field of view of the cameras 26.
- the illustrated cameras 26 are single lens reflex (SLR) cameras.
- the cameras 26 may, in other embodiments, be other types of still cameras, such as point-and-shoot cameras, web cameras, or other devices having an image sensor.
- the cameras 26 may include or may be video cameras.
- the illustrated cameras 26 are operative to sense light in the visible portion of the electromagnetic spectrum, though in some embodiments, the cameras 26 may additionally or alternatively be operable to sense light in other portions of the electromagnetic spectrum, for example in the infrared portion of the electromagnetic spectrum.
- Each of the cameras 26 may include an image sensor, such as a charge coupled device (CCD) or a complementary metal oxide semiconductor (CMOS) image sensor.
- the image sensor may be coupled to a processor and memory within each of the cameras 26, and in some embodiments, the cameras 26 may be coupled to external memory and an external processor, as described in greater detail below with reference to Figure 7.
- the illustrated cameras 26 are generally rotationally symmetrically disposed about the vertical axis 14, and this embodiment, there are four cameras 26, each oriented at an azimuth (e.g., angle in a horizontal plane) that is approximately 90-degrees different from an azimuth each of the adjacent cameras 26.
- the illustrated cameras 26 each define an optical axis 34 that is at a negative oblique altitude, or angle with respect to one or more of the following: the horizon, the water line, or a plane normal to the vertical axis 14.
- the altitude of the optical axes 34 may be at approximately -45 -degrees relative to the one or more of the following: the water line, the horizon, or a plane normal to the vertical axis 14.
- the altitude may be at some other angle, for example an altitude less than -15-degrees and greater than -75-degrees, or less than -25-degrees and greater than -65- degrees, for instance.
- multiple cameras may be positioned at each azimuth, and each camera at each azimuth may be at a different altitude, for example one camera at a North- facing azimuth may be at an altitude of approximately -15-degrees, one camera may be at an altitude of approximately -45 -degrees, and another camera may be at an altitude of approximately -75 -degrees, and a set of three such cameras may be positioned that each of four different azimuth or at some other number of azimuths.
- some embodiments may include a nadir camera having an optical axis that is generally vertical (e.g., perpendicular to the horizon and the waterline of the watercraft 10) and a downward oriented field of view.
- the altitude of each of the cameras 26 may be adjusted based on a depth of features being imaged, for instance the altitude may be decreased (that is, the cameras may be angled further downward) when imaging a relatively deep underwater feature, and the altitude may be increased (that is, that cameras may be angled upward, with the optical axis 34 approaching closer to the horizon) when imaging relatively shallow underwater features.
- the image-capture module 12 may include a depth sensor, such as an ultrasound depth sensor, and a servo or other drive may adjust the altitude of the cameras 26 based on (for example exclusively based on or based on additional data) data indicative of the depth sensed by the depth sensor.
- Each of the cameras 26 may define a field of view, and the optical axes 34 may be generally centered in the field of view.
- the number of radially symmetrically disposed cameras may be selected based on the field of view.
- the number of radially symmetrically disposed cameras may be selected such that the fields of view of each of the cameras 26 overlap to facilitate stitching of images based on features appearing in overlapping portions of the images.
- Figure 3 illustrates an embodiment of an array of water-born image sensors 35 having a plurality of watercraft 36, 38, and 40 each having an image-capture module 12.
- the array 35 may be operable to image an underwater geographic area by capturing images along a plurality of different path traversed by the watercraft 36, 38, and 40 at generally the same time.
- the illustrated array 35 includes three watercraft 36, 38, and 40 disposed in spaced relation to one another at distance 42 apart from one another.
- the distance 42 between each of the watercraft 36, 38, 40 may be approximately equal and, in some embodiments, may be selected based on one or more of the following: a vertical field of view of cameras within the image-capture module 12, an altitude of an optical axis of the cameras, and a depth of features imaged by the image-capture module 12 below the surface of the water.
- the distance 42 may be selected such that the vertical field of view of the sideways facing cameras overlapped by some desired amount, for example between 5 and 15 percent of the images.
- Overlapping fields of view are expected to facilitate image-stitching between the overlapping images and determining the positions of the image-capture modules 12 relative to one another based on the overlapping images.
- the distances 42 may be adjustable, for example dynamically based on depth measurements (e.g., the distance 42 may be increased in response to an increase in depth and vice versa, for instance by a servo coupled to a rack and pinion and controlled based on a depth measurement), or the distances 42 may be generally fixed. Further, in some embodiments, the distances 42 may be different from one another.
- the illustrated embodiment includes three watercraft 36, 38, and 40 positioned generally parallel to one another, but other embodiments may include other arrangements.
- the watercraft 36, 38, and 40 may be arranged in an approximately V-shaped formation or in an approximately slanted formation with one of the watercraft 36, 38, and 40 leading the other watercraft.
- the shape of the array may be selected such that the wake from leading watercraft 36, 38, or 40 interacts with the other watercraft in a favorable manner, for example by diffusing light in an area imaged by one of the other watercraft.
- Other embodiments may include more or fewer watercraft, for example more than one watercraft, more than two watercraft, more than three watercraft, more than four watercraft, or more than five watercraft.
- each of the watercraft 36, 38, and 40 are generally similar or identical to one another, and each may be an example of the watercraft 10 described above with reference to Figure 1.
- the watercraft 36, 38, and 40 may be different from one another.
- one or more of the watercraft may be an outrigger, a raft, a flotation device, a human-bearing watercraft, a non-human-bearing watercraft, a submersible, or other cantilevered, floating, or fully immersed structure.
- the watercraft 36 and 40 may be omitted, which is not to suggest that any other feature described herein may not also be omitted, and the image-capture modules 12 on either side of the watercraft 38 may be supported by a cantilevered beam extending from either side of the watercraft 38.
- the image-capture modules 12 are each disposed in the approximately the same location and in approximately the same orientation on each of the watercraft 36, 38, and 40, but in other embodiments, the positions and orientations of the image-capture modules 12 may be varied, and some or all of the watercraft 36, 38, and 40 may include additional image- capture modules 12.
- the watercraft 36, 38, and 40 are held in spaced relation with beams 44.
- the illustrated beams 44 are generally rigid structures made from any of a variety of materials, such as wooden beams, aluminum beams, or plastic beams secured on each end to one of the watercraft 36, 38, and 40.
- the beams 44 in some embodiments, may be configured to telescope (e.g., by sliding constituent beams over one another and locking in one of a plurality of selectable positions) to accommodate different distances 42.
- the beams 44 may each be secured to a pair of the watercraft 36, 38, and 40 with a single degree of freedom relative to each watercraft in the pair, for example with a hinge on each end that permits the watercraft 36, 38, and 40 to translate vertically and rotate about a horizontal axis parallel to the beams 44 relative to one another, for example in response to wave propagating past each of the watercraft 36, 38, and 40 at different times.
- the illustrated embodiment includes cross-cables 41 extending generally diagonally between the ends of the beams 44, and in some embodiments, additional beams, e.g., crossmembers, may extend approximately diagonally between each of the watercraft 36, 38, and 40 to provide additional support to counteract shear stress arising from one watercraft 36, 38, or 40 moving faster than the others.
- the beams 44 may be substantially rigid or flexible beams (for example composite beings formed from material such as fiberglass or carbon fiber or plastic beams) coupled to the watercraft 36, 38, and 40 (e.g., removably coupled or permanently coupled) with zero of freedom at the end of the beams.
- the flex of the beams is expected to accommodate some movement of the watercraft 36, 38, and 40 relative to one another in response to a wave or differences in speed of movement.
- each of the watercraft 36, 38, and 40 may move along three separate paths over a geographic area that is underwater, and the movement may occur generally simultaneously along paths that are generally parallel to one another.
- the image-capture modules 12 may capture images from different positions along each of these paths, for example at a plurality of positions, in a plurality of different directions, along each of the plurality of paths.
- Capturing images along a plurality of paths that are traversed generally concurrently is expected to reduce labor costs and speed image acquisition relative to systems that traverse a single path, as a single operator may steer, and in some embodiments propel, all three of the watercraft 36, 38, 40, thereby potentially reducing the number of times a given operator traverses a geographic area while imaging the geographic area from multiple paths.
- Figure 4A illustrates an embodiment including the watercraft 10, the image-capture module 12, and a bubble shield 43.
- the bubble shield 43 may be coupled to the bottom of the watercraft 10, for example on an upstream side of the watercraft 10 relative to the image-capture module 12 and, as described in greater detail below, may tend to reduce the number of bubbles flowing in front of the cameras of the image-capture module 12, thereby potentially improving image quality.
- the bubble shield 43 of the present embodiment is a generally semi-circular in shape and incudes a lip 45 at a distal end of the bubble shield 43.
- the bubble shield 43 may be characterized as having a generally convex upstream surface that is generally reflectively symmetric about the keel 18 of the watercraft 10, and the bubble shield 43 may be generally of uniform thickness or may have some other profile.
- the lip 45 may extend generally perpendicular to the bubble shield 43 generally in an upstream direction, though other embodiments may include a lip 43 that is differently oriented.
- Figure 4B illustrates another example of a bubble shield 46.
- the bubble shield 46 includes two curved plates 48 that are generally reflectively symmetric about the keel 18 of the watercraft 10.
- a generally reflectively symmetric bubble shield 46 is expected to interfere less with the navigation of the watercraft 10 relative to non-symmetric bubble shields, but other embodiments may include non-symmetric bubble shields.
- the illustrated bubble shield 46 generally has a V-shape, but other embodiments may have other shapes, for example a generally convex or C-shape with an apex of the convex shape disposed upstream.
- the plates 48 may be generally straight, generally curved, or have some other shape.
- the illustrated plates 48 are generally curved about a vertical axis, extending generally perpendicular to the waterline of the watercraft 10, but in other embodiments the surface of the bubble shield 46 may be curved about other axes as well, for example about a generally horizontal axis.
- the illustrated bubble shield 36 is formed from plates 48 generally having a generally uniform thickness, but other embodiments may include a bubble shield 46 having another shape.
- the bubble shield may include a single generally horizontal window through which each of the cameras 26 shoot, with the lip 45 positioned to be generally co-planar with the window, and the plates 48 may define the housing in which the cameras 26 ( Figure 2) are disposed.
- the bubble shields 43 and 46 may be made from a variety of materials, for example sheet-metal, plastic, or composite materials, such as fiberglass or carbon fiber.
- the bubble shields 43 and 46 may be coupled (e.g., bolted, glued, welded, etc.) to the watercraft 10 or to the image-capture module 12, and in some embodiments, the bubble shields 43 and 46 may be integrally formed with the watercraft 10 or the image-capture module 12.
- the bubble shields 43 and 46 may be positioned such that the bubble shield 46 lies outside a field of view of each of the cameras of the image-capture module 12, in some embodiments.
- the illustrated bubble shields 43 and 46 lie entirely or substantially entirely upstream relative to the image-capture module 12, but in other embodiments, some or all of the bubble shields 43 and 46 may be parallel to or downstream of the image-capture module 12.
- the bubble shield 46 may have a generally teardrop-shape or oval-shape that envelops the image-capture module 12.
- the bottom edge of the teardrop-shaped shield may be approximately co-planar, e.g., generally flat, and a lip may extend radially outward from the bottom edge.
- the bubble shields 43 and 46 may be made from a generally translucent or transparent material, and the material may have a refractive index similar to that of water, thereby potentially rendering portions of the bubble shields 43 and 46 within the field of view of the image-capture module 12 difficult to see within images captured by the image- capture module 12.
- the bubble shields 43 and 46 may be made from glass or acrylic glass.
- FIG. 4A and 4B depict single bubble shields 43 and 46 on a single watercraft 10 having a single image-capture module 12, but other embodiments may include additional bubble shields, additional watercraft 10, or additional image-capture modules 12.
- the illustrated bubble shields 43 and 46 may be coupled to the other embodiments of watercraft described herein.
- the bubble shields 43 and 46 are expected to divert bubbles away from the field of view of the image-capture module 12. Bubbles in the water are expected to reduce the visibility of underwater features when the bubbles pass within the field of view of the image- capture module 12 (e.g., the aggregate fields of view of the cameras of the image-capture module 12). Diverting the bubbles, or some of the bubbles, is expected to mitigate this effect and improve the visibility of underwater features imaged by the image-capture module 12 relative to systems without a bubble shield.
- the bubble shield 46 like many of the other features described herein, may be omitted in some embodiments.
- FIG. 5 illustrates another embodiment of an imaging system 50 including a plurality of watercraft 52 and 53 each having an image-capture module 12, a rudder 54, and a connecting cable 56.
- the illustrated watercraft 52 and 53 may be similar or identical to the other watercraft described herein.
- the rudders 54 may tend to push the watercraft 52 and 53 apart while the cable 56 may tend to hold the watercraft 52 and 53 together, placing the cable 56 in tension, thereby holding the watercraft 52 and 54 generally in fixed relation to one another.
- the illustrated rudders 54 may also be bubble shields, such as the bubble shield 46 described above with reference to Figure 4.
- the rudders 54 may be constructed from the materials described above with reference to the bubble shield 46 and may be secured (e.g., coupled to the watercraft 52 and 53) in the manner described above with reference to the bubble shield 46.
- the rudders 54 may not be bubble shields or may be included along with a separate or integrated bubble shield.
- the illustrated rudders 54 are generally sloped or curved plates that tend to push the watercraft 52 and 53 away from one another as the watercraft 52 and 53 move along parallel paths through the water.
- the rudders 54 may have another shape, for example a shape without a uniform thickness, a shape that is not curved, or a shape that is not generally continuous, for example an array of smaller plates or baffles.
- the illustrated embodiment includes one rudder 54 for each watercraft 52 and 53, but other embodiments may include additional rudders.
- some embodiments may include one rudder disposed upstream of the image-capture module 12 and one rudder disposed generally downstream of the image-capture module 12 on each of the watercraft 52 and 53.
- the illustrated cables 56 may be coupled, for example bolted or tied, to each of the watercraft 52 and 53 on facing sides of the watercraft 52 and 53 on each end of the cables 56.
- the cables 56 may be made from a variety of materials (e.g., materials that support loads in tension but not in compression), for instance steel cabling, rope, chains, or other materials.
- the illustrated embodiment includes two cables 56, but other embodiments may include additional cables or additional rigid structures extending between the watercraft 52 and 53.
- the watercraft 52 and 53 may move along approximately parallel paths at the same time or approximately the same time while the image-capture modules 12 capture a plurality of images from different positions along each of those paths in the manner described above.
- water flow over the rudders 54 may tend to create a generally horizontal force pushing the watercraft 52 away from the watercraft 53 and pushing the watercraft 53 away from the watercraft 52.
- the cable 56 may be pulled taut as the watercraft 52 and 53 move away from one another, and the cables 56 may be held in tension by the force from the rudders 54 and may prevent further sideways relative movement of the watercraft 52 and 53 away from one another.
- the watercraft 52 and 53 may be held in spaced relation while traversing an area without securing the watercraft 52 and 53 to one another with a rigid structure.
- the cables 56 are expected to be easier to transport, as cables can typically be coiled and packed in a small area, relative to systems in which the watercraft 52 and 53 are secured to one another with a rigid structure that consumes a relatively large area during transport.
- the illustrated embodiment includes two watercraft 52 and 53, but other embodiments may include additional watercraft.
- a third watercraft may be secured to each of the watercraft 52 and 53 between each of the watercraft 52 and 53.
- additional watercraft may be secured between each of the watercraft 52 and 53, for example by cables extending between each of the watercraft, and the watercraft 52 and 53 may pull the cables in tension, thereby potentially holding an array of intermediate watercraft approximately in spaced relation.
- Figure 6 illustrates an embodiment of a water-born imaging system 58 including a watercraft 10 and a diffuser 60.
- the diffuser 60 may diffuse light such that regions underneath (e.g., directly below or below and to the side of) the watercraft 10 being imaged by an image-capture module (not show in this perspective) receives a portion of the ambient light, such as sunlight.
- the illustrated diffuser 60 includes a pair of generally symmetric cantilevered diffuser portions 62 extending outward from the sides of the watercraft 10.
- the illustrated diffuser portions 62 are generally reflectively symmetric about a keel 18 of the watercraft 10, but in other embodiments, the portions 62 may not be symmetric, or one or both portion may be omitted, which is not to suggest that any other feature described herein may not also be omitted.
- the diffuser 60 in this embodiment, includes beams 64 and a pair of diffuser surfaces 66 disposed on either side of the watercraft 10 over the water when the watercraft 10 is in the water.
- the beams 64 may be made of a generally rigid or flexible material, for example aluminum beams, fiberglass beams, or plastic beams, or wooden beams (or combinations thereof), and the diffuser plates 66 may be made of a transparent or generally translucent material, such as glass or plastic, such as acrylic glass.
- the diffuser surfaces 66 in some embodiments, may be a structure configured to diffuse light passing through the diffuser surfaces 66, such as frosted glass, a lenticular array, a plenoptic lens, or a substantially transparent material having an irregularly-shaped surface.
- the diffuser surfaces 66 may be a generally rigid material, such as glass, or the surfaces 66 may be a generally flexible material, such as a sheet of plastic, e.g., bubble wrap, extending between the beams 64.
- the diffusing surfaces 66 may include apertures or channels for the removal of water that splashes onto the surfaces 66 and to reduce forces from wind blowing against the surfaces 66.
- the diffusing surfaces 66 may be slanted downward, for example toward or away from the watercraft 10 to further channel water off of the surfaces 66.
- the surfaces 66 may be generally horizontal and generally coplanar. In the illustrated embodiment, the surfaces 66 are spaced away from the watercraft 10, such that a gap 68 exists on either side. In some embodiments, the gap 68 may be sized such that a canoe paddle fits within the gap 68.
- the beams 64 may be coupled to the watercraft 10 with a variety of techniques.
- the beams 64 may be bolted, clamped, strapped, or otherwise attached, removably or permanently to the watercraft 10.
- the diffusing surfaces 66 may be coupled to the beams 64 with a variety of teaching techniques, as well, for example, bolting, gluing, riveting, or otherwise attaching (removably or permanently) the end of the diffusing surfaces 66 to the beams 64.
- the beams 64 and the diffusing surface 66 may be integrally formed, for example from a single material, such as acrylic glass having a curved or bent perimeter to provide support.
- the water craft 10 may include a mast, and a distal portion of the beams may be connected to the mast by a cable to provide additional support.
- the diffusing surfaces 66 may be disposed between beams or cables extending between watercraft, such as the cables 56 of Figure 5 or the beams 44 of Figure 3.
- the illustrated embodiment includes a single diffusing surface 66 on either side of the watercraft 10, but other embodiments may include a diffusing surface 66 on only one side of the watercraft 10 or multiple surfaces on either side of the watercraft 10, for example a diffusing surface disposed toward the bow and a diffusing surface disposed towards the aft of the watercraft 10.
- the diffusing surfaces 66 may diffuse light such that the light reaches areas imaged by the image-capture module attached to the watercraft 10.
- the watercraft 10 may cast a shadow over regions being imaged by an image-capture module.
- some regions of an underwater area being imaged may include structures that cast their own shadows. The underwater regions falling within the shadows may be difficult to image, as the regions within the shadow may have insufficient light to be sensed accurately by the image-capture module.
- the diffuser 60 may diffuse sunlight such that a portion of the sunlight is redirected into regions that would otherwise fall within a shadow, thereby potentially illuminating regions that would otherwise be difficult to image.
- other techniques such as other types of diffusers, may be used to redirect sunlight.
- the surface of water on either side of the watercraft 10 may be disturbed, for instance with a comb-like structure extending from the sides of the watercraft 10 into the water, with bubbles injected into the water on either side of the watercraft 10, or with water sprayed over the sides of the watercraft 10.
- These other types of diffusers may be configured to disturb the surface of the water, which may also redirect sunlight into areas that would otherwise be difficult to image.
- FIG. 7 illustrates additional details of the above-described image-capture modules 12.
- the image-capture module 12 includes an underwater module 70 and an above -water module 72.
- a portion or substantially all of the under-water sensor module 70 may be immersed in water, and a portion or substantially all of the above- water module 72 may be disposed above the water, for example within a hull of one of the above-described watercraft.
- the above-water module 72 may be coupled to the under-water sensor module 70 by a power connection 74 and a data connection 76.
- the power connection 74 may conduct electrical power from the above -water module 72 to components of the under-water sensor module 70
- the data connection 76 may convey data and commands between the above-water module 72 and the under-water sensor module 70.
- the under-water sensor module 70 may be operable to obtain images of underwater features and approximately concurrently sense the orientation of the underwater sensor module while obtaining the images, and the above-water module 72 may be operable to control the under-water sensor module 70, provide power to the under-water sensor module 70, and receive positioning signals for determining the geographic location of the image- capture module 12, as described in greater detail below.
- the under-water sensor module 70 may include the features described above with reference to Figure 2.
- the under-water sensor module 70 of the present embodiment includes an inertial measurement unit (IMU) 78 and the above-described cameras 26 and lights 30.
- the under-water sensor module 70 may be partially or entirely disposed within the above-described housing 24 ( Figure 2).
- the inertial measurement unit 78 may be secured to the cameras 26, for example with zero degrees of freedom, either removably or irremovably, such that the inertial measurement unit 78 translates and is reoriented as the cameras 26 translate and are reoriented, for instance when a wave passes under a watercraft coupled to the image-capture module 12.
- the inertial measurement unit 78 may include a gyroscope or an accelerometer (e.g., a combination of a gyroscope and an accelerometer), such as a three axis gyroscope or accelerometer configured to sense rotation and acceleration along and about three, generally orthogonal axes.
- the inertial measurement unit 78 may include a sensor configured to detect changes in velocity or changes in rotational velocity of the cameras 26 and an integrator configured to integrate signals from the sensor such that a net movement may be calculated, for instance by a processor of the inertial measurement unit 78, based on an integrated movement about or along each of a plurality of axes.
- the inertial measurement unit 78 may be configured to receive location signals or error signals from the above -water module 70, e.g., from a global positioning system module described below or a compass, and suppress accumulated error based on the received signals, e.g., based on a difference between an expected change in position based on the inertial measurement unit 78 and an actual change in position or orientation received from the above -water module 72.
- the illustrated inertial measurement unit 78 is disposed in the under-water sensor module 72, thereby positioning the inertial measurement unit 78 relatively near the cameras 26 (e.g. in this context, within approximately 30 cm, 10 cm, or 5 cm) such that movement of the inertial measurement unit 78 relatively closely corresponds to that of the cameras 26, but in some embodiments, the inertial measurement unit 78 or another inertial measurement unit 78 may be disposed in the above-water module 72. Or in some embodiments, the inertial measurement unit 78, like other features described herein, may be omitted.
- the inertial measurement unit 78 may receive power from the above-water module 72 and output an orientation signal or a position signal indicative of the orientation or position, respectively, of the under-water sensor module 70 and the cameras 26.
- the inertial measurement unit 78 may output such a signal in response to a command from the above-water module 72 to the cameras 26 to capture an image (e.g., solely in response to this signal or in response to this signal and other conditions obtaining), in response to some duration of time elapsing that corresponds to a time between activation of the cameras 26 to capture an image (e.g., periodically), or in response to a signal from the cameras 26 indicating that an image is going to be, recently was, or is being captured.
- a measurement may be performed by the inertial measurement unit 78 in response to an image being captured or requested.
- the inertial measurement unit 78 may output a signal with a timestamp by which measurements may be selected and correlated with images from the cameras 26, images which may also be associated with a timestamp indicating the time at which an image was captured such that images and measurements may be associated based on differences between the timestamps, e.g., with the closest timestamp.
- some embodiments may include other orientation sensors.
- some embodiments may include a magnetometer configured to sense the azimuth of the cameras relative to magnetic North.
- Some embodiments may include a light detecting and ranging (LIDAR) sensor configured to sense data indicate of the altitude of the cameras relative to the surface of the water.
- Some embodiments may include a sonar sensor operable to sense the ground under a watercraft and provide data indicative of the altitude and azimuth of the cameras.
- signals from the above-mentioned sensors may be combined, e.g., with sensor fusion techniques, such that a more accurate aggregate orientation measurement is obtained relative to the measurements provided by the individual sensors.
- the illustrated cameras 26 may receive power from the above-water module 72 via the power connection 74, and the cameras 26 of this embodiment, may exchange commands and data with the above-water module 72 via the data connection 76.
- images captured by the cameras 26 may be stored in memory on the cameras 26, or images captured by the cameras 26 may be transmitted to the above-water module via the connection 76.
- the under-water sensor module 20 may have its own power source and may not be communicatively coupled to the above-water module 72 during image capture, e.g., images may be associated with measurements above water based on timestamps after the data is acquired.
- the lights 30 may receive power from the above-water module 72 via the power connection 74.
- power to the lights 30 may be interrupted (e.g., by the power source in response to a command from the below-described central processing unit) when (e.g., in response to) the cameras 26 are not capturing an image and may be restored when the cameras 26 are capturing an image to conserve power.
- the lights 30 may be selectively turned on by the power source based on an amount of light available to the cameras 26, for example based on signals from a light sensor on the cameras 26 indicative of an amount of light from a feature being imaged within a field of view of the cameras 26.
- the above-water module 72 is separate from the under-water sensor module 70 and is at least partially above water in order to facilitate reception of signals indicative of the geographic location of the image-capture module 12, such as global positioning system (GPS) signals from satellites.
- GPS global positioning system
- the above-water module 72 may be integrated with the under-water sensor module 70 and, for example, an antenna may extend above the surface of the water. Or in some embodiments, signal indicative of geographic location are not received by the image-capture module 12.
- the above -water module 72 includes a power source 79, a GPS module 80, memory 82, and a CPU (or other processor) 84.
- the above-water module 72 of this embodiment may be disposed within a water resistant or waterproof housing, such as a plastic housing disposed within one of the above-described watercraft.
- a cable or cables may extend over (or through) a side of the watercraft to the under-water sensor module and contain the power connection 74 and the data connection 76.
- the power source 79 in this embodiment may include a variety of different sources of power.
- the power source 79 may include a battery, a generator, a fuel cell, a solar cell, a generator powered by a wind turbine, a manually powered generator, or other sources electrical power.
- the power source 79 may be disposed in the under-water sensor module 70 in order to lower the center of gravity of the image-capture module 12 and potentially render the watercraft more stable.
- the illustrated GPS module 80 is an example of a location module.
- the GPS module 80 may be operable to receive signals from global positioning system satellites and determine a position, for example a relative geographic location or an absolute geographic location in latitude and longitude, of the image-capture module 12.
- a location module operable to determine location based on other signals, for example signals from a beacon at a selected origin or signals from the current wireless environment, for example signals from cellular towers by which position may be determined. Placing the GPS module 80, or an antenna of the GPS module 80, above the surface of the water is expected to improve the accuracy of the GPS module 80, as signals from global positioning system satellites or other location signals are expected to be attenuated or substantially eliminated underwater.
- the memory 82 may include any of a variety of different types of memory, and in some embodiments, the memory 82 and the CPU 84 may be part of a computing device, such as one of the examples of computing devices described below with reference to Figure 10.
- the memory 82 is shown as being a single functional block within the above-water module 82, but another embodiments, the memory 82 may be embodied by multiple memory devices disposed within the above-water module 72 or within the under-water sensor module 70 (e.g., in both), including memory within the cameras 26.
- the illustrated CPU 84 is an example of a processor that may be included within the above-water module 72.
- the CPU 84 may output commands to the GPS module 80, the power source 79, the memory 82, and the components of the under-water sensor module 70 to effectuate some or all of the functionality described herein.
- the functionality described herein may be loosely coordinated by the CPU 84, and some or all of the functionality may be performed by processors within the other components of the image-sensor module 12.
- the CPU 84 or other processor may execute instructions stored by the memory 82, which may be a tangible non-transitory machine readable memory, and executing those instructions may cause the image-capture module 12 to perform the functionality described herein, e.g., data-acquisition and calibration portions of the process described below with reference to Figure 8.
- the image-capture module 12 may be carried along a path on or near (e.g., within 2 meters of, within 1 meter of, or within 50 cm of) the surface of the water by one of the above-described watercraft, and the image-capture module 12 may capture images at a plurality of different locations along that path in a plurality of different directions at each location, along with orientation data indicative of the orientation of the cameras 26 when the images are captured and position data indicative of the position of the image-capture module 12 when the images are captured.
- the CPU 84 may output a command instructing the cameras 26 to capture an image, a command instructing the inertial measurement unit 78 to measure an orientation, and a command instructing the GPS module 80 to determine a position.
- the devices 26, 78, and 80 receiving these commands may perform the requested task and transmit the obtained data to the memory 82, which may associate the obtained data with one another.
- the memory 82 may associate the obtained data with one another.
- an interactive underwater navigable view of a geographic area for example from the perspective of a snorkeler, may be formed and presented to users in an interface on a user device, as described in detail below with reference to Figure 9.
- some embodiments may include multiple image-capture modules.
- some of the features of the image-capture module 12 may only be present in some of the plurality of image-capture modules, which is not to suggest that any feature described herein may not also be omitted in some embodiments.
- some embodiments may include a plurality of image-capture modules mounted on a single watercraft, for instance at different positions along the keel of the watercraft, at different positions perpendicular to the keel, or (i.e., and/or) at different depths, or some embodiments may include multiple watercraft, each having one or more image-capture modules, as described above.
- one of the plurality of image-capture modules may include the above-water module 72, and the single above-water module 72 may be coupled to a plurality of under-water sensor modules 70 distributed in one of the fashions described above. Sharing an above-water module 72 is expected to reduce the cost of the image-capture module 12, though other embodiments may include one above -water module 72 for every under-water sensor module 70, e.g., for purposes of standardizing the image-capture modules 12 and simplifying training, operation, configuration, and repairs.
- Figure 8 illustrates an example of a method 86 of capturing underwater images with an image-capture module moving along a path through the water.
- the illustrated method 86 may be performed by the above-described image-capture modules 12.
- instructions for performing portions of the method 86 may be stored in the memory 82 ( Figure 7) and may be executed by the CPU 84 described above.
- the method 86 may yield data for forming an interactive underwater view from the perspective of a snorkeler of underwater features, and users may view this data to tour underwater areas as described below with reference to Figure 9.
- the obtained data may be used for other purposes, for example for surveying underwater features (e.g., counting a species of fish or other wildlife or searching for candidate items for salvage), generating three-dimensional maps of underwater structures based on stereoscopic views of those structures (e.g., based on images from different paths of the same feature), or capturing images that are presented or analyzed in some other fashion.
- surveying underwater features e.g., counting a species of fish or other wildlife or searching for candidate items for salvage
- generating three-dimensional maps of underwater structures based on stereoscopic views of those structures e.g., based on images from different paths of the same feature
- capturing images that are presented or analyzed in some other fashion for example for surveying underwater features (e.g., counting a species of fish or other wildlife or searching for candidate items for salvage), generating three-dimensional maps of underwater structures based on stereoscopic views of those structures (e.g., based on images from different paths of the same feature), or capturing images that are presented or analyzed in
- the method begins with attaching cameras to a watercraft, as illustrative of block 88.
- Attaching cameras to watercraft may include attaching one of the above- described image-capture modules 12 to one of the above-described watercraft 10.
- the cameras may be calibrated, as illustrated by block 90.
- Calibrating the cameras may include navigating the watercraft over a reference structure, for example an image disposed on a heavier-than water plate that depicts a reference pattern, which may include regions of a know color for calculating white balance and regions of a known spatial distribution for spatially calibrating the cameras, e.g., to characterize distortion from a lens of the cameras such that the distortion can be reversed with a transformation based on the spatial calibration data.
- the reference structure may include an underwater grid, for example a grid formed by a net stretched over an underwater area, and the cameras may be calibrated based on images captured of the net, for example by determining a spatial distortion of the image-capture modules by comparing images of the grid to a known shape of the grid and mapping distorted pixels to a rectilinear representation of the image.
- images captured by the image-capture module may be transformed by reversing distortions measured with step 90 based on the calibration data.
- the watercraft may be propelled through the water, as illustrated by block 92.
- Propelling the watercraft may include manually propelling the watercraft with a paddle or by pushing the watercraft while standing on the sea floor, or in some embodiments, the watercraft may be propelled with an onboard propulsion system, such as an outboard electric or gasoline motor, or with a sail, for example.
- Propelling the watercraft may also include towing one or more watercraft with one or more powered watercraft.
- images of underwater features may be captured with the cameras, as illustrated by block 94.
- Capturing images of underwater features with the cameras may include diffusing light, such as sunlight such that the underwater features are illuminated by the diffused light, as described above with reference to Figure 6.
- Capturing images of underwater features may also include capturing a plurality of images from a given position in a plurality of different directions, as described above with reference to Figure 2.
- the present embodiment of method 86 may also include sensing the geographic location of the watercraft, as illustrated by block 96.
- Sensing the geographic location may include sensing the latitude and longitude of the watercraft, as described above with reference to the GPS module 80 of Figure 7.
- geographic location may be sensed at approximately the same time as images are captured in the step labeled with block 94, or the geographic location may be sensed at some other time and correlated with the images, for example the location sensed closest to the time in which the images are captured may be the sensed location of the step labeled with block 96.
- the method 86 may further include sensing the orientation of the cameras, as illustrated by block 98.
- Sensing the orientation of the cameras may include sensing the orientation with the above-described inertial measurement unit 78 of Figure 7. As with sensing location, orientation may be sensed at approximately the same time as the images are captured, or orientation may be sensed at a different time, for example shortly after or shortly before the images are captured, for instance before the cameras move substantially.
- the images, sensed location, and sensed orientation may be stored in memory, as illustrated by block 100. Storing this data may include storing the data in the above-describe memory 82 of Figure 7.
- the images, sensed location, and sensed orientation may also be associated with one another in memory, as illustrated by block 102. This data may be associated based on similar time stamps or based on the manner in which the data is stored. For instance, the sensed location and sensed orientation may be stored as meta-data of image files. In some embodiments, this data may be associated in a database, e.g., as entries of a relational database in the same entry or linked to one another through key values of separate tables.
- the data may be associated in some embodiments by storing the sensed data as attributes of an instance of the same object, e.g., an list of image objects, or in some embodiments, the data may be associated by storing the data as related entries in a document, e.g., as sibling (or otherwise related) fields in a hierarchical document, such as an extensible markup language (XML) document or a JavaScript object notation (JSON) document.
- XML extensible markup language
- JSON JavaScript object notation
- the images may be processed based on the associated sensed orientation data and position data to populate a data store for displaying images of the imaged area, e.g., in the form of an interactive, navigable view described below with reference to Figure 9.
- the images may be stitched together based on the position and orientation data.
- the orientation data may indicate a change in orientation between consecutive images, and the images may be aligned with one another for stitching based on the indicated change in orientation such artifacts from the stitching process (e.g., discontinuities in the image) are reduced relative to systems in which stitching occurs without orientation data.
- the resulting stitched images may be indexed or otherwise addressed according to the position data such that a user may select images based on position and view a sequence of positions along a path, also as described below.
- the above-described process may obtain relatively high-quality, well lit images and data for forming such interfaces at a relative low cost and relatively quickly, though not all embodiments necessarily provide all of these benefits.
- the images acquired with the above-described techniques may be texture mapped onto a surface by performing a bundle adjustment based on the orientation data and stitching the images together such that the images may be the texture mapped onto a surface.
- a pose, or 3-dimenstional orientation and location of the camera indicating azimuth and altitude of the optical axis of each camera and position in space of each camera, may be calculated based on the orientation data and location data.
- the pose may be calculated based on a combination of a number of signals indicative of orientation, e.g., by fusing signals from a number of orientation sensors.
- the images may be texture mapped onto a surface representative of the imaged geographic area, e.g., a flat plane or a 3 -dimensional, e.g., faceted, surface, by perspective transforming the images based on the calculated pose relative to the surface to which the image is mapped.
- Images depicting the imaged underwater area may be formed by rendering views of the texture mapped surface from viewer-selected positions such that a viewer may navigate to different perspectives of the imaged area.
- Some embodiments may also stitch images captured in the same direction from different positions into mosaic oblique views that are viewable by scrolling side-to-side along a traversed path.
- the images captured with the above-described systems and methods may be accessed and viewed by users of a geographic information service (GIS) or a mapping service, an example of which is the mapping service 104 illustrated in Figure 9.
- the illustrated mapping service 104 is part of a computing system 106 including the mapping service 104, a network 108, user devices 110, and a Web server 112.
- the mapping service 104 may display interactive maps to users, who may interact with the maps to select and view panoramas, such as the panoramas formed in accordance with the techniques described above and provided by a panorama-providing service of the mapping service 104.
- the mapping service 104 may be a panorama-providing service without the capacity to provide maps, which is not to suggest that any other feature described herein may not also be omitted.
- mapping service 104 may be remote from client devices 110, both of which may be remote from Web server 112.
- client devices 110 may be remote from Web server 112.
- one or more of these components 104, 112, and 110 may be located in the same place (e.g., a shared local-area network or physical building) or integrated into a single computing device.
- each of the components 104, 112, and 110 of the computing system 106 may include one or more computing devices, such as one or more of the examples of computing devices described below with reference to Figure 10, and the components 104, 112, and 110 may serve different roles in different embodiments and different use cases: for example the mapping service 104 may operate as a server and the devices 112 and 110 may operate as clients of the server 104, or in some use cases or embodiments, the components 104, 110, and 112 may operate as peers in a peer-to-peer architecture.
- the components of the computing system 106 are connected by network 108, which in some embodiments may include the Internet.
- the network 108 may also include intermediary networks, such as local-area networks, wireless-area networks, cellular networks, and the like.
- communication over the network 108 may include communication via intermediary devices, such as servers that cache content, such as webpages depicting maps or portions of panoramas, for expedited delivery to user devices 110, and intermediary devices that pre-render portions of content for user devices 110, e.g., by resizing images based on a display on user devices 110 or constructing portions of a document- object model, a style tree, a rule tree, a context tree, or a render tree based on the communicated content for the user device 110.
- intermediary devices such as servers that cache content, such as webpages depicting maps or portions of panoramas, for expedited delivery to user devices 110
- intermediary devices that pre-render portions of content for user devices 110, e.g., by resizing images based on a display on user devices 110 or constructing portions of a document- object model, a style tree, a rule tree, a context tree, or a render tree based on the communicated content for the user device 110.
- the mapping service 104 includes a Web server 114, an application program interface (API) server 116, a map server 118, a search engine 120, and a spatial-data store 122.
- API application program interface
- the mapping service 104 may be a computing system, such as a computing system having one or more computing devices connected to one another over a network.
- the illustrated functional blocks 114, 116, 118, 120, and 122 may be embodied in whole or in part at hardware, for example as a field programmable gate array or an application-specific integrated circuit, or in whole or in part as software executing one or more processes that provide some or all of the functionality described herein, and such software may be stored on a non-transitory tangible- machine readable medium, examples of which are described below with reference to Figure 10.
- the functional blocks 114, 116, 118, 120, and 122 are described as discrete and separate functional blocks, but in some embodiments, structures or code by which these blocks are implemented may be intermingled or otherwise organized in different groupings than those illustrated.
- the Web server 1 14 may be operable to receive requests for webpages, transmit requests for content to construct the requested pages, received the requested content, and transmit the webpages including requested content to the requesting device via the network 108.
- the Web server 114 may listen to a Transmission Control Protocol/Internet Protocol (TCP/IP) port connected to the network 108 at an Internet Protocol (IP) address of the network 108 and detect a request for a webpage, such as a webpage depicting a map.
- IP Internet Protocol
- the Web server 114 may also be operative to maintain a plurality of sessions, such as one session with each of the user devices 110 occurring over overlapping periods of time, in which a user interacts with a requested interactive map webpage on one of the user devices 110.
- the Web server 114 may store state data indicative of the state of a session or receive such state data from the user devices 110 and receive data indicative of user interactions with an interactive map and change the state data in response to the user interactions.
- the Web server 114 may transmit a request for map data to the map server 118 or the spatial-data store 122 in response to a request for a webpage or in response to receipt of data indicative of user interactions with the webpage, e.g., user commands requesting a different view.
- the Web server 114 may receive user search queries and transmit those search queries to search engine 120 to request searches based on the queries.
- the Web server 114 may receive content responsive to these requests, encode the received content in a webpage or content for a webpage, and transmit the encoded data to the user devices 110 or the Web server 112.
- the API server 116 may also receive requests for content and, in response, request other components of the mapping service 104 to obtain the requested content, which the API server 116 may transmit to a requesting user device 110 or the Web server 1 12.
- the API server 116 may be operable to provide content upon which other webpages are based, such as webpages served by the Web server 112, or provide content upon which a display is based in a special-purpose application, such as a mapping application, a navigation application, a social networking application, or other application, which may operate outside of a web browser.
- the illustrated map server 118 may be operable to receive requests for map data from the Web server 114 or the API server 116 and, in response, obtain the map data from the spatial- data store 122 and transmit the obtain map data to the requesting Web server 114 or the API server 1 16. Examples of such map data are described below with reference to a user interface operating on the user devices 110.
- the search engine 120 may be operable to receive search queries from the Web server 114 or the API server 116, identify content responsive to the query, and transmit the identified content (which may also include transmitting a pointer to the content, such as a uniform resource identifier (URI)) to the requesting Web server 114 or API server 116.
- URI uniform resource identifier
- the spatial-data store 122 may store a variety of different types of data relating to geographically distributed objects, such as roads, buildings, rivers, lakes, mountains, political boundaries, and the like.
- the spatial-data store may include images of geographically distributed objects, and images may be associated with metadata indicating the geographic area to which the image pertains such that images of adjacent geographic areas can be displayed adjacent one another to depict a larger geographic area than that depicted by either one of the images being combined.
- the stored images may depict the same geographic area at different resolutions or granularity, which may also be reflected by the metadata such that a resolution can be selected.
- the spatial-data store may also include data describing attributes of geographically distributed objects.
- the data describing such attributes may include traffic data describing the conditions of roads; weather data describing the weather, weather forecast, or past weather in geographic areas; social networking data describing the location of members of adjacent nodes of a social graph and their relationship or other attributes; business listings describing attributes of geographically distributed businesses; and the like.
- the spatial-data store 122 may be operable to receive requests for data stored within the spatial-data store 122 from the other components of the mapping service 104 and transmit the requested data to the requesting component.
- the spatial-data store 122 may store images depicting different views of geographic areas.
- the spatial-data store may include images depicting a satellite -view of the geographic area, a birds-eye level view of a geographic area (for example images acquired from an airplane), an eye-level view of geographic areas (for example images acquired from a camera mounted to a structure on the ground, such as a ground-based vehicle, a person, or a tripod), and a snorkeler-view based on images acquired with the above-described techniques.
- one or more of these views may include images stitched to one another with the above-described techniques.
- the illustrated Web server 112 may be operable to serve webpages to the user devices 110, and the webpages may include content from the mapping service 104.
- the Web server 112 may receive a request for a webpage or other content from a user device 110 and, in response, may construct the requested webpage by requesting content for the webpage from the mapping service 104 via either the Web server 114 or the API server 116.
- the Web server 1 12 may be operable to serve a webpage to the user devices 110 that includes a request for content from the mapping service 104, and the user device 110 may request this content from the mapping service 104.
- the illustrated user devices 110 may be a non-portable device, such as a desktop computer, a gaming console, or a set-top box, or a portable device, such as a laptop, a tablet computer, or a smart phone, each of which may include a power source (e.g., a battery or fuel cell) for off-grid operation.
- each of the user devices 110 are connected to the network 108 and include a display, a processor, and memory.
- the user devices 110 may be one of the computing devices described below with reference to Figure 10.
- the user devices 110 may include an operating system and a web browser that are stored in memory and executed on a processor of the user devices 110 or an application that uses data from the mapping service 104 that is stored in memory and executed by the processors of the user devices 110.
- Examples of interactive maps that may be displayed by the user devices 110 are illustrated by user interfaces 124 and 126.
- the user interfaces 124 and 126 may be displayed on a display of the user devices 110 and, in some embodiments, may be interactive.
- interface 124 illustrates an example of an interactive map.
- the interactive map of interface 124 may be rendered by a browser of the user devices 110 or may be displayed by a special-purpose application.
- the interface 124 is operable to display a map of a geographic area and receive user interactions by which the user requests to view different geographic areas, a subset of the depicted geographic area at a higher resolution or different view, or information about geographically distributed objects.
- Users may interact with the interface 124 through a variety of techniques, for example in some embodiments users may click on areas of the map with a mouse, click and drag areas of the map with a mouse, enter text commands with a keyboard, enter verbal commands with a microphone processed by voice recognition, or touch and drag on a single-touch or multi-touch surface of the display to enter commands.
- the illustrated interface 124 depict a plan view of a geographic area, for example a satellite view.
- the interface 124 includes panning commands 128 by which a user may command the interactive map to pan to the East, West, North, or South, and a zoom interface 130 by which a user may move a slider or click buttons to change the extent of the map depicted in the user interface 124.
- a search interface 132 may receive text or spoken search requests by a user, for example search queries relating to geographically distributed objects
- a navigation interface 134 may receive text or spoken navigation requests by which a user may request a route between different geographic areas, for example a route for a particular mode of transport, such as by walking, by bicycle, bike car, or by train.
- a layer interface 136 may receive user requests to overlay the map of the interface 124 with map data, such as traffic data, weather data, photographs, social-networking data, and the like.
- a view selector interface 138 may receive user requests to view a different view of a geographic area within the interactive map interface 124. For example, a user may click on the view selector interface 138 and drag the view selector interface 138 to a region of the illustrated map in which the user would like to view a eye level view.
- the interface 124 is operable to select a snorkeler eye-level view.
- the selector interface 138 may be dragged onto an imaged region of water, or a user may zoom in to an imaged region of water.
- the user device 110 may receive the request from the user, transmit a request for an snorkeler eye-level view to the mapping service 104, received snorkeler eye-level view interface 126 from the mapping service 104, and display the snorkeler eye-level view interface 126.
- the illustrated snorkeler eye-level view interface 126 is an interactive snorkeler eye- level view based upon the images captured and processed with the above-described techniques and stored in the spatial-data store 122.
- a user may interact with the interface 126 by clicking, dragging, pressing a button, or otherwise indicating that a user wishes to view within the interface 126 an image of a different direction from a given position, and in response, the user device 110 may display an image in the requested direction, such as an image formed with the above-described stitching process.
- images for adjacent directions may be stored in cache in the user device 110, or the user device 110 may request the images corresponding to the selected view from the mapping service 104, for example via the API server 116 or the Web server 114.
- the interface 126 further includes zoom interface 140 by which a user may request to zoom in or out of the image depicted by the interface 126, and a panning interface 142 by which a user may request images captured from a different position, for example images captured from a different position and stored and processed in accordance with the above-describe techniques.
- users may interact with a view-azimuth interface 144 to select a view of an imaged region from a different perspective. For instance, a user may rotate the interface 144 90-degrees to request a view from a camera 26 ( Figure 2) 90- degrees clockwise or counterclockwise relative to the camera from which the present view was captured.
- the images presented within the interface 126 may be relatively accurately stitched together, for example with relatively few stitching artifacts, in virtue of some of the above-describe techniques. This should be noted, however, that not all embodiments use the above-describe techniques for the purpose of providing data to a mapping service or for the purpose of stitching images.
- the map server 118 may be operable to itself, or in combination with another device, determine at least one of the following: whether a user associated with one or more of the user devices 110 has subscribed to a panorama-providing service (e.g., to a service at least partially provided by the map server 118); whether a user associated with one or more of the user devices 110 has a license to view images from the panorama-providing service; or an advertisement to be transmitted to the a user associated with one or more of the user devices 110 based on the requested at least a portion of the panorama.
- access to the images may be conditioned upon the result of one or more of these determinations.
- Figure 10 is a diagram that illustrates an exemplary computing system 1000 in accordance with embodiments of the present technique.
- Various portions of systems and methods described herein may include or be executed on one or more computer systems similar to computing system 1000. Further, processes and modules described herein may be executed by one or more processing systems similar to that of computing system 1000.
- Computing system 1000 may include one or more processors (e.g., processors 1010a- 1010 ⁇ ) coupled to system memory 1020, an input/output I/O device interface 1030 and a network interface 1040 via an input/output (I/O) interface 1050.
- processors may include a single processor or a plurality of processors (e.g., distributed processors).
- a processor may be any suitable processor capable of executing or otherwise performing instructions.
- a processor may include a central processing unit (CPU) that carries out program instructions to perform the arithmetical, logical, and input/output operations of computing system 1000.
- CPU central processing unit
- a processor may execute code (e.g., processor firmware, a protocol stack, a database management system, an operating system, or a combination thereof) that creates an execution environment for program instructions.
- a processor may include a programmable processor.
- a processor may include general or special purpose microprocessors.
- a processor may receive instructions and data from a memory (e.g., system memory 1020).
- Computing system 1000 may be a uni-processor system including one processor (e.g., processor 1010a), or a multi-processor system including any number of suitable processors (e.g., lOlOa-lOlOn). Multiple processors may be employed to provide for parallel or sequential execution of one or more portions of the techniques described herein.
- Processes, such as logic flows, described herein may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating corresponding output. Processes described herein may be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Computing system 1000 may include a plurality of computing devices (e.g., distributed computer systems) to implement various processing functions.
- I/O device interface 1030 may provide an interface for connection of one or more I/O devices 1060 to computer system 1000.
- I/O devices may include devices that receive input (e.g., from a user) or output information (e.g., to a user).
- I/O devices 1060 may include, for example, graphical user interface presented on displays (e.g., a cathode ray tube (CRT) or liquid crystal display (LCD) monitor), pointing devices (e.g., a computer mouse or trackball), keyboards, keypads, touchpads, scanning devices, voice recognition devices, gesture recognition devices, printers, audio speakers, microphones, cameras, or the like.
- I/O devices 1060 may be connected to computer system 1000 through a wired or wireless connection.
- I/O devices 1060 may be connected to computer system 1000 from a remote location. I/O devices 1060 located on remote computer system, for example, may be connected to computer system 1000 via a network and network interface 1040.
- Network interface 1040 may include a network adapter that provides for connection of computer system 1000 to a network.
- Network interface may 1040 may facilitate data exchange between computer system 1000 and other devices connected to the network.
- Network interface 1040 may support wired or wireless communication.
- the network may include an electronic communication network, such as the Internet, a local area network (LAN), a wide area (WAN), a cellular communications network or the like.
- System memory 1020 may be configured to store program instructions 1100 or data 1110.
- Program instructions 1100 may be executable by a processor (e.g., one or more of processors 1010a- 1010 ⁇ ) to implement one or more embodiments of the present techniques.
- Instructions 1100 may include modules of computer program instructions for implementing one or more techniques described herein with regard to various processing modules.
- Program instructions may include a computer program (which in certain forms is known as a program, software, software application, script, or code).
- a computer program may be written in a programming language, including compiled or interpreted languages, or declarative or procedural languages.
- a computer program may include a unit suitable for use in a computing environment, including as a stand-alone program, a module, a component, a subroutine.
- a computer program may or may not correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one or more computer processors located locally at one site or distributed across multiple remote sites and interconnected by a communication network.
- System memory 1020 may include a tangible program carrier having program instructions stored thereon.
- a tangible program carrier may include a non-transitory computer readable storage medium.
- a non-transitory computer readable storage medium may include a machine readable storage device, a machine readable storage substrate, a memory device, or any combination thereof.
- Non-transitory computer readable storage medium may include, nonvolatile memory (e.g., flash memory, ROM, PROM, EPROM, EEPROM memory), volatile memory (e.g., random access memory (RAM), static random access memory (SRAM), synchronous dynamic RAM (SDRAM)), bulk storage memory (e.g., CD-ROM and/or DVD- ROM, hard-drives), or the like.
- nonvolatile memory e.g., flash memory, ROM, PROM, EPROM, EEPROM memory
- volatile memory e.g., random access memory (RAM), static random access memory (SRAM), synchronous dynamic RAM (SDRAM)
- bulk storage memory e.g.
- System memory 1020 may include a non-transitory computer readable storage medium may have program instructions stored thereon that are executable by a computer processor (e.g., one or more of processors lOlOa- ⁇ ) to cause the subject matter and the functional operations described herein.
- a memory e.g., system memory 1020
- the program may be conveyed by a propagated signal, such as a carrier wave or digital signal conveying a stream of packets.
- I/O interface 1050 may be configured to coordinate I/O traffic between processors lOlOa- ⁇ , system memory 1020, network interface 1040, I/O devices 1060 and/or other peripheral devices. I/O interface 1050 may perform protocol, timing or other data transformations to convert data signals from one component (e.g., system memory 1020) into a format suitable for use by another component (e.g., processors lOlOa-lOlOn). I/O interface 1050 may include support for devices attached through various types of peripheral buses, such as a variant of the Peripheral Component Interconnect (PCI) bus standard or the Universal Serial Bus (USB) standard.
- PCI Peripheral Component Interconnect
- USB Universal Serial Bus
- Embodiments of the techniques described herein may be implemented using a single instance of computer system 1000, or multiple computer systems 1000 configured to host different portions or instances of embodiments. Multiple computer systems 1000 may provide for parallel or sequential processing/execution of one or more portions of the techniques described herein.
- Computer system 1000 is merely illustrative and is not intended to limit the scope of the techniques described herein.
- Computer system 1000 may include any combination of devices or software that may perform or otherwise provide for the performance of the techniques described herein.
- computer system 1000 may include or be a combination of a cloud-computing system, a data center, a server rack, a server, a virtual server, a desktop computer, a laptop computer, a tablet computer, a server device, a client device, a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a vehicle-mounted computer, or a Global Positioning System (GPS), or the like.
- PDA personal digital assistant
- GPS Global Positioning System
- Computer system 1000 may also be connected to other devices that are not illustrated, or may operate as a stand-alone system.
- the functionality provided by the illustrated components may in some embodiments be combined in fewer components or distributed in additional components.
- the functionality of some of the illustrated components may not be provided or other additional functionality may be available.
- instructions stored on a computer-accessible medium separate from computer system 1000 may be transmitted to computer system 1000 via transmission media or signals such as electrical, electromagnetic, or digital signals, conveyed via a communication medium such as a network or a wireless link.
- Various embodiments may further include receiving, sending or storing instructions or data implemented in accordance with the foregoing description upon a computer-accessible medium. Accordingly, the present invention may be practiced with other computer system configurations.
- a special purpose computer or a similar special purpose electronic processing or computing device is capable of manipulating or transforming signals, for instance signals represented as physical electronic, optical, or magnetic quantities within memories, registers, or other information storage devices, transmission devices, or display devices of the special purpose computer or similar special purpose processing or computing device.
Abstract
Description
Claims
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261592362P | 2012-01-30 | 2012-01-30 | |
PCT/US2013/023120 WO2013116100A1 (en) | 2012-01-30 | 2013-01-25 | Apparatus and method for acquiring underwater images |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2810018A1 true EP2810018A1 (en) | 2014-12-10 |
EP2810018A4 EP2810018A4 (en) | 2015-12-30 |
Family
ID=48905719
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP13743981.6A Withdrawn EP2810018A4 (en) | 2012-01-30 | 2013-01-25 | Apparatus and method for acquiring underwater images |
Country Status (4)
Country | Link |
---|---|
US (1) | US20150002621A1 (en) |
EP (1) | EP2810018A4 (en) |
AU (1) | AU2013215493A1 (en) |
WO (1) | WO2013116100A1 (en) |
Families Citing this family (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2531970B (en) * | 2010-11-15 | 2016-08-17 | Edesix Ltd | Image recording apparatus |
US9087385B2 (en) * | 2012-11-12 | 2015-07-21 | FMV Innovations, LLC. | Method for improving images captured underwater |
ITMI20131701A1 (en) | 2013-10-15 | 2015-04-16 | Consiglio Nazionale Ricerche | ACQUISITION SYSTEM AND PROCESSING OF UNDERWATER IMAGES |
US9998661B1 (en) * | 2014-05-13 | 2018-06-12 | Amazon Technologies, Inc. | Panoramic camera enclosure |
US20170049090A1 (en) * | 2014-05-13 | 2017-02-23 | Eco Net Ltd. | Digital Sports Fishing |
US9805061B2 (en) | 2014-11-18 | 2017-10-31 | International Business Machines Corporation | Image search for a location |
DE102016111239A1 (en) * | 2016-06-20 | 2017-12-21 | Ocean Maps GmbH | Method for generating 3D data of an object |
US20190014261A1 (en) * | 2016-10-20 | 2019-01-10 | Sky Light Electronic (Shenzhen) Limited Corporation | Photographing method and photographing system compatible in air and water |
CN106851065B (en) * | 2017-03-03 | 2023-04-25 | 核动力运行研究所 | Panoramic camera device |
WO2018157397A1 (en) * | 2017-03-03 | 2018-09-07 | 核动力运行研究所 | Panoramic camera device |
JP7165320B2 (en) * | 2017-12-22 | 2022-11-04 | 国立研究開発法人海洋研究開発機構 | Image recording method, image recording program, information processing device, and image recording device |
US11112241B2 (en) * | 2018-10-29 | 2021-09-07 | University Of New Hampshire | Apparatus and method for fault-proof collection of imagery for underwater survey |
CN113538702B (en) * | 2021-06-30 | 2023-05-23 | 大连海事大学 | Ocean cultivation area underwater scene panorama generation method |
KR102611117B1 (en) * | 2023-06-22 | 2023-12-08 | 주식회사 제이디 | Above-water and underwater images matching system using distortion correction and method thereof |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
IT1291570B1 (en) * | 1997-04-15 | 1999-01-11 | Stolfo Vincenzo Di | DOUBLE HULL SAILBOAT |
WO2002008799A2 (en) * | 2000-07-14 | 2002-01-31 | Ledalite Architectural Products Inc. | Light control devices with kinoform diffusers |
PL2562578T3 (en) * | 2007-03-16 | 2017-12-29 | Kollmorgen Corporation | System for panoramic image processing |
WO2009034477A2 (en) * | 2007-04-16 | 2009-03-19 | The Governors Of The University Of Calgary | Frame mapping and force feedback methods, devices and systems |
ITMI20072267A1 (en) * | 2007-12-03 | 2009-06-04 | Sidel Holdings & Technology Sa | DETECTION SYSTEM AND ANGULAR ORIENTATION OF CONTAINERS IN LABELING MACHINES |
US8345098B2 (en) * | 2008-03-17 | 2013-01-01 | International Business Machines Corporation | Displayed view modification in a vehicle-to-vehicle network |
US8493408B2 (en) * | 2008-11-19 | 2013-07-23 | Apple Inc. | Techniques for manipulating panoramas |
US8886206B2 (en) * | 2009-05-01 | 2014-11-11 | Digimarc Corporation | Methods and systems for content processing |
WO2011143622A2 (en) * | 2010-05-13 | 2011-11-17 | Google Inc. | Underwater acquisition of imagery for 3d mapping environments |
-
2013
- 2013-01-25 EP EP13743981.6A patent/EP2810018A4/en not_active Withdrawn
- 2013-01-25 US US14/375,448 patent/US20150002621A1/en not_active Abandoned
- 2013-01-25 WO PCT/US2013/023120 patent/WO2013116100A1/en active Application Filing
- 2013-01-25 AU AU2013215493A patent/AU2013215493A1/en not_active Abandoned
Also Published As
Publication number | Publication date |
---|---|
EP2810018A4 (en) | 2015-12-30 |
US20150002621A1 (en) | 2015-01-01 |
AU2013215493A1 (en) | 2014-08-21 |
WO2013116100A1 (en) | 2013-08-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20150002621A1 (en) | Apparatus and Method for Acquiring Underwater Images | |
US10908678B2 (en) | Video and image chart fusion systems and methods | |
US10942028B2 (en) | Video sensor fusion and model based virtual and augmented reality systems and methods | |
US11328155B2 (en) | Augmented reality labels systems and methods | |
US11181637B2 (en) | Three dimensional target selection systems and methods | |
US20210206459A1 (en) | Video sensor fusion and model based virtual and augmented reality systems and methods | |
US20220024549A1 (en) | System and method for measuring the distance to an object in water | |
CN110246177A (en) | Automatic wave measuring method based on vision | |
CN106468552A (en) | A kind of two-shipper crossing location method based on airborne photoelectric platform | |
González-Jorge et al. | UAV photogrammetry application to the monitoring of rubble mound breakwaters | |
CN114926739B (en) | Unmanned collaborative acquisition processing method for geographical space information on water and under water of inland waterway | |
Sun et al. | Imaging-based nearshore bathymetry measurement using an unmanned aircraft system | |
WO2012066642A1 (en) | Field-of-view video information generating apparatus | |
WO2018102772A1 (en) | System and method for augmented reality comprising labels | |
CN101793517B (en) | Online quick method for improving accuracy of attitude determination of airborne platform | |
CN103744085A (en) | Underwater robot five component ranging sonar inclined shaft three dimensional imaging system and imaging method | |
CN113920447A (en) | Ship harbor detection method and device, computer equipment and storage medium | |
CN113552382A (en) | Wind speed and direction measuring method, device and system | |
WO2018140645A1 (en) | Three dimensional target selection systems and methods | |
CN112985398A (en) | Target positioning method and system | |
Vos | Remote sensing of the nearshore zone using a rotary-wing UAV | |
CN115082811A (en) | Method for identifying and measuring distance of marine navigation ship according to image data | |
CN105869193B (en) | Side-scanning sonar image auxiliary interpretation method based on UUV | |
Ishibashi | The low distortion all-around view system using fisheye lens for an underwater vehicle | |
Jia et al. | Ship Target Recognition and Positioning Based on Aerial Photos |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20140724 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAX | Request for extension of the european patent (deleted) | ||
RA4 | Supplementary search report drawn up and despatched (corrected) |
Effective date: 20151126 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G03B 17/08 20060101AFI20151120BHEPIpc: G03B 37/04 20060101ALI20151120BHEP |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE APPLICATION IS DEEMED TO BE WITHDRAWN |
|
18D | Application deemed to be withdrawn |
Effective date: 20160626 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230519 |