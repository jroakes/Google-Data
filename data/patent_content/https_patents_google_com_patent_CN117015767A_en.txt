CN117015767A - On-chip interconnect for memory channel controllers - Google Patents
On-chip interconnect for memory channel controllers Download PDFInfo
- Publication number
- CN117015767A CN117015767A CN202280006948.0A CN202280006948A CN117015767A CN 117015767 A CN117015767 A CN 117015767A CN 202280006948 A CN202280006948 A CN 202280006948A CN 117015767 A CN117015767 A CN 117015767A
- Authority
- CN
- China
- Prior art keywords
- memory
- channel
- oci
- data
- system memory
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 41
- 239000013598 vector Substances 0.000 claims abstract description 39
- 238000000605 extraction Methods 0.000 claims abstract description 19
- 238000010801 machine learning Methods 0.000 claims abstract description 16
- 238000013528 artificial neural network Methods 0.000 claims description 38
- 238000013507 mapping Methods 0.000 claims description 11
- 238000004364 calculation method Methods 0.000 claims description 6
- 230000008878 coupling Effects 0.000 abstract description 2
- 238000010168 coupling process Methods 0.000 abstract description 2
- 238000005859 coupling reaction Methods 0.000 abstract description 2
- 238000012545 processing Methods 0.000 description 56
- 238000004422 calculation algorithm Methods 0.000 description 21
- 239000011159 matrix material Substances 0.000 description 17
- 230000008569 process Effects 0.000 description 17
- 238000004590 computer program Methods 0.000 description 12
- 238000004891 communication Methods 0.000 description 11
- 238000012549 training Methods 0.000 description 10
- 230000004044 response Effects 0.000 description 9
- 238000003860 storage Methods 0.000 description 7
- 238000012546 transfer Methods 0.000 description 7
- 238000013519 translation Methods 0.000 description 5
- 241001522296 Erithacus rubecula Species 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 230000001575 pathological effect Effects 0.000 description 4
- 230000004913 activation Effects 0.000 description 3
- 238000001994 activation Methods 0.000 description 3
- 238000013459 approach Methods 0.000 description 3
- 235000019580 granularity Nutrition 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000000306 recurrent effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000001131 transforming effect Effects 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000010923 batch production Methods 0.000 description 1
- 230000001351 cycling effect Effects 0.000 description 1
- 238000013499 data model Methods 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 238000011156 evaluation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000007170 pathology Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1668—Details of memory controller
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F15/00—Digital computers in general; Data processing equipment in general
- G06F15/76—Architectures of general purpose stored program computers
- G06F15/80—Architectures of general purpose stored program computers comprising an array of processing units with common control, e.g. single instruction multiple data processors
- G06F15/8053—Vector processors
- G06F15/8061—Details on data memory access
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F15/00—Digital computers in general; Data processing equipment in general
- G06F15/76—Architectures of general purpose stored program computers
- G06F15/78—Architectures of general purpose stored program computers comprising a single central processing unit
- G06F15/7839—Architectures of general purpose stored program computers comprising a single central processing unit with memory
- G06F15/7842—Architectures of general purpose stored program computers comprising a single central processing unit with memory on one IC chip (single chip microcontrollers)
- G06F15/7846—On-chip cache and off-chip main memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/0223—User address space allocation, e.g. contiguous or non contiguous base addressing
- G06F12/023—Free address space management
- G06F12/0238—Memory management in non-volatile memory, e.g. resistive RAM or ferroelectric memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1605—Handling requests for interconnection or transfer for access to memory bus based on arbitration
- G06F13/161—Handling requests for interconnection or transfer for access to memory bus based on arbitration with latency improvement
- G06F13/1621—Handling requests for interconnection or transfer for access to memory bus based on arbitration with latency improvement by maintaining request order
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1605—Handling requests for interconnection or transfer for access to memory bus based on arbitration
- G06F13/161—Handling requests for interconnection or transfer for access to memory bus based on arbitration with latency improvement
- G06F13/1626—Handling requests for interconnection or transfer for access to memory bus based on arbitration with latency improvement by reordering requests
- G06F13/1631—Handling requests for interconnection or transfer for access to memory bus based on arbitration with latency improvement by reordering requests through address comparison
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1605—Handling requests for interconnection or transfer for access to memory bus based on arbitration
- G06F13/1642—Handling requests for interconnection or transfer for access to memory bus based on arbitration with request queuing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/16—Handling requests for interconnection or transfer for access to memory bus
- G06F13/1605—Handling requests for interconnection or transfer for access to memory bus based on arbitration
- G06F13/1652—Handling requests for interconnection or transfer for access to memory bus based on arbitration in a multiprocessor architecture
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/38—Information transfer, e.g. on bus
- G06F13/40—Bus structure
- G06F13/4004—Coupling between buses
- G06F13/4022—Coupling between buses using switching circuits, e.g. switching matrix, connection or expansion network
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F15/00—Digital computers in general; Data processing equipment in general
- G06F15/16—Combinations of two or more digital computers each having at least an arithmetic unit, a program unit and a register, e.g. for a simultaneous processing of several programs
- G06F15/163—Interprocessor communication
- G06F15/173—Interprocessor communication using an interconnection network, e.g. matrix, shuffle, pyramid, star, snowflake
Abstract
Methods, systems, apparatus, and computer readable media for an integrated circuit that accelerates machine learning computations are described. The circuit includes processor cores, each processor core including: a multi-channel controller; an interface controller for coupling each channel controller to any memory channel of the system memory; and an extraction unit in each channel controller. Each extraction unit is configured to: receiving channel data encoding addressing information; based on the addressing information, retrieving data from any memory channel of the system memory using the interface controller; and writing the acquired data to a vector memory of the processor core via a respective channel controller including the respective fetch unit.
Description
Cross Reference to Related Applications
The present application claims priority from U.S. provisional application No. 63/167,593 filed on 3/29 of 2021, which is incorporated herein by reference in its entirety.
The present application relates to U.S. provisional application No. 63/001,216 filed on 3/27 of 2020. The contents of which are incorporated herein by reference.
The present application relates to U.S. application Ser. No. 16/865,539, filed 5/4/2020. The contents of which are incorporated herein by reference.
Background
The present specification relates generally to using circuitry to perform neural network calculations.
Neural networks are machine learning models that employ one or more layers of nodes to generate output, e.g., classification, for a received input. In addition to the output layer, some neural networks include one or more hidden layers. The output of each hidden layer is used as input to one or more other layers in the network, such as other hidden layers or output layers of the network. Some layers of the network produce outputs from the received inputs based on the current values of the respective parameter sets.
Some neural networks are Convolutional Neural Networks (CNNs) (e.g., for image processing) or Recurrent Neural Networks (RNNs) (e.g., for speech and language processing). Each of these neural networks comprises a respective set of convolutional or recurrent neural network layers. The neural network layer may have an associated set of kernels and an embedding layer for processing the input to generate a set of vectors for training the neural network. The Kernel (Kernel) may be represented as a tensor of weights, i.e., a multidimensional array. For example, the embedding layer may process a set of inputs, such as an input of image pixel data or an activation value generated by a neural network layer. The set of inputs or the set of activation values may also be represented as tensors.
Disclosure of Invention
The distributed system may include a memory for storing values that are accessed and used to perform operations or calculate values. Each value may be stored in memory at a corresponding location identified by an address. The memory may be arranged to include different memory channels, where each channel includes a set of memory locations identified by a respective set of addresses. The channel controller is used to control and manage access to particular memory locations of a given memory channel to retrieve data specified by the request. More specifically, the channel controller uses the communication channels of the distributed system to manage the data flow to and from the memory.
Based on this context, techniques are described for implementing a hardware interface controller, which is a shared interconnect or crossbar device configured to allow any channel controller of an integrated circuit to communicate with any memory channel of an example system memory. For example, the interface controller is configured to provide dynamic coupling of a given channel controller to various memory channels of the high bandwidth memory. The channel controller may be included in a hardware structure of the interface controller. The integrated circuit may be a hardware machine learning accelerator or other special purpose processor that includes multiple processor cores. The interface controllers may be integrated in the processing pipeline of the accelerator's circuitry to enable each channel controller to read data from any channel of the high bandwidth memory system, as well as write data.
One aspect of the subject matter described in this specification can be embodied in integrated circuits configured to accelerate machine learning computations. The circuit includes a plurality of processor cores, and each processor core includes a plurality of channel controllers, an on-chip interconnect (OCI) interface configured to couple each of the plurality of channel controllers to each memory channel of a system memory; and a respective extraction unit in each of the plurality of channel controllers. The respective extraction unit is configured to: i) Receiving a request for encoding addressing information; ii) based on the addressing information, obtaining data from any memory channel of the system memory using the interface controller; and iii) writing said data acquired from any memory channel to a vector memory of said processor core by said channel controller comprising said fetch unit.
These and other implementations can each optionally include one or more of the following features. For example, in some embodiments, an interface controller has or is operable to control an in-memory OCI node, comprising: i) A plurality of read interfaces for retrieving data from any memory location on any memory channel of the system memory; and ii) a plurality of write interfaces for writing data to any memory location on any memory channel along the system memory.
In some implementations, an in-memory OCI node includes: a first-in-first-out (FIFO) memory queue configured to store incoming OCI transaction requests during arbitration of existing OCI transactions for: i) Retrieving data from any memory location of the system memory, or ii) writing data to any memory location of the system memory. In some embodiments, the integrated circuit further comprises an interface controller configured to: receiving the request for encoding the addressing information; generating a plurality of control signals based on a plurality of IDs in the addressing information; and providing the plurality of control signals to any memory channel of the system memory to extract data stored in memory locations of the system memory.
In some implementations, the addressing information is derived from one or more incoming OCI transaction requests received at the interface controller; and at least one existing OCI transaction request accessing any memory location of the system memory is previously stored in the FIFO memory queue. Addressing information may be derived from one or more incoming OCI transaction requests; and each incoming OCI transaction request may include a plurality of Identifiers (IDs) corresponding to any set of memory locations on any memory channel of the system memory.
In some embodiments, the OCI interface comprises two or more addressing modes; and the interface controller is operable to generate control signals that allow selection between the two or more addressing modes of the interface controller. A first addressing mode of the two or more addressing modes is a channel interleaving mode that limits mapping of an embedded table relative to a memory channel of the system memory. A second addressing mode of the two or more addressing modes is a stack interleaving mode that expands the mapping of the embedded table relative to a memory channel of the system memory.
In some implementations, each OCI transaction request is received by the fetch unit; and processing by the interface controller to initiate access to any memory location indicated in the OCI transaction request. In some implementations, each OCI transaction request to access data stored in system memory encodes the following: i) A 32 byte length corresponding to the data being accessed; ii) a token ID representing a source address specific to any memory location in system memory; and iii) a destination address of a memory location of the vector memory. In some implementations, the embedding table is split into a plurality of data slices that are allocated into memory channels of the overall system memory; the interface controller selects the stack interleave pattern using any of the plurality of channel controllers to access any portion of the embedded table.
The integrated circuit further includes: a respective refresh unit in each of the plurality of channel controllers, the respective refresh unit configured to: i) Receiving a request for encoding addressing information; ii) writing data from a source address in the vector memory to any memory location of the system memory using the interface controller based on the addressing information. In some embodiments, the machine learning calculation is performed on a neural network input by a neural network layer; the refresh unit is used to write parameters of the neural network layer to any memory location of the system memory using the interface controller.
Another aspect of the subject matter described in this specification can be implemented in computer-implemented methods that are performed using integrated circuits configured to accelerate machine learning computations. The integrated circuit includes a system memory and a processor core including a plurality of channel controllers, and the method includes: a plurality of requests are received, wherein each of the plurality of requests encodes addressing information. For a first request of a plurality of requests, the method includes: identifying any memory location of any memory channel of the system memory based on the first requested addressing information; retrieving data from a memory location in any memory location of the system memory using an on-chip interconnect (OCI) interface that couples each of the plurality of channel controllers to each memory channel of the system memory; and writing the data retrieved from the memory location to a vector memory of the processor core based on the addressing information of the first request.
These and other implementations can each optionally include one or more of the following features. For example, in some embodiments, the OCI interface comprises an in-memory OCI node comprising a plurality of read interfaces and a plurality of write interfaces; and the method further comprises: acquiring data from any memory location on any memory channel of the system memory using a plurality of read interfaces based on control signals generated by an interface controller; and writing data to any memory location on any memory channel of the system memory using the plurality of write interfaces based on control signals generated by the interface controller.
In some embodiments, the in-memory OCI node comprises a first-in-first-out (FIFO) memory queue, and the method comprises: during arbitration of existing OCI transactions, incoming OCI transaction requests are stored in the FIFO memory queue for: i) Retrieving data from any memory location of the system memory, or ii) writing data to any memory location of the system memory.
In some embodiments, the method further comprises: the interface controller receiving the request to encode the addressing information; the interface controller generating a plurality of control signals based on a plurality of IDs in the addressing information; and the interface controller providing the plurality of control signals to any memory channel of the system memory to extract data stored in memory locations of the system memory.
In some implementations, the addressing information is derived from one or more incoming OCI transaction requests received at the interface controller; and at least one existing OCI transaction request accessing any memory location of the system memory is previously stored in the FIFO memory queue. In some implementations, the addressing information is derived from one or more incoming OCI transaction requests; and each incoming OCI transaction request includes a plurality of Identifiers (IDs) corresponding to any set of memory locations on any memory channel of the system memory.
Other implementations of this and other aspects include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods encoded on computer storage devices. The system of one or more computers may be configured by means of software, firmware, hardware or a combination thereof installed on the system such that in operation the system performs actions. One or more computer programs may be configured with instructions that, when executed by a data processing apparatus, cause the apparatus to perform the actions.
The subject matter described in this specification can be implemented in specific embodiments to realize one or more of the following advantages.
The circuitry for the crossbar/on-chip interconnect may be implemented on dedicated hardware circuitry, such as a hardware accelerator used in a distributed system. The crossbar allows each channel controller to read and write data to any address location of a memory cell in any channel of the high-bandwidth memory system in communication with the processor core or accelerator chip. This avoids the need to map a channel controller to a particular memory channel, which may lead to load imbalance of performance penalty. The crossbar is implemented to load balance the allocation of addresses by allocating addresses to any channel controller to process on all memory channels. Thus, the crossbar switch may improve performance in a distributed system compared to existing approaches.
The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example computing system.
Fig. 2 is a block diagram of an example of a structure including a control unit, a channel controller, and a memory channel.
Fig. 3 illustrates an example algorithm for implementing load balancing of a channel controller.
Fig. 4 illustrates an example allocation of requests to different channel controllers.
Fig. 5A and 5B illustrate an example OCI interface controller.
FIG. 6 illustrates an example flow chart of a write operation involving an OCI processor node.
FIG. 7 illustrates an example flow chart of a read operation involving an OCI processor node.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
FIG. 1 illustrates a block diagram of an example computing system 100 configured to retrieve data elements stored in a memory of the system 100. The data elements may be retrieved and used to perform neural network calculations for example machine learning workloads. For example, the data elements may be processed to calculate the output of the neural network layer or to perform an embedding layer operation to generate an embedding set for training the neural network.
The embedded output is generated when the neural network of the system 100 is trained to perform certain computational functions, such as calculations related to machine translation, natural language understanding, ranking models, or content recommendation models. In some implementations, training the neural network includes updating an embedded set previously stored in an embedded table of the neural network, such as during a previous stage of training the neural network. For example, the embedding of the embedding layer of the neural network may be trained with the neural network for which the embedding is to be used. Thus, the techniques described in this specification can be used to update the embedding during training of the neural network with improved efficiency over the prior art.
Typically, an embedding layer of the neural network is used to embed features into a feature/embedding space corresponding to the embedding layer. The embedded vector may be a respective vector of numbers mapped to respective features in a set of features of a lookup table representing the embedded layer. A feature may be an attribute or characteristic shared by the individual units on which analysis or prediction is to be performed. For example, the individual elements may be groups of words or pixels of images in a vocabulary that form part of items such as images and other documents. An embedded algorithm that trains the embedded layer may be executed by the neural network processor to map features to the embedded vectors. In some implementations, the embedding of the embedding table is learned together with other layers for which the embedded neural network is to be used. This type of learning occurs by back-propagating gradients to update the embedded tables.
In other embodiments, the embedding may be learned independently of other layers to use the embedded neural network, such as when the embedding is pre-trained. For example, the neural network processor may use the algorithm to compute the embedding by processing information about the discrete input features to determine a mapping or placement of similar inputs of geometrically close embedding vectors in the embedding space. In some cases, the process of computing the embeddings may represent techniques for feature learning or feature engineering that allow the system to automatically discover the representations needed for feature detection from the raw input data.
In some implementations, a given "input" can have one or more characteristics of one or more types, and the embedding layer generates a respective embedding for each of these types. For example, the input may be for a search query having several different feature types. The feature types may include attributes of the user or user device (e.g., location, preferences, device type, etc.), query tokens, previously submitted queries, or other relevant types that may correspond to attributes of the search query. For any feature type where a given input has more than one feature, the computing system is operable to retrieve individual embeddings of each of these features. The system is further operable to combine the retrieved embeddings, for example by calculating an average of the embedment values, to generate a final embedment for the feature type.
The computing system 100 includes a host 102, a multi-core processing unit 104, and a memory unit 105 ("memory 105"). Memory 105 includes data slices (shards) 106a-106k, where k is an integer greater than 1. The memory 105 will be described in more detail below. In general, host 102 may be a processing unit, such as a processor, multiple processors, or multiple processor cores. Thus, host 102 may include one or more processors and is operable to generate or process instructions for accessing a target dense matrix and send instructions 110 to multicore processing unit 104 to generate the target dense matrix. As described in more detail below, performing the embedding layer operation may include transforming sparse elements from one or more matrices to generate a dense matrix.
The multi-core processing unit 104 accesses respective elements 108a-108n from one or more data slices 106a-106k in the memory 105, where n is an integer greater than 1. The multi-core processing unit 104 generates a target dense matrix 112 using the corresponding elements 108a-108n and provides the target dense matrix 112 to the host 102 for further processing. The multi-core processing unit 104 may generate the target dense matrix 112 by transforming each of the elements 108a-108n into vectors and concatenating the n vectors into a single vector.
In general, in the case of embedding, the "sparse" information corresponding to the sparse elements may be a single hotspot vector identifying the feature values. For example, if there are five possible values for a given feature (e.g., a, B, C, D, E), the sparse vector identifies the feature value "a" as (1,0,0,0,0,0), and the embedding layer maps (1, 0) to the dense embedding vector of feature value "a". In some implementations, during training of the embedding layer to learn embedding, the elements 108a-108n may be embedding vectors that are transformed into weight values of an embedding table of vectors, such as eigenvalues "B" or "C. The weight values may be transformed using a neural network processor of the multi-core processing unit 104, the multi-core processing unit 104 executing a training algorithm to compute the embedding based at least on a mapping of features to the embedding vectors.
The host 102 may process instructions for updating the target dense matrix and send the updated dense matrix to the multi-core processing unit 104. For example, the target dense matrix may correspond to an embedding of a neural network. Thus, host 102 may process instructions to update the embedding to produce an updated dense matrix. For example, during subsequent iterations of training the neural network to update the embedding, reverse pass may be performed to update the embedding by determining a new mapping of input features to the embedding vectors and generating an updated dense matrix based on the new mapping. In some implementations, the multi-core processing unit 104 is operable to transform the updated dense matrix into corresponding sparse elements and update one or more sparse elements (e.g., weights) stored in the data shards 106a-106k accordingly.
As described above, host 102 is configured to process instructions executing within computing system 100. In some implementations, the host 102 is configured to process the target dense matrix 112 generated by the multi-core processing unit 104. In some other implementations, the host 102 may be configured to request the multicore processing unit 104 to generate the target dense matrix 112, and another processing unit may be configured to process the target dense matrix 112.
Each processor of the multi-core processing unit 104 is configured to retrieve data elements stored in the memory of the system 100. The memory may include a plurality of data slices 106a-106k storing data including elements 108a-108 n. The data may include inputs, activations, gain values or weight values corresponding to parameters or kernels of the weight matrix structure. In some implementations, the data slices 106a-106k can be one or more volatile memory units. In some other implementations, the data slices 106a-106k can be one or more nonvolatile memory cells.
As used in this document, a data slice may include a memory bank, a memory unit, or a related portion of memory, such as on-chip memory, high bandwidth system memory, external memory, or a combination of these. The data slices 106a-106k may also be another form of computer-readable medium, such as a device or other configuration in a storage area network. The data slices 106a-106k may be coupled to the multi-core processing unit 104 using electrical, optical, or wireless connections. In some implementations, the data slices 106a-106k may be part of a multi-core processing unit 104 and based on a processor-in-memory (PIM) architecture.
The multi-core processing unit 104 is configured to determine a dense matrix based on the sparse elements. The multi-core processing unit 104 includes a plurality of interconnected processors or processor cores. For example, the multi-core processing unit 104 may be a distributed processing system including a plurality of interconnected processor cores. In general, the terms "processor" and "processor core" are used interchangeably to describe the discrete interconnected processing resources of the multi-core processing unit 104.
The system 100 also includes a process ID control unit 114 ("control unit 114"). The control unit 114 receives a set of ID headers (headers) and performs operations to assign the ID headers or to assign portions of information included in the ID headers. The ID header is assigned to the channel controller, which will be described in more detail below with reference to fig. 2. In some embodiments, the system 100 includes a plurality of control units 114. For example, the system 100 may include a control unit 114 for each processor or processor core at the system 100. Each control unit 114 coupled to the processor/core of the multi-core processing unit 104 receives a set of ID headers from a source. The source may be the host 102 or another processor of the multi-core processing unit 104.
The ID header may represent a request that includes information specifying an address of a memory location in memory 105. Memory 105 may represent a High Bandwidth Memory (HBM) or input/output (I/O) device that exchanges data communication with a control unit 114 in a processor core of example hardware circuitry included in system 100. For example, memory 105 may exchange data communication with a processor core of multicore processing unit 104 to pass input to the core and receive output generated by one or more computing resources of the core. The input and data values stored in memory locations of memory 105 or written to memory locations of memory 105 may represent vector elements or arrays of vector values.
Memory 105 may be a Dynamic Random Access Memory (DRAM) resource of system 100. In some implementations, the memory 105 is external or off-chip memory relative to example hardware circuitry including one or more processors or processor cores. Memory 105 is configured to exchange data communication (described below) with on-chip resources of a hardware circuit, such as a Vector Processing Unit (VPU) or a vector store of VPUs. For example, memory 105 may be disposed in a physical location outside of an integrated circuit die (die) representing the hardware circuitry of system 100. Thus, memory 105 may be remote or non-local with respect to computing resources disposed within the integrated circuit die. Alternatively, the memory 105, or portions of its resources, may be disposed within an integrated circuit die representing a dedicated hardware circuit such that the memory 105 is local to or co-located with the computing resources of the circuit.
Fig. 2 is a block diagram of an architecture 200 including a channel controller 202 and a memory channel 204 of the memory 105, as well as the control unit 114 architecture 200 described above. Each of the memory channels 204 may represent a memory bank of the memory 105, a set of memory banks of the memory 105, a set of memory locations of the memory 105, or a combination thereof.
The set of channel controllers 202 includes a plurality of corresponding channel controllers, denoted at least as C0, C1, C2, and C15. In the example of fig. 2, the architecture 200 may include 16 channel controllers. In some embodiments, the architecture 200 includes more or fewer channel controllers. For example, architecture 200 may include N channel controllers and N memory channels 204. These aspects of structure 200 are represented by reference numeral 202-n for a single channel controller and reference numeral 204-n for a memory channel for a single memory channel.
The implementation of fig. 2 illustrates an example in which individual channel controllers, such as channel controllers 202-0 (C0) and 202-2 (C2), are hard mapped to particular corresponding memory channels, such as memory channels 204-0 (C0) and 204-2 (C2), respectively. As described above, system memory 105 with channel controller 202 hard-mapped to a particular memory channel may experience load imbalance. These load imbalances may delay or substantially delay the operation of the neural network calculations performed at the system 100, such as the operation for generating the output of the embedded layer.
This existing approach of mapping a particular channel controller 202 to a particular memory channel may have other challenges. For example, the method may have a constraint that data be required to be stored in a manner that is sensitive to the address and how the data is mapped to a particular channel controller 202. In addition, this approach may be inefficient when the system needs to perform a large number of randomized searches to retrieve vectors from a large space in memory. To address these challenges, on-chip interconnects (OCIs) or cross-bar (crossbar) are integrated on dedicated hardware circuits (described below). The crossbar may be integrated in a processing pipeline (pipeline) of the accelerator circuit to enable each channel controller to read data from any channel of the high-bandwidth memory system, as well as write data.
In some implementations, the dedicated circuitry is a multi-core hardware accelerator, and the OCI is a channel controller interface that is uniquely configured based at least on a multi-core structure of the hardware accelerator. For example, the channel controller interface is configured to allow communication between each core of the multi-core hardware accelerator and each memory channel of the memory 105, including different types of memory structures corresponding to the memory channels.
The size of the channel controller interface may be 32B x 4 instead of 128B x 1. Based on this example size, the channel controller interface may include multiple independent transaction threads (transaction thread) between the memory 105 and the channel controller 202 without requiring an unrelated port of OCI hardware. In some implementations, the channel controller interface is configured to efficiently handle dynamic bandwidth requirements at each channel and at different computing stages. For example, gigabytes per second (GBps) bandwidth requirements may vary for different computations for different access scales, e.g., 32 byte (32B) access, 64 byte (64B) access, 128 byte (128B) access. These stages may include forward pass computations, backward pass computations, and backward pass computations that implement optimization algorithms, such as adagard, to update the learned values of particular vectors based on gradients generated from the evaluation of the neural network of certain training data.
The channel controller interface may be uniquely configured to include a plurality of node interfaces. For example, the crossbar may include: i) An intra-client node interface operable to carry Direct Memory Access (DMA) descriptors and control messages; ii) an in-memory node interface operable to carry read/write commands and data for various memory structures of the memory system (e.g., buffer memory, instruction memory, shared memory, vector memory, host memory); iii) An in-processor node interface (lower portion) operable to transfer load/store traffic from the first/lower set of channel controllers 202 to memory 105; and iv) an intra-processor node interface (upper) operable to transfer load/store traffic from the second/upper set of channel controllers 202 to memory 105. This is described in more detail below with reference to at least fig. 5A and 5B.
The channel controller interface allows the channel controller group to access any memory channel/address of the memory 105. However, even when the addresses specified in the request are distributed among a set of channel controllers 202, large-scale execution of certain machine learning workloads may exhibit a data access pattern that results in a particular channel controller receiving a significant amount of data processing load relative to other channel controllers. In the example of fig. 2, channel controller 202-0 shows an imbalance in which the channel receives a significant amount of data processing load relative to other channel controllers (e.g., C1, C2). To address this issue, a crossbar is used to implement a particular control scheme to control the allocation of addresses or requests to each channel controller 202. The control scheme causes the addresses to be substantially equally allocated among the channel controllers 202. This is described in more detail below with reference to fig. 3.
Fig. 3 illustrates an example algorithm 300 for implementing load balancing of the memory channel controller 202.
As described above, data access for an exemplary machine learning workload may present certain pathology patterns (pathological pattern). For example, even though a set of requests and addresses may generally be distributed among channel controllers 202, there may be some modes in which a particular channel controller is required to operate on a large number of larger features or large vectors. This mode may cause the control unit 114 to dispatch a set of processing tasks or ID headers, which may still result in load imbalance at the channel controller 202. For example, these modes may have burst characteristics that cause them to occur in certain short processing windows, for example between 20 and 100 cycles. Load imbalance may occur even if any of the channel controllers 202 is configured to access any memory location of the memory 105 and any memory channel 204.
Algorithm 300 corresponds to the control scheme described above and is an exemplary scheduling algorithm for implementing load balancing of memory channel controller 202 of system 100. Algorithm 300 may include pseudo code, as shown in the example of fig. 3, representing one or more instruction steps of scheduling algorithm 300. In some embodiments, algorithm 300 is a modified round-robin scheduling algorithm. The modified polling attribute of the scheduling algorithm 300 allows a set of ID headers to be parsed and scheduled to the channel controller 202.
For example, the modified polling scheduling algorithm 300 is configured to interrupt or disable potential pathological sequences that may occur during data access by machine learning workloads. Thus, the modified polling scheduling algorithm 300 is configured to allow an ID header (e.g., an active address or gradient) to be assigned in a manner that load balances across each channel controller 202 in a set of channel controllers (350). The standard polling method for the scheduling procedure indicates that the channel controllers are selected in a simple round robin order, where the selection is performed without priority.
To address the burst mode described above, the polling method may be modified or modified to first detect the initial completion of the first round robin selection order. In response to detecting the initial completion, the control unit 114 may then adjust the delta parameters to modify the initial channel controller selected for the second or subsequent round of cycling (round).
For example, system 100 may include 16 channel controllers (e.g., CC0-CC 15). Control unit 114 may select each channel controller 202 during an initial round and detect completion of the initial round based on a count parameter indicating that CC15 has been selected during the round. The count parameter may correspond to a total number of channel controllers (16) such that selection of CC15 during an initial round indicates selection of each of the 16 channel controllers. Control unit 114 may then adjust the value of the delta parameter to bypass the selection of a particular channel controller.
For example, control unit 114 may increase the delta parameter to bypass the selection of CC0 and select CC1 at the beginning of a subsequent channel selection round. Likewise, control unit 114 may again increase the delta parameter to bypass the selection of CC1 and select CC2 at the beginning of another subsequent channel selection round. In some implementations, the control unit 114 may periodically adjust the value of the delta parameter to increase (or decrease) the delta of the channel count based on one or more observed data access patterns, as described in more detail below with reference to fig. 4.
Fig. 4 illustrates a table 400 showing an exemplary sequence 410 for selecting channel controllers 202 to achieve balanced allocation of requests to different channel controllers 202.
As briefly described above, native (active) polling schemes may suffer from pathological patterns in the input data being accessed for computation. For example, the pattern may be that every 16 th ID header would belong to an embedding table with the longest embedding vector and the greatest computation intensive optimizer. Even in the original polling scheme, the example mode may cause load imbalance. The control unit 114 may be a hardware component of a processor core that executes instructions corresponding to the scheduling algorithm 300 to implement a modified poll ID header scheduling scheme.
Based on algorithm 300, the scheduling scheme is operable to reduce the probability of load imbalance due to pathological patterns in the input dataset. Algorithm 300 may be used to generate an example sequence 410 for selecting channel controller 202. Each number in the sequence indicates a channel controller to be selected. In some implementations, the sequence 410 may initially iterate through each channel controller in the set (e.g., 0 to 15) based on the initial unmodified polling stream.
After an initial iteration of selecting each channel controller, the polling stream may be modified to select channel controller CC1 instead of starting again with select channel controller CC 0. Also, after the second iteration of selecting each channel controller, the polling stream may be modified to select channel controller CC2 instead of selecting channel controller CC1 to start over. This modified selection scheme provides an example of how control unit 114 selects each channel controller in the set to allow equal or substantially equal allocation addresses in the set. In some implementations, the system 100 monitors the data access patterns of each channel controller and dynamically adjusts or modifies the scheduling scheme based on the observed patterns.
The control unit 114 uses the modified scheduling scheme to generate a set of channel numbers for the set of channel controllers 202. The generated set of channel numbers is processed at the control unit 114 to forward the ID header to the corresponding channel controller 204. In some implementations, the control unit 114 forwards the ID header to the corresponding channel controller 204 based on the example sequence 410 derived from the modified scheduling scheme. To ensure adequate load balancing of the processing workload of the ID header between the channel controllers 202, the algorithm 300 causes the control unit 114 to implement certain characteristics for selecting the channel number. In some embodiments, algorithm 300 is used for channel selection based on the example steps of pseudo code shown in fig. 3.
For example, the channel selection attribute requires that the generation of channel numbers be fair and non-bursty. The "fairness" attribute for generating the channel number causes (or requires) all channel controllers to be equally or substantially equally selected for a given machine learning task. The "non-bursty" nature of the channel signals used to generate them causes (or requires) that in repeated selection of a particular channel controller for a given machine learning task, the channel controller be selected without intermittent increases. For example, the channel number sequence is "0,1,0,1,4,5,0," a "not ideal pattern, and the" non-bursty "attribute of generating channel numbers is not satisfied.
An example set of metrics (metrics) may be used to determine whether each of the above-described attributes (e.g., fairness and non-burstiness) are met. The metrics include a count, average (mean) and median of the number of occurrences of the channel number for selection. For a "count" indicator, the system 100 is operable to determine a count of the number of times each process iteration includes a channel or channel number. The number of times should be the same for all channels 202 or channel controllers 202. If the system 100 determines that the number of times is not the same, the system 100 may detect that the particular mode selected by the channel controller is biased and not load balanced for a given set of operations.
For the "average" indicator, the system 100 is operable to determine, for each channel number, whether the number of times that the selected channel number occurs after a threshold number of iterations converges to N, where N is an integer greater than or equal to 1. For example, if the system 100 includes 16 channel controllers, the system 100 is operable to determine, for each channel number, whether the number of times that the selected channel number occurs after a threshold number of iterations or ID headers converges to 16. In some implementations, the threshold number of iterations varies based on the size and complexity of the data being retrieved and operated on.
The "median" indicator indicates the burstiness of a particular channel controller. For example, if system 100 determines that channel controller 204-n has a low median select value, it will receive more ID headers in the burst than other channel controllers, which may indicate an imbalance. Table 400 includes sample metric values for each channel number for an example processing iteration run for a threshold 2048 ID headers. As previously described, the system 100 may monitor the data access patterns of each channel controller, correlate to the metrics and attributes discussed above, and dynamically adjust or modify the scheduling/control scheme based on the observed patterns. For example, control unit 114 may periodically adjust the value of the increment parameter to increase (or decrease) the increment of the channel count based on the data access pattern.
Fig. 5A and 5B illustrate an example OCI interface 500 that includes a channel OCI controller 502, the channel OCI controller 502 managing or controlling operations associated with the OCI architecture (or structure) of the system 100. System 100 (and OCI interface 500) may include a plurality of OCI controllers 502. In some embodiments, OCI controllers 502 correspond to the respective channel controllers 202 described above. In some other embodiments, OCI controller 502 is different from the corresponding channel controller 202 described above. OCI controller 502 is an interface controller that is operable to control OCI interface 500, including nodes and read or write interfaces of the OCI interface. In some implementations, one or more features of OCI interface 500 are included in and/or may be controlled by an interface controller.
The OCI interface 500 may be a network node of an integrated circuit comprising a plurality of chips, wherein each chip may represent a processor, a processor core, or a hardware accelerator. For example, OCI interface 500 may include a plurality of sub-network nodes that cooperate to couple or interconnect each chip of an integrated circuit such that data obtained via one chip may be routed to another chip via a network node.
In general, controller 502 controls operations associated with one or more interface devices of the OCI interface of system 100. The OCI interface 500 may include a respective 32B random access load/store interface from each channel controller and may support multiple threads (e.g., eight threads) of valid 32B read/write access to memory 105 per memory channel. The controller 502 is configured to reorder one or more incoming responses from the memory 105 to update the fetch or refresh unit of a given channel controller. This will be described in more detail below.
In some implementations, the OCI interface includes a 4 x 32B read-write interface for each channel 504, 506. These read and write interfaces may represent intra-processor OCI nodes, where each node serves two channels 504,506, or channel controllers corresponding to channels 504 and 506. The example of FIG. 5A illustrates read operations associated with read interface 508, while the example of FIG. 5B illustrates write operations associated with write interface 510. The 4 x 32B configuration of the OCI interface 500 allows independent transaction threads to occur between the memory 105 and the channel controller. For example, as described above, the size of the OCI interface 500 may be 32b×4 instead of 128b×1. Based on this example size, OCI interface 500 may include multiple independent transaction threads between memory 105 and channel controller 202 without requiring an unrelated port of OCI hardware.
The OCI interface 500 is configured to issue channel credits maintained by each data interface. In some embodiments, each data interface corresponds to a subnetwork or node of OCI interface 500. The controller 502 may generate one or more control signals corresponding to each channel credit. For example, controller 502 may generate control signals based on various IDs in addressing information encoded in the OCI transaction request. The controller 502 may also provide or route control signals to any memory channels of the system memory to obtain data stored in memory locations of the system memory.
In some embodiments, each node of OCI interface 500 serves two channel controllers and credits on that interface node are shared between the two channel controllers. Unlike a single 128B interface, when other channel controllers are busy, a lightly loaded channel controller may experience long delays in its transactions, with the 4 x 32B configuration of the OCI interface 500, this interdependence exists primarily (or only) between two channels arbitrated for the same port.
The mapping of channel controllers to OCI interfaces may be fixed or dynamic. In embodiments where the mapping is fixed, one port may be busy for a short period of time while the other ports are idle, but the overall configuration allows for distribution of the IDs such that all ports are equally well utilized.
The OCI interface 500 may include at least two interfaces that interact with a Direct Memory Access (DMA) unit of the system 100 that handles bulk transfer and control messages for a given processor core. For example, the DMA unit interacts with at least two OCI interfaces, a DMA client interface, and a DMA memory interface. The DMA client interface is a bulk data move interface for BMEM, BIMEM, SMEM, VIMEM and HMF and includes a descriptor sub-interface to send descriptors to the OCI node and a message sub-interface for control messages related to synchronization flags and interrupt operations. The DMA memory interface processes read and write requests from the OCI and includes a read sub-interface and a write sub-interface. The read sub-interface receives the read command from the controller 502 of the OCI interface 500 and sends back a response on the 128 byte data interface. The write sub-interface receives the write command and writes data on the 128-byte interface.
BMEM is a vector memory structure for storing data vectors, while SMEM is a scalar memory structure for storing scalar data values. BIMEM is a processor core instruction memory, while VIMEM is a vector instruction memory (VIMEM) that stores programs for execution by the address processor VPU units. In some embodiments, the DMA unit of system 100 treats (or configures) the vimm across 16 channel controllers as one large sequential memory.
In some cases, the read and write interfaces of OCI interface 500 are part of an OCI node within the memory of system 100. The node or element may include an input first-in-first-out (FIFO) memory queue (or buffer) that stores incoming OCI transactions while different memory blocks/stacks are arbitrated for writing to or reading from operations. In some embodiments, one or more incoming OCI transaction requests processed during an existing operation are pre-stored in a FIFO memory queue as a step of arbitrating a set of operations.
The size of the input FIFO may be determined based on the worst-case BMEM access latency. In some implementations, the FIFO is sized in this way, even though the BMEM access latency may not be the slowest memory. This is because memory other than BMEM can only be accessed at the start of a batch, so any performance penalty on OCI interface 500 due to these accesses will only be suffered once.
Memory 105 (e.g., HBM) may include four HBM stacks interleaved at a 64 byte granularity. For example, memory 105 may include HBM stack 0 interleaved in bytes [0-63] [128-191], HBM stack 1 interleaved in bytes [64-127] [192-255], HBM stack 2 interleaved in bytes [256-319] [384-447], and HBM stack 3 interleaved in bytes [320-383] [448-511 ]. In some implementations, memory 105 may include more or fewer stacks and various byte granularities. For a 64 byte granularity, when 128 bytes are accessed from HBM stack 0, the stack will return data stored in the memory location corresponding to bytes [0-63] [128-191 ]. This may represent an example Q128 addressing mode on the OCI structure.
The OCI controller 502 of the example processor core may cause the core to operate in a channel interleaved mode or a stack interleaved mode. Each of the channel interleave pattern and the stack interleave pattern represent an example addressing pattern of the processor core. The channel interleaved addressing mode may map the embedded table to a single HBM stack closest to the processor core such that all memory accesses are made to the closest HBM stack, which may provide less latency relative to another HBM stack.
In stack interleaving mode, the embedded table may be partitioned across all High Bandwidth Memory (HBM) stacks on the chip (e.g., stacks of memory 105) to utilize different system addressing schemes. For example, during stack interleaving mode, the same address scheme as the rest of system 100 may be used to access the embedded tables, and the software control of system 100 may be configured to use the entire capacity of memory 105, even if it chooses to use less processing cores on an integrated circuit chip that includes multiple cores.
In some embodiments, the channel controller of system 100 includes at least two (and optionally three) processing stages: extraction, computation and refresh (optionally). This may be a sequential processing pipeline that processes channel IDs in FIFO order.
The extraction process is implemented using an extraction ID unit, such as extraction ID unit 702 (or 704) discussed below with reference to fig. 7. In some implementations, the fetch ID unit is the first processing stage in the channel controller pipeline. The fetch ID unit receives the channel ID data structure from the processor core ID unit 114. The channel ID data encodes the data length (in 32 bytes) to be retrieved, the token ID (e.g., the source address of memory 105) and the BMEM destination address. The extraction ID unit is used to load data from the memory 105 and store the obtained data to the BMEM address specified in the channel ID data. The channel ID is forwarded to the address processor unit of the channel controller and the corresponding data is stored in the BMEM.
In some implementations, the data may be a parameter vector stored in a cache (e.g., a circular cache) in the BMEM. Each time the fetch ID unit issues a load request to memory, the cache write pointer is moved. Once the data in the cache has been consumed, the subsequent pipeline stages increment the read pointer. The fetch ID unit may stop issuing memory 105 loads when the cache does not have sufficient space for memory 105 to load data to be stored in the BMEM. In addition, memory 105 may generate unordered read responses. The fetch ID unit includes response reordering logic to ensure that the channel IDs are sent to the address processor in order.
The refresh process is implemented using refresh ID cells, such as refresh ID cell 602 (or 604) discussed below with reference to fig. 6. In some embodiments, the refresh ID unit is an optional third processing stage in the channel controller pipeline. Typically, the refresh ID unit is used to move data from, for example, a BMEM source address to a memory 105 destination address. The refresh ID unit receives the channel ID data structure from the address processor into the FIFO. The channel ID data encodes the data length to be retrieved (in 32 bytes), the token ID (e.g., the destination address of memory 105) and the BMEM source address. Similar to the fetch ID unit, the refresh ID unit has a simplified responsibility for loading parameters from the BMEM source address (i.e., memory location) and storing them in the destination address (i.e., memory location) of the memory 105. Thus, the refresh ID unit may write parameters of the neural network layer to any memory location of the high bandwidth system memory using the OCI interface 500.
The OCI interface 500 may require that all requests be broken down into 128 byte (128B) transmissions. In some cases, the request may be broken down into 64B or 128B transfers depending on the address mode of the processor core implementing the request. The processor core may be configured to support multiple addressing modes. Address translation is completed and the transfer split into multiple 128B (or 64B) requests before sending the request to the OCI interface/crossbar 500.
After receiving the stored response from the memory 105, the synchronization flag is updated by the refresh ID unit. This allows the load from the subsequent batch process to see the latest data all the time. Since the responses may arrive out of order, the refresh ID unit uses a simple mechanism to compare the total number of memory commands sent to the total number of responses received from memory 105. When the counts match and no additional channel ID data is incomplete, the synchronization flag is updated.
FIG. 6 illustrates an example flow chart of a write operation 600 involving an OCI processor node. Write operation 600 may include one or more transactions. Each channel controller is connected to memory 105 through an OCI interface and is used to implement the write pipeline of operation 600. Each channel controller includes a respective refresh Identification (ID) unit that expands transactions associated with write operation 600. In the example of fig. 6, a first channel controller 0 includes a refresh ID unit 602 ("refresh unit 602") and performs one or more expansion and translation operations 606, while a second, different channel controller 1 includes a refresh ID unit 604 ("refresh unit 604") and performs one or more expansion and translation operations 608. In some embodiments, channel controller 0 and channel controller 1 are the same channel controller.
Each of the refresh units 602,604 may expand the respective transaction to a multiple of 128B or 64B depending on the channel operation mode of the given processor core including the respective channel controllers 0 and 1. For example, as described above, an OCI transaction request encodes addressing information for use in obtaining data from any memory channel of a system memory using an OCI interface, or for writing data obtained from any memory channel to a vector memory of a processor core. Thus, in some implementations, the addressing information is from one or more incoming OCI transaction requests received at the OCI controller. Each incoming OCI transaction request may include a plurality of Identifiers (IDs) corresponding to any set of memory locations on any memory channel of the system memory.
The OCI transaction request may be expanded and/or translated (e.g., decomposed) into 64B or 128B transfers depending on the address mode of the processor core implementing the request. Expanding the corresponding transaction to multiples of 128B or 64B may also align 32B and send to OCI controller 502. As described above, each processor core of system 100 may operate in a channel interleaved mode or a stack interleaved mode, each mode representing an example addressing (or operating) mode of the processor core.
Each OCI controller 502 may communicate with two different refresh units, such as refresh units 602,604, or refresh units corresponding to one or more other memory channels of memory 105. Based on the requester, controller 502 assigns an OCI ID to each write request and sends out the write request on the corresponding write interface. For example, the controller 502 may send or send a request via a write interface coupled to an example channel 0 or channel 1 by selecting between the two refresh units 602,604 in a round robin fashion using the example selection logic of the channel arbiter 610. The selection logic may be based on the modified poll scheduling algorithm 300 described above. In some implementations, each of the refresh ID units 602,604 can generate a "deallocation" signal to indicate each time channel ID data processing is complete. As described above, reverse pass may be performed during the training phase of the neural network machine learning/data model to update the embedded tables. During a backward pass operation for a parameter, a deallocation signal is transmitted to the corresponding fetch ID unit.
In some implementations, the operations performed by each of the refresh units 602,604 are not affected by the order in which the write operations are completed. Because a given refresh unit 602,604 may not be concerned with the order in which writes are completed, it is not necessary to use more than two ocids for the writes. Thus, controller 502 may simplify its operation and achieve computational savings or efficiency by minimizing the assignment of additional OCI IDs to write requests. In some implementations, the OCI interface 500 may use a bit vector to track the outstanding IDs. Based on the write complete OCI ID, the ID is forwarded to a refresh unit that generates a transaction in which the number of outstanding completions is counted.
FIG. 7 illustrates an example flow chart of a read operation 700 involving an OCI processor node.
Each channel controller (e.g., controller 502) includes a respective fetch Identification (ID) unit that expands transactions associated with read operations 700. In the example of fig. 7, the first channel controller 0 includes an extraction ID unit 702 ("extraction unit 702") and performs a expand and translate operation 706, while the second different channel controller 1 includes an extraction ID unit 704 ("extraction unit 704"). For example read operations, transaction expansion and address translation are completed within the corresponding OCI controller 502, after which a request is issued to channel arbiter 610 to send a read request to the OCI. In this example, OCI controller 502 corresponds to channel controller 0 or channel controller 1.
In some implementations, each fetch unit 702,704 tracks completion of the read operation using the count tag memory and updates the sync mark memory to account for the completion. In some other implementations, the control module of the unroll transaction is configured to generate at least one 128B transaction every 4 cycles to match the rate at which read data can be accepted. The channel arbiter 610 services both channel controllers in a round robin fashion and looks up the available IDs in the tag tracking table 710. For example, the tracking table may hold one entry for each ID, and may include 256 entries. In some implementations, the tracking table may be configured to hold more or fewer entries. In the 256 entry example, rows 0-127 of the table may be reserved for the extraction unit 702, and rows 128-255 of the table may be reserved for the extraction unit 704.
Based on the requestor fetch unit, the channel arbiter 610 may perform a lookup to determine an area of the table that includes one or more available IDs. The IDs may be assigned in an incremental manner. For example, once ID0 is assigned, it is not reassigned even though ID 0's read data has been returned until ID127 is assigned. This configuration simplifies the implementation of the table and provides the availability of 128 IDs per channel controller in a manner that reduces or eliminates the likelihood that the OCI will experience a lack of IDs. In some implementations, each entry in the tag tracking table holds a memory address of 32B for the least significant 32B of the transaction, 13B for the BMEM address to which data is to be written, 2B for indicating the transaction length (a multiple of 32B), 5B for counting tags received from the fetch unit to track completed (possibly larger) transactions, and 1B for caching to mark whether the transaction is cacheable in the read bypass cache. In some cases, the least significant 32B of the transaction is required to write to the read bypass cache.
Data read from OCI interface 500 may be sent into the FIFO in 1-4 cycles depending on the size of the transaction. Writing the first 32B line of the transaction to this FIFO triggers a lookup of the tag table to retrieve the BMEM address, HBM address, and count tag attached to this data. A transaction may include a cache enable bit (cache enable bit). If the cache enable bit for the transaction is set to 0 in table 710, then the read bypass cache is not updated and the ID is deallocated. If the cache bit is set to 1, the data is written to the read bypass cache.
Evictions (elictions) in the read bypass cache may occur in FIFO order, with the least significant 32B of the transaction written first. This, in combination with the fact that the HBM address match list is looked up with the least significant 32B of the transaction, will ensure that if the lower 32B of the transaction causes a "hit," then the remaining bytes of the transaction may also be present in the cache. The data looked up from the fetch units 702,704 is such that accesses to a particular HBM address will have the same length, which causes each transaction to that particular HBM address to be spread out in the same way. This implementation detail avoids the situation where a "hit" is caused on only a portion of the transactions in the read bypass cache.
The memory 105 address from tag table 710 may be added to the matching list and corresponding data updated in the cache and by arbitrating write ports between writes from each of the four channel OCI controllers. To avoid the risk of evicting a "hit" entry in the memory 105 address list, memory addresses that are located near the next eviction candidate may be marked as "misses". In some cases, this is because there is an arbitration delay between the actual hit and the time it takes to read the data from the bypass buffer.
Depending on the structural layout of the integrated circuit, there may be some delay difference between rewriting the various copies of the HBM address matching list and bypassing the cache. In some implementations, system 100 includes control parameters that define how close a hit should be to a threshold of eviction candidates to be marked as a miss. The memory 105 address list may be replicated among the channel OCI controllers to avoid the need for arbitration to determine if an entry exists in the bypass cache. The system 100 may be configured to invalidate some (or all) entries in the read bypass cache between batches to ensure that the system does not use stale data. An invalidation request to bypass the cache may be sent with the last ID in the batch of fetch units.
The data read from the OCI or bypass cache is sent to the BMEM in line 32B along with the BMEM address it must write to. As described above, the corresponding extraction unit (e.g., extraction unit 702 or 704) tracks read completion using the count tag memory and performs an update of the sync mark memory.
One or more transactions may be marked as uncacheable and sent to another round-robin channel arbiter 610, which selects between those transactions and any cacheable transactions with a lookup miss. The grant provided by the channel arbiter is based on the available credits indicated at the OCI interface.
Transactions marked as cacheable may be sent to a separate FIFO queue or table for lookup of memory 105 addresses present in the read bypass cache. The system 100 performs address matching by searching the memory 105 addresses in parallel in, for example, 512 entries for the least significant 32B of the transaction. In some embodiments, system 100 may include 4 channel OCI controllers, each having its own copy of the memory 105 address list present in the bypass cache shared among all 4 controllers. An update to the bypass cache may result in all copies of the address being updated. In the event of a hit in the address list, the transaction must arbitrate with transactions from the other 3 controllers to access the read data.
In some implementations, the OCI ID assigned to the transaction is released and the transaction is queued for read data lookup. Read data from the bypass cache is queued to be sent back to the fetch unit from which the request originated, and this may be retrieved from the most significant bit of the OCI ID originally assigned to the transaction. This sending of read data is performed by polling an arbiter that selects between data from the OCI interface 500 and data from the bypass cache.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory program carrier, for execution by, or to control the operation of, data processing apparatus.
Alternatively or additionally, the program instructions may be encoded on a manually-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiving apparatus for execution by data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
The term "computing system" includes various means, devices, and machines for processing data, including for example, a programmable processor, a computer, or multiple processors or computers. The apparatus may comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may include code that creates an execution environment for the computer program in question, such as code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
The computer program may, but need not, correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array), an ASIC (application-specific integrated circuit), or a GPGPU (general purpose graphics processing unit).
For example, a computer adapted to execute a computer program may be based on a general purpose or special purpose microprocessor or both, or any other type of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. Some elements of a computer are a central processing unit for executing or executing instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Furthermore, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., an LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on a user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification. Or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship between client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated into a single software product or packaged into multiple software products.
Specific embodiments of the present subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes illustrated in the accompanying figures do not necessarily require the particular order or sequence illustrated to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (20)
1. An integrated circuit configured to accelerate machine learning computations, the circuit comprising:
A plurality of processor cores, each processor core comprising:
a plurality of channel controllers;
an interface controller configured to couple each of the plurality of channel controllers to each memory channel of a system memory; and
a respective extraction unit in each of the plurality of channel controllers, the respective extraction unit configured to:
i) Receiving a request for encoding addressing information;
ii) based on the addressing information, retrieving data from any memory channel of the system memory using the interface controller; and
iii) The data acquired from any memory channel is written to a vector memory of the processor core by the channel controller including the fetch unit.
2. The integrated circuit of claim 1, wherein the interface controller is operable to control: an in-memory OCI node comprising:
i) A plurality of read interfaces for retrieving data from any memory location on any memory channel of the system memory; and
ii) a plurality of write interfaces for writing data to any memory location on any memory channel along the system memory.
3. The integrated circuit of claim 2, wherein the in-memory OCI node comprises:
a first-in-first-out (FIFO) memory queue configured to store incoming OCI transaction requests during arbitration of existing OCI transactions for: i) Retrieving data from any memory location of the system memory, or ii) writing data to any memory location of the system memory.
4. The integrated circuit of claim 3, wherein the interface controller is configured to:
receiving the request to encode the addressing information;
generating a plurality of control signals based on a plurality of IDs in the addressing information; and
the plurality of control signals are provided to any memory channel of the system memory to extract data stored in memory locations of the system memory.
5. The integrated circuit of claim 4, wherein:
the addressing information is derived from one or more incoming OCI transaction requests received at the interface controller; and
at least one existing OCI transaction request to access any memory location of the system memory is previously stored in the FIFO memory queue.
6. The integrated circuit of claim 5, wherein:
each incoming OCI transaction request includes a plurality of Identifiers (IDs) corresponding to any set of memory locations on any memory channel of the system memory.
7. The integrated circuit of claim 6, wherein:
the OCI interface includes two or more addressing modes; and
the interface controller is operable to generate control signals that allow selection between the two or more addressing modes of the interface controller.
8. The integrated circuit of claim 7, wherein:
a first addressing mode of the two or more addressing modes is a channel interleaving mode that limits mapping of an embedded table relative to a memory channel of the system memory.
9. The integrated circuit of claim 8, wherein:
a second addressing mode of the two or more addressing modes is a stack interleaving mode that expands the mapping of the embedded table relative to a memory channel of the system memory.
10. The integrated circuit of claim 7, wherein each OCI transaction request is:
The acquisition unit receives; and
processed by the interface controller to initiate access to any memory location indicated in the OCI transaction request.
11. The integrated circuit of claim 10, wherein each OCI transaction request to access data stored in the system memory encodes:
i) A 32 byte length corresponding to the data being accessed;
ii) a token ID representing a source address specific to any memory location in system memory; and
iii) The destination address of the memory location of the vector memory.
12. The integrated circuit of claim 8, wherein:
splitting the embedded table into a plurality of data slices, the plurality of data slices being allocated into memory channels of an overall system memory; and
the interface controller selects the stack interleave pattern using any of the plurality of channel controllers to access any portion of the embedded table.
13. The integrated circuit of claim 1, further comprising:
a respective refresh unit in each of the plurality of channel controllers, the respective refresh unit configured to:
i) Receiving a request for encoding addressing information;
ii) writing data from a source address in the vector memory to any memory location of the system memory using the interface controller based on the addressing information.
14. The integrated circuit of claim 13, wherein:
performing the machine learning calculation on the neural network input through a neural network layer; and
the refresh unit is used to write parameters of the neural network layer to any memory location of the system memory using the interface controller.
15. A computer-implemented method performed using an integrated circuit configured to accelerate machine learning computations, the integrated circuit comprising a system memory and a processor core, the processor core comprising a plurality of channel controllers, the method comprising:
receiving a plurality of requests, wherein each of the plurality of requests encodes addressing information;
for a first request of the plurality of requests:
identifying any memory location of any memory channel of the system memory based on the first requested addressing information;
retrieving data from a memory location in any memory location of the system memory using an on-chip interconnect (OCI) interface that couples each of the plurality of channel controllers to each memory channel of the system memory; and
The data retrieved from the memory location is written to a vector memory of the processor core based on the addressing information of the first request.
16. The method according to claim 15, wherein:
the OCI interface comprises an in-memory OCI node, and the in-memory OCI node comprises a plurality of read interfaces and a plurality of write interfaces; and
the method further comprises the steps of:
acquiring data from any memory location on any memory channel of the system memory using a plurality of read interfaces based on control signals generated by an interface controller; and
based on control signals generated by the interface controller, data is written to any memory location on any memory channel of the system memory using the plurality of write interfaces.
17. The method of claim 16, wherein the in-memory OCI node comprises a first-in-first-out (FIFO) memory queue, and the method comprises:
during arbitration of existing OCI transactions, incoming OCI transaction requests are stored in the FIFO memory queue for: i) Retrieving data from any memory location of the system memory, or ii) writing data to any memory location of the system memory.
18. The method as recited in claim 17, further comprising:
the interface controller receiving the request to encode the addressing information;
the interface controller generating a plurality of control signals based on a plurality of IDs in the addressing information; and
the interface controller provides the plurality of control signals to any memory channel of the system memory to extract data stored in memory locations of the system memory.
19. The method of claim 18, wherein:
the addressing information is derived from one or more incoming OCI transaction requests received at the interface controller; and
at least one existing OCI transaction request to access any memory location of the system memory is previously stored in the FIFO memory queue.
20. The method of claim 19, wherein:
each incoming OCI transaction request includes a plurality of Identifiers (IDs) corresponding to any set of memory locations on any memory channel of the system memory.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163167593P | 2021-03-29 | 2021-03-29 | |
US63/167,593 | 2021-03-29 | ||
PCT/US2022/022401 WO2022212415A1 (en) | 2021-03-29 | 2022-03-29 | On-chip interconnect for memory channel controllers |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117015767A true CN117015767A (en) | 2023-11-07 |
Family
ID=81581256
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280006948.0A Pending CN117015767A (en) | 2021-03-29 | 2022-03-29 | On-chip interconnect for memory channel controllers |
Country Status (7)
Country | Link |
---|---|
US (1) | US20220309011A1 (en) |
EP (1) | EP4315089A1 (en) |
JP (1) | JP2024512843A (en) |
KR (1) | KR20230062651A (en) |
CN (1) | CN117015767A (en) |
TW (1) | TW202238368A (en) |
WO (1) | WO2022212415A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
TWI826216B (en) * | 2022-12-29 | 2023-12-11 | 瑞昱半導體股份有限公司 | Memory control system and memory control method |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6751698B1 (en) * | 1999-09-29 | 2004-06-15 | Silicon Graphics, Inc. | Multiprocessor node controller circuit and method |
WO2008040028A2 (en) * | 2006-09-28 | 2008-04-03 | Virident Systems, Inc. | Systems, methods, and apparatus with programmable memory control for heterogeneous main memory |
CN106575279B (en) * | 2014-05-29 | 2019-07-26 | 阿尔特拉公司 | Accelerator architecture on programmable platform |
US20210109577A1 (en) * | 2020-12-22 | 2021-04-15 | Intel Corporation | Temperature-based runtime variability in victim address selection for probabilistic schemes for row hammer |
-
2022
- 2022-03-29 JP JP2023523148A patent/JP2024512843A/en active Pending
- 2022-03-29 WO PCT/US2022/022401 patent/WO2022212415A1/en active Application Filing
- 2022-03-29 US US17/707,849 patent/US20220309011A1/en active Pending
- 2022-03-29 CN CN202280006948.0A patent/CN117015767A/en active Pending
- 2022-03-29 KR KR1020237012453A patent/KR20230062651A/en unknown
- 2022-03-29 TW TW111111974A patent/TW202238368A/en unknown
- 2022-03-29 EP EP22721515.9A patent/EP4315089A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP2024512843A (en) | 2024-03-21 |
WO2022212415A9 (en) | 2023-08-10 |
KR20230062651A (en) | 2023-05-09 |
WO2022212415A1 (en) | 2022-10-06 |
TW202238368A (en) | 2022-10-01 |
US20220309011A1 (en) | 2022-09-29 |
EP4315089A1 (en) | 2024-02-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP3729280B1 (en) | Dynamic per-bank and all-bank refresh | |
US11222258B2 (en) | Load balancing for memory channel controllers | |
US10152434B2 (en) | Efficient arbitration for memory accesses | |
EP3732578B1 (en) | Supporting responses for memory types with non-uniform latencies on same channel | |
US9420036B2 (en) | Data-intensive computer architecture | |
JP2016532933A (en) | Data movement and timing controlled by memory | |
US8166246B2 (en) | Chaining multiple smaller store queue entries for more efficient store queue usage | |
EP4184324A1 (en) | Efficient accelerator offload in multi-accelerator framework | |
US20110320722A1 (en) | Management of multipurpose command queues in a multilevel cache hierarchy | |
US11410032B2 (en) | Word2VEC processing system | |
CN117015767A (en) | On-chip interconnect for memory channel controllers | |
US20200117505A1 (en) | Memory processor-based multiprocessing architecture and operation method thereof | |
CN116382599B (en) | Distributed cluster-oriented task execution method, device, medium and equipment | |
CN103218259A (en) | Computer-implemented method for selection of a processor, which is incorporated in multiple processors to receive work, which relates to an arithmetic problem | |
CN108062279A (en) | For handling the method and apparatus of data | |
KR100328726B1 (en) | Memory access system and method thereof | |
US8589633B2 (en) | Control apparatus | |
CN112088368B (en) | Dynamic per bank and full bank refresh | |
US20220365725A1 (en) | Data structure engine | |
CN118012788A (en) | Data processor, data processing method, electronic device, and storage medium | |
CN118051470A (en) | Method for operating a computing device and storage device | |
CN116627887A (en) | Method and chip for processing graph data | |
CN113448897A (en) | Array structure and optimization method suitable for pure user mode remote direct memory access |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |