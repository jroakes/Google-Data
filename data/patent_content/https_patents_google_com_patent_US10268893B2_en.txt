US10268893B2 - System and method for automatic detection of spherical video content - Google Patents
System and method for automatic detection of spherical video content Download PDFInfo
- Publication number
- US10268893B2 US10268893B2 US15/680,594 US201715680594A US10268893B2 US 10268893 B2 US10268893 B2 US 10268893B2 US 201715680594 A US201715680594 A US 201715680594A US 10268893 B2 US10268893 B2 US 10268893B2
- Authority
- US
- United States
- Prior art keywords
- video
- candidate image
- image frames
- score
- content
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/35—Categorising the entire scene, e.g. birthday party or wedding scene
-
- G06K9/00718—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7847—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using low-level visual features of the video content
-
- G06F17/3079—
-
- G06F17/30799—
-
- G06K9/00684—
-
- G06K9/00744—
-
- G06K9/224—
-
- G06K9/4604—
-
- G06K9/4633—
-
- G06K9/52—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/13—Edge detection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/10—Segmentation; Edge detection
- G06T7/136—Segmentation; Edge detection involving thresholding
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/40—Analysis of texture
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/42—Global feature extraction by analysis of the whole pattern, e.g. using frequency domain transformations or autocorrelation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/48—Extraction of image or video features by mapping characteristic values of the pattern into a parameter space, e.g. Hough transformation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/22—Character recognition characterised by the type of writing
- G06V30/228—Character recognition characterised by the type of writing of three-dimensional handwriting, e.g. writing in the air
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/139—Format conversion, e.g. of frame-rate or size
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/698—Control of cameras or camera modules for achieving an enlarged field of view, e.g. panoramic image capture
-
- H04N5/23238—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/08—Indexing scheme for image data processing or generation, in general involving all processing steps from image acquisition to 3D model generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
Definitions
- This description generally relates to methods and devices for capturing and processing spherical image content.
- Spherical video sharing platforms can allow users to upload and share captured spherical image content.
- Spherical image content can be captured using a number of cameras or camera rigs configured to capture all rays directed outward from a single point. The rays may be used to generate three-dimensional spherical panoramas of scenes.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- One general aspect includes a computer-implemented method that includes receiving, at a video server, video content, extracting a plurality of image frames from the video content, classifying, in a first stage, the plurality of image frames according to a first set of features, the classifying including identifying a portion of the plurality of image frames as candidate image frames, classifying, in a second stage, the candidate image frames according to a second set of features, the classifying including assigning a frame score to each of the candidate image frames, the frame score being based on at least one of the second set of features, selecting a portion of the candidate image frames having a frame score satisfying a threshold frame score condition.
- the method also includes generating a video score for the video content by aggregating together the portion of the candidate image frames having a frame score satisfying the threshold frame score condition, and identifying the video content as spherical video content in response to determining that the video score satisfies a threshold video score.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations may include one or more of the following features.
- the method further including monitoring for additional video content, at the video server and in response to detecting the additional video content, automatically performing the first stage and the second stage to classify the additional video content and identifying the additional video content as spherical video content or non-spherical video content based at least in part on an output of the first stage and the second stage.
- the method in which the first stage and the second stage are configured using a plurality of training videos that model spherical and non-spherical video content according to the first set of features and the second set of features.
- the method in which the first stage includes analyzing at least two edges of each of the plurality of image frames.
- the method in which the second stage includes analyzing central regions of the candidate image frames.
- the method in which the second stage includes performing at least one of generating a grayscale version of the candidate image frames and calculating a standard deviation of the grayscale version of the candidate image frames, calculating entropy of the grayscale version of the candidate image frames, and calculating entropy of an edge detected version of the candidate image frames.
- the method can also include using a first set of features that include at least two calculations corresponding to the plurality of image frames, the calculations including an aspect ratio, video dimension, standard deviation at both image poles, and left and right border variances.
- the second set of features can include at least two calculations corresponding to the candidate image frames, the calculations including a standard deviation, an edge detection, an image entropy, an image entropy of the edge detection, a Hough transform, a measure of texture for at least one pole defined in at least one image frame, and a metric corresponding to an edge of at least one image frame and a column of pixels adjacent to the edge.
- the method can include using the second set of features that are defined and calculated according to at least one image projection type selected from the group including of an equirectangular projection, a warped equirectangular projection, a rectilinear projection, a circular projection, and a stereographic projection.
- Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.
- the system may include a video server configured to receive, from a plurality of users, uploaded video content, the uploaded video content including a plurality of images, an extractor module configured to extract a plurality of image frames from the video content, a first classifier module configured to classify the plurality of image frames according to a first set of features being associated with the plurality of image frames and identify a portion of the plurality of image frames as candidate image frames, and a second classifier module configured to classify the candidate image frames according to a second set of features being associated with the candidate image frames.
- the system may also include a scoring module configured to generate a plurality of frame scores corresponding to the plurality of image frames and a plurality of video scores corresponding to the video content, the scoring module generating a categorization marker applicable to the video content, the categorization marker indicating spherical video content or non-spherical video content and being based on the plurality of frames scores and the plurality of video scores.
- a scoring module configured to generate a plurality of frame scores corresponding to the plurality of image frames and a plurality of video scores corresponding to the video content, the scoring module generating a categorization marker applicable to the video content, the categorization marker indicating spherical video content or non-spherical video content and being based on the plurality of frames scores and the plurality of video scores.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations may include one or more of the following features.
- the video management system may also include an image analyzer configured to analyze image characteristics in the plurality of image frames, the image analyzer adapted to perform calculations based on the first set of features and on the second set of features.
- the video management system may include and use the first set of features includes at least two calculations corresponding to the plurality of image frames, the calculations including an aspect ratio, video dimension, standard deviation at both image poles, and left and right border variances.
- the video management system may include and use the second set of features includes at least two calculations corresponding to the candidate image frames, the calculations including a standard deviation, an edge detection, an image entropy, an image entropy of the edge detection, a Hough transform, a measure of texture for at least one pole defined in at least one image frame, and a metric corresponding to an edge of at least one image frame and a column of pixels adjacent to the edge.
- the video management system may be configured for using the second classifier module to perform at least one of generating a grayscale version of the candidate image frames and calculating a standard deviation of the grayscale version of the candidate image frames, calculating entropy of the grayscale version of the candidate image frames, and/or calculating entropy of an edge detected version of the candidate image frames.
- classification criteria is generated for the first classifier module and the second classifier module using a plurality of training videos that model spherical and non-spherical video content according to the first set of features and the second set of features.
- a non-transitory recordable storage medium having recorded and stored thereon instructions.
- the instructions when executed, perform actions such as receiving, at a video server, video content, extracting a plurality of image frames from the video content, and classifying, during a first stage, the plurality of image frames according to a first set of features.
- the classifying may include identifying a candidate image frame from the plurality of image frames.
- the instructions may further include assigning, during a second stage, a frame score to the candidate image based on at least one of a second set of features, selecting the candidate image frame when the frame score satisfies a threshold frame score condition, generating a video score for the video content based on the candidate image frame, and defining the video content as spherical video content in response to determining that the video score satisfies a threshold video score.
- Example implementations may include one or more of the following features.
- the instructions may also include monitoring for additional video content, at the video server, and in response to detecting the additional video content, automatically performing the stage and the second stage to classify the additional video content.
- the first set of features may include at least two calculations corresponding to the plurality of image frames, the calculations including an aspect ratio, video dimension, standard deviation at both image poles, and left and right border variances.
- the second set of features may include at least two calculations corresponding to the candidate image frames, the calculations including a standard deviation, an edge detection, an image entropy, an image entropy of the edge detection, a Hough transform, a measure of texture for at least one pole defined in at least one image frame, and a metric corresponding to an edge of at least one image frame and a column of pixels adjacent to the edge.
- the second set of features are defined and calculated according to at least one image projection type selected from the group consisting of an equirectangular projection, a warped equirectangular projection, a rectilinear projection, a circular projection, and a stereographic projection.
- FIG. 1 is a block diagram of an example system for detecting, processing, and rendering spherical image content.
- FIG. 2 is a diagram depicting an example of spherical video represented in 2D parameterization.
- FIG. 3 is a diagram depicting an example image frame classification process.
- FIG. 4 is a diagram depicting an example of a ratio of a Hough transform of square regions and unwarped images.
- FIG. 5 is an example of classification results achieved using the classification processes described herein.
- FIG. 6 is a flow chart diagramming one embodiment of a process to classify video content.
- FIG. 7 shows an example of a computer device and a mobile computer device that can be used to implement the techniques described herein.
- Spherical video content e.g., 360-video
- Spherical video content may include a variety of images and data that can be presented to users to provide interactive viewing experiences.
- spherical video content may be used to offer three-dimensional, 360 degrees of video depicting sporting events, cultural preservation, and virtual presence content, just to name a few examples.
- Properly presenting such spherical video content on a client device may depend on configuration details of a media player performing playback of the content as well as detected formatting of the video content.
- the systems and methods described in this disclosure provide a way to automatically detect whether particular image content includes spherical video content or features.
- the systems and methods described herein can be configured to playback the video content in a 3D form with spherical aspects.
- users of systems described herein can upload video content to a video server system.
- the video server system can automatically analyze image frames within the video content to detect image features that signify spherically captured video content.
- the detection process can provide resilience to errors in explicit video container level signaling and may function to combat intentional erroneous container level signaling, such as when a user attempts to bypass copyright detection by suggesting that particular video content is formatted in a different representation other than spherical video.
- An example view of spherical video content may be represented as an unwrapped version of the globe, and can be parameterized on a two-dimensional (2D) plane in which the poles of the globe can be mapped to the top and bottom of the content, respectively. Such a representation may be captured using equirectangular projection.
- the systems and methods described in this disclosure can use a set of labels to indicate whether spherical content is present in particular video content. For example, the systems and methods can be used to apply a positive label to indicate spherical content. Similarly, the systems and methods can be used to apply a negative label to indicate non-spherical content.
- the labels can be applied to sample video content and such sample video content and attached labels can be used to configure (e.g., train, teach, instruct) a two-level cascade of machine learning detectors to classify unseen videos.
- the labels can be used by media players to detect and configure playback of spherical content in response to detecting spherically captured content is available within a particular video.
- the two-level cascade can be used to apply the labels to indicate presences of spherical or non-spherical content in videos uploaded to system 100 , for example.
- the two-level cascade may include two stages, both of which can include a number of analyzing and scoring steps.
- a first stage the systems and methods described herein can perform a number of low cost computations to compute features (e.g., such as aspect ratio, video dimension, standard deviation at the image poles, and the left-right border distances, etc.).
- the first stage may be used to reduce (e.g., prune, trim) a large number of videos that are readily classified as non-spherical.
- a second classification stage may also be performed to find elaborate features on the internal areas within a set of image frames.
- the features in the second stage can include image standard deviation, whether straight lines are straight (using a Hough transform), relative left to right border ratios, and stronger methods to compute whether the poles (top and bottom) of the image are actually spherical.
- the second classification stage can then use such features to eliminate non-spherical videos that may have passed through the first classification stage undetected.
- the systems and methods described herein can be used to passively monitor uploaded video content and to identify the video content in which users have uploaded spherical content, but have inadvertently provided the content with missing spherical metadata or labeling. In some implementations, the systems and methods can also detect whether video content is mismarked as including spherical content or provided with unrecognizable projection formats.
- the systems and methods described herein can provide advantages, including, but not limited to using a two-level cascaded classification system to reject a large number of uploaded videos, without investing costly computational efforts for all uploaded videos received at a video content server.
- an additional advantage may include providing an automated/machine learned classification process that can automatically tune particular thresholds and feature weighting.
- the systems and methods described herein can provide a general video content detector that can be trained on specific training samples such that spherical content in equirectangular projection is provided by the video content server. Other content types and projections can be configured using such a detector.
- a block diagram is shown that includes an example system 100 for detecting, processing, and rendering spherical image content.
- a user may upload (e.g., using laptop 102 ) one or more videos (e.g., 103 a , 103 b , 103 c , 103 d , etc.) by selecting an application or website with an upload video content control 104 .
- the videos 103 a - d may include video content that can be provided over a network 105 , or alternatively, provided directly to a video server system 106 for analysis and processing.
- the video server system 106 can perform a number of calculations and processes on the image content to determine whether the video content is spherical or non-spherical and to determine which projection is associated with the video content. In some implementations, the video server system 106 can also provide the video content to other users (not shown) or to an HMD device 108 for rendering, storage, or further processing.
- the HMD device 108 may represent a virtual reality headset, glasses, eyepiece, or other wearable device capable of displaying virtual reality content.
- the HMD device 108 can execute a VR application (not shown) which can playback received and/or processed images to a user.
- the VR application can be hosted by one or more of the devices 102 or 106 , shown in FIG. 1 .
- the video server system 106 may be configured to receive, from a number of users, uploaded video content.
- the uploaded video content may or may not include spherically captured image content.
- the uploaded video content includes still image frames that make up video content.
- the example video server system 106 includes an extractor module 110 , an image analyzer 112 , a first classifier module 114 , a second classifier module 116 , and a scoring module 118 . As shown in FIG. 1 , the video server system 106 also includes a spherical videos repository 120 and a non-spherical videos repository 122 to store video content upon receiving and categorizing such content.
- the extractor module 110 may be configured to extract a number of image frames from the video content. For example, a sequence of images with particular heights, widths, and color channels can be extracted for analysis. Extracted images can be used to classify training videos to enable recognition of format of any incoming video received at the video server system 106 .
- the image analyzer 112 may be configured to analyze image characteristics in the image frames of the video content uploaded to the video server system 106 .
- the image analyzer 112 may be adapted to perform calculations based on a first set of features and on a second set of features that can be ascertained from the video content and image frames making up the video content.
- the first and second sets of features may be associated with a number of observations and/or calculations performed on the image frames.
- the calculations may include ascertaining an aspect ratio, video dimensions, standard deviations at image poles, and left and right border variances of the image frames.
- Additional calculations may include obtaining a standard deviation, an edge detection, an image entropy, an image entropy of the edge detection, a Hough transform, a measure of texture for at least one pole defined in at least one image frame, and a metric corresponding to an edge of at least one image frame and a column of pixels adjacent to the edge. Additional information regarding such features is described in detail below.
- the first classifier module 114 may be configured to classify the image frames according to a first set of features and to define a portion of the image frames as candidate image frames.
- the system 100 can receive an uploaded video and can begin to analyze image frames in the video.
- the analysis may include determining an aspect ratio associated with the video/image frames.
- the content includes 360 degrees of image views around the equator and 180 degrees of image views up and down. Accordingly, the aspect ratio of typical spherical content may be about 2:1, indicating two units wide and one unit high.
- the first classifier module 114 can determine that the particular video content with the aspect ratio of 2:1 may be spherical and can perform additional analysis in the first stage or can pass the particular video content to the next classifier stage for analysis in a second stage.
- the additional analysis in the first stage may include determining a variance throughout the video content.
- the first classifier module 114 can analyze image intensities and determine a variance across the intensities.
- the classifier module 114 can compare a left most column of pixels from an image frame and a right most column of pixels from the image frame. Since typical spherical video generates a sphere of content that can be presented flat in 2D when unwarped, the left and right columns of pixels will likely match closely for variance from top to bottom between the two columns.
- the variance can be used in combinations with a number of other features to determine whether particular videos are spherical.
- the system 100 can use the variance level between pixels/columns as one of many inputs and can decide how important the variance features may be (compared to other image features) and can do so automatically through machine learning to achieve a highly ranked result according to a particular training set.
- the classifier modules 114 and 116 can automatically tune particular feature weights by analyzing statistics of one or more of the features across positive (e.g., spherical) training examples and negative (non-spherical) training examples.
- the second classifier module 116 may be configured to classify the candidate image frames determined as possible spherical content by the first classifier module 114 in the first stage. For example, the second classifier module 116 can classify the candidate image frames according to one or more other features described above. In addition, the second classifier module 116 can assign a frame score for each candidate image frame. The frame score may pertain to a likelihood of spherical content being present within the image frame. That is, if calculations pertaining to particular features indicate spherical content, the second classifier module 116 can assign a score indicating high likelihood of spherical content.
- the second classifier module 116 may be configured to generate a grayscale version of the candidate image frames and calculate a standard deviation of the grayscale version of the candidate image frames.
- the second classifier module 116 may be configured to calculate an entropy of the grayscale version of the candidate image frames or to calculate an entropy of an edge detected version of the candidate image frames.
- Entropy i.e., image entropy
- Image entropy can be calculated using compression algorithms to compress images and determine the size of such compressed images. The size can be compared to typical sizes of compressed images generated for spherical video content.
- classification criteria is generated for the first classifier module and the second classifier module using training videos configured to model spherical and non-spherical video content.
- the training videos may include metadata or additional data that describes one or more features associated with the training video. For example, any of the above described features can be associated with multiple measurements that can indicate whether particular video content is spherical video content to non-spherical video content.
- the scoring module 118 may be configured to generate frame scores corresponding to a number of image frames that included spherical video features. In addition, the scoring module 118 may be configured to generate a number of video scores corresponding to an overall likelihood of spherical video features detected in the video content for a particular video.
- the scoring module 118 can generate a categorization marker applicable to the video content. For example, the scoring module 118 can take output from the first classifier module 114 and the second classifier module 116 and aggregate one or more scores associated with particular video content in order to determine and apply a categorization marker.
- the categorization marker can be used to indicate whether the content is spherical video content or non-spherical video content.
- a user operating laptop 102 can upload video content using upload video content control 104 .
- the video server system 106 can extract image frames from the uploaded video content using extractor module 110 .
- the image analyzer 112 and the classifier modules 114 and 116 can analyze and classify the extracted image frames (and/or associated metadata) to determine whether image features, associated with the image frames, signify spherically captured video content. Scoring can be applied using scoring module 118 . Image frames that meet a threshold scoring level can be classified and labeled as including spherical content. Image frames that do not meet the threshold scoring level may be classified as including non-spherical content.
- the video server system 106 may generate one or more messages to indicate, to the user operating laptop 102 , that the uploaded content is spherical or non-spherical. For example, the system 106 can generate a message 124 indicating that the uploaded content is not spherical video content. Other notifications are possible.
- FIG. 2 is a diagram 200 depicting an example of spherical video 202 represented in 2D parameterization 204 .
- spherical video 202 depicts content in all viewable directions from a single point.
- the spherical video 202 is represented in a 2D planar parameterization 204 .
- the 2D planar parameterization (e.g., projection) 204 is equirectangular.
- users can interactively select a viewpoint to have the video content played back from the selected point.
- the media player can unwarp the content and provide the content during playback as shown at unwarped image 206 .
- FIG. 3 is a diagram depicting an example image frame classification process.
- selected frames 300 e.g., I 1 , I 2 , I t , I t+I
- classifiers 302 and 306 may correspond to classifier modules 114 and 116 ( FIG. 1 ), respectively.
- Aggregator 304 may correspond to scoring module 118 .
- the system 100 can perform a classification process that is based on the pooling of a subset of image frames from the video (shown by image frames I 1 , I 2 , I t , I t+1 ).
- a two-level cascaded classification process may be applied and particular classification labels and scores 304 may be fed as features into the separate video classification module 306 .
- I may be removed, as these features may be extracted from each selected image.
- V i where i goes from 1 to H, and corresponding class labels, y i ⁇ 1, +1 ⁇ , where ⁇ 1 represents a non-spherical label and +1 represents a spherical label.
- the system 100 can be configured to extract a set of frames and corresponding class labels from a set of training videos.
- the classification process can include minimizing an energy (e.g., F energy ) over a number of classifier parameters ⁇ D ⁇ and at least one threshold ⁇ , which may allow a trade-off of the true-positive rate to the false-positive rate using the training set.
- an Ada-Boost classifier may be used.
- the first stage may be trained by minimizing a classification energy function, as shown by equation (1) below.
- D 1 , ⁇ 1 argmin D, ⁇ F energy ( D , ⁇ ( y i ,f i l1 ) ⁇ ) (1) where i goes from 1 to N and the features, f l1 , belong to a set of features F quick that can be efficiently extracted.
- the class label for an unseen instance may then be obtained by evaluating the classifier on a corresponding set of features, as shown by equation (2) below.
- y l1 C 1 ( f l1 ;D 1 , ⁇ 1 ) (2)
- the parameter ⁇ 1 may be chosen such that the first stage retains high recall. Any false positive image frames at the first stage can be handled by the next stage.
- D 2 , ⁇ 2 argmin D, ⁇ F energy ( D , ⁇ , ⁇ ( y i ,f i l2 ) ⁇ i ⁇ S ) (3)
- the features, f i l2 may be derived from a more computationally expensive process that also inspects the interior of each particular image frame.
- the two-level classification of an unseen image can then obtained as:
- the classification results of the previously mentioned classifier e.g., the first classification stage
- a subset of video frames e.g., every nth frame
- y ik be the class label of the classification of the k-th frame of the i-th video
- a vector of all individual classification results is composed, as shown by equation (5) below.
- f v ( C i1 ,C i2 , . . . ,C iK ,y i1 ,y i2 , . . . ,y iK ) (5)
- a final video classification can be made by training a classifier on features derived from these individual classification results (e.g., non-linear combinations of the feature elements can be obtained to generate a larger class of features).
- an assumption can be made to use the features from equation (5) to train a classifier according to equation (6) below.
- D v , ⁇ v argmin D, ⁇ F energy ( D, ⁇ ; ⁇ y i ,f v ⁇ i ) (6)
- a machine learned classifier can be trained on a set of efficient features, f l1 ⁇ F quick , with positive cases being refined on a set of features, f l2 ⁇ F, where F is a union of F quick and F interior , that includes a class of more computationally expensive features, F interior , that analyze the frame contents.
- the features described herein may focus on an equirectangular projection, however, this should not be limiting, as similar features can be defined for other projection types, or other projection types can be warped to the equirectangular domain.
- other project types used with the features described here in may include a rectilinear projection, a circular projection, a Mercator projection, and/or a stereographic projection.
- the features, for equirectangular images may include an aspect ratio of the image frames.
- ⁇ , where the tolerance ⁇ 0:02 (8)
- the left and right edges of the parameterized sphere belong to a single seam on the edge of the sphere and should have similar values.
- f border _ diff max g L2 ( I, 1 ,n,c ), where c ⁇ C (13)
- border _ var min(max_std( ⁇ I ( i, 1, c ) ⁇ ),max_std( ⁇ I ( i,n,c ) ⁇ )) (14) where i goes from 1 to m and, where c ⁇ C
- f top , f bot , f border _ diff , and f border _ var represent features that may be used in the first stage of classification.
- R k 400 is shown with square regions (R 1 ) 402 a , (R 2 ) 404 a , (R 3 ) 406 a , and (R 4 ) 408 a .
- unwarped images for each compass direction, S k , 401 are shown at (S 1 ) 402 b , (S 2 ) 404 b , (S 3 ) 406 b , and (S 4 ) 408 b .
- the ratio of a Hough transform of the square regions 402 a - 408 a to the unwarped images 402 b - 408 b can be calculated.
- lines in the image frames are straight in image frames S k 401 .
- the second stage of classification can be configured to perform in depth analysis of the image frames. Successfully passing in the first stage may indicate that a particular set of image frames was determined to include spherical content within the frames. Since the second stage of the cascaded classifier (e.g., classifier module 116 ) may operate on image frames in which the first stage (e.g., classifier module 114 ) has succeeded, the system 100 can be configured to invest additional computation time for feature extraction for the second stage than the system 100 allotted in the first stage. Further, a number of features analyzed in the first stage may not have been configured to examine the central regions of one or more image frames.
- the second stage of classification can be configured to perform in depth analysis of the image frames. Successfully passing in the first stage may indicate that a particular set of image frames was determined to include spherical content within the frames. Since the second stage of the cascaded classifier (e.g., classifier module 116 ) may operate on image frames in which the first stage (e.g., classifier module
- the second stage can examine complex constraints at the boundaries of the image frames and within the image frames. For example, calculating a standard deviation for content within an image frame may be a useful feature to analyze to avoid computer generated images.
- a simple letterbox frame with black borders on top and bottom of a white rectangle in the center may include borders that obey the particular features (e.g., low variance at top/bottom of image and left and right edges that agree), but an interior variance may be low and thus, the image may be unlikely to be from a 360 spherical panorama video.
- Equation (15) shows an example equation for the standard deviation calculation, where i goes from 1 to m images, where j goes from 1 to n images, and where c ⁇ C.
- f sd max_ var ( ⁇ I ( i,j,c ) ⁇ )
- a similar feature can be defined (f sd _ hp ) to represent the standard deviation of a high-pass filtered (Sobel edge detector) version of a grayscale version of the image.
- the system 100 can also compute f entropy as the entropy of this grayscale image, and f entropy _ hp as the entropy of the edge detected image, which both may serve a similar purpose.
- straight lines can project to straight lines.
- lines are often curved. For real equirectangular images, more straight lines are expected after the media player has unwarped a particular image.
- a relative feature that extracts four sub-regions from each frame can be performed. These sub-regions may correspond to the 90-degree field of view images along the North, South, East, and West directions.
- the following experiments were performed with a total of 138 spherical videos and 612 non-spherical videos. In each video, the experiment sampled multiple frames for a total of 2904 positive image frames (i.e., positive for spherical content) and 2156 negative image frames.
- the first stage was trained using an initially gathered subset.
- the files used to test the second stage were obtained after running the first stage on unseen data and keeping the cases that the first stage had marked as positives. Results for testing on the training data are shown below in Table 1. Note that the first stage was trained on an initial subset of data, and the second stage is trained on positives from the first level, as well as an augmented set of training data (e.g., false positives) from executing the first stage on initially unseen data.
- Stage1 Stage2 Combined accuracy 0.9576 0.8549 0.81343 precision 1 0.9717 0.98418 TPR 0.8351 0.7696 0.68595 FPR 0.0 0.0301 0.01484
- the experiment used an additional 4249 negative videos and 52 positive examples for testing.
- the thresholds were tuned for a low false positive rate in training, with which a precision of 0.7169 and a recall of 0.7307 were achieved, as shown by Table 2 below.
- g left max g L2 ( I, 1,2, c ), where c ⁇ C (18)
- g right max g L2 ( I,n,n ⁇ 1, c ), where c ⁇ C (19)
- equation (20) can be used.
- ROC receiver-operating characteristic
- using an Ada-Boost classifier may show that a classifier score is proportional to a linear combination of weak binary classifiers that are based on the individual features. If a threshold is used on the classifier response at the absolute lowest response level, then all values can be taken as spherical and the classification system provides 100% true positive rates and 100% true false positive rates. As shown in FIG. 5 , a linear portion 502 of the graph 500 on the right hand side may be due to a number of positive and negative test samples sharing the next lowest value (including those that were rejected by the first layer).
- the remaining variations in the graph may be due to the Ada-Boost classifier outputting a confidence score that, while continuous, may only take on a discrete set of values (pow(2, num_features)).
- the lower recall may be due to many of the spherical videos having long introduction or title screens with no identifiable spherical content that took up tens of seconds at the beginning of the videos. Such videos could be properly classified if the experiment had been using frames from a longer segment of the video.
- the remaining false negatives may have been due to variability beyond what was seen in the training data, for example, they were stitched with different camera rigs with large black regions at the bottom, they were computer graphic generated images with content in regions of the frame that didn't lead to spherical distortion, or they were captured in evenings with much darker image statistics.
- the recall of the detector could be improved with a second round of training that includes these types of variation.
- FIG. 6 is a flow chart diagramming one embodiment of a process 600 to classify video content.
- the process 600 may be carried out, in one example, by devices and logic shown in FIG. 1 .
- the process 600 may include receiving, at a video server, video content.
- the video content may be uploaded to a video server system 106 by a user.
- the user may upload video content to a file sharing video website.
- the video server system 106 can perform a number of operations to analyze the uploaded video content.
- the process 600 may include extracting a plurality of image frames from the video content.
- the extractor module 110 can extract particular image frames form the uploaded video content.
- the extractor module 110 can extract video frames from the first ten seconds of video content.
- the extractor module 110 can extract other portions of image frames of the video content (randomly or sequentially).
- the process 600 may include classifying, in a first stage, the plurality of image frames according to a first set of features.
- the classifying may be performed by the classifier module 114 .
- the classifying may include defining or identifying a portion of the plurality of image frames as candidate image frames. For example, the portion of image frames may be selected based on feature analysis.
- the feature analysis may include calculating or reviewing one or more of the first set of features.
- the first set of features may include at least two calculations pertaining to an aspect ratio for the image frames, a video dimension for the image frames, a standard deviation at both image poles for the image frames, and/or a left and right border variance for the image frames.
- the classifying in the first stage may include analyzing at least two edges of each of the plurality of image frames.
- the process 600 may include classifying, in a second stage, the candidate image frames according to a second set of features.
- the classifying may be performed by the classifier module 116 .
- the classifying may include assigning a frame score to each of the candidate image frames.
- the frame score may be based on at least one of the second set of features.
- the second set of features may includes at least two calculations corresponding to the candidate image frames and the calculations may include calculating a standard deviation for particular image frames, calculating an edge detection for particular image frames, calculating an image entropy or an image entropy of the edge detection for particular image frames, calculating a Hough transform, a measure of texture for at least one pole defined in at least one image frame, and/or a metric corresponding to an edge of at least one image frame and a column of pixels adjacent to the edge.
- the classifying performed in the second stage includes analyzing central regions of the candidate image frames.
- the second stage of classification may include performing at least one of generating a grayscale version of the candidate image frames and calculating a standard deviation of the grayscale version of the candidate image frames, calculating entropy of the grayscale version of the candidate image frames, and/or calculating entropy of an edge detected version of the candidate image frames.
- the process 600 may include selecting a portion of the candidate image frames having a frame score satisfying a threshold frame score condition.
- the selection may be performed by the video server system 106 using image analyzer 112 .
- the threshold frame score may be defined such that the false positive rate (FPR) of the two-level image classification is kept below a particular level.
- the thresholds may be chosen to maintain high true positive rate (of ideally 1), while reducing as many of the false positives as possible.
- the threshold may be chosen to keep false positives rate low at the expense of sacrificing some true positives.
- the process 600 may include generating a video score for the video content by aggregating together the portion of the candidate image frames having a frame score that satisfies the threshold frame score.
- satisfying the threshold frames score includes scoring above the threshold frame score.
- the scoring module 118 may be used to generate a video score for each video.
- the threshold video score may be defined such that the final video classification false positive rate is kept below a desired rate, for example, less than about 0.1%.
- the process may include identifying the video content as spherical video content in response to determining that the video score exceeds a threshold video score.
- the process 600 may also include monitoring for additional video content, at a video server.
- the video server system 106 can monitor uploaded video content to ensure proper classification and accessibility is provided for spherically captured content.
- the video server system 106 can automatically perform the first stage and the second stage classifications steps described herein in order to classify the uploaded video content.
- the first and second classification stages are configured using a plurality of training videos configured to model spherical and non-spherical video content according to the first set of features and the second set of features.
- the process 600 may include receiving video content at a video server and extracting image frames from the video content.
- the method may also include classifying, during a first stage, the image frames according to a first set of features.
- the classifying may include identifying a candidate image frame from the extracted image frames.
- the process 600 can also include assigning, during a second stage, a frame score to the candidate image based on at least one of the second set of features and selecting the candidate image frame when the frame score satisfies a threshold frame score condition.
- the process 600 may additionally include generating a video score for the video content based on the candidate image frame and defining the video content as spherical video content in response to determining that the video score satisfies a threshold video score.
- Computing device 700 includes a processor 702 , memory 704 , a storage device 706 , a high-speed interface 708 connecting to memory 704 and high-speed expansion ports 710 , and a low speed interface 712 connecting to low speed bus 714 and storage device 706 .
- Each of the components 702 , 704 , 706 , 708 , 710 , and 712 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 702 can process instructions for execution within the computing device 700 , including instructions stored in the memory 704 or on the storage device 706 to display graphical information for a GUI on an external input/output device, such as display 716 coupled to high speed interface 708 .
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 704 stores information within the computing device 700 .
- the memory 704 is a volatile memory unit or units.
- the memory 704 is a non-volatile memory unit or units.
- the memory 704 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 706 is capable of providing mass storage for the computing device 700 .
- the storage device 706 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 704 , the storage device 706 , or memory on processor 702 .
- the high speed controller 708 manages bandwidth-intensive operations for the computing device 700 , while the low speed controller 712 manages lower bandwidth-intensive operations.
- the high-speed controller 708 is coupled to memory 704 , display 716 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 710 , which may accept various expansion cards (not shown).
- low-speed controller 712 is coupled to storage device 706 and low-speed expansion port 714 .
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 700 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 720 , or multiple times in a group of such servers. It may also be implemented as part of a rack server system 724 . In addition, it may be implemented in a personal computer such as a laptop computer 722 . Alternatively, components from computing device 700 may be combined with other components in a mobile device (not shown), such as device 750 . Each of such devices may contain one or more of computing device 700 , 750 , and an entire system may be made up of multiple computing devices 700 , 750 communicating with each other.
- Computing device 750 includes a processor 752 , memory 764 , an input/output device such as a display 754 , a communication interface 766 , and a transceiver 768 , among other components.
- the device 750 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 750 , 752 , 764 , 754 , 766 , and 768 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 752 can execute instructions within the computing device 750 , including instructions stored in the memory 764 .
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 750 , such as control of user interfaces, applications run by device 750 , and wireless communication by device 750 .
- Processor 752 may communicate with a user through control interface 758 and display interface 756 coupled to a display 754 .
- the display 754 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 756 may comprise appropriate circuitry for driving the display 754 to present graphical and other information to a user.
- the control interface 758 may receive commands from a user and convert them for submission to the processor 752 .
- an external interface 762 may be provide in communication with processor 752 , so as to enable near area communication of device 750 with other devices. External interface 762 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 764 stores information within the computing device 750 .
- the memory 764 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 774 may also be provided and connected to device 750 through expansion interface 772 , which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 774 may provide extra storage space for device 750 , or may also store applications or other information for device 750 .
- expansion memory 774 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 774 may be provide as a security module for device 750 , and may be programmed with instructions that permit secure use of device 750 .
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 764 , expansion memory 774 , or memory on processor 752 , that may be received, for example, over transceiver 768 or external interface 762 .
- Device 750 may communicate wirelessly through communication interface 766 , which may include digital signal processing circuitry where necessary. Communication interface 766 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 768 . In addition, short-range communication may occur, such as using a Bluetooth, Wi-Fi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 770 may provide additional navigation- and location-related wireless data to device 750 , which may be used as appropriate by applications running on device 750 .
- GPS Global Positioning System
- Device 750 may also communicate audibly using audio codec 760 , which may receive spoken information from a user and convert it to usable digital information. Audio codec 760 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 750 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 750 .
- Audio codec 760 may receive spoken information from a user and convert it to usable digital information. Audio codec 760 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 750 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 750 .
- the computing device 750 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 780 . It may also be implemented as part of a smart phone 782 , personal digital assistant, or other similar mobile device.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- the computing devices depicted in FIG. 7 can include sensors that interface with a virtual reality (VR headset 790 ).
- VR headset 790 virtual reality
- one or more sensors included on a computing device 750 or other computing device depicted in FIG. 7 can provide input to VR headset 790 or in general, provide input to a VR space.
- the sensors can include, but are not limited to, a touchscreen, accelerometers, gyroscopes, pressure sensors, biometric sensors, temperature sensors, humidity sensors, and ambient light sensors.
- the computing device 750 can use the sensors to determine an absolute position and/or a detected rotation of the computing device in the VR space that can then be used as input to the VR space.
- the computing device 750 may be incorporated into the VR space as a virtual object, such as a controller, a laser pointer, a keyboard, a weapon, etc. Positioning of the computing device/virtual object by the user when incorporated into the VR space can allow the user to position the computing device to view the virtual object in certain manners in the VR space.
- one or more input devices included on, or connect to, the computing device 750 can be used as input to the VR space.
- the input devices can include, but are not limited to, a touchscreen, a keyboard, one or more buttons, a trackpad, a touchpad, a pointing device, a mouse, a trackball, a joystick, a camera, a microphone, earphones or buds with input functionality, a gaming controller, or other connectable input device.
- a user interacting with an input device included on the computing device 750 when the computing device is incorporated into the VR space can cause a particular action to occur in the VR space.
- a touchscreen of the computing device 750 can be rendered as a touchpad in VR space.
- a user can interact with the touchscreen of the computing device 750 .
- the interactions are rendered, in VR headset 790 for example, as movements on the rendered touchpad in the VR space.
- the rendered movements can control objects in the VR space.
- one or more output devices included on the computing device 750 can provide output and/or feedback to a user of the VR headset 790 in the VR space.
- the output and feedback can be visual, tactical, or audio.
- the output and/or feedback can include, but is not limited to, vibrations, turning on and off or blinking and/or flashing of one or more lights or strobes, sounding an alarm, playing a chime, playing a song, and playing of an audio file.
- the output devices can include, but are not limited to, vibration motors, vibration coils, piezoelectric devices, electrostatic devices, light emitting diodes (LEDs), strobes, and speakers.
- one or more input devices in addition to the computing device can be rendered in a computer-generated, 3D environment.
- the rendered input devices e.g., the rendered mouse, the rendered keyboard
- Computing device 700 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 750 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
Abstract
Description
D 1,τ1=argminD,τ F energy(D,τ{(y i ,f i l1)}) (1)
where i goes from 1 to N and the features, fl1, belong to a set of features Fquick that can be efficiently extracted. The class label for an unseen instance may then be obtained by evaluating the classifier on a corresponding set of features, as shown by equation (2) below.
y l1 =C 1(f l1 ;D 1,τ1) (2)
D 2,τ2=argminD,τ F energy(D,τ,{(y i ,f i l2)}i∈S) (3)
f v=(C i1 ,C i2 , . . . ,C iK ,y i1 ,y i2 , . . . ,y iK) (5)
D v,τv=argminD,τ F energy(D,τ;{y i ,f v}i) (6)
f aspect _ thresh=|α−2|≤τ, where the tolerance τ=0:02 (8)
f aspect=α (9)
f top=maxvar({I(1,j,c)}), where j goes from 1:n and c∈C (10)
f bot=maxvar({I(m,j,c)}), where j goes from 1:n and c∈C (11)
f border _ diff=max g L2(I,1,n,c), where c∈C (13)
f border _ var=min(max_std({I(i,1,c)}),max_std({I(i,n,c)})) (14)
where i goes from 1 to m and, where c∈C
f sd=max_var({I(i,j,c)}) (15)
TABLE 1 | ||||
Stage1 | Stage2 | Combined | ||
accuracy | 0.9576 | 0.8549 | 0.81343 | ||
|
1 | 0.9717 | 0.98418 | ||
TPR | 0.8351 | 0.7696 | 0.68595 | ||
FPR | 0.0 | 0.0301 | 0.01484 | ||
TABLE 2 | ||||
Level1 | Level2 | Combined | ||
accuracy | 0.9125 | 0.9083 | 0.9932 | ||
precision | 0.1086 | 0.0928 | 0.7169 | ||
TPR | 0.8653 | 0.75 | 0.7307 | ||
FPR | 0.0868 | 0.0897 | 0.0035 | ||
g left=max g L2(I,1,2,c), where c∈C (18)
g right=max g L2(I,n,n−1,c), where c∈C (19)
TABLE 3 | ||
Video | ||
accuracy | 0.9409 | ||
precision | 0.9947 | ||
TPR | 0.6051 | ||
FPR | 0.00037 | ||
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/680,594 US10268893B2 (en) | 2015-10-30 | 2017-08-18 | System and method for automatic detection of spherical video content |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/927,795 US9767363B2 (en) | 2015-10-30 | 2015-10-30 | System and method for automatic detection of spherical video content |
US15/680,594 US10268893B2 (en) | 2015-10-30 | 2017-08-18 | System and method for automatic detection of spherical video content |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/927,795 Continuation US9767363B2 (en) | 2015-10-30 | 2015-10-30 | System and method for automatic detection of spherical video content |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170344830A1 US20170344830A1 (en) | 2017-11-30 |
US10268893B2 true US10268893B2 (en) | 2019-04-23 |
Family
ID=57227139
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/927,795 Expired - Fee Related US9767363B2 (en) | 2015-10-30 | 2015-10-30 | System and method for automatic detection of spherical video content |
US15/680,594 Active US10268893B2 (en) | 2015-10-30 | 2017-08-18 | System and method for automatic detection of spherical video content |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/927,795 Expired - Fee Related US9767363B2 (en) | 2015-10-30 | 2015-10-30 | System and method for automatic detection of spherical video content |
Country Status (2)
Country | Link |
---|---|
US (2) | US9767363B2 (en) |
WO (1) | WO2017074786A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP4195663A1 (en) * | 2021-12-08 | 2023-06-14 | ImmerVR GmbH | Device and method |
Families Citing this family (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2017074005A1 (en) * | 2015-10-26 | 2017-05-04 | 이형용 | Cctv automatic selection monitoring system, and cctv automatic selection monitoring management server and management method |
US9767363B2 (en) | 2015-10-30 | 2017-09-19 | Google Inc. | System and method for automatic detection of spherical video content |
US10460501B2 (en) * | 2016-07-04 | 2019-10-29 | Liquid Cinema Inc., Canada | System and method for processing digital video |
US10297086B2 (en) | 2016-07-04 | 2019-05-21 | Liquid Cinema Inc. | System and method for processing digital video |
US10671852B1 (en) | 2017-03-01 | 2020-06-02 | Matroid, Inc. | Machine learning in video classification |
US10506255B2 (en) * | 2017-04-01 | 2019-12-10 | Intel Corporation | MV/mode prediction, ROI-based transmit, metadata capture, and format detection for 360 video |
WO2018206551A1 (en) * | 2017-05-09 | 2018-11-15 | Koninklijke Kpn N.V. | Coding spherical video data |
US11301684B1 (en) * | 2017-09-29 | 2022-04-12 | Amazon Technologies, Inc. | Vision-based event detection |
KR20190069231A (en) * | 2017-12-11 | 2019-06-19 | 삼성전자주식회사 | Wearable display apparatus and controlling method thereof |
CN111684795B (en) * | 2017-12-15 | 2022-08-12 | Pcms控股公司 | Method for using viewing path in 360 ° video navigation |
GB2574052B (en) * | 2018-05-24 | 2021-11-03 | Advanced Risc Mach Ltd | Image processing |
AU2018204004A1 (en) * | 2018-06-06 | 2020-01-02 | Canon Kabushiki Kaisha | Method, system and apparatus for selecting frames of a video sequence |
US20190385372A1 (en) * | 2018-06-15 | 2019-12-19 | Microsoft Technology Licensing, Llc | Positioning a virtual reality passthrough region at a known distance |
US10863160B2 (en) | 2018-08-08 | 2020-12-08 | Liquid Cinema Inc. Canada | Conditional forced perspective in spherical video |
US10735778B2 (en) | 2018-08-23 | 2020-08-04 | At&T Intellectual Property I, L.P. | Proxy assisted panoramic video streaming at mobile edge |
EP3915269A1 (en) * | 2019-01-24 | 2021-12-01 | PCMS Holdings, Inc. | System and method for adaptive spatial content streaming with multiple levels of detail and degrees of freedom |
CN113010735B (en) * | 2019-12-20 | 2024-03-08 | 北京金山云网络技术有限公司 | Video classification method and device, electronic equipment and storage medium |
CN111814617B (en) * | 2020-06-28 | 2023-01-31 | 智慧眼科技股份有限公司 | Fire determination method and device based on video, computer equipment and storage medium |
US11829413B1 (en) * | 2020-09-23 | 2023-11-28 | Amazon Technologies, Inc. | Temporal localization of mature content in long-form videos using only video-level labels |
Citations (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010033303A1 (en) | 1999-05-13 | 2001-10-25 | Anderson Eric C. | Method and system for accelerating a user interface of an image capture unit during play mode |
US20060023105A1 (en) | 2003-07-03 | 2006-02-02 | Kostrzewski Andrew A | Panoramic video system with real-time distortion-free imaging |
US20060187305A1 (en) * | 2002-07-01 | 2006-08-24 | Trivedi Mohan M | Digital processing of video images |
US20070030396A1 (en) | 2005-08-05 | 2007-02-08 | Hui Zhou | Method and apparatus for generating a panorama from a sequence of video frames |
US7333646B2 (en) | 2004-06-01 | 2008-02-19 | Siemens Medical Solutions Usa, Inc. | Watershed segmentation to improve detection of spherical and ellipsoidal objects using cutting planes |
US20100231687A1 (en) | 2009-03-16 | 2010-09-16 | Chase Real Estate Services Corporation | System and method for capturing, combining and displaying 360-degree "panoramic" or "spherical" digital pictures, images and/or videos, along with traditional directional digital images and videos of a site, including a site audit, or a location, building complex, room, object or event |
US20100250120A1 (en) | 2009-03-31 | 2010-09-30 | Microsoft Corporation | Managing storage and delivery of navigation images |
US20100299630A1 (en) | 2009-05-22 | 2010-11-25 | Immersive Media Company | Hybrid media viewing application including a region of interest within a wide field of view |
WO2011098936A2 (en) | 2010-02-09 | 2011-08-18 | Koninklijke Philips Electronics N.V. | 3d video format detection |
US20110267360A1 (en) | 2010-04-29 | 2011-11-03 | Acer Incorporated | Stereoscopic content auto-judging mechanism |
US8217956B1 (en) | 2008-02-29 | 2012-07-10 | Adobe Systems Incorporated | Method and apparatus for rendering spherical panoramas |
US20120300027A1 (en) | 2011-05-24 | 2012-11-29 | Funai Electric Co., Ltd. | Stereoscopic image display device |
US20130215221A1 (en) * | 2012-02-21 | 2013-08-22 | Sen Wang | Key video frame selection method |
US20130216094A1 (en) * | 2012-01-25 | 2013-08-22 | Bruno Delean | Systems, methods and computer program products for identifying objects in video data |
US20130326419A1 (en) | 2012-05-31 | 2013-12-05 | Toru Harada | Communication terminal, display method, and computer program product |
US20140023348A1 (en) * | 2012-07-17 | 2014-01-23 | HighlightCam, Inc. | Method And System For Content Relevance Score Determination |
US20150077416A1 (en) | 2013-03-13 | 2015-03-19 | Jason Villmer | Head mounted display for viewing and creating a media file including omnidirectional image data and corresponding audio data |
US20150091899A1 (en) * | 2013-09-30 | 2015-04-02 | Sisvel Technology S.R.L. | Method and Device For Edge Shape Enforcement For Visual Enhancement of Depth Image Based Rendering of A Three-Dimensional Video Stream |
US20150256746A1 (en) | 2014-03-04 | 2015-09-10 | Gopro, Inc. | Automatic generation of video from spherical content using audio/visual analysis |
US20160012855A1 (en) * | 2014-07-14 | 2016-01-14 | Sony Computer Entertainment Inc. | System and method for use in playing back panorama video content |
US20160142697A1 (en) | 2014-11-14 | 2016-05-19 | Samsung Electronics Co., Ltd. | Coding of 360 degree videos using region adaptive smoothing |
US20160198140A1 (en) | 2015-01-06 | 2016-07-07 | 3DOO, Inc. | System and method for preemptive and adaptive 360 degree immersive video streaming |
US20160352791A1 (en) | 2015-05-27 | 2016-12-01 | Google Inc. | Streaming spherical video |
US9582731B1 (en) | 2014-04-15 | 2017-02-28 | Google Inc. | Detecting spherical images |
US20170124398A1 (en) | 2015-10-30 | 2017-05-04 | Google Inc. | System and method for automatic detection of spherical video content |
-
2015
- 2015-10-30 US US14/927,795 patent/US9767363B2/en not_active Expired - Fee Related
-
2016
- 2016-10-20 WO PCT/US2016/057879 patent/WO2017074786A1/en active Application Filing
-
2017
- 2017-08-18 US US15/680,594 patent/US10268893B2/en active Active
Patent Citations (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010033303A1 (en) | 1999-05-13 | 2001-10-25 | Anderson Eric C. | Method and system for accelerating a user interface of an image capture unit during play mode |
US20060187305A1 (en) * | 2002-07-01 | 2006-08-24 | Trivedi Mohan M | Digital processing of video images |
US20060023105A1 (en) | 2003-07-03 | 2006-02-02 | Kostrzewski Andrew A | Panoramic video system with real-time distortion-free imaging |
US7333646B2 (en) | 2004-06-01 | 2008-02-19 | Siemens Medical Solutions Usa, Inc. | Watershed segmentation to improve detection of spherical and ellipsoidal objects using cutting planes |
US20070030396A1 (en) | 2005-08-05 | 2007-02-08 | Hui Zhou | Method and apparatus for generating a panorama from a sequence of video frames |
US8217956B1 (en) | 2008-02-29 | 2012-07-10 | Adobe Systems Incorporated | Method and apparatus for rendering spherical panoramas |
US20100231687A1 (en) | 2009-03-16 | 2010-09-16 | Chase Real Estate Services Corporation | System and method for capturing, combining and displaying 360-degree "panoramic" or "spherical" digital pictures, images and/or videos, along with traditional directional digital images and videos of a site, including a site audit, or a location, building complex, room, object or event |
US20100250120A1 (en) | 2009-03-31 | 2010-09-30 | Microsoft Corporation | Managing storage and delivery of navigation images |
US20100299630A1 (en) | 2009-05-22 | 2010-11-25 | Immersive Media Company | Hybrid media viewing application including a region of interest within a wide field of view |
US20120314028A1 (en) | 2010-02-09 | 2012-12-13 | Koninklijke Philips Electronics N.V. | 3d video format detection |
WO2011098936A2 (en) | 2010-02-09 | 2011-08-18 | Koninklijke Philips Electronics N.V. | 3d video format detection |
US20110267360A1 (en) | 2010-04-29 | 2011-11-03 | Acer Incorporated | Stereoscopic content auto-judging mechanism |
US20120300027A1 (en) | 2011-05-24 | 2012-11-29 | Funai Electric Co., Ltd. | Stereoscopic image display device |
US20130216094A1 (en) * | 2012-01-25 | 2013-08-22 | Bruno Delean | Systems, methods and computer program products for identifying objects in video data |
US20130215221A1 (en) * | 2012-02-21 | 2013-08-22 | Sen Wang | Key video frame selection method |
US20130326419A1 (en) | 2012-05-31 | 2013-12-05 | Toru Harada | Communication terminal, display method, and computer program product |
US20140023348A1 (en) * | 2012-07-17 | 2014-01-23 | HighlightCam, Inc. | Method And System For Content Relevance Score Determination |
US20150077416A1 (en) | 2013-03-13 | 2015-03-19 | Jason Villmer | Head mounted display for viewing and creating a media file including omnidirectional image data and corresponding audio data |
US20150091899A1 (en) * | 2013-09-30 | 2015-04-02 | Sisvel Technology S.R.L. | Method and Device For Edge Shape Enforcement For Visual Enhancement of Depth Image Based Rendering of A Three-Dimensional Video Stream |
US20150256746A1 (en) | 2014-03-04 | 2015-09-10 | Gopro, Inc. | Automatic generation of video from spherical content using audio/visual analysis |
US9582731B1 (en) | 2014-04-15 | 2017-02-28 | Google Inc. | Detecting spherical images |
US20160012855A1 (en) * | 2014-07-14 | 2016-01-14 | Sony Computer Entertainment Inc. | System and method for use in playing back panorama video content |
US20160142697A1 (en) | 2014-11-14 | 2016-05-19 | Samsung Electronics Co., Ltd. | Coding of 360 degree videos using region adaptive smoothing |
US20160198140A1 (en) | 2015-01-06 | 2016-07-07 | 3DOO, Inc. | System and method for preemptive and adaptive 360 degree immersive video streaming |
US20160352791A1 (en) | 2015-05-27 | 2016-12-01 | Google Inc. | Streaming spherical video |
US20170124398A1 (en) | 2015-10-30 | 2017-05-04 | Google Inc. | System and method for automatic detection of spherical video content |
WO2017074786A1 (en) | 2015-10-30 | 2017-05-04 | Google Inc. | System and method for automatic detection of spherical video content |
Non-Patent Citations (15)
Title |
---|
Bublcam, "Bublcam spherical video camera," Mar. 2014, retrieved on Sep. 30, 2015 from http://www.bublcam.com, 8 pages. |
Ferreira et al., "3D Video Shot Boundary Detection Based on Clustering of Depth-Temporal Features", 2013 11th International Workshop on Content-Based Multimedia Indexing (CBMI), Veszprem, 2013, pp. 1-6. |
Friedman, et al, "Additive Logistic Regression: A Statistical View of Boosting", Special Invited Paper, The Annals of Statistics, vol. 28, No. 2, 2000, pp. 337-407. |
Google, "Spherical metadata standard," retrieved on Sep. 30, 2015 from http://github.com/google/spatial-media, 1 page. |
Holbrook, "Blog-The Latest for Creators, Brands and Fans", Apr. 27, 2015, 6 pages. |
Holbrook, "Blog—The Latest for Creators, Brands and Fans", Apr. 27, 2015, 6 pages. |
Ihang, et al., "Automatic 3D video format detection", Stereoscopic Displays and Applications XXII, SPIE, vol. 7863, No. 1, Feb. 10, 2011, pp. 1-10. |
International Search Report and Written Opinion for PCT Application No. PCT/US2016/57879, dated Dec. 14, 2016, 15 pages. |
Moon, "Facebook explains the tech behind its 360-degree videos", retrieved from http://www.engadget.com/2015/10/15/facebook-360-degree-video-tech/ on Oct. 15, 2015, 7 pages. |
Notice of Allowance for U.S. Appl. No. 14/927,795, dated May 19, 2017, 28 pages. |
Ricoh, "Ricoh theta 360 video camera," Mar. 2014, retrieved on Sep. 30, 2015 from http://theta360.com/en/, 4 pages. |
Stahlberg, et al., "Digital Image Processing in Natural Sciences and Medecine", Digital Image Processing in Natural Sciences and Medecine, Sep. 7, 2002, pp. 97-130. |
Steedly et al., "Efficiently Registering Video Into Panoramic Mosaics", Tenth IEEE International Conference on Computer Vision (ICCV'05), vol. 1, Beijing, 2005, pp. 1300-1307. |
Su, Sheng , et al., "A Novel 3D Video Format Identification Algorithm", Proceedings of the Second International Conference on Communications, Signal Processing, and Systems vol. 246 of the Series Lecture Notes in Electrical Engineering, Oct. 24, 2013, pp. 225-232. |
Yueli, et al., "Automatic 3D video detection", 2014 International Conference on Information Science, Electronics and Electrical Engineering, IEEE, Apr. 26, 2014, pp. 274-277. |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP4195663A1 (en) * | 2021-12-08 | 2023-06-14 | ImmerVR GmbH | Device and method |
Also Published As
Publication number | Publication date |
---|---|
US20170344830A1 (en) | 2017-11-30 |
WO2017074786A1 (en) | 2017-05-04 |
US9767363B2 (en) | 2017-09-19 |
US20170124398A1 (en) | 2017-05-04 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10268893B2 (en) | System and method for automatic detection of spherical video content | |
US9721156B2 (en) | Gift card recognition using a camera | |
US8792722B2 (en) | Hand gesture detection | |
US8750573B2 (en) | Hand gesture detection | |
US11798278B2 (en) | Method, apparatus, and storage medium for classifying multimedia resource | |
US20190304102A1 (en) | Memory efficient blob based object classification in video analytics | |
CN111062871B (en) | Image processing method and device, computer equipment and readable storage medium | |
US9436883B2 (en) | Collaborative text detection and recognition | |
WO2021139324A1 (en) | Image recognition method and apparatus, computer-readable storage medium and electronic device | |
CN106650662B (en) | Target object shielding detection method and device | |
US9436875B2 (en) | Method and apparatus for semantic extraction and video remix creation | |
CN109325933A (en) | A kind of reproduction image-recognizing method and device | |
CN111476306A (en) | Object detection method, device, equipment and storage medium based on artificial intelligence | |
CN109670444B (en) | Attitude detection model generation method, attitude detection device, attitude detection equipment and attitude detection medium | |
CN108875750B (en) | Object detection method, device and system and storage medium | |
CN112052186A (en) | Target detection method, device, equipment and storage medium | |
CN111914812A (en) | Image processing model training method, device, equipment and storage medium | |
US20140126819A1 (en) | Region of Interest Based Image Registration | |
CN111242090A (en) | Human face recognition method, device, equipment and medium based on artificial intelligence | |
CN111931877A (en) | Target detection method, device, equipment and storage medium | |
CN109074497A (en) | Use the activity in depth information identification sequence of video images | |
CN113822136A (en) | Video material image selection method, device, equipment and storage medium | |
CN108875506B (en) | Face shape point tracking method, device and system and storage medium | |
CN109785439A (en) | Human face sketch image generating method and Related product | |
US20140050404A1 (en) | Combining Multiple Image Detectors |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BIRKBECK, NEIL AYLON CHARLES;LAM, KA-KIT;KELLY, DAMIEN;AND OTHERS;SIGNING DATES FROM 20151030 TO 20151101;REEL/FRAME:044511/0887 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
CC | Certificate of correction | ||
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |