DE202018100752U1 - Bayesian methodology for geospatial object / feature recognition - Google Patents
Bayesian methodology for geospatial object / feature recognition Download PDFInfo
- Publication number
- DE202018100752U1 DE202018100752U1 DE202018100752.1U DE202018100752U DE202018100752U1 DE 202018100752 U1 DE202018100752 U1 DE 202018100752U1 DE 202018100752 U DE202018100752 U DE 202018100752U DE 202018100752 U1 DE202018100752 U1 DE 202018100752U1
- Authority
- DE
- Germany
- Prior art keywords
- images
- interest
- image
- location
- image capture
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013397 bayesian methodology Methods 0.000 title 1
- 238000000034 method Methods 0.000 claims abstract description 21
- 230000015654 memory Effects 0.000 claims description 24
- 230000006870 function Effects 0.000 description 19
- 238000001514 detection method Methods 0.000 description 12
- 238000004364 calculation method Methods 0.000 description 7
- 238000003384 imaging method Methods 0.000 description 5
- 230000001419 dependent effect Effects 0.000 description 3
- 230000004807 localization Effects 0.000 description 3
- 238000010606 normalization Methods 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 230000007613 environmental effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 241001465754 Metazoa Species 0.000 description 1
- 241000836710 Trichaptum sector Species 0.000 description 1
- 238000013145 classification model Methods 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 238000010586 diagram Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000005672 electromagnetic field Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000000873 masking effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 230000003936 working memory Effects 0.000 description 1
- 238000013316 zoning Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/73—Determining position or orientation of objects or cameras using feature-based methods
- G06T7/74—Determining position or orientation of objects or cameras using feature-based methods involving reference images or patches
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/70—Determining position or orientation of objects or cameras
- G06T7/77—Determining position or orientation of objects or cameras using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/10—Terrestrial scenes
- G06V20/13—Satellite images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/50—Context or environment of the image
- G06V20/56—Context or environment of the image exterior to a vehicle by using sensors mounted on the vehicle
- G06V20/58—Recognition of moving objects or obstacles, e.g. vehicles or pedestrians; Recognition of traffic objects, e.g. traffic signs, traffic lights or roads
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20076—Probabilistic image processing
Abstract
Computerlesbarer Speicher, der Anweisungen speichert, die durch einen oder mehrere Prozessoren ausführbar sind, um ein Verfahren zum Bestimmen eines Standortes eines Objekts von Interesse auszuführen, das Verfahren umfassend:Identifizieren eines Satzes von vielen Bildern aus einer Bilddatenbank, die sich auf eine Region von Interesse beziehen, wobei jedes der vielen Bilder mit Bilderfassungsinformationen verknüpft ist, die mindestens einen Bilderfassungsstandort beinhalten;Anwenden eines Bilderkennungs-Tools auf jedes Bild in dem Satz von vielen Bildern;Bestimmen, basierend auf dem Anwenden des Bilderkennungs-Tools, welche der Bilder das Objekt von Interesse beinhalten und welche der Bilder das Objekt von Interesse nicht beinhalten;für jeden von mehreren Kandidatenstandorten in der Region von Interesse Berechnen eines Wahrscheinlichkeitswertes des an dem Kandidatenstandort vorhandenen Objekts von Interesse unter Verwendung der Bilderfassungsinformationen für Bilder in dem Satz von vielen Bildern, die so bestimmt sind, dass sie das Objekt von Interesse beinhalten, und unter Verwendung der Bilderfassungsinformationen für Bilder in dem Satz von vielen Bildern, die so bestimmt sind, dass sie das Objekt von Interesse nicht beinhalten; undBestimmen des Standorts des Objekts unter Verwendung der Wahrscheinlichkeitswerte für die mehreren Kandidatenstandorte.A computer-readable storage storing instructions executable by one or more processors to perform a method of determining a location of an object of interest, the method comprising: identifying a set of many images from an image database relating to a region of interest wherein each of the plurality of images is associated with image capture information including at least one image capture site; applying an image recognition tool to each image in the set of multiple images; determining, based on the application of the image recognition tool, which of the images is the object of Include interest and which of the images do not include the object of interest; for each of a plurality of candidate sites in the region of interest, calculate a probability value of the object of interest present at the candidate site using the image capture information for images in the set of many images that are determined to include the object of interest, and using the image capture information for images in the set of many images that are determined not to include the object of interest; anddetermining the location of the object using the probability values for the plurality of candidate sites.
Description
In Übereinstimmung mit den Bestimmungen des Gebrauchsmustergesetzes sind nur Vorrichtungen, wie sie in den anliegenden Schutzansprüchen definiert sind, geschützt und vom Gebrauchsmuster abgedeckt, nicht jedoch Verfahren. Soweit in der nachstehenden Beschreibung gegebenenfalls auf Verfahren Bezug genommen wird, dienen diese Hinweise nur zur exemplarischen Erläuterung der mit den anliegenden Schutzansprüchen geschützten Vorrichtung(en).In accordance with the provisions of the Utility Model Law, only devices as defined in the appended claims are protected and covered by utility model, but not methods. Insofar as reference is made in the description below to methods, these instructions serve only to exemplify the device (s) protected by the attached claims.
HINTERGRUNDBACKGROUND
Im Laufe der Zeit sind immer größere Mengen an Geodaten verfügbar geworden. Das Ausführen semantischer Suchen durch solche Bilder ist mit zunehmendem Volumen anspruchsvoller geworden. Die semantische Suche ist die Suche nach einem bestimmten Objekt oder Umweltattribut, wie z. B. einem Laternenpfahl oder einer Parklandschaft, nach Namen und nicht nach einem exakten Pixelmuster. Herkömmliche Verfahren der semantischen Suche erfordern eine manuelle Überprüfung, und neue Verfahren erfordern aufwändig trainierte und abgestimmte, zweckgebundene Bilderkennungsmodelle. Beispielsweise ist manuelle Triangulation sehr arbeitsintensiv, langsam und kostspielig, während automatische Triangulation ausgefeilte und mühsam erstellte lokalisierungsspezifische Bilderkennungsmodelle oder sehr hochauflösende Bilder erfordert und dennoch einen ungenauen Standort bereitstellen kann. Obwohl allgemein einsetzbare Bildklassifikationsmodelle für die Speicherung von Fotos oder die Suche im Internet verfügbar werden, fehlt diesen nicht spezialisierten Modellen die Präzision, um Objekte angemessen zu lokalisieren.Over time, ever larger amounts of geodata have become available. Performing semantic searches through such images has become more demanding as volume increases. The semantic search is the search for a specific object or environmental attribute, such. A lamppost or parkland, by name and not by exact pixel pattern. Conventional methods of semantic search require manual checking, and new methods require elaborately trained and matched, dedicated image recognition models. For example, manual triangulation is labor-intensive, slow and costly, while automatic triangulation requires sophisticated and cumbersome localization-specific image recognition models or very high-resolution images, yet can provide an inaccurate location. Although general image classification models are available for storing photos or searching the Internet, these non-specialized models lack the precision to properly locate objects.
KURZDARSTELLUNG DER OFFENBARUNGSHORT DESCRIPTION OF THE REVELATION
Ein Aspekt der Offenbarung stellt einen computerlesbaren Speicher bereit, der Anweisungen speichert, die durch einen oder mehrere Prozessoren ausführbar sind, um ein Verfahren zum Bestimmen eines Standortes eines Objekts von Interesse auszuführen. Dieses Verfahren beinhaltet Identifizieren eines Satzes von vielen Bildern aus einer Bilddatenbank, die sich auf eine Region von Interesse beziehen, wobei jedes der vielen Bilder mit Bilderfassungsinformationen verknüpft ist, die mindestens einen Bilderfassungsstandort beinhalten. Das Verfahren beinhaltet ferner Anwenden eines Bilderkennungs-Tools auf jedes Bild in dem Satz von vielen Bildern; und Bestimmen, basierend auf dem Anwenden des Bilderkennungs-Tools, welche der Bilder das Objekt von Interesse beinhalten und welche der Bilder das Objekt von Interesse nicht beinhalten. Für jeden von mehreren Kandidatenstandorten in der Region von Interesse wird ein Wahrscheinlichkeitswert des an dem Kandidatenstandort vorhandenen Objekts, unter Verwendung der Bilderfassungsinformationen für Bilder in dem Satz von vielen Bildern, berechnet, die so bestimmt sind, dass sie das Objekt von Interesse beinhalten, und unter Verwendung der Bilderfassungsinformationen für Bilder in dem Satz von vielen Bildern, die so bestimmt sind, dass sie das Objekt von Interesse nicht beinhalten. Der Standort des Objekts wird unter Verwendung der Wahrscheinlichkeitswerte für die mehreren Kandidatenstandorte bestimmt.One aspect of the disclosure provides a computer-readable storage that stores instructions that are executable by one or more processors to perform a method of determining a location of an object of interest. This method includes identifying a set of many images from an image database relating to a region of interest, wherein each of the plurality of images is associated with image capture information that includes at least one image capture site. The method further includes applying an image recognition tool to each image in the set of many images; and determining, based on the application of the image recognition tool, which of the images include the object of interest and which of the images do not include the object of interest. For each of a plurality of candidate sites in the region of interest, a probability value of the object existing at the candidate site is calculated using the image capture information for images in the set of many images determined to include the object of interest, and under Use the image capture information for images in the set of many images that are determined not to contain the object of interest. The location of the object is determined using the probability values for the multiple candidate sites.
Nach einigen Beispielen beinhalten die Bilderfassungsinformationen für mindestens einige der Bilder einen Bilderfassungsstandort und eine Bilderfassungsausrichtung, während Berechnen des Wahrscheinlichkeitswertes des an einem bestimmten Standort vorhandenen Objekts Verwenden der Bilderfassungsausrichtung umfasst.According to some examples, the image capture information for at least some of the images includes a capture location and an image capture orientation, while calculating the probability score of the object present at a particular location includes using the capture orientation.
In einer der vorhergehenden Ausführungsformen kann Bestimmen, welche der Bilder ein Objekt von Interesse beinhalten und welche der Bilder das Objekt von Interesse nicht beinhalten, Bestimmen eines Vertrauensfaktors beinhalten, dass die Bilder das Objekt von Interesse beinhalten oder nicht beinhalten, während Berechnen des Wahrscheinlichkeitswertes des an dem Standort vorhandenen Objekts von Interesse Verwenden des bestimmten Vertrauensfaktors beinhalten kann.In one of the foregoing embodiments, determining which of the images include an object of interest and which of the images do not include the object of interest may include determining a confidence factor that the images include or do not include the object of interest while calculating the probability value of Using the specific confidence factor may be present in the location of the object of interest.
In einer der vorhergehenden Ausführungsformen kann Berechnen des Wahrscheinlichkeitswertes des an dem Kandidatenstandort vorhandenen Objekts von Interesse Anwenden eines Faktors mit einem ersten Zeichen für die Bilder in dem Satz von vielen Bildern beinhalten, die so bestimmt sind, dass sie das Objekt von Interesse beinhalten, und Anwenden eines Faktors mit einem zweiten Zeichen für die Bilder in dem Satz von vielen Bildern, die so bestimmt sind, dass sie das Objekt von Interesse nicht beinhalten, wobei das zweite Zeichen dem ersten Zeichen gegenüberliegt.In one of the foregoing embodiments, computing the probability value of the object of interest at the candidate site may include applying a factor having a first character to the images in the set of many images determined to include the object of interest and applying a factor having a second character for the images in the set of many images that are determined not to include the object of interest, the second character being opposite to the first character.
In einer der vorhergehenden Ausführungsformen können die mehreren Kandidatenstandorte in der Region von Interesse ein Raster aus mehreren Standorten beinhalten.In one of the previous embodiments, the plurality of candidate sites in the region of interest may include a grid of multiple sites.
In einer der vorhergehenden Ausführungsformen können die mehreren Kandidatenstandorte Standorte beinhalten, die in einem Sichtfeld von jedem Bild in dem Satz von vielen Bildern enthalten sind.In one of the preceding embodiments, the plurality of candidate sites may include locations included in a field of view of each image in the set of many images.
In einer der vorhergehenden Ausführungsformen kann das Bilderkennungs-Tool so konfiguriert werden, dass es diskrete Objekte erkennt. In one of the previous embodiments, the image recognition tool may be configured to recognize discrete objects.
In einer der vorhergehenden Ausführungsformen kann das Bilderkennungs-Tool zum Erkennen von Objekten konfiguriert werden, die spezifische Eigenschaften aufweisen.In one of the preceding embodiments, the image recognition tool may be configured to recognize objects having specific characteristics.
In jeder der vorhergehenden Ausführungsformen kann das Bilderkennungs-Tool aus einer Bibliothek von Bilderkennungs-Tools ausgewählt werden.In any of the foregoing embodiments, the image recognition tool may be selected from a library of image recognition tools.
Ein weiterer Aspekt der Offenbarung stellt ein System bereit, das einen Arbeitsspeicher, der eine Vielzahl von Bildern in Verbindung mit Standortinformationen speichert, und einen oder mehrere Prozessoren, die mit dem Arbeitsspeicher kommunizieren, bereitstellt. Der eine oder die mehreren Prozessoren sind so programmiert, dass sie aus der Vielzahl von Bildern einen Satz von Bildern identifizieren, die sich auf eine Region von Interesse beziehen, unter Verwendung von Bilderkennung bestimmen, welche der Bilder das Objekt von Interesse beinhalten, und unter Verwendung von Bilderkennung bestimmen, welche der Bilder das Objekt von Interesse nicht beinhalten. Für jeden von mehreren Kandidatenstandorten in der Region von Interesse wird ein Wahrscheinlichkeitswert des an dem Kandidatenstandort vorhandenen Objekts unter Verwendung der Standortinformationen für Bilder in dem Satz von vielen Bildern berechnet, die so bestimmt sind, dass sie das Objekt von Interesse beinhalten, und unter Verwendung der Standortinformationen für Bilder in dem Satz von Bildern, die so bestimmt sind, dass sie das Objekt von Interesse nicht beinhalten. Der eine oder die mehreren Prozessoren sind ferner programmiert, um den Standort des Objekts unter Verwendung der Wahrscheinlichkeitswerte für die mehreren Kandidatenstandorte zu bestimmen. In einigen Beispielen kann das System ferner ein Bilderkennungs-Tool beinhalten, mit dem Objekte oder Attribute in den Bildern identifiziert werden können. Ferner können der eine oder die mehreren Prozessoren auch so konfiguriert werden, dass sie die bestimmten Standortinformationen zur Ausgabe auf einer Anzeige bereitstellen.Another aspect of the disclosure provides a system that provides a memory that stores a plurality of images associated with location information and one or more processors that communicate with the memory. The one or more processors are programmed to identify, from among the plurality of images, a set of images relating to a region of interest, using image recognition to determine which of the images include the object of interest, and using of image recognition determine which of the images do not contain the object of interest. For each of a plurality of candidate sites in the region of interest, a probability value of the object existing at the candidate site is calculated using the location information for images in the set of many images determined to include the object of interest, and using the Location information for images in the set of images that are determined not to include the object of interest. The one or more processors are further programmed to determine the location of the object using the probability values for the plurality of candidate sites. In some examples, the system may further include an image recognition tool that can identify objects or attributes in the images. Further, the one or more processors may also be configured to provide the determined location information for display on a display.
Ein weiterer Aspekt der Offenbarung stellt ein computerlesbares Medium bereit, das Anweisungen speichert, die durch einen Prozessor ausführbar sind, um ein Verfahren zum Bestimmen des Standorts eines Objekts von Interesse auszuführen. Jene Anweisungen sehen vor, aus einer Bilddatenbank einen Satz von vielen Bildern zu identifizieren, die sich auf eine Region von Interesse beziehen, wobei jedes der Bilder mit Bilderfassungsinformationen verknüpft ist, die mindestens einen Bilderfassungsstandort umfassen, ein Bilderkennungs-Tool auf jedes Bild in dem Satz von Bildern anzuwenden, basierend auf dem Anwenden des Bilderkennungs-Tools zu bestimmen, welche der Bilder das Objekt von Interesse beinhalten und welche der Bilder das Objekt von Interesse nicht beinhalten. Für jeden von mehreren Kandidatenstandorten in der Region von Interesse stellen die Anweisungen ferner einen Wahrscheinlichkeitswert des an dem Kandidatenstandort vorhandenen Objekts unter Verwendung der Bilderfassungsinformationen für Bilder in dem Satz von vielen Bildern bereit, die so bestimmt sind, dass sie das Objekt von Interesse beinhalten, und unter Verwendung der Bilderfassungsinformationen für Bilder in dem Satz von vielen Bildern, die so bestimmt sind, dass sie das Objekt von Interesse nicht beinhalten, und Bestimmen des Standortes des Objekts unter Verwendung der Wahrscheinlichkeitswerte für die mehreren Kandidatenstandorte.Another aspect of the disclosure provides a computer-readable medium storing instructions executable by a processor to perform a method of determining the location of an object of interest. Those instructions provide to identify from an image database a set of many images relating to a region of interest, each of the images being associated with image capture information comprising at least one image capture site, an image recognition tool on each image in the sentence apply images based on applying the image recognition tool to determine which of the images include the object of interest and which of the images do not include the object of interest. For each of multiple candidate sites in the region of interest, the instructions further provide a probability value of the object present at the candidate site using the image capture information for images in the set of many images determined to include the object of interest, and using the image capture information for images in the set of many images that are determined not to include the object of interest, and determining the location of the object using the probability values for the multiple candidate sites.
Figurenlistelist of figures
-
1 zeigt ein Blockdiagramm eines exemplarischen Verfahrens gemäß Aspekten der Offenbarung.1 FIG. 12 is a block diagram of an exemplary method in accordance with aspects of the disclosure. FIG. -
2 zeigt eine Draufsicht-Abbildung von exemplarischen Beobachtungen und Nicht-Beobachtungen gemäß Aspekten der Offenbarung.2 FIG. 12 shows a top view illustration of exemplary observations and non-observations in accordance with aspects of the disclosure. FIG. -
3 veranschaulicht Straßenansichten in Verbindung mit der Draufsicht-Abbildung von Beobachtungen und Nicht-Beobachtungen von2 .3 illustrates street views in conjunction with the top view illustration of observations and non-observations of2 , -
4 zeigt eine Draufsicht-Abbildung von Beobachtungen und Nicht-Beobachtungen im Verhältnis zu einer Vielzahl von Zellen gemäß Aspekten der Offenbarung.4 FIG. 11 shows a top view illustration of observations and non-observations relative to a plurality of cells in accordance with aspects of the disclosure. FIG. -
5 zeigt eine Draufsicht-Abbildung eines anderen Beispiels von Beobachtungen und Nicht-Beobachtungen gemäß Aspekten der Offenbarung.5 FIG. 12 is a plan view illustration of another example of observations and non-observations in accordance with aspects of the disclosure. FIG. -
6 zeigt eine Draufsicht-Abbildung eines anderen Beispiels von Beobachtungen und Nicht-Beobachtungen gemäß Aspekten der Offenbarung..6 FIG. 12 is a plan view illustration of another example of observations and non-observations in accordance with aspects of the disclosure. FIG. -
7 zeigt eine Draufsicht-Abbildung einer exemplarischen fokussierten Beobachtung unter Verwendung eines Begrenzungsfeldes gemäß Aspekten der Offenbarung.7 FIG. 10 is a plan view illustration of an exemplary focused observation using a bounding box in accordance with aspects of the disclosure. FIG. -
8 zeigt eine Draufsicht-Abbildung von exemplarischen Hindernissen gemäß Aspekten der Offenbarung.8th FIG. 10 is a plan view illustration of exemplary obstacles in accordance with aspects of the disclosure. FIG. -
9 zeigt eine Draufsicht-Abbildung einer exemplarischen Wahrscheinlichkeit von Hindernissen, die Beobachtungen gemäß Aspekten der Offenbarung beeinflussen.9 FIG. 10 is a plan view illustration of an exemplary likelihood of obstacles affecting observations in accordance with aspects of the disclosure. FIG. -
10 veranschaulicht ein exemplarisches Ausgeben semantischer Suchvorgänge unter Verwendung von Beobachtungen und Nicht-Beobachtungen gemäß Aspekten der Offenbarung.10 FIG. 12 illustrates an exemplary outputting of semantic searches using observations and non-observations in accordance with aspects of the disclosure. FIG. -
11 zeigt ein Ablaufdiagramm, das ein exemplarisches Verfahren zum Bestimmen von Objektstandorten unter Verwendung von Beobachtungen und Nicht-Beobachtungen gemäß Aspekten der Offenbarung veranschaulicht.11 FIG. 12 is a flowchart illustrating an exemplary method of determining object locations using observations and non-observations, in accordance with aspects of the disclosure. FIG.
AUSFÜHRLICHE BESCHREIBUNGDETAILED DESCRIPTION
ÜbersichtOverview
Die Technologie betrifft im Allgemeinen Bilderkennung und Zuordnung. Insbesondere werden Informationen aus Beobachtungen und Nicht-Beobachtungen verwendet, um ein bestimmtes Objekt oder Merkmal zu lokalisieren. Um beispielsweise alle Feuerhydranten innerhalb einer geografischen Zielregion zu lokalisieren, kann Bilderkennung auf Bildern der Region durchgeführt werden, wobei Bilder, die Hydranten beinhalten und Bilder, die keine Hydranten beinhalten, verwendet werden, um die Standorte von jedem Hydranten genauer zu berechnen. Die Nicht-Beobachtungen in der Nähe einer Beobachtung schränken den Bereich von Standorten, an denen sich ein Objekt befinden könnte, erheblich ein. Durch die Verwendung eines Bilderkennungs-Tools und die Berechnung von Wahrscheinlichkeiten für mehrere Standorte kann der Standort des Objekts relativ einfach und mit relativ hoher Sicherheit bestimmt werden, selbst wenn die Bilder nicht hochauflösend sind und/oder mit niedriger Auflösung oder unzuverlässigen Bilderfassungsinformationen verknüpft sind. Dies kann ermöglichen, die Objektstandortidentifikation auch unter Verwendung von nicht-professionellen Inhalten, z. B. von Smartphones, auszuführen. Durch Verwendung von Informationen für Bilder, die nicht für die Aufnahme des Objekts bestimmt sind, kann die Objektstandortidentifikation verbessert werden. Darüber hinaus kann dies ohne, oder nur mit relativ geringen Berechnungskosten erreicht werden.The technology generally relates to image recognition and mapping. In particular, information from observations and non-observations is used to locate a particular object or feature. For example, to locate all fire hydrants within a geographic target region, image recognition may be performed on images of the region, with images including hydrants and images containing no hydrants used to more accurately calculate the locations of each hydrant. The non-observations near an observation significantly limit the range of locations where an object could be located. By using an image recognition tool and calculating multi-site probabilities, the location of the object can be determined relatively easily and with relatively high security, even if the images are not high-resolution and / or associated with low resolution or unreliable image capture information. This may enable object location identification also using non-professional content, e.g. B. from smartphones to execute. By using information for images that are not intended to capture the object, object location identification can be improved. In addition, this can be achieved without, or only with relatively low calculation costs.
Ein initialisierter Wert wird für einen Satz von Kandidatenstandorten für das jeweilige Objekt oder Merkmal bestimmt. Beispielsweise kann der initialisierte Wert für jeden Kandidatenstandort auf Null gesetzt werden oder, in einer ausgefeilteren Version, auf einen früheren Wahrscheinlichkeitswert vor der Bilderkennungsanalyse gesetzt werden, der eine Wahrscheinlichkeit darstellt, dass das Objekt oder Merkmal an diesem Standort vorhanden ist. Beispielsweise kann für jeden Standort vor der Anwendung eines Bilderkennungs-Tools die Wahrscheinlichkeit, dass der Standort einen Feuerhydranten enthält, x % betragen, basierend auf einer Anzahl von Feuerhydranten in dem Zielbereich und einer Größe des Zielbereichs. Die Kandidatenstandorte können z. B. durch eine diskrete Liste von Standorten (z. B. nur Straßenecken), eine Raster- oder Gitteraufteilung der Zielregion in Zellen, eine gerasterte Karte der Zielregion (z. B. jedes Pixel des Rasters ist eine Zelle) oder eine fortlaufende Vektor- und Gradienten-Definition von Regionen definiert werden.An initialized value is determined for a set of candidate sites for the particular object or feature. For example, the initialized value for each candidate site may be set to zero, or, in a more sophisticated version, set to an earlier probability value prior to image recognition analysis that represents a likelihood that the object or feature exists at that site. For example, for each site prior to application of an image recognition tool, the probability that the site contains a fire hydrant may be x%, based on a number of fire hydrants in the target area and a size of the target area. The candidate locations can, for. A raster or grid division of the target region into cells, a rasterized map of the target region (eg, each pixel of the raster is a cell), or a continuous vector image. and gradient definition of regions.
Für jedes Bild in einer Vielzahl von Bildern wird ein Bilderkennungs-Tool auf das Bild angewendet, um eine Punktzahl oder eine Vertrauensbewertung zu erhalten, dass das jeweilige Objekt oder Merkmal in dem Bild sichtbar ist. Diese Punktzahl oder Vertrauensbewertung kann in einen normalisierten Wert umgewandelt werden, der einen Betrag angibt, dass die Wahrscheinlichkeit, dass sich das Objekt in einer Region befindet, die in dem Bild dargestellt ist, nach der Bilderkennung zugenommen hat. Beispielsweise wenn die vorherige Wahrscheinlichkeit (vor der Bilderkennungsanalyse), dass das Bild einen Feuerhydranten enthält, x % beträgt, kann die Wahrscheinlichkeit nach der Bilderkennung von x % auf (x+n)% steigen oder von (x %) auf (x-m)% sinken, je nachdem, ob ein Hydrant in dem Bild erkannt wurde oder nicht. Und nach der Verarbeitung aufeinanderfolgender Bilder kann sich die vorherige Wahrscheinlichkeit nacheinander (zusätzlich oder anderweitig) auf eine spätere Wahrscheinlichkeit und/oder eine normalisierte Wahrscheinlichkeitspunktzahl erhöhen oder verringern. In einigen Beispielen kann ein Log-Bayes-Faktor verwendet werden. Dieser kann eine besonders rechnerisch effiziente Art und Weise der Verwendung sowohl von Objektbeobachtungen als auch von Nicht-Beobachtungen des Objekts zur Identifizierung des Standorts des Objekts bereitstellen.For each image in a plurality of images, an image recognition tool is applied to the image to obtain a score or confidence rating that the respective object or feature is visible in the image. This score or confidence score may be converted to a normalized value indicating an amount that the likelihood that the object is in a region represented in the image has increased after the image recognition. For example, if the previous probability (before image recognition analysis) that the image contains a fire hydrant is x%, the probability of image recognition may increase from x% to (x + n)% or from (x%) to (xm)% decrease depending on whether or not a hydrant has been detected in the image. And after processing successive images, the previous probability may increase or decrease sequentially (in addition or otherwise) to a later probability and / or a normalized probability score. In some examples, a Log-Bayesian factor may be used. This may provide a particularly computationally efficient way of using both object observations and non-observations of the object to identify the location of the object.
Kandidatenstandorte, die in einem Sichtfeld eines jeden Bildes enthalten sind, werden z. B. unter Verwendung von Kameramerkmalen und Pose identifiziert. In einigen Beispielen können die Kamerageometrie und einige Standardhorizontabstände einen Sektor definieren, der mit dem initialisierten Satz von Kandidatenstandorten verglichen wird, um zu bestimmen, welche Kandidatenstandorte in dem Sektor enthalten sind. In anderen Beispielen kann eine Falloff-Funktion verwendet werden, die eine Wahrscheinlichkeit darstellt, dass ein Objekt in dem Bild sichtbar ist, wenn es tatsächlich vorhanden ist, z. B. unter Berücksichtigung einer möglichen Verdeckung des Objekts in dem Bild. Die Falloff-Funktion ist möglicherweise nicht radial und/oder standortabhängig, z. B. unter Berücksichtigung anderer Faktoren wie der lokalen Bevölkerungsdichte oder der Vegetationsdichte, die das Verdeckungsrisiko erhöhen würden. Die Verwendung von Informationen in Verbindung mit der Ausrichtung eines Bilderfassungsgerätes kann eine verbesserte Objektstandorterkennung ermöglichen. Die Objektstandorterkennung kann jedoch auch dann ausgeführt werden, wenn einige oder alle Bilder keine Bilderfassungsausrichtungsinformationen enthalten.Candidate locations contained in a field of view of each image are e.g. B. identified using camera features and pose. In some examples, the camera geometry and some standard horizon distances may define a sector that is compared to the initialized set of candidate sites to determine which candidate sites are included in the sector. In other examples, a falloff function may be used that represents a likelihood that an object will be visible in the image, if it actually exists, e.g. Considering a possible occlusion of the object in the image. The falloff function may not be radial and / or location dependent, e.g. Taking into account other factors such as local population density or vegetation density that would increase the risk of concealment. The use of information in conjunction with the orientation of an image capture device may allow for improved object location recognition. However, object location detection may be performed even if some or all of the images do not contain image capture alignment information.
Es kann ein Wahrscheinlichkeitswert für jeden Kandidatenstandort berechnet werden, der basierend auf dem normalisierten Wert und dem initialisierten Wert für jeden dieser Standortkandidaten in dem Bild sichtbar ist. Beispielsweise kann der normalisierte Wert zu dem initialisierten Wert addiert oder mit diesem multipliziert, herabgesetzt werden, wenn eine Falloff-Funktion verwendet wird. In einigen Beispielen kann der Wahrscheinlichkeitswert mit einem Schwellenwert verglichen werden, ab dem das Objekt so bestimmt werden kann, dass es sich an dem Kandidatenstandort befindet, der dieser Wahrscheinlichkeitspunktzahl entspricht. In anderen Beispielen kann die Wahrscheinlichkeitspunktzahl in eine Wahrscheinlichkeit umgewandelt werden, dass das Objekt oder Attribut an diesem Kandidatenstandort vorhanden ist. Dies kann es ermöglichen, dass Informationen zur Objektstandortidentifikation verwendet werden können, auch wenn es kein absolutes Vertrauen in die Objektidentifikation durch das Bilderkennungs-Tool gibt.A probability value can be calculated for each candidate location that is visible in the image based on the normalized value and the initialized value for each of these location candidates. For example, the normalized value may be added to or multiplied by the initialized value if a falloff function is used. In some examples, the likelihood value may be compared to a threshold at which the object may be determined to be at the candidate location corresponding to that likelihood score. In other examples, the probability score may be converted to a probability that the object or attribute exists at that candidate location. This may allow information for object location identification to be used, even if there is no absolute reliance on object identification by the image recognition tool.
Exemplarische SystemeExemplary systems
Arbeitsspeicher
Die Anweisungen
Daten
Der eine oder die mehreren Prozessoren
Obwohl
Jedes der Computergeräte
Beispielsweise kann jedes der Computergeräte
Jedes der Client-Computergeräte
Obwohl die Client-Computergeräte
In anderen Beispielen können ein oder mehrere der Client-Geräte
Wie bei Arbeitsspeicher
Speichersystem
Sowohl die Beobachtungen als auch die Nicht-Beobachtungen werden verwendet, um einen Standort des Objekts
In einigen Beispielen kann eine größere Präzision durch Verwenden einer Falloff-Funktion erreicht werden, wie z. B. einer radialen Falloff-Funktion, die die Wahrscheinlichkeit darstellt, dass das Objekt in jedem Bild sichtbar ist, wenn es sich tatsächlich dort befindet. Beispielsweise kann es vorkommen, dass das Objekt in dem Bild von anderen Objekten wie Bäumen, Autos, Zäunen usw. verdeckt wird. Eine noch höhere Genauigkeit kann durch die Verwendung einer nicht radialen oder standortabhängigen Falloff-Funktion erreicht werden. Diese Funktion kann auch andere Faktoren wie die lokale Bevölkerungsdichte oder die Vegetationsdichte berücksichtigen, die das Verdeckungsrisiko erhöhen würden. In diesen Beispielen ist das Ergebnis nicht nur ein einfaches boolesches Ja/Nein für Einschließung, sondern auch ein Abzinsungsfaktor, der die Wahrscheinlichkeit eines Falsch-Negativs aus anderen Gründen als Nichtvorhandensein, wie z. B. Verdeckung, widerspiegelt.In some examples, greater precision may be achieved by using a falloff function such as A radial falloff function representing the likelihood that the object will be visible in each image when it is actually there. For example, the object in the image may be obscured by other objects such as trees, cars, fences, and so forth. Even greater accuracy can be achieved by using a non-radial or location-dependent falloff function. This feature may include other factors such as the local Account for population density or vegetation density that would increase the risk of In these examples, the result is not only a simple Boolean Yes / No for inclusion, but also a discount factor that reduces the likelihood of a false negative for reasons other than non-existence, such as the presence of a false negative. As occlusion reflects.
Neben Erkennen von Objekten können diese Techniken auch zum Bestimmen des genauen Standortes von Attributen eingesetzt werden. So kann z. B. nach sonnigen geografischen Gebieten gesucht werden, z. B. für die Immobiliensuche. Dementsprechend können Bilder, die einen natürlichen Lichtwert über einem Schwellenwert darstellen, als Beobachtungen identifiziert werden, und Bilder, die einen natürlichen Lichtwert unter dem Schwellenwert darstellen, können als Nicht-Beobachtungen identifiziert werden. Relevante Zeitstempelinformationen können ebenfalls berücksichtigt werden, z. B. durch Beschränkung der gesuchten Bilder auf Bilder, die zu einer bestimmten Tageszeit (z. B. tagsüber) erfasst wurden. Die mit den Nicht-Beobachtungen verknüpften Bilderfassungsinformationen können zur präzisen Lokalisierung der sonnigen Gebiete verwendet werden.In addition to recognizing objects, these techniques can also be used to determine the exact location of attributes. So z. B. are searched for sunny geographical areas, z. B. for the real estate search. Accordingly, images representing a natural light level above a threshold may be identified as observations, and images representing a natural light level below the threshold may be identified as non-observations. Relevant timestamp information may also be considered, e.g. For example, by restricting the images you are looking for to images captured at a particular time of day (eg, during the day). The image acquisition information associated with the non-observations can be used to precisely locate the sunny areas.
In einigen Beispielen können Regionen von Interessen durch ein Gitter mit einer Vielzahl von Zellen definiert werden. Beispielsweise stellt
Das Raster
Die Bilder
Einige Zellen in einem bestimmten Sichtfeld können, z. B. basierend auf der Position der Zelle im Vergleich zu einer allgemeinen Position des Objekts
In einigen Beispielen kann die Vertrauensbewertung erhöht oder weiter verfeinert werden, wenn aufeinanderfolgende Bilder überprüft werden. Wenn beispielsweise das Bild
Der normalisierte Wert, der einen Betrag angibt, der darauf hindeutet, dass die Wahrscheinlichkeit, dass sich das Objekt in einem in dem Bild dargestellten Bereich befindet, sich nach der Bilderkennung erhöht hat, wird dann mit dem initialisierten Wert für jeden Kandidatenstandort addiert oder multipliziert. Das Ergebnis für jeden Kandidatenstandort kann mit anderen Ergebnissen für andere Kandidatenstandorte verglichen werden. Wenn der sich ergebende Wert, z. B. durch einen vorgegebenen numerischen Faktor, für einen ersten Kandidatenstandort vergleichsweise höher ist als das Ergebnis für andere Standorte, kann der erste Kandidatenstandort als der Standort des Objekts bestimmt werden. In anderen Beispielen wird das Ergebnis mit einem Schwellenwert verglichen. In diesem Zusammenhang kann ein Kandidatenstandort, dessen Ergebniswert über dem Schwellenwert liegt, als ein Standort für das Objekt bestimmt werden.The normalized value indicating an amount indicating that the likelihood that the object is in an area displayed in the image has increased after the image recognition is then added or multiplied by the initialized value for each candidate location. The result for each candidate site can be compared to other results for other candidate sites. If the resulting value, e.g. B. by a predetermined numerical factor, for a first candidate location is relatively higher than the result for other locations, the first candidate location can be determined as the location of the object. In other examples, the result is compared to a threshold. In this context, a candidate site whose result value is above the threshold may be determined as a location for the object.
Während in den obigen Beispielen jedes Bild mit einem Standort und einer Richtung gespeichert ist, können in einigen Beispielen Objekte ohne Verwendung von Richtungsinformationen lokalisiert werden.
Obwohl in vielen der obigen Beispiele Bilder dahingehend beschrieben werden, dass sie von Standorten entlang einer Straße aufgenommen werden, versteht sich, dass die Bilder, die zum Lokalisieren von Objekten oder Attributen verwendet werden, von einer beliebigen Anzahl von Nicht-Straßenstandorten erfasst werden können. Beispielsweise können Bilder von Benutzern hochgeladene Fotos beinhalten, die in Parks, innerhalb von Gebäuden oder anderswo aufgenommen wurden.Although in many of the above examples images are described as being taken from locations along a road, it will be understood that the images used to locate objects or attributes may be captured by any number of non-road locations. For example, images of users may include uploaded photos taken in parks, within buildings, or elsewhere.
Wie zuvor erwähnt, kann die Berechnung der normalisierten Vertrauenspunktzahl durch Verdeckung beeinflusst werden, z. B. wenn andere Objekte die Ansicht eines Objekts von Interesse in einem Bild behindern.
Gemäß einigen Beispielen kann die Verdeckung als Ergebnis der Leistung eines Bilderkennungs-Tools auftreten. Während sich beispielsweise keine Objekte zwischen der Kamera und dem Objekt befinden, kann es vorkommen, dass das Bilderkennungs-Tool das Objekt von Interesse nicht erkennt, z. B. wenn das Objekt weit entfernt, klein und/oder nur durch wenige Pixel dargestellt ist. Dementsprechend kann die Bedeutung einer Beobachtung oder Nicht-Beobachtung vernachlässigt werden, wenn ein Objekt weit entfernt, klein, durch wenige Pixel dargestellt ist usw.According to some examples, the occlusion may occur as a result of the performance of an image recognition tool. For example, while there are no objects between the camera and the object, the image recognition tool may not recognize the object of interest, e.g. B. if the object is far away, small and / or shown only by a few pixels. Accordingly, the importance of observation or non-observation can be neglected when an object is far away, small, represented by a few pixels, and so on.
Die Falloff-Funktion kann für verschiedene Arten von Verdeckungen oder Effekten verantwortlich sein. So können z. B. bei der Anwendung der Falloff-Funktion verschiedene Verdeckungsarten addiert oder multipliziert werden. Des Weiteren kann die Falloff-Funktion auf Beobachtungen anders angewendet werden als auf Nicht-Beobachtungen. Nicht-Beobachtungen können z. B. für Verdeckungen, nicht aber für Beobachtungen abgezinst werden. Die Falloff-Funktion kann auch trainiert werden, z. B. wenn ein Standort eines bestimmten Objekts bekannt ist. Wenn die Standorte jedes Feuerhydranten in einer bestimmten Stadt bekannt sind und Bilder für die Stadt verfügbar sind, können verschiedene Berechnungen mit verschiedenen Falloff-Funktionen durchgeführt werden, und die Funktion, die ein Ergebnis produziert, das den bekannten Standorten am nächsten kommt, kann ausgewählt werden.The falloff feature may be responsible for various types of occlusion or effects. So z. For example, when using the falloff function, different types of occlusion may be added or multiplied. Furthermore, the falloff function can be applied to observations other than non-observations. Non-observations can z. B. for masking, but not for observations are discounted. The falloff function can also be trained, eg. When a location of a particular object is known. If the locations of each fire hydrant in a particular city are known and images are available to the city, various calculations can be made with different falloff functions, and the function that produces a result closest to the known locations can be selected ,
Die Wahrscheinlichkeit von Verdeckung kann mit zunehmendem Abstand zwischen einem Bilderfassungsstandort und einem Objekt von Interesse zunehmen.
Der berechnete Standort von Objekten von Interesse, der sich aus einer semantischen Suche ergibt, kann in einer beliebigen Anzahl von Anwendungen verwendet werden. Beispielsweise können die Standorte dazu verwendet werden, eine Karte zu füllen, um bestimmte Merkmale, wie z. B. Standorte von Skateparks, Hundeparks, Feuerhydranten, Straßenlaternen, Verkehrszeichen usw., darzustellen. Die Standorte können außerdem von Endbenutzern genutzt werden, die nach bestimmten Zielen suchen oder versuchen, die Landschaft einer bestimmten Nachbarschaft zu erkunden. Beispielsweise kann ein Benutzer, der ein Haus kaufen möchte, nach Kraftwerken oder Stromleitungen in der Nähe eines potenziellen Eigenheims suchen und den genauen Standort solcher Objekte in Bezug auf das Eigenheim bestimmen. Der potenzielle Hausbesitzer möchte vielleicht eine allgemeinere Suche nach Gebieten durchführen, die z. B. als „ländlich“ erscheinen, in diesem Fall können lokalisierte Objekte oder Attribute wie Bäume oder „grün“ ausgegeben werden. Ein Vermarkter kann bestimmen wollen, wo bestimmte Branchen angesiedelt sind, um Werbung für diese Branchen zu schalten. Der Vermarkter will ggf. auch wissen, welche seiner Wettbewerber in einem bestimmten Bereich werben. Ein Mitarbeiter eines Energieversorgungsunternehmens kann alle Straßenlaternen suchen und lokalisieren. Dies sind nur einige von zahlreichen möglichen Anwendungsbeispielen.The computed location of objects of interest resulting from a semantic search can be used in any number of applications. For example, the locations may be used to populate a map to identify certain features, such as a map. B. locations of skate parks, dog parks, fire hydrants, street lamps, traffic signs, etc. represent. Locations can also be used by end users looking for specific destinations or trying to explore the landscape of a specific neighborhood. For example, a user who wants to buy a home may search for power plants or power lines near a potential home, and determine the exact location of such home-related objects. The potential homeowner may wish to conduct a more general search for areas that may be, for example: For example, they may appear as "rural", in which case localized objects or attributes such as trees or "green" may be output. A marketer may want to determine where certain industries are located to advertise these industries. If necessary, the marketer also wants to know which of his competitors advertise in a particular area. An employee of a utility company can search and locate all street lights. These are just a few of many possible application examples.
Exemplarische VerfahrenExemplary procedure
Zusätzlich zu den zuvor beschriebenen exemplarischen Systemen veranschaulicht
In Block
In Block
In Block
Jedes der in den Blöcken
In Block
Durch Verwendung von Beobachtungen und Nicht-Beobachtungen zum Lokalisieren eines bestimmten Objekts oder Merkmals kann der Standort des Objekts relativ einfach und mit relativ hoher Sicherheit bestimmt werden, selbst wenn die Bilder nicht hochauflösend sind und/oder mit geringer Auflösung oder unzuverlässigen Bilderfassungsinformationen verknüpft sind. Dies kann ermöglichen, die Objektstandortidentifikation auch unter Verwendung von nicht-professionellen Inhalten, z. B. von Smartphones, auszuführen. Durch Verwendung von Informationen für Bilder, die nicht für die Aufnahme des Objekts bestimmt sind, kann die Objektstandortidentifikation verbessert werden. Darüber hinaus kann dies ohne, oder nur mit relativ geringen Berechnungskosten erreicht werden.By using observations and non-observations to locate a particular object or feature, the location of the object can be determined relatively easily and with relatively high security, even if the images are not high resolution and / or associated with low resolution or unreliable image acquisition information. This may enable object location identification also using non-professional content, e.g. B. from smartphones to execute. By using information for images that are not intended to capture the object, object location identification can be improved. In addition, this can be achieved without, or only with relatively low calculation costs.
Obwohl die vorangegangenen Beispiele in Bezug auf Bilder beschrieben werden, versteht sich, dass es sich bei jenen Bildern um Bilder im herkömmlichen Sinne, wie z. B. eine Ansammlung von Pixeln, handelt, oder dass es sich um Frames aus einem Video, Videobeobachtungen (z. B. einen Ausschnitt eines Videos), LIDAR-Bildgebung, Radar, Bildgebung, Sonarbildgebung oder auch „hörende“ Beobachtungen wie Audioaufnahmen oder Radiofrequenzempfang, handeln kann. Ebenso kann es sich bei der in diesen Beispielen durchgeführten Bilderkennung um Videoerkennung, LIDAR-Erkennung, Audioerkennung usw. handeln. Dementsprechend können Beobachtungen und Nicht-Beobachtungen, die diese oder andere Arten von Bildgebung verwenden, verwendet werden, um ein Objekt oder Attribut präzise zu lokalisieren.Although the foregoing examples are described with respect to images, it will be understood that those images are conventional images, such as images. A collection of pixels, or frames from a video, video observations (e.g., a portion of a video), LIDAR imaging, radar, imaging, sonar imaging, or even "listening" observations such as audio or radio frequency reception , can act. Likewise, the image recognition performed in these examples may be video detection, LIDAR detection, audio detection, etc. Accordingly, observations and non-observations using these or other types of imaging may be used to precisely locate an object or attribute.
Sofern nicht anders angegeben, schließen sich die meisten der vorstehenden alternativen Beispiele nicht gegenseitig aus, sondern können in unterschiedlichen Kombinationen implementiert werden, um charakteristische Vorteile zu erzielen. Während diese und andere Variationen und Kombinationen der vorstehend beschriebenen Merkmale verwendet werden können ohne vom Gegenstand, der von den Ansprüchen definiert wird, abzuweichen, sollte die vorhergehende Beschreibung der Ausführungsformen eher als Veranschaulichung und nicht als Gelegenheit angesehen werden, den von den Ansprüchen definierten Gegenstand der Erfindung zu begrenzen. Darüber hinaus sollte das Bereitstellen der hierin beschriebenen Beispiele, sowie Klauseln, die mit Begriffen wie etwa „zum Beispiel“, „einschließlich“ und dergleichen formuliert sind, nicht als Begrenzung des Gegenstands der Ansprüche auf diese spezifischen Beispiele interpretiert werden, da die Beispiele dazu dienen, nur eine der vielen möglichen Ausführungsformen zu veranschaulichen. Ferner können die gleichen Referenznummern in unterschiedlichen Zeichnungen die gleichen oder ähnliche Elemente identifizieren.Unless otherwise stated, most of the foregoing alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve characteristic advantages. While these and other variations and combinations of the features described above may be utilized without departing from the subject matter as defined by the claims, the foregoing description of the embodiments should be taken as illustrative rather than as an opportunity to address the subject matter defined by the claims To limit the invention. Moreover, the provision of the examples described herein, as well as terms formulated with terms such as "for example," "including," and the like, should not be interpreted as limiting the scope of the claims to these specific examples, as the examples serve this purpose to illustrate just one of the many possible embodiments. Furthermore, the same reference numbers in different drawings may identify the same or similar elements.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
DE202018100752.1U DE202018100752U1 (en) | 2018-02-12 | 2018-02-12 | Bayesian methodology for geospatial object / feature recognition |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
DE202018100752.1U DE202018100752U1 (en) | 2018-02-12 | 2018-02-12 | Bayesian methodology for geospatial object / feature recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
DE202018100752U1 true DE202018100752U1 (en) | 2018-03-23 |
Family
ID=61912771
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
DE202018100752.1U Active DE202018100752U1 (en) | 2018-02-12 | 2018-02-12 | Bayesian methodology for geospatial object / feature recognition |
Country Status (1)
Country | Link |
---|---|
DE (1) | DE202018100752U1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DE102018118215A1 (en) * | 2018-07-27 | 2020-01-30 | Man Truck & Bus Se | Method for updating a map of the surroundings, device for carrying out method steps of the method on the vehicle, vehicle, device for carrying out method steps of the method on the central computer, and computer-readable storage medium |
CN111950440A (en) * | 2020-08-10 | 2020-11-17 | 杭州萤石软件有限公司 | Method, device and storage medium for identifying and positioning door |
-
2018
- 2018-02-12 DE DE202018100752.1U patent/DE202018100752U1/en active Active
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DE102018118215A1 (en) * | 2018-07-27 | 2020-01-30 | Man Truck & Bus Se | Method for updating a map of the surroundings, device for carrying out method steps of the method on the vehicle, vehicle, device for carrying out method steps of the method on the central computer, and computer-readable storage medium |
DE102018118215B4 (en) * | 2018-07-27 | 2020-04-16 | Man Truck & Bus Se | Method for updating a map of the surroundings, device for carrying out method steps of the method on the vehicle, vehicle, device for carrying out method steps of the method on the central computer, and computer-readable storage medium |
US11940291B2 (en) | 2018-07-27 | 2024-03-26 | Volkswagen Aktiengesellschaft | Method for updating a map of the surrounding area, device for executing method steps of said method on the vehicle, vehicle, device for executing method steps of the method on a central computer, and computer-readable storage medium |
CN111950440A (en) * | 2020-08-10 | 2020-11-17 | 杭州萤石软件有限公司 | Method, device and storage medium for identifying and positioning door |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
DE102019000675A1 (en) | USE A MODEL BASED ON A DEEP NEURONAL NETWORK TO IDENTIFY VISUALLY SIMILAR DIGITAL IMAGES BASED ON USER-SELECTED VISUAL PROPERTIES | |
DE202010018498U1 (en) | Matching an approximately localized request image with a reference image sentence | |
DE102012218390B4 (en) | Optimizing the detection of objects in images | |
DE102019005423A1 (en) | Space-time storage or storage network for localizing a target object in video content | |
EP3550516B1 (en) | Environmental parameter based selection of a data model for recognizing an object of a real environment | |
DE202012013403U1 (en) | Capture and process adaptive images with image analysis feedback | |
DE112019001310T5 (en) | SYSTEMS AND METHODS FOR REDUCING THE AMOUNT OF DATA STORAGE DURING MACHINE LEARNING | |
DE112016002817T5 (en) | CHANGE BASED BASED IMAGE RECORDING SYSTEM | |
DE112015006255T5 (en) | Object recognition device, object recognition method, and program | |
DE202014010927U1 (en) | Reference point identification from point clouds created from geographic image data | |
DE112016005006T5 (en) | AUTOMATIC VIDEO EXECUTIVE SUMMARY | |
DE202011110876U1 (en) | Identifying plants in images | |
DE202011110874U1 (en) | System for the determination of building numbers | |
DE102013222023A1 (en) | Multiple viewing area analysis | |
DE202014010887U1 (en) | Navigating through time and space spanning geolocalized images | |
DE202014010935U1 (en) | Updates 3D models using crowd-sourced video | |
DE102022131673A1 (en) | AUTONOMOUS VEHICLE PERCEPTION MULTIMODAL SENSOR DATA MANAGEMENT | |
DE112021001971T5 (en) | AUTOMATED TERRAIN-BASED DETECTION OF DRILLING PLATFORMS AND THEIR SURROUNDINGS | |
DE202016008004U1 (en) | Automatically associate images using visual property references to related applications | |
DE112008000017T5 (en) | Imaging images with designators | |
DE202014010966U1 (en) | Geo-photo search based on the expected conditions at a location | |
DE112016001039T5 (en) | Apparatus and method for extraction of a region of interest | |
CN112668461A (en) | Intelligent supervision system with wild animal identification function | |
DE112020005732T5 (en) | GENERATION OF TRAINING DATA FOR OBJECT RECOGNITION | |
DE202018100752U1 (en) | Bayesian methodology for geospatial object / feature recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
R207 | Utility model specification | ||
R082 | Change of representative |
Representative=s name: VENNER SHIPLEY GERMANY LLP, DERepresentative=s name: VENNER SHIPLEY LLP, DE |
|
R150 | Utility model maintained after payment of first maintenance fee after three years | ||
R079 | Amendment of ipc main class |
Free format text: PREVIOUS MAIN CLASS: G06K0009620000Ipc: G06V0030190000 |
|
R151 | Utility model maintained after payment of second maintenance fee after six years |