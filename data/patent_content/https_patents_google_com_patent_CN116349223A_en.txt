CN116349223A - Sender-side geometry fusion of depth data - Google Patents
Sender-side geometry fusion of depth data Download PDFInfo
- Publication number
- CN116349223A CN116349223A CN202080106540.1A CN202080106540A CN116349223A CN 116349223 A CN116349223 A CN 116349223A CN 202080106540 A CN202080106540 A CN 202080106540A CN 116349223 A CN116349223 A CN 116349223A
- Authority
- CN
- China
- Prior art keywords
- depth
- views
- view
- image
- updated
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000004927 fusion Effects 0.000 title claims description 53
- 238000000034 method Methods 0.000 claims abstract description 94
- 230000002776 aggregation Effects 0.000 claims abstract description 36
- 238000004220 aggregation Methods 0.000 claims abstract description 36
- 238000012545 processing Methods 0.000 claims abstract description 34
- 230000015654 memory Effects 0.000 claims description 44
- 230000008569 process Effects 0.000 claims description 24
- 230000006835 compression Effects 0.000 claims description 15
- 238000007906 compression Methods 0.000 claims description 15
- 238000007781 pre-processing Methods 0.000 claims description 7
- 230000002123 temporal effect Effects 0.000 claims description 7
- 230000004044 response Effects 0.000 claims description 6
- 238000004891 communication Methods 0.000 description 18
- 238000009877 rendering Methods 0.000 description 15
- 238000004590 computer program Methods 0.000 description 9
- 238000005516 engineering process Methods 0.000 description 9
- 230000002194 synthesizing effect Effects 0.000 description 9
- 230000005540 biological transmission Effects 0.000 description 8
- 239000002131 composite material Substances 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 238000005286 illumination Methods 0.000 description 6
- 230000003993 interaction Effects 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 238000005266 casting Methods 0.000 description 5
- 238000007796 conventional method Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 5
- 230000000007 visual effect Effects 0.000 description 4
- 238000013459 approach Methods 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004888 barrier function Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 210000003128 head Anatomy 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000007726 management method Methods 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 238000004441 surface measurement Methods 0.000 description 2
- 230000003044 adaptive effect Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000000712 assembly Effects 0.000 description 1
- 238000000429 assembly Methods 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000012937 correction Methods 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 230000006837 decompression Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000007547 defect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 210000000887 face Anatomy 0.000 description 1
- 239000010408 film Substances 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- 238000007499 fusion processing Methods 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 125000006850 spacer group Chemical group 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000003786 synthesis reaction Methods 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/128—Adjusting depth or disparity
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/111—Transformation of image signals corresponding to virtual viewpoints, e.g. spatial image interpolation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/122—Improving the 3D impression of stereoscopic images by modifying image signal contents, e.g. by filtering or adding monoscopic depth cues
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/156—Mixing image signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
- H04N13/161—Encoding, multiplexing or demultiplexing different image signal components
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/261—Image signal generators with monoscopic-to-stereoscopic image conversion
- H04N13/268—Image signal generators with monoscopic-to-stereoscopic image conversion based on depth image-based rendering [DIBR]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/271—Image signal generators wherein the generated image signals comprise depth maps or disparity maps
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N7/00—Television systems
- H04N7/14—Systems for two-way working
- H04N7/15—Conference systems
Abstract
Systems and methods are described for performing operations including, with an image processing system having at least one processing device: the method includes receiving a plurality of depth views of an object, each of the plurality of depth views being captured from a respective viewpoint of the object, each of the plurality of depth views including respective depth data associated with a depth image of the object captured from the respective viewpoint, performing an aggregation operation on the plurality of depth views, and generating an image of the object from a target viewpoint based on the updated depth views, the target viewpoint being different from each of the respective viewpoints from which each of the plurality of depth views was captured.
Description
RELATED APPLICATIONS
The present application relates to application Ser. No. 16/523,247 entitled "SPATIALLY ADAPTIVE VIDEO COMPRESSION FOR MULTIPLE STREAMS OF COLOR AND DEPTH" filed on 7.26.2019, now published as U.S. Pat. No. 10,757,410, and to application Ser. No. 16/523,702 entitled "IMAGEBASED GEOMETRIC FUSION OF MULTIPLE DEPTH IMAGES USING RAY CASTING" filed on 7.26.2019, the contents of which are incorporated herein by reference in their entirety.
Technical Field
The present specification relates generally to fusion of depth data.
Background
The computing device may be configured to generate an image of the object, which may be compressed using a compression scheme and transmitted over the network. For example, multi-view video coding (MVC) may be used to compress stereoscopic video to efficiently encode video sequences captured simultaneously from multiple camera angles in a single video stream. The MVC compression scheme maintains all artifacts, including noise, missing data, etc., for each captured view. Such conventional methods preserve the original view (e.g., color and depth) and often use methods that increase or decrease video quality in order to reduce the bit rate of the image used to transmit the object over the network.
Disclosure of Invention
A system of one or more computers (e.g., an image processing system) may be configured to perform particular operations or actions by installing software, firmware, hardware, or a combination thereof on the system that alone or in combination cause the system to perform the actions. One or more computer programs may be configured to perform particular operations or acts by including instructions that, when executed by a data processing apparatus, cause the apparatus to perform the acts.
In one general aspect, a system and method are described for performing a computer-implemented method with an image processing system having at least one processing device, including the operations of: a plurality of depth views of the object are received, wherein each of the plurality of depth views is captured from a respective viewpoint of the object, and wherein each of the plurality of depth views comprises respective depth data associated with a depth image of the object captured from the respective viewpoint. The method may further include performing an aggregation operation on the plurality of depth views, wherein the aggregation operation includes generating an updated depth view corresponding to each of the plurality of depth views, each updated depth view being based on a respective viewpoint and representing a portion of the respective depth data from each of the remaining plurality of depth views of the object, and replacing each of the plurality of depth views with the corresponding updated depth view. The method may also include generating an image of the object from a target viewpoint based on the updated depth views, wherein the target viewpoint is different from each of the respective viewpoints from which each of the plurality of depth views was captured.
These and other aspects may include one or more of the following, alone or in combination. According to some aspects, the methods, systems, and computer-readable media claimed herein may include one or more (e.g., all) of the following features (or any combination thereof).
In some implementations, performing the aggregation operation results in generating missing data associated with at least one of the received plurality of depth views of the object. In some implementations, the missing data includes an occlusion region associated with at least one of the received plurality of depth views of the object. In some implementations, performing the aggregation operation results in reducing noise associated with at least one of the received plurality of depth views of the object.
In some implementations, the aggregation operation is a geometric fusion of the plurality of received depth views to generate updated depth views, wherein the aggregation operation is performed as preprocessing in response to a request to transmit the plurality of depth views over the network, and wherein the preprocessing further includes video compression of each updated depth view. In some implementations, the target viewpoint is selected to synthesize a previously uncaptured image of the object. In some implementations, the image processing system is a telepresence system configured to synthesize images from multiple target viewpoints using updated depth views. In some implementations, the plurality of depth views represent a plurality of depth maps corresponding to the captured image of the object, and generating the plurality of depth maps and replacing the plurality of depth maps with updated depth maps improves temporal coherence of the generated depth image of the object from the target viewpoint.
Implementations of the described technology may include hardware, methods, or processes on a computer-accessible medium, or computer software. The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a block diagram illustrating an example 3D content system for generating and displaying synthesized content on a display device according to an implementation described throughout this disclosure.
Fig. 2 is a block diagram of an example system for generating a depth view and for synthesizing content for rendering on a display device according to an implementation described throughout this disclosure.
Fig. 3A-3D depict examples of synthesizing novel images using sender-side geometric fusion according to implementations described throughout this disclosure.
Fig. 4 depicts an example of synthesizing regions of missing data using sender-side geometry fusion according to an implementation described throughout this disclosure.
FIG. 5 is a block diagram of an example pipeline for synthesizing novel images for rendering on a display device according to an implementation described throughout this disclosure.
Fig. 6 is a flowchart of one example of a process of performing sender-side geometric fusion for generating image content according to an implementation described throughout this disclosure.
FIG. 7 illustrates an example of a computer device and a mobile computer device that may be used with the techniques described here.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
In general, this document describes geometric fusion techniques that may be applied to objects captured by multiple cameras. Geometric fusion techniques may be used to replace multiple views of depth image data (e.g., captured depth views of an object) with updated (e.g., calculated) views of the depth image data. The updated depth view may be generated as a view of the object that contains depth data from the captured depth view and additionally contains image and/or depth information from each of any other available captured depth views of the object. One or more of the updated depth views may be used to synthesize additional (and new) views of the object by utilizing the stored depth image data and image and/or depth information associated with multiple other views of the object.
Any number of algorithms may be used to fuse the depth image data to replace each (input) depth view by a new depth view incorporating depth data information from several other depth views. The input depth view may represent the depth view prior to modification by the algorithms described herein. The algorithm described herein may replace each (input) depth view by a new depth view that incorporates depth data information from any number of other (input) depth views to accomplish missing data and/or reduce noise in the image. As used herein, a depth view represents a distance between a particular capture object and an image capture device associated with an image processing system described herein. The updated depth view may begin with the depth view and modify, merge, add and/or remove data from the depth view to generate the updated depth view.
In some implementations, updated depth views may be generated for synthesizing novel (e.g., unseen) views of objects. For example, the document includes examples related to generating novel images of objects (e.g., images captured by a user in a telepresence session, virtual content captured by a camera, physical objects captured by a camera, etc.). As used herein, a novel (e.g., unseen, new) view may include image content and/or video content that has been interpreted (e.g., synthesized, interpolated, simulated, etc.) based on one or more frames of image content (e.g., objects, depth, illumination, etc.) and/or video content captured by a camera. For example, an interpretation of image content and/or video content captured by a camera may be used in conjunction with the techniques described herein to create unseen 3D versions and views (e.g., gestures, angles, etc.) of the captured image content and/or video content.
An example technique for generating unseen 3D images may include sender-side geometric fusion applied to a set of depth views captured at a sender system. The sender-side geometric fusion may result in generating an updated depth view that includes depth data from each of any number of captured views. The fused depth views may be compressed and transmitted over a network. At the recipient system, a decompression process may be performed on the fused depth view, and a geometric fusion technique may be performed again to synthesize new content for rendering at the recipient system. Thus, two geometric fusions may be performed; the depth view is entered (e.g., captured) once (on the sender system) before transmission and the final composite view once when merging the received (sender-side fused) depth views.
Examples are described that utilize merging (e.g., fusing) of data across a particular depth view to improve the compressibility of the data to reduce network bandwidth (e.g., bits per second) associated with transmitting depth data from a sender location to a receiver location. In some implementations, the systems and methods described herein may utilize a geometric fusion algorithm to fuse multiple views of depth image data at a sender location prior to sending the fused depth image data to a receiver side in order to reduce network bandwidth resources when transmitting such data.
In some implementations, the systems and methods described herein may reduce network bandwidth (e.g., bit rate) for transmitting a particular depth view from a sender client to a receiver client, as the particular depth view may be used to generate new depth views by merging (e.g., fusing) information across the particular depth view to reduce the size of noise and/or fill occlusion regions, which in turn may reduce the bit rate at which such content is transmitted. In some implementations, occlusion regions that lack data may be filled with data from other depth views.
In some implementations, the techniques described herein may be used to synthesize images that are expressively accurate and realistic for display on a screen of a 3D display, such as used in a multi-path 3D video conference. The systems and methods described herein may use such techniques to generate and display accurate and realistic views (e.g., image content, video content) of 3D objects (e.g., users, virtual objects, physical objects, etc.). The views include unseen views that may be traditionally difficult to depict in a 3D manner.
In some implementations, the techniques described herein may be used by virtual assistant devices or other intelligent agents that may perform image processing to identify objects, recreate objects, and/or depth using the techniques described herein, and/or generate composite images from the objects.
A technical problem involved in the above-described conventional method of network transmission bit rate is that this method is ineffective in the case where there are a plurality of depth views. For example, there is considerable overhead in redundancy of depth views, i.e., the fact that points in the environment are often visible in several depth views.
In contrast to conventional approaches to solving the above-described technical problems, the technical solution provided by the systems and methods described herein includes calculating, for each captured depth view, a geometrically fused depth view (i.e., an updated depth view) based on a similarity between a depth image surface normal and a view direction associated with the depth view.
The technical advantage of the above-described technical solution is that it allows for more efficient compression of video images having multiple depth views, because the sender-side fusion technique uses information from each depth view to modify and/or improve each of the other depth views (i.e., by filling in missing or undefined regions and by reducing the size of noise associated with the view). The resulting depth views remain redundant with respect to each other, but are more compressible than conventional depth views. Multiple depth views are transmitted over the network using lower bit rates (better compressed representation) because they are modified in the sender-side system using geometric fusion. In some implementations, conventional techniques (including MVC) may then be applied subsequently (i.e., after sender-side geometry fusion) to reduce this redundancy.
Fig. 1 is a block diagram illustrating an example 3D content system 100 for generating and displaying content on a stereoscopic display device according to an implementation described throughout this disclosure. For example, the 3D content system 100 may be used by multiple users, for example, to conduct 3D video conferencing communications (e.g., telepresence). In general, the system of fig. 1 may be used to capture video and/or images of a user during a 3D video conference and use the systems and techniques described herein to capture multiple color views and depth views at a sender location (e.g., 3D system 106), perform geometric fusion techniques on the captured depth views to generate updated depth views, and transmit the views to a receiver location, where the views are used to synthesize novel views for viewers.
The system 100 may benefit from the use of the techniques described herein because such techniques may generate and display novel views of gestures, expressions, and user image portions, for example, within a video conference that accurately represent 3D depth adjustment views of objects and/or users in the video conference. The novel views (e.g., images) may be used with the techniques described herein to generate accurate textures, depths, and images of a user and/or object, which may be displayed to another user in a 3D manner, for example, via system 100.
As shown in fig. 1, a first user 102 and a second user 104 are using the 3D content system 100. For example, users 102 and 104 are using 3D content system 100 to participate in a 3D telepresence session. In such an example, 3D content system 100 may allow each of users 102 and 104 to see other highly realistic and visually consistent representations, thereby facilitating user interaction in a manner similar to each other's physical presence.
Each user 102, 104 may have a corresponding 3D system. Here, user 102 has 3D system 106, and user 104 has 3D system 108. The 3D systems 106, 108 may provide functionality related to 3D content including, but not limited to, capturing images for 3D display, processing and presenting image information, and processing and presenting audio information. The 3D system 106 and/or the 3D system 108 may constitute a collection of sensing devices integrated into one unit. The 3D system 106 and/or the 3D system 108 may include some or all of the components described with reference to fig. 2 and 8.
The 3D content system 100 may include one or more 3D displays. Here, 3D display 110 is provided for 3D system 106 and 3D display 112 is provided for 3D system 108. The 3D displays 110, 112 may use any of a variety of types of 3D display technology to provide an autostereoscopic view to a respective viewer (e.g., user 102 or user 104 herein). In some implementations, the 3D displays 110, 112 may be stand-alone units (e.g., self-supporting or hanging on a wall). In some implementations, the 3D displays 110, 112 may include or have access to wearable technology (e.g., controllers, head mounted displays, etc.). In some implementations, the displays 110, 112 may be 2D displays.
In general, a display such as displays 110, 112, etc. may provide an image approximating the 3D optical characteristics of a physical object in the real world without using a Head Mounted Display (HMD) device. In general, the displays described herein may include flat panel displays, lenticular lenses (e.g., microlens arrays), and/or parallax gratings to redirect images to multiple different viewing regions associated with the display.
In some implementations, the displays 110, 112 may include high resolution and glasses-free lenticular 3D displays. For example, the displays 110, 112 may include a microlens array (not shown) that includes a plurality of lenses (e.g., microlenses) with glass spacers coupled (e.g., bonded) to the microlenses of the display. The microlenses may be designed such that, from a selected viewing position, a left eye of a display user can view a first set of pixels and a right eye of the user can view a second set of pixels (e.g., wherein the second set of pixels is mutually exclusive from the first set of pixels).
In some example displays, there may be a single location that provides a 3D view of image content (e.g., users, objects, etc.) provided by such displays. The user can sit in a single location to experience proper parallax, minimal distortion, and realistic 3D images. If the user moves to a different physical location (or changes head position or eye gaze position), image content (e.g., the user, objects worn by the user, and/or other objects) may begin to appear less realistic, 2D, and/or distorted. The systems and techniques described herein may reconfigure image content projected from a display to ensure that a user can walk around, but still experience proper parallax and depth, low distortion rate, and realistic 3D images in real time. Accordingly, the systems and techniques described herein provide the advantage of maintaining or improving provided 3D image content and objects for display to a user, regardless of any user movement that occurs while the user is viewing the 3D display.
As shown in fig. 1, the 3D content system 100 may be connected to one or more networks. Here, network 114 is connected to 3D system 106 and 3D system 108. The network 114 may be a publicly available network (e.g., the internet) or a private network, just to name two examples. The network 114 may be wired, wireless, or a combination of both. The network 114 may include or utilize one or more other devices or systems, including but not limited to one or more servers (not shown).
The 3D systems 106, 108 may include a number of components related to the capturing, processing, sending or receiving of 3D information and/or the rendering of 3D content. The 3D systems 106, 108 may include one or more cameras for capturing image content of images to be included in the 3D presentation. Here, 3D system 106 includes cameras 116 and 118. For example, the cameras 116 and/or 118 may be disposed substantially within the housing of the 3D system 106 such that the objective lenses or lenses of the respective cameras 116 and/or 118 capture image content through one or more openings in the housing. In some implementations, the cameras 116 and/or 118 may be separate from the housing, such as in the form of a stand-alone device (e.g., having wired and/or wireless connections with the 3D system 106). Cameras 116 and 118 may be positioned and/or oriented to capture a sufficiently representative view of a user (e.g., user 102). Although the cameras 116 and 118 will generally not obscure the view of the 3D display 110 of the user 102, the placement of the cameras 116 and 118 may be arbitrarily selected. For example, one of the cameras 116, 118 may be positioned somewhere above the face of the user 102 and the other positioned somewhere below the face. For example, one of the cameras 116, 118 may be positioned somewhere on the right side of the face of the user 102 and the other on the left side of the face. For example, 3D system 108 may include cameras 120 and 122 in a similar manner. Additional cameras are also possible. For example, a third camera may be placed near or behind the display 110.
In some implementations, the 3D systems 106, 108 may include one or more depth sensors to capture depth data to be used in the 3D presentation. Such depth sensors may be considered part of a depth capture component in 3D content system 100 for characterizing a scene captured by 3D systems 106 and/or 108 to properly represent the scene on a 3D display. Further, the system may track the position and orientation of the viewer's head, enabling rendering of a 3D presentation by appearance corresponding to the viewer's current perspective. Here, the 3D system 106 includes a depth sensor 124. In a similar manner, 3D system 108 may include depth sensor 126. Any of a variety of types of depth sensing or depth capturing may be used to generate depth data.
In some implementations, an auxiliary stereoscopic depth capture is performed. For example, a scene may be illuminated using a light point, and a stereo-match may be made between two respective cameras. Such illumination may be accomplished using waves of a selected wavelength or range of wavelengths. For example, infrared light (IR) may be used. In some implementations, for example, when generating a view on a 2D device, a depth sensor may not be utilized.
For example, the depth data may include or be based on any information about the scene reflecting the distance between the depth sensor (e.g., depth sensor 124) and the object in the scene. For content in an image corresponding to an object in a scene, the depth data reflects a distance (or depth) from the object. For example, the spatial relationship between the camera and the depth sensor may be known and may be used to correlate the image from the camera with the signal from the depth sensor to generate depth data for the image.
The images captured by the 3D content system 100 may be processed and later displayed as a 3D presentation. As depicted in the example of fig. 1, a 3D image 104' of the user 104 is presented on the 3D display 110. In this way, the user 102 may perceive the 3D image 104' as a 3D representation of the user 104, and the user 104 may be remote from the user 102. The 3D image 102' is presented on the 3D display 112. In this way, the user 104 may perceive the 3D image 102' as a 3D representation of the user 102.
The 3D content system 100 may allow participants (e.g., users 102, 104) to engage in audio communications with each other and/or with other people. In some implementations, the 3D system 106 includes a speaker and microphone (not shown). For example, 3D system 108 may similarly include a speaker and a microphone. In this way, 3D content system 100 may allow users 102 and 104 to participate in 3D telepresence sessions with each other and/or with other people. In general, the systems and techniques described herein may operate with system 100 to generate image content and/or video content for display between users of system 100.
Generating image content to be displayed on telepresence system 100 may include using any number of input depth views 134 (e.g., obtained from input image 201). For example, the system 106 may capture the image 201, retrieve the depth view 134, and process the view 134 (or send the view to be processed at the server) using the geometric fusion technique 136 in order to generate the updated depth view 138. The updated depth view 138 represents a replacement of the view from each original depth view of the captured depth view 134 by a recalculated depth view that incorporates information from other depth views corresponding to the captured depth view 134. By recalculating the depth view on the sender side, the system 100 may actually reduce the bit rate of transmitting the depth view from the sender client (e.g., 3D system 106) to the receiver client (e.g., 3D system 108) by merging the information across views 134 in order to reduce the size of the noise and fill any detected occlusion regions.
Fig. 2 is a block diagram of an example system for generating a depth view and for synthesizing content for rendering on a display device according to an implementation described throughout this disclosure. The system 200 may be used as or included in one or more implementations described herein, and/or may be used to perform operations that represent one or more examples of the composition, processing, simulation, or rendering of views of image data described herein. One or more of the entire system 200 and/or various components thereof may be implemented in accordance with one or more examples described herein.
The system 200 may be configured to render a computer graphical object at a specified view given a plurality of existing views. For example, given several image views (e.g., input image 201) captured from a camera regarding a scene that includes such computer graphics objects, the target may be a new (e.g., novel) view of the synthesized scene from different viewpoints. The scene may be real, in which case the view is captured using physical color and depth sensors, or synthetic, in which case the view is captured using rendering algorithms such as rasterization or ray tracing. For real scenes, there are many depth sensing technologies, such as time-of-flight sensors, structured light-based sensors, and stereo or multi-view stereo algorithms, any and all of which may be used by the system 200. Such techniques may involve visible or infrared sensors having passive or active illumination patterns, where the patterns may be time-varying, any and all of which may be employed by the system 200.
The system 200 may include one or more 3D systems 202. In the depicted example, 3D systems 202A, 202B, through 202N are shown, where index N indicates any number. The 3D system 202 may provide capture of visual and audio information for 2D or 3D presentation and forward the 2D or 3D information for processing. Such information may include an image of the scene, depth data about the scene, and audio from the scene. For example, 3D system 202 may be used as or included in system 106 and 2D/3D display 110 (fig. 1).
The system 200 may include a plurality of cameras, as shown by camera 204. Any type of light sensing technology may be used to capture images, such as the type of image sensor used in conventional digital cameras. The cameras 204 may be of the same type or of different types. For example, the camera location may be placed in any location on a 3D system (e.g., system 106). In some implementations, a camera (e.g., camera sensor) may be located at the periphery of the display 212 (e.g., 112), and such a camera may be used to synthesize novel views from near the center of the display 212 (e.g., 112).
The system 202A includes a depth sensor 206. In some implementations, the depth sensor 206 operates by projecting IR signals onto a scene and detecting response signals. For example, depth sensor 206 may generate and/or detect beams 128A-B and/or 130A-B. In some implementations, the depth sensor 206 is an optional component, for example, in 2D video conferencing applications that do not utilize depth sensing. The system 202A also includes at least one microphone 208 and a speaker 210. In some implementations, the microphone 208 and speaker 210 may be part of the system 106.
In addition, the system 202 includes a 3D display 212 that can present 3D images. In some implementations, the 3D display 212 may be a stand-alone display, while in some other implementations, the 3D display 212. In some implementations, the 3D display 212 operates using parallax barrier technology. For example, a parallax barrier may comprise parallel vertical stripes of a substantially opaque material (e.g. an opaque film) placed between the screen and the viewer. Different portions of the screen (e.g., different pixels) are viewed by the respective left and right eyes due to parallax between the eyes of the viewer. In some implementations, the 3D display 212 operates using lenticular lenses. For example, alternating rows of lenses may be placed in front of the screen, with the rows directing light from the screen to the left and right eyes of the viewer, respectively.
The system 200 includes an image processing system 214 that may perform certain tasks of data processing, data modeling, depth image management and modification, compression management, data coordination, and/or data transmission. System 214 may represent a server or client computing system having any or all of the components described with reference to fig. 7 and/or 3D system 202.
In some implementations, the system 214 may be configured to render an image of the object. The system 214 includes a network interface 216, one or more processing units 218, and a memory 220. Network interface 216 includes, for example, an ethernet adapter or the like, for converting electronic and/or optical signals received from network 222 into electronic form for use by system 214. The set of processing units 218 includes one or more processing chips and/or assemblies. Memory 220 includes volatile memory (e.g., RAM) and nonvolatile memory such as one or more ROMs, magnetic disk drives, solid state drives, and the like. Together, the set of processing units 218 and the memory 220 form a control circuit configured and arranged to perform the various methods and functions described herein.
In some implementations, one or more components of the system 214 can include or have access to a processor (e.g., the processing unit 218) configured to process instructions stored in the memory 220. Such instructions may be executed, for example, by depth image manager 224, optional color image manager 226, depth image identification manager 228, depth image generator 230, and compression manager 232. Further, as shown in fig. 2, the memory 220 is configured to store various data, which is described with respect to a corresponding manager using such data.
The depth image manager 224 is configured to receive depth image data (e.g., the depth view 134). In some implementations, the depth image manager 224 receives the depth view 134 over the network 222 via the network interface 216. In some implementations, the depth image manager 224 receives the depth view 134 from a local storage device (e.g., hard disk drive, flash drive, storage disk, etc.).
The depth view 134 represents a plurality of depth images (e.g., depth maps) of depth images 134 (1.) of..134 (N) shown as objects (e.g., virtual objects, physical objects, users, etc.). An example of a depth image, such as depth image 134 (1), may be seen in fig. 1. Each depth image represents a mapping of distances (or depths) along a line from at least one camera to pixels on the surface of the object. At least one camera is oriented at an angle relative to the object that is indicated by the viewpoint from which the depth image is captured. In the examples described herein, there are three given depth images of an object to be fused into a new depth image captured from a specified viewpoint. In some implementations, there may be fewer or more than three depth images to be fused.
In some implementations, the system 214 may optionally be configured to compress color images. Color image manager 226 is configured to receive color image data 227. In some implementations, color image manager 226 receives color image data 227 over network 222 via network interface 216. In some implementations, color image manager 226 receives color image data 227 from a local storage device (e.g., hard disk drive, flash drive, storage disk, etc.).
The color image data 227 represents a plurality of color images 227 (1.) of the object. Each depth image (e.g., color image 227 (1)) represents a mapping of distances (or depths) along a line from at least one camera to pixels on the surface of the object. For example, each pixel value in the depth image encodes the location of the nearest object surface intersected by a line (e.g., a ray) passing through that pixel from the center of the camera. Such encoding may be implemented as distance or depth. The distance measurement is along the length of a line (ray) from the camera center through each pixel to the surface point. The depth measures the coordinates of surface points along the camera principal axis (commonly referred to as the z-axis of the depth camera). At least one camera is oriented at an angle relative to the object that is indicated by the viewpoint from which the depth image is captured.
The depth image recognition manager 228 is configured to recognize a depth image of a plurality of depth images associated with the color image. In some implementations, the depth image recognition manager 228 recognizes the depth image (e.g., the depth view 134 (1)) having a projection center closest to the projection center of the color image 227 (1) as the depth image associated with the color image (e.g., the color image 227 (1)).
In some implementations, a viewpoint may be received at system 214. The view data represents the orientation of the target view from which the new depth image data may be generated. In some implementations, the viewpoint data includes a camera matrix. In some implementations, the camera matrix is a 3 x 4 matrix representing a mapping from 3D camera coordinates to 2D image coordinates.
The depth image generator 230 may be a manager configured to generate a depth image of an object captured from a target viewpoint represented by viewpoint data from which a particular depth image may be captured.
In some implementations, a ray casting manager (not shown) may be used with system 200 to generate ray data based on a 3D scene represented by an image from a perspective of a target viewpoint. For example, the light projection manager may project a respective light for each pixel of the image. In some implementations, the ray casting manager uses a parallel process, i.e., multiple threads and/or processors are used simultaneously to cast rays. In such an implementation, the operations on each ray that has been projected are similarly performed in parallel. In some implementations, the light projection manager projects light in parallel on pixels of the image using warping. In some implementations, the ray casting manager uses an OpenGL fragment shader to cast rays in parallel on pixels of an image.
Ray data (not shown) represents rays used to form an image of a 3D scene including objects. Each ray represented by ray data is associated with a pixel of an image. Light represented by the light data is emitted from a viewpoint source (e.g., a camera) to pixels of an image. In some implementations, a Signed Distance Value (SDV) manager (not shown) may be part of the system 200 and may be configured to generate SDV data by calculating an SDV for each of the depth images along each ray at a different location along the ray. To achieve this, in some implementations, the SDV manager is configured to iteratively step along the ray until a stop condition is met. In some implementations, the stop condition is that the position of the next step intersects the surface of the object associated with the depth image. In some implementations, the step size along the ray is proportional to the absolute value of the distance between the current position of the ray along the step and the surface of the object. In this way, the step size becomes finer as the position approaches the surface of the object associated with the viewpoint of the depth image. In some implementations, if the absolute value of the SDV is greater than a certain cutoff threshold, the SDV is replaced with the specified value. In some implementations, the specified value is undefined. In some implementations, the SDV data 162 represents an SDV (signed distance value) along each ray for each depth image. As a convention, the sign of an SDV herein is positive for the location of light rays between the viewpoint source and the surface associated with the viewpoint, and negative for locations along the light rays outside the surface.
In an example, a point along a ray is denoted as p=o+αv, where o denotes the target viewpoint, v is the unit view direction of the ray, and the scalar α encodes the parameter position along the ray. Given a ray point p, for each depth image j, we transform p into the camera space of the depth image, calculate perspective projections to determine the pixel coordinates of the point p in the camera image, and sample the stored depth values. In some implementations, weight values are also stored. Subtracting the depth value from the z-coordinate of the camera spatial point to obtain a signed distance value s j . Note that, as described above, if the point p is located in front of the forefront surface visible from the depth camera, s j Positive, otherwise negative.
A root lookup manager (not shown) is configured to perform a root lookup operation to generate a root of the aggregated SDV along each ray. In some implementations, the root lookup operation includes determining where the aggregated SDV changes sign (e.g., from positive to negative), and performing a binary search operation to place the root (e.g., where the aggregated SDV along the ray is zero or some other constant). The root location data may represent the root of the aggregated SDV along each ray as determined via the root lookup operation described above. The depth image generator 230 generates an object surface based on the root represented by the root position data along these lines. In some implementations, the depth image generator 230 performs interpolation operations to produce a continuous surface from discrete roots.
The compression manager 232 is configured to perform a compression operation on each depth view according to the determined depth view to generate compressed data 234. The compressed data 234 is transmitted to the recipient where it is decompressed (decoded) and fused together to create a composite view 240.
Conventional video systems enable real-time transmission of video by utilizing hardware video encoding and decoding of multiple video streams (e.g., three depth views and four color views). These views are fused in the recipient to create a low-latency left/right view based on the tracked eyes of the recipient. For such systems, the general problem is to reduce the network transmission bit rate required by the video stream, in particular by multiple color views. Conventional methods of reducing the network transmission bit rate include selectively increasing or decreasing the quality of video in particular spatial regions of the frame. For example, in a video conferencing scenario, such conventional methods may be used to keep more detail on the user's face while allowing other portions of the frame to have reduced quality. Other conventional approaches attempt to maintain the quality of each of a plurality of color images; this is done by adapting the quality of the view images (e.g. by compressing the compression quality in space) with the purpose of allowing a high quality rendering of the final object as a combination of compressed views.
The system 200 may perform the geometric fusion technique 136. The technique 136 may be used in the context of computer graphics rendered from an existing view. For example, given several depth images (and/or color images) captured from a camera regarding a scene, it is desirable to synthesize a new view of the scene from different viewpoints. The scene may be physical (in which case the view is captured using a physical color sensor and a depth sensor) or synthetic (in which case the view is captured using a rendering algorithm such as rasterization or ray tracing). For physical scenes, there are many depth sensing technologies, such as time-of-flight sensors, structured light based sensors, and stereo (or multi-view stereo) algorithms. These techniques may involve visual or infrared sensors, optionally with passive or active illumination patterns, where the patterns may vary in time.
The system 200 may combine depth information and/or other image information from multiple views into a consistent representation of the scene so that the reconstructed scene may be rendered with the correct inter-surface occlusion and parallax from the specified viewpoint. In a physical scene, both the depth sensor and the color sensor create noise data. In addition, the acquired depth image may have large errors, especially near depth discontinuities such as contours (silhouettes). Thus, it is desirable to adaptively change the importance given to different views when merging them, e.g. if the scene surface is visible from another view in the direction of the correction plane (head-on), then less preference is given to views that see the scene surface obliquely. The geometric fusion technique used by system 200 may include merging multiple depth views into a consistent representation from a particular view (i.e., as represented using a new depth image from the particular view). Thus, a geometrical fusion may be performed on the sender side to merge all depth views, generating a new depth image for each original depth view. Furthermore, a geometric fusion technique may be performed on the receiver side to merge all depth views to generate depth images from novel views (e.g., for left and right eyes).
In some implementations, geometric fusion techniques may include techniques that aggregate information together based on a particular geometry. In some implementations, the geometry fusion technique may use multiple sets of overlapping surface measurements (e.g., as found in the depth view/map) to generate an updated depth view/map that includes data from other depth views/maps having overlapping surface measurements.
In operation, a system with system 200 may perform sender-side geometric fusion applied to a set of views captured at a sender system. Sender-side geometric fusion may result in generating updated views that include depth data from each captured view. At the recipient system, the geometric fusion technique may again be performed to synthesize novel content for rendering at the recipient system. Thus, two geometric fusions can be made; each input (e.g., captured) depth view (on the sender system) is once before transmission and the final composite view is once when merging the received (sender-side fused) depth views.
The above-described exemplary components are described as being implemented in system 214, which may communicate with one or more 3D systems 202 over network 222 (which may be similar or identical to network 114 in fig. 1). In some implementations, the components depicted in memory 220 may alternatively or additionally be implemented in some or all of 3D system 202. For example, the above-described methods and/or processes may be performed by a system that initiates 3D information before forwarding the 3D information to one or more receiving systems.
The updated depth view 138 represents a replacement of the view from each original depth view of the captured depth view 134 by a recalculated depth view that incorporates information from other depth views corresponding to the captured depth view 134. Re-computing the depth view on the sender side may include merging information across the depth view 134 in order to reduce the size of noise and fill any detected occlusion regions, as described in further detail below.
The composite view 240 represents a 3D stereoscopic image of a particular object (e.g., user image 104') having appropriate parallax and viewing configurations for both eyes associated with a user accessing a display (e.g., display 212) based at least in part on sender-side geometry fusion, as described herein.
In some implementations, the processor 218 may include (or communicate with) a Graphics Processing Unit (GPU). In operation, a processor may include (or access memory, storage, and other processors (e.g., CPUs)). To facilitate graphics and image generation, the processor may communicate with a GPU to display images on a display device (e.g., display device 212). The CPU and GPU may be connected via a high-speed bus (e.g., PCI, AGP, or PCI-Express). The GPU may be connected to the display through another high speed interface (e.g., HDMI, DVI, or DisplayPort). Typically, GPUs may render image content in pixels. The display device 212 may receive image content from the GPU and may display the image content on a display screen.
Fig. 3A-3D depict examples of synthesizing novel images using sender-side geometric fusion according to implementations described throughout this disclosure. Fig. 3A includes a depth map 302 of a user's view 304 a. Depth map 302 represents a depth image of a seated user from a first oblique viewpoint. Fig. 3B includes depth map 306 of view 304B of the same seated user. Depth map 306 is a depth image of a seated user from a right angle viewpoint. Fig. 3C is a depth map 308 of view 304C of the same seated user. The depth map 308 is a depth image of a seated user from a second oblique viewpoint.
In the depth maps 302, 306, and 308, the depth values are shown in gray levels ranging from dark (indicating small depth values) to light (indicating large depth values). White indicates background (substantially infinite depth). Black indicates unknown or undefined depth (e.g., missing data, noise, etc.).
The system 200 may use the depth maps 302, 306, and 308 to generate an updated depth view (not shown) that may be used to generate a resulting depth image 310, as shown in fig. 3D. For example, depth data from each of the depth maps 302, 306, and 308 may be used to generate a new (e.g., updated) depth view for each originally captured depth view. In some implementations, the updated depth view represents merged information from multiple depth views (e.g., views 302, 306, and 308). In some implementations, the updated depth view includes depth information from multiple depth views, where particularly low quality depth data is determined and removed. The updated depth view may be used to generate (e.g., synthesize) a final depth image 310.
The depth image 310 may be generated from a specified viewpoint. For example, the designated viewpoint of the depth image 310 for fusion faces directly in front of the user. In some implementations, the above-described ray casting method may be used to generate the depth image 310, which uses significantly less resources than previous volume-based techniques and may be performed in real-time. This is useful in applications such as video conferencing.
Fig. 4 depicts an example of synthesizing regions of missing data using sender-side geometry fusion according to an implementation described throughout this disclosure. Performing sender-side geometry fusion may reduce network bandwidth for transmitting updated versions of the originally captured depth view.
As shown in fig. 4, a first input depth map 402 (e.g., a view representing a depth image 134 (1)) is shown. Depth map 402 represents the captured depth data. Similarly, depth map 404 and depth map 406 represent other depth views, such as depth images 134 (2) and 134 (N).
As shown, each of the depth maps 402, 404, and 406 includes noise (e.g., blur and illumination defects) and areas of missing data (e.g., dark shadows and black areas) due to the 3D depth estimation process performed on a particular input image (e.g., 201). The dashed line shape highlights some examples of such noise and missing data. In some implementations, the missing data may be represented as black values in the input depth view shown by depth maps 402, 404, and 406. Furthermore, the noise and missing data often vary in time (i.e., from one time frame to the next), making it difficult to efficiently compress the depth data stream.
Thus, to generate a new (e.g., novel) image using the updated depth data, system 200 may recalculate the depth view for each depth map that considers (e.g., combines) depth data from other views, as indicated by arrow groups 408, 410, and 412. For example, sender-side geometry fusion techniques may be applied to depth maps 402, 404, and 406 to generate updated depth maps/views as alternative data sources to the original captured depth maps (e.g., depth maps 402, 404, and 406). Alternative depth maps (shown here as maps 414, 416, and 418) may be used for depth information for any of the views shown in conventional depth maps 402, 404, and 406. In short, the replacement views 414-418 reduce the necessary bandwidth for transmitting depth map/data by applying a geometric fusion technique on the sender side to replace each original depth view by incorporating a recalculated depth view from the information of two or more other available depth views.
As shown in views 414, 416, and 418, the above-described process may provide the effect of reducing the bit rate of transmitting depth views from a sender system (e.g., a computing system) to a receiver client by merging cross-view information in order to reduce the size of noise and fill occlusion regions. Each updated depth view 414, 416, and 418 has a corrected view without occlusion regions and without noise indicated in the corresponding captured depth views 402, 404, and 406.
For example, as shown by updated depth views 414, 416, and 418 (representing sender-side fused depth views), areas of missing data (black in the uplink) have been filled with data from other views 402, 404, and 406. Furthermore, the size of noise is reduced in the updated depth views 414-418. Furthermore, temporal continuity is improved, which may help with certain compression schemes, such as video codecs h.264, VP9, h.265, and AVI, to name a few.
For example, when a sender-side geometrically fused (i.e., updated) depth view is received on a receiver system, the geometry fusion process may be performed again using the received three updated depth views in order to synthesize an image from a new viewpoint.
FIG. 5 is a block diagram of an example pipeline 500 for synthesizing novel images for rendering on a display device according to an implementation described throughout this disclosure. Here, three input depth views 502, 504, and 506 (e.g., similar to representations 134 (1), 134 (2), and 134 (N)) may be captured by system 202, for example, during a videoconference session. The depth view may represent depth data and/or color data captured at the sender location. The geometric fusion technique 508 may be performed on each of the three depth views 134 (1), 134 (2), and 134 (N) to generate three respective updated depth views 510, 512, and 514. In general, sender-side geometry fusion technique 508 may include an aggregation operation that combines portions of depth views from any number of captured depth views. The sender-side geometric fusion may result in the output of a plurality of updated depth views modified from the originally received depth view, wherein the modification improves the quality of the depth image data stored in the particular depth view relative to the subsequent synthesis of an image representing an object in the particular depth view using the depth view. The sender-side geometry fusion may perform an aggregation operation to remove noise, complete missing data or occluded image portions, improve data compression, and improve temporal consistency, to name a few examples.
Upon completion of the fusion 508, the updated depth views 510, 512, and 514 may be compressed and transmitted 516 over the network. For example, 3D system 202A may transmit compressed depth views 510, 512, and 514 to 3D system 202B. For example, 3D system 202B may decompress the received depth view to utilize such view when rendering content on a display. In particular, system 202B may use decompressed depth views 518, 520, and 522 to generate final rendered view 526 by performing another geometric fusion 524.
For example, on the recipient system (e.g., system 202B), geometric fusion is performed using the three received depth views 518, 520, and 522 to again synthesize a novel view of the user, as shown in view 526. Thus, the geometry fusion technique is applied twice-once for each input depth view (on the sender side) before transmission, and once for the final output view when merging the received (sender side fused) depth views. Geometric fusion generates a smoother, more complete coverage of the surface of the original depth view of the object than conventional systems.
In some implementations, the image processing system is either system 214 or system 202, either of which may be a side of a telepresence system configured to synthesize images from multiple target viewpoints using updated views and/or resulting generated depth images. For example, a telepresence system operating on system 214 or system 202 may use a user's previously captured and modified depth views to generate a novel image of the user in real-time. Thus, a target viewpoint may be selected to synthesize a previously uncaptured image of the user (e.g., or other object in the scene).
In some implementations, the geometric fusion is an aggregate operation performed using all received depth views to generate updated depth views, and such operation may be performed as preprocessing in response to a request to transmit multiple depth views over a network. The preprocessing may include image and/or video compression of each updated depth view.
Fig. 6 is a flowchart of one example of a process 600 for performing sender-side geometric fusion for generating image content according to an implementation described throughout this disclosure. Briefly, the process 600 may provide an example of modifying a captured depth image to generate a novel view of an object (e.g., not previously captured by a camera). For example, process 600 utilizes the systems and algorithms described herein to perform depth view aggregation operations in order to generate a particular image from a particular viewpoint. In general, the described process 600 may be performed on image content, video content, virtual content, UI elements, application content, or other camera captured content.
In general, systems 100, 200, and/or 700 may be used in the description and execution of process 600. In some implementations, each of systems 100, 200, and/or 700 may represent a single system. In some implementations, the telepresence system described in system 202 may perform the operations of the claims. In some implementations, the system 214 accessed via the system 202 may instead perform the operations of the claims. Process 600 is a computer-implemented method of performing operations with an image processing system (e.g., system 214 or 3D system 202) having at least one processing device.
At block 602, the process 600 includes receiving a plurality of depth views of an object. For example, the system 214 may capture, receive, or otherwise obtain an input image 201, which may include a depth view 134 (e.g., including depth images/maps 134 (1) corresponding to depth maps 302, 306, and 308). Each of the plurality of depth views 134 may be captured from a respective viewpoint of an object (e.g., a user). Each of the plurality of depth views may include respective depth data associated with one depth image of the object captured from a respective viewpoint. For example, the first depth view 304a shown in the depth map 302 may be associated with the depth image 134 (1). In some implementations, the first depth view may also be associated with the color image 227 (1). Typically, the first depth view corresponds to an actually captured input image 201 captured, for example, by an onboard camera.
At block 604, the process 600 includes performing an aggregation operation on the plurality of depth views 134. The aggregation operation may include performing sender-side geometry fusion on the depth view 134, as described throughout this disclosure. The plurality of depth views 134 may represent a plurality of depth maps (e.g., 302, 306, and 308) corresponding to the captured image 201 of the object, the plurality of depth maps corresponding to the depth image 134 (1.) a. 134 (N.).
In some implementations, the aggregation operation may include generating at least an updated depth view 138 corresponding to each of the plurality of depth views 134 (e.g., views 304a, 304b, and 304 c), as shown in block 606. Each updated depth view 138 may be based at least in part on the respective viewpoint from which the original depth view was captured. The updated depth view 138 for each depth view 134 may represent portions of the respective depth data from each of the remaining portions of the plurality of depth views 138 (e.g., from the images 134 (1.) 134 (N.) represented by the depth maps 302, 306, and 308). For example, portions of other depth data from other images of the same object may include other angles of the object (e.g., pose, viewpoint, etc.), other illumination of the object, or other information related to the depth data of the object as determined by system 214 (or system 202).
In some implementations, the process 600 includes replacing each of the plurality of depth views 134 with a corresponding updated depth view 138, as shown in block 608. For example, the plurality of depth views 134 may represent a plurality of depth maps (e.g., 302, 306, and 308), and the system 214 may generate the plurality of depth maps 302, 306, and 308 upon completion of sender-side geometric fusion (e.g., an aggregation operation), and replace the plurality of depth maps 302, 306, and 308 with updated depth maps (as shown by depth views 414, 416, and 418 of fig. 4).
In some implementations, an aggregation operation (e.g., replacement of depth views) may be performed to improve temporal coherence of the generated depth image of the object from the target viewpoint. The updated depth view includes more information than any of the single captured depth views. Thus, an aggregation operation may be performed to smooth certain resulting images that utilize updated depth views, as such images may be generated with more information, higher quality information, and/or information that satisfies certain depth thresholds.
In some implementations, an aggregation operation may be performed to generate missing (or occluded) data associated with at least one of the received plurality of depth views of the object. For example, the depth view shown in depth map 308 may be missing the left side of the user's face because the capture angle of the original depth view is from the right side of the user's face. Thus, the system 214 may utilize image data from the left side of the face of the depth map 302 using an aggregation operation, e.g., geometrically merge depth view information, in order to generate missing data that may represent occlusion regions in one or more of the other depth maps/depth views available to the system 214.
Occlusion regions may occur in systems such as system 200 (e.g., a stereo-based depth inference system), for example, when computing depth images from two relatively close images (e.g., 15cm apart) using a stereo algorithm. The depth image created by the stereo algorithm is calculated with respect to one of the two captured images. The stereoscopic algorithm may fail when the surface is above a tilt angle threshold with respect to the two image sensors, or when the surface is viewable by one of the two image sensors but not the other of the two image sensors. Such visibility constraints may lead to occlusion problems.
In some implementations, replacement of the depth view may be performed to improve video compressibility of the depth view (e.g., less storage space, faster speed, etc.) before the depth view is sent over the network to the recipient system. In some implementations, replacement of the depth view (i.e., via an aggregation operation) may be performed to reduce noise associated with at least one of the received plurality of depth views of the particular object.
At block 610, the process 600 includes generating an image of an object from a target viewpoint based on the updated depth view. For example, image 526 may be generated from a target viewpoint (e.g., a previously uncaptured viewpoint of the user shown in image 526). That is, the target viewpoint may be different from each of the respective viewpoints from which each of the plurality of depth views (e.g., shown by depth maps 302, 306, and 308) was captured.
In some implementations, the image processing system is system 214 or system 202, either of which may be a side of a telepresence system configured to synthesize images from multiple target viewpoints using updated depth views. For example, a telepresence system operating on system 214 or system 202 may use a user's previously captured and modified depth views to generate a novel image of the user in real-time. Thus, a target viewpoint may be selected to synthesize a previously uncaptured image of the user (e.g., or other object in the scene).
Fig. 7 illustrates an example of a computer device 700 and a mobile computer device 750 that may be used with the described technology. Computing device 700 may include a processor 702, a memory 704, a storage device 706, a high-speed interface 708 connected to memory 704 and a plurality of high-speed expansion ports 710, a low-speed interface 712 connected to a low-speed bus 714 and storage device 706. The components 702, 704, 706, 708, 710, and 712 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 702 may process instructions for execution in the computing device 700, including instructions stored in the memory 704 or on the storage device 706, to display graphical information for a GUI on an external input/output device, such as a display 716 coupled to the high-speed interface 708. In some implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. In addition, multiple computing devices 700 may be connected, with each device providing a portion of the necessary operations (e.g., as a server bank, a group of blade servers, or a multiprocessor system).
The storage device 706 is capable of providing mass storage for the computing device 700. In one embodiment, the storage device 706 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. The computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that when executed perform one or more methods, such as the methods described herein. The information carrier may be a computer-readable medium or a machine-readable medium, such as the memory 704, the storage device 706, or memory on processor 702.
The high speed controller 708 manages bandwidth-intensive operations for the computing device 700, while the low speed controller 712 manages lower bandwidth-intensive operations. This allocation of functions is merely exemplary. In one embodiment, the high-speed controller 708 is coupled to the memory 704, the display 716 (e.g., via a graphics processor or accelerator), and to the high-speed expansion port 710, which high-speed expansion port 710 may house various expansion cards (not shown). A low speed controller 712 may be coupled to the storage device 706 and to a low speed expansion port 714. The low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device, such as a switch or router, for example, through a network adapter.
As shown in the figures, computing device 700 may be implemented in a number of different forms. For example, it may be implemented as a standard server 720, or multiple times in a group of such servers. In addition, it may be implemented as part of a rack server system 724. Furthermore, it may be implemented in a personal computer such as a laptop computer 722. Alternatively, components from computing device 700 may be combined with other components in a mobile device (not shown), such as device 750. Each of these devices may contain one or more computing devices 700, 750, and the entire system may be made up of multiple computing devices 700, 750 in communication with each other.
The processor 752 may execute instructions in the computing device 750, including instructions stored in the memory 764. The processor may be implemented as a chipset of chips that include separate multiple analog and digital processors. The processor may provide, for example, for collaboration of other components of the device 750, such as control of a user interface, applications running through the device 750, and wireless communication through the device 750.
The processor 752 may communicate with a user through a control interface 758 and a display interface 756 coupled to a display 754. The display 754 may be, for example, a TFTLCD (thin film transistor liquid crystal display) display or an OLED (organic light emitting diode) display or other suitable display technology. The display interface 756 may comprise appropriate circuitry for driving the display 754 to present graphical and other information to a user. The control interface 758 may receive commands from a user and convert them for submission to the processor 752. In addition, an external interface 762 may communicate with processor 752, thereby implementing near field communication of device 750 with other devices. For example, in some embodiments, the external interface 762 may provide wired or wireless communication, and multiple interfaces may be used.
The memory may include, for example, flash memory and/or NVRAM memory, as described below. In one embodiment, the computer program product may be tangibly embodied in an information carrier. The computer program product contains instructions that when executed perform one or more methods, such as the methods described above. The information carrier may be a computer-readable medium or a machine-readable medium, such as the memory 764, expansion memory 784, or memory on processor 752, for example, as received by transceiver 768 or external interface 762.
The device 750 may also communicate audibly using an audio codec 760, and the audio codec 760 may receive speech information from a user and convert it to usable digital information. Likewise, audio codec 760 may generate audible sound for a user, such as through a speaker, e.g., in a headset of device 750. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.), and may also include sound generated by applications operating on device 750.
As shown in the figures, computing device 750 may be implemented in a number of different forms. For example, it may be implemented as a cellular telephone 780. It may also be implemented as part of a smart phone 782, personal digital assistant, or other similar mobile device.
Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These different implementations may include implementations in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with a user; for example, feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and may receive input from a user in any form, including acoustic, speech, or tactile input.
The systems and techniques described here can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an embodiment of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN), a Wide Area Network (WAN), and the Internet.
The computing system may include clients and servers. The client and server are substantially remote from each other and typically interact through a communication network. The relationship between client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
In some embodiments, the computing device shown in fig. 7 may include a sensor that interfaces with a virtual reality or headset (VR headset/AR headset/HMD device 790). For example, one or more sensors included on computing device 750 or other computing devices shown in fig. 7 may provide input to AR/VR headset 790 or, in general, to an AR/VR space. The sensors may include, but are not limited to, touch screens, accelerometers, gyroscopes, pressure sensors, biometric sensors, temperature sensors, humidity sensors, and ambient light sensors. The computing device 750 may use the sensors to determine an absolute position of the computing device in the AR/VR space and/or a detected rotation, which is then used as an input to the AR/VR space. For example, computing device 750 may be incorporated as a virtual object into an AR/VR space such as a controller, laser pointer, keyboard, weapon, etc. Locating the computing device/virtual object by the user when incorporated into the AR/VR space may allow the user to locate the computing device to view the virtual object in the AR/VR space in some manner.
In some embodiments, one or more input devices included on or connected to computing device 750 may be used as inputs to the AR/VR space. The input device may include, but is not limited to, a touch screen, keyboard, one or more buttons, a touch pad, a pointing device, a mouse, a trackball, a joystick, a camera, a microphone, an earphone or earbud with input capabilities, a game controller, or other connectable input devices. When incorporating a computing device into an AR/VR space, a user interacting with an input device included on computing device 750 may cause certain actions to occur in the AR/VR space.
In some embodiments, one or more output devices included on computing device 750 may provide output and/or feedback to a user of AR/VR headset 790 in an AR/VR space. The output and feedback may be visual, tactile or audio. The output and/or feedback may include, but is not limited to, rendering an AR/VR space or virtual environment, vibrating, turning on and off or flashing and/or flashing one or more lights or flashes, sounding an alarm, playing a ring tone, playing a song, and playing an audio file. Output devices may include, but are not limited to, vibration motors, vibration coils, piezoelectric devices, electrostatic devices, light Emitting Diodes (LEDs), flashlights, and speakers.
In some embodiments, computing device 750 may be placed within AR/VR head-mounted device 790 to create an AR/VR system. The AR/VR headset 790 may include one or more positioning elements that allow the computing device 750 (such as the smart phone 782) to be placed in position within the AR/VR headset 790. In such an embodiment, the display of the smart phone 782 may render a stereoscopic image representing an AR/VR space or virtual environment.
In some embodiments, computing device 750 may appear as another object in a computer-generated 3D environment. User interaction with the computing device 750 (e.g., rotation, shaking, touching of a touch screen, sliding of a finger across a touch screen) may be interpreted as interaction with objects in the AR/VR space. As just one example, the computing device may be a laser pointer. In such examples, computing device 750 appears as a virtual laser pointer in a computer-generated 3D environment. As the user manipulates computing device 750, the user in the AR/VR space sees the movement of the laser pointer. The user receives feedback from interactions with the computing device 750 in an AR/VR environment on the computing device 750 or the AR/VR headset 790.
In some embodiments, computing device 750 may include a touch screen. For example, a user may interact with the touch screen in a particular manner that may mimic what happens on the touch screen and what happens in the AR/VR space. For example, a user may zoom content displayed on a touch screen with a pinch-type action. Such pinch-type actions on the touch screen may result in the information provided in the AR/VR space being enlarged. In another example, a computing device may be rendered as a virtual book in a computer-generated 3D environment. In the AR/VR space, the pages of the book may be displayed in the AR/VR space and sliding the user's finger across the touch screen may be interpreted as flipping/turning the pages of the virtual book. As each page turns/flips, in addition to seeing the page content changes, audio feedback may be provided to the user, such as the sound of turning pages in a book.
In some embodiments, one or more input devices (e.g., mouse, keyboard) other than the computing device may be rendered in a computer-generated 3D environment. The rendered input device (e.g., rendered mouse, rendered keyboard) may be used for rendering in the AR/VR space to control objects in the AR/VR space.
Furthermore, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. Further, other steps may be provided to, removed from, and added to, or removed from the illustrated flow. Accordingly, other embodiments are within the scope of the following claims.
Claims (20)
1. A computer-implemented method of performing operations with an image processing system having at least one processing device, the operations comprising:
receiving a plurality of depth views of an object, each depth view of the plurality of depth views being captured from a respective viewpoint of the object, each depth view of the plurality of depth views comprising respective depth data associated with a depth image of the object captured from the respective viewpoint,
performing an aggregation operation on the plurality of depth views, the aggregation operation comprising:
generating an updated depth view corresponding to each of the plurality of depth views, each updated depth view being based on the respective viewpoint and representing a portion of the respective depth data from each remaining depth view of the plurality of depth views of the object; and
replacing each depth view of the plurality of depth views with a corresponding updated depth view; and
an image of the object is generated based on the updated depth views from a target viewpoint that is different from each of the respective viewpoints from which each of the plurality of depth views was captured.
2. The method of claim 1, wherein performing the aggregation operation results in generating missing data associated with at least one depth view of the received plurality of depth views of the object.
3. The method of claim 2, wherein the missing data includes an occlusion region associated with at least one depth view of the received plurality of depth views of the object.
4. The method of any of claims 1-3, wherein performing the aggregation operation results in reducing noise associated with at least one depth view of the received plurality of depth views of the object.
5. The method of any one of claims 1 to 4, wherein:
the aggregation operation is a geometric fusion of the received plurality of depth views to generate the updated depth view, the aggregation operation being performed as a pre-process in response to a request to transmit the plurality of depth views over a network; and
the preprocessing also includes video compression of each updated depth view.
6. The method of any of claims 1 to 5, wherein the target viewpoint is selected to synthesize a previously uncaptured image of the object.
7. The method of any of claims 1 to 6, wherein the image processing system is a telepresence system configured to synthesize images from multiple target viewpoints using the updated depth view.
8. The method of any one of claims 1 to 7, wherein:
the plurality of depth views represents a plurality of depth maps corresponding to captured images of the object; and
generating the plurality of depth maps and replacing the plurality of depth maps with updated depth maps improves temporal coherence of the generated depth image of the object from the target viewpoint.
9. An image processing system, comprising:
at least one processing device; and
a memory storing instructions that, when executed, cause the system to perform operations comprising:
receiving a plurality of depth views of an object, each depth view of the plurality of depth views being captured from a respective viewpoint of the object, each depth view of the plurality of depth views comprising respective depth data associated with a depth image of the object captured from the respective viewpoint,
performing an aggregation operation on the plurality of depth views, the aggregation operation comprising:
Generating an updated depth view corresponding to each of the plurality of depth views, each updated depth view being based on the respective viewpoint and representing a portion of the respective depth data from each remaining depth view of the plurality of depth views of the object; and
replacing each depth view of the plurality of depth views with a corresponding updated depth view; and
an image of the object is generated based on the updated depth views from a target viewpoint that is different from each of the respective viewpoints from which each of the plurality of depth views was captured.
10. The system of claim 9, wherein performing the aggregation operation results in generating missing data associated with at least one depth view of the received plurality of depth views of the object.
11. The system of claim 9 or 10, wherein:
the aggregation operation is a geometric fusion of the received plurality of depth views to generate the updated depth view, the aggregation operation being performed as a pre-process in response to a request to transmit the plurality of depth views over a network; and
The preprocessing also includes video compression of each updated depth view.
12. The system of any of claims 9 to 11, wherein the target viewpoint is selected to synthesize a previously uncaptured image of the object.
13. The system of any of claims 9 to 12, wherein the image processing system is a telepresence system configured to synthesize images from multiple target viewpoints using the updated depth view.
14. The system of any of claims 9 to 13, wherein:
the plurality of depth views represents a plurality of depth maps corresponding to captured images of the object; and
generating the plurality of depth maps and replacing the plurality of depth maps with updated depth maps improves temporal coherence of the generated depth image of the object from the target viewpoint.
15. A non-transitory machine-readable medium having instructions stored thereon that, when executed by a processor, cause a computing device to:
receiving a plurality of depth views of an object, each depth view of the plurality of depth views being captured from a respective viewpoint of the object, each depth view of the plurality of depth views comprising respective depth data associated with a depth image of the object captured from the respective viewpoint,
Performing an aggregation operation on the plurality of depth views, the aggregation operation comprising:
generating an updated depth view corresponding to each of the plurality of depth views, each updated depth view being based on the respective viewpoint and representing a portion of the respective depth data from each remaining depth view of the plurality of depth views of the object; and
replacing each depth view of the plurality of depth views with a corresponding updated depth view; and
an image of the object is generated based on the updated depth views from a target viewpoint that is different from each of the respective viewpoints from which each of the plurality of depth views was captured.
16. The machine-readable medium of claim 15, wherein performing the aggregation operation results in generating missing data associated with at least one depth view of the received plurality of depth views of the object.
17. The machine-readable medium of claim 15 or 16, wherein performing the aggregation operation results in reducing noise associated with at least one depth view of the received plurality of depth views of the object.
18. The machine readable medium of any of claims 15 to 17, wherein:
the aggregation operation is a geometric fusion of the received plurality of depth views to generate the updated depth view, the aggregation operation being performed as a pre-process in response to a request to transmit the plurality of depth views over a network; and
the preprocessing also includes video compression of each updated depth view.
19. The machine-readable medium of any of claims 15 to 18, wherein the depth view is generated by a telepresence system configured to synthesize images from a plurality of target viewpoints using the updated depth view.
20. The machine readable medium of any of claims 15 to 19, wherein:
the plurality of depth views represents a plurality of depth maps corresponding to captured images of the object; and
generating the plurality of depth maps and replacing the plurality of depth maps with updated depth maps improves temporal coherence of the generated depth image of the object from the target viewpoint.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/070690 WO2022086579A1 (en) | 2020-10-23 | 2020-10-23 | Sender-side geometric fusion of depth data |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116349223A true CN116349223A (en) | 2023-06-27 |
Family
ID=73498346
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080106540.1A Pending CN116349223A (en) | 2020-10-23 | 2020-10-23 | Sender-side geometry fusion of depth data |
Country Status (6)
Country | Link |
---|---|
US (1) | US20230396751A1 (en) |
EP (1) | EP4233311A1 (en) |
JP (1) | JP2023546693A (en) |
KR (1) | KR20230088484A (en) |
CN (1) | CN116349223A (en) |
WO (1) | WO2022086579A1 (en) |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6066108B2 (en) * | 2014-04-16 | 2017-01-25 | コニカミノルタ株式会社 | Electronic document generation system and program |
US10341633B2 (en) * | 2015-11-20 | 2019-07-02 | Qualcomm Incorporated | Systems and methods for correcting erroneous depth information |
-
2020
- 2020-10-23 JP JP2023524720A patent/JP2023546693A/en active Pending
- 2020-10-23 EP EP20811220.1A patent/EP4233311A1/en active Pending
- 2020-10-23 US US18/250,059 patent/US20230396751A1/en active Pending
- 2020-10-23 KR KR1020237016765A patent/KR20230088484A/en not_active Application Discontinuation
- 2020-10-23 WO PCT/US2020/070690 patent/WO2022086579A1/en active Application Filing
- 2020-10-23 CN CN202080106540.1A patent/CN116349223A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20230396751A1 (en) | 2023-12-07 |
KR20230088484A (en) | 2023-06-19 |
JP2023546693A (en) | 2023-11-07 |
WO2022086579A1 (en) | 2022-04-28 |
EP4233311A1 (en) | 2023-08-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10546364B2 (en) | Smoothly varying foveated rendering | |
US11710287B2 (en) | Generative latent textured proxies for object category modeling | |
US20220130111A1 (en) | Few-shot synthesis of talking heads | |
KR102612529B1 (en) | Neural blending for new view synthesis | |
US11676330B2 (en) | 3d conversations in an artificial reality environment | |
CN107562185B (en) | Light field display system based on head-mounted VR equipment and implementation method | |
US20230396751A1 (en) | Sender-side geometric fusion of depth data | |
US20230316810A1 (en) | Three-dimensional (3d) facial feature tracking for autostereoscopic telepresence systems | |
US11386614B2 (en) | Shading images in three-dimensional content system | |
US20220232201A1 (en) | Image generation system and method | |
JP2020178239A (en) | All-sky video processing device and program |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |