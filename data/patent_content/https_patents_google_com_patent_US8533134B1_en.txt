US8533134B1 - Graph-based fusion for video classification - Google Patents
Graph-based fusion for video classification Download PDFInfo
- Publication number
- US8533134B1 US8533134B1 US12/938,309 US93830910A US8533134B1 US 8533134 B1 US8533134 B1 US 8533134B1 US 93830910 A US93830910 A US 93830910A US 8533134 B1 US8533134 B1 US 8533134B1
- Authority
- US
- United States
- Prior art keywords
- training
- categories
- videos
- video
- category
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
Definitions
- the present invention generally relates to the field of digital video, and more specifically, to methods of generating accurate classifiers for identifying categories of a video.
- Video hosting services such as YOUTUBETM have become an increasingly popular way of sharing and viewing digital videos, with users contributing tens of millions of videos each year. Accurate categorization of a video is of great value in such systems, permitting users to search for videos corresponding to given categories, and the video hosting service to more accurately match videos with relevant advertising, and the like.
- the descriptive metadata information provided by the user who contributes a video often does not result in the correct categorization of the video. For example, for a set of predetermined categories including categories such as “Sports,” “Basketball,” “Tennis,” and the like, a video entitled with the name of a tennis player featured in the video would not in itself directly permit the video to be properly categorized as “Tennis” or “Sports.”
- Machine learning algorithms can be employed to train a classifier function for a given category (e.g., “Tennis”) that, when applied to features of a video (such as metadata of the video), outputs a measure of the relevance of the video to the category. Then, the trained classifier function for a given category can be applied to a video and the resulting measure of relevance used to determine whether the video is within that category.
- a classifier function for a category most learning algorithms employ supervised learning, which requires as input a training set of videos known a priori to be representative of the category. Further, supervised learning tends to produce more accurate classifier functions when trained on a larger training set, and/or a training set with features that are more useful for categorization purposes.
- a training set useful for supervised learning can be produced by humans viewing videos and manually categorizing them into different categories, but manual categorization is time-consuming due to the need to watch the entire video, or at least a significant representative portion thereof.
- humans can efficiently view and label a sufficiently large sample of videos in a video hosting service where tens of thousands of videos are being uploaded every day.
- the classifier functions trained using these small training sets are therefore not as effective as could be desired in categorizing the large number of videos that have not already been manually categorized.
- a classifier training system trains a classifier functions (hereinafter simply “classifiers”) for categorizing videos representing different categories of a category graph.
- the classifiers are referred to as unified classifiers in that they unify the outputs of a number of separate initial classifiers trained from multiple disparate subsets of a training set of media items. Each unified classifier thus unites different types of training information within a single classifier.
- the training of a unified classifier takes into account inter-category relationships, which enhances training accuracy.
- the classifier training system begins with some set of pre-determined categories arranged in a graph indicating relationships between categories, wherein nodes in the graph represent categories, and edges between nodes represent relationships between categories.
- the category graph could include a node for the category “Tennis” with an edge to a node for the parent category “Sports.”
- the classifier training system further begins with a training set of media items from which the unified classifier is trained.
- the media items can include—in addition to videos—other types of content, such as web pages and other textual documents, images, sound files, and the like.
- the training set comprises a first subset of videos, each of which has been previously authoritatively labeled (e.g., by a human expert) as representing one or more of the categories.
- the classifier training system also begins with one or more other subsets of media items, each set of which can be of a different content type (e.g., web pages rather than videos) or obtained in a different manner (e.g., videos that are automatically inferred to be in the same categories as one of the videos of the first subset because they were “co-watched” along with the first video).
- a different content type e.g., web pages rather than videos
- a different manner e.g., videos that are automatically inferred to be in the same categories as one of the videos of the first subset because they were “co-watched” along with the first video.
- each category there are multiple initial classifiers, each of which outputs a value indicating whether a given media item represents the initial classifier's category.
- the initial classifiers are trained from combinations of the first subset of videos with the other subsets of media items. For example, for each category there could be (1) an initial classifier trained from the textual features of the videos of the first subset and of videos co-watched with the videos of the first subset, and (2) another initial classifier trained from the audiovisual features of these same two subsets.
- the training of the initial classifiers may be performed by the classifier training system itself, or it may be performed by another system, with the initial classifiers being provided as input to the classifier training system.
- the initial classifiers for each category are then applied to each media item in the training set to generate scores representing whether (or how strongly) the media item represents the associated categories. For a given media item, these scores—e.g., Boolean or scalar values—form a feature vector describing characteristics of the media items to which they correspond. For example, a given media item there would have a feature vector of M*K scores, one for each of the K categories and the M initial classifiers per category. Thus, features from subsets of the media items with potentially different characteristics are combined within a single feature vector.
- these scores e.g., Boolean or scalar values
- the scores computed from the initial classifiers as applied to the media items of the training set are provided as input for a fusion process that trains the unified classifiers, one per category.
- the fusion process learns classifier parameters of a probabilistic graph-based algorithm based on the known category labels of the media items of the training set, on the computed scores, and on the relationships between the categories of the category graph.
- the fusion process relates the scores associated with neighboring nodes of the category graph as part of its computations, thus taking into account the inter-category relationships.
- the unified classifiers can then be applied to an arbitrary video, each unified classifier producing as its output a score for each of the categories that indicates how strongly the video represents that category.
- the classifier training system is not limited to learning unified classifiers for digital videos, specifically, but rather can learn unified classifiers for any media object for which meaningful features can be extracted, including images, presentations, text documents, audio files, and the like.
- FIG. 1 is a block diagram of a video hosting service in which classifier training can be employed, according to one embodiment.
- FIG. 2 illustrates the various components of the classifier training subsystem of FIG. 1 , according to one embodiment.
- FIG. 3 is a data flow diagram representing the relationships and derivations of the various types of data analyzed and/or produced by the training subsystem, according to one embodiment.
- FIG. 4 is a data flow diagram representing the inputs and outputs of processes for training and for applying a unified classifier, respectively, according to one embodiment.
- FIG. 1 is a block diagram of a video hosting service 100 in which classifier training can be employed, according to one embodiment.
- the video hosting service 100 represents a system such as that of YOUTUBETM that stores and provides videos to users via client devices 135 .
- the video hosting service 100 communicates with a plurality of content providers 130 and client devices 135 via a network 140 to facilitate sharing of video content between users.
- the video hosting service 100 can additionally obtain data from various external websites 125 .
- FIG. 1 depicts only one instance of website 125 , content provider 130 and client device 135 , though there could be any number of each.
- a user of the content provider device 130 provides video content to the video hosting service 100 and a (usually different) user uses a client device 135 (also referred to simply as “client”) to view that content.
- client device 135 also referred to simply as “client”
- content provider devices 130 may also be used to view content.
- a particular content provider device 130 may be operated by the same entity that operates the video hosting service 100 .
- Content provider functions may include, for example, uploading a video to the video hosting service 100 , editing a video stored by the video hosting service 100 , editing metadata information about a video, or editing content provider preferences associated with a video.
- a client device 135 is a computing device that executes client software, e.g., a web browser or built-in client application, to connect to the video hosting service 100 via a network 140 and to display videos.
- client software e.g., a web browser or built-in client application
- the client device 135 might be, for example, a personal computer, a personal digital assistant, a cellular, mobile, or smart phone, a television “set-top box,” or a laptop computer.
- the client 135 includes an embedded video player such as, for example, the FLASH player from Adobe Systems, Inc. or any other player adapted for the video file formats used in the video hosting service 100 .
- client and content provider may refer to software providing client and content providing functionality, to hardware devices on which the software executes, or to the entities operating the software and/or hardware, as is apparent from the context in which the terms are used.
- the website 125 comprises one or more web pages accessible to the video hosting service 100 via the network 140 .
- the web pages comprise, for example, textual content such as HTML.
- the website may make available additional types of media content, such as general textual documents, presentations, audio files, image files, and the like.
- the network 140 is typically the Internet, but may be any network, including but not limited to a LAN, a MAN, a WAN, a mobile wired or wireless network, a private network, or a virtual private network.
- the video hosting service 100 operates on the video data from the content providers 130 (and, optionally, from the website 125 ) when training video classifiers.
- the video hosting service includes a front end interface 102 , a video serving module 104 , a video search module 106 , an upload server 108 , and a video repository 116 .
- Other conventional features, such as firewalls, load balancers, authentication servers, application servers, failover servers, site management tools, and so forth are not shown so as to more clearly illustrate the features of the video hosting service 100 .
- One example of a suitable service 100 is the YOUTUBETM website, found at www.youtube.com.
- Other video hosting sites are known, as well, and can be adapted to operate according to the teachings disclosed herein.
- module refers to computational logic for providing the specified functionality.
- a module can be implemented in hardware, firmware, and/or software. It will be understood that the named modules described herein represent one embodiment of the present invention, and other embodiments may include other modules. In addition, other embodiments may lack modules described herein and/or distribute the described functionality among the modules in a different manner. Additionally, the functionalities attributed to more than one module can be incorporated into a single module. Where the modules described herein are implemented as software, the module can be implemented as a standalone program, but can also be implemented through other means, for example as part of a larger program, as a plurality of separate programs, or as one or more statically or dynamically linked libraries. In any of these software implementations, the modules are stored on the computer readable persistent storage devices of the service 100 , loaded into memory, and executed by the one or more processors of the service's computers. The foregoing further applies to components described herein as “servers.”
- the upload server 108 of the video hosting service 100 receives video content from a content provider 130 . Received content is stored in the video repository 116 .
- a video serving module 104 provides video data from the video repository 116 to the clients. Clients 135 may also search for videos of interest stored in the video repository 116 using a video search module 106 , such as by entering textual queries containing keywords of interest.
- the front end interface 102 provides the interface between client 135 and the various components of the video hosting service 100 .
- the video repository 116 contains a set of videos 117 submitted by content providers 130 .
- the video repository 116 can contain any number of videos 117 , such as tens of thousands or hundreds of millions. Each of the videos 117 has a unique video identifier that distinguishes it from each of the other videos, such as a textual name (e.g., the string “a91qrx8”), an integer, or any other way of uniquely naming a video.
- the videos 117 can be packaged in various containers such as AVI, MP4, or MOV, and can be encoded using video codecs such as MPEG-2, MPEG-4, H.264, and the like.
- the videos 117 further have associated metadata 117 A, e.g., textual metadata such as a title, description, and/or tags provided by a content provider 130 who uploaded the video.
- the video hosting service 100 further comprises a classifier training subsystem 119 that trains an accurate video classifier for a predetermined set of categories, even in the absence of a large number of labeled videos to use as training examples. The trained classifier can then be applied to a given video to determine which of the categories the video represents.
- the classifier training subsystem 119 is part of the video hosting service 100 , as depicted in FIG. 1 .
- the classifier training subsystem 119 is separate from the video hosting service 100 , receiving input from it and providing output to it.
- the classifier training subsystem 119 is described in greater detail in FIG. 2 .
- the service 100 may be implemented using a single computer, or a network of computers, including cloud-based computer implementations.
- the computers are preferably server class computers including one or more high-performance CPUs and 1 G or more of main memory, as well as 500 Gb to 2 Tb of computer readable, persistent storage, and running an operating system such as LINUX or variants thereof.
- the operations of the service 100 as described herein can be controlled through either hardware or through computer programs installed in computer storage and executed by the processors of such servers to perform the functions described herein.
- the service 100 includes other hardware elements necessary for the operations described here, including network interfaces and protocols, input devices for data entry, and output devices for display, printing, or other presentations of data.
- FIG. 2 illustrates the various components of the classifier training subsystem 119 , according to one embodiment.
- the classifier training subsystem 119 trains classifiers based in part on a set of categories and on a set of videos that have been previously authoritatively labeled by an authoritative source as representing one or more of the categories.
- a module of the classifier training subsystem 119 further identifies additional training sample items, possibly of different media types, for each of the categories in addition to the labeled videos.
- the different media types for these training items may include—in addition to videos—textual documents such as web pages, audio, and images.
- Each of the additional samples is identified using one of a set of different techniques, such as identifying co-played videos or identifying web pages authoritatively labeled as representing a particular category.
- a training set of significant size can be created even where the manually labeled videos themselves are relatively few in number, thus increasing the accuracy of the trained classifiers.
- the combination of the labeled videos and the additional training sample items of different media types then serves as a supplemental training set that is provided as input to a classifier training algorithm.
- the various modules of the classifier training subsystem 119 process the supplemental training set, thereby training initial classifiers that can automatically categorize videos with a certain degree of accuracy.
- Output of the classifiers is then combined to train unified classifiers with still higher degrees of accuracy.
- the training of the unified classifiers is done using a graph algorithm that takes into account the structure of the category graph, including the relationships between neighboring categories (e.g., parent or child categories) in the graph.
- the classifier training subsystem 119 stores a predefined category set 205 representing different possible categories that a video could represent, such as “Sports,” “Tennis,” “Gymnastics,” “Arts & Entertainment,” “Movies,” “TV & Video,” and the like.
- the various categories of the category set 205 have some relationship with one or more other categories in the set, such that the set can be represented as a category graph, each node of the graph being a category, and edges between the nodes representing some relationship between the categories represented by the nodes connected by the edge.
- the category set 205 is more specifically a taxonomy in which the categories of the graph are arranged hierarchically as a tree, with categories being more general parents and/or more specialized children of other categories.
- categories being more general parents and/or more specialized children of other categories.
- “Sports” could be a top-level parent category with child categories “Tennis” and “Gymnastics”
- “Arts & Entertainment” could be another top-level category with child categories “Movies” and “TV & Video.”
- the child categories may in turn have their own child categories.
- the taxonomy may have any number of levels of depth.
- the category set 205 is non-hierarchical, with the nodes of the graph arbitrarily connected.
- a category can be represented in various manners as would be appreciated by one of skill in the art, such as a textual label (e.g., the string “Sports”) and an associated unique shorthand numerical identifier (e.g., 1).
- a video can be said to represent a category if the video contains some semantically understandable representation of the category within the video content itself. For example, a video represents a category “Tennis” if (for instance) there are scenes of a tennis match within the visual content of the video.
- a video can represent many different categories (e.g., both “Sports” and “News”), but users typically are interested in what may be considered the most salient category for the video.
- each of the categories has associated with it some minimum number of training sample items. That is, although there may initially have been some large number of potential categories, the final category set 205 will include only the potential categories that have the minimum number of associated training items.
- the classifier training subsystem 119 further comprises a training set 220 containing various content items, such as videos and web pages, that can be associated with various ones of the categories.
- the content items are labeled—either explicitly or implicitly—with one or more of the categories from the category set 205 .
- the content items, along with their labels, are then used to train classifiers for the various categories with which the content items are associated.
- the training set 220 comprises a set of authoritatively labeled videos 224 , previously authoritatively labeled as representing one or more categories from the category set 205 .
- the videos are manually labeled by one or more human experts trained in the meaning and use of the category set 205 ; this involves the human expert viewing a representative portion of each video and then specifying which categories the video represents based on the portion viewed.
- the authoritative labeling is performed by a labeling algorithm that categorizes videos with a high degree of accuracy.
- a hybrid approach is used in which videos are initially categorized by a labeling algorithm, with subsequent human review and validation of some (or all) of the labels.
- a video is labeled with the most applicable leaf node category in the taxonomy. For example, a video about tennis is labeled “Tennis,” not “Sports”; the fact that the video also more generally represents sports can be inferred from the fact that “Tennis” is a child category of “Sports” in the taxonomy tree.
- the videos which are authoritatively labeled represent a subset of the video repository 116 that was determined to be particularly popular, such as the videos most frequently viewed over a particular recent time period.
- a category in order for a category to have a threshold number associated training items in this embodiment, it must first have a sufficient number of popular videos, and then a minimum number of these popular videos must be authoritatively labeled. In other embodiments, the videos need not meet any popularity criterion prior to being labeled.
- the training set 220 also beneficially comprises a set of supplemental training items 245 that can be labeled with less effort, albeit possibly with less certainty. These supplemental training items can then be combined with the authoritatively labeled videos to form a larger training set.
- the supplemental training set preferably includes both similar media items (e.g., other videos) as well as other types of media items (e.g., web pages, images).
- the supplemental training items repository 245 comprises subsets of videos 246 and 247 automatically labeled based on observed relationships, and a subset of web pages 248 authoritatively labeled in a manner similar to that by which the authoritatively labeled videos 224 were labeled. It is appreciated, however, that these particular sets of items are merely representative of one embodiment, and that the supplemental training items repository 245 need not contain items of those same content types, or items labeled using the same techniques, and many contain different numbers of subsets than the three depicted in FIG. 2 .
- the co-watched videos 246 of the supplemental training set 245 are automatically labeled based on observed relationships with the authoritatively labeled videos 224 resulting from user actions. That is, for a given one of the authoritatively labeled videos 224 , the videos that are played directly before or after it, or within some set time period of it (e.g., 20 minutes), by the same user are considered “co-watched” and are assumed to represent the same category or categories as those of the authoritatively labeled video. Each of the labels of the given authoritatively labeled video are then copied to its co-watched videos 246 .
- a threshold e.g. 100
- the number of non-representative co-watched videos 246 may also be reduced through use of additional techniques, such as computing a degree of similarity between keywords or other metadata of the authoritatively labeled video and the co-watched videos 246 and removing co-watched videos lacking some minimum degree of similarity.
- the searched videos 247 of the supplemental training set 245 are automatically labeled based on the results of video search engines, such as those of YOUTUBETM. That is, each category label is provided as input to a video search engine, and a set of videos is identified as a result. (E.g., the category label “Tennis” is entered, and the video search engine provides a set of videos determined to represent tennis.) The category is then applied as a label to each of the videos in the search result set. In one embodiment, a maximum of 5,000 such videos is labeled for each category, including the videos found to be representative of child categories.
- the authoritatively labeled text documents 248 of the supplemental training set 245 contain primarily textual content, and in one embodiment comprise web pages from various web sites. In other embodiments, the text documents 248 additionally and/or alternatively comprise other types of documents with significant textual content, such as word processing files, pure text files, and the like.
- the text documents 248 may contain non-textual elements, such as backgrounds, images, etc., in addition to their purely textual portions.
- the text documents 248 are labeled in the same manner as the authoritatively labeled videos 224 . That is, a human expert (for example) examines a text document, such as a web page, and labels it with the label of one or more of the categories of the category set 205 .
- the authoritatively labeled web pages 248 can be labeled with far less cost and effort and can thus include a far greater number of items.
- the text documents are tentatively labeled by an automated process, such as a computer program that assigns the tentative labels based on metadata keywords, and the human experts review the tentative labels, accepting or rejecting them as appropriate.
- the training set 220 includes content items obtained using different techniques, such as manual labeling, automatic identification of co-watched videos, and execution of queries on a video search engine.
- the content items may be of different content types, such as video and text, which have different content properties—that is, videos have both audiovisual content and also textual content (e.g., associated textual metadata), whereas text documents have only textual content.
- the various items of the training set 220 need not be equally distributed amongst the various categories of the category set 205 . Rather, different categories may have different numbers of corresponding items. In one embodiment, categories lacking some threshold number of items—e.g., at least 50 authoritatively labeled videos 224 —are discarded for purposes of future classifier training. Thus, for example, although the category set 205 might originally have a thousand distinct categories, only a small number of categories (e.g., 30) might have a sufficient number of associated training items 220 to be included in the classifier training.
- each content item can be associated with some representation of the one or more categories that it represents, e.g., via a list of the shorthand numerical identifiers for the various categories.
- the association could be reversed, with each category being associated with identifiers of the associated content items, such as video identifiers, URLs of web pages or other textual documents, and the like.
- the associations may be stored in any type of data structure that makes access and analysis of the labels efficient.
- the labels can be stored with the metadata of each item 220 , or in a separate table, array, index, tree, or other structure.
- the content of the training set 220 need not wholly or even partially be physically present within the classifier training subsystem 119 of the video hosting service 100 .
- one or more of the authoritatively labeled videos 224 and the supplemental training items 245 are stored on computing systems remote from the video hosting service 100 , and the classifier training subsystem 119 accesses them from their remote locations.
- the authoritatively labeled text documents 248 may be stored remotely on websites 125 from which they originate, with the training set 220 only including a reference such as an URL pointing to the text documents 248 and labels corresponding to categories of the category set 205 that the text documents represent.
- the various videos 224 , 246 , 247 could likewise be stored remotely.
- the classifier training subsystem 119 further comprises a classifier repository 240 containing the classifiers learned based on the content items of the training items repository 220 .
- the classifier repository 240 comprises initial classifiers 241 that result from training on combinations of the authoritatively labeled videos 224 with each of the supplemental training items 245 , such as the combination of the authoritatively labeled videos 224 and the co-watched videos 246 , and the combination of the authoritatively labeled videos 224 and the authoritatively labeled text documents 248 .
- the classifier repository 240 additionally comprises unified classifiers 242 that result from application of a graph-based fusion algorithm to the results of the various initial classifiers 241 .
- the initial classifiers 241 represent preliminary classifiers that are used to produce the final, more effective unified classifiers 242 .
- Each of the initial classifiers 241 is associated with one of the categories in the category set 205 and, when applied to a video, provides a measure of how strongly the video represents that category. For example, an initial classifier 241 for the category “Tennis” provides a score measuring how strongly a given video represents the sport of tennis, e.g., based on the audiovisual content of the video and/or its textual metadata 117 A.
- a classifier produces Boolean scores representing whether or not a given video represents the category of the classifier; in another embodiment, a classifier produces a real number scalar (e.g., ranging from 0.0 to 1.0) representing how strongly (e.g., a measure of likelihood or probability) the video represents the categories, and numbers greater than some threshold can be considered to mean that the video represents the category.
- a real number scalar e.g., ranging from 0.0 to 1.0
- the classifier training subsystem 119 further comprises a classifier training module 230 that analyzes the content items in the training set 220 and trains the set of unified classifiers 242 that can be used to automatically categorize videos.
- the classifier training module 230 comprises a data supplementation module 231 that identifies certain ones of the supplemental training items 245 and adds them to the training set 220 .
- the data supplementation module 231 examines an access log of the video hosting service 100 to determine which videos were co-watched, and compares these videos to the authoritatively labeled videos 224 to identify the set of co-watched videos 246 .
- the data supplementation module 231 performs queries of the video search module 106 using each category label of the category set 205 to identify the searched videos 247 .
- the classifier training module 230 further comprises a partitioning module 232 that partitions the labeled content items of the training items repository 220 —that is, the authoritatively labeled videos 224 and supplemental training items 245 —into positive and negative training subsets based on their labels.
- the co-watched videos 246 and the searched videos 247 inherit the labels of the authoritatively labeled videos 224 to which they are related.
- the partitioning module 232 partitions the training items into some positive subset of items representing the category and some negative subset of items not representing the category.
- the documents representing the category are defined to be the documents labeled as representing either the category or one of its descendant categories (i.e., any child subcategory of the category, or any subcategories of a subcategory, etc.).
- the documents not representing the category are then derivatively defined to be the documents not in the positive subset.
- the partitioning module 232 could partition the training items 220 into a positive subset of content items representing “Sports” and a negative subset of documents not representing “Sports,” a positive subset representing “Tennis” and a negative subset not representing “Tennis,” and so forth, for each category in the category set 205 .
- These training subsets can then be used by an initial classifier training module 234 to train the initial classifiers 241 , as described below.
- the classifier training module 230 further comprises a feature extraction module 233 that extracts relevant features from the various training items 220 , an initial classifier training module 234 that trains a number of initial classifiers 241 for the various categories within the category set 205 using the features extracted by the feature extraction module 233 , and a fusion module 235 that trains the unified classifiers 242 based in part on values produced by the initial classifiers 241 .
- a feature extraction module 233 that extracts relevant features from the various training items 220
- an initial classifier training module 234 that trains a number of initial classifiers 241 for the various categories within the category set 205 using the features extracted by the feature extraction module 233
- a fusion module 235 that trains the unified classifiers 242 based in part on values produced by the initial classifiers 241 .
- the feature extraction module 233 extracts features from the content items of the training items repository 220 , the features serving as descriptive representations of their respective content items for use in learning category classifiers.
- the types of features differ based on the type of the content item from which they are extracted. For example, in the case of text documents from the authoritatively labeled text documents 248 , the feature extraction module 233 extracts textual features from the contents of the documents.
- the textual features that are extracted are weighted text clusters obtained from noisysy-Or Bayesian networks. (For more details, see, for example, Learning Bayesian Networks by R. E.
- the feature extraction module 233 additionally extracts various features from video content of the videos 224 , 247 , 248 .
- the types of features include:
- Audio and visual features are extracted in the same time interval. Then, a 1 D Haar wavelet decomposition is applied to the extracted features at 8 scales. Instead of using the wavelet coefficients directly, the maximum, minimum, mean and variance of the wavelet coefficients are used the features in each scale. This multi-scale feature extraction is applied to all of the audio and video content features, except the histogram of local features.
- the initial classifier training module 234 trains the various initial classifiers for each category of the category set 205 . Different initial classifiers are trained based on the different combinations of the authoritatively labeled videos 224 with the various subsets of the supplemental training items 245 . For example, in the embodiment depicted in FIG. 2 , in which the supplemental training items 245 comprise a subset of co-watched videos 246 , a subset of searched videos 247 , and a subset of authoritatively labeled text documents 248 , initial classifiers are trained for the three combinations of the authoritatively labeled videos 224 with each of these three subset. Further, for each combination, classifiers may be learned based on the types of features obtainable from the combination.
- the two combinations of the videos 224 with the videos 246 and 247 each support both audiovisual features and text features, since videos have both audiovisual content and textual metadata, and thus both audiovisual and textual classifiers 241 are trained.
- the combination of the videos 224 with the text documents 248 supports only textual features, since the text documents 248 lack audiovisual content, and thus only textual classifiers are trained.
- each of the K categories has 5 distinct classifiers associated with it, and so there are 5K distinct classifiers in total.
- classifiers are trained only for categories having a sufficient number of associated training items, e.g., at least 50 authoritatively labeled videos 224 .
- the initial classifier training module 234 provides the features extracted by the feature extraction module 233 from the positive and negative training subsets produced by the partitioning module 232 as input to a training algorithm.
- the AdaBoost training algorithm is used for training the classifiers 241 based on audiovisual features from video content
- the SVM algorithm is used for training the classifiers based on textual features (i.e. the textual features extracted the textual metadata of videos 224 and from the content of text documents 248 ) where there are 10,000 or fewer content items for training, and the LibLinear algorithm where there are more than 10,000 content items.
- a fusion module 235 trains the unified classifiers 242 based (in part) on the combined outputs of the classifiers.
- the training of the unified classifiers takes into account relationships between neighboring categories in the category graph, and also combines training items from different sources and possibly different content domains (e.g., video, text, audio).
- the unified classifiers 242 may then be applied to an arbitrary video (or, in other embodiments, to another type of media item of interest), producing as output a score for each of the categories of the category set 205 , the scores measuring how strongly the video represents that category. Further details on the fusion module 235 are provided below with respect to FIGS. 3A and 3B .
- the classifier training subsystem 119 optionally comprises a video classification module 250 that applies the initial classifiers 241 and the unified classifiers 242 produced by the classifier training module 230 to determine categories of the category set 205 that are applicable to a given video. Discussion of the use of the video classification module 250 is provided below.
- FIG. 3 illustrates the relationships between the various modules described above, and data processing involved in the process of deriving the unified classifiers 242 , according to one embodiment.
- the process begins with the authoritatively-labeled videos 224 , each of which has been authoritatively labeled as representing one or more categories of the category set 205 .
- the supplemental training items 245 are then added to the authoritatively labeled videos 224 , thereby forming the training set 220 .
- the supplemental training items 245 include co-played videos 246 , searched videos 247 , and authoritatively labeled web pages 248 , as described above with respect to FIG. 2 .
- Video items 246 and 247 inherit the same category labels as the authoritatively labeled videos 224 to which they are related; the authoritatively labeled web pages 248 are independently labeled, such as by human experts or labeling algorithms.
- the authoritatively labeled videos 224 are combined 305 with each of the sets of supplemental training items 246 - 248 .
- N 1 authoritatively labeled videos 224 and N 2 co-played videos 246 they are combined into a first combined set having N 1 +N 2 items.
- appropriate features are then extracted 310 from the media items of each of the combined sets that represent the category, and an initial classifier 241 is trained 315 based on those features.
- Features are said to be appropriate for a combined set if they are of a type obtainable from both the authoritatively labeled videos 224 and the other set of supplemental training items 245 in the combination.
- both the authoritatively labeled videos 224 and the co-played videos 246 are videos having both textual features (e.g., from textual metadata) and audiovisual features
- both a text classifier and an audiovisual classifier are trained for the combination of videos 224 and 246 .
- Each of the initial classifiers 241 is then applied 320 to score each of the media items in the training set 220 .
- the set of scores of the various media items, along with the corresponding labels, are provided as input to the fusion module 235 , which fuses the data together, training 325 a single unified classifier 242 for each of the categories 205 . More specifically, the training 325 results in learning of model parameters, as described in more detail below with respect to FIG. 4 .
- These parameters can then be used in conjunction with the other input data to infer the categories represented by an arbitrary video 320 , as described in more detail below with respect to FIG. 4 .
- the steps of FIG. 3 need not be performed in the exact order described.
- the feature extraction 310 could be performed before the subsets of the training set 220 are combined, and the combination could combine only the extracted feature vectors.
- the features need not be extracted 310 from all the combinations of the training set 220 before training 315 is performed; rather, extraction and training could be performed for one combination at a time, before proceeding to the next combination.
- some of the steps could occur partially or entirely in parallel on different processing units (e.g., different machines, different processors of a single machine, or different cores of a single processor).
- the unified classifiers 242 need not apply solely to digital videos, but in different embodiments applies to different types of media items, such as audio files, image files, textual files such as web pages, and the like.
- the process for training the unified classifier 242 in these embodiments is analogous to that disclosed above for training the unified classifiers 242 to categorize videos. That is, with the exception of possibly using different types or numbers of subsets within the training set 220 and extracting different types of feature vectors for different types of media items, the process is fundamentally the same in all such embodiments.
- FIG. 4 is a data flow diagram representing the inputs and outputs of processes for training and for applying the unified classifiers 242 , respectively, according to one embodiment.
- the unified classifiers 242 of FIG. 2 are trained by the fusion module 235 .
- the fusion module 235 applies the initial classifiers 241 to each of the media items in the set of training items 220 .
- the classifiers 241 produce a set of M*K scores, where there are K categories and M distinct classifiers per category.
- the set of scores is linearized to form a feature vector for the media item.
- graph-based statistical methods of a fusion algorithm are applied to these feature vectors, to the known category labels of the media items of the training items 220 , and to the graph of the category set 205 to calculate parameter values used in the unified classifiers 242 . More detail on the fusion algorithm is now provided.
- the fusion algorithm uses a Conditional Random Field/Discriminative Random Field (CRF/DRF).
- CRF/DRF is a graph with a set of nodes S and edges E.
- y), is calculated as follows:
- each node i in S represents one of the categories in S (e.g., “Tennis” or “Arts & Entertainment”)
- x i and x i are binary label occurrence variables for the i th and i th nodes in S indicating whether an input video represents the categories of i th and j th nodes
- y is a set of observations y i , each y i being the score resulting from applying the i th initial classifier to the input video
- Z is a normalizing constant mapping the result to the range
- w i T and v T are parameters tending to maximize A i and I ij
- h i is the set of classifier scores and their quadratic combinations
- N i denotes the neighbor nodes of i (e.g., its parent and/or child nodes).
- w i T parameter value there is a distinct w i T parameter value for each node i, and thus the parameter w is non-homogeneous.
- This training of a potentially different w value for each category node increases the number of parameters needed for the model, but this is computationally feasible where the number of categories is not overly large. For example, this is the case where only a modest number of potential categories have an adequate number of associated authoritatively labeled videos 224 .
- Making w non-homogenous beneficially increases the flexibility of the model, thereby allowing the model to better simulate real-world data.
- association potential A i is a function only of node i
- interaction potential I ij is a function of both node i and its neighbor nodes j.
- a i contributes to the decision of the label occurrences to apply to a node i locally, without considering label occurrences of neighboring nodes
- I ij regulates the label occurrences of node i by performing data dependent smoothing of labels on the neighboring nodes, which serves to take into account inter-category relationships.
- I ij relates the scores associated with neighboring nodes of the graph by computing the product x i *x j , as described further below.
- FIG. 4 represents the inputs and outputs for the training of the unified classifiers 242 by the fusion module 235 using CRF/DRF, operating a fusion algorithm in a training mode 400 A, according to one embodiment.
- the training performed by the fusion algorithm in training mode 400 A takes as input the category set 205 , each category of which represents one of the nodes i in the set S of the CRF/DRF graph. Further, the neighbors N i of each node i can be determined from the graph.
- the neighbors N i may include both child and parent nodes, both of which are treated equivalently by the algorithm. In another embodiment, child and parent neighbors are treated differently.
- the training further takes as input the set of observations y and the set of label occurrences x, which are derived from the media items of the training set 220 .
- the observations y are the score feature vectors obtained by applying the initial classifiers 241 to the media items of the training set, and h i and ⁇ ij are a direct result of the values of y.
- each label occurrence value x i of x is expressed as +1 or ⁇ 1, based on whether it indicates that the respective media item does, or does not, represent the category corresponding to node i.
- the interaction potential of equation [3] takes into account inter-category relationships. Specifically, the label occurrences x i and x j of neighboring nodes i and j are multiplied when computing the interaction potential, and using the values +1 and ⁇ 1 for the label occurrences has the effect of producing a negative product when “neighbor” concepts have different label occurrences, thereby reducing the interaction potential. This penalizes parameter values that lead neighbor concepts to have different label occurrences for the same video, which is a desired result, since parent and child concepts are expected to represent the same concepts with higher probability than non-neighboring concepts.
- the authoritative labels of the media items act as the label occurrences x that represent the categories applicable to the media items; that is, each category label that has been authoritatively applied to a training set item causes the label occurrence x i for the item to be +1, and the label occurrences for the category labels not authoritatively applied are ⁇ 1.
- N media items in the training set 220 K categories in the category set 205 , and M different classifiers per category, then there are K*N label occurrences in x (one label per category), and M*K*N scores in observations y (M scores for each of the K categories, for each of the N media items).
- the CRF/DRF expressed by equation [1] above quantifies the probability that a particular video or other media item represents given categories (corresponding to label occurrences x), given that the initial classifiers 241 when applied to that video produce a particular set of scores (observation y).
- the fusion module 235 applies statistical methods to find the parameter values that maximize the probability expressed by equation [1], given the other variables with known values.
- maximum likelihood estimation MLE is used in combination with belief propagation (BP), as would be known to one of skill in the art in the field of statistics.
- FIG. 4 further represents the inputs and outputs for the application of the unified classifiers 242 when the fusion algorithm is operating in inference mode 400 B to categorize a video 420 , according to one embodiment.
- node set S and the neighbor nodes N i are known, as are the initial classifiers 241 , and the score observations y can be obtained by applying the initial classifiers 241 to the appropriate feature vectors extracted from the video 420 .
- the categories S (including neighbors for each node i), the observations y, and the parameters w i T and v T act as input for the CRF/DRF algorithm.
- the output, x is a set of label occurrences, one for each category of the category set 205 , each label occurrence representing whether the video 420 is of the corresponding category.
- the fusion algorithm infers whether the video 420 represents the various categories 205 , given the properties of the video represented by the scores of the observation y.
- the inference mode 400 B uses a maximum posterior marginal (MPM) algorithm with belief propagation (BP) to determine the category label occurrences x by maximizing the conditional probability p(x
- MPM maximum posterior marginal
- BP belief propagation
- the above-described techniques have a number of properties that are particularly useful for performing video recognition in conjunction with a graph of categories.
- the inclusion of the scores from the different types of initial classifiers 241 for a category 205 within the single observation y integrates different types of data within a single classifier, thereby allowing the use of training items 220 obtained in different ways and/or having different types of data.
- the use of the graph algorithms, and in particular the interaction potentials that examine the interplays of neighboring category nodes takes into account the parent-child relationships of the various categories, thereby capturing additional information and resulting in a more accurate classifier.
- the unified classifiers 242 can, when applied to a video (e.g., by the video classification module 250 ), produce a score for every category of the set of categories 205 .
- the unified classifiers 242 may then be applied to categorize videos 117 from the video repository 116 that do not already have authoritatively applied category labels.
- the unified classifiers 242 are provided, as input, with the same type of information that was used to train them—that is, feature vectors of classifier scores.
- the initial classifiers 241 are applied to the input video, producing a feature vector of scores.
- This feature vector of scores is then provided to each of the unified classifiers 242 as depicted in FIG. 4 , and the set of category scores is produced as output.
- the video is assigned a category label for each category for which the associated unified classifier 242 produces a score above some threshold level indicating sufficient certainty that the video is indeed representative of the category.
- only the top-scoring C categories are used to label the video, for some integer C.
- the category set 205 is a taxonomy tree
- a video is not assigned a label with a general parent category having a child sub-category that also applies. For example, if the classifiers for both the “Sports” category and its “Tennis” child category output scores for a given video above the threshold level, the video would be labeled with “Tennis” but not with “Sports,” since the latter can be inferred from the former.
- any category for which the associated unified classifier 242 produces a score for a video above some threshold level has its label assigned to the video, regardless of its parent/child relationships with other categories.
- the category labels assigned to a video can then be used in various ways by users of the video hosting service 100 .
- the labels of the determined categories can be viewed along with other metadata of a video, e.g., appearing as video tags for the video when viewed in a web-based user interface of a video sharing site.
- the labels can also be used to browse videos of a particular type, such as showing all videos that were assigned labels of a chosen category, such as “Tennis.”
- the category can be chosen in various ways, such as by direct user selection of a category.
- the category can be chosen by user selection of a document from which the category can be inferred, such as by application of the initial classifiers to a textual document, by application of the unified classifier to a video, or the like, thus implementing “show videos like this document” functionality.
- they can be integrated into more sophisticated search functionality as one input, such as a search for videos of a particular category and having other additional attributes, such as particular text in their titles, a creation date within a given timeframe, or the like.
- the possible uses of the assigned category label are numerous, and are not limited to these specific examples.
- Certain aspects of the present invention include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of the present invention could be embodied in software, firmware or hardware, and when embodied in software, could be downloaded to reside on and be operated from different platforms used by real time network operating systems.
- the present invention also relates to an apparatus for performing the operations herein.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable medium that can be accessed by the computer.
- a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of computer-readable storage medium suitable for storing electronic instructions, and each coupled to a computer system bus.
- the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.
- the present invention is well suited to a wide variety of computer network systems over numerous topologies.
- the configuration and management of large networks comprise storage devices and computers that are communicatively coupled to dissimilar computers and storage devices over a network, such as the Internet.
Abstract
Description
-
- A color histogram computed using hue and saturation in HSV color space;
- Color motion defined as the cosine distance of color histograms between two consecutive frames;
- Skin color features;
- Edge features using edges detected by a Canny edge detector in regions of interest;
- Line features using lines detected by a probabilistic Hough Transform;
- A histogram of local features using Laplacian-of-Gaussian (LoG) and SIFT (Scale Invariant Feature Transform);
- A histogram of textons;
- Entropy features for each frame using a normalized intensity histogram and entropy differences for multiple frames;
- Facial features such as a number of faces, or the size and aspect ratio of largest face region, with faces being detected by an extension of the AdaBoost classifier;
- Shot boundary detection based features using differences of color histograms from consecutive frames;
- Audio features such as audio volume and 32-bin spectrogram in a fixed time frame centered at the corresponding video frame; and
- “Adult” content features based on a boosting-based classifier in addition to frame-based adult-content features.
where, for purposes of video classification, S is the category graph of the category set 205, each node i in S represents one of the categories in S (e.g., “Tennis” or “Arts & Entertainment”) xi and xi are binary label occurrence variables for the ith and ith nodes in S indicating whether an input video represents the categories of ith and jth nodes, y is a set of observations yi, each yi being the score resulting from applying the ith initial classifier to the input video, and Z is a normalizing constant mapping the result to the range [0.0, 1.0]. The indices i and j thus enumerate over the set of nodes in S. Ai and Iij are referred to as the association potential and the interaction potential, respectively, and are defined as follows:
and
I ij(x i ,x j ,y)=x i x j v Tμij(y),jεN i, [3]
where wi T and vT are parameters tending to maximize Ai and Iij, hi is the set of classifier scores and their quadratic combinations, is a pairwise feature vector for category nodes i and j, and Ni denotes the neighbor nodes of i (e.g., its parent and/or child nodes).
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/938,309 US8533134B1 (en) | 2009-11-17 | 2010-11-02 | Graph-based fusion for video classification |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US26212509P | 2009-11-17 | 2009-11-17 | |
US12/938,309 US8533134B1 (en) | 2009-11-17 | 2010-11-02 | Graph-based fusion for video classification |
Publications (1)
Publication Number | Publication Date |
---|---|
US8533134B1 true US8533134B1 (en) | 2013-09-10 |
Family
ID=49084219
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/938,309 Active 2031-05-09 US8533134B1 (en) | 2009-11-17 | 2010-11-02 | Graph-based fusion for video classification |
Country Status (1)
Country | Link |
---|---|
US (1) | US8533134B1 (en) |
Cited By (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120314941A1 (en) * | 2011-06-13 | 2012-12-13 | Microsoft Corporation | Accurate text classification through selective use of image data |
US20130124462A1 (en) * | 2011-09-26 | 2013-05-16 | Nicholas James Bryan | Clustering and Synchronizing Content |
US20140047091A1 (en) * | 2012-08-10 | 2014-02-13 | International Business Machines Corporation | System and method for supervised network clustering |
US8706675B1 (en) * | 2011-08-29 | 2014-04-22 | Google Inc. | Video content claiming classifier |
US20140307958A1 (en) * | 2013-04-16 | 2014-10-16 | The Penn State Research Foundation | Instance-weighted mixture modeling to enhance training collections for image annotation |
US8873836B1 (en) * | 2012-06-29 | 2014-10-28 | Emc Corporation | Cluster-based classification of high-resolution data |
US20150052090A1 (en) * | 2013-08-16 | 2015-02-19 | International Business Machines Corporation | Sequential anomaly detection |
US20160203318A1 (en) * | 2012-09-26 | 2016-07-14 | Northrop Grumman Systems Corporation | System and method for automated machine-learning, zero-day malware detection |
US20170006347A1 (en) * | 2013-08-16 | 2017-01-05 | The Directv Group, Inc. | Method and system for using hierarchical metadata for searching and recording content |
US9542623B2 (en) | 2014-06-11 | 2017-01-10 | Samsung Electronics Co., Ltd. | Image classification device, method for operating the same and electronic system comprising the image classification device |
US20170046573A1 (en) * | 2015-08-11 | 2017-02-16 | Google Inc. | Feature-based Video Annotation |
CN106503723A (en) * | 2015-09-06 | 2017-03-15 | 华为技术有限公司 | A kind of video classification methods and device |
CN106537390A (en) * | 2014-07-23 | 2017-03-22 | 微软技术许可有限责任公司 | Identifying presentation styles of educational videos |
US20170262633A1 (en) * | 2012-09-26 | 2017-09-14 | Bluvector, Inc. | System and method for automated machine-learning, zero-day malware detection |
US20170323178A1 (en) * | 2010-12-08 | 2017-11-09 | Google Inc. | Learning highlights using event detection |
US10007679B2 (en) | 2008-08-08 | 2018-06-26 | The Research Foundation For The State University Of New York | Enhanced max margin learning on multimodal data mining in a multimedia database |
CN110168579A (en) * | 2016-11-23 | 2019-08-23 | 启创互联公司 | For using the system and method for the representation of knowledge using Machine learning classifiers |
US20200074321A1 (en) * | 2018-09-04 | 2020-03-05 | Rovi Guides, Inc. | Methods and systems for using machine-learning extracts and semantic graphs to create structured data to drive search, recommendation, and discovery |
US10649740B2 (en) * | 2015-01-15 | 2020-05-12 | International Business Machines Corporation | Predicting and using utility of script execution in functional web crawling and other crawling |
CN111694954A (en) * | 2020-04-28 | 2020-09-22 | 北京旷视科技有限公司 | Image classification method and device and electronic equipment |
US11205103B2 (en) | 2016-12-09 | 2021-12-21 | The Research Foundation for the State University | Semisupervised autoencoder for sentiment analysis |
US11250488B1 (en) * | 2018-07-16 | 2022-02-15 | A9.Com, Inc. | Method and system for determining new categories with which a target user has not interacted |
US11475668B2 (en) | 2020-10-09 | 2022-10-18 | Bank Of America Corporation | System and method for automatic video categorization |
US11544579B2 (en) | 2016-11-23 | 2023-01-03 | Primal Fusion Inc. | System and method for generating training data for machine learning classifier |
US20240095293A1 (en) * | 2021-07-26 | 2024-03-21 | Beijing Zitiao Network Technology Co., Ltd. | Processing method and apparatus based on interest tag, and device and storage medium |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020059221A1 (en) | 2000-10-19 | 2002-05-16 | Whitehead Anthony David | Method and device for classifying internet objects and objects stored on computer-readable media |
US6853982B2 (en) * | 1998-09-18 | 2005-02-08 | Amazon.Com, Inc. | Content personalization based on actions performed during a current browsing session |
US20060031217A1 (en) * | 2004-08-03 | 2006-02-09 | International Business Machines Corporation | Method and apparatus for ontology-based classification of media content |
US7039856B2 (en) * | 1998-09-30 | 2006-05-02 | Ricoh Co., Ltd. | Automatic document classification using text and images |
US7107520B2 (en) | 2002-11-18 | 2006-09-12 | Hewlett-Packard Development Company, L.P. | Automated propagation of document metadata |
US7203669B2 (en) | 2003-03-17 | 2007-04-10 | Intel Corporation | Detector tree of boosted classifiers for real-time object detection and tracking |
US20070255755A1 (en) | 2006-05-01 | 2007-11-01 | Yahoo! Inc. | Video search engine using joint categorization of video clips and queries based on multiple modalities |
US20080228749A1 (en) * | 2007-03-13 | 2008-09-18 | Microsoft Corporation | Automatic tagging of content based on a corpus of previously tagged and untagged content |
US20090281970A1 (en) * | 2008-05-09 | 2009-11-12 | Yahoo! Inc. | Automated tagging of documents |
US20090292685A1 (en) | 2008-05-22 | 2009-11-26 | Microsoft Corporation | Video search re-ranking via multi-graph propagation |
US20090327336A1 (en) | 2008-06-27 | 2009-12-31 | Microsoft Corporation | Guided content metadata tagging for an online content repository |
US20100125539A1 (en) * | 2006-03-30 | 2010-05-20 | Sony France S.A. | Hybrid audio-visual categorization system and method |
US8090621B1 (en) * | 2007-06-27 | 2012-01-03 | Amazon Technologies, Inc. | Method and system for associating feedback with recommendation rules |
-
2010
- 2010-11-02 US US12/938,309 patent/US8533134B1/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6853982B2 (en) * | 1998-09-18 | 2005-02-08 | Amazon.Com, Inc. | Content personalization based on actions performed during a current browsing session |
US7039856B2 (en) * | 1998-09-30 | 2006-05-02 | Ricoh Co., Ltd. | Automatic document classification using text and images |
US20020059221A1 (en) | 2000-10-19 | 2002-05-16 | Whitehead Anthony David | Method and device for classifying internet objects and objects stored on computer-readable media |
US7107520B2 (en) | 2002-11-18 | 2006-09-12 | Hewlett-Packard Development Company, L.P. | Automated propagation of document metadata |
US7203669B2 (en) | 2003-03-17 | 2007-04-10 | Intel Corporation | Detector tree of boosted classifiers for real-time object detection and tracking |
US20060031217A1 (en) * | 2004-08-03 | 2006-02-09 | International Business Machines Corporation | Method and apparatus for ontology-based classification of media content |
US20100125539A1 (en) * | 2006-03-30 | 2010-05-20 | Sony France S.A. | Hybrid audio-visual categorization system and method |
US20070255755A1 (en) | 2006-05-01 | 2007-11-01 | Yahoo! Inc. | Video search engine using joint categorization of video clips and queries based on multiple modalities |
US20080228749A1 (en) * | 2007-03-13 | 2008-09-18 | Microsoft Corporation | Automatic tagging of content based on a corpus of previously tagged and untagged content |
US8090621B1 (en) * | 2007-06-27 | 2012-01-03 | Amazon Technologies, Inc. | Method and system for associating feedback with recommendation rules |
US20090281970A1 (en) * | 2008-05-09 | 2009-11-12 | Yahoo! Inc. | Automated tagging of documents |
US20090292685A1 (en) | 2008-05-22 | 2009-11-26 | Microsoft Corporation | Video search re-ranking via multi-graph propagation |
US20090327336A1 (en) | 2008-06-27 | 2009-12-31 | Microsoft Corporation | Guided content metadata tagging for an online content repository |
Non-Patent Citations (158)
Title |
---|
"OpenDirectoryProject," 1998-2010, [online] [Retrieved on Jan. 16, 2011] Retrieved from the internet . |
"OpenDirectoryProject," 1998-2010, [online] [Retrieved on Jan. 16, 2011] Retrieved from the internet <URL:http://www.dmoz.org/>. |
"YouTube—Broadcast Yourself," 2011, [online] [Retrieved on Jan. 16, 2011] Retrieved from the internet <URL:http://www.youtube.com>. |
Agarwal, N., et al., "Blocking Objectionable Web Content by Leveraging Multiple Information Sources," SIGKDD Explor. Newsl., 2006, vol. 8, No. 1, pp. 17-26. |
Anderson, R., A local algorithm for finding dense subgraphs, In Proc. 19th Annual ACM-SIAM Symposium on Discrete Algorithms, 2008, pp. 1003-1009. |
Aradhye, H., et al., "Video2Text: Learning to annotate video content," IEEE International Conference on Data Mining Workshops, Dec. 6, 2009, pp. 144-151. |
Ayad, H.G., et al., "On Voting-Based Consensus of Cluster Ensembles," Pattern Recognition, May 2010, vol. 43, No. 5, pp. 1943-1953. |
Ayad, H.G., et al., Cumulative Voting Consensus Method for Partitions with Variable No. of Clusters, IEEE Transactions on Pattern Analysis and Machine Intelligence, Jan. 2008, vol. 30, No. 1, pp. 160-173. |
Baluja, S., et al., "Video Suggestion and Discovery for YouTube: Taking Random Walks Through the View Graph," Proceedings of the International World Wide Web Conference, 2008, 10 pages. |
Barnard, K., et al., "Matching Words and Pictures," Journal of Machine Learning Research, 2003, vol. 3, pp. 1107-1135. |
Belkin, M., et al., "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples," Journal of Machine Learning Research, 2006, vol. 7, pp. 2399-2434. |
Blei, D., et al., "Modeling Annotated Data," Proc. ACM SIGIR, 2003, pp. 127-134. |
Blum, A., et al., "Combining Labeled and Unlabeled Data with Co-Training," COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 1998, pp. 92-100. |
Blum, A., et al., "Combining Labeled and Unlabeled Data with Co-Training," COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 1998. |
Boureau, Y.L., et al., "Learning Mid-Level Features for Recognition," Conference on Computer Vision and Pattern Recognition, 2010, pp. 2559-2566. |
Brubaker, S. C., et al., "On the Design of Cascades of Boosted Ensembles for Face Detection," International Journal of Computer Vision (IJCV), May 2008, vol. 77, No. 1-3, pp. 65-86. |
Brubaker, S. C., et al., "Towards Optimal Training of Cascaded Detectors," Computer Vision-ECCV, Lecture Notes in Computer Science, 2006, vol. 3951/2006, pp. 325-337. |
Brubaker, S. C., et al., "Towards the Optimal Training of Cascades of Boosted Ensembles," Toward Category-Level Object Recognition (CLOR), Lecture Notes in Computer Science, 2006, vol. 4170/2006, pp. 301-320. |
Cai, C., et al., "Hierarchical clustering of WWW image search results using visual, textual and link information," in Proc. of ACM MM'04, Oct. 10-16, 2004, pp. 952-959. |
Cao, L., et al., "Annotating Photo Collections by Label Propagation According to Multiple Similarity Cues," Proceeding of the 16th ACM international conference on Multimedia, 2008, 9 pages. |
Cao, X., et al., "Video Shot Motion Characterization Based on Hierarchical Overlapped Growing Neural Gas Networks," Multimedia Systems, Springer-Verlag 2003, School of Electrical and Electronic Engineering, Nanyang Technological University, Nanyang Avenue, Singapore, 639798, pp. 1-8. |
Carvalho, R., "Metadata goes where Metadata is: contextual networks in the photographic domain," ESWC 2008 Ph. D. Symposium, 2008, 5 pages. |
Chang et al, Combining text and audio-visual features in video indexing, 2005. * |
Chang, C.-C., et al., "Liblinear: A Library for Large Linear Classification," 2010, [online] [Retrieved on Jan. 17, 2011], Software available at . |
Chang, C.-C., et al., "Liblinear: A Library for Large Linear Classification," 2010, [online] [Retrieved on Jan. 17, 2011], Software available at <http://www.csie.ntu.edu.tw/˜cjlin/liblinear/>. |
Chang, E., et al., "A Unified Learning Paradigm for Large-Scale Personalized Information Management," Proceedings of the Emerging Information Technology Conference, 2005. |
Chang, S., et al., "Recent Advances and Challenges of Semantic Image/Video," IEEE International Conference on Acoustics, Speech and Signal Processing, 2007, 4 pages. |
Chou, Y.Y., et al., "A Hierarchical Multiple Classifier Learning Algorithm," Proceedings of the 15th International Conference on Pattern Recognition-ICPR 2000, vol. II, pp. 152-155. |
Christoudias, C. M., et al., Co-Training with Noisy Perceptual Observations, CVPR, 2009, pp. 1-10. |
Cilibrasi, R. L., et al., "A Fast Quartet Tree Heuristic for Hierarchical Clustering," Pattern Recognition, Mar. 2011, vol. 44, No. 3, pp. 662-677. |
Concepción Morales, E.R., et al., "Building and Assessing a Constrained Clustering Hierarchical Algorithm," Proceedings of the 13th Iberoamerican Congress on Pattern Recognition, CIARP 2008, Sep. 9-12, 2008, pp. 211-218. |
Cordella, L. P., et al., "Combining Single Class Features for Improving Performance of a Two Stage Classifier," Proceedings of the 2010 International Conference on Pattern Recognition, Aug. 23-26, 2010, pp. 4352-4355. |
Cour, T., et al., "Learning from Ambiguously Labeled Images," Technical Report, U. Penn., 2009, pp. 1-8. |
Cui, F., et al., "Content-enriched classifier for web video classification," In Proc. of SIGIR'10, Jul. 19-23, 2010, pp. 619-626. |
Davison, B. D., "Topical locality in the web," In Proc. 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2004, pp. 272-279. |
Day, M., et al., "Constructing Efficient Cascade Classifiers for Object Detection," Proceedings of 2010 IEEE 17th International Conference on Image Processing, Hong Kong, Sep. 26-29, 2010, pp. 3781-3784. |
Dekel, O., et al., "Large Margin Hierarchical Classification," ICML, 2004, 8 pages. |
Deng, J., et al., "ImageNet: A Large-Scale Hierarchical Image Database," CVPR09, 2009, 8 pages. |
Duan, L., et al., "Domain Transfer SVM for Video Concept Detection," Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 1375-1381. |
Duchenne, O., et al., "Automatic Annotation of Human Actions in Video," ICCV, 2009, 8 pages. |
Dumais, S., et al., "Hierarchical classification of web content," In SIGIR '00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 256-263, New York, NY, USA, 2000. ACM. |
Dundar, M. M., et al., "Joint Optimization of Cascaded Classifiers for Computer Aided Detection," CVPR07, pp. 1-8. |
Duygulu, P., et al., "Object Recognition as Machine Translation: Learning a Lexicon for a XED Image Vocabulary," Computer Vision-ECCV 2002, Proceedings of the 7th European Conference on Computer Vision, Copenhagen, Denmark, May 28-31, 2002, Part IV, pp. 97-112. |
El-Sherif, E., et al., "Automatic Generation of Optimum Classification Cascades," International Conference on Pattern Recognition (ICPR), 2008, pp. 1-4. |
El-Shishini, H., et al., "A Multistage Algorithm for Fast Classification of Patterns," Pattern Recognition Letters, Oct. 1989, vol. 10, No. 4, pp. 211-215. |
Everingham, M., et al., "Hello! My Name is . . . Buffy-Automatic Naming of Characters in TV Video," BMVC, 2006, 10 pages. |
Ewerth, R., et al., "Semi-Supervised Learning for Semantic Video Retrieval," Proceedings of the Conference on Image and Video Retrieval (CIVR), Amsterdam, The Netherlands, Jul. 9-11, 2007, pp. 154-161. |
Fan, J., et al., "Incorporating Concept Ontology for Hierarchical Video Classification, Annotation, and Visualization," IEEE Transactions on Multimedia, Aug. 2007, vol. 9, No. 5, pp. 939-957. |
Fan, R.-E., et al., "Liblinear: a library for large linear classification," Journal of Machine Learning Research, 2008, vol. 9, pp. 1871-1874. |
Feng, H., et al., "A Bootstrapping Framework for Annotating and Retrieving WWW Images," Proc. ACM MM, 2004, 8 pages. |
Feng, H., et al., A boot-strapping framework for annotating and retrieving WWW images. In Proc. of ACM MM'04, Oct. 10-16, 2004, pp. 55-62. |
Feng, S. L., et al., "Multiple Bernoulli Relevance Models for Image and Video Annotation," Proc. CVPR, 2004, pp. 1-8. |
Fergus, R., et al., "A Visual Category Filter for Google Images," ECCV, 2004. |
Fergus, R., et al., "Learning Object Categories from Google's Image Search," Proceedings of the 10th IEEE International Conference on Computer Vision (ICCV), 2005, vol. 2, pp. 1816-1823. |
Fischer, S., et al., Automatic recognition of film genres, in Proc. of ACM Multimedia'95, Nov. 5-9, 1995, pp. 295-304. |
Foo, B., et al., "A Distributed Approach for Optimizing Cascaded Classifier Topologies in Real-Time Stream Mining Systems," IEEE Transactions on Image Processing, Nov. 2010, vol. 19, No. 11, pp. 3035-3048. |
Freund, Y., et al., "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting," Journal of Computer and System Sciences, Aug. 1997, vol. 55, No. 1, pp. 119-139. |
Freund, Y., et al., "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting," Proceedings of European Conference on Computational Learning Theory (ECCV), Barcelona, Spain, Mar. 13-15, 1995, pp. 23-37. |
Giusti, N., et al., "Theoretical and Experimental Analysis of a Two-Stage System for Classification," IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), Jul. 2002, vol. 24, No. 7, pp. 893-904. |
Goldman, S., et al., "Enhancing supervised learning with unlabeled data," In Proc. 17th International Conference on Machine Learning, 2000, pp. 327-334. |
Guillaumin, M., et al., "Multimodal semi-supervised learning for image classification," In Proc. IEEE Conf. Computer Vision and Pattern Recognition, Jun. 2010, pp. 902-909. |
Gupta, S., et al., "Watch, Listen & Learn: Co-Training on Captioned Images and Videos," Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML), 2008, 16 pages. |
Halevy, A., et al., "The unreasonable effectiveness of data," Intelligent Systems, IEEE, Mar. 2009, pp. 8-12, vol. 24, No. 2. |
Hall, K., et al., "MapReduce/Bigtable for distributed optimization," In NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds, 2010, pp. 1-7. |
Hays, J., et al., "IM2GPS: Estimating Geographic Information from a Single Image," Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008. |
Heiseleyz, B., et al., "Feature Reduction and Hierarchy of Classifiers for Fast Object Detection in Video Images," Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR01), Kauai, Hawaii, Dec. 8-14, 2001, vol. II, pp. 18-24. |
Hore, P., et al., "A Scalable Framework for Cluster Ensembles," Pattern Recognition, May 2009, vol. 42, No. 5, pp. 676-688. |
Huang, G., et al., Text-based video content classification for online video- sharing sites. Journal of the American Society for Information Science and Technology, 2010, vol. 61, No. 5, pp. 891-906. |
Huang, J., et al., "Exploring web scale language models for search query processing," In Proc. 19th international conference on World wide web, Apr. 26-30, 2010, pp. 451-460. |
Ikizler-Cinbis, N., et al., "Learning Actions from the Web," Proceedings of 12th International Conference on Computer Vision (ICCV), 2009, 8 pages. |
Jones, M. J., et al., "Statistical Color Models with Application to Skin Detection," International Journal of Computer Vision (IJCV), Jan. 2002, vol. 46, No. 1, pp. 81-96. |
Jordan, M.I., et al., "Hierarchical Mixture of Experts and the EM Algorithm," Neural Computation, 1994, vol. 6, pp. 181-214. |
Kalogerakis, E., et al., "Image Sequence Geolocation with Human Travel Priors," Proceedings of the IEEE International Conference on Computer Vision (ICCV'09), 2009, 8 pages. |
Kavukcuoglu, K., et al., "Learning Invariant Features Through Topographic Filter Maps," CVPR09, pp. 1605-1612. |
Koller, D., et al., "Hierarchically classifying documents using very few words," In the Proceedings of the Fourteenth International Conference on Machine Learning, ICML, Jul. 8-12, 1997, pp. 170-178. |
Kukenys, I., et al., "Classifier Cascades for Support Vector Machines," 2008 23RD International Conference Image and Vision Computing New Zealand (IVCNZ08), Nov. 26-28, 2008, pp. 1-6. |
Kumar, S., et al., "Discriminative Fields for Modeling Spatial Dependencies in Natural Images," Advances in Neural Information Processing Systems (NIPS), 2004, 8 pages. |
Lafferty, J., et al., "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data," Proceedings of International Conference on Machine Learning (ICML), 2001, 8 pages. |
Laptev, I., et al., "Learning Realistic Human Actions from Movies," Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, 8 pages. |
Leung, T., et al., "Representing and Recognizing the Visual Appearance of Materials Using Three-Dimensional Textons," International Journal of Computer Vision (IJCV), 2001, vol. 43, No. 1, pp. 29-44. |
Li, L., et al., "Optimol: Automatic Online Picture Collection Via Incremental Model Learning," Proc. Computer Vision and Pattern Recognition (CVPR), 2007. |
Li, L.-J., et al., "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework," In Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009, pp. 2036-2043. |
Li, Y., et al., "Bagging Based Efficient Kernel Fisher Discriminant Analysis for Face Recognition," the 18th International Conference on Pattern Recognition (ICPR06), vol. 3, pp. 523-526. |
Lin, A., et al., "News video classification using SVM-based multimodal classifiers and combination strategies," in Proc. of ACM Multimedia'02, Dec. 1-6, 2002, pp. 323-326. |
Liu, J., "Recognizing Realistic Actions from Videos "in the Wild"," Computer Vision and Pattern Recognition (CVPR), 2009, 1996-2003. |
Liu, T.-Y., et al., "Support Vector Machines Classification with a Very Large-Scale Taxonomy," SIGKDD Explorations, 2005, vol. 1, pp. 36-43. |
Lopez-Maury, L., "A Two-Component Signal Transduction System Involved in Nickel Sensing in the Cyanobacterium synechocystis sp. PCC 6803," Molecular Microbiology, 2002, vol. 43, No. 1, pp. 247-256. |
Lowe, D. G., "Distinctive Image Features from Scale-Invariant Keypoints," International Journal of Computer Vision (IJCV), 2004, vol. 60, No. 2, pp. 91-110. |
Luo, H., "Optimization Design of Cascaded Classifiers," CVPR05, vol. 1, pp. 480-485. |
Ma, B. P., et al., "Discriminant Analysis for Perceptionally Comparable Classes," FG08, pp. 1-6. |
Mahajan, D., et al., "Image classification using the web graph," In Proc. Multimedia, Oct. 25-29, 2010, pp. 991-994. |
Mangai, U. G., et al., "A Hierarchical Multi-Classifier Framework for Landform Segmentation Using Multi-Spectral Satellite Images-A Case Study Over the Indian Subcontinent," 2010 Fourth Pacific-Rim Symposium on Image and Video Technology, (PSIVT10), Nov. 14-17, 2010, Singapore, pp. 306-313. |
Mirzaei, A., et al., "Combining Hierarchical Clusterings Using Min-Transitive Closure," ICPR08, pp. 1-4. |
Montagnuuolo, M., et al., "Parallel Neural Networks for Multimodal Video Genre Classification," Multimedia Tools and Applications, Jan. 2009, vol. 41, pp. 125-159. |
Nam, M. Y., et al., "Cascade of Fusion for Adaptive Classifier Combination Using Context-Awareness," AMD006, pp. 252-261. |
Neapolitan, R. E., "Learning Bayesian Networks," Prentice Hall, Upper Saddle River, NJ, USA, 2003. |
Neven, H., et al., "Image Recognition with an Adiabatic Quantum Computer I. Mapping to Quadratic Unconstrained Binary Optimization," 2008, pp. 107. |
Niebles, J. C., et al., "Extracting moving people from internet videos," In ECCV '08: Proceedings of the 10th European Conference on Computer Vision, 2008, pp. 527-540, Part IV, LNCS 5305. |
Nister, D., et al., "Scalable Recognition with a Vocabulary Tree," CVPR '06: Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE Computer Society, Washington, DC, USA, Jun. 2006, vol. 2, pp. 2161-2168. |
Patterson, R. D., et al., "Complex Sounds and Auditory Images," Proc. Auditory Physiology and Perception, 1992, pp. 429-446. |
Qi, G.-J., et al., "Correlative Multilabel Video Annotation with Temporal Kernels," ACM Transactions on Multimedia Computing, Communications, and Applications, 2008, vol. 5, No. 1, Article 3, pp. 1-27. |
Quost, B., et al., "Pairwise Classifier Combination Using Belief Functions," Pattern Recognition Letters (PRL), Apr. 1, 2007, vol. 28, No. 5, pp. 644-653. |
Raina, R., et al., "Self-Taught Learning: Transfer Learning from Unlabeled Data," Proceeding of International Conference on Machine Learning (ICML), Corvallis, OR, 2007, pp. 759-166. |
Ramachandran, C., et al., "VideoMule: A Consensus Learning Approach to Multi-Label Classification from Noisy User-Generated Videos," ACM Multimedia, 2009, 4 pages. |
Ranzato, M., et al., "Modeling Pixel Means and Covariances using Factorized Third-Order Boltzmann Machines," Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR10), 2010, pp. 2551-2558. |
Ranzato, M., et al., "Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition," CVPR07, pp. 1-8. |
Rehn, M., et al., "Sound Ranking Using Auditory Sparse-Code Representations," Proc. ICML: Workshop on Sparse Methods for Music Audio, 2009, 3 pages. |
Rodriguez, J. J., "Forests of Nested Dichotomies," Pattern Recognition Letters (PRL), Jan. 15, 2010, vol. 31, No. 2, pp. 125-132. |
Rodriguez, M., et al., "Automatic Metadata Generation Using Associative Networks," ACM Transactions on Information Systems, Feb. 2009, pp. 7:1-7:20, vol. 27, No. 2, Article 7. |
Sargin, E. M., et al., "Audiovisual Celebrity Recognition in Unconstrained Web Videos," Proc. ICASSP, 2009, 4 pages. |
Schapire, R. E., "The Boosting Approach to Machine Learning: An Overview," MSRI Workshop on Nonlinear Estimation and Classification, 2002, pp. 1-23. |
Schapire, R. E., et al., "Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods," The Annals of Statistics, 1998, vol. 26, No. 5, pp. 1651-1686. |
Schindler, G., et al., "Internet Video Category Recognition," First IEEE Workshop on Internet Vision, CVPR, 2008, 7 pages. |
Serdyukov, P., et al., "Placing Flickr Photos on a Map," SIGIR'09: Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 2009, New York, NY, USA, pp. 484-491. |
Siersdorfer et al, Automatic Video Tagging using Content Redundancy, Jul. 2009. * |
Singh, R., et al., "Reduced-Complexity Delayed-Decision Algorithm for Context-Based Image Processing Systems," IEEE Transaction on Image Processing, Aug. 2007, vol. 16, No. 8, pp. 1937-1945. |
Slonim, N., et al., "Discriminative Feature Selection via Multiclass Variable Memory Markov Model," Proceedings of the Nineteenth International Conference on Machine Learning, 2002, 8 pages. |
Smeaton, A. F., et al., "Evaluation Campaigns and TRECVid," Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval, ACM Press, Oct. 26-27, 2006, pp. 321-330. |
Snoek, C. G.M., et al., "Early Versus Late Fusion in Semantic Video Analysis," ACM Multimedia 2005, Nov. 6-11, 2005, Singapore, 4 pages. |
Snoek, C., et al., "The MediaMill TRECVID 2008 Semantic Video Search Engine," 2009, 14 pages. |
Song, Y., et al., "Taxonomic Classification for Web-Based Videos," Conference on Computer Vision and Pattern Recognition (CVPR), 2010. |
Su, Y., et al., "Hierarchical Ensemble of Global and Local Classifiers for Face Recognition," IP, Aug. 2009, vol. 18, No. 8, pp. 1885-1896. |
Sun F., et al., "Two-Stage Computational Cost Reduction Algorithm Based on Mahalanobis Distance Approximations," ICPR00, vol. II, pp. 696-699. |
Sun, A., et al., Hierarchical Text Classification and Evaluation, Proceedings of the 2001 IEEE International Conference on Data Mining (ICDM), Nov. 2001, pp. 521-528. |
Szczot, M., et al., "Package Boosting for Readaption of Cascaded Classifiers," 2010 International Conference on Pattern Recognition (ICPR10), pp. 552-555. |
Tang, L., et al., "Large scale multi-label classification via metalabeler," In Proc. 18th International Conference on World Wide Web, Apr. 20-24, 2009, pp. 211-220. |
Toderici et al, Finding Meaning on YouTube: Tag Recommendation and Category Discovery, 2010. * |
Toderici, H., et al., "Finding meaning on YouTube: Tag recommendation and category discovery," In Proc. of IEEE CVPR-10, 2010, 8 Pages. |
Tola, E., et al., "A Fast Local Descriptor for Dense Matching," Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, Alaska, USA, pp. 1-15. |
Van Laere, O., et al., "Towards Automated Georeferencing of Flickr Photos," GIR'10: Proceedings of the 6th Workshop on Geographic Information Retrieval, Feb. 18-19, 2010, pp. 1-7. |
Viola, P., et al., "Rapid Object Detection Using a Boosted Cascade of Simple Features," Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 2001, vol. 1, pp. 511-518. |
Visentini, I, et al., "On-Line Boosted Cascade for Object Detection," ICPR08, pp. 1-4. |
Visentini, I., et al., "Cascaded Online Boosting," Journal of Real-Time Image Processing, Dec. 2010, vol. 5, No. 4, pp. 245-257. |
Vuurpijl, L., et al., "An Overview and Comparison of Voting Methods for Pattern Recognition," Proceedings of the Eighth International Workshop on Frontiers in Handwriting Recognition (IWFHR02), 2002, pp. 195-200. |
Vuurpijl, L., et al., "Architectures for Detecting and Solving Conflicts: Two-Stage Classification and Support Vector Classifiers," International Journal on Document Analysis and Recognition (IJDAR), Jul. 2003, vol. 5, No. 4, pp. 213-223. |
Wang, P., et al., "Training a Multi-Exit Cascade with Linear Asymmetric Classification for Efficient Object Detection," Proceedings of 2010 IEEE 17th International Conference on Image Processing (ICIP10), Hong Kong, Sep. 26-29, 2010, pp. 61-64. |
Wang, Z., et al., "Youtubecat: Learning to categorize wild web videos," In Proc. IEEE Conf. Computer Vision and Pattern Recognition, Jun. 2010, pp. 879-886. |
Wu D., et al., "Resilient Subclass Discriminant Analysis," ICCV09, pp. 389-396. |
Wu, D., et al., "A Min-Max Framework of Cascaded Classifier with Multiple Instance Learning for Computer Aided Diagnosis," Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR09), 2009, pp. 1359-1366. |
Wu, J. X., et al., "Fast Asymmetric Learning for Cascade Face Detection," IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), Mar. 2008, vol. 30, No. 3, pp. 369-382. |
Wu, X., et al., "Incorporating Prior Knowledge with Weighted Margin Support Vector Machines," Proceedings of the International Conference on Knowledge Discovery and Data Mining (SIGKDD), 2004, pp. 326-333. |
Yagnik, J., et al., "Learning People Annotation from the Web Via Consistency Learning," Proc. Workshop on MIR, Augsburg, Bavaria, Germany, Sep. 28-29, 2007, pp. 285-290. |
Yan, S.Y., et al., "Matrix-Structural Learning (MSL) of Cascaded Classifier from Enormous Training Set," Computer Vision and Pattern Recognition (CVPR07), 2007, pp. 1-7. |
Yanai, K., et al., "Probabilistic Web Image Gathering," Proc. ACM SIGMM Workshop on MIR, 2005, pp. 57-64. |
Yang, E., et al., "Multi-modality web image categorization," in Proc. of ACM MIR-07, Sep. 28-29, 2007, pp. 265-274. |
Yang, J., et al., "Cross-Domain Video Concept Detection Using Adaptive SVMS," Proceedings of the 15th International Conference on Multimedia, Sep. 2007, Augsburg, Bavaria, Germany, 10 pages. |
Yang, L., et al., "Multi-Modality Web Video Categorization," Proc. MIR, 2007, ACM Press, pp. 265-274. |
Zanetti, S., et al., "A Walk Through the Web's Video Clips," First IEEE Workshop on Internet Vision, Computer Vision and Pattern Recognition (CVPRO8), 2008, 8 pages. |
Zhang, B.C., et al., "Discriminant Analysis Based on Kernelized Decision Boundary for Face Recognition," AVBPA05, LNCS 3546, 2005, pp. 966. |
Zhang, D., et al., ""Joint categorization of queries and clips for web-based video search, in Proc. of MIR'06, Oct. 26-27, 2006, pp. 193-202. |
Zhang, H., et al., "Automatic Partitioning of Full-Motion Video," Multimedia Systems, 1993, vol. 1, No. 1, pp. 10-28. |
Zhang, X.Q., "Clustering by Evidence Accumulation on Affinity Propagation," ICPR08, 2008, pp. 1-4. |
Zhao, M., et al., "Large Scale Learning and Recognition of Faces in Web Videos," Proceedings of the 8th IEEE International Conference on Automatic Face and Gesture Recognition, 2008, 7 pages. |
Zhao, R., et al., "Narrowing the Semantic Gap—Improved Text-Based Web Document Retrieval Using Visual Features," IEEE Transactions on Multimedia, Jun. 2002, vol. 4, No. 2, pp. 189-200. |
Zheng, Y.-T., et. al, "Tour the World: Building a Web-Scale Landmark Recognition Engine", Proceedings of the International Conference on Computer Vision and Pattern Recognition, Jun. 2009, Miami, Florida, U.S.A., pp. 1-8. |
Zhu, X., "Semi-Supervised Learning Literature Survey," Computer Sciences Technical Report 1530, University of Wisconsin-Madison, 2005, pp. 1-38. |
Zhu, X., et al., "Learning from Labeled and Unlabeled Data with Label Propagation," CMU CALD Tech Report CMU-CALD-02-107, 19 pages. |
Zhu, X., Semi-supervised learning literature survey. In Tech Report. University of Wisconsin—Madison, Jul. 2008, pp. 1-60. |
Cited By (47)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10007679B2 (en) | 2008-08-08 | 2018-06-26 | The Research Foundation For The State University Of New York | Enhanced max margin learning on multimodal data mining in a multimedia database |
US20170323178A1 (en) * | 2010-12-08 | 2017-11-09 | Google Inc. | Learning highlights using event detection |
US10867212B2 (en) * | 2010-12-08 | 2020-12-15 | Google Llc | Learning highlights using event detection |
US11556743B2 (en) * | 2010-12-08 | 2023-01-17 | Google Llc | Learning highlights using event detection |
US20120314941A1 (en) * | 2011-06-13 | 2012-12-13 | Microsoft Corporation | Accurate text classification through selective use of image data |
US8768050B2 (en) * | 2011-06-13 | 2014-07-01 | Microsoft Corporation | Accurate text classification through selective use of image data |
US9098807B1 (en) | 2011-08-29 | 2015-08-04 | Google Inc. | Video content claiming classifier |
US8706675B1 (en) * | 2011-08-29 | 2014-04-22 | Google Inc. | Video content claiming classifier |
US8924345B2 (en) * | 2011-09-26 | 2014-12-30 | Adobe Systems Incorporated | Clustering and synchronizing content |
US20130124462A1 (en) * | 2011-09-26 | 2013-05-16 | Nicholas James Bryan | Clustering and Synchronizing Content |
US8873836B1 (en) * | 2012-06-29 | 2014-10-28 | Emc Corporation | Cluster-based classification of high-resolution data |
US10135723B2 (en) * | 2012-08-10 | 2018-11-20 | International Business Machines Corporation | System and method for supervised network clustering |
US20140047091A1 (en) * | 2012-08-10 | 2014-02-13 | International Business Machines Corporation | System and method for supervised network clustering |
US11126720B2 (en) * | 2012-09-26 | 2021-09-21 | Bluvector, Inc. | System and method for automated machine-learning, zero-day malware detection |
US20170262633A1 (en) * | 2012-09-26 | 2017-09-14 | Bluvector, Inc. | System and method for automated machine-learning, zero-day malware detection |
US20160203318A1 (en) * | 2012-09-26 | 2016-07-14 | Northrop Grumman Systems Corporation | System and method for automated machine-learning, zero-day malware detection |
US9665713B2 (en) * | 2012-09-26 | 2017-05-30 | Bluvector, Inc. | System and method for automated machine-learning, zero-day malware detection |
US9646226B2 (en) * | 2013-04-16 | 2017-05-09 | The Penn State Research Foundation | Instance-weighted mixture modeling to enhance training collections for image annotation |
US20140307958A1 (en) * | 2013-04-16 | 2014-10-16 | The Penn State Research Foundation | Instance-weighted mixture modeling to enhance training collections for image annotation |
US20170006347A1 (en) * | 2013-08-16 | 2017-01-05 | The Directv Group, Inc. | Method and system for using hierarchical metadata for searching and recording content |
US9727821B2 (en) * | 2013-08-16 | 2017-08-08 | International Business Machines Corporation | Sequential anomaly detection |
US20150052090A1 (en) * | 2013-08-16 | 2015-02-19 | International Business Machines Corporation | Sequential anomaly detection |
US10231026B2 (en) * | 2013-08-16 | 2019-03-12 | The Directv Group, Inc. | Method and system for using hierarchical metadata for searching and recording content |
US9542623B2 (en) | 2014-06-11 | 2017-01-10 | Samsung Electronics Co., Ltd. | Image classification device, method for operating the same and electronic system comprising the image classification device |
CN106537390A (en) * | 2014-07-23 | 2017-03-22 | 微软技术许可有限责任公司 | Identifying presentation styles of educational videos |
US10248865B2 (en) * | 2014-07-23 | 2019-04-02 | Microsoft Technology Licensing, Llc | Identifying presentation styles of educational videos |
CN106537390B (en) * | 2014-07-23 | 2019-08-16 | 微软技术许可有限责任公司 | Identify the presentation style of education video |
US10649740B2 (en) * | 2015-01-15 | 2020-05-12 | International Business Machines Corporation | Predicting and using utility of script execution in functional web crawling and other crawling |
US10740071B2 (en) * | 2015-01-15 | 2020-08-11 | International Business Machines Corporation | Predicting and using utility of script execution in functional web crawling and other crawling |
CN107533638A (en) * | 2015-08-11 | 2018-01-02 | 谷歌有限责任公司 | Video is annotated using label probability of correctness |
US9779304B2 (en) * | 2015-08-11 | 2017-10-03 | Google Inc. | Feature-based video annotation |
CN107533638B (en) * | 2015-08-11 | 2023-08-11 | 谷歌有限责任公司 | Annotating video with tag correctness probabilities |
US10482328B2 (en) | 2015-08-11 | 2019-11-19 | Google Llc | Feature-based video annotation |
US20170046573A1 (en) * | 2015-08-11 | 2017-02-16 | Google Inc. | Feature-based Video Annotation |
US11200423B2 (en) | 2015-08-11 | 2021-12-14 | Google Llc | Feature-based video annotation |
CN106503723A (en) * | 2015-09-06 | 2017-03-15 | 华为技术有限公司 | A kind of video classification methods and device |
CN110168579A (en) * | 2016-11-23 | 2019-08-23 | 启创互联公司 | For using the system and method for the representation of knowledge using Machine learning classifiers |
US11544579B2 (en) | 2016-11-23 | 2023-01-03 | Primal Fusion Inc. | System and method for generating training data for machine learning classifier |
EP3545425A4 (en) * | 2016-11-23 | 2020-07-15 | Primal Fusion Inc. | System and method for using a knowledge representation with a machine learning classifier |
US11205103B2 (en) | 2016-12-09 | 2021-12-21 | The Research Foundation for the State University | Semisupervised autoencoder for sentiment analysis |
US11250488B1 (en) * | 2018-07-16 | 2022-02-15 | A9.Com, Inc. | Method and system for determining new categories with which a target user has not interacted |
US20200074321A1 (en) * | 2018-09-04 | 2020-03-05 | Rovi Guides, Inc. | Methods and systems for using machine-learning extracts and semantic graphs to create structured data to drive search, recommendation, and discovery |
US20200074322A1 (en) * | 2018-09-04 | 2020-03-05 | Rovi Guides, Inc. | Methods and systems for using machine-learning extracts and semantic graphs to create structured data to drive search, recommendation, and discovery |
CN111694954A (en) * | 2020-04-28 | 2020-09-22 | 北京旷视科技有限公司 | Image classification method and device and electronic equipment |
CN111694954B (en) * | 2020-04-28 | 2023-12-08 | 北京旷视科技有限公司 | Image classification method and device and electronic equipment |
US11475668B2 (en) | 2020-10-09 | 2022-10-18 | Bank Of America Corporation | System and method for automatic video categorization |
US20240095293A1 (en) * | 2021-07-26 | 2024-03-21 | Beijing Zitiao Network Technology Co., Ltd. | Processing method and apparatus based on interest tag, and device and storage medium |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8533134B1 (en) | Graph-based fusion for video classification | |
US8954358B1 (en) | Cluster-based video classification | |
US8649613B1 (en) | Multiple-instance-learning-based video classification | |
US8452778B1 (en) | Training of adapted classifiers for video categorization | |
US11693902B2 (en) | Relevance-based image selection | |
US8930288B2 (en) | Learning tags for video annotation using latent subtags | |
US20210192220A1 (en) | Video classification method and apparatus, computer device, and storage medium | |
US8473981B1 (en) | Augmenting metadata of digital media objects using per object classifiers | |
US9355330B2 (en) | In-video product annotation with web information mining | |
Tian et al. | Query-dependent aesthetic model with deep learning for photo quality assessment | |
US9087297B1 (en) | Accurate video concept recognition via classifier combination | |
US8396286B1 (en) | Learning concepts for video annotation | |
US8856051B1 (en) | Augmenting metadata of digital objects | |
EP2774119B1 (en) | Improving image matching using motion manifolds | |
US9177208B2 (en) | Determining feature vectors for video volumes | |
WO2017070656A1 (en) | Video content retrieval system | |
US8706655B1 (en) | Machine learned classifiers for rating the content quality in videos using panels of human viewers | |
Li et al. | A study on content-based video recommendation | |
US8990134B1 (en) | Learning to geolocate videos | |
WO2023040506A1 (en) | Model-based data processing method and apparatus, electronic device, computer-readable storage medium, and computer program product | |
Hazrati et al. | Addressing the New Item problem in video recommender systems by incorporation of visual features with restricted Boltzmann machines | |
Ciaparrone et al. | A comparison of deep learning models for end-to-end face-based video retrieval in unconstrained videos | |
Tsikrika et al. | Reliability and effectiveness of clickthrough data for automatic image annotation | |
Mironica et al. | Fisher kernel based relevance feedback for multimodal video retrieval | |
Papapanagiotou et al. | Improving concept-based image retrieval with training weights computed from tags |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:ZHAO, MING;WANG, ZHESHEN;SONG, YANG;SIGNING DATES FROM 20101028 TO 20101102;REEL/FRAME:025244/0169 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |