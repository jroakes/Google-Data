US20180136794A1 - Determining graphical element(s) for inclusion in an electronic communication - Google Patents
Determining graphical element(s) for inclusion in an electronic communication Download PDFInfo
- Publication number
- US20180136794A1 US20180136794A1 US15/350,040 US201615350040A US2018136794A1 US 20180136794 A1 US20180136794 A1 US 20180136794A1 US 201615350040 A US201615350040 A US 201615350040A US 2018136794 A1 US2018136794 A1 US 2018136794A1
- Authority
- US
- United States
- Prior art keywords
- user
- graphical element
- graphical
- communication
- graphical elements
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G06F17/276—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04817—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance using icons
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/274—Converting codes to words; Guess-ahead of partial word inputs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q50/00—Systems or methods specially adapted for specific business sectors, e.g. utilities or tourism
- G06Q50/01—Social networking
-
- H04L51/22—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/42—Mailbox-related aspects, e.g. synchronisation of mailboxes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/015—Input arrangements based on nervous system activity detection, e.g. brain waves [EEG] detection, electromyograms [EMG] detection, electrodermal response detection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/02—Input arrangements using manually operated switches, e.g. using keyboards or dials
- G06F3/023—Arrangements for converting discrete items of information into a coded form, e.g. arrangements for interpreting keyboard generated codes as alphanumeric codes, operand codes or instruction codes
- G06F3/0233—Character input methods
- G06F3/0237—Character input methods using prediction or retrieval techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0489—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using dedicated keyboard keys or combinations thereof
- G06F3/04895—Guidance during keyboard input operation, e.g. prompting
Definitions
- a user may include graphical elements in one or more electronic communications in addition to, or in lieu of, textual content.
- Graphical elements may take many forms such as emojis, Graphics Interchange Format images (GIFs), so-called stickers (which may be curated GIFs, JPGs, etc.), etc.
- Some graphical elements may themselves be completely void of any text when displayed (e.g., a smiley face emoji), while other graphical elements may include text when displayed (e.g., a GIF that includes graphical text in addition to other content).
- This specification is directed to methods, apparatus, and computer readable media related to determining graphical element(s) (e.g., emojis, GIFs, stickers) for inclusion in an electronic communication being formulated by a user via a computing device of the user, and providing the graphical element(s) for inclusion in the electronic communication.
- the graphical element(s) may be provided for presentation to the user via a display of the computing device of the user and, in response to user interface input directed to one of the graphical element(s), that graphical element may be incorporated in the electronic communication.
- the electronic communication is a communication to be submitted as part of a dialog that involves the user and one or more additional users.
- provided graphical element(s) are determined based at least in part on one or more features that are in addition to any input provided by the user in formulating the electronic communication for which the graphical elements are provided for inclusion.
- the graphical element(s) may be provided for inclusion in the electronic communication before the user has provided any textual content for inclusion in the electronic communication and/or before the user has provided any other content for inclusion in the electronic communication.
- the graphical element(s) may be provided in response to user interface input related to the electronic communication, such as the user selecting a “reply” interface element to initiate formulation of the electronic communication and/or selecting a “content suggestion” interface element.
- the features that are in addition to any input provided by the user in formulating the electronic communication may include one or more of: submitted communication feature(s) of one or more previously submitted electronic communications of the dialog to which the electronic communication is to be submitted (e.g., prior dialog communication(s) of the user and/or of additional users); current state feature(s) that indicate current state(s), of one or more dynamic user states (e.g., location, sentiment), of one or more users engaged in the dialog; and/or historical usage of graphical elements by the user in the dialog, in other dialogs, and/or in other communications.
- the graphical elements may additionally or alternatively be determined based on subsequent communication features that are based on input provided by the user in formulating the electronic communication for which the graphical elements are to be provided for inclusion.
- which graphical element(s) are provided for inclusion in an electronic communication of a dialog may be determined based on output generated over a trained machine learning model in response to applying one or more features (e.g., one or more of those described above) as input to the trained machine learning model.
- submitted communication features of submitted electronic communications of a dialog and current state(s) of user(s) engaged in the dialog may be provided as input to the trained machine learning model.
- Output may be generated over the model based on the input, and the output may indicate one or more graphical element features of graphical elements. The output may be used to select one or more graphical elements to provide for potential inclusion in the electronic communication, based on those graphical elements having graphical element features that match the output.
- a corpus of past electronic communications is analyzed to determine relationships between graphical elements and one or more features (e.g., one or more of those described above). For example, the corpus may be analyzed to determine, for a given graphical element, one more features of communications in which the given graphical element is included and/or one or more features of previously submitted communications in which the given graphical element is included in a reply to those previously submitted communications. In some implementations, a corpus of past electronic communications is analyzed to determine relationships between previously submitted communication(s) of dialogs and one or more subsequent communications of the dialogs.
- the corpus may be analyzed to determine that a dialog that includes submitted communication(s) that include the n-gram “dinner” are likely to include a subsequent communication that includes certain term(s) and/or graphical elements having certain graphical element features (e.g., graphical elements having an assigned “food” feature).
- determining the relationships may be achieved via generating appropriate training examples based on a corpus of electronic communications and training a machine learning model based on those training examples to effectively embed the relationships in the trained machine learning model.
- some implementations of determining and providing graphical elements to present to a user based on various features described herein may increase the likelihood that presented graphical elements are relevant to the user and/or to a current context of a dialog. This may mitigate the need for the user to navigate through and/or explicitly search a litany of irrelevant available graphical elements in an attempt to locate a relevant graphical element. This may reduce the use of various computational resources, such as resources of computing device(s) that are required for rendering and/or retrieving the irrelevant graphical elements.
- some implementations of determining and providing graphical elements to present to a user based on various features described herein may enable a more concise dialog to be achieved as a result of the presented graphical elements being selected by the user for inclusion in a communication of the dialog, and providing relevant additional context to the communication. This may obviate the need for certain further communications of the dialog that would otherwise be needed to provide the additional context, thereby reducing the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the further communications to the user(s). Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- a method performed by one or more processors includes receiving an electronic communication submitted as part of a dialog that includes a user and one or more additional users.
- the electronic communication is based on user interface input generated by an additional user, of the additional users, via one or more user interface input devices of an additional user computing device of the additional user.
- the method further includes determining at least one communication feature of the electronic communication and determining a group of one or more graphical elements for inclusion in a subsequent electronic communication, of the user, to be submitted as part of the dialog. Determining the group of graphical elements includes, prior to any textual input provided by the user for the subsequent electronic communication: selecting, from candidate graphical elements, at least one graphical element to include in the group.
- Selecting the graphical element is based on the at least one communication feature of the electronic communication.
- the method further includes, prior to any textual input provided by the user for the subsequent electronic communication, providing the group of graphical elements for potential inclusion in the subsequent electronic communication.
- the group of graphical elements are provided for presentation via a display of a user computing device of the user.
- the method further includes identifying user interface interaction with a user interface element that is graphically presented to the user via the display in combination with a presentation of the electronic communication. In some of those implementations, providing the group of graphical elements is in response to identifying the user interface interaction with the user interface element.
- the at least one communication feature is based on one or more of a plurality of terms of the electronic communication.
- the at least one communication feature further includes at least one non-textual feature that is based at least in part on one or more signals that are in addition to the terms of the electronic communication.
- the method further includes determining at least one additional communication feature of an additional electronic communication of the dialog, and selecting the graphical element further based on the at least one additional communication feature.
- the method further includes determining at least one current state feature that indicates a current state of at least one dynamic user state of the user or the additional user, and selecting the graphical element further based on the current state feature.
- the dynamic user state includes a physical location or a sentiment.
- the method further includes identifying a historical usage feature for the graphical element, and selecting the graphical element further based on the historical usage feature.
- the historical usage feature is based at least in part on past usage by the user of additional graphical elements sharing one or more graphical element features with the graphical element.
- selecting the graphical element based on the at least one communication feature includes: determining a graphical element feature based on the at least one communication feature, the graphical element feature being assigned to the graphical element and being assigned to a plurality of additional graphical elements; and selecting the graphical element to include in the group in lieu of one or more of the additional graphical elements based on the graphical element being associated with a greater degree of historical usage, by the user, than the non-selected one or more of the additional graphical elements.
- the graphical element is included in a cohesive pack of graphical elements and the historical usage of the graphical element is based on the historical usage, by the user, of the graphical elements of the cohesive pack.
- selecting the graphical element based on the at least one communication feature includes: applying input to a trained machine learning model stored in one or more computer readable media, the input comprising the at least one communication feature; generating, over the model and based on the input, output that indicates one or more graphical element features; and selecting the graphical element based on the graphical element features being assigned to the graphical element.
- the output that indicates one or more graphical element features includes one or more terms and selecting the graphical element based on the graphical element features being assigned to the graphical element comprises selecting the graphical element based on it being indexed by at least one of the one or more terms.
- the method further includes: generating the trained machine learning model based on a plurality of training examples derived from past user dialogs, where of the training examples includes: training example input based on communication features of a corresponding original electronic communication, and training example output based on text of a corresponding reply electronic communication that is a reply to the corresponding original electronic communication.
- Generating the trained machine learning model may include training the trained machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples.
- the method further includes: generating the trained machine learning model based on a plurality of training examples derived from past user dialogs, where each of the training examples includes: training example input based on communication features of a corresponding original electronic communication, and training example output based on graphical element features of a corresponding reply electronic communication that is a reply to the corresponding original electronic communication.
- Generating the trained machine learning model may include training the trained machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples.
- the communication feature includes one or more terms and selecting the graphical element based on the at least one communication feature includes selecting the graphical element based on it being indexed by at least one of the one or more terms.
- the method further includes, after providing the group of graphical elements for potential inclusion in the subsequent electronic communication: receiving textual input provided by the user for the subsequent electronic communication, the textual input being based on user interface input generated by the user via one or more user interface input devices of the user computing device; determining a modified group of graphical elements based on the textual input and based on the at least one communication feature; and providing the modified group of graphical elements for potential inclusion in the subsequent electronic communication in combination with the textual input.
- a method performed by one or more processors includes receiving an electronic communication submitted as part of a dialog that includes the user and one or more additional users.
- the electronic communication is based on user interface input generated by an additional user of the additional users via one or more user interface input devices of an additional user computing device of the additional user.
- the method further includes: determining at least one communication feature of the electronic communication; receiving textual input provided by the user for a subsequent electronic communication, of the user, to be submitted as part of the dialog; and determining a group of one or more graphical elements for inclusion in the subsequent electronic communication.
- Determining the group of graphical elements includes: selecting, from candidate graphical elements, at least one graphical element to include in the group, where selecting the graphical element is based on the textual input and is based on the at least one communication feature; and providing the group of graphical elements for potential inclusion in the subsequent electronic communication.
- the group of graphical elements are provided for presentation via a display of a user computing device of the user.
- providing the group of graphical elements is in response to receiving the textual input and occurs without further user interface input that explicitly requests provision of graphical elements.
- the communication feature includes one or more terms and selecting the graphical element based on the at least one communication feature includes selecting the graphical element based on it being indexed by at least one of the one or more terms.
- the method further includes determining at least one current state feature that indicates a current state of at least one dynamic user state of the user or the additional user, and selecting the graphical element further based on the current state feature.
- a method performed by one or more processors includes receiving content provided by a user for inclusion in an electronic communication to be submitted by the user as part of a dialog.
- the content is based on user interface input generated by the user via one or more user interface input devices of a computing device of the user.
- the method further includes determining at least one feature of the content and determining a group of one or more graphical elements for inclusion in the electronic communication. Determining the group of graphical elements includes: selecting, from candidate graphical elements, at least one graphical element to include in the group, where selecting the graphical element is based on the feature of the content matching at least one graphical element feature assigned to the graphical element; and providing the group of graphical elements for potential inclusion in the electronic communication.
- the group of graphical elements are provided for presentation via a display of a user computing device of the user.
- implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- FIG. 1 is a block diagram of an example environment in which implementations disclosed herein may be implemented.
- FIG. 2 illustrates an example of using components of the example environment of FIG. 1 in selecting graphical elements for providing to a user.
- FIG. 3A illustrates an example client computing device with a display screen displaying an example of dialog that includes a user of the client computing device and a plurality of additional users; the dialog includes a submitted electronic communication from each of two of the additional users.
- FIGS. 3 B 1 , 3 B 2 , and 3 B 3 illustrate an example of how graphical elements may be presented to the user and included in a subsequent electronic communication that is submitted by the user as part of the dialog of FIG. 3A .
- FIG. 3C illustrates another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog of FIG. 3A .
- FIGS. 3 D 1 and 3 D 2 illustrate yet another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog of FIG. 3A .
- FIG. 4 illustrates the example client computing device of FIG. 3A , and illustrates another example of a dialog and another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog.
- FIG. 5A is a flowchart illustrating an example method of providing a group of graphical elements for inclusion in a dialog according to implementations disclosed herein.
- FIG. 5B is a flowchart illustrating an example of one block of the flowchart of FIG. 5A .
- FIG. 5C is a flowchart illustrating another example of the one block of the flowchart of FIG. 5A .
- FIG. 6 is a flowchart illustrating an example method of assigning features to graphical elements.
- FIG. 7 is a flowchart illustrating an example method of training a machine learning content model.
- FIG. 8 illustrates an example architecture of a computing device.
- This specification is directed to technical features related to determining graphical element(s) for inclusion in an electronic communication being formulated by a user via a computing device of the user, and providing the graphical element(s) for inclusion in the electronic communication.
- the graphical element(s) may be provided for presentation to the user via a display of the computing device and, in response to user interface input directed to one of the graphical element(s), that graphical element may be incorporated in the electronic communication.
- provided graphical element(s) are determined based at least in part on one or more features that are in addition to any input provided by the user in formulating the electronic communication for which the graphical elements are provided for inclusion.
- the graphical element(s) may be provided for inclusion in the electronic communication before the user has provided any textual content and/or any other content for inclusion in the electronic communication.
- the graphical element(s) may be provided in response to user interface input related to the electronic communication.
- the features that are in addition to any input provided by the user in formulating the electronic communication may include one or more of: submitted communication feature(s) of one or more previously submitted electronic communications of the dialog to which the electronic communication is to be submitted; current state feature(s) that indicate current state(s), of one or more dynamic user states, of one or more users engaged in the dialog; and/or historical usage of graphical elements by the user in the dialog, in other dialogs, and/or in other communications.
- the graphical elements may additionally or alternatively be determined based on subsequent communication features that are based on input provided by the user in formulating the electronic communication for which the graphical elements are to be provided for inclusion.
- which graphical element(s) are provided for inclusion in an electronic communication of a dialog may be determined based on output generated over a trained machine learning model in response to applying one or more features (e.g., one or more of those described above) as input to the trained machine learning model.
- Output may be generated over the model based on the input, and the output may indicate one or more graphical element features of graphical elements.
- the output may be used to select one or more graphical elements to provide for potential inclusion in the electronic communication, based on those graphical elements having graphical element features that match the output.
- a corpus of past electronic communications is analyzed to determine relationships between graphical elements and one or more features (e.g., one or more of those described above). In some implementations, a corpus of past electronic communications is analyzed to determine relationships between previously submitted communication(s) of dialogs and one or more subsequent communications of the dialogs.
- determining the relationships may be achieved via generating appropriate training examples based on a corpus of electronic communications and training a machine learning model based on those training examples to effectively embed the relationships in the trained machine learning model.
- some implementations of determining and providing graphical elements to present to a user based on various features described herein may increase the likelihood that presented graphical elements are relevant to the user and/or to a current context of a dialog. This may mitigate the need for the user to navigate through and/or explicitly search a litany of irrelevant available graphical elements in an attempt to locate a relevant graphical element. This may reduce the use of various computational resources, such as resources of computing device(s) that are required for rendering and/or retrieving the irrelevant graphical elements.
- some implementations of determining and providing graphical elements to present to a user based on various features described herein may enable a more concise dialog to be achieved as a result of the presented graphical elements being selected by the user for inclusion in a communication of the dialog, and providing relevant additional context to the communication. This may obviate the need for certain further communications of the dialog that would otherwise be needed to provide the additional context, thereby reducing the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the further communications to the user(s). Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- the example environment includes a communication network 101 that optionally facilitates communication between the various components in the environment.
- the communication network 101 may include the Internet, one or more intranets, and/or one or more bus subsystems.
- the communication network 101 may optionally utilize one or more standard communications technologies, protocols, and/or inter-process communication techniques.
- the example environment also includes a client device 106 A, additional client device(s) 106 B-N, electronic communications system 110 , graphical element system 120 , content model training engine 135 , and graphical element features engine 137 .
- the example environment further includes an electronic communications database 152 , a graphical elements database 154 , a content model(s) database 156 , a user state model(s) database 158 , and a historical model(s) database 160 .
- client device(s) 106 A-N include one or more of: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a vehicle of the user (e.g., an in-vehicle communications system, an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a watch of the user having a computing device, glasses of the user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided.
- a desktop computing device e.g., a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a vehicle of the user (e.g., an in-vehicle communications system, an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a
- a given user may communicate with all or aspects of graphical element system 120 utilizing a plurality of client devices that collectively form a coordinated “ecosystem” of client devices.
- client devices that collectively form a coordinated “ecosystem” of client devices.
- some examples described in this disclosure will focus on the user operating a single client device.
- Electronic communications system 110 , graphical element system 120 , and/or engines 135 and/or 137 may each be implemented in one or more computing devices that communicate, for example, through a network (e.g., network 101 and/or other network).
- Electronic communications system 110 , graphical element system 120 , and engines 135 and 137 are example components via which the systems and techniques described herein may be implemented and/or with which systems and techniques described herein may interface. They may each include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network.
- electronic communications system 110 , graphical element system 120 , and/or engines 135 and/or 137 may include one or more components of the example computing device of FIG. 8 . The operations performed by electronic communications system 110 , graphical element system 120 , and/or engines 135 and/or 137 may be distributed across multiple computer systems.
- one or more aspects of one or more of electronic communications system 110 , graphical element system 120 , and/or engines 135 and/or 137 may be combined in a single system and/or one or more aspects may be implemented on the client device 106 A and/or one or more of the additional client device(s) 106 B-N.
- the client device 106 A may include an instance of one or more aspects of the graphical element system 120 and each of the additional client device(s) 106 B-N may also include an instance of one or more aspects of the graphical element system 120 .
- the client device 106 A and each of the additional client device(s) 106 B-N may each include an instance of the electronic communications system 110 (e.g., the electronic communications system 110 may be an application installed and executing on each of the devices). As yet another example, one or more aspects of electronic communications system 110 and graphical element system 120 may be combined.
- the electronic communications database 152 includes one or more storage mediums that include all, or portions of (e.g., extracted features of), electronic communications of past dialogs of a plurality of users.
- the electronic communications database 152 is maintained by the electronic communications system 110 .
- the electronic communications system 110 may include one or more chat systems and the electronic communications database 152 may include a plurality of electronic communications that are sent and/or received via the system(s).
- the electronic communications system 110 may include one or more social networking systems and the electronic communications database 152 may include a plurality of messages, posts, or other electronic communications that are sent and/or received via the social networking system(s).
- an “electronic communication” or “communication” may refer to an email, a text message (e.g., RCS, SMS, MMS), a chat message, an instant message, or any other electronic communication that is sent from a user to a group of one or more additional users as part of a dialog.
- an electronic communication may include various metadata and the metadata may optionally be utilized in one or more techniques described herein.
- an electronic communication such as chat message may include one or more sender identifiers, one or more recipient identifiers, a date sent, a type of device that sent and/or received the electronic communication, and so forth.
- Graphical element system 120 determines and provides groups of graphical elements for potential inclusion in an electronic communication to be submitted by a user as part of a dialog. For convenience, many examples described herein will be described with respect to graphical element system 120 providing client device 106 A with graphical elements for inclusion in an electronic communication, where the electronic communication is being composed by a user via the client device 106 A to be submitted as part of a dialog that involves the user and additional users of additional client device(s) 106 B-N. It is understood that graphical element system 120 (or other instances thereof) may also provide each of client device(s) 106 B-N with graphical elements for inclusion in communications being composed via those client device(s) 106 B-N
- the graphical element system 120 may determine a group of graphical elements to provide for an electronic communication to be submitted by a user as part of a dialog, and may determine the group based on: one or more submitted communication features of previously submitted electronic communications of the dialog and/or one or more additional features, such as those described herein. In some implementations, the graphical element system 120 may determine and/or provide the group of graphical elements independent of any textual input provided by the user in generating the electronic communication to be submitted as part of the dialog and/or independent any other content provided by the user in generating the electronic communication to be submitted as part of the dialog.
- the graphical elements determined and provided by graphical element system 120 include one or more emojis, GIFs, JPGs, and/or stickers.
- the graphical elements may be selected from graphical elements database 154 , which is provided on one or more storage mediums.
- the graphical elements database 154 is provided locally on client device 106 A.
- the graphical element system 120 is operated, in whole or in part, by one or more processors of the client device 106 A.
- all or portions of the graphical elements database 154 and/or one or more aspects of the graphical element system 120 are additionally or alternatively provided remote from the client device 106 A.
- the graphical elements database 154 may define, for each of the graphical elements, associations of the graphical element to one or more graphical element features, and optionally weights for one or more of the associations.
- each of the graphical elements may be indexed by one or more corresponding graphical element features and weights for each of the graphical element features.
- a weight of a given graphical element feature of a graphical element indicates how strongly the given graphical element feature is associated with the graphical element. For example, a given graphical element that depicts an alligator may be associated with a graphical element feature of “alligator” more strongly than it is associated with a graphical element feature of “animal”.
- graphical element features may include descriptors of content included in the graphical element.
- a graphical element that includes a depiction of an alligator may include descriptors such as a descriptor of “alligator”, a descriptor of “animal”.
- non-textual descriptors may be provided such as a non-textual descriptor that identifies an entity associated with a graphical element.
- graphical element features may include features of electronic communications, of a corpus of past electronic communications, in which the graphical element was included in those electronic communications.
- a graphical element that includes a depiction of an alligator may include a textual feature of “later” that indicates it at least sometimes co-occurs in electronic communications with the n-gram “later.”
- graphical element features may include features of electronic communications, of a corpus of past dialogs, in which those electronic communications preceded, in the dialogs, a subsequent electronic communication that includes the graphical element.
- a graphical element that includes a depiction of an alligator may include a feature that indicates a “departing remark” such as “talk to you soon”, “bye”, etc. that indicates the graphical element at least sometimes occurs in electronic communications that are subsequent to such “departing remark” electronic communications in dialogs.
- graphical element features may include features of user states associated with the graphical element.
- a graphical element that includes a depiction of an alligator may include a feature that indicates the graphical element at least sometimes occurs when user(s) of a dialog are located in Florida or Louisiana and/or when user(s) of a dialog have a “jovial” sentiment.
- one or more graphical element features may be assigned to graphical elements by graphical element features engine 137 .
- the graphical element features engine 137 may assign features to a given graphical element based on analysis of past dialogs that include the graphical element and/or based on those features being assigned (directly or indirectly) to the graphical element in content model(s) of content model(s) database 156 (described in more detail below).
- one or more graphical element features of a graphical element may additionally or alternatively be assigned based on image analysis of the graphical element, may be assigned by human reviewers, and/or may be assigned utilizing other techniques.
- the content mode(s) database 156 includes one or more storage mediums that store one or more content models optionally utilized in various techniques described herein.
- one or more of the content models define relationships between various dialog features described herein and features of graphical elements.
- a content model may define relationships between communication features of electronic communications of a dialog and one or more features of graphical elements.
- one or more of the content models define relationships between various dialog features described herein and features of text.
- a content model may define relationships between communication features of earlier submitted electronic communications of a dialog and one or more features of text included in later submitted communications of the dialog.
- Such a content model may be utilized, for example, to identify graphical elements that are indexed by, or otherwise associated with, graphical element features corresponding to those features of text.
- the content models of database 156 include one or more learned content models that are each trained by content model training engine 135 based on training examples derived from past electronic communications of electronic communications database 152 .
- a content model of database 156 may be a trained machine learning model that has been trained to predict features of graphical elements based on various dialog features. For instance, it may be trained based on training examples that each include training example input that includes features of one or more earlier submitted communications of a corresponding dialog and training example output that includes feature(s) of graphical element(s) included in a later submitted communication of the corresponding dialog (e.g., a later submitted communication that is a direct reply to a corresponding one of the earlier submitted communications).
- graphical element system 120 may include a submitted communication(s) features engine 122 , a user state engine 124 , a subsequent communication features engine 126 , a historical usage engine 128 , a selection engine 130 , and/or a presentation engine 132 .
- User state engine 124 may optionally be in communication with one or more user state models of user state models database 158 .
- Historical usage engine 128 may optionally be in communication with one or more historical models of historical models database 160 .
- all or aspects of engines 122 , 124 , 126 , 128 , 130 , and/or 132 may be omitted.
- engines 122 , 124 , 126 , 128 , 130 , and/or 132 may be combined. In some implementations, all or aspects of engines 122 , 124 , 126 , 128 , 130 , and/or 132 may be implemented in a component that is separate from graphical element system 120 , such as client device 106 A and/or electronic communications system 110 .
- the submitted communication(s) features engine 122 determines one or more communication features of one or more communications that have already been submitted in a dialog. For example, the engine 122 may determine communication feature(s) for the most recently submitted communication in the dialog, for a threshold quantity of the most recently submitted communications in the dialog, and/or for any communications submitted in the dialog within a threshold amount of time relative to a current time.
- the engine 122 may determine various communication features. For example, the engine 122 may determine one or more n-grams in the submitted electronic communication(s) as communication features. For instance, one or more of the n-grams may be determined based on term frequency of the n-gram in the submitted electronic communication(s) (i.e., how often the n-gram occurs in the electronic communication) and/or inverse document frequency of the n-gram in a collection of documents (i.e., how often the n-gram occurs in a collection of documents, such as a collection of electronic communications).
- term frequency of the n-gram in the submitted electronic communication(s) i.e., how often the n-gram occurs in the electronic communication
- inverse document frequency of the n-gram in a collection of documents i.e., how often the n-gram occurs in a collection of documents, such as a collection of electronic communications.
- the engine 122 may determine co-occurrence of two or more n-grams in one or more of the submitted electronic communication(s) as a communication feature, such as co-occurrence in a particular order (e.g., a first n-gram before a second n-gram), in a certain positional relationship (e.g., within n terms or characters of one another), etc.
- submitted communication(s) features engine 122 may determine one or more communication features based on one or more natural language processing tags or other labels applied to text of the submitted electronic communication(s) (e.g., parts of speech, named entities, entity types, tone); features based on text that occurs specifically in the first sentences, the last sentences, or other portion of the electronic communications; features based on metadata of the electronic communications such as a time the electronic communications were sent, day of the week the electronic communication were sent, a number of participants in the dialog, type(s) of device(s) that sent the electronic communication(s), etc. Also, for example, the engine 122 may determine one or more graphical element features of graphical elements included in one or more of the submitted electronic communication(s).
- natural language processing tags or other labels applied to text of the submitted electronic communication(s) e.g., parts of speech, named entities, entity types, tone
- features based on text that occurs specifically in the first sentences, the last sentences, or other portion of the electronic communications
- features based on metadata of the electronic communications such as a
- the engine 122 may determine predicted textual suggestion(s) that are contextually relevant to the submitted electronic communications (e.g., textual suggestions 194 of FIG. 3C ) and utilize those textual suggestion(s) as communication feature(s).
- the engine 122 may determine an embedding vector of one or more features from the entirety of one or more submitted electronic communications or subset(s) of the submitted electronic communication(s) (e.g., one or more paragraphs, one or more sentences, one or more words). For instance, the engine 122 may apply or more features from a communication to a learned word embedding and determine, over the word embedding, an embedding vector of values mapped to the features in the word embedding.
- the embedding vector of values may be of a lower dimension than the applied features.
- the features utilized to determine the embedding vector may include one or more n-grams, labels applied to one or more n-grams, syntactic features, semantic features, metadata features, and/or other features.
- the engine 122 provides determined communication features to selection engine 130 for utilization by selection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog.
- the subsequent communication features engine 126 determines one or more subsequent communication features (if any) of the subsequent communication. For example, where the user has provided content to be included in the subsequent communication (e.g., textual input, graphical element(s)), the engine 126 may determine features of that content. Various features may be determined, such as one or more of those described above with respect to the engine 122 . The engine 126 provides determined subsequent communication features to selection engine 130 for utilization by selection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog.
- the user state engine 124 determines one or more current states of one or more dynamic states of one or more users engaged in the dialog.
- the user state engine 124 may determine the current state(s) of user(s) after appropriate authorization is obtained from those user(s).
- a current state may indicate a location of one or more users engaged in the dialog.
- a current state may indicate a current location of the user that is formulating the subsequent communication to be submitted as part of the dialog.
- the location may be defined at various levels of granularity such as a city, state, ZIP code, a class of location (e.g., home, restaurant, Italian restaurant, concert venue).
- a current state may indicate the current location(s) of one or more additional users in the dialog, such as the user that most recently submitted a communication in the dialog.
- a current state may indicate one or more classes and/or magnitudes of sentiment for one or more users engaged in the dialog.
- classes and/or magnitudes may be utilized.
- classes that are indicative of happy, sad, neutral, active, tired, stressed, and/or other user state(s) may be utilized.
- the user state engine 124 may also determine a magnitude of one or more of the sentiments. For instance, the user state engine 124 may determine a weight of “0.8” for “happy”, a weight of “0.7” for “active”, and a weight of “0.0” for the other classes.
- more granular classes may additionally and/or alternatively be utilized such as slightly happy, medium happy, and very happy (e.g., in lieu of a general “happy” class).
- the user state engine 124 may utilize content of communications of the dialog in determining sentiment for one or more users.
- the user state model(s) database 158 may include a sentiment classifier model and the engine 124 may apply, as input to the model, textual content from one or more communications of the dialog and generate, over the model based on the input, an indication of a sentiment indicated by the textual content.
- graphical elements included in submitted communications of a dialog may be mapped to graphical element features that indicate sentiment associated with those graphical elements. The user state engine 124 may utilize such mappings in determining sentiment.
- the user state engine 124 determines sentiment for one or more users using one or more indicators that are in addition to content of communications of the dialog, such as indicators based on one or more sensors (e.g., camera, microphone, keyboard sensors, touchscreen sensors, heart rate sensor) of a computing device (e.g., one of client devices 106 A-N) of at least one of the users participating in the dialog.
- the user state engine 124 determines class(es) and/or magnitude(s) of sentiment based on applying one or more user state indicators to one or more trained classifier model(s) or other trained machine learning model(s) of user state models database 158 .
- one or more images of a user captured by a camera may be passed through a face detector and further through a facial expression classifier whose output indicates whether the facial expression is happy, tired, sad, and/or other class.
- the user state engine 124 may determine the sentiment based at least in part on the output of the facial expression classifier.
- a typing speed of a user determined from sensor data associated with one of the user interface input devices of a client device of the user may be applied by the user state engine 124 to a user state model of database 158 and generated output utilized to determine sentiment of the user.
- the user state engine 124 determines sentiment based on synthesizing multiple indicators. For example, the user state engine 124 may apply an image of a user captured by a camera to a first classifier trained to predict class(es) of sentiment based on images and may apply heart rate sensor data to a second classifier trained to predict class(es) of sentiment based on heart rate sensor data. The user state engine 124 may consider the outputs of the two classifiers in determining the sentiment. For example, the user state engine 124 may average the outputs to determine the sentiment, use the greater magnitude output in determining the sentiment, and/or otherwise consider and/or combine both outputs in determining the sentiment.
- the user state engine 124 determines current state features(s) indicative of current states of multiple users involved in a dialog.
- the user state engine 124 may determine current state feature(s) indicative of the current states of multiple users utilizing various techniques. For example, the user state engine 124 may determine one or more classes and/or magnitudes of sentiment for a first user, determine one or more classes and/or magnitudes of sentiment for a second user, and determine a combined sentiment based on the classes and/or magnitudes for both users. For example, the user state engine 124 may average the classes and magnitudes of the two users. Additional or alternative techniques of determining current state feature(s) that is indicative of the user states of multiple users may be utilized.
- the user state engine 124 may determine current state feature(s) based on sensor data from one or more user interface input device(s) used by users engaged in the dialog. For example, when a given user has provided typed input in a dialog (e.g., using a physical keyboard or virtual keyboard), the sentiment of the given user may be determined based on a typing speed of the typed input, applied pressure for one or more characters of typed input (e.g., as sensed by a touch screen implementing a virtual keyboard), a “delay time” for starting to provide the typed input (e.g., when the typed input is provided responsive to other content), and/or other sensed features of the typed input.
- a dialog e.g., using a physical keyboard or virtual keyboard
- the sentiment of the given user may be determined based on a typing speed of the typed input, applied pressure for one or more characters of typed input (e.g., as sensed by a touch screen implementing a virtual keyboard), a “delay time” for starting
- the sentiment of the given user may be determined based on tone, inflection, intonation, accent, pitch, volume, breathing volume, breathing rate, background noise level, background noise type, and/or other features of the voice input.
- the user state engine 124 may determine sentiment for the given user based on audio input from a microphone, even when the microphone isn't used by the given user to provide textual input.
- the user state engine 124 may determine the sentiment based on the presence and/or absence of certain types of sound (e.g., laughter, crying, music) in the audio input, background noise level in the audio input, breathing noise level in the audio input, breathing rate in the audio input, aspects of detected speech in the audio input (e.g., intonation, accent), etc.
- certain types of sound e.g., laughter, crying, music
- background noise level in the audio input e.g., breathing noise level in the audio input
- breathing rate in the audio input e.g., breathing rate in the audio input
- aspects of detected speech in the audio input e.g., intonation, accent
- the user state engine 124 provides determined current state feature(s) to selection engine 130 for utilization by selection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog.
- the historical usage engine 128 stores, in historical models database 160 , historical usage features related to past usage of graphical elements by the user composing the communication to be submitted as part of the dialog.
- Various types of historical usage features may be stored.
- the historical usage engine 128 may store a quantity and/or frequency of: usage of each of a plurality of individual graphical elements (e.g., frequency of usage of a particular emoji); usage of graphical elements having particular graphical features (e.g., frequency of usage of graphical elements having a “happy” graphical element feature); usage of graphical elements from a particular cohesive pack (e.g., frequency of usage of graphical elements from a “custom” sticker pack installed by the user).
- historical usage features may optionally be associated with context information that indicates context of the historical usage, such as users(s) engaged in the dialog(s) associated with the usage; locations, dates, and/or times associated with the usage; etc.
- context information such as users(s) engaged in the dialog(s) associated with the usage; locations, dates, and/or times associated with the usage; etc.
- historical usage features may indicate a frequency of usage of graphical elements having a particular graphical feature in combination with dialog(s) involving a group of one or more other users.
- the historical usage engine 128 provides historical usage features to selection engine 130 for utilization by selection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog.
- the selection engine 130 utilizes one or more of the features provided by the engines 122 , 124 , 126 , and/or 128 to determine a group of graphical elements to provide for inclusion in subsequent communication to be submitted as part of the dialog.
- the selection engine 130 utilizes features from each of the engines 122 , 124 , 126 , and 128 .
- features may only be provided by a subset of the engines 122 , 124 , 126 , and 128 (e.g., one or more engines may be omitted) and/or the selection engine 130 may only utilize a subset of provided features.
- the selection engine 130 selects one or more graphical elements for inclusion in the group based on provided features matching one or more graphical element features that are assigned to those graphical elements in graphical elements database 154 . In some of those implementations, the selection engine 130 may further select the graphical elements for inclusion based on weights of the matching graphical element features for the selected graphical elements. As one example, assume submitted communication features engine 122 provides a communication feature that is an n-gram “dinner” and that user state engine 124 provides a current state feature that indicates a current state of “jovial”.
- the selection engine 130 may select one or more graphical elements that have graphical element features that include an n-gram of “dinner” and a current state of “jovial”, and may optionally select them based on the weights of “dinner” and “jovial” for those graphical elements.
- submitted communication features engine 122 provides a predicted textual suggestion of “later alligator” that is contextually relevant to a submitted electronic communication (e.g., a submitted electronic communication of “bye”).
- the selection engine 130 may select a graphical element that is indexed by one or more terms of the predicted textual suggestion, such as a graphical element that depicts an alligator and is indexed by the term “alligator”.
- the selection engine 130 may apply one or more provided features as input to one or more of the content models of content models database 156 , and select one or more graphical elements from database 154 based on output generated over the content model(s) based on the applied input.
- one of the content models may define relationships between submitted communication features of a dialog and one or more features of graphical elements.
- the selection engine 130 may apply, as input to the content model, submitted communication features provided by engine 122 and generate, over the content model based on the input, an indication of one or more graphical element features.
- the selection engine 130 may utilize the generated indication to select one or more graphical elements to include in the group.
- one of the content models may define relationships between communication features of earlier submitted electronic communications of a dialog and one or more features of text included in later submitted communications of the dialog.
- the selection engine 130 may apply, as input to the model, various received dialog features and generate, over the model based on the input, an indication of features of text that is typically submitted in later communications of the dialog.
- the selection engine 130 may utilize the features of the text to select one or more graphical elements to include in the group based on those graphical elements having graphical element features that match the features of text.
- the presentation engine 132 provides the group of graphical elements determined by the selection engine 130 for inclusion in an electronic communication to be submitted as part of the dialog.
- Providing a group of graphical elements may include, for example, providing the actual graphical elements or providing indications of the graphical elements.
- the presentation engine 132 may provide the group of graphical elements for presentation and, when a user generating the communication selects one of the graphical elements, the corresponding graphical element may be incorporated in the communication.
- the group of graphical elements may be provided and/or presented in response to the user selecting a “reply” user interface element, an “additional content” user interface element, or in response to other user interface input indicative of a desire to include graphical elements in the communication.
- the presentation engine 132 provides the graphical elements for presentation independent of any textual input and/or other content provided via a computing device of the user in generating the communication. In some implementations, the presentation engine 132 provides the graphical elements for inclusion in the reply based on scores for the graphical elements that are determined by the selection engine 130 . For example, in some implementations the presentation engine 132 may provide the graphical elements for presentation with prominences that are based on their associated scores. For example, the graphical element with the score most indicative of relevance may be provided for presentation most prominently (e.g., the topmost, leftmost, and/or largest).
- the group of graphical elements may be provided for presentation automatically or in response to certain first user interface input, whereas separate user interface input may be required when the score(s) fail to satisfy the threshold.
- the group of graphical elements may be provided and presented to the user when the user selects a “reply” interface element without requiring the user to separately select an “additional content” interface element that explicitly solicits suggested graphical elements and/or other additional content.
- the score doesn't satisfy the threshold, the group of graphical elements may not be provided and presented to the user when the user selects a reply interface element. Rather, selection of the separate “additional content” interface element may be required before providing and/or presenting the group of graphical elements to the user.
- one or more (e.g., all) aspects of the presentation engine 132 may be implemented by the client device 106 A and/or the electronic communications system 110 .
- the presentation engine 132 provides only indications of graphical elements of a group, and potentially prominence information, and the client device 106 A and/or electronic communications system 110 may generate a display of the group of graphical elements based on the provided data.
- the presentation engine 132 may additionally provide some or all of the data necessary to generate the display. In some of those implementations, any provided prominence information may be incorporated in the data that indicates how the display should be presented.
- user interface input devices 106 A 1 - 106 N 1 of corresponding client devices 106 A- 106 N are utilized in generating submitted communications 201 that have been submitted as part of a dialog.
- a user of client device 106 B may utilize user interface input device 106 B 1 to submit one of the submitted communications 201 as part of a dialog that involves the user of the client device 106 B, the user of the client device 106 A, and optionally one or more additional users.
- the submitted communication(s) features engine 122 determines one or more submitted communication features 222 of the submitted communications 201 and provides the submitted communication features 222 to the selection engine 130 .
- the user of the client device 106 A optionally utilizes user interface input device(s) 106 A 1 in providing subsequent content 203 for inclusion in a subsequent electronic communication to be submitted by the user as part of the dialog.
- the content may include text based on spoken or typed input of the user.
- the subsequent communication features engine 126 determines one or more subsequent communication features 226 based on the subsequent content 203 and provides the subsequent communication features 226 to the selection engine 130 .
- the user may not have provided any subsequent content 203 for inclusion in the subsequent electronic communication and subsequent communication features 226 may not be determined or provided to the selection engine 130 .
- the user state engine 124 determines current state features 224 for one or more users engaged in the dialog based on content of the submitted communications 201 , based on the subsequent content 203 , and/or based one or more sensors 106 A 2 - 106 N 2 of corresponding client devices 106 A- 106 N.
- the user state engine 124 may optionally utilize one or more user state models of database 158 in determining the current state features 224 based on such input.
- the user state engine 124 provides the current state features 224 to the selection engine 130 .
- the historical usage engine 128 provides historical usage features 228 to the selection engine 130 based on one or more historical models of historical model(s) database 160 .
- the selection engine 130 determines a group of graphical elements 207 based on one or more of the provided features 222 , 224 , 226 , and 228 . In some implementations, the selection engine 130 selects one or more of the graphical elements of the group 207 based on those graphical elements each having assigned graphical element features, in the graphical elements database 154 , that match one or more of the provided features 222 , 224 , 226 , and 228 .
- the selection engine 130 additionally or alternatively selects one or more of the graphical elements of the group 207 based on applying one or more of the provided features 222 , 224 , 226 , and 228 as input to one or more of the content models of content model(s) database 156 , generating output over the content model(s) based on the input, and selecting graphical elements having assigned graphical element features, in the graphical elements database 154 , that match the generated output.
- the presentation engine 132 provides the group of graphical elements 207 for presentation to the user via a user interface output device 106 A 3 (e.g., a display) of the client device 106 A.
- a user interface output device 106 A 3 e.g., a display
- FIG. 2 is illustrated with each of the engines 122 , 124 , 126 , and 128 providing feature to the selection engine 130 , in other implementations one or more of the engines may not provide features to the selection engine 130 (e.g., one or more engines may be omitted).
- FIGS. 3A-4 various examples of implementations of the graphical element system 120 are described.
- FIGS. 3A , 3 B 1 , 3 B 2 , 3 B 3 , 3 C, 3 D 1 , 3 D 2 , and 4 each illustrate the client device 106 A and a display screen 140 of the client device 106 A (screen 140 may be the user interface output device 106 A 3 of FIG. 2 ).
- the display screen 140 in the various figures displays graphical user interfaces with examples of dialog that involves the user and one or more additional users, and examples of how graphical elements may be presented to the user for inclusion in a subsequent electronic communication to be submitted by the user as part of the dialog, according to implementations disclosed herein.
- One or more aspects of the graphical element system 120 may be implemented on the client device 106 A and/or on one or more computing devices that are in network communication with the client device 106 A.
- the display screen 140 of FIGS. 3A , 3 B 1 , 3 B 2 , 3 B 3 , 3 C, 3 D 1 , 3 D 2 , and 4 further includes a reply interface element 188 that the user may select to generate user interface input via a virtual keyboard and a voice reply interface element 189 that the user may select to generate user interface input via a microphone.
- the user may generate user interface input via the microphone without selection of the voice reply interface element 189 .
- active monitoring for audible user interface input via the microphone may occur to obviate the need for the user to select the voice reply interface element 189 .
- the voice reply interface element 189 may be omitted.
- the reply interface element 188 may additionally and/or alternatively be omitted (e.g., the user may only provide audible user interface input and interact with suggested graphical elements and/or textual suggestions).
- the display screen 140 of FIGS. 3A , 3 B 1 , 3 B 2 , 3 B 3 , 3 C, 3 D 1 , 3 D 2 , and 4 also includes system interface elements 181 , 182 , 183 that may be interacted with by the user to cause the client device 106 A to perform one or more actions.
- a transcript of a dialog between a user of the client device 106 A and at least two additional users is displayed in a graphical user interface.
- a first additional user (Bob) in the dialog has submitted the electronic communication 321 of “How About Dinner Tomorrow?” and a second additional user (Tom) in the dialog has the electronic communication 322 of “I am in!”.
- the reply interface element 188 is provided, which the user may select to provide content for an electronic communication to be submitted as part of the dialog.
- An additional content interface element 187 is also provided, which the user may select to be presented with a group of one or more graphical elements for potential inclusion in an electronic communication to be submitted as part of the dialog.
- FIGS. 3 B 1 , 3 B 2 , and 3 B 3 illustrate an example of how graphical elements may be presented to the user, in response to selection of the additional content interface element 187 , and included in a subsequent electronic communication that is submitted by the user as part of the dialog of FIG. 3A .
- FIG. 3 B 1 illustrates the graphical interface after the user has selected the additional content interface element 187 in FIG. 3A .
- a group of graphical elements 191 are presented to the user in response to the selection of the additional content interface element 187 .
- Five graphical elements are included in the group 191 and an ellipsis is also shown to the right of the group 191 .
- the ellipsis may be selected to view additional graphical elements, such as those that are also contextually relevant but not included in the initial group and/or those that are not contextually relevant (e.g., to browse all available graphical elements).
- One or more of the graphical elements of the group 191 may be selected by the selection engine 130 based on various feature described herein. For example, they may be selected by the selection engine 130 based on submitted communication feature(s) of electronic communications 321 and/or 322 as provided by the submitted communication(s) features engine 122 .
- the submitted communication feature(s) may include a textual feature of “dinner” based on presence of that term in communication 321 .
- the selection engine 130 may select the “pizza” and “hamburger” graphical elements based on those being assigned graphical element features (e.g., dinner and/or food) that match the textual feature of “dinner.” In some implementations, the selection engine 130 may select the “pizza”, “hamburger”, and/or “wine glass” graphical elements based on applying the “dinner” textual feature to one or more of the content models of database 156 and generating, based on application of the textual feature to the content models, output that indicates that subsequent communications in dialogs that include submitted communications with a “dinner” textual feature often include “food” and “drink” graphical elements and/or “food” and “drink” text. The selection engine 130 may select the “pizza”, “hamburger”, and “wine glass” graphical elements based on each of those being assigned graphical element features that match one or more aspects of the output.
- graphical element features e.g., dinner and/or food
- the selection engine 130 may select the “OK hand signal” and “smiley” graphical elements based on applying additional or alternative submitted communication features to a content model, and generating, based on application of the submitted communication features to the content model, output that indicates that subsequent communications in dialogs that include those submitted communication features often include “confirmatory” and “happy” graphical elements and/or “confirmatory” and “happy” text.
- the selection engine 130 may select the “OK hand signal” graphical element based on it being assigned a graphical element feature of “confirmatory” and may select the “smiley face” graphical element based on it being assigned a “happy” graphical element feature.
- FIG. 3 B 2 illustrates the graphical interface after the user has selected the “OK hand signal” graphical element, then selected the “pizza” graphical element, then provided further user interface input (e.g., typed input) of “?”.
- further user interface input e.g., typed input
- the selected graphical elements and the further user interface input are populated sequentially in the reply interface element 188 , as part of a subsequent communication being composed by the user.
- a submission interface element 193 is also provided in FIG. 3 B 2 .
- FIG. 3 B 3 illustrates the graphical interface after the user has selected the submission interface element 193 in FIG. 3 B 2 to submit, for inclusion in the dialog, the subsequent communication being composed in FIG. 3 B 1 .
- the subsequent communication being composed in FIG. 3 B 1 has been submitted and is included in the dialog. This is illustrated by the updated transcript of the dialog in FIG. 3 B 3 that now includes the subsequent communication 332 .
- FIG. 3C illustrates another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog of FIG. 3A .
- the graphical interface of FIG. 3C may be presented in lieu of that of FIG. 3 B 1 in response to selection of the additional content interface element 187 .
- the graphical interface of FIG. 3C may be presented in response to selection of the reply interface element 188 in FIG. 3A in lieu of selection of the additional content interface element 187 .
- the group of graphical elements 191 is the same as that presented in FIG. 3B .
- the interface also includes a virtual keyboard 187 to enable the user to formulate textual input, and includes textual suggestions 194 that are contextually relevant to the dialog.
- the textual suggestions 194 may be generated, for example, by applying submitted communication features to one or more of the content models of content models database 156 to generate output that indicates text included in subsequent communications in dialogs that include those submitted communication features.
- the user may use the keyboard 187 to generate textual content for inclusion in the communication to be submitted, may select one of the textual suggestions 194 to include as content in the communication to be submitted, and/or may select one of the graphical elements of the group 191 to include as content.
- the user may submit it for inclusion in the dialog utilizing the submission interface element 193 .
- one or more of the textual suggestions 194 may be utilized as a feature for selecting one or more of the graphical elements 191 .
- the “OK hand signal” graphical element and/or the “happy” graphical element may be selected based at least in part on being indexed by the phrase “sounds good” and/or the term “good”.
- FIGS. 3 D 1 and 3 D 2 illustrate yet another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog of FIG. 3A .
- the graphical interface of 3 D 1 may be presented in response to selection of the reply interface element 188 in FIG. 3A .
- the user has used the virtual keyboard 187 to provide textual input of “Yes, what can I bring?” in a subsequent communication to be submitted in the dialog.
- FIG. 3 D 2 illustrates an example of a group of graphical elements 195 that may be presented in response to user selection of the additional content interface element 187 after providing the textual input of FIG. 3 D 1 .
- the group of graphical elements 195 differs from the group 191 (FIGS. 3 B 1 , 3 B 2 , 3 C) based on being selected while also taking into account subsequent communication features that are based on the textual input of “Yes, what can I bring?”.
- the selection engine 130 may utilize one or more subsequent communication features provided by subsequent communication features engine 126 in selecting the graphical elements for inclusion in the group 195 .
- Such utilization of the subsequent communication features causes the display prominence of the “wine glass” graphical element to be promoted and causes new “pie” and “salad” graphical elements to be included in the group 195 .
- the user may optionally select one or more of the graphical elements to include the selected graphical elements in the communication to be submitted, and submit the communication through selection of the submission interface element 193 .
- the user in advance of providing the textual input of “Yes, what can I bring?” in FIG. 3 D 1 , the user may first select the additional content interface element 187 and be presented with the group 191 of FIGS. FIGS. 3 B 1 , 3 B 2 , 3 C.
- the group 191 in response to providing the textual input of “Yes, what can I bring?” in FIG. 3 D 1 , the group 191 may be supplanted with the group 195 (FIG. 3 D 2 ) based on consideration of subsequent communication features that are based on that textual input.
- further submitted communication(s) from additional users that are submitted as part of the dialog during composition of a subsequent communication of the user may be utilized to update a provided group of graphical elements (i.e., by further taking into account submitted communication features of the further submitted communication(s)).
- FIG. 4 illustrates the example client computing device of FIG. 3A , and illustrates another example of dialog and another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog.
- a transcript of a dialog between a user of the client device 106 A and at least one additional user (Bob) is displayed in a graphical user interface.
- An additional user (Bob) in the dialog has submitted the electronic communication 421 of “Talk to you soon”.
- a group of graphical elements 197 is presented to the user and includes an “alligator” graphical element and a “smiley” graphical element.
- the group 197 may be presented to the user without explicit user interface input requesting suggested graphical elements be provided.
- the group 197 may be provided based on the selection engine 130 determining that scores associated with one or more of the graphical elements of the group satisfy a threshold.
- the selection engine 130 may determine the group 197 based on submitted communication feature(s) of communication 421 , subsequent communication feature(s) based on the textual input “Later”, and based on sentiment of the user and/or of “Bob” that indicates they are “jovial”.
- the “alligator” graphical element may be indexed with graphical element features that indicate it has a strong weight for subsequent communications that include the term “later”, has a strong weight for submitted communications that include a “departing remark” such as “talk to you soon”, and has a strong weight for “jovial” sentiments.
- FIG. 5A is a flowchart illustrating an example method 500 of providing a group of graphical elements for inclusion in a dialog according to implementations disclosed herein.
- This system may include various components of various computer systems, such as one or more components of graphical element system 120 .
- operations of method 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system receives an electronic communication submitted as part of a dialog that includes a user and one or more additional users.
- the system determines one or more communication features of the electronic communication. For example, the system may determine one or more submitted communication features of one or more submitted communications that have been submitted as part of the dialog. Also, for example, the system may additionally or alternatively determine one or more subsequent communication features based on content (if any) that has been provided by the user for inclusion in a subsequent communication to submit as part of the dialog.
- the system determines one or more additional features. For example, the system may determine current state feature(s) that indicate the current state of one or more dynamic states associated with user(s) engaged in the dialog. Also, for example, the system may additionally or alternatively determine historical usage features that indicate historical usage, of the user, of one or more graphical elements.
- the system determines a group of graphical elements for inclusion in a subsequent communication of the user. In some implementations, the system performs block 559 as part of performing block 558 . At block 559 , the system selects one or more graphical elements for the group based on the communication feature(s) determined at block 554 and optionally based on the additional feature(s) determined at block 556 . Two examples of block 559 are illustrated in FIGS. 5B and 5C .
- the system provides the group of graphical elements for potential inclusion in a subsequent communication of the dialog.
- blocks 552 , 554 , 556 , 558 , and 560 may occur during a dialog to enable graphical elements to be adapted to various changes in context of the dialog that may occur throughout the dialog.
- FIG. 5B is a flowchart illustrating an example of implementations of block 559 of the method 500 in additional detail. While the operations of the flowchart of FIG. 5B are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system identifies and optionally scores graphical elements that are associated with one or more of the communication features of block 554 ( FIG. 5 ) and/or one or more of the additional features of block 556 ( FIG. 5 ). For example, the system may identify graphical elements that are indexed by (e.g., include assigned graphical element features that correspond to) one or more of the features. The system may score each of the graphical elements based on a quantity of the features with which the graphical element is associated and/or based on optional weights assigned to the associations between the matching features and the graphical element.
- the system determines historical usage features of the user for one or more of the graphical elements identified at block 562 .
- the historical usage features may be features determined at block 556 of FIG. 5 .
- the system may determine: a frequency of use by the user of the given graphical element; a frequency of use by the user of all graphical elements sharing one or more graphical element characteristics with the given graphical element; and/or a frequency of use by the user of graphical elements from a cohesive graphical element pack to which the given graphical element belongs.
- the system optionally scores the identified graphical elements based on the historical usage features.
- the system may modify the optional score determined at block 562 and/or may generate new scores based on the historical usage features. For example, the system may assign more prominent scores to graphical element(s) associated with at least a threshold degree of past usage, than it assigns to other graphical elements that do not have the threshold degree of past usage. Also, for example, the system may modify the optional score determined at block 562 by promoting the scores of graphical element(s) whose historical usage feature(s) indicate at least a threshold degree of past usage by the user.
- the system selects one or more of the identified graphical elements, optionally based on the scores. For example, the system may select the N most prominently scored graphical element for inclusion in a group to be initially displayed to a user. N may be selected based on various factors such as display properties of a client device on which the graphical elements will be provided, the scores themselves (e.g., only graphical elements having scores that satisfy a threshold will be selected), etc.
- FIG. 5C is a flowchart illustrating another example of implementations of block 559 of the method 500 in additional detail. While the operations of the flowchart of FIG. 5C are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system applies communication features and/or additional features as input to a trained machine learning model.
- the trained machine learning model may embed relationships between features of electronic communications of a dialog and one or more graphical element features and/or one or more features of text.
- the system generates, over the model based on the input applied at block 572 , output that is indicative of one or more graphical elements.
- the output may indicate one or more graphical element features and/or one or more features of text.
- the output may also optionally indicate weights for one or more of the indicated features.
- the system selects one or more graphical elements based on the output of block 574 .
- the system may select graphical elements that are indexed by, or otherwise associated with, graphical element features corresponding to feature(s) indicated by the output of block 574 .
- optional weights indicated in the output for the features may be utilized to select one or more of the graphical elements. For example, those graphical elements associated with features having stronger weights may be more likely to be selected than those graphical elements associated only with features having weaker weights.
- scores may be determined for each of the graphical elements based on the weights and the scores utilized to select some of the graphical elements. In some implementations, the scores may additionally or alternatively be determined based on historical usage features as described with respect to blocks 564 and 566 of FIG. 5B .
- FIG. 5B and FIG. 5C are illustrated as different figures, in some implementations one or more of the blocks of FIG. 5B and one or more of the blocks of FIG. 5C may both be performed in a given iteration of block 559 of method 500 .
- some graphical elements may be selected based on blocks of FIG. 5B and other graphical elements may be selected based on blocks of FIG. 5C .
- scores generated based on blocks of FIG. 5B and scores generated based on blocks of FIG. 5C may be combined and the combined scores utilized to select a subset of the graphical elements identified based on blocks of FIG. 5B and blocks of FIG. 5C .
- a given iteration of block 559 may include only blocks of FIG. 5B or only blocks of FIG. 5C .
- FIG. 6 is a flowchart illustrating an example method 600 of assigning features to graphical elements. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as graphical element features engine 137 . Moreover, while operations of method 600 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system selects one or more graphical elements. For example, the system may select a single graphical element or may select a group of graphical elements. When the system selects the group of graphical elements, that group may be selected based on the graphical elements of the group all belonging to the same cohesive pack, and/or based on the graphical elements of the group all having one or more other already assigned graphical element features in common.
- the system identifies features of dialogs that include the graphical elements. For example, the system may identify features of communications of the dialogs, where the graphical elements were included in those communications. Also, for example, the system may additionally or alternatively identify features of communications of the dialogs, where the graphical elements were not included in those communications but, instead, were included in subsequent communications of the dialogs. As yet another example, the system may additionally or alternatively identify user state features based on communications of the dialogs.
- the system optionally determines weights for the features. For example, the system may determine a weight for a given feature based on its quantity of occurrences in the dialogs. For instance, for a graphical element that is a depiction of an alligator, the system may determine it is included with the term “later” in 500 communications of the dialogs, and that it is included with the term “Florida” in 300 communications of the dialogs. Based on its occurrence with “later” in a greater quantity of communications than its occurrence with “Florida”, a greater weight may be determined for the feature of “later” may than a weight determined for a feature of “Florida”.
- the system assigns, to the graphical elements, one or more of the features and optionally the weights. For example, the system may assign, to the graphical elements, those features having at least a threshold weight.
- the features may optionally be assigned with an indication of parameters for the feature. For example, where a feature is based on communications in which the graphical elements were included, that feature may be assigned with an indication that it is associated with communications for which the graphical element is to be included. For instance, it may be assigned with an indication that indicates that feature is relevant to the graphical element only when that feature is included in a communication for which the graphical element is to be provided for inclusion.
- a feature may be assigned with an indication that it is associated with submitted communications of a dialog. For instance, it may be assigned with an indication that indicates that feature is relevant to the graphical element only when that feature is included in an already submitted communication of a dialog, and the graphical element is to be provided for inclusion in a subsequent communication of the dialog.
- method 600 may be performed to assign features to a variety of different graphical elements.
- FIG. 7 is a flowchart illustrating an example method 700 of training a machine learning content model.
- This system may include various components of various computer systems, such as content model training engine 135 .
- operations of method 700 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system selects one or more graphical elements. For example, the system may select all graphical elements available to the system or a subset of graphical elements available to the system.
- the system identifies features of dialogs that include the graphical elements. For example, the system may identify features of communications of the dialogs, where the graphical elements were included in those communications. Also, for example, the system may additionally or alternatively identify features of communications of the dialogs, where the graphical elements were not included in those communications but, instead, were included in subsequent communications of the dialogs. As yet another example, the system may additionally or alternatively identify user state features based on communications of the dialogs.
- each of the training examples may include: training example input based on features from one of the dialogs and training example output based on graphical element features of one or more graphical elements included in the dialog.
- the system trains a machine learning model based on the training examples. For example, the system may train the machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples.
- the method 700 assume a machine learning model with an input layer, multiple hidden layers, and an output layer that is a vector of 256 different graphical element features.
- a first training example may be generated based on presence of a first graphical element in a first dialog.
- the training example input of the first training example may include one or more features of the first dialog, such as communication features for the communication that includes the first graphical element and/or communication features of one or more other communications of the first dialog.
- the training example output of the first training example may be the vector of 256 different graphical element features, with the values for each of the graphical element features being assigned based on stored assignments of those graphical element features to the first graphical element.
- the training example output may include “1s” (or other “present” value(s)) for those 5 graphical element features and may include “0s” (or other “absent” value(s)) for the 251 other graphical element features.
- the “present” values may optionally be weighted based on, for example, corresponding weights of the assignments of those 5 graphical element features for the first graphical element.
- a second training example may be generated based on presence of a second graphical element in a second dialog.
- the training example input of the second training example may include one or more features of the second dialog, such as communication features for the communication that includes the first graphical element and/or communication features of one or more other communications of the second dialog.
- the training example output of the second training example may be the vector of 256 different graphical element features, with the values for each of the graphical element features being assigned based on stored assignments of those graphical element features to the second graphical element.
- the training example output may include “1s” (or other “present” value(s)) for those 4 graphical element features and may include “0s” (or other “absent” value(s)) for the 252 other graphical element features.
- the “present” values may optionally be weighted based on, for example, corresponding weights of the assignments of those 4 graphical element features for the second graphical element.
- Additional training examples may be generated in a similar manner based on additional graphical elements and/or dialogs.
- features of a new dialog may be applied as input to the trained machine learning model, and output generated over the model based on the applied input.
- the output will include values for each of the 256 different graphical element features. Each value indicates the likelihood that, given the applied features of the new dialog, that the corresponding graphical element feature is relevant to the new dialog. Those values may then be utilized to select one or more graphical elements for providing for inclusion in a subsequent message to be submitted as part of the dialog. For example, assume 20 particular values of the 256 values satisfy a threshold value. Graphical elements may be selected that are assigned the greatest quantity of those 20 particular values.
- FIG. 8 is a block diagram of an example computing device 810 that may optionally be utilized to perform one or more aspects of techniques described herein.
- one or more of a client computing device, automated assistant 120 , and/or other component(s) may comprise one or more components of the example computing device 810 .
- Computing device 810 typically includes at least one processor 814 which communicates with a number of peripheral devices via bus subsystem 812 .
- peripheral devices may include a storage subsystem 824 , including, for example, a memory subsystem 825 and a file storage subsystem 826 , user interface output devices 820 , user interface input devices 822 , and a network interface subsystem 816 .
- the input and output devices allow user interaction with computing device 810 .
- Network interface subsystem 816 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.
- User interface input devices 822 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computing device 810 or onto a communication network.
- User interface output devices 820 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computing device 810 to the user or to another machine or computing device.
- Storage subsystem 824 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 824 may include the logic to perform selected aspects of the methods of FIGS. 5A, 5B, 5C, 6 , and/or 7 .
- Memory 825 used in the storage subsystem 824 can include a number of memories including a main random access memory (RAM) 830 for storage of instructions and data during program execution and a read only memory (ROM) 832 in which fixed instructions are stored.
- a file storage subsystem 826 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 826 in the storage subsystem 824 , or in other machines accessible by the processor(s) 814 .
- Bus subsystem 812 provides a mechanism for letting the various components and subsystems of computing device 810 communicate with each other as intended. Although bus subsystem 812 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computing device 810 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 810 depicted in FIG. 8 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 810 are possible having more or fewer components than the computing device depicted in FIG. 8 .
- users are provided with one or more opportunities to control whether information is collected, whether the personal information is stored, whether the personal information is used, and how the information is collected about the user, stored and used. That is, the systems and methods discussed herein collect, store and/or use user personal information only upon receiving explicit authorization from the relevant users to do so. For example, a user is provided with control over whether programs or features collect user information about that particular user or other users relevant to the program or feature.
- Each user for whom personal information is to be collected is presented with one or more options to allow control over the information collection relevant to that user, to provide permission or authorization as to whether the information is collected and as to which portions of the information are to be collected.
- users can be provided with one or more such control options over a communication network.
- certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined.
- a user's geographic location may be generalized to a larger region so that the user's particular location cannot be determined.
Abstract
Description
- Individuals engage in electronic dialogs with one another utilizing a variety of electronic communications such as emails, rich communication services (RCS) messages, short message service (SMS) messages, multimedia messaging service (MMS) messages, over-the-top (OTT) chat messages, social networking messages, etc. Moreover, users engage in such dialogs utilizing a variety of computing devices, such as smart phones, tablet computers, wearable devices, automobile systems, and so forth. In many situations a user may include graphical elements in one or more electronic communications in addition to, or in lieu of, textual content. Graphical elements may take many forms such as emojis, Graphics Interchange Format images (GIFs), so-called stickers (which may be curated GIFs, JPGs, etc.), etc. Some graphical elements may themselves be completely void of any text when displayed (e.g., a smiley face emoji), while other graphical elements may include text when displayed (e.g., a GIF that includes graphical text in addition to other content).
- This specification is directed to methods, apparatus, and computer readable media related to determining graphical element(s) (e.g., emojis, GIFs, stickers) for inclusion in an electronic communication being formulated by a user via a computing device of the user, and providing the graphical element(s) for inclusion in the electronic communication. For example, the graphical element(s) may be provided for presentation to the user via a display of the computing device of the user and, in response to user interface input directed to one of the graphical element(s), that graphical element may be incorporated in the electronic communication. In various implementations, the electronic communication is a communication to be submitted as part of a dialog that involves the user and one or more additional users.
- In some implementations, provided graphical element(s) are determined based at least in part on one or more features that are in addition to any input provided by the user in formulating the electronic communication for which the graphical elements are provided for inclusion. In some of those implementations, the graphical element(s) may be provided for inclusion in the electronic communication before the user has provided any textual content for inclusion in the electronic communication and/or before the user has provided any other content for inclusion in the electronic communication. In some implementations, the graphical element(s) may be provided in response to user interface input related to the electronic communication, such as the user selecting a “reply” interface element to initiate formulation of the electronic communication and/or selecting a “content suggestion” interface element.
- In some implementations, the features that are in addition to any input provided by the user in formulating the electronic communication may include one or more of: submitted communication feature(s) of one or more previously submitted electronic communications of the dialog to which the electronic communication is to be submitted (e.g., prior dialog communication(s) of the user and/or of additional users); current state feature(s) that indicate current state(s), of one or more dynamic user states (e.g., location, sentiment), of one or more users engaged in the dialog; and/or historical usage of graphical elements by the user in the dialog, in other dialogs, and/or in other communications. In some implementations, the graphical elements may additionally or alternatively be determined based on subsequent communication features that are based on input provided by the user in formulating the electronic communication for which the graphical elements are to be provided for inclusion.
- In some implementations, which graphical element(s) are provided for inclusion in an electronic communication of a dialog may be determined based on output generated over a trained machine learning model in response to applying one or more features (e.g., one or more of those described above) as input to the trained machine learning model. For example, submitted communication features of submitted electronic communications of a dialog and current state(s) of user(s) engaged in the dialog may be provided as input to the trained machine learning model. Output may be generated over the model based on the input, and the output may indicate one or more graphical element features of graphical elements. The output may be used to select one or more graphical elements to provide for potential inclusion in the electronic communication, based on those graphical elements having graphical element features that match the output.
- In some implementations, a corpus of past electronic communications is analyzed to determine relationships between graphical elements and one or more features (e.g., one or more of those described above). For example, the corpus may be analyzed to determine, for a given graphical element, one more features of communications in which the given graphical element is included and/or one or more features of previously submitted communications in which the given graphical element is included in a reply to those previously submitted communications. In some implementations, a corpus of past electronic communications is analyzed to determine relationships between previously submitted communication(s) of dialogs and one or more subsequent communications of the dialogs. As one example, the corpus may be analyzed to determine that a dialog that includes submitted communication(s) that include the n-gram “dinner” are likely to include a subsequent communication that includes certain term(s) and/or graphical elements having certain graphical element features (e.g., graphical elements having an assigned “food” feature).
- These relationships that are learned from analyzing the corpus of past electronic communications may be utilized to determine which graphical elements to provide for potential inclusion in an electronic communication being composed for submission in a dialog, as described herein. In some implementations, determining the relationships may be achieved via generating appropriate training examples based on a corpus of electronic communications and training a machine learning model based on those training examples to effectively embed the relationships in the trained machine learning model.
- Some of the above mentioned and other implementations of the specification may achieve various technical advantages. For example, some implementations of determining and providing graphical elements to present to a user based on various features described herein may increase the likelihood that presented graphical elements are relevant to the user and/or to a current context of a dialog. This may mitigate the need for the user to navigate through and/or explicitly search a litany of irrelevant available graphical elements in an attempt to locate a relevant graphical element. This may reduce the use of various computational resources, such as resources of computing device(s) that are required for rendering and/or retrieving the irrelevant graphical elements. As another example, some implementations of determining and providing graphical elements to present to a user based on various features described herein may enable a more concise dialog to be achieved as a result of the presented graphical elements being selected by the user for inclusion in a communication of the dialog, and providing relevant additional context to the communication. This may obviate the need for certain further communications of the dialog that would otherwise be needed to provide the additional context, thereby reducing the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the further communications to the user(s). Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- In some implementations, a method performed by one or more processors is provided that includes receiving an electronic communication submitted as part of a dialog that includes a user and one or more additional users. The electronic communication is based on user interface input generated by an additional user, of the additional users, via one or more user interface input devices of an additional user computing device of the additional user. The method further includes determining at least one communication feature of the electronic communication and determining a group of one or more graphical elements for inclusion in a subsequent electronic communication, of the user, to be submitted as part of the dialog. Determining the group of graphical elements includes, prior to any textual input provided by the user for the subsequent electronic communication: selecting, from candidate graphical elements, at least one graphical element to include in the group. Selecting the graphical element is based on the at least one communication feature of the electronic communication. The method further includes, prior to any textual input provided by the user for the subsequent electronic communication, providing the group of graphical elements for potential inclusion in the subsequent electronic communication. The group of graphical elements are provided for presentation via a display of a user computing device of the user.
- These and other implementations of technology disclosed herein may optionally include one or more of the following features.
- In some implementations, the method further includes identifying user interface interaction with a user interface element that is graphically presented to the user via the display in combination with a presentation of the electronic communication. In some of those implementations, providing the group of graphical elements is in response to identifying the user interface interaction with the user interface element.
- In some implementations, the at least one communication feature is based on one or more of a plurality of terms of the electronic communication. In some versions of those implementations, the at least one communication feature further includes at least one non-textual feature that is based at least in part on one or more signals that are in addition to the terms of the electronic communication. In some other versions of those implementations, the method further includes determining at least one additional communication feature of an additional electronic communication of the dialog, and selecting the graphical element further based on the at least one additional communication feature.
- In some implementations, the method further includes determining at least one current state feature that indicates a current state of at least one dynamic user state of the user or the additional user, and selecting the graphical element further based on the current state feature. In some of those implementations, the dynamic user state includes a physical location or a sentiment.
- In some implementations, the method further includes identifying a historical usage feature for the graphical element, and selecting the graphical element further based on the historical usage feature. In some of those implementations, the historical usage feature is based at least in part on past usage by the user of additional graphical elements sharing one or more graphical element features with the graphical element.
- In some implementations, selecting the graphical element based on the at least one communication feature includes: determining a graphical element feature based on the at least one communication feature, the graphical element feature being assigned to the graphical element and being assigned to a plurality of additional graphical elements; and selecting the graphical element to include in the group in lieu of one or more of the additional graphical elements based on the graphical element being associated with a greater degree of historical usage, by the user, than the non-selected one or more of the additional graphical elements. In some of those implementations, the graphical element is included in a cohesive pack of graphical elements and the historical usage of the graphical element is based on the historical usage, by the user, of the graphical elements of the cohesive pack.
- In some implementations, selecting the graphical element based on the at least one communication feature includes: applying input to a trained machine learning model stored in one or more computer readable media, the input comprising the at least one communication feature; generating, over the model and based on the input, output that indicates one or more graphical element features; and selecting the graphical element based on the graphical element features being assigned to the graphical element. In some versions of those implementations, the output that indicates one or more graphical element features includes one or more terms and selecting the graphical element based on the graphical element features being assigned to the graphical element comprises selecting the graphical element based on it being indexed by at least one of the one or more terms. In some further versions of those implementations, the method further includes: generating the trained machine learning model based on a plurality of training examples derived from past user dialogs, where of the training examples includes: training example input based on communication features of a corresponding original electronic communication, and training example output based on text of a corresponding reply electronic communication that is a reply to the corresponding original electronic communication. Generating the trained machine learning model may include training the trained machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples. In some other versions of those implementations, the method further includes: generating the trained machine learning model based on a plurality of training examples derived from past user dialogs, where each of the training examples includes: training example input based on communication features of a corresponding original electronic communication, and training example output based on graphical element features of a corresponding reply electronic communication that is a reply to the corresponding original electronic communication. Generating the trained machine learning model may include training the trained machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples.
- In some implementations, the communication feature includes one or more terms and selecting the graphical element based on the at least one communication feature includes selecting the graphical element based on it being indexed by at least one of the one or more terms.
- In some implementations, the method further includes, after providing the group of graphical elements for potential inclusion in the subsequent electronic communication: receiving textual input provided by the user for the subsequent electronic communication, the textual input being based on user interface input generated by the user via one or more user interface input devices of the user computing device; determining a modified group of graphical elements based on the textual input and based on the at least one communication feature; and providing the modified group of graphical elements for potential inclusion in the subsequent electronic communication in combination with the textual input.
- In some implementations, a method performed by one or more processors is provided that includes receiving an electronic communication submitted as part of a dialog that includes the user and one or more additional users. The electronic communication is based on user interface input generated by an additional user of the additional users via one or more user interface input devices of an additional user computing device of the additional user. The method further includes: determining at least one communication feature of the electronic communication; receiving textual input provided by the user for a subsequent electronic communication, of the user, to be submitted as part of the dialog; and determining a group of one or more graphical elements for inclusion in the subsequent electronic communication. Determining the group of graphical elements includes: selecting, from candidate graphical elements, at least one graphical element to include in the group, where selecting the graphical element is based on the textual input and is based on the at least one communication feature; and providing the group of graphical elements for potential inclusion in the subsequent electronic communication. The group of graphical elements are provided for presentation via a display of a user computing device of the user.
- These and other implementations of technology disclosed herein may optionally include one or more of the following features.
- In some implementations, providing the group of graphical elements is in response to receiving the textual input and occurs without further user interface input that explicitly requests provision of graphical elements.
- In some implementations, the communication feature includes one or more terms and selecting the graphical element based on the at least one communication feature includes selecting the graphical element based on it being indexed by at least one of the one or more terms.
- In some implementations, the method further includes determining at least one current state feature that indicates a current state of at least one dynamic user state of the user or the additional user, and selecting the graphical element further based on the current state feature.
- In some implementations, a method performed by one or more processors is provided that includes receiving content provided by a user for inclusion in an electronic communication to be submitted by the user as part of a dialog. The content is based on user interface input generated by the user via one or more user interface input devices of a computing device of the user. The method further includes determining at least one feature of the content and determining a group of one or more graphical elements for inclusion in the electronic communication. Determining the group of graphical elements includes: selecting, from candidate graphical elements, at least one graphical element to include in the group, where selecting the graphical element is based on the feature of the content matching at least one graphical element feature assigned to the graphical element; and providing the group of graphical elements for potential inclusion in the electronic communication. The group of graphical elements are provided for presentation via a display of a user computing device of the user.
- In addition, some implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
-
FIG. 1 is a block diagram of an example environment in which implementations disclosed herein may be implemented. -
FIG. 2 illustrates an example of using components of the example environment ofFIG. 1 in selecting graphical elements for providing to a user. -
FIG. 3A illustrates an example client computing device with a display screen displaying an example of dialog that includes a user of the client computing device and a plurality of additional users; the dialog includes a submitted electronic communication from each of two of the additional users. - FIGS. 3B1, 3B2, and 3B3 illustrate an example of how graphical elements may be presented to the user and included in a subsequent electronic communication that is submitted by the user as part of the dialog of
FIG. 3A . -
FIG. 3C illustrates another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog ofFIG. 3A . - FIGS. 3D1 and 3D2 illustrate yet another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog of
FIG. 3A . -
FIG. 4 illustrates the example client computing device ofFIG. 3A , and illustrates another example of a dialog and another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog. -
FIG. 5A is a flowchart illustrating an example method of providing a group of graphical elements for inclusion in a dialog according to implementations disclosed herein. -
FIG. 5B is a flowchart illustrating an example of one block of the flowchart ofFIG. 5A . -
FIG. 5C is a flowchart illustrating another example of the one block of the flowchart ofFIG. 5A . -
FIG. 6 is a flowchart illustrating an example method of assigning features to graphical elements. -
FIG. 7 is a flowchart illustrating an example method of training a machine learning content model. -
FIG. 8 illustrates an example architecture of a computing device. - This specification is directed to technical features related to determining graphical element(s) for inclusion in an electronic communication being formulated by a user via a computing device of the user, and providing the graphical element(s) for inclusion in the electronic communication. For example, the graphical element(s) may be provided for presentation to the user via a display of the computing device and, in response to user interface input directed to one of the graphical element(s), that graphical element may be incorporated in the electronic communication.
- In some implementations, provided graphical element(s) are determined based at least in part on one or more features that are in addition to any input provided by the user in formulating the electronic communication for which the graphical elements are provided for inclusion. In some of those implementations, the graphical element(s) may be provided for inclusion in the electronic communication before the user has provided any textual content and/or any other content for inclusion in the electronic communication. In some implementations, the graphical element(s) may be provided in response to user interface input related to the electronic communication.
- In some implementations, the features that are in addition to any input provided by the user in formulating the electronic communication may include one or more of: submitted communication feature(s) of one or more previously submitted electronic communications of the dialog to which the electronic communication is to be submitted; current state feature(s) that indicate current state(s), of one or more dynamic user states, of one or more users engaged in the dialog; and/or historical usage of graphical elements by the user in the dialog, in other dialogs, and/or in other communications. In some implementations, the graphical elements may additionally or alternatively be determined based on subsequent communication features that are based on input provided by the user in formulating the electronic communication for which the graphical elements are to be provided for inclusion.
- In some implementations, which graphical element(s) are provided for inclusion in an electronic communication of a dialog may be determined based on output generated over a trained machine learning model in response to applying one or more features (e.g., one or more of those described above) as input to the trained machine learning model. Output may be generated over the model based on the input, and the output may indicate one or more graphical element features of graphical elements. The output may be used to select one or more graphical elements to provide for potential inclusion in the electronic communication, based on those graphical elements having graphical element features that match the output.
- In some implementations, a corpus of past electronic communications is analyzed to determine relationships between graphical elements and one or more features (e.g., one or more of those described above). In some implementations, a corpus of past electronic communications is analyzed to determine relationships between previously submitted communication(s) of dialogs and one or more subsequent communications of the dialogs.
- These relationships that are learned from analyzing the corpus of past electronic communications may be utilized to determine which graphical elements to provide for potential inclusion in an electronic communication being composed for submission in a dialog, as described herein. In some implementations, determining the relationships may be achieved via generating appropriate training examples based on a corpus of electronic communications and training a machine learning model based on those training examples to effectively embed the relationships in the trained machine learning model.
- Some of the above mentioned and other implementations of the specification may achieve various technical advantages. For example, some implementations of determining and providing graphical elements to present to a user based on various features described herein may increase the likelihood that presented graphical elements are relevant to the user and/or to a current context of a dialog. This may mitigate the need for the user to navigate through and/or explicitly search a litany of irrelevant available graphical elements in an attempt to locate a relevant graphical element. This may reduce the use of various computational resources, such as resources of computing device(s) that are required for rendering and/or retrieving the irrelevant graphical elements. As another example, some implementations of determining and providing graphical elements to present to a user based on various features described herein may enable a more concise dialog to be achieved as a result of the presented graphical elements being selected by the user for inclusion in a communication of the dialog, and providing relevant additional context to the communication. This may obviate the need for certain further communications of the dialog that would otherwise be needed to provide the additional context, thereby reducing the use of various computational resources, such as resources of computing device(s), that are required for visually and/or audibly presenting the further communications to the user(s). Additional or alternative technical advantages may be achieved, such as one or more described elsewhere in this specification.
- In
FIG. 1 , an example environment in which techniques disclosed herein may be implemented is illustrated. The example environment includes acommunication network 101 that optionally facilitates communication between the various components in the environment. In some implementations, thecommunication network 101 may include the Internet, one or more intranets, and/or one or more bus subsystems. Thecommunication network 101 may optionally utilize one or more standard communications technologies, protocols, and/or inter-process communication techniques. - The example environment also includes a
client device 106A, additional client device(s) 106B-N,electronic communications system 110,graphical element system 120, contentmodel training engine 135, and graphical element features engine 137. The example environment further includes anelectronic communications database 152, agraphical elements database 154, a content model(s)database 156, a user state model(s)database 158, and a historical model(s)database 160. - Some non-limiting examples of client device(s) 106A-N include one or more of: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a vehicle of the user (e.g., an in-vehicle communications system, an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a watch of the user having a computing device, glasses of the user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided. In some implementations, a given user may communicate with all or aspects of
graphical element system 120 utilizing a plurality of client devices that collectively form a coordinated “ecosystem” of client devices. However, for the sake of brevity, some examples described in this disclosure will focus on the user operating a single client device. -
Electronic communications system 110,graphical element system 120, and/orengines 135 and/or 137 may each be implemented in one or more computing devices that communicate, for example, through a network (e.g.,network 101 and/or other network).Electronic communications system 110,graphical element system 120, andengines 135 and 137 are example components via which the systems and techniques described herein may be implemented and/or with which systems and techniques described herein may interface. They may each include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. In some implementations,electronic communications system 110,graphical element system 120, and/orengines 135 and/or 137 may include one or more components of the example computing device ofFIG. 8 . The operations performed byelectronic communications system 110,graphical element system 120, and/orengines 135 and/or 137 may be distributed across multiple computer systems. - In some implementations, one or more aspects of one or more of
electronic communications system 110,graphical element system 120, and/orengines 135 and/or 137 may be combined in a single system and/or one or more aspects may be implemented on theclient device 106A and/or one or more of the additional client device(s) 106B-N. For example, theclient device 106A may include an instance of one or more aspects of thegraphical element system 120 and each of the additional client device(s) 106B-N may also include an instance of one or more aspects of thegraphical element system 120. As another example, theclient device 106A and each of the additional client device(s) 106B-N may each include an instance of the electronic communications system 110 (e.g., theelectronic communications system 110 may be an application installed and executing on each of the devices). As yet another example, one or more aspects ofelectronic communications system 110 andgraphical element system 120 may be combined. - The
electronic communications database 152 includes one or more storage mediums that include all, or portions of (e.g., extracted features of), electronic communications of past dialogs of a plurality of users. In some implementations, theelectronic communications database 152 is maintained by theelectronic communications system 110. For example, theelectronic communications system 110 may include one or more chat systems and theelectronic communications database 152 may include a plurality of electronic communications that are sent and/or received via the system(s). As another example, theelectronic communications system 110 may include one or more social networking systems and theelectronic communications database 152 may include a plurality of messages, posts, or other electronic communications that are sent and/or received via the social networking system(s). - As used herein, an “electronic communication” or “communication” may refer to an email, a text message (e.g., RCS, SMS, MMS), a chat message, an instant message, or any other electronic communication that is sent from a user to a group of one or more additional users as part of a dialog. In various implementations, an electronic communication may include various metadata and the metadata may optionally be utilized in one or more techniques described herein. For example, an electronic communication such as chat message may include one or more sender identifiers, one or more recipient identifiers, a date sent, a type of device that sent and/or received the electronic communication, and so forth.
-
Graphical element system 120 determines and provides groups of graphical elements for potential inclusion in an electronic communication to be submitted by a user as part of a dialog. For convenience, many examples described herein will be described with respect tographical element system 120 providingclient device 106A with graphical elements for inclusion in an electronic communication, where the electronic communication is being composed by a user via theclient device 106A to be submitted as part of a dialog that involves the user and additional users of additional client device(s) 106B-N. It is understood that graphical element system 120 (or other instances thereof) may also provide each of client device(s) 106B-N with graphical elements for inclusion in communications being composed via those client device(s) 106B-N - The
graphical element system 120 may determine a group of graphical elements to provide for an electronic communication to be submitted by a user as part of a dialog, and may determine the group based on: one or more submitted communication features of previously submitted electronic communications of the dialog and/or one or more additional features, such as those described herein. In some implementations, thegraphical element system 120 may determine and/or provide the group of graphical elements independent of any textual input provided by the user in generating the electronic communication to be submitted as part of the dialog and/or independent any other content provided by the user in generating the electronic communication to be submitted as part of the dialog. - In some implementations, the graphical elements determined and provided by
graphical element system 120 include one or more emojis, GIFs, JPGs, and/or stickers. The graphical elements may be selected fromgraphical elements database 154, which is provided on one or more storage mediums. In some implementations, thegraphical elements database 154 is provided locally onclient device 106A. In some of those implementations, thegraphical element system 120 is operated, in whole or in part, by one or more processors of theclient device 106A. In some implementations, all or portions of thegraphical elements database 154 and/or one or more aspects of thegraphical element system 120 are additionally or alternatively provided remote from theclient device 106A. - The
graphical elements database 154 may define, for each of the graphical elements, associations of the graphical element to one or more graphical element features, and optionally weights for one or more of the associations. For example, each of the graphical elements may be indexed by one or more corresponding graphical element features and weights for each of the graphical element features. A weight of a given graphical element feature of a graphical element indicates how strongly the given graphical element feature is associated with the graphical element. For example, a given graphical element that depicts an alligator may be associated with a graphical element feature of “alligator” more strongly than it is associated with a graphical element feature of “animal”. - Various graphical element features may be defined for a graphical element. As one example, graphical element features may include descriptors of content included in the graphical element. For instance, a graphical element that includes a depiction of an alligator may include descriptors such as a descriptor of “alligator”, a descriptor of “animal”. Also, for instance, non-textual descriptors may be provided such as a non-textual descriptor that identifies an entity associated with a graphical element. As another example, graphical element features may include features of electronic communications, of a corpus of past electronic communications, in which the graphical element was included in those electronic communications. For instance, a graphical element that includes a depiction of an alligator may include a textual feature of “later” that indicates it at least sometimes co-occurs in electronic communications with the n-gram “later.” As another example, graphical element features may include features of electronic communications, of a corpus of past dialogs, in which those electronic communications preceded, in the dialogs, a subsequent electronic communication that includes the graphical element. For instance, a graphical element that includes a depiction of an alligator may include a feature that indicates a “departing remark” such as “talk to you soon”, “bye”, etc. that indicates the graphical element at least sometimes occurs in electronic communications that are subsequent to such “departing remark” electronic communications in dialogs. As yet another example, graphical element features may include features of user states associated with the graphical element. For instance, a graphical element that includes a depiction of an alligator may include a feature that indicates the graphical element at least sometimes occurs when user(s) of a dialog are located in Florida or Louisiana and/or when user(s) of a dialog have a “jovial” sentiment.
- In some implementations, one or more graphical element features may be assigned to graphical elements by graphical element features engine 137. For example, the graphical element features engine 137 may assign features to a given graphical element based on analysis of past dialogs that include the graphical element and/or based on those features being assigned (directly or indirectly) to the graphical element in content model(s) of content model(s) database 156 (described in more detail below). In some implementations, one or more graphical element features of a graphical element may additionally or alternatively be assigned based on image analysis of the graphical element, may be assigned by human reviewers, and/or may be assigned utilizing other techniques.
- The content mode(s)
database 156 includes one or more storage mediums that store one or more content models optionally utilized in various techniques described herein. In some implementations, one or more of the content models define relationships between various dialog features described herein and features of graphical elements. For example, a content model may define relationships between communication features of electronic communications of a dialog and one or more features of graphical elements. In some implementations, one or more of the content models define relationships between various dialog features described herein and features of text. For example, a content model may define relationships between communication features of earlier submitted electronic communications of a dialog and one or more features of text included in later submitted communications of the dialog. Such a content model may be utilized, for example, to identify graphical elements that are indexed by, or otherwise associated with, graphical element features corresponding to those features of text. - In some implementations, the content models of
database 156 include one or more learned content models that are each trained by contentmodel training engine 135 based on training examples derived from past electronic communications ofelectronic communications database 152. For example, a content model ofdatabase 156 may be a trained machine learning model that has been trained to predict features of graphical elements based on various dialog features. For instance, it may be trained based on training examples that each include training example input that includes features of one or more earlier submitted communications of a corresponding dialog and training example output that includes feature(s) of graphical element(s) included in a later submitted communication of the corresponding dialog (e.g., a later submitted communication that is a direct reply to a corresponding one of the earlier submitted communications). - In various implementations,
graphical element system 120 may include a submitted communication(s) featuresengine 122, auser state engine 124, a subsequent communication featuresengine 126, ahistorical usage engine 128, aselection engine 130, and/or apresentation engine 132.User state engine 124 may optionally be in communication with one or more user state models of userstate models database 158.Historical usage engine 128 may optionally be in communication with one or more historical models ofhistorical models database 160. In some implementations, all or aspects ofengines engines engines graphical element system 120, such asclient device 106A and/orelectronic communications system 110. - The submitted communication(s) features
engine 122 determines one or more communication features of one or more communications that have already been submitted in a dialog. For example, theengine 122 may determine communication feature(s) for the most recently submitted communication in the dialog, for a threshold quantity of the most recently submitted communications in the dialog, and/or for any communications submitted in the dialog within a threshold amount of time relative to a current time. - The
engine 122 may determine various communication features. For example, theengine 122 may determine one or more n-grams in the submitted electronic communication(s) as communication features. For instance, one or more of the n-grams may be determined based on term frequency of the n-gram in the submitted electronic communication(s) (i.e., how often the n-gram occurs in the electronic communication) and/or inverse document frequency of the n-gram in a collection of documents (i.e., how often the n-gram occurs in a collection of documents, such as a collection of electronic communications). Also, for example, theengine 122 may determine co-occurrence of two or more n-grams in one or more of the submitted electronic communication(s) as a communication feature, such as co-occurrence in a particular order (e.g., a first n-gram before a second n-gram), in a certain positional relationship (e.g., within n terms or characters of one another), etc. Also, for example, submitted communication(s) featuresengine 122 may determine one or more communication features based on one or more natural language processing tags or other labels applied to text of the submitted electronic communication(s) (e.g., parts of speech, named entities, entity types, tone); features based on text that occurs specifically in the first sentences, the last sentences, or other portion of the electronic communications; features based on metadata of the electronic communications such as a time the electronic communications were sent, day of the week the electronic communication were sent, a number of participants in the dialog, type(s) of device(s) that sent the electronic communication(s), etc. Also, for example, theengine 122 may determine one or more graphical element features of graphical elements included in one or more of the submitted electronic communication(s). - As another example of communication features, the
engine 122 may determine predicted textual suggestion(s) that are contextually relevant to the submitted electronic communications (e.g.,textual suggestions 194 ofFIG. 3C ) and utilize those textual suggestion(s) as communication feature(s). As yet another example of communication features, theengine 122 may determine an embedding vector of one or more features from the entirety of one or more submitted electronic communications or subset(s) of the submitted electronic communication(s) (e.g., one or more paragraphs, one or more sentences, one or more words). For instance, theengine 122 may apply or more features from a communication to a learned word embedding and determine, over the word embedding, an embedding vector of values mapped to the features in the word embedding. The embedding vector of values may be of a lower dimension than the applied features. The features utilized to determine the embedding vector may include one or more n-grams, labels applied to one or more n-grams, syntactic features, semantic features, metadata features, and/or other features. - The
engine 122 provides determined communication features toselection engine 130 for utilization byselection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog. - The subsequent communication features
engine 126 determines one or more subsequent communication features (if any) of the subsequent communication. For example, where the user has provided content to be included in the subsequent communication (e.g., textual input, graphical element(s)), theengine 126 may determine features of that content. Various features may be determined, such as one or more of those described above with respect to theengine 122. Theengine 126 provides determined subsequent communication features toselection engine 130 for utilization byselection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog. - The
user state engine 124 determines one or more current states of one or more dynamic states of one or more users engaged in the dialog. Theuser state engine 124 may determine the current state(s) of user(s) after appropriate authorization is obtained from those user(s). In some implementations, a current state may indicate a location of one or more users engaged in the dialog. For example, a current state may indicate a current location of the user that is formulating the subsequent communication to be submitted as part of the dialog. The location may be defined at various levels of granularity such as a city, state, ZIP code, a class of location (e.g., home, restaurant, Italian restaurant, concert venue). As another example, a current state may indicate the current location(s) of one or more additional users in the dialog, such as the user that most recently submitted a communication in the dialog. - In some implementations, a current state may indicate one or more classes and/or magnitudes of sentiment for one or more users engaged in the dialog. Various classes and/or magnitudes may be utilized. For example, classes that are indicative of happy, sad, neutral, active, tired, stressed, and/or other user state(s) may be utilized. In some of those and/or other implementations, the
user state engine 124 may also determine a magnitude of one or more of the sentiments. For instance, theuser state engine 124 may determine a weight of “0.8” for “happy”, a weight of “0.7” for “active”, and a weight of “0.0” for the other classes. Also, for example, more granular classes may additionally and/or alternatively be utilized such as slightly happy, medium happy, and very happy (e.g., in lieu of a general “happy” class). - In some implementations, the
user state engine 124 may utilize content of communications of the dialog in determining sentiment for one or more users. For example, the user state model(s)database 158 may include a sentiment classifier model and theengine 124 may apply, as input to the model, textual content from one or more communications of the dialog and generate, over the model based on the input, an indication of a sentiment indicated by the textual content. As another example, graphical elements included in submitted communications of a dialog may be mapped to graphical element features that indicate sentiment associated with those graphical elements. Theuser state engine 124 may utilize such mappings in determining sentiment. - In some implementations, the
user state engine 124 determines sentiment for one or more users using one or more indicators that are in addition to content of communications of the dialog, such as indicators based on one or more sensors (e.g., camera, microphone, keyboard sensors, touchscreen sensors, heart rate sensor) of a computing device (e.g., one ofclient devices 106A-N) of at least one of the users participating in the dialog. In some of those implementations, theuser state engine 124 determines class(es) and/or magnitude(s) of sentiment based on applying one or more user state indicators to one or more trained classifier model(s) or other trained machine learning model(s) of userstate models database 158. For example, one or more images of a user captured by a camera may be passed through a face detector and further through a facial expression classifier whose output indicates whether the facial expression is happy, tired, sad, and/or other class. Theuser state engine 124 may determine the sentiment based at least in part on the output of the facial expression classifier. As another example, a typing speed of a user determined from sensor data associated with one of the user interface input devices of a client device of the user may be applied by theuser state engine 124 to a user state model ofdatabase 158 and generated output utilized to determine sentiment of the user. - In some implementations, the
user state engine 124 determines sentiment based on synthesizing multiple indicators. For example, theuser state engine 124 may apply an image of a user captured by a camera to a first classifier trained to predict class(es) of sentiment based on images and may apply heart rate sensor data to a second classifier trained to predict class(es) of sentiment based on heart rate sensor data. Theuser state engine 124 may consider the outputs of the two classifiers in determining the sentiment. For example, theuser state engine 124 may average the outputs to determine the sentiment, use the greater magnitude output in determining the sentiment, and/or otherwise consider and/or combine both outputs in determining the sentiment. - In some implementations, the
user state engine 124 determines current state features(s) indicative of current states of multiple users involved in a dialog. Theuser state engine 124 may determine current state feature(s) indicative of the current states of multiple users utilizing various techniques. For example, theuser state engine 124 may determine one or more classes and/or magnitudes of sentiment for a first user, determine one or more classes and/or magnitudes of sentiment for a second user, and determine a combined sentiment based on the classes and/or magnitudes for both users. For example, theuser state engine 124 may average the classes and magnitudes of the two users. Additional or alternative techniques of determining current state feature(s) that is indicative of the user states of multiple users may be utilized. - In some implementations, the
user state engine 124 may determine current state feature(s) based on sensor data from one or more user interface input device(s) used by users engaged in the dialog. For example, when a given user has provided typed input in a dialog (e.g., using a physical keyboard or virtual keyboard), the sentiment of the given user may be determined based on a typing speed of the typed input, applied pressure for one or more characters of typed input (e.g., as sensed by a touch screen implementing a virtual keyboard), a “delay time” for starting to provide the typed input (e.g., when the typed input is provided responsive to other content), and/or other sensed features of the typed input. As another example, when the given user has provided voice input in a dialog (e.g., as sensed by a microphone), the sentiment of the given user may be determined based on tone, inflection, intonation, accent, pitch, volume, breathing volume, breathing rate, background noise level, background noise type, and/or other features of the voice input. As another example, theuser state engine 124 may determine sentiment for the given user based on audio input from a microphone, even when the microphone isn't used by the given user to provide textual input. For example, theuser state engine 124 may determine the sentiment based on the presence and/or absence of certain types of sound (e.g., laughter, crying, music) in the audio input, background noise level in the audio input, breathing noise level in the audio input, breathing rate in the audio input, aspects of detected speech in the audio input (e.g., intonation, accent), etc. - The
user state engine 124 provides determined current state feature(s) toselection engine 130 for utilization byselection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog. - The
historical usage engine 128 stores, inhistorical models database 160, historical usage features related to past usage of graphical elements by the user composing the communication to be submitted as part of the dialog. Various types of historical usage features may be stored. For example, thehistorical usage engine 128 may store a quantity and/or frequency of: usage of each of a plurality of individual graphical elements (e.g., frequency of usage of a particular emoji); usage of graphical elements having particular graphical features (e.g., frequency of usage of graphical elements having a “happy” graphical element feature); usage of graphical elements from a particular cohesive pack (e.g., frequency of usage of graphical elements from a “custom” sticker pack installed by the user). Moreover, historical usage features may optionally be associated with context information that indicates context of the historical usage, such as users(s) engaged in the dialog(s) associated with the usage; locations, dates, and/or times associated with the usage; etc. For example, historical usage features may indicate a frequency of usage of graphical elements having a particular graphical feature in combination with dialog(s) involving a group of one or more other users. - The
historical usage engine 128 provides historical usage features toselection engine 130 for utilization byselection engine 130 in determining graphical elements to provide for inclusion in a subsequent communication to be submitted as part of the dialog. - The
selection engine 130 utilizes one or more of the features provided by theengines selection engine 130 utilizes features from each of theengines engines selection engine 130 may only utilize a subset of provided features. - In some implementations, the
selection engine 130 selects one or more graphical elements for inclusion in the group based on provided features matching one or more graphical element features that are assigned to those graphical elements ingraphical elements database 154. In some of those implementations, theselection engine 130 may further select the graphical elements for inclusion based on weights of the matching graphical element features for the selected graphical elements. As one example, assume submitted communication featuresengine 122 provides a communication feature that is an n-gram “dinner” and thatuser state engine 124 provides a current state feature that indicates a current state of “jovial”. Theselection engine 130 may select one or more graphical elements that have graphical element features that include an n-gram of “dinner” and a current state of “jovial”, and may optionally select them based on the weights of “dinner” and “jovial” for those graphical elements. As another example, assume submitted communication featuresengine 122 provides a predicted textual suggestion of “later alligator” that is contextually relevant to a submitted electronic communication (e.g., a submitted electronic communication of “bye”). Theselection engine 130 may select a graphical element that is indexed by one or more terms of the predicted textual suggestion, such as a graphical element that depicts an alligator and is indexed by the term “alligator”. - In some implementations, the
selection engine 130 may apply one or more provided features as input to one or more of the content models ofcontent models database 156, and select one or more graphical elements fromdatabase 154 based on output generated over the content model(s) based on the applied input. For example, one of the content models may define relationships between submitted communication features of a dialog and one or more features of graphical elements. Theselection engine 130 may apply, as input to the content model, submitted communication features provided byengine 122 and generate, over the content model based on the input, an indication of one or more graphical element features. Theselection engine 130 may utilize the generated indication to select one or more graphical elements to include in the group. As another example, one of the content models may define relationships between communication features of earlier submitted electronic communications of a dialog and one or more features of text included in later submitted communications of the dialog. Theselection engine 130 may apply, as input to the model, various received dialog features and generate, over the model based on the input, an indication of features of text that is typically submitted in later communications of the dialog. Theselection engine 130 may utilize the features of the text to select one or more graphical elements to include in the group based on those graphical elements having graphical element features that match the features of text. - The
presentation engine 132 provides the group of graphical elements determined by theselection engine 130 for inclusion in an electronic communication to be submitted as part of the dialog. Providing a group of graphical elements may include, for example, providing the actual graphical elements or providing indications of the graphical elements. In some implementations, thepresentation engine 132 may provide the group of graphical elements for presentation and, when a user generating the communication selects one of the graphical elements, the corresponding graphical element may be incorporated in the communication. In some implementations, the group of graphical elements may be provided and/or presented in response to the user selecting a “reply” user interface element, an “additional content” user interface element, or in response to other user interface input indicative of a desire to include graphical elements in the communication. - In some implementations, the
presentation engine 132 provides the graphical elements for presentation independent of any textual input and/or other content provided via a computing device of the user in generating the communication. In some implementations, thepresentation engine 132 provides the graphical elements for inclusion in the reply based on scores for the graphical elements that are determined by theselection engine 130. For example, in some implementations thepresentation engine 132 may provide the graphical elements for presentation with prominences that are based on their associated scores. For example, the graphical element with the score most indicative of relevance may be provided for presentation most prominently (e.g., the topmost, leftmost, and/or largest). As another example, when the scores for one or more of the selected graphical elements of a group satisfy a threshold, the group of graphical elements may be provided for presentation automatically or in response to certain first user interface input, whereas separate user interface input may be required when the score(s) fail to satisfy the threshold. For example, if the scores satisfy the threshold, the group of graphical elements may be provided and presented to the user when the user selects a “reply” interface element without requiring the user to separately select an “additional content” interface element that explicitly solicits suggested graphical elements and/or other additional content. However, if the score doesn't satisfy the threshold, the group of graphical elements may not be provided and presented to the user when the user selects a reply interface element. Rather, selection of the separate “additional content” interface element may be required before providing and/or presenting the group of graphical elements to the user. - In some implementations, one or more (e.g., all) aspects of the
presentation engine 132 may be implemented by theclient device 106A and/or theelectronic communications system 110. In some implementations, thepresentation engine 132 provides only indications of graphical elements of a group, and potentially prominence information, and theclient device 106A and/orelectronic communications system 110 may generate a display of the group of graphical elements based on the provided data. In some implementations, thepresentation engine 132 may additionally provide some or all of the data necessary to generate the display. In some of those implementations, any provided prominence information may be incorporated in the data that indicates how the display should be presented. - Referring now to
FIG. 2 , additional description is provided of the components ofFIG. 1 . InFIG. 2 , user interface input devices 106A1-106N1 ofcorresponding client devices 106A-106N are utilized in generating submittedcommunications 201 that have been submitted as part of a dialog. For example, a user ofclient device 106B may utilize user interface input device 106B1 to submit one of the submittedcommunications 201 as part of a dialog that involves the user of theclient device 106B, the user of theclient device 106A, and optionally one or more additional users. - The submitted communication(s) features
engine 122 determines one or more submitted communication features 222 of the submittedcommunications 201 and provides the submitted communication features 222 to theselection engine 130. - The user of the
client device 106A optionally utilizes user interface input device(s) 106A1 in providingsubsequent content 203 for inclusion in a subsequent electronic communication to be submitted by the user as part of the dialog. For example, the content may include text based on spoken or typed input of the user. The subsequent communication featuresengine 126 determines one or more subsequent communication features 226 based on thesubsequent content 203 and provides the subsequent communication features 226 to theselection engine 130. As described herein, in some implementations the user may not have provided anysubsequent content 203 for inclusion in the subsequent electronic communication and subsequent communication features 226 may not be determined or provided to theselection engine 130. - The
user state engine 124 determines current state features 224 for one or more users engaged in the dialog based on content of the submittedcommunications 201, based on thesubsequent content 203, and/or based one or more sensors 106A2-106N2 ofcorresponding client devices 106A-106N. Theuser state engine 124 may optionally utilize one or more user state models ofdatabase 158 in determining the current state features 224 based on such input. Theuser state engine 124 provides the current state features 224 to theselection engine 130. - The
historical usage engine 128 provides historical usage features 228 to theselection engine 130 based on one or more historical models of historical model(s)database 160. - The
selection engine 130 determines a group ofgraphical elements 207 based on one or more of the provided features 222, 224, 226, and 228. In some implementations, theselection engine 130 selects one or more of the graphical elements of thegroup 207 based on those graphical elements each having assigned graphical element features, in thegraphical elements database 154, that match one or more of the provided features 222, 224, 226, and 228. In some implementations, theselection engine 130 additionally or alternatively selects one or more of the graphical elements of thegroup 207 based on applying one or more of the provided features 222, 224, 226, and 228 as input to one or more of the content models of content model(s)database 156, generating output over the content model(s) based on the input, and selecting graphical elements having assigned graphical element features, in thegraphical elements database 154, that match the generated output. - The
presentation engine 132 provides the group ofgraphical elements 207 for presentation to the user via a user interface output device 106A3 (e.g., a display) of theclient device 106A. - Although
FIG. 2 is illustrated with each of theengines selection engine 130, in other implementations one or more of the engines may not provide features to the selection engine 130 (e.g., one or more engines may be omitted). - Referring now to
FIGS. 3A-4 , various examples of implementations of thegraphical element system 120 are described.FIGS. 3A , 3B1, 3B2, 3B3, 3C, 3D1, 3D2, and 4 each illustrate theclient device 106A and adisplay screen 140 of theclient device 106A (screen 140 may be the user interface output device 106A3 ofFIG. 2 ). Thedisplay screen 140 in the various figures displays graphical user interfaces with examples of dialog that involves the user and one or more additional users, and examples of how graphical elements may be presented to the user for inclusion in a subsequent electronic communication to be submitted by the user as part of the dialog, according to implementations disclosed herein. One or more aspects of thegraphical element system 120 may be implemented on theclient device 106A and/or on one or more computing devices that are in network communication with theclient device 106A. - The
display screen 140 ofFIGS. 3A , 3B1, 3B2, 3B3, 3C, 3D1, 3D2, and 4 further includes areply interface element 188 that the user may select to generate user interface input via a virtual keyboard and a voicereply interface element 189 that the user may select to generate user interface input via a microphone. In some implementations, the user may generate user interface input via the microphone without selection of the voicereply interface element 189. For example, during the dialog, active monitoring for audible user interface input via the microphone may occur to obviate the need for the user to select the voicereply interface element 189. In some of those and/or in other implementations, the voicereply interface element 189 may be omitted. Moreover, in some implementations, thereply interface element 188 may additionally and/or alternatively be omitted (e.g., the user may only provide audible user interface input and interact with suggested graphical elements and/or textual suggestions). Thedisplay screen 140 ofFIGS. 3A , 3B1, 3B2, 3B3, 3C, 3D1, 3D2, and 4 also includessystem interface elements client device 106A to perform one or more actions. - In
FIG. 3A , a transcript of a dialog between a user of theclient device 106A and at least two additional users (Bob and Tom) is displayed in a graphical user interface. A first additional user (Bob) in the dialog has submitted theelectronic communication 321 of “How About Dinner Tomorrow?” and a second additional user (Tom) in the dialog has theelectronic communication 322 of “I am in!”. - In the example of
FIG. 3A , thereply interface element 188 is provided, which the user may select to provide content for an electronic communication to be submitted as part of the dialog. An additionalcontent interface element 187 is also provided, which the user may select to be presented with a group of one or more graphical elements for potential inclusion in an electronic communication to be submitted as part of the dialog. - FIGS. 3B1, 3B2, and 3B3 illustrate an example of how graphical elements may be presented to the user, in response to selection of the additional
content interface element 187, and included in a subsequent electronic communication that is submitted by the user as part of the dialog ofFIG. 3A . - FIG. 3B1 illustrates the graphical interface after the user has selected the additional
content interface element 187 inFIG. 3A . In FIG. 3B1, a group ofgraphical elements 191 are presented to the user in response to the selection of the additionalcontent interface element 187. Five graphical elements are included in thegroup 191 and an ellipsis is also shown to the right of thegroup 191. In some implementations, the ellipsis may be selected to view additional graphical elements, such as those that are also contextually relevant but not included in the initial group and/or those that are not contextually relevant (e.g., to browse all available graphical elements). - One or more of the graphical elements of the
group 191 may be selected by theselection engine 130 based on various feature described herein. For example, they may be selected by theselection engine 130 based on submitted communication feature(s) ofelectronic communications 321 and/or 322 as provided by the submitted communication(s) featuresengine 122. For instance, the submitted communication feature(s) may include a textual feature of “dinner” based on presence of that term incommunication 321. In some implementations, theselection engine 130 may select the “pizza” and “hamburger” graphical elements based on those being assigned graphical element features (e.g., dinner and/or food) that match the textual feature of “dinner.” In some implementations, theselection engine 130 may select the “pizza”, “hamburger”, and/or “wine glass” graphical elements based on applying the “dinner” textual feature to one or more of the content models ofdatabase 156 and generating, based on application of the textual feature to the content models, output that indicates that subsequent communications in dialogs that include submitted communications with a “dinner” textual feature often include “food” and “drink” graphical elements and/or “food” and “drink” text. Theselection engine 130 may select the “pizza”, “hamburger”, and “wine glass” graphical elements based on each of those being assigned graphical element features that match one or more aspects of the output. - As another example, the
selection engine 130 may select the “OK hand signal” and “smiley” graphical elements based on applying additional or alternative submitted communication features to a content model, and generating, based on application of the submitted communication features to the content model, output that indicates that subsequent communications in dialogs that include those submitted communication features often include “confirmatory” and “happy” graphical elements and/or “confirmatory” and “happy” text. Theselection engine 130 may select the “OK hand signal” graphical element based on it being assigned a graphical element feature of “confirmatory” and may select the “smiley face” graphical element based on it being assigned a “happy” graphical element feature. - FIG. 3B2 illustrates the graphical interface after the user has selected the “OK hand signal” graphical element, then selected the “pizza” graphical element, then provided further user interface input (e.g., typed input) of “?”. In response to the selections and the further user interface input, the selected graphical elements and the further user interface input are populated sequentially in the
reply interface element 188, as part of a subsequent communication being composed by the user. Asubmission interface element 193 is also provided in FIG. 3B2. - FIG. 3B3 illustrates the graphical interface after the user has selected the
submission interface element 193 in FIG. 3B2 to submit, for inclusion in the dialog, the subsequent communication being composed in FIG. 3B1. In FIG. 3B3, the subsequent communication being composed in FIG. 3B1 has been submitted and is included in the dialog. This is illustrated by the updated transcript of the dialog in FIG. 3B3 that now includes thesubsequent communication 332. -
FIG. 3C illustrates another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog ofFIG. 3A . In some implementations, the graphical interface ofFIG. 3C may be presented in lieu of that of FIG. 3B1 in response to selection of the additionalcontent interface element 187. In some implementations, the graphical interface ofFIG. 3C may be presented in response to selection of thereply interface element 188 inFIG. 3A in lieu of selection of the additionalcontent interface element 187. - In
FIG. 3C , the group ofgraphical elements 191 is the same as that presented inFIG. 3B . However, inFIG. 3C the interface also includes avirtual keyboard 187 to enable the user to formulate textual input, and includestextual suggestions 194 that are contextually relevant to the dialog. Thetextual suggestions 194 may be generated, for example, by applying submitted communication features to one or more of the content models ofcontent models database 156 to generate output that indicates text included in subsequent communications in dialogs that include those submitted communication features. InFIG. 3C , the user may use thekeyboard 187 to generate textual content for inclusion in the communication to be submitted, may select one of thetextual suggestions 194 to include as content in the communication to be submitted, and/or may select one of the graphical elements of thegroup 191 to include as content. Once the user has formulated the communication to be submitted with desired content, the user may submit it for inclusion in the dialog utilizing thesubmission interface element 193. As described herein, in some implementations one or more of thetextual suggestions 194 may be utilized as a feature for selecting one or more of thegraphical elements 191. For example, the “OK hand signal” graphical element and/or the “happy” graphical element may be selected based at least in part on being indexed by the phrase “sounds good” and/or the term “good”. - FIGS. 3D1 and 3D2 illustrate yet another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog of
FIG. 3A . In some implementations, the graphical interface of 3D1 may be presented in response to selection of thereply interface element 188 inFIG. 3A . InFIG. 3D , the user has used thevirtual keyboard 187 to provide textual input of “Yes, what can I bring?” in a subsequent communication to be submitted in the dialog. - FIG. 3D2 illustrates an example of a group of
graphical elements 195 that may be presented in response to user selection of the additionalcontent interface element 187 after providing the textual input of FIG. 3D1. The group ofgraphical elements 195 differs from the group 191 (FIGS. 3B1, 3B2, 3C) based on being selected while also taking into account subsequent communication features that are based on the textual input of “Yes, what can I bring?”. For example, theselection engine 130 may utilize one or more subsequent communication features provided by subsequent communication featuresengine 126 in selecting the graphical elements for inclusion in thegroup 195. Such utilization of the subsequent communication features causes the display prominence of the “wine glass” graphical element to be promoted and causes new “pie” and “salad” graphical elements to be included in thegroup 195. The user may optionally select one or more of the graphical elements to include the selected graphical elements in the communication to be submitted, and submit the communication through selection of thesubmission interface element 193. - In some implementations, in advance of providing the textual input of “Yes, what can I bring?” in FIG. 3D1, the user may first select the additional
content interface element 187 and be presented with thegroup 191 of FIGS. FIGS. 3B1, 3B2, 3C. In some of those implementations, in response to providing the textual input of “Yes, what can I bring?” in FIG. 3D1, thegroup 191 may be supplanted with the group 195 (FIG. 3D2) based on consideration of subsequent communication features that are based on that textual input. Also, in some implementations, further submitted communication(s) from additional users that are submitted as part of the dialog during composition of a subsequent communication of the user may be utilized to update a provided group of graphical elements (i.e., by further taking into account submitted communication features of the further submitted communication(s)). -
FIG. 4 illustrates the example client computing device ofFIG. 3A , and illustrates another example of dialog and another example of how graphical elements may be presented to the user for potential inclusion in a subsequent communication to be submitted by the user as part of the dialog. InFIG. 4 , a transcript of a dialog between a user of theclient device 106A and at least one additional user (Bob) is displayed in a graphical user interface. An additional user (Bob) in the dialog has submitted theelectronic communication 421 of “Talk to you soon”. - The user has used the
virtual keyboard 187 to provide, in thereply interface element 188, textual input of “Later” as content for a subsequent electronic communication to be submitted as part of the dialog. A group ofgraphical elements 197 is presented to the user and includes an “alligator” graphical element and a “smiley” graphical element. In some implementations, thegroup 197 may be presented to the user without explicit user interface input requesting suggested graphical elements be provided. In some of those implementations, thegroup 197 may be provided based on theselection engine 130 determining that scores associated with one or more of the graphical elements of the group satisfy a threshold. In some implementations, theselection engine 130 may determine thegroup 197 based on submitted communication feature(s) ofcommunication 421, subsequent communication feature(s) based on the textual input “Later”, and based on sentiment of the user and/or of “Bob” that indicates they are “jovial”. For example, the “alligator” graphical element may be indexed with graphical element features that indicate it has a strong weight for subsequent communications that include the term “later”, has a strong weight for submitted communications that include a “departing remark” such as “talk to you soon”, and has a strong weight for “jovial” sentiments. -
FIG. 5A is a flowchart illustrating anexample method 500 of providing a group of graphical elements for inclusion in a dialog according to implementations disclosed herein. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components ofgraphical element system 120. Moreover, while operations ofmethod 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 552, the system receives an electronic communication submitted as part of a dialog that includes a user and one or more additional users. - At
block 554, the system determines one or more communication features of the electronic communication. For example, the system may determine one or more submitted communication features of one or more submitted communications that have been submitted as part of the dialog. Also, for example, the system may additionally or alternatively determine one or more subsequent communication features based on content (if any) that has been provided by the user for inclusion in a subsequent communication to submit as part of the dialog. - At
optional block 556, the system determines one or more additional features. For example, the system may determine current state feature(s) that indicate the current state of one or more dynamic states associated with user(s) engaged in the dialog. Also, for example, the system may additionally or alternatively determine historical usage features that indicate historical usage, of the user, of one or more graphical elements. - At
block 558, the system determines a group of graphical elements for inclusion in a subsequent communication of the user. In some implementations, the system performs block 559 as part of performingblock 558. Atblock 559, the system selects one or more graphical elements for the group based on the communication feature(s) determined atblock 554 and optionally based on the additional feature(s) determined atblock 556. Two examples ofblock 559 are illustrated inFIGS. 5B and 5C . - At
block 560, the system provides the group of graphical elements for potential inclusion in a subsequent communication of the dialog. - Multiple iterations of
blocks -
FIG. 5B is a flowchart illustrating an example of implementations ofblock 559 of themethod 500 in additional detail. While the operations of the flowchart ofFIG. 5B are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 562, the system identifies and optionally scores graphical elements that are associated with one or more of the communication features of block 554 (FIG. 5 ) and/or one or more of the additional features of block 556 (FIG. 5 ). For example, the system may identify graphical elements that are indexed by (e.g., include assigned graphical element features that correspond to) one or more of the features. The system may score each of the graphical elements based on a quantity of the features with which the graphical element is associated and/or based on optional weights assigned to the associations between the matching features and the graphical element. - At block 564, the system determines historical usage features of the user for one or more of the graphical elements identified at
block 562. The historical usage features may be features determined atblock 556 ofFIG. 5 . For example, for a given graphical element the system may determine: a frequency of use by the user of the given graphical element; a frequency of use by the user of all graphical elements sharing one or more graphical element characteristics with the given graphical element; and/or a frequency of use by the user of graphical elements from a cohesive graphical element pack to which the given graphical element belongs. - At block 566, the system optionally scores the identified graphical elements based on the historical usage features. The system may modify the optional score determined at
block 562 and/or may generate new scores based on the historical usage features. For example, the system may assign more prominent scores to graphical element(s) associated with at least a threshold degree of past usage, than it assigns to other graphical elements that do not have the threshold degree of past usage. Also, for example, the system may modify the optional score determined atblock 562 by promoting the scores of graphical element(s) whose historical usage feature(s) indicate at least a threshold degree of past usage by the user. - At
block 568, the system selects one or more of the identified graphical elements, optionally based on the scores. For example, the system may select the N most prominently scored graphical element for inclusion in a group to be initially displayed to a user. N may be selected based on various factors such as display properties of a client device on which the graphical elements will be provided, the scores themselves (e.g., only graphical elements having scores that satisfy a threshold will be selected), etc. -
FIG. 5C is a flowchart illustrating another example of implementations ofblock 559 of themethod 500 in additional detail. While the operations of the flowchart ofFIG. 5C are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 572, the system applies communication features and/or additional features as input to a trained machine learning model. For example, the trained machine learning model may embed relationships between features of electronic communications of a dialog and one or more graphical element features and/or one or more features of text. - At
block 574, the system generates, over the model based on the input applied atblock 572, output that is indicative of one or more graphical elements. For example, the output may indicate one or more graphical element features and/or one or more features of text. In some implementations, the output may also optionally indicate weights for one or more of the indicated features. - At
block 576, the system selects one or more graphical elements based on the output ofblock 574. For example, the system may select graphical elements that are indexed by, or otherwise associated with, graphical element features corresponding to feature(s) indicated by the output ofblock 574. In some implementations, optional weights indicated in the output for the features may be utilized to select one or more of the graphical elements. For example, those graphical elements associated with features having stronger weights may be more likely to be selected than those graphical elements associated only with features having weaker weights. For instance, scores may be determined for each of the graphical elements based on the weights and the scores utilized to select some of the graphical elements. In some implementations, the scores may additionally or alternatively be determined based on historical usage features as described with respect to blocks 564 and 566 ofFIG. 5B . - Although
FIG. 5B andFIG. 5C are illustrated as different figures, in some implementations one or more of the blocks ofFIG. 5B and one or more of the blocks ofFIG. 5C may both be performed in a given iteration ofblock 559 ofmethod 500. For example, some graphical elements may be selected based on blocks ofFIG. 5B and other graphical elements may be selected based on blocks ofFIG. 5C . Also, for example, scores generated based on blocks ofFIG. 5B and scores generated based on blocks ofFIG. 5C may be combined and the combined scores utilized to select a subset of the graphical elements identified based on blocks ofFIG. 5B and blocks ofFIG. 5C . In other implementations, a given iteration ofblock 559 may include only blocks ofFIG. 5B or only blocks ofFIG. 5C . -
FIG. 6 is a flowchart illustrating anexample method 600 of assigning features to graphical elements. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as graphical element features engine 137. Moreover, while operations ofmethod 600 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 652, the system selects one or more graphical elements. For example, the system may select a single graphical element or may select a group of graphical elements. When the system selects the group of graphical elements, that group may be selected based on the graphical elements of the group all belonging to the same cohesive pack, and/or based on the graphical elements of the group all having one or more other already assigned graphical element features in common. - At
block 654, the system identifies features of dialogs that include the graphical elements. For example, the system may identify features of communications of the dialogs, where the graphical elements were included in those communications. Also, for example, the system may additionally or alternatively identify features of communications of the dialogs, where the graphical elements were not included in those communications but, instead, were included in subsequent communications of the dialogs. As yet another example, the system may additionally or alternatively identify user state features based on communications of the dialogs. - At
block 656, the system optionally determines weights for the features. For example, the system may determine a weight for a given feature based on its quantity of occurrences in the dialogs. For instance, for a graphical element that is a depiction of an alligator, the system may determine it is included with the term “later” in 500 communications of the dialogs, and that it is included with the term “Florida” in 300 communications of the dialogs. Based on its occurrence with “later” in a greater quantity of communications than its occurrence with “Florida”, a greater weight may be determined for the feature of “later” may than a weight determined for a feature of “Florida”. - At
block 658, the system assigns, to the graphical elements, one or more of the features and optionally the weights. For example, the system may assign, to the graphical elements, those features having at least a threshold weight. In some implementations, the features may optionally be assigned with an indication of parameters for the feature. For example, where a feature is based on communications in which the graphical elements were included, that feature may be assigned with an indication that it is associated with communications for which the graphical element is to be included. For instance, it may be assigned with an indication that indicates that feature is relevant to the graphical element only when that feature is included in a communication for which the graphical element is to be provided for inclusion. Also, for example, where a feature is based on communications of a dialog in which the graphical elements were included in subsequent communications of the dialog, that feature may be assigned with an indication that it is associated with submitted communications of a dialog. For instance, it may be assigned with an indication that indicates that feature is relevant to the graphical element only when that feature is included in an already submitted communication of a dialog, and the graphical element is to be provided for inclusion in a subsequent communication of the dialog. - Multiple iterations of
method 600 may be performed to assign features to a variety of different graphical elements. -
FIG. 7 is a flowchart illustrating anexample method 700 of training a machine learning content model. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as contentmodel training engine 135. Moreover, while operations ofmethod 700 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 752, the system selects one or more graphical elements. For example, the system may select all graphical elements available to the system or a subset of graphical elements available to the system. - At
block 754, the system identifies features of dialogs that include the graphical elements. For example, the system may identify features of communications of the dialogs, where the graphical elements were included in those communications. Also, for example, the system may additionally or alternatively identify features of communications of the dialogs, where the graphical elements were not included in those communications but, instead, were included in subsequent communications of the dialogs. As yet another example, the system may additionally or alternatively identify user state features based on communications of the dialogs. - At
block 756, the system generates training examples based on the dialog features and based on graphical element features of the graphical elements. For example, each of the training examples may include: training example input based on features from one of the dialogs and training example output based on graphical element features of one or more graphical elements included in the dialog. - At
block 758, the system trains a machine learning model based on the training examples. For example, the system may train the machine learning model based on application of the training example input of the training examples and backpropagation based on the training example output of the training examples. - As one particular example of the
method 700, assume a machine learning model with an input layer, multiple hidden layers, and an output layer that is a vector of 256 different graphical element features. - A first training example may be generated based on presence of a first graphical element in a first dialog. The training example input of the first training example may include one or more features of the first dialog, such as communication features for the communication that includes the first graphical element and/or communication features of one or more other communications of the first dialog. The training example output of the first training example may be the vector of 256 different graphical element features, with the values for each of the graphical element features being assigned based on stored assignments of those graphical element features to the first graphical element. For instance, if the first graphical element has only 5 of the 256 different graphical element features assigned, the training example output may include “1s” (or other “present” value(s)) for those 5 graphical element features and may include “0s” (or other “absent” value(s)) for the 251 other graphical element features. In some implementations, the “present” values may optionally be weighted based on, for example, corresponding weights of the assignments of those 5 graphical element features for the first graphical element.
- A second training example may be generated based on presence of a second graphical element in a second dialog. The training example input of the second training example may include one or more features of the second dialog, such as communication features for the communication that includes the first graphical element and/or communication features of one or more other communications of the second dialog. The training example output of the second training example may be the vector of 256 different graphical element features, with the values for each of the graphical element features being assigned based on stored assignments of those graphical element features to the second graphical element. For instance, if the second graphical element has only 4 of the 256 different graphical element features assigned, the training example output may include “1s” (or other “present” value(s)) for those 4 graphical element features and may include “0s” (or other “absent” value(s)) for the 252 other graphical element features. In some implementations, the “present” values may optionally be weighted based on, for example, corresponding weights of the assignments of those 4 graphical element features for the second graphical element.
- Additional training examples may be generated in a similar manner based on additional graphical elements and/or dialogs. Once trained based on the training examples, features of a new dialog may be applied as input to the trained machine learning model, and output generated over the model based on the applied input. The output will include values for each of the 256 different graphical element features. Each value indicates the likelihood that, given the applied features of the new dialog, that the corresponding graphical element feature is relevant to the new dialog. Those values may then be utilized to select one or more graphical elements for providing for inclusion in a subsequent message to be submitted as part of the dialog. For example, assume 20 particular values of the 256 values satisfy a threshold value. Graphical elements may be selected that are assigned the greatest quantity of those 20 particular values.
-
FIG. 8 is a block diagram of anexample computing device 810 that may optionally be utilized to perform one or more aspects of techniques described herein. In some implementations, one or more of a client computing device, automatedassistant 120, and/or other component(s) may comprise one or more components of theexample computing device 810. -
Computing device 810 typically includes at least oneprocessor 814 which communicates with a number of peripheral devices viabus subsystem 812. These peripheral devices may include astorage subsystem 824, including, for example, amemory subsystem 825 and afile storage subsystem 826, userinterface output devices 820, userinterface input devices 822, and anetwork interface subsystem 816. The input and output devices allow user interaction withcomputing device 810.Network interface subsystem 816 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices. - User
interface input devices 822 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term “input device” is intended to include all possible types of devices and ways to input information intocomputing device 810 or onto a communication network. - User
interface output devices 820 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term “output device” is intended to include all possible types of devices and ways to output information fromcomputing device 810 to the user or to another machine or computing device. -
Storage subsystem 824 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, thestorage subsystem 824 may include the logic to perform selected aspects of the methods ofFIGS. 5A, 5B, 5C, 6 , and/or 7. - These software modules are generally executed by
processor 814 alone or in combination with other processors.Memory 825 used in thestorage subsystem 824 can include a number of memories including a main random access memory (RAM) 830 for storage of instructions and data during program execution and a read only memory (ROM) 832 in which fixed instructions are stored. Afile storage subsystem 826 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations may be stored byfile storage subsystem 826 in thestorage subsystem 824, or in other machines accessible by the processor(s) 814. -
Bus subsystem 812 provides a mechanism for letting the various components and subsystems ofcomputing device 810 communicate with each other as intended. Althoughbus subsystem 812 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses. -
Computing device 810 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description ofcomputing device 810 depicted inFIG. 8 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations ofcomputing device 810 are possible having more or fewer components than the computing device depicted inFIG. 8 . - In situations in which certain implementations discussed herein may collect or use personal information about users (e.g., user data extracted from other electronic communications, information about a user's social network, a user's location, a user's biometric information, and a user's activities and demographic information), users are provided with one or more opportunities to control whether information is collected, whether the personal information is stored, whether the personal information is used, and how the information is collected about the user, stored and used. That is, the systems and methods discussed herein collect, store and/or use user personal information only upon receiving explicit authorization from the relevant users to do so. For example, a user is provided with control over whether programs or features collect user information about that particular user or other users relevant to the program or feature. Each user for whom personal information is to be collected is presented with one or more options to allow control over the information collection relevant to that user, to provide permission or authorization as to whether the information is collected and as to which portions of the information are to be collected. For example, users can be provided with one or more such control options over a communication network. In addition, certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed. As one example, a user's identity may be treated so that no personally identifiable information can be determined. As another example, a user's geographic location may be generalized to a larger region so that the user's particular location cannot be determined.
- While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (25)
Priority Applications (6)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/350,040 US10416846B2 (en) | 2016-11-12 | 2016-11-12 | Determining graphical element(s) for inclusion in an electronic communication |
CN202211006361.8A CN115373566A (en) | 2016-11-12 | 2017-09-21 | Determining graphical elements for inclusion in an electronic communication |
JP2019518995A JP2019537110A (en) | 2016-11-12 | 2017-09-21 | Determining graphical elements for inclusion in electronic communication |
PCT/US2017/052713 WO2018089109A1 (en) | 2016-11-12 | 2017-09-21 | Determining graphical elements for inclusion in an electronic communication |
EP17780938.1A EP3538980A1 (en) | 2016-11-12 | 2017-09-21 | Determining graphical elements for inclusion in an electronic communication |
CN201780069884.8A CN109983430B (en) | 2016-11-12 | 2017-09-21 | Determining graphical elements included in an electronic communication |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/350,040 US10416846B2 (en) | 2016-11-12 | 2016-11-12 | Determining graphical element(s) for inclusion in an electronic communication |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180136794A1 true US20180136794A1 (en) | 2018-05-17 |
US10416846B2 US10416846B2 (en) | 2019-09-17 |
Family
ID=60037703
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/350,040 Active 2037-01-04 US10416846B2 (en) | 2016-11-12 | 2016-11-12 | Determining graphical element(s) for inclusion in an electronic communication |
Country Status (5)
Country | Link |
---|---|
US (1) | US10416846B2 (en) |
EP (1) | EP3538980A1 (en) |
JP (1) | JP2019537110A (en) |
CN (2) | CN109983430B (en) |
WO (1) | WO2018089109A1 (en) |
Cited By (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180253196A1 (en) * | 2015-09-07 | 2018-09-06 | Samsung Electronics Co., Ltd. | Method for providing application, and electronic device therefor |
US20200007672A1 (en) * | 2018-06-29 | 2020-01-02 | North Inc. | Systems, devices, and methods for generating messages |
US10540694B2 (en) * | 2017-06-29 | 2020-01-21 | Tyler Peppel | Audience-based optimization of communication media |
US10572232B2 (en) * | 2018-05-17 | 2020-02-25 | International Business Machines Corporation | Automatically converting a textual data prompt embedded within a graphical user interface (GUI) to a widget |
US20200151773A1 (en) * | 2017-06-29 | 2020-05-14 | Tyler Peppel | Audience-based optimization of communication media |
WO2021006906A1 (en) * | 2019-07-11 | 2021-01-14 | Google Llc | System and method for providing an artificial intelligence control surface for a user of a computing device |
US10970329B1 (en) * | 2018-03-30 | 2021-04-06 | Snap Inc. | Associating a graphical element to media content item collections |
US11043230B1 (en) * | 2018-01-25 | 2021-06-22 | Wideorbit Inc. | Targeted content based on user reactions |
WO2021127778A1 (en) * | 2019-12-23 | 2021-07-01 | Myplanet Internet Solutions Ltd | Method and system for recognizing user intent and updating a graphical user interface |
USD933693S1 (en) * | 2019-10-11 | 2021-10-19 | menu, Inc. | Display screen with graphical user interface |
US11209964B1 (en) * | 2020-06-05 | 2021-12-28 | SlackTechnologies, LLC | System and method for reacting to messages |
EP3992877A1 (en) * | 2020-10-30 | 2022-05-04 | Honda Research Institute Europe GmbH | Social interaction opportunity detection method and system |
EP4064011A1 (en) * | 2021-03-26 | 2022-09-28 | Beijing Xiaomi Mobile Software Co., Ltd. | Method and device for displaying keyboard toolbar and storage medium |
US11488603B2 (en) * | 2019-06-06 | 2022-11-01 | Beijing Baidu Netcom Science And Technology Co., Ltd. | Method and apparatus for processing speech |
US11496425B1 (en) * | 2018-05-10 | 2022-11-08 | Whatsapp Llc | Modifying message content based on user preferences |
US20230005079A1 (en) * | 2019-12-30 | 2023-01-05 | Joyme Pte. Ltd. | Social information processing method and apparatus, and electronic device |
US20230042757A1 (en) * | 2020-04-30 | 2023-02-09 | Beijing Bytedance Network Technology Co., Ltd. | Human-computer interaction method and apparatus, and electronic device |
US11869039B1 (en) | 2017-11-13 | 2024-01-09 | Wideorbit Llc | Detecting gestures associated with content displayed in a physical environment |
Families Citing this family (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108781175B (en) | 2015-12-21 | 2021-09-21 | 谷歌有限责任公司 | Method, medium, and system for automatic suggestion of message exchange contexts |
EP3395019B1 (en) | 2015-12-21 | 2022-03-30 | Google LLC | Automatic suggestions and other content for messaging applications |
US10387461B2 (en) | 2016-08-16 | 2019-08-20 | Google Llc | Techniques for suggesting electronic messages based on user activity and other context |
US10511450B2 (en) | 2016-09-20 | 2019-12-17 | Google Llc | Bot permissions |
US10015124B2 (en) | 2016-09-20 | 2018-07-03 | Google Llc | Automatic response suggestions based on images received in messaging applications |
CN117634495A (en) | 2016-09-20 | 2024-03-01 | 谷歌有限责任公司 | Suggested response based on message decal |
US10416846B2 (en) | 2016-11-12 | 2019-09-17 | Google Llc | Determining graphical element(s) for inclusion in an electronic communication |
CN109478187A (en) * | 2017-04-25 | 2019-03-15 | 微软技术许可有限责任公司 | Input Method Editor |
US10860854B2 (en) | 2017-05-16 | 2020-12-08 | Google Llc | Suggested actions for images |
US10404636B2 (en) | 2017-06-15 | 2019-09-03 | Google Llc | Embedded programs and interfaces for chat conversations |
US10348658B2 (en) | 2017-06-15 | 2019-07-09 | Google Llc | Suggested items for use with embedded applications in chat conversations |
US10891526B2 (en) | 2017-12-22 | 2021-01-12 | Google Llc | Functional image archiving |
CN110263197B (en) * | 2019-06-12 | 2023-11-28 | 腾讯科技（深圳）有限公司 | Image searching method, device, computer equipment and storage medium |
CN111208930A (en) * | 2020-01-20 | 2020-05-29 | 北京元心科技有限公司 | Keyboard layout method and device, storage equipment and terminal equipment |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170075878A1 (en) * | 2015-09-15 | 2017-03-16 | Apple Inc. | Emoji and canned responses |
US20170153792A1 (en) * | 2015-11-30 | 2017-06-01 | Samsung Electronics Co., Ltd. | User terminal device and displaying method thereof |
US20170344224A1 (en) * | 2016-05-27 | 2017-11-30 | Nuance Communications, Inc. | Suggesting emojis to users for insertion into text-based messages |
Family Cites Families (164)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6092102A (en) | 1997-10-24 | 2000-07-18 | University Of Pittsburgh Of The Commonwealth System Of Higher Education | System and method for notifying users about information or events of an enterprise |
US7636750B2 (en) | 2001-10-24 | 2009-12-22 | Sprint Spectrum L.P. | Method and system for controlling scope of user participation in a communication session |
US7283992B2 (en) | 2001-11-30 | 2007-10-16 | Microsoft Corporation | Media agent to suggest contextually related media content |
US20040001099A1 (en) | 2002-06-27 | 2004-01-01 | Microsoft Corporation | Method and system for associating actions with semantic labels in electronic documents |
US7234117B2 (en) | 2002-08-28 | 2007-06-19 | Microsoft Corporation | System and method for shared integrated online social interaction |
US20110107223A1 (en) | 2003-01-06 | 2011-05-05 | Eric Tilton | User Interface For Presenting Presentations |
JP3669702B2 (en) | 2003-02-25 | 2005-07-13 | 松下電器産業株式会社 | Application program prediction method and mobile terminal |
BRPI0410362B1 (en) | 2003-05-16 | 2017-06-20 | Google Inc. | SYSTEMS AND METHODS OF SHARING NETWORK AND NETWORK MEDIA |
US10635723B2 (en) | 2004-02-15 | 2020-04-28 | Google Llc | Search engines and systems with handheld document data capture devices |
US20060029106A1 (en) | 2004-06-14 | 2006-02-09 | Semandex Networks, Inc. | System and method for providing content-based instant messaging |
US7720436B2 (en) | 2006-01-09 | 2010-05-18 | Nokia Corporation | Displaying network objects in mobile devices based on geolocation |
US7603413B1 (en) | 2005-04-07 | 2009-10-13 | Aol Llc | Using automated agents to facilitate chat communications |
US7860319B2 (en) | 2005-05-11 | 2010-12-28 | Hewlett-Packard Development Company, L.P. | Image management |
US7512580B2 (en) | 2005-08-04 | 2009-03-31 | Sap Ag | Confidence indicators for automated suggestions |
US7747785B2 (en) | 2006-04-14 | 2010-06-29 | Microsoft Corporation | Instant messaging plug-ins |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20080120371A1 (en) | 2006-11-16 | 2008-05-22 | Rajat Gopal | Relational framework for non-real-time audio/video collaboration |
US8082151B2 (en) | 2007-09-18 | 2011-12-20 | At&T Intellectual Property I, Lp | System and method of generating responses to text-based messages |
US20090119584A1 (en) | 2007-11-02 | 2009-05-07 | Steve Herbst | Software Tool for Creating Outlines and Mind Maps that Generates Subtopics Automatically |
CA2708757A1 (en) | 2007-12-17 | 2009-06-25 | Play Megaphone | System and method for managing interaction between a user and an interactive system |
US20110022992A1 (en) * | 2008-03-31 | 2011-01-27 | Koninklijke Philips Electronics N.V. | Method for modifying a representation based upon a user instruction |
US20090282114A1 (en) | 2008-05-08 | 2009-11-12 | Junlan Feng | System and method for generating suggested responses to an email |
US8166019B1 (en) | 2008-07-21 | 2012-04-24 | Sprint Communications Company L.P. | Providing suggested actions in response to textual communications |
JP5273712B2 (en) * | 2008-08-11 | 2013-08-28 | シャープ株式会社 | Information processing apparatus, information processing method, and information processing program |
US8391618B1 (en) | 2008-09-19 | 2013-03-05 | Adobe Systems Incorporated | Semantic image classification and search |
US20100228590A1 (en) | 2009-03-03 | 2010-09-09 | International Business Machines Corporation | Context-aware electronic social networking |
US8938677B2 (en) * | 2009-03-30 | 2015-01-20 | Avaya Inc. | System and method for mode-neutral communications with a widget-based communications metaphor |
US9195898B2 (en) | 2009-04-14 | 2015-11-24 | Qualcomm Incorporated | Systems and methods for image recognition using mobile devices |
US20120131520A1 (en) | 2009-05-14 | 2012-05-24 | Tang ding-yuan | Gesture-based Text Identification and Selection in Images |
US9782527B2 (en) | 2009-05-27 | 2017-10-10 | Tc1 Llc | Monitoring of redundant conductors |
CA2767033A1 (en) | 2009-07-02 | 2011-01-06 | Livechime, Inc. | System and method for enhancing digital content |
US8572084B2 (en) | 2009-07-28 | 2013-10-29 | Fti Consulting, Inc. | System and method for displaying relationships between electronically stored information to provide classification suggestions via nearest neighbor |
US9128610B2 (en) | 2009-09-30 | 2015-09-08 | At&T Mobility Ii Llc | Virtual predictive keypad |
US8831279B2 (en) | 2011-03-04 | 2014-09-09 | Digimarc Corporation | Smartphone-based methods and systems |
US8400548B2 (en) | 2010-01-05 | 2013-03-19 | Apple Inc. | Synchronized, interactive augmented reality displays for multifunction devices |
US8650210B1 (en) | 2010-02-09 | 2014-02-11 | Google Inc. | Identifying non-search actions based on a search query |
US8655965B2 (en) | 2010-03-05 | 2014-02-18 | Qualcomm Incorporated | Automated messaging response in wireless communication systems |
JP5733907B2 (en) | 2010-04-07 | 2015-06-10 | キヤノン株式会社 | Image processing apparatus, image processing method, and computer program |
US9929982B2 (en) | 2010-04-08 | 2018-03-27 | Microsoft Technology Licensing, Llc | Designating automated agents as friends in a social network service |
US20170098122A1 (en) | 2010-06-07 | 2017-04-06 | Affectiva, Inc. | Analysis of image content with associated manipulation of expression presentation |
US20120030289A1 (en) | 2010-07-30 | 2012-02-02 | Avaya Inc. | System and method for multi-model, context-sensitive, real-time collaboration |
US8781152B2 (en) | 2010-08-05 | 2014-07-15 | Brian Momeyer | Identifying visual media content captured by camera-enabled mobile device |
US9936333B2 (en) | 2010-08-10 | 2018-04-03 | Microsoft Technology Licensing, Llc | Location and contextual-based mobile application promotion and delivery |
KR101722687B1 (en) | 2010-08-10 | 2017-04-04 | 삼성전자주식회사 | Method for providing information between objects or object and user, user device, and storage medium thereof |
US9262517B2 (en) | 2010-08-18 | 2016-02-16 | At&T Intellectual Property I, L.P. | Systems and methods for social media data mining |
KR101753031B1 (en) | 2010-11-15 | 2017-06-30 | 엘지전자 주식회사 | Mobile terminal and Method for setting metadata thereof |
US9066126B2 (en) * | 2010-12-06 | 2015-06-23 | Google Technology Holdings LLC | Remote control apparatus, method and user interface system |
KR101060753B1 (en) | 2011-01-04 | 2011-08-31 | (주)올라웍스 | Method, terminal, and computer-readable recording medium for supporting collection of object included in inputted image |
US20120179717A1 (en) | 2011-01-11 | 2012-07-12 | Sony Corporation | System and method for effectively providing entertainment recommendations to device users |
US8688698B1 (en) | 2011-02-11 | 2014-04-01 | Google Inc. | Automatic text suggestion |
US8938669B1 (en) | 2011-03-15 | 2015-01-20 | Google Inc. | Inline user addressing in chat and document editing sessions |
US20130262574A1 (en) | 2011-03-15 | 2013-10-03 | Gabriel Cohen | Inline User Addressing in Chat Sessions |
US8554701B1 (en) | 2011-03-18 | 2013-10-08 | Amazon Technologies, Inc. | Determining sentiment of sentences from customer reviews |
JP2012221480A (en) * | 2011-04-06 | 2012-11-12 | L Is B Corp | Message processing system |
EP2523436A1 (en) | 2011-05-11 | 2012-11-14 | Alcatel Lucent | Mobile device and method of managing applications for a mobile device |
US8832284B1 (en) | 2011-06-16 | 2014-09-09 | Google Inc. | Virtual socializing |
US8589407B2 (en) | 2011-06-17 | 2013-11-19 | Google Inc. | Automated generation of suggestions for personalized reactions in a social network |
US8700480B1 (en) | 2011-06-20 | 2014-04-15 | Amazon Technologies, Inc. | Extracting quotes from customer reviews regarding collections of items |
US9245253B2 (en) | 2011-08-19 | 2016-01-26 | Disney Enterprises, Inc. | Soft-sending chat messages |
US8659667B2 (en) | 2011-08-29 | 2014-02-25 | Panasonic Corporation | Recipe based real-time assistance for digital image capture and other consumer electronics devices |
US9179278B2 (en) | 2011-09-01 | 2015-11-03 | Qualcomm Incorporated | Systems and methods involving augmented menu using mobile device |
US10102546B2 (en) | 2011-09-15 | 2018-10-16 | Stephan HEATH | System and method for tracking, utilizing predicting, and implementing online consumer browsing behavior, buying patterns, social networking communications, advertisements and communications, for online coupons, products, goods and services, auctions, and service providers using geospatial mapping technology, and social networking |
KR101402506B1 (en) | 2011-12-01 | 2014-06-03 | 라인 가부시키가이샤 | System and method for providing information interactively by instant messaging application |
GB2499395A (en) | 2012-02-14 | 2013-08-21 | British Sky Broadcasting Ltd | Search method |
US8620021B2 (en) | 2012-03-29 | 2013-12-31 | Digimarc Corporation | Image-related methods and arrangements |
US20130346235A1 (en) | 2012-06-20 | 2013-12-26 | Ebay, Inc. | Systems, Methods, and Computer Program Products for Caching of Shopping Items |
US9191786B2 (en) | 2012-06-27 | 2015-11-17 | At&T Intellectual Property I, L.P. | Method and apparatus for generating a suggested message to be sent over a network |
KR20140011073A (en) | 2012-07-17 | 2014-01-28 | 삼성전자주식회사 | Method and apparatus for recommending text |
KR101899817B1 (en) | 2012-08-01 | 2018-09-19 | 엘지전자 주식회사 | Mobile terminal and controlling method thereof |
US20140047413A1 (en) | 2012-08-09 | 2014-02-13 | Modit, Inc. | Developing, Modifying, and Using Applications |
KR102068604B1 (en) | 2012-08-28 | 2020-01-22 | 삼성전자 주식회사 | Apparatus and method for recognizing a character in terminal equipment |
US9218333B2 (en) | 2012-08-31 | 2015-12-22 | Microsoft Technology Licensing, Llc | Context sensitive auto-correction |
US20140088954A1 (en) | 2012-09-27 | 2014-03-27 | Research In Motion Limited | Apparatus and method pertaining to automatically-suggested emoticons |
US10691743B2 (en) | 2014-08-05 | 2020-06-23 | Sri International | Multi-dimensional realization of visual content of an image collection |
US9299060B2 (en) | 2012-10-12 | 2016-03-29 | Google Inc. | Automatically suggesting groups based on past user interaction |
US20150286371A1 (en) | 2012-10-31 | 2015-10-08 | Aniways Advertising Solutions Ltd. | Custom emoticon generation |
US9244905B2 (en) | 2012-12-06 | 2016-01-26 | Microsoft Technology Licensing, Llc | Communication context based predictive-text suggestion |
US20140164506A1 (en) | 2012-12-10 | 2014-06-12 | Rawllin International Inc. | Multimedia message having portions of networked media content |
US20140171133A1 (en) | 2012-12-18 | 2014-06-19 | Google Inc. | Query response |
CN103067490B (en) | 2012-12-26 | 2015-11-25 | 腾讯科技（深圳）有限公司 | The Notification Method of mobile terminal communication session, terminal, server and system |
GB201322037D0 (en) * | 2013-12-12 | 2014-01-29 | Touchtype Ltd | System and method for inputting images/labels into electronic devices |
US8930481B2 (en) | 2012-12-31 | 2015-01-06 | Huawei Technologies Co., Ltd. | Message processing method, terminal and system |
US20140189538A1 (en) | 2012-12-31 | 2014-07-03 | Motorola Mobility Llc | Recommendations for Applications Based on Device Context |
KR20140091633A (en) | 2013-01-11 | 2014-07-22 | 삼성전자주식회사 | Method for providing recommended items based on conext awareness and the mobile terminal therefor |
US20140237057A1 (en) | 2013-02-21 | 2014-08-21 | Genesys Telecommunications Laboratories, Inc. | System and method for processing private messages in a contact center |
JP6255646B2 (en) * | 2013-03-04 | 2018-01-10 | 株式会社Ｌ ｉｓ Ｂ | Message system |
US20140298364A1 (en) * | 2013-03-26 | 2014-10-02 | Rawllin International Inc. | Recommendations for media content based on emotion |
US8825474B1 (en) | 2013-04-16 | 2014-09-02 | Google Inc. | Text suggestion output using past interaction data |
US9177318B2 (en) | 2013-04-22 | 2015-11-03 | Palo Alto Research Center Incorporated | Method and apparatus for customizing conversation agents based on user characteristics using a relevance score for automatic statements, and a response prediction function |
US9923849B2 (en) | 2013-05-09 | 2018-03-20 | Ebay Inc. | System and method for suggesting a phrase based on a context |
US10599765B2 (en) | 2013-06-27 | 2020-03-24 | Avaya Inc. | Semantic translation model training |
US10162884B2 (en) | 2013-07-23 | 2018-12-25 | Conduent Business Services, Llc | System and method for auto-suggesting responses based on social conversational contents in customer care services |
US9161188B2 (en) | 2013-08-22 | 2015-10-13 | Yahoo! Inc. | System and method for automatically suggesting diverse and personalized message completions |
US9329692B2 (en) | 2013-09-27 | 2016-05-03 | Microsoft Technology Licensing, Llc | Actionable content displayed on a touch screen |
US20150100537A1 (en) | 2013-10-03 | 2015-04-09 | Microsoft Corporation | Emoji for Text Predictions |
US8996639B1 (en) | 2013-10-15 | 2015-03-31 | Google Inc. | Predictive responses to incoming communications |
US20150127753A1 (en) * | 2013-11-04 | 2015-05-07 | Meemo, Llc | Word Recognition and Ideograph or In-App Advertising System |
KR20150071768A (en) | 2013-12-18 | 2015-06-29 | 에스케이하이닉스 주식회사 | Image sensor and method for fabricating the same |
US10565268B2 (en) | 2013-12-19 | 2020-02-18 | Adobe Inc. | Interactive communication augmented with contextual information |
WO2015100362A1 (en) | 2013-12-23 | 2015-07-02 | 24/7 Customer, Inc. | Systems and methods for facilitating dialogue mining |
US9817813B2 (en) | 2014-01-08 | 2017-11-14 | Genesys Telecommunications Laboratories, Inc. | Generalized phrases in automatic speech recognition systems |
US20150207765A1 (en) | 2014-01-17 | 2015-07-23 | Nathaniel Brantingham | Messaging Service with Conversation Suggestions |
WO2015120019A1 (en) | 2014-02-10 | 2015-08-13 | Google Inc. | Smart camera user interface |
US10095748B2 (en) | 2014-03-03 | 2018-10-09 | Microsoft Technology Licensing, Llc | Personalized information query suggestions |
US9544257B2 (en) | 2014-04-04 | 2017-01-10 | Blackberry Limited | System and method for conducting private messaging |
US9213941B2 (en) | 2014-04-22 | 2015-12-15 | Google Inc. | Automatic actions based on contextual replies |
US10482163B2 (en) | 2014-04-23 | 2019-11-19 | Klickafy, Llc | Clickable emoji |
US10785173B2 (en) | 2014-07-03 | 2020-09-22 | Nuance Communications, Inc. | System and method for suggesting actions based upon incoming messages |
US9043196B1 (en) | 2014-07-07 | 2015-05-26 | Machine Zone, Inc. | Systems and methods for identifying and suggesting emoticons |
WO2016018111A1 (en) | 2014-07-31 | 2016-02-04 | Samsung Electronics Co., Ltd. | Message service providing device and method of providing content via the same |
CN104202718A (en) * | 2014-08-05 | 2014-12-10 | 百度在线网络技术（北京）有限公司 | Method and device for providing information for user |
US10218652B2 (en) | 2014-08-08 | 2019-02-26 | Mastercard International Incorporated | Systems and methods for integrating a chat function into an e-reader application |
US10447621B2 (en) | 2014-09-04 | 2019-10-15 | Microsoft Technology Licensing, Llc | App powered extensibility of messages on an existing messaging service |
US10146748B1 (en) | 2014-09-10 | 2018-12-04 | Google Llc | Embedding location information in a media collaboration using natural language processing |
US11010726B2 (en) * | 2014-11-07 | 2021-05-18 | Sony Corporation | Information processing apparatus, control method, and storage medium |
US20160140477A1 (en) | 2014-11-13 | 2016-05-19 | Xerox Corporation | Methods and systems for assigning tasks to workers |
US9569728B2 (en) | 2014-11-14 | 2017-02-14 | Bublup Technologies, Inc. | Deriving semantic relationships based on empirical organization of content by users |
US20160179816A1 (en) | 2014-12-22 | 2016-06-23 | Quixey, Inc. | Near Real Time Auto-Suggest Search Results |
KR20160089152A (en) * | 2015-01-19 | 2016-07-27 | 주식회사 엔씨소프트 | Method and computer system of analyzing communication situation based on dialogue act information |
KR101634086B1 (en) | 2015-01-19 | 2016-07-08 | 주식회사 엔씨소프트 | Method and computer system of analyzing communication situation based on emotion information |
US20160224524A1 (en) | 2015-02-03 | 2016-08-04 | Nuance Communications, Inc. | User generated short phrases for auto-filling, automatically collected during normal text use |
US20160226804A1 (en) | 2015-02-03 | 2016-08-04 | Google Inc. | Methods, systems, and media for suggesting a link to media content |
US9661386B2 (en) | 2015-02-11 | 2017-05-23 | Google Inc. | Methods, systems, and media for presenting a suggestion to watch videos |
US10079785B2 (en) | 2015-02-12 | 2018-09-18 | Google Llc | Determining reply content for a reply to an electronic communication |
US10353542B2 (en) * | 2015-04-02 | 2019-07-16 | Facebook, Inc. | Techniques for context sensitive illustrated graphical user interface elements |
US9883358B2 (en) | 2015-05-08 | 2018-01-30 | Blackberry Limited | Electronic device and method of determining suggested responses to text-based communications |
US10909329B2 (en) | 2015-05-21 | 2021-02-02 | Baidu Usa Llc | Multilingual image question answering |
US10504509B2 (en) | 2015-05-27 | 2019-12-10 | Google Llc | Providing suggested voice-based action queries |
KR20160148260A (en) | 2015-06-16 | 2016-12-26 | 삼성전자주식회사 | Electronic device and Method for controlling the electronic device thereof |
US10274911B2 (en) | 2015-06-25 | 2019-04-30 | Intel Corporation | Conversational interface for matching text of spoken input based on context model |
US9712466B2 (en) | 2015-11-10 | 2017-07-18 | Wrinkl, Inc. | Integrating actionable objects into an on-line chat communications platform |
US11025569B2 (en) | 2015-09-30 | 2021-06-01 | Apple Inc. | Shared content presentation with integrated messaging |
KR102393928B1 (en) | 2015-11-10 | 2022-05-04 | 삼성전자주식회사 | User terminal apparatus for recommanding a reply message and method thereof |
US10129193B2 (en) | 2015-11-17 | 2018-11-13 | International Business Machines Corporation | Identifying relevant content contained in message streams that appear to be irrelevant |
US20170171117A1 (en) | 2015-12-10 | 2017-06-15 | International Business Machines Corporation | Message Suggestion Using Dynamic Information |
CN108781175B (en) | 2015-12-21 | 2021-09-21 | 谷歌有限责任公司 | Method, medium, and system for automatic suggestion of message exchange contexts |
EP3395019B1 (en) | 2015-12-21 | 2022-03-30 | Google LLC | Automatic suggestions and other content for messaging applications |
KR101712180B1 (en) | 2015-12-29 | 2017-03-06 | 라인 가부시키가이샤 | Computer Readable Recording Medium with Program, method and apparatus for Transmitting/Receiving Message |
US9560152B1 (en) | 2016-01-27 | 2017-01-31 | International Business Machines Corporation | Personalized summary of online communications |
US11477139B2 (en) | 2016-02-25 | 2022-10-18 | Meta Platforms, Inc. | Techniques for messaging bot rich communication |
US20170250935A1 (en) | 2016-02-25 | 2017-08-31 | Facebook, Inc. | Techniques for messaging bot app interactions |
US20170250930A1 (en) | 2016-02-29 | 2017-08-31 | Outbrain Inc. | Interactive content recommendation personalization assistant |
US10831802B2 (en) | 2016-04-11 | 2020-11-10 | Facebook, Inc. | Techniques to respond to user requests using natural-language machine learning based on example conversations |
US10452671B2 (en) | 2016-04-26 | 2019-10-22 | Facebook, Inc. | Recommendations from comments on online social networks |
US9866693B2 (en) | 2016-05-06 | 2018-01-09 | Genesys Telecommunications Laboratories, Inc. | System and method for monitoring progress of automated chat conversations |
US9990128B2 (en) | 2016-06-12 | 2018-06-05 | Apple Inc. | Messaging application interacting with one or more extension applications |
US10254935B2 (en) | 2016-06-29 | 2019-04-09 | Google Llc | Systems and methods of providing content selection |
US10515393B2 (en) | 2016-06-30 | 2019-12-24 | Paypal, Inc. | Image data detection for micro-expression analysis and targeted data services |
US10387888B2 (en) | 2016-07-08 | 2019-08-20 | Asapp, Inc. | Assisting entities in responding to a request of a user |
US10049310B2 (en) | 2016-08-30 | 2018-08-14 | International Business Machines Corporation | Image text analysis for identifying hidden text |
CN117634495A (en) | 2016-09-20 | 2024-03-01 | 谷歌有限责任公司 | Suggested response based on message decal |
US10511450B2 (en) | 2016-09-20 | 2019-12-17 | Google Llc | Bot permissions |
US10015124B2 (en) | 2016-09-20 | 2018-07-03 | Google Llc | Automatic response suggestions based on images received in messaging applications |
WO2018057537A1 (en) | 2016-09-20 | 2018-03-29 | Google Llc | Bot interaction |
US11176931B2 (en) | 2016-09-23 | 2021-11-16 | Microsoft Technology Licensing, Llc | Conversational bookmarks |
US10416846B2 (en) | 2016-11-12 | 2019-09-17 | Google Llc | Determining graphical element(s) for inclusion in an electronic communication |
US20180196854A1 (en) | 2017-01-11 | 2018-07-12 | Google Inc. | Application extension for generating automatic search queries |
US10146768B2 (en) | 2017-01-25 | 2018-12-04 | Google Llc | Automatic suggested responses to images received in messages using language model |
US10229427B2 (en) | 2017-04-10 | 2019-03-12 | Wildfire Systems, Inc. | Virtual keyboard trackable referral system |
US20180316637A1 (en) | 2017-05-01 | 2018-11-01 | Microsoft Technology Licensing, Llc | Conversation lens for context |
AU2018261870B2 (en) | 2017-05-05 | 2020-11-05 | Seetvun AMIR | Dynamic response prediction for improved bot task processing |
US10860854B2 (en) | 2017-05-16 | 2020-12-08 | Google Llc | Suggested actions for images |
US10348658B2 (en) | 2017-06-15 | 2019-07-09 | Google Llc | Suggested items for use with embedded applications in chat conversations |
US10404636B2 (en) | 2017-06-15 | 2019-09-03 | Google Llc | Embedded programs and interfaces for chat conversations |
-
2016
- 2016-11-12 US US15/350,040 patent/US10416846B2/en active Active
-
2017
- 2017-09-21 CN CN201780069884.8A patent/CN109983430B/en active Active
- 2017-09-21 CN CN202211006361.8A patent/CN115373566A/en active Pending
- 2017-09-21 JP JP2019518995A patent/JP2019537110A/en active Pending
- 2017-09-21 EP EP17780938.1A patent/EP3538980A1/en not_active Withdrawn
- 2017-09-21 WO PCT/US2017/052713 patent/WO2018089109A1/en active Search and Examination
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170075878A1 (en) * | 2015-09-15 | 2017-03-16 | Apple Inc. | Emoji and canned responses |
US20170153792A1 (en) * | 2015-11-30 | 2017-06-01 | Samsung Electronics Co., Ltd. | User terminal device and displaying method thereof |
US20170344224A1 (en) * | 2016-05-27 | 2017-11-30 | Nuance Communications, Inc. | Suggesting emojis to users for insertion into text-based messages |
Cited By (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10552004B2 (en) * | 2015-09-07 | 2020-02-04 | Samsung Electronics Co., Ltd | Method for providing application, and electronic device therefor |
US20180253196A1 (en) * | 2015-09-07 | 2018-09-06 | Samsung Electronics Co., Ltd. | Method for providing application, and electronic device therefor |
US10540694B2 (en) * | 2017-06-29 | 2020-01-21 | Tyler Peppel | Audience-based optimization of communication media |
US11631110B2 (en) * | 2017-06-29 | 2023-04-18 | Tyler Peppel | Audience-based optimization of communication media |
US20200151773A1 (en) * | 2017-06-29 | 2020-05-14 | Tyler Peppel | Audience-based optimization of communication media |
US11869039B1 (en) | 2017-11-13 | 2024-01-09 | Wideorbit Llc | Detecting gestures associated with content displayed in a physical environment |
US11043230B1 (en) * | 2018-01-25 | 2021-06-22 | Wideorbit Inc. | Targeted content based on user reactions |
US11604819B2 (en) * | 2018-03-30 | 2023-03-14 | Snap Inc. | Associating a graphical element to media content item collections |
US20230252069A1 (en) * | 2018-03-30 | 2023-08-10 | Snap Inc. | Associating a graphical element to media content item collections |
US10970329B1 (en) * | 2018-03-30 | 2021-04-06 | Snap Inc. | Associating a graphical element to media content item collections |
US11496425B1 (en) * | 2018-05-10 | 2022-11-08 | Whatsapp Llc | Modifying message content based on user preferences |
US11936601B1 (en) * | 2018-05-10 | 2024-03-19 | Whatsapp Llc | Modifying message content based on user preferences |
US10572232B2 (en) * | 2018-05-17 | 2020-02-25 | International Business Machines Corporation | Automatically converting a textual data prompt embedded within a graphical user interface (GUI) to a widget |
US11144285B2 (en) * | 2018-05-17 | 2021-10-12 | International Business Machines Corporation | Automatically converting a textual data prompt embedded within a graphical user interface (GUI) to a widget |
US20210400132A1 (en) * | 2018-06-29 | 2021-12-23 | Google Llc | Systems, devices, and methods for generating messages |
US20200007672A1 (en) * | 2018-06-29 | 2020-01-02 | North Inc. | Systems, devices, and methods for generating messages |
US11089147B2 (en) * | 2018-06-29 | 2021-08-10 | Google Llc | Systems, devices, and methods for generating messages |
US11488603B2 (en) * | 2019-06-06 | 2022-11-01 | Beijing Baidu Netcom Science And Technology Co., Ltd. | Method and apparatus for processing speech |
WO2021006906A1 (en) * | 2019-07-11 | 2021-01-14 | Google Llc | System and method for providing an artificial intelligence control surface for a user of a computing device |
USD933693S1 (en) * | 2019-10-11 | 2021-10-19 | menu, Inc. | Display screen with graphical user interface |
WO2021127778A1 (en) * | 2019-12-23 | 2021-07-01 | Myplanet Internet Solutions Ltd | Method and system for recognizing user intent and updating a graphical user interface |
US20230005079A1 (en) * | 2019-12-30 | 2023-01-05 | Joyme Pte. Ltd. | Social information processing method and apparatus, and electronic device |
US20230042757A1 (en) * | 2020-04-30 | 2023-02-09 | Beijing Bytedance Network Technology Co., Ltd. | Human-computer interaction method and apparatus, and electronic device |
US11809690B2 (en) * | 2020-04-30 | 2023-11-07 | Douyin Vision Co., Ltd. | Human-computer interaction method and apparatus, and electronic device |
US11829586B2 (en) | 2020-06-05 | 2023-11-28 | Slack Technologies, Llc | System and method for reacting to messages |
US11209964B1 (en) * | 2020-06-05 | 2021-12-28 | SlackTechnologies, LLC | System and method for reacting to messages |
EP3992877A1 (en) * | 2020-10-30 | 2022-05-04 | Honda Research Institute Europe GmbH | Social interaction opportunity detection method and system |
US11614863B2 (en) | 2021-03-26 | 2023-03-28 | Beijing Xiaomi Mobile Software Co., Ltd. | Method and device for displaying keyboard toolbar and storage medium |
EP4064011A1 (en) * | 2021-03-26 | 2022-09-28 | Beijing Xiaomi Mobile Software Co., Ltd. | Method and device for displaying keyboard toolbar and storage medium |
Also Published As
Publication number | Publication date |
---|---|
CN109983430B (en) | 2022-09-13 |
WO2018089109A1 (en) | 2018-05-17 |
JP2019537110A (en) | 2019-12-19 |
CN109983430A (en) | 2019-07-05 |
EP3538980A1 (en) | 2019-09-18 |
CN115373566A (en) | 2022-11-22 |
US10416846B2 (en) | 2019-09-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10416846B2 (en) | Determining graphical element(s) for inclusion in an electronic communication | |
US11929072B2 (en) | Using textual input and user state information to generate reply content to present in response to the textual input | |
US11762865B2 (en) | Automatically augmenting message exchange threads based on tone of message | |
US11030515B2 (en) | Determining semantically diverse responses for providing as suggestions for inclusion in electronic communications | |
US11727220B2 (en) | Transitioning between prior dialog contexts with automated assistants | |
US11922945B2 (en) | Voice to text conversion based on third-party agent content | |
US10733529B1 (en) | Methods and apparatus for determining original electronic messages that contain asks | |
US11397857B2 (en) | Methods and systems for managing chatbots with respect to rare entities | |
US11842206B2 (en) | Generating content endorsements using machine learning nominator(s) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CASSIDY, RYAN;SRIVASTAVA, UTKARSH;VOLKOV, ANTON;AND OTHERS;SIGNING DATES FROM 20161111 TO 20161202;REEL/FRAME:040537/0105 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |