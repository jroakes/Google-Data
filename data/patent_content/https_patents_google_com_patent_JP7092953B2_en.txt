JP7092953B2 - Phoneme-based context analysis for multilingual speech recognition with an end-to-end model - Google Patents
Phoneme-based context analysis for multilingual speech recognition with an end-to-end model Download PDFInfo
- Publication number
- JP7092953B2 JP7092953B2 JP2021564950A JP2021564950A JP7092953B2 JP 7092953 B2 JP7092953 B2 JP 7092953B2 JP 2021564950 A JP2021564950 A JP 2021564950A JP 2021564950 A JP2021564950 A JP 2021564950A JP 7092953 B2 JP7092953 B2 JP 7092953B2
- Authority
- JP
- Japan
- Prior art keywords
- speech recognition
- language
- model
- bias
- phrase
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/083—Recognition networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/187—Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/193—Formal grammars, e.g. finite state automata, context free grammars or word networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/285—Memory allocation or algorithm optimisation to reduce hardware requirements
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/32—Multiple recognisers used in sequence or in parallel; Score combination systems therefor, e.g. voting systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
- G10L2015/025—Phonemes, fenemes or fenones being the recognition units
Description
特許法第３０条第２項適用 令和１年６月２１日にウェブサイトのアドレス ｈｔｔｐｓ：／／ａｒｘｉｖ．ｏｒｇ／ａｂｓ／１９０６．０９２９２にて発表Application of Article 30, Paragraph 2 of the Patent Act Website address https: // arxiv. Announced at org / abs / 1906.09292
本開示は、エンドツーエンドモデルにおける多言語（クロスリンガル）音声認識のための音素（ｐｈｏｎｅｍｅ）ベースのコンテキスト化（文脈解析）に関する。 The present disclosure relates to phoneme-based contextualization (context analysis) for multilingual speech recognition in an end-to-end model.
音声の文脈（コンテキスト）を認識することは、自動音声認識（ＡＳＲ）システムの目標である。しかし、人が話す言葉は多種多様であり、アクセントや発音にも違いがあるので、音声の文脈を認識することは困難である。多くの場合、人が話す単語やフレーズの種類（タイプ）は、その人が置かれている文脈に応じて変化する。 Recognizing the context of speech is the goal of an automated speech recognition (ASR) system. However, it is difficult to recognize the context of speech because the words spoken by humans are diverse and have different accents and pronunciations. In many cases, the type of word or phrase a person speaks varies depending on the context in which the person is placed.
文脈的（コンテクスチュアル）自動音声認識ＡＳＲは音声認識を、ユーザ自身のプレイリスト、連絡先、地理的な地名など、与えられた文脈（コンテキスト）に偏らせる（バイアスする）。文脈情報には、通常、認識すべき関連フレーズのリストが含まれており、このリストには、珍しいフレーズや、学習（トレーニング）ではあまり見られない外国語が含まれていることが多い。文脈バイアスを行うべく、従来の自動音声認識ＡＳＲシステムでは、文脈情報をｎ－ｇｒａｍ重み付き有限状態変換器（ＷＦＳＴ：ｗｅｉｇｈｔｅｄ Ｆｉｎｉｔｅ Ｓｔａｔｅ Ｔｒａｎｓｄｕｃｅｒ）を用いて、独立した文脈言語モデル（ＬＭ：Ｌａｎｇｕａｇｅ Ｍｏｄｅｌ）でモデル化し、その独立した文脈言語モデルＬＭをベースライン言語モデルＬＭと合成して、オンザフライ（ＯＴＦ）再スコアリングを行うことがある。 Contextual automatic speech recognition ASR biases speech recognition to a given context, such as your own playlists, contacts, or geographic place names. Contextual information usually contains a list of related phrases to be recognized, often containing unusual phrases and foreign languages that are rarely found in learning (training). In order to perform context bias, in the conventional automatic speech recognition ASR system, the context information is converted into an independent context language model (LM: Language Model) by using an n-gram weighted finite state converter (WFST). The independent contextual language model LM may be combined with the baseline language model LM for on-the-fly (OTF) rescoring.
近年、エンドツーエンド（Ｅ２Ｅ）モデルが自動音声認識ＡＳＲに大きな期待を寄せており、従来のオンデバイスモデルと比較して、ワードエラーレート（ＷＥＲ）やレイテンシの指標（メトリックス）が改善されている。これらのＥ２Ｅモデルは、音響モデル（ＡＭ）、発音モデル（ＰＭ：Ｐｒｏｎｕｎｃｉａｔｉｏｎ Ｍｏｄｅｌ）、および言語モデルＬＭを単一のネットワークに折り畳んで、音声とテキストの写像（スピーチツーテキストマッピング）を直接学習するものであり、音響モデルＡＭ、発音モデルＰＭ、および言語モデルＬＭを個別に有する従来のＡＳＲシステムと比較して、競争力のある結果を示している。代表的なＥ２Ｅモデルには、単語ベースのＣＴＣ（Ｃｏｎｎｅｃｔｉｏｎｉｓｔ ｔｅｍｐｏｒａｌ Ｃｌａｓｓｉｆｉｃａｔｉｏｎ）モデルと、ＲＮＮ－Ｔ（リカレントニューラルネットワークトランスデューサ）モデルと、ＬＡＳ（Ｌｉｓｔｅｎ， Ａｔｔｅｎｄ， およびＳｐｅｌｌ）などの注意ベースモデル（アテンションベースモデル）とがある。Ｅ２Ｅモデルは、ビーム検索復号時（ビームサーチデコーディング時）に限られた数の認識候補を保持しているので、文脈的自動音声認識ＡＳＲはＥ２Ｅモデルにとって困難である。 In recent years, the end-to-end (E2E) model has high expectations for automatic speech recognition ASR, and the word error rate (WER) and latency index (metrics) have been improved compared to the conventional on-device model. .. These E2E models fold an acoustic model (AM), a pronunciation model (PM), and a language model LM into a single network to directly learn speech and text mapping (speech-to-text mapping). It shows competitive results as compared to conventional ASR systems that have separate acoustic model AM, pronunciation model PM, and language model LM. Typical E2E models include a word-based CTC (Connectionist temporal Classification) model, an RNN-T (recurrent neural network transducer) model, and attention-based models (attention-based models) such as LAS (Listen, Atend, and Spel). ). Contextual automatic speech recognition ASR is difficult for the E2E model because the E2E model holds a limited number of recognition candidates during beam search decoding (beam search decoding).
本開示の一態様は、バイアス用語（ｂｉａｓｉｎｇ ｔｅｒｍ）リストに存在する用語に音声認識結果をバイアスする（偏らせる）方法を提供する。この方法は、データ処理ハードウェアにおいて、第１言語のネイティブスピーカによって話される発話を符号化（エンコーディング）する音声データ（オーディオデータ）を受け取る工程と、データ処理ハードウェアにおいて、第１言語とは異なる第２言語の１つまたは複数の用語（ｔｅｒｍｓ）を備えているバイアス用語リストを受け取る工程とを備えている。本方法は、データ処理ハードウェアによって、音声認識モデルを用いて、音声データから得られた音響特徴を処理して、第１言語における語句（ワードピース（Ｗｏｒｄｐｉｅｃｅ））と、対応する音素シーケンス（音素列）との両方に対する音声認識スコアを生成する工程も備えている。また、本方法は、データ処理ハードウェアによって、バイアス用語リスト内の１つまたは複数の用語に基づき、音素シーケンスに対する音声認識スコアを再スコアリングする工程を備えている。本方法はまた、データ処理ハードウェアによって、語句に対する音声認識スコアと、音素シーケンスに対する再スコアリングされた音声認識スコアとを用いて、発話に対する転写（トランスクリプション）を生成するための復号グラフ（デコーディンググラフ）を実行する工程を備えている。 One aspect of the present disclosure provides a method of biasing speech recognition results to terms present in a biasing term list. This method is a process of receiving voice data (audio data) that encodes a speech spoken by a native speaker of a first language in data processing hardware, and what is the first language in data processing hardware. It comprises the step of receiving a bias term list with one or more terms (terms) of different second languages. In this method, a speech recognition model is used by data processing hardware to process acoustic features obtained from speech data, and a phrase (wordpiece) in a first language and a corresponding phoneme sequence (phoneme) are processed. It also has a step of generating speech recognition scores for both columns). The method also comprises the step of rescoring the speech recognition score for the phoneme sequence based on one or more terms in the bias term list by the data processing hardware. The method also uses data processing hardware to generate a transcript for an utterance using a speech recognition score for a phrase and a rescored speech recognition score for a phoneme sequence (decoding graph). It has a process to execute the decoding graph).
本開示の実装は、以下のオプション機能のうちの１つまたは複数を備えていることができる。いくつかの実装では、音素シーケンスに対する音声認識スコアを再スコアリングする工程は、バイアスのかかった有限状態変換器（ｂａｉａｓｉｎｇ Ｆｉｎｉｔｅ Ｓｔａｔｅ Ｔｒａｎｓｄｕｃｅｒ。バイアスＦＳＴ）を使用して、音素シーケンスに対する音声認識スコアを再スコアリングする工程を備えている。これらの実装では、本方法は、データ処理ハードウェアによって、バイアス用語リスト内の各用語を、第２言語における対応する音素シーケンスにトークン化する工程と、データ処理ハードウェアによって、第２言語における各対応する音素シーケンスを、第１言語における対応する音素シーケンスに写像する工程と、データ処理ハードウェアによって、第１言語における各対応する音素シーケンスに基づき、バイアス有限状態変換器ＦＳＴを生成する工程と、を備えていることもできる。 Implementations of the present disclosure may include one or more of the following optional features: In some implementations, the step of rescoring the speech recognition score for a phoneme sequence uses a biased Finite State Transducer (bias FST) to re-score the speech recognition score for the phoneme sequence. It has a scoring process. In these implementations, the method tokenizes each term in the bias term list into a corresponding phonetic sequence in the second language by the data processing hardware and each in the second language by the data processing hardware. A step of mapping the corresponding phonetic sequence to the corresponding phonetic sequence in the first language, and a step of generating a bias finite state converter FST based on each corresponding phonetic sequence in the first language by data processing hardware. Can also be equipped with.
いくつかの例では、音声認識モデルは、エンドツーエンドの語句－音素モデルを備えている。特定の例では、エンドツーエンドの語句－音素モデルは、リカレントニューラルネットワーク－変換器（ＲＮＮ－Ｔ）を備えている。 In some examples, the speech recognition model comprises an end-to-end phrase-phoneme model. In a particular example, the end-to-end phrase-phoneme model comprises a recurrent neural network-transducer (RNN-T).
いくつかの実装では、復号グラフの実行中に、復号グラフは、バイアス用語リスト内の１つまたは複数の用語のいずれかを有利にするように転写（トランスクリプション）をバイアスする。音声認識モデルは、第１言語のみの学習発話で学習されてもよい。さらに、バイアス用語リスト内のいずれの用語も、音声認識モデルの学習に使用されなくてもよい。 In some implementations, during execution of the decoding graph, the decoding graph biases transcription to favor either one or more of the terms in the bias term list. The speech recognition model may be learned by learning utterances of only the first language. Moreover, none of the terms in the bias term list need to be used to train the speech recognition model.
データ処理ハードウェアおよび音声認識モデルは、ユーザ装置上、またはユーザ装置に通信するリモート計算装置上に存在してもよい。データ処理ハードウェアおよび音声認識モデルがリモート計算装置上に存在する場合、発話を符号化する音声データを受け取る工程は、ユーザ装置から、発話を符号化する音声データを受け取る工程を備えてもよい。 The data processing hardware and speech recognition model may reside on the user device or on a remote computing device that communicates with the user device. When the data processing hardware and the speech recognition model are present on the remote computer, the step of receiving the speech data encoding the utterance may include the step of receiving the speech data encoding the utterance from the user device.
本開示の別の態様は、バイアス用語リストに存在する用語に音声認識結果を偏らせるシステムを提供する。このシステムは、データ処理ハードウェアと、データ処理ハードウェアに通信するメモリハードウェアであって、データ処理ハードウェア上で実行されるとデータ処理ハードウェアに動作を実行させる命令を格納するメモリハードウェアとを備えている。この動作は、第１言語のネイティブスピーカによって話された発話を符号化する音声データを受け取る工程と、第１言語とは異なる第２言語の１つまたは複数の用語を備えているバイアス用語リストを受け取る工程と、音声認識モデルを使用して、音声データから得られた音響特徴を処理して、第１言語の語句と、対応する音素シーケンスとの両方に対する音声認識スコアを生成する工程とを備えている。また、動作は、バイアス用語リスト内の１つまたは複数の用語に基づき、音素シーケンスに対する音声認識スコアを再スコアリングする工程と、語句に対する音声認識スコアと、音素シーケンスに対する再スコアリングされた音声認識スコアとを使用して、復号グラフを実行して、発話に対する転写を生成する工程とを備えている。 Another aspect of the present disclosure provides a system that biases speech recognition results to terms present in the bias term list. This system is data processing hardware and memory hardware that communicates with the data processing hardware and stores instructions that cause the data processing hardware to perform operations when executed on the data processing hardware. And have. This behavior involves receiving audio data that encodes speech data spoken by a native speaker in the first language, and a bias term list that includes one or more terms in a second language that is different from the first language. It comprises the steps of receiving and processing the acoustic features obtained from the speech data using a speech recognition model to generate speech recognition scores for both the first language phrase and the corresponding phonetic sequence. ing. Also, the action is the process of rescoring the speech recognition score for the phoneme sequence, the speech recognition score for the phrase, and the rescored speech recognition for the phoneme sequence based on one or more terms in the bias term list. It comprises a step of performing a decoding graph using a score and generating a transcription for speech.
この態様は、以下のオプション機能の１つまたは複数を備えていることができる。いくつかの実装において、音素シーケンスに対する音声認識スコアを再スコアリングする工程は、バイアスのかかった有限状態変換器（ＦＳＴ）を使用して、音素シーケンスに対する音声認識スコアを再スコアリングする工程を備えている。これらの実装では、動作は、バイアス用語リスト内の各用語を第２言語の対応する音素シーケンスにトークン化する工程と、第２言語の各対応する音素シーケンスを第１言語の対応する音素シーケンスに写像する工程と、第１言語の各対応する音素シーケンスに基づきバイアス有限状態変換器ＦＳＴを生成する工程と、を備えていることもできる。 This aspect may include one or more of the following optional features: In some implementations, the step of rescoring the speech recognition score for a phoneme sequence comprises the step of rescoring the speech recognition score for a phoneme sequence using a biased finite state converter (FST). ing. In these implementations, the behavior is to tokenize each term in the bias term list into the corresponding phoneme sequence in the second language, and each corresponding phoneme sequence in the second language into the corresponding phoneme sequence in the first language. It may also include a step of mapping and a step of generating a bias finite state converter FST based on each corresponding phoneme sequence of the first language.
いくつかの例では、音声認識モデルは、エンドツーエンドの語句－音素モデルを備えている。特定の例では、エンドツーエンドの語句－音素モデルは、リカレントニューラルネットワーク－変換器（ＲＮＮ－Ｔ）を備えている。 In some examples, the speech recognition model comprises an end-to-end phrase-phoneme model. In a particular example, the end-to-end phrase-phoneme model comprises a recurrent neural network-transducer (RNN-T).
いくつかの実装では、復号グラフの実行中に、復号グラフは、バイアス用語リスト内の１つまたは複数の用語のいずれかに有利になるように転写（トランスクリプション）をバイアスする。音声認識モデルは、第１言語のみの学習発話で学習されてもよい。さらに、バイアス用語リスト内のいずれの用語も、音声認識モデルの学習に使用されなくてもよい。 In some implementations, during execution of the decoding graph, the decoding graph biases transcription in favor of one or more of the terms in the bias term list. The speech recognition model may be learned by learning utterances of only the first language. Moreover, none of the terms in the bias term list need to be used to train the speech recognition model.
データ処理ハードウェアおよび音声認識モデルは、ユーザ装置上、またはユーザ装置に通信するリモート計算装置上に存在してもよい。データ処理ハードウェアおよび音声認識モデルがリモート計算装置上に存在する場合、発話を符号化する音声データを受け取る工程は、ユーザ装置から、発話を符号化する音声データを受け取る工程を備えてもよい。 The data processing hardware and speech recognition model may reside on the user device or on a remote computing device that communicates with the user device. When the data processing hardware and the speech recognition model are present on the remote computer, the step of receiving the speech data encoding the utterance may include the step of receiving the speech data encoding the utterance from the user device.
本開示の１つまたは複数の実装の詳細は、添付の図面および以下の説明に記載されている。他の態様、特徴、および利点は、説明および図面、ならびに特許請求の範囲から明らかになるであろう。 Details of one or more implementations of the present disclosure are given in the accompanying drawings and the description below. Other aspects, features, and advantages will become apparent from the description and drawings, as well as the claims.
様々な図面における同様の参照記号は、同様の要素を示す。
本明細書の実装は、他の動作の中でも、外国語音素セットを自動音声認識ＡＳＲモデルの言語（例えば、アメリカ英語）の音素セットに写像（マッピング）して、音素レベルでバイアスをかける有限状態変換器（ＦＳＴ）において外国語のモデリングを可能にすることで、外国語を認識する文脈的（コンテクチュアル）自動音声認識（ＡＳＲ）モデルを強化することに向けられている。さらなる実装は、自動音声認識ＡＳＲモデルが、モデリング空間における自動音声認識ＡＳＲモデルの言語（例えば、アメリカ英語）のための語句（ワードピース）および音素を備えている語句（ワードピース）－音素モデルを組み込むことに向けられている。例として、文脈的自動音声認識ＡＳＲモデルは、語句－音素モデルおよび文脈的バイアス有限状態変換器ＦＳＴを使用して音声発話（スポークン発話）を復号（デコード）し、発話の転写を文脈的に１つまたは複数の外国語に偏らせるように構成される。たとえば、アメリカ英語を話す人が、クレテイユ（Ｃｒｅｔｅｉｌ。Ｃｒｅのｅの上にアクソンテグュが付されている）という単語がフランス語である、「クレテイユまでの道順」（Ｄｉｒｅｃｔｉｏｎｓ ｔｏ Ｃｒｅｔｅｉｌ）という発話をすると、文脈的自動音声認識ＡＳＲモデルは、アメリカ英語以外の言語の単語で学習されていないにもかかわらず、語句－音素モデルと文脈的バイアス有限状態変換器ＦＳＴを利用して、外国語であるクレテイユ（Ｃｒｅｔｅｉｌ）を認識するように転写を偏らせることができる。この例では、外国語のクレテイユ（Ｃｒｅｔｅｉｌ）は、現在の文脈に基づきバイアスをかけた単語リストに含まれる複数のフランス語のうちの１つである可能性がある。例えば、ユーザが現在フランスにいて車を運転している場合、現在の文脈（コンテキスト）は、フランスの都市名／地域名が関連していることを示している可能性があり、したがって、文脈的（コンテキストに基づく）自動音声認識ＡＳＲモデルは、これらのフランスの都市名／地域名に偏っている（バイアスしている）可能性がある。
Similar reference symbols in various drawings indicate similar elements.
The implementation of this specification is a finite state that maps a foreign language phoneme set to a phoneme set in the language of an automatic speech recognition ASR model (eg, American English) and biases it at the phoneme level, among other actions. It is aimed at enhancing the contextual automatic speech recognition (ASR) model that recognizes foreign languages by enabling modeling of foreign languages in the converter (FST). A further implementation is that the automatic speech recognition ASR model has words (wordpieces) and phonemes for the language (eg, American English) of the automatic speech recognition ASR model in the modeling space-phoneme model. It is aimed at incorporating. As an example, the contextual automatic speech recognition ASR model uses the phrase-phoneme model and the contextual bias finite state converter FST to decode speech speech (spoken speech) and contextually transcribing
図１を参照すると、いくつかの実装では、強化（エンハンスト）された自動音声認識ＡＳＲシステム１００は、外国語の単語（ワード）を認識するように強化されている。示された例では、自動音声認識ＡＳＲシステム１００は、ユーザ１１０のユーザ装置１０２上、および／または、ユーザ装置に通信するリモート計算装置（リモート計算装置）２０１（例えば、クラウド計算環境で実行される分散システムの１つまたは複数のサーバ）上に存在する。ユーザ装置１０２は、モバイル計算装置（例えば、スマートフォン）として描かれているが、ユーザ装置１０２は、限定されないが、タブレットデバイス、ラップトップ／デスクトップコンピュータ、ウェアラブルデバイス、デジタルアシスタントデバイス、スマートスピーカ／ディスプレイ、スマートアプライアンス、自動車インフォテイメントシステム、またはＩｏＴ（インターネットオブシングス）デバイスなどの任意のタイプの計算装置に対応してもよい。
Referring to FIG. 1, in some implementations, the enhanced automatic speech
ユーザ装置１０２は音声サブシステム１０３を備えており、音声サブシステム１０３は、ユーザ１１０によって話された発話１０６を受け取り（例えば、ユーザ装置１０２は、話された発話１０６を記録するための１つまたは複数のマイクロフォンを備えてもよい）、発話１０６を、自動音声認識ＡＳＲシステム１００によって処理可能なパラメータ化された入力音響フレーム１０４に関連する対応するデジタルフォーマットに変換するように構成されている。示されている例では、ユーザは「クレテイユまでの道順」（Ｄｉｒｅｃｔｉｏｎｓ ｔｏ Ｃｒｅｔｅｉｌ）というフレーズに対するそれぞれの発話１０６を話し、音声サブシステム１０３は発話１０６を、自動音声認識ＡＳＲシステム１００に入力するための対応する音響フレーム１０４に変換する。例えば、音響フレーム１０４は、短い、例えば２５ｍｓのウィンドウで計算され、数ミリ秒、例えば１０ミリ秒ごとにシフトされた、それぞれが８０次元のｌｏｇ－Ｍｅｌ特徴を備えている一連（シリーズ）のパラメータ化された入力音響フレームであってもよい。
The
その後、自動音声認識ＡＳＲシステム１００は、入力として、発話１０６に対応する音響フレーム１０４を受け取り、出力として、発話１０６に対応する転写（トランスクリプション。例えば、認識結果／認識仮説）１１６を生成／予測する。図示の例では、ユーザ装置１０２および／またはリモート計算装置２０１は、ユーザ装置１０２のユーザインタフェース１３６において、発話１０６の転写１１６の表現（レプレゼンテーション）をユーザ１１０に提示するように構成されたユーザインタフェース生成システム１０７も実行する。いくつかの例では、ユーザインタフェース１３６は、ユーザ装置１０２に通信しているスクリーン上に表示されてもよい。
After that, the automatic speech
いくつかの構成では、自動音声認識ＡＳＲシステム１００から出力された転写１１６は、例えば、ユーザ装置１０２またはリモート計算装置２０１上で実行される自然言語理解（ＮＬＵ）モジュールによって、ユーザコマンドを実行するべく処理される。さらに、または代替として、音声合成（テキストツースピーチ）システム（例えば、ユーザ装置１０２またはリモート計算装置２０１の任意の組み合わせ上で実行される）は、別のデバイスによる可聴出力のために、転写を合成音声に変換してもよい。例えば、元の発話１０６は、ユーザ１１０が友人に送信しているメッセージに対応していてもよく、その場合、転写１１６は、元の発話１０６で伝えられたメッセージを聞くべく、友人への可聴出力のために合成音声に変換される。
In some configurations, the
強化された自動音声認識ＡＳＲシステム１００は、バイアス構成要素１１５と、語句－音素モデル２００およびバイアス有限状態変換器ＦＳＴ３００を有する音声認識装置１５０と、学習構成要素１１４とを備えている。バイアス構成要素１１５は、バイアス有限状態変換器ＦＳＴ３００を生成するように構成され、学習構成要素１１４は、音素レベルで外国語を再スコアリングすることで文脈的バイアスを実行するように、語句－音素モデル２００およびバイアス有限状態変換器ＦＳＴ３００を学習するように構成される。明らかになるように、音声認識装置１５０は、学習された語句－音素モデル２００およびバイアス有限状態変換器ＦＳＴ３００を使用して、外国語の単語に向かってバイアスをかけることで、文脈的な音声認識を実行する。
The enhanced automatic speech
学習構成要素１１４は、単一の言語、例えば、アメリカ英語のテキストのコーパスを有する辞書（レキシコン、語彙集）１１７と、頻度チェッカ１１８と、モデル学習器１２０とを備えている。頻度（フリーケンシ）チェッカ１１８は、コーパスのテキストの中での単一言語の用語の相対的な頻度を決定するように構成され、モデル学習器１２０は、テキストコーパスの用語の語句と音素の両方に基づき語句－音素モデル２００を学習し、モデリング空間に語句と音素の両方を含めるように構成される。いくつかの例では、語句－音素モデル２００は、単一の言語のみ、例えば、アメリカ英語のみからの語句－音素セットを含む一方で、他の言語からの語句－音素セットを除外した学習データを用いて、モデル学習器１２０によってエンドツーエンドで学習される。モデル学習器１２０は、単語頻度ベースのサンプリング戦略を採用して、辞書１１７を用いて、稀な単語をターゲットシーケンスの音素にランダムにトークン化してもよい。段階（ステージ）Ａにおいて、学習構成要素１１４は、辞書１１７からのテキストを使用して、語句－音素モデル２００を学習する。
The learning component 114 comprises a single language, eg, a dictionary (lexicon, lexicon) 117 with a corpus of American English texts, a
いくつかの例では、辞書１１７は約５０万個の単語を含み、その頻度は音素シーケンスを使用するタイミングを決定するべく使用される。辞書１１７は、学習データからの単語とその頻度を含み、同音異義語（ホモフォン。例えば、「ｆｌｏｗｅｒ」（花）と「ｆｌｏｕｒ」（小麦粉））、同形異義語（ホモグラフ。例えば、動詞または形容詞としての「ｌｉｖｅ」（生きる、生の））、および発音変種（プロナンシエイションバリエント。例えば、「ｅｉｔｈｅｒ」（イーザーまたはアイザー））を除去してトリミングされる。このように、辞書１１７には、綴りから発音へまたはその逆の場合に、曖昧さがない項目のみが含まれている。 In some examples, the dictionary 117 contains about 500,000 words, the frequency of which is used to determine when to use the phoneme sequence. The dictionary 117 contains words from the learning data and their frequencies, as homographs (homophones, eg, "flower" (flowers) and "flow" (wheat flour)), homographs (homographs, eg, verbs or adjectives). "Live" (living, raw)), and pronunciation variants (pronunciation variants, such as "ether" (easer or iser)) are removed and trimmed. Thus, the dictionary 117 contains only unambiguous items from spelling to pronunciation and vice versa.
いくつかの実装では、モデル学習器（トレーナ）１２０は、学習入力発話を２５ｍｓのフレームに分割し、１０ｍｓのレート（速度）で窓を開けシフトする。各フレームで８０次元のｌｏｇ－Ｍｅｌ特徴が抽出され、現在のフレームと左隣の２つのフレームが連結されて２４０次元のｌｏｇ－Ｍｅｌ特徴が生成される。これらの特徴は、その後、３０ｍｓのレートでダウンサンプリングされる。 In some implementations, the model learner (trainer) 120 divides the learning input utterance into 25 ms frames, opens windows at a 10 ms rate, and shifts. An 80-dimensional log-Mel feature is extracted in each frame, and the current frame and the two frames to the left are concatenated to generate a 240-dimensional log-Mel feature. These features are then downsampled at a rate of 30 ms.
いくつかの実装では、語句－音素モデル２００は、シーケンスツーシーケンスモデルを備えている。いくつかの例では、語句－音素モデル２００は、ＲＮＮ－Ｔ（リカレントニューラルネットワーク－トランスデューサ）シーケンスツーシーケンスモデルアーキテクチャを備えている。他の例では、語句－音素モデル２００は、リッスン、アテンド、スペルのシーケンスツーシーケンスモデルアーキテクチャを備えている。
In some implementations, the phrase-
語句－音素モデル２００は、学習において少数の語句を選択的に音素に分解することができる点で、語句のみのモデルとは異なる。このモデルの出力は、記号セット（シンボルセット）が語句記号と音素記号との組合わせである、単一のソフトマックスである。単語の音素シーケンスを得るためには、発音辞書（レキシコン）が用いられる。音素は、希少な単語の認識に強みを発揮するので、これらの単語はより頻繁に音素として提示される。ターゲット文では、ｉ番目の単語が確率ｐ（ｉ）＝ｐ０・ｍｉｎ（Ｔ／（ｃ（ｉ）），１．０）でランダムに音素として提示される。ここでｐ０とＴは定数であり、ｃ(ｉ)は、学習コーパス全体での単語の出現回数を表す整数である。出現回数がＴ回以下の単語は、確率ｐ０で音素として提示される。Ｔ回よりも多く出現する単語については、頻度が高いほど音素として提示されないことになる。いくつかの例では、Ｔは１０に等しく、ｐ０は０．５に等しくなっているが、他の例では異なる値を選択することができる。なお、単語と音素のどちらを使用するかの決定は、勾配のイテレーションごとにランダムに行われるので、ある文は、異なるエポックで異なるターゲットシーケンスを持つ可能性がある。いくつかの実装では、音素は文脈に依存しない音素である。
The phrase-
図２を参照すると、語句－音素モデル２００は、インタラクティブアプリケーションに関連付けられたレイテンシ制約に準拠したエンドツーエンド（Ｅ２Ｅ）のＲＮＮ－Ｔモデル２００を備えていることができる。ＲＮＮ－Ｔモデル２００は、小さな計算フットプリントを提供し、従来の自動音声認識ＡＳＲアーキテクチャよりも少ないメモリ要件を利用するので、ＲＮＮ－Ｔモデルアーキテクチャは、ユーザ装置１０２上で完全に音声認識を実行するのに適している（例えば、リモートサーバとの通信は必要とされない）。ＲＮＮ－Ｔモデル２００は、符号化器ネットワーク（エンコーダネットワーク）２１０と、予測ネットワーク２２０と、結合ネットワーク（ジョイントネットワーク）２３０とを備えている。符号化器ネットワーク２１０は、従来の自動音声認識ＡＳＲシステムにおける音響モデル（ＡＭ）にほぼ類似しており、積層されたＬＳＴＭ（Ｌｏｎｇ Ｓｈｏｒｔ－Ｔｅｒｍ Ｍｅｍｏｒｙ）層のリカレントネットワークを備えている。例えば符号化器は、ｘｔ∈Ｒｄ（Ｒは白抜き文字）であるｄ次元特徴ベクトル（例えば、音響フレーム１０４（図１））のシーケンスｘ＝（ｘ１，ｘ２，・・・ ，ｘＴ）を読み込み、各時間ステップで高次の特徴表現を生成する。この高次の特徴表現は、ｈ１
ｅｎｃ，・・・，ｈＴ
ｅｎｃのように示される。
Referring to FIG. 2, the phrase-
同様に、予測ネットワーク２２０もＬＳＴＭネットワークであり、言語モデル（ＬＭ）のように、これまでに最終ソフトマックス層２４０が出力した非空白記号のシーケンスｙ０，・・・，ｙｕｉ－１を処理して、高密度の表現Ｐｕｉにする。最後に、ＲＮＮ－Ｔモデルのアーキテクチャでは、符号化器ネットワーク２１０および予測ネットワーク２２０によって生成された表現同士は、結合ネットワーク２３０によって結合される。結合ネットワーク２３０は、次の出力記号に対する分布である予測Ｐ（ｙｉ｜ｘ１，・・・，ｘｔｉ，ｙ０，・・・，ｙｕｉ－１）を行う。別の言い方をすると、結合ネットワーク２３０は、各出力ステップ（例えば、時間ステップ）において、可能性のある音声認識仮説に対する確率分布を生成する。ここで、「可能性のある音声認識仮説」（ポシブルスピーチレコグニションヒポセシス）は、指定された自然言語の記号／文字（キャラクタ）をそれぞれが表す出力ラベルの第１セットと、指定された自然言語の音素をそれぞれが表す出力ラベルの第２セットとに対応する。したがって、結合ネットワーク２３０は、所定の出力ラベルのセットのそれぞれの発生の可能性（ライクリフッドオブオカレンス）を示す一連の値を出力することができる。この値のセットは、ベクトルとすることができ、出力ラベルのセットに対する確率分布を示すことができる。いくつかのケースでは、出力ラベルは、第１セットでは書記素（ｇｒａｐｈｅｍｅｓ。例えば、個々の文字、および潜在的には句読点および他の記号）であり、第２セットでは音素であるが、出力ラベルのセットはそのように限定されない。結合ネットワーク２３０の出力分布は、異なる出力ラベル同士のそれぞれに対する事後確率値（ポステリアプロバビリティバリュー）を備えていることができる。したがって、異なる書記素または他の記号を表す１００個の異なる出力ラベルがある場合、結合ネットワーク２３０の出力ｙｉは、各出力ラベルに対して１つずつになるように、１００個の異なる確率値を備えていることができる。次に、確率分布は、転写１１６を決定するためのビーム探索プロセス（例えば、ソフトマックス層２４０による）において、正書法の候補要素（ｃａｎｄｉｄａｔｅ ｏｒｔｈｇｒａｐｈｉｃ ｅｌｅｍｅｎｔ）（例えば、書記素、語句、単語、音素）を選択し、スコアを割り当てるべく使用することができる。 Similarly, the prediction network 220 is also an LSTM network, and processes the non-blank symbol sequences y 0 , ..., Y ui-1 previously output by the final softmax layer 240, as in the language model (LM). Then, it becomes a high-density expression UI . Finally, in the RNN-T model architecture, the representations produced by the encoder network 210 and the predictive network 220 are coupled by the coupled network 230. The combined network 230 performs a prediction P (y i | x 1 , ..., x ti , y 0 , ..., y ui-1 ) which is a distribution for the next output symbol. In other words, the coupled network 230 produces a probability distribution for possible speech recognition hypotheses at each output step (eg, time step). Here, the "possible speech recognition hypothesis" (possible speech recognition hypothesis) is designated as the first set of output labels, each representing a specified natural language symbol / character. Corresponds to the second set of output labels, each representing a natural language phoneme. Thus, the combined network 230 can output a set of values indicating the likelihood of each occurrence of a given set of output labels (lyclifed of occurrence). This set of values can be a vector and can show the probability distribution for a set of output labels. In some cases, the output label is a grapheme in the first set, eg, individual characters, and potentially punctuation marks and other symbols, and in the second set it is a phoneme, but the output label. The set of is not so limited. The output distribution of the combined network 230 can have posterior probability values (posterior probability values) for each of the different output labels. Therefore, if there are 100 different output labels representing different graphemes or other symbols, then 100 different probability values so that the output y i of the combined network 230 is one for each output label. Can be equipped. The probability distribution is then a candidate element of orthography (eg, grapheme, phrase, word, phoneme) in the beam search process for determining transcription 116 (eg, by Softmax layer 240). Can be selected and used to assign a score.
ソフトマックス層２４０は、対応する出力ステップでモデル２００によって予測される次の出力記号として、分布内で最も高い確率を持つ出力ラベル／記号を選択するべく、任意の技術を採用することができる。このようにして、ＲＮＮ－Ｔモデル２００は条件付き独立性仮定を行わず、むしろ各記号の予測は、音響だけでなくこれまでに出力されたラベルのシーケンスにも条件付けられている。ＲＮＮ－Ｔモデル２００は、出力記号が将来の音響フレーム１０４から独立していると仮定しており、これによって、ＲＮＮ－Ｔモデルをストリーミング方式で採用することができる。
The softmax layer 240 can employ any technique to select the output label / symbol with the highest probability in the distribution as the next output symbol predicted by the
いくつかの例では、ＲＮＮ－Ｔモデル２００の符号化器ネットワーク２１０は、８個の２０４８次元ＬＳＴＭ層で構成され、それぞれの後に６４０次元の投影（プロジェクション）層が続く。モデルのレイテンシを低減するべく、符号化器の第２ＬＳＴＭ層の後に、低減（リダクション）係数が２の時間低減層を挿入してもよい。また、予測ネットワーク２２０は、２個の２０４８次元ＬＳＴＭ層を有していてもよく、それぞれの後に６４０次元の投影層が続いている。最後に、結合ネットワーク２３０は、６４０個の隠れユニットと、それの後に続く４０９６個のソフトマックス出力も有していてもよい。具体的には、出力ユニットは、４１個の文脈非依存音素を含み、残りは語句（ワードピース）である。
In some examples, the encoder network 210 of the RNN-
図１に戻って、自動音声認識ＡＳＲシステム１００のバイアス構成要素１１５は、バイアスされるべき外国語のバイアス用語リスト１０５からの用語を外国語音素にトークン化するように構成されたトークン化器１２１と、トークン化された用語の外国語音素を単一言語、例えば、アメリカ英語に関連する類似の音素に写像（マッピング）するように構成された音素写像器（マッパー）１２３とを備えている。音素写像器１２３は、人間が生成したソース言語からターゲット言語への音素ペアを備えている辞書によって表されてもよく、Ｘ－ＳＡＭＰＡ音素セットはすべての言語に使用される。注目すべきは、音素写像器１２３は、語句－音素モデル２００が、単一の言語、例えば、アメリカ英語に関連する音素のみを備えている場合に有用である。
Returning to FIG. 1, the bias component 115 of the automatic speech
例えば、ナビゲーションクエリ「クレテイユまでの道順」（ｄｉｒｅｃｔｉｏｎｓ ｔｏ Ｃｒｅｔｅｉｌ）の発話１０６と、フランス語の単語「クレテイユ」（Ｃｒｅｔｅｉｌ）がバイアス用語リスト１０５内にあるという仮定が与えられた場合、「クレテイユ」（Ｃｒｅｔｅｉｌ）は、まずトークン化器１２１によって「ｋ Ｒ ｅ ｔ Ｅ ｊ」としてフランス語の音素にトークン化され、次に音素写像器１２３によって「ｋ ｒ￥ Ｅ ｔ Ｅ ｊ」として英語の音素に写像されて、音素レベルのバイアス有限状態変換器ＦＳＴ３００の生成に使用される。語句－音素モデル２００が単一の言語、例えば、アメリカ英語からの音素のみをモデリングユニットとして備えているので、音素写像は使用される。
For example, given the
本開示は、どのような用語がバイアス用語リスト１０５に含まれるか、または用語がバイアス用語リスト１０５に含まれるようにどのように選択されるかに限定されない。バイアス用語リスト１０５は、関連する文脈（コンテキスト）に基づき、動的に更新されてもよい。例えば、文脈情報は、ユーザ装置１０２上でどのようなアプリケーションが開いていて使用中であるか、ユーザの連絡先リストからの連絡先名、ユーザ１１０のメディアライブラリ内のアーティスト名／アルバム名、ユーザ１１０の位置などを示してもよい。例えば、ユーザ１１０はアメリカ英語を話すことができ、ナビゲーション／地図アプリケーションがユーザ装置１０２上で開かれていることと、ユーザ１１０の場所がフランスであることとを示す文脈情報に基づき、バイアス用語リスト１０５は、フランスの都市名および／または地域名に関連する用語を備えていることができる。
The present disclosure is not limited to what terms are included in the
また、バイアス構成要素１１５は、音素レベルのバイアス有限状態変換器ＦＳＴ生成器１２５を備えており、音素レベルのバイアス有限状態変換器ＦＳＴ生成器１２５は、バイアス用語リスト１０５内の外国語（例えば、フランス語）用語のそれぞれを表す母語（例えば、アメリカ英語）の音素シーケンスに基づき、バイアス有限状態変換器ＦＳＴ３００を生成するように構成されている。いくつかの例では、バイアス有限状態変換器ＦＳＴ生成器１２５は、音素レベルで重みを割り当てるべく押す重み（ｗｅｉｇｈｔ ｐｕｓｈｉｎｇ）を使用し、過剰バイアスを避けるべく失敗アーク（ｆａｉｌｕｒｅ ａｒｃｓ）を追加する。いくつかの実装では、復号化において、すべてのバイアス語を使用して、各アーク（ａｒｃ）が同じ重みを持つ文脈的有限状態変換器ＦＳＴを構築する。これらの重みは、異なるモデルに対して独立して、調整することができる。
Further, the bias component 115 includes a phoneme level bias finite state
音声認識装置１５０は、バイアス構成要素１１５によって生成されたバイアス有限状態変換器ＦＳＴ３００を使用して、語句－音素モデル２００によって出力された音素を再スコアリングし、一方、復号グラフ４００は、バイアス有限状態変換器ＦＳＴ３００からの再スコアリングされた音素と、語句－音素モデル２００によって出力された語句とを消費して、転写１１６に含めるための語句を生成する。復号グラフ４００は、発話１０６に対する１つまたは複数の転写候補を決定するビーム探索復号処理に対応してもよい。
The
いくつかの例では、語句－音素モデル２００による復号中に、バイアス有限状態変換器ＦＳＴ３００は、語句－音素モデル２００によって出力された英語音素記号を消費し、外国語辞書および音素写像を使用して語句を生成してもよく、すなわち「ｋ ｒ￥ Ｅ ｔ Ｅ ｊ」→クレテイユ（Ｃｒｅｔｅｉｌ）である。復号グラフ４００によって出力された語句は、連結器（コンカチネータ）１３４によって、ユーザ装置１０２の他の構成要素に出力される転写１１６の単語（ワード）に連結され、ここでユーザ装置１０２の他の構成要素は、例えば、ユーザインタフェース生成システム１０７や、他の自然言語処理構成要素である。
In some examples, during decoding by the phrase-
図３は、音素レベルでの単語「クレテイユ」（Ｃｒｅｔｅｉｌ）に対する、例示的なバイアス有限状態変換器ＦＳＴ３００を示す。そして、このバイアス有限状態変換器ＦＳＴは、以下の式（１）を用いて、語句－音素モデルの音素出力をオンザフライで再スコアリングするべく使用される。 FIG. 3 shows an exemplary bias finite state transducer FST300 for the word "Créteil" at the phoneme level. Then, this bias finite state converter FST is used to re-score the phoneme output of the phrase-phoneme model on the fly using the following equation (1).
式（１）において、ｘは音響観測値であり、ｙはサブ単語（サブワード）ユニットシーケンスであり、ＰはＥ２Ｅモデルからの確率推定であり、Ｐｃはバイアス再スコアリング確率である。λは、再スコアリングにおける文脈言語モデルＬＭの重みを制御する。 In equation (1), x is an acoustic observation, y is a subword unit sequence, P is a probability estimate from the E2E model, and P c is a bias rescoring probability. λ controls the weight of the contextual language model LM in rescoring.
図１に戻ると、語句－音素モデル２００は、モデリングユニットとして語句だけでなく音素も組み込み、バイアス用語リスト１０５内の外国語用語に向けた文脈上のバイアスのためにバイアス有限状態変換器ＦＳＴ３００を使用する。すべての音素モデルとは対照的に、音素と語句の両方をモデル化する語句－音素モデル２００は、通常の単語（レギュラーワード）を認識する際の回帰を緩和する。
Returning to FIG. 1, the phrase-
語句－音素モデル２００が段階Ａで学習（トレーニング）された後、段階Ｂで、ユーザ１１０は、発話１０６「クレテイユへの道順」（ｄｉｒｅｃｔｉｎｏｓ ｔｏ Ｃｒｅｔｅｉｌ）をユーザ装置１０２に話す。段階Ｃにおいて、音声サブシステム１０３は、例えばマイクロフォンを使用して、発話を受け取り、受け取った発話を、一連のパラメータ化された入力音響フレーム１０４に変換する。例えば、パラメータ化された入力音響フレーム１０４はそれぞれ、８０次元のｌｏｇ－Ｍｅｌ特徴を備えてもよく、ここで８０次元のｌｏｇ－Ｍｅｌ特徴は、短い、例えば２５ｍｓのウィンドウで計算されるとともに、数ミリ秒ごと、例えば１０ミリ秒ごとにシフトされる。
After the phrase-
段階Ｄにおいて、自動音声認識ＡＳＲシステム１００は、上述したようにパラメータ化された入力音響フレームを処理し、文脈的にバイアスされた転写１１６、すなわちテキスト「クレテイユ（Ｃｒｅｔｅｉｌ）への道順」を出力する。段階Ｅにおいて、ユーザインタフェース生成システム１０７は、転写の表現を備えているグラフィカルユーザインタフェース１３６のためのコンピュータコードを生成し、段階Ｆにおいて、ユーザインタフェース１３６に表示するべく、そのコンピュータコードをモバイル装置（１０２）に送信する。
In step D, the automatic speech
自動音声認識ＡＳＲシステム１００によって実行される追加の詳細は、段階Ｄの期間内に発生する可能性がある。例えば、段階Ｄ′の期間内に、バイアス構成要素１１５は、用語「クレテイユ」（Ｃｒｅｔｅｉｌ）を備えているバイアス用語リスト１０５の受け取りに基づき、バイアス有限状態変換器ＦＳＴ３００を生成する。段階Ｄ′′において、音声認識装置１５０の学習された語句－音素モデル２００は、ユーザ１１０の発話１０６に基づき、語句と、対応する音素シーケンスとの両方に対する音声認識スコアを生成し、音素に対する音声認識スコアは、バイアス有限状態変換器ＦＳＴ３００によって再スコアリングおよび再写像され、語句と、再スコアリング／再写像された音素とは、段階Ｄ′′において、転写１１６で出力するための語句を生成するべく、復号グラフ４００によって消費される。復号グラフ４００および連結器１３４は、文脈的にバイアスされた転写１１６を生成し、出力用の転写を、例えば、ユーザ装置１０２のＧＵＩ１３６に表示するべくユーザインタフェース生成システム１０７に提供する。注目すべきは、バイアス用語リスト１０５内の用語のいずれかに対応する音素シーケンスをバイアス有限状態変換器ＦＳＴ３００が再スコアリングした後に、復号グラフ４００は実行されることである。このように、バイアス用語リスト１０５内の外国語に対応する低い音声認識スコアを有する語句は、早々には剪定（ｐｒｕｎｅ）されない。
Additional details performed by the automatic speech
テスト中、語句－音素モデル２００およびバイアス有限状態変換器ＦＳＴ３００を採用して認識結果をバイアス用語リスト１０５内の用語に向けて文脈的にバイアスする音声認識装置１５０は、書記素のみのバイアスモデルと語句のみのバイアスモデルとの両方よりも顕著に優れたＷＥＲ率で、外国語単語の認識に成功することが示された。また、語句－音素モデル２００は、モデルのスケーラビリティの問題なく、他の外国語に直接適用してバイアスをかけることができるという利点がある。
During the test, the
図４は、音声認識装置１５０が音声認識結果を文脈的にバイアスするべく実行する、例示的な復号グラフ４００を示す。具体的には、例示的な復号グラフ４００は、英語のクロスリンガル発音「ｋ ｒ￥ Ｅ Ｓ」を有する単語「クレイシュ」（ｃｒｅｃｈｅ。ｃｒｅのｅの上にアクソングラーヴが付されている。英語では「デイケア」（ｄａｙｃａｒｅ））と、発音「ｋ ｒ￥ Ｅ ｔ Ｅ ｊ」を有する単語「クレテイユ」（Ｃｒｅｔｅｉｌ。フランスの都市）とに対する復号を描いている。なお、わかりやすくするべく、「０」という状態の語句はほとんど省略している。
FIG. 4 shows an
復号グラフ４００は、語句－音素モデル２００から出力された音素と語句の両方を入力として受け取るように構成されている。音声復号化処理は、復号グラフ（デコーディンググラフ）４００を検索して、出力として単語を生成する。図示の例では、復号有限状態変換器ＦＳＴは、状態０を中心とした語句ループを有するが、発音有限状態変換器ＦＳＴ、すなわち状態１～１４を有し、それら状態は音素を入力とし、対応する語句を出力とする接頭辞（ｐｒｅｆｉｘ）ツリーを備えている。発音有限状態変換器ＦＳＴは、すべてのバイアス用語について、バイアス時に使用された発音と同じ発音を用いて構築される。常に語句である最終出力記号は、（例えば、図１の連結器１３４によって）単語（ワード）に連結される。
The
図４の復号グラフ４００は、全体的な復号戦略に２つの追加の改善をもたらす。第１に、復号グラフ４００の性質を考慮すると、同じコストで同じ入力を消費するが、同じ出力を持たないいくつかの仮説が存在する可能性がある。例えば、状態７で終了する仮説は、状態９で終了する仮説と同じコストを持つことになる。このため、すべてが等価な多くの仮説によって、ビームが埋め尽くされてしまうという問題が生じる。本明細書に記載されている強化された自動音声認識ＡＳＲ技術は、このように、状態９で終わる仮説のみを保持することで、ビームを刈り取る（ｐｒｕｎｅする）。
The
第２改善点は、結合（マージ）された経路（ｐａｔｈ）に関する。学習と復号との性質を考慮すると、与えられた単語は、直接語句で出力されるか、または、音素から語句に変換される。同等の仮説が追跡され、それらの確率を加算することで再結合され、最も可能性の高い仮説に合計確率を割り当て、他のものをビームから削除する。 The second improvement concerns the combined path. Given the nature of learning and decoding, a given word is either output directly as a phrase or converted from a phoneme to a phrase. Equivalent hypotheses are tracked and recombinated by adding those probabilities, assigning the total probabilities to the most probable hypotheses and removing others from the beam.
語句－音素モデル２００のバイアスの結果を、語句のみのモデルと、書記素のみのモデルとに対して比較するテストが行われた。後者の２つのモデルは語句－音素モデル２００と同じ構造を有しており、違いは、書記素モデルが出力として７６個の書記素を有している一方で、語句モデルが４０９６個の語句（ワードピース）を有することである。この違いによって、書記素モデルと語句モデルのパラメータは、それぞれ約１１７Ｍ個と１２０Ｍ個になる。なお、この２つのモデルの出力記号は英語であり、全英語データを用いて学習されている。これらの２つのモデルでは、フランス語のバイアス単語（ワード）の英語音訳版を使用して、書記素レベルまたは語句レベルのみでバイアスが行われる。
A test was performed to compare the results of the phrase-
一般的に、テストでは、３つのモデルはバイアスをかけなくても同じように動作することが示された。これは、地名がフランス語であり、それらが学習では見られたことがないためであり、すなわち、ほぼ１００％の単語ＯＯＶ率である。さらに、すべてのモデルは、バイアスをかけることで大幅に性能が向上する。バイアスをかけない場合と比較して、ＷＥＲの減少が顕著である。 In general, tests have shown that the three models behave similarly without bias. This is because the place names are French and they have never been seen in learning, that is, the word OOV rate is almost 100%. In addition, all models are significantly improved in performance by being biased. The decrease in WER is remarkable as compared with the case where no bias is applied.
異なるバイアス戦略を比較すると、語句－音素モデル２００が最も優れた性能を示し、書記素モデルおよび語句モデルの両方よりも有意に良好に動作した。語句－音素モデルの優れた性能は、ＯＯＶの単語に対する音素のロバスト性に起因する。語句－音素モデル２００は、モデリングユニットとして語句と音素の両方を備えているので、音素有限状態変換器ＦＳＴに加えて語句有限状態変換器ＦＳＴを構築することで、音素ベースのバイアスに加えて語句バイアスを実行することができる。この語句単位の有限状態変換器ＦＳＴを追加することで、ＷＥＲがさらに減少することが実証されており、語句単位のバイアスと音素単位のバイアスとは相互に補完し合う関係にあることがわかる。音素と語句のバイアスに使用する重みは、同じでもよいし、異なっていてもよい。観察によると、長いユニットをマッチングする際のスパース性の問題から、語句単位の方が、書記素（ｇｒａｐｈｅｍｅ）単位よりも性能が高い場合がある。
Comparing the different bias strategies, the phrase-
テストの結果、バイアスは外国の地名を認識するのに役立つことがわかった。例えば、バイアスをかけると、正しいフランス語の単語（ｗｏｒｄ）が生成され、逆にバイアスをかけないと、音韻的には似ているが間違った英語の単語が生成される。誤りは、フランス語の音韻的に類似した単語が原因であることが多い。 Testing has shown that bias helps recognize foreign place names. For example, biasing produces the correct French word (word), and conversely, unbiasing produces phonologically similar but incorrect English words. Mistakes are often caused by phonologically similar words in French.
バイアスなしのシナリオでの回帰がないことをより確実にするべく、通常の英語の発話の復号（デコーディング）で３つのモデルを比較した。復号では、バイアスフレーズの空リストを使用することで、バイアスメカニズムをオフにした。テストの結果、語句モデルは、書記素（ｇｒａｐｈｅｍｅ）モデルよりも優れた性能を示すことがわかった。語句－音素モデルは、書記素モデルよりもやや良好な結果となったが、これは学習時に語句の頻度が高かったことに起因していると考えられる。語句モデルと比較して、語句－音素モデルは非常にわずかに劣化している。これは、モデリングに電話を導入したことによる。回帰性を向上させるための潜在的なアプローチとしては、語句ベースの再スコアリングと同様に、再スコアリングに音素の英語外部言語モデルを組み込むことが考えられる。しかし、全音素（ａｌｌ－ｐｈｏｎｅｍｅ）モデルに比べて、回帰が著しく小さくなる。 To make it more certain that there is no regression in the bias-free scenario, we compared the three models with normal English utterance decoding. Decryption turned off the bias mechanism by using an empty list of bias phrases. As a result of the test, it was found that the phrase model showed better performance than the grapheme model. The phrase-phoneme model gave slightly better results than the grapheme model, probably due to the high frequency of phrases during learning. Compared to the phrase model, the phrase-phoneme model is very slightly degraded. This is due to the introduction of the telephone in modeling. A potential approach to improving regression is to incorporate an English external language model of phonemes into rescoring, as well as phrase-based rescoring. However, the regression is significantly smaller than that of the all-phoneme model.
図５は、バイアス用語リスト内の外国語用語に向かって転写を文脈的にバイアスする方法の動作の例示的な配置のフローチャートである。動作５０２において、方法５００は、第１言語のネイティブスピーカ（１１０）によって話される発話１０６を符号化（エンコーディング）する音声データを受け取る工程を備えている。発話１０６は、第１言語とは異なる第２言語の１つまたは複数の外国語を備えてもよい。動作５０４において、方法５００は、第２言語の１つまたは複数の用語を備えているバイアス用語リスト１０５を受け取る工程を備えている。
FIG. 5 is a flow chart of an exemplary arrangement of the behavior of a method of contextually biasing transcription towards a foreign language term in a bias term list. In
動作５０６において、方法５００は、音声認識モデル２００を使用して、音声データから導出された音響特徴（１０４）を処理して、第１言語における語句と、対応する音素シーケンスとの両方に対する音声認識スコアを生成する工程も備えている。動作５０８において、方法５００は、バイアス用語リスト内の１つまたは複数の用語に基づき、音素シーケンスに対する音声認識スコアを再スコアリングする工程も備えている。動作５０６において、方法５００は、語句に対する音声認識スコアと、音素シーケンスに対する再スコアリングされた音声認識スコアとを用いて、復号グラフ（デコーディンググラフ）４００を実行して、発話１０６に対する転写（トランスクリプション）１１６を生成する工程を備えている。
In
ソフトウェアアプリケーション（すなわち、ソフトウェアリソース）は、計算装置にタスクを実行させるコンピュータソフトウェアを指すことがある。いくつかの例では、ソフトウェアアプリケーションは、「アプリケーション」、「アプリ」、または「プログラム」と呼ばれることがある。アプリケーションの例としては、システム診断アプリケーション、システム管理アプリケーション、システムメンテナンスアプリケーション、ワープロアプリケーション、表計算アプリケーション、メッセージングアプリケーション、メディアストリーミングアプリケーション、ソーシャルネットワーキングアプリケーション、ゲームアプリケーションなどがあるが、これらに限定されない。 Software applications (ie, software resources) may refer to computer software that causes a computer to perform a task. In some examples, software applications may be referred to as "applications," "apps," or "programs." Examples of applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, table computing applications, messaging applications, media streaming applications, social networking applications, gaming applications, and the like.
非一過性メモリは、計算装置が使用するためのプログラム（例えば、命令のシーケンス）またはデータ（例えば、プログラムの状態情報）を一時的または永久的に保存するべく使用される物理デバイスであってもよい。非一時的メモリは、揮発性および／または不揮発性のアドレス可能な半導体メモリであってもよい。不揮発性メモリの例としては、フラッシュメモリ、リードオンリーメモリ（ＲＯＭ）／プログラマブルリードオンリーメモリ（ＰＲＯＭ）／消去可能プログラマブルリードオンリーメモリ（ＥＰＲＯＭ）／電子的消去可能プログラマブルリードオンリーメモリ（ＥＥＰＲＯＭ）（例えば、ブートプログラムなどのファームウェアに典型的に使用される）などがあるが、これらに限定されない。揮発性メモリの例としては、ランダムアクセスメモリ（ＲＡＭ）、ダイナミックランダムアクセスメモリ（ＤＲＡＭ）、スタティックランダムアクセスメモリ（ＳＲＡＭ）、フェイズチェンジメモリ（ＰＣＭ）のほか、ディスクやテープなどが挙げられるが、これらに限定されるものではない。 Non-transient memory is a physical device used to temporarily or permanently store a program (eg, a sequence of instructions) or data (eg, program state information) for use by a computing device. May be good. The non-temporary memory may be a volatile and / or non-volatile addressable semiconductor memory. Examples of non-volatile memory include flash memory, read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (eg, erasable programmable read-only memory (EEPROM)). (Typically used for firmware such as boot programs)), but is not limited to these. Examples of volatile memory include random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM), as well as disks and tapes. Not limited to.
図６は、本書で説明したシステムおよび方法を実施するべく使用することができる例示的な計算装置６００の概略図である。計算装置６００は、ラップトップ、デスクトップ、ワークステーション、パーソナルデジタルアシスタント、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピュータなど、様々な形態のデジタルコンピュータを表すことを意図している。ここに示されている構成要素、それらの接続および関係、ならびにそれらの機能は、例示的なものであることを意図しており、本書に記載および／または請求されている発明の実施を制限することを意図していない。
FIG. 6 is a schematic diagram of an exemplary
計算装置６００は、プロセッサ６１０と、メモリ６２０と、記憶装置（ストレージデバイス）６３０と、メモリ６２０および高速拡張ポート６５０に接続する高速インタフェース／コントローラ６４０と、および低速バス６７０および記憶装置（ストレージデバイス）６３０に接続する低速インタフェース／コントローラ６６０とを備えている。構成要素６１０、６２０、６３０、６４０、６５０、６６０のそれぞれは、様々なバスを用いて相互に接続されており、共通のマザーボードに搭載されていてもよいし、適宜他の態様で搭載されていてもよい。プロセッサ６１０は、高速インタフェース６４０に結合されたディスプレイ６８０などの外部入出力デバイスにグラフィカルユーザインタフェース（ＧＵＩ）のためのグラフィカル情報を表示するべく、メモリ６２０または記憶装置６３０に格納された命令を備えている、計算装置６００内で実行するための命令を処理することができる。他の実装では、複数のプロセッサおよび／または複数のバスが、複数のメモリおよびメモリの種類とともに、適宜使用されてもよい。また、複数の計算装置６００が接続され、各デバイスが必要な動作の一部を提供してもよい（例えば、サーババンク、ブレードサーバ群、またはマルチプロセッサシステムとして）。
The
メモリ６２０は、計算装置６００内の情報を非一時的に格納する。メモリ６２０は、コンピュータ可読媒体、揮発性メモリユニット（複数可）、または不揮発性メモリユニット（複数可）であってもよい。不揮発性メモリ６２０は、計算装置６００による使用のために、プログラム（例えば、命令のシーケンス）またはデータ（例えば、プログラム状態情報）を一時的または永久的に格納するべく使用される物理デバイスであってもよい。不揮発性メモリの例には、フラッシュメモリおよびリードオンリーメモリ（ＲＯＭ）／プログラマブルリードオンリーメモリ（ＰＲＯＭ）／消去可能プログラマブルリードオンリーメモリ（ＥＰＲＯＭ）／電子的消去可能プログラマブルリードオンリーメモリ（ＥＥＰＲＯＭ）（例えば、ブートプログラムなどのファームウェアに典型的に使用される）が含まれるが、これらに限定されない。揮発性メモリの例としては、ランダムアクセスメモリ（ＲＡＭ）、ダイナミックランダムアクセスメモリ（ＤＲＡＭ）、スタティックランダムアクセスメモリ（ＳＲＡＭ）、フェイズチェンジメモリ（ＰＣＭ）のほか、ディスクやテープなどが挙げられるが、これらに限定されるものではない。
The
記憶装置６３０は、計算装置６００に大容量記憶を提供することができる。いくつかの実施態様において、記憶装置６３０は、コンピュータ可読媒体である。様々な異なる実装において、記憶装置（ストレージデバイス）６３０は、フロッピー（登録商標）ディスクデバイス、ハードディスクデバイス、光ディスクデバイス、またはテープデバイス、フラッシュメモリまたは他の類似のソリッドステートメモリデバイス、またはストレージエリアネットワークまたは他の構成のデバイスを備えている、デバイスのアレイであってもよい。追加の実装では、コンピュータプログラム製品が、情報キャリアに有形的に具現化される。コンピュータプログラム製品は、実行されると、上述したような１つまたは複数の方法を実行する命令を備えている。情報キャリア（情報担体）は、メモリ６２０、記憶装置６３０、またはプロセッサ６１０上のメモリなどの、コンピュータまたは機械可読媒体である。
The
高速コントローラ６４０は、計算装置６００のための帯域幅集中型の動作を管理し、低速コントローラ６６０は、より低い帯域幅集中型の動作を管理する。このような職務の割り当ては、例示的なものに過ぎない。いくつかの実装では、高速コントローラ６４０は、メモリ６２０と、ディスプレイ６８０（例えば、グラフィックプロセッサまたはアクセラレータを介して）と、および、様々な拡張カード（図示せず）を受け入れ得る高速拡張ポート６５０とに結合される。いくつかの実装では、低速コントローラ６６０は、記憶装置６３０および低速拡張ポート６９０に結合される。様々な通信ポート（例えば、ＵＳＢ、Ｂｌｕｅｔｏｏｔｈ（登録商標）、イーサネット（登録商標）、ワイヤレスイーサネット（登録商標））を備えてもよい低速拡張ポート６９０は、キーボード、ポインティングデバイス、スキャナなどの１つまたは複数の入出力デバイスに、またはスイッチやルータなどのネットワークデバイスに、例えばネットワークアダプタを介して結合されてもよい。
The
計算装置６００は、図に示すように、いくつかの異なる形態で実装されてもよい。例えば、計算装置は、標準的なサーバ６００ａまたはそのようなサーバ６００ａのグループにおける複数倍として、ラップトップコンピュータ６００ｂとして、またはラックサーバシステム６００ｃの一部として、実装されてもよい。
The
本明細書に記載されたシステムおよび技術の様々な実装は、デジタル電子および／または光学回路、集積回路、特別に設計されたＡＳＩＣ（特定用途向け集積回路）、コンピュータハードウェア、ファームウェア、ソフトウェア、および／またはそれらの組み合わせで実現することができる。これらの様々な実装は、プログラム可能なシステム上で実行可能および／または解釈可能な１つまたは複数のコンピュータプログラムでの実装を備えていることができ、ここでプログラム可能なシステムは、データおよび命令を記憶装置から受け取り、データおよび命令を記憶装置に送信するように記憶装置に結合された、特殊目的または汎用の少なくとも１つのプログラム可能なプロセッサと、少なくとも１つの入力装置と、および少なくとも１つの出力装置とを備えている。 Various implementations of the systems and techniques described herein include digital electronic and / or optical circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and. / Or can be realized by a combination thereof. These various implementations can be implemented in one or more computer programs that can be run and / or interpreted on a programmable system, where the programmable system is data and instructions. At least one programmable processor for special purpose or general purpose, at least one input device, and at least one output coupled to the storage device to receive from the storage device and send data and instructions to the storage device. It is equipped with a device.
これらのコンピュータプログラム（プログラム、ソフトウェア、ソフトウェアアプリケーション、またはコードとも呼ばれる）は、プログラマブルプロセッサのための機械命令を含み、高レベルの手続き型および／またはオブジェクト指向のプログラミング言語、および／またはアセンブリ／機械言語で実装することができる。本明細書において、「機械可読媒体」および「コンピュータ可読媒体」という用語は、機械可読信号として機械命令を受け取る機械可読媒体を含んでいる、機械命令および／またはデータをプログラマブルプロセッサに提供するべく使用される任意のコンピュータプログラム製品、非一時的なコンピュータ可読媒体、装置（アパレイタス）および／またはデバイス（例えば、磁気ディスク、光ディスク、メモリ、プログラマブルロジックデバイス（ＰＬＤ））を意味する。「機械可読信号」とは、機械命令および／またはデータをプログラマブルプロセッサに提供するべく使用されるあらゆる信号を指す。 These computer programs (also called programs, software, software applications, or codes) include machine instructions for programmable processors, high-level procedural and / or object-oriented programming languages, and / or assembly / machine languages. Can be implemented with. As used herein, the terms "machine readable medium" and "computer readable medium" are used to provide machine instructions and / or data to a programmable processor, including machine readable media that receive machine instructions as machine readable signals. It means any computer program product, non-temporary computer-readable medium, device (apparatus) and / or device (eg, magnetic disk, optical disk, memory, programmable logic device (PLD)). "Machine-readable signal" refers to any signal used to provide machine instructions and / or data to a programmable processor.
本明細書に記載されている処理および論理フローは、データ処理ハードウェアとも呼ばれる１つまたは複数のプログラマブルプロセッサが、１つまたは複数のコンピュータプログラムを実行して、入力データを操作して出力を生成することで機能を実行することができる。また、ＦＰＧＡ（Ｆｉｅｌｄ Ｐｒｏｇｒａｍｍａｂｌｅ Ｇａｔｅ Ａｒｒａｙ）やＡＳＩＣ（Ａｐｐｌｉｃａｔｉｏｎ Ｓｐｅｃｉｆｉｃ Ｉｎｔｅｇｒａｔｅｄ Ｃｉｒｃｕｉｔ）などの特殊な論理回路によっても処理や論理フローを実行することができる。コンピュータプログラムの実行に適したプロセッサには、一例として、汎用および特殊目的のマイクロプロセッサ、および任意の種類のデジタルコンピュータの任意の１つまたは複数のプロセッサが含まれる。一般に、プロセッサは、読み取り専用メモリまたはランダムアクセスメモリ、あるいはその両方から命令とデータを受け取る。コンピュータの本質的な要素は、命令を実行するためのプロセッサと、命令やデータを格納するための１つまたは複数のメモリデバイスである。一般に、コンピュータは、データを格納するための１つまたは複数の大容量記憶装置、例えば、磁気ディスク、光磁気ディスク、または光ディスクを備えているか、またはデータを受け取るか、またはデータを転送するか、もしくは両方であるように動作可能に結合される。しかし、コンピュータはそのようなデバイスを持っている必要はない。コンピュータプログラムの命令やデータを格納するのに適したコンピュータ可読媒体には、あらゆる形態の不揮発性メモリ、媒体、およびメモリデバイスが含まれ、例として、半導体メモリデバイス、例えばＥＰＲＯＭ、ＥＥＰＲＯＭ、およびフラッシュメモリデバイス、磁気ディスク、例えば内蔵ハードディスクまたはリムーバブルディスク、光磁気ディスク、およびＣＤ－ＲＯＭおよびＤＶＤ－ＲＯＭディスクが挙げられる。プロセッサとメモリは、特別な目的の論理回路によって補完されるか、またはそれに組み込まれることができる。 The processing and logical flows described herein are such that one or more programmable processors, also referred to as data processing hardware, run one or more computer programs to manipulate the input data and produce output. You can execute the function by doing. Further, the processing and the logical flow can be executed by a special logic circuit such as FPGA (Field Programmable Gate Array) or ASIC (Application Specific Integrated Circuit). Suitable processors for running computer programs include, for example, general purpose and special purpose microprocessors, and any one or more processors of any type of digital computer. In general, processors receive instructions and data from read-only memory and / or random access memory. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. In general, a computer is equipped with one or more mass storage devices for storing data, such as magnetic disks, magneto-optical disks, or optical disks, or receives or transfers data. Or they are operably combined to be both. However, the computer does not have to have such a device. Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, such as semiconductor memory devices such as EPROM, EEPROM, and flash memory. Devices, magnetic disks, such as internal hard disks or removable disks, magneto-optical disks, and CD-ROMs and DVD-ROM disks. Processors and memory can be complemented or incorporated into logic circuits of special purpose.
ユーザとの対話（相互作用）を提供するべく、本開示の１つまたは複数の態様は、ユーザに情報を表示するためのディスプレイデバイス、例えばＣＲＴ（ｃａｔｈｏｄｅ ｒａｙ ｔｕｂｅ）、ＬＣＤ（ｌｉｑｕｉｄ ｃｒｙｓｔａｌ ｄｉｓｐｌａｙ）モニタ、またはタッチスクリーンと、任意でキーボードおよびポインティングデバイス、例えばマウスまたはトラックボールを有し、それによってユーザがコンピュータに入力を提供することができるコンピュータ上で実装することができる。同様にユーザに相互作用を提供できる多の種類の装置が使用でき、例えば、ユーザに提供されるフィードバックは、視覚的なフィードバック、聴覚的なフィードバック、触覚的なフィードバックなど、あらゆる形態の感覚的なフィードバックであり、ユーザからの入力は、音響的な入力、音声的な入力、触覚的な入力など、あらゆる形態で受け取ることができる。さらに、コンピュータは、ユーザが使用するデバイスにドキュメントを送信したり、デバイスからドキュメントを受け取ったりすることで、ユーザと対話することができる。例えば、ウェブブラウザから受け取った要求に応答して、ユーザのクライアントデバイス上のウェブブラウザにウェブページを送信することで、ユーザと対話することができる。 In order to provide interaction with the user, one or more aspects of the present disclosure are display devices for displaying information to the user, such as a CRT (catagraphary tabe), LCD (liquid crystal display) monitor. , Or a touch screen and optionally a keyboard and pointing device, such as a mouse or trackball, which can be implemented on a computer where the user can provide input to the computer. Many types of devices can be used that can similarly provide interaction to the user, for example, the feedback provided to the user can be any form of sensory feedback, including visual feedback, auditory feedback, and tactile feedback. It is feedback, and the input from the user can be received in any form such as acoustic input, audio input, and tactile input. In addition, the computer can interact with the user by sending and receiving documents from the device used by the user. For example, you can interact with a user by sending a web page to the web browser on the user's client device in response to a request received from the web browser.
多数の実施例を説明してきた。それにもかかわらず、本開示の精神および範囲から逸脱することなく、様々な変更を行うことができることが理解されるであろう。したがって、他の実施態様は、以下の請求項の範囲内にある。 Many examples have been described. Nevertheless, it will be appreciated that various changes can be made without departing from the spirit and scope of this disclosure. Therefore, other embodiments are within the scope of the following claims.
Claims (20)
前記データ処理ハードウェア（６１０）において、前記第１言語とは異なる第２言語の１つまたは複数の用語を備えているバイアス用語リスト（１０５）を受け取る工程と、
前記データ処理ハードウェア（６１０）において、音声認識モデル（２００）を用いて、前記音声データから得られる音響特徴（１０４）を処理して、前記第１言語の語句と、対応する音素シーケンスとの両方に対する音声認識スコアを生成する工程と、
前記データ処理ハードウェア（６１０）によって、前記バイアス用語リスト（１０５）内の前記１つまたは複数の用語に基づき、前記音素シーケンスに対する前記音声認識スコアを再スコアリングする工程と、
前記データ処理ハードウェア（６１０）によって、前記語句に対する前記音声認識スコアと、前記音素シーケンスに対する再スコアリングされた音声認識スコアとを用いて、復号グラフ（４００）を実行して、前記発話（１０６）に対する転写（１１６）を生成する工程と、
を備えている方法（５００）。 In the data processing hardware (610), a process of receiving voice data encoding an utterance (106) spoken by a native speaker (110) of a first language,
In the data processing hardware (610), a step of receiving a bias term list (105) having one or more terms in a second language different from the first language.
In the data processing hardware (610), the speech recognition model (200) is used to process the acoustic feature (104) obtained from the speech data to obtain the phrase of the first language and the corresponding phoneme sequence. The process of generating speech recognition scores for both,
A step of rescoring the speech recognition score for the phoneme sequence by the data processing hardware (610) based on the one or more terms in the bias term list (105).
The data processing hardware (610) performs the decoding graph (400) with the speech recognition score for the phrase and the rescored speech recognition score for the phoneme sequence to perform the utterance (106). ) To generate the transcription (116) and
The method (500).
請求項１に記載の方法（５００）。 The step of rescoring the speech recognition score for the phoneme sequence comprises the step of rescoring the speech recognition score for the phoneme sequence using a bias finite state converter (FST).
The method according to claim 1 (500).
前記データ処理ハードウェア（６１０）によって、前記バイアス用語リスト（１０５）の各用語を、前記第２言語の対応する音素シーケンスにトークン化する工程と、
前記データ処理ハードウェア（６１０）によって、前記第２言語における各対応する音素シーケンスを、前記第１言語における対応する音素シーケンスに写像する工程と、
前記データ処理ハードウェア（６１０）によって、前記第１言語における各対応する音素シーケンスに基づき、前記バイアス有限状態変換器（３００）を生成する工程と、
を備えている、請求項２に記載の方法（５００）。 The method further
A step of tokenizing each term in the bias term list (105) into a corresponding phoneme sequence in the second language by the data processing hardware (610).
A step of mapping each corresponding phoneme sequence in the second language to the corresponding phoneme sequence in the first language by the data processing hardware (610).
A step of generating the bias finite state converter (300) based on each corresponding phoneme sequence in the first language by the data processing hardware (610).
2. The method according to claim 2 (500).
請求項１～３のいずれか一項に記載の方法（５００）。 The speech recognition model (200) comprises an end-to-end phrase-phoneme model (200).
The method (500) according to any one of claims 1 to 3.
請求項４に記載の方法（５００）。 The end-to-end phrase-phoneme model (200) comprises a recurrent neural network-transducer (RNN-T).
The method according to claim 4 (500).
請求項１～５のいずれか一項に記載の方法（５００）。 During the execution of the decoding graph (400), the decoding graph (400) may favor any one or more of the terms in the bias term list (105) by the transcription (116). Bias,
The method (500) according to any one of claims 1 to 5.
請求項１～６のいずれか一項に記載の方法（５００）。 The speech recognition model (200) is learned by learning utterances of only the first language.
The method (500) according to any one of claims 1 to 6.
請求項１～７のいずれか一項に記載の方法（５００）。 None of the terms in the bias term list (105) were used to train the speech recognition model (200).
The method (500) according to any one of claims 1 to 7.
請求項１～８のいずれか一項に記載の方法（５００）。 The data processing hardware (610) and the speech recognition model (200) are present on the user device (102).
The method (500) according to any one of claims 1 to 8.
前記発話（１０６）を符号化する前記音声データを受け取る工程は、前記リモート計算装置（２０１）に通信しているユーザ装置（１０２）から、前記発話（１０６）を符号化する前記音声データを受け取る工程を備えている、
請求項１～９のいずれか一項に記載の方法（５００）。 The data processing hardware (610) and the speech recognition model (200) exist on the remote computing unit (201).
The step of receiving the voice data encoding the utterance (106) receives the voice data encoding the utterance (106) from the user device (102) communicating with the remote computing device (201). Equipped with a process,
The method (500) according to any one of claims 1 to 9.
前記データ処理ハードウェア（６１０）に通信するメモリハードウェア（６２０）であって、前記メモリハードウェア（６２０）は、前記データ処理ハードウェア（６１０）上で実行されると前記データ処理ハードウェア（６１０）に、以下を備えている動作を実行させる命令を格納する、前記メモリハードウェア（６２０）と、
を備えているシステム（１００）であって、前記動作は、
第１言語のネイティブスピーカ（１１０）によって話される発話（１０６）を符号化する音声データを受け取る工程と、
前記第１言語とは異なる第２言語による１つまたは複数の用語を備えているバイアス用語リスト（１０５）を受け取る工程と、
音声認識モデル（２００）を用いて、前記音声データから得られる音響特徴（１０４）を処理して、前記第１言語の語句と、対応する音素シーケンスとの両方に対する音声認識スコアを生成する工程と、
前記バイアス用語リスト（１０５）の前記１つまたは複数の用語に基づき、前記音素シーケンスに対する前記音声認識スコアを再スコアリングする工程と、
前記語句に対する前記音声認識スコアと、前記音素シーケンスに対する再スコアリングされた音声認識スコアとを用いて、復号グラフ（４００）を実行して、前記発話（１０６）に対する転写（１１６）を生成する工程と、
を備えている、システム（１００）。 Data processing hardware (610) and
The memory hardware (620) that communicates with the data processing hardware (610), and the memory hardware (620) is the data processing hardware (620) when executed on the data processing hardware (610). The memory hardware (620) and the memory hardware (620), which stores an instruction to execute an operation including the following, in 610).
The system (100) is equipped with the above-mentioned operation.
The process of receiving voice data encoding the utterance (106) spoken by the native speaker (110) of the first language,
A process of receiving a bias term list (105) comprising one or more terms in a second language different from the first language.
A step of processing the acoustic feature (104) obtained from the speech data using the speech recognition model (200) to generate a speech recognition score for both the phrase of the first language and the corresponding phoneme sequence. ,
A step of rescoring the speech recognition score for the phoneme sequence based on the one or more terms in the bias term list (105).
A step of executing a decoding graph (400) using the speech recognition score for the phrase and the rescored speech recognition score for the phoneme sequence to generate a transcription (116) for the utterance (106). When,
The system (100).
請求項１１に記載のシステム（１００）。 The step of rescoring the speech recognition score for the phoneme sequence comprises the step of rescoring the speech recognition score for the phoneme sequence using a bias finite state converter (FST).
The system (100) according to claim 11.
前記バイアス用語リスト（１０５）の各用語を、前記第２言語の対応する音素シーケンスにトークン化する工程と、
前記第２言語における各対応する音素シーケンスを、前記第１言語における対応する音素シーケンスに写像する工程と、
前記第１言語における各対応する音素シーケンスに基づき、前記バイアス有限状態変換器（３００）を生成する工程と、
を備えている、請求項１２に記載のシステム（１００）。 The above operation further
A step of tokenizing each term in the bias term list (105) into a corresponding phoneme sequence in the second language.
A step of mapping each corresponding phoneme sequence in the second language to the corresponding phoneme sequence in the first language.
A step of generating the bias finite state converter (300) based on each corresponding phoneme sequence in the first language, and
The system (100) according to claim 12.
請求項１１～１３のいずれか一項に記載のシステム（１００）。 The speech recognition model (200) comprises an end-to-end phrase-phoneme model (200).
The system (100) according to any one of claims 11 to 13.
請求項１４に記載のシステム（１００）。 The end-to-end phrase-phoneme model (200) comprises a recurrent neural network-transducer (RNN-T).
The system (100) according to claim 14.
請求項１１～１５のいずれか一項に記載のシステム（１００）。 During the execution of the decoding graph (400), the decoding graph (400) may favor any one or more of the terms in the bias term list (105) by the transcription (116). Bias,
The system (100) according to any one of claims 11 to 15.
請求項１１～１６のいずれか一項に記載のシステム（１００）。 The speech recognition model (200) is learned by learning utterances of only the first language.
The system (100) according to any one of claims 11 to 16.
請求項１１～１７のいずれか一項に記載のシステム（１００）。 None of the terms in the bias term list (105) were used to train the speech recognition model (200).
The system (100) according to any one of claims 11 to 17.
請求項１１～１８のいずれか一項に記載のシステム（１００）。 The data processing hardware (610) and the speech recognition model (200) are present on the user device (102).
The system (100) according to any one of claims 11 to 18.
前記発話（１０６）を符号化する前記音声データを受け取る工程は、前記リモート計算装置（２０１）に通信しているユーザ装置（１０２）から、前記発話（１０６）を符号化する前記音声データを受け取る工程を備えている、
請求項１１～１９のいずれか一項に記載のシステム（１００）。 The data processing hardware (610) and the speech recognition model (200) exist on the remote computing unit (201).
The step of receiving the voice data encoding the utterance (106) receives the voice data encoding the utterance (106) from the user device (102) communicating with the remote computing device (201). Equipped with a process,
The system (100) according to any one of claims 11 to 19.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962842571P | 2019-05-03 | 2019-05-03 | |
US62/842,571 | 2019-05-03 | ||
PCT/US2020/030321 WO2020226948A1 (en) | 2019-05-03 | 2020-04-28 | Phoneme-based contextualization for cross-lingual speech recognition in end-to-end models |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2022523883A JP2022523883A (en) | 2022-04-26 |
JP7092953B2 true JP7092953B2 (en) | 2022-06-28 |
Family
ID=70922127
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2021564950A Active JP7092953B2 (en) | 2019-05-03 | 2020-04-28 | Phoneme-based context analysis for multilingual speech recognition with an end-to-end model |
Country Status (6)
Country | Link |
---|---|
US (2) | US11270687B2 (en) |
EP (1) | EP3948849A1 (en) |
JP (1) | JP7092953B2 (en) |
KR (2) | KR20220038514A (en) |
CN (2) | CN117935785A (en) |
WO (1) | WO2020226948A1 (en) |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11501764B2 (en) * | 2019-05-10 | 2022-11-15 | Spotify Ab | Apparatus for media entity pronunciation using deep learning |
EP4073789B1 (en) * | 2020-01-17 | 2023-11-08 | Google LLC | Alphanumeric sequence biasing for automatic speech recognition |
KR102637025B1 (en) * | 2021-03-26 | 2024-02-16 | 구글 엘엘씨 | Multilingual rescoring models for automatic speech recognition |
CN117396879A (en) * | 2021-06-04 | 2024-01-12 | 谷歌有限责任公司 | System and method for generating region-specific phonetic spelling variants |
CN113643718A (en) * | 2021-08-16 | 2021-11-12 | 北京房江湖科技有限公司 | Audio data processing method and device |
CN113936647B (en) * | 2021-12-17 | 2022-04-01 | 中国科学院自动化研究所 | Training method of voice recognition model, voice recognition method and system |
CN114078469B (en) * | 2022-01-19 | 2022-05-10 | 广州小鹏汽车科技有限公司 | Voice recognition method, device, terminal and storage medium |
US20240127801A1 (en) * | 2022-10-13 | 2024-04-18 | International Business Machines Corporation | Domain adaptive speech recognition using artificial intelligence |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2019507362A (en) | 2016-02-05 | 2019-03-14 | グーグル エルエルシー | Speech re-recognition using an external data source |
Family Cites Families (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040078191A1 (en) * | 2002-10-22 | 2004-04-22 | Nokia Corporation | Scalable neural network-based language identification from written text |
US7415411B2 (en) * | 2004-03-04 | 2008-08-19 | Telefonaktiebolaget L M Ericsson (Publ) | Method and apparatus for generating acoustic models for speaker independent speech recognition of foreign words uttered by non-native speakers |
TWI233589B (en) * | 2004-03-05 | 2005-06-01 | Ind Tech Res Inst | Method for text-to-pronunciation conversion capable of increasing the accuracy by re-scoring graphemes likely to be tagged erroneously |
EP1693828B1 (en) * | 2005-02-21 | 2008-01-23 | Harman Becker Automotive Systems GmbH | Multilingual speech recognition |
EP1975923B1 (en) * | 2007-03-28 | 2016-04-27 | Nuance Communications, Inc. | Multilingual non-native speech recognition |
US7991615B2 (en) * | 2007-12-07 | 2011-08-02 | Microsoft Corporation | Grapheme-to-phoneme conversion using acoustic data |
CN101727901B (en) * | 2009-12-10 | 2011-11-09 | 清华大学 | Method for recognizing Chinese-English bilingual voice of embedded system |
US8886533B2 (en) * | 2011-10-25 | 2014-11-11 | At&T Intellectual Property I, L.P. | System and method for combining frame and segment level processing, via temporal pooling, for phonetic classification |
WO2014005142A2 (en) * | 2012-06-29 | 2014-01-03 | Rosetta Stone Ltd | Systems and methods for modeling l1-specific phonological errors in computer-assisted pronunciation training system |
US9159317B2 (en) * | 2013-06-14 | 2015-10-13 | Mitsubishi Electric Research Laboratories, Inc. | System and method for recognizing speech |
KR102084646B1 (en) * | 2013-07-04 | 2020-04-14 | 삼성전자주식회사 | Device for recognizing voice and method for recognizing voice |
US9502032B2 (en) * | 2014-10-08 | 2016-11-22 | Google Inc. | Dynamically biasing language models |
US9966066B1 (en) * | 2016-02-03 | 2018-05-08 | Nvoq Incorporated | System and methods for combining finite state transducer based speech recognizers |
JP6727607B2 (en) * | 2016-06-09 | 2020-07-22 | 国立研究開発法人情報通信研究機構 | Speech recognition device and computer program |
US10235991B2 (en) * | 2016-08-09 | 2019-03-19 | Apptek, Inc. | Hybrid phoneme, diphone, morpheme, and word-level deep neural networks |
US10074369B2 (en) * | 2016-09-01 | 2018-09-11 | Amazon Technologies, Inc. | Voice-based communications |
US10311876B2 (en) * | 2017-02-14 | 2019-06-04 | Google Llc | Server side hotwording |
US11093110B1 (en) * | 2017-07-17 | 2021-08-17 | Amazon Technologies, Inc. | Messaging feedback mechanism |
CN112262430A (en) * | 2018-08-23 | 2021-01-22 | 谷歌有限责任公司 | Automatically determining language for speech recognition of a spoken utterance received via an automated assistant interface |
US10861446B2 (en) * | 2018-12-10 | 2020-12-08 | Amazon Technologies, Inc. | Generating input alternatives |
US11069353B1 (en) * | 2019-05-06 | 2021-07-20 | Amazon Technologies, Inc. | Multilingual wakeword detection |
-
2020
- 2020-04-28 WO PCT/US2020/030321 patent/WO2020226948A1/en unknown
- 2020-04-28 KR KR1020227008217A patent/KR20220038514A/en active Application Filing
- 2020-04-28 JP JP2021564950A patent/JP7092953B2/en active Active
- 2020-04-28 EP EP20729879.5A patent/EP3948849A1/en active Pending
- 2020-04-28 US US16/861,190 patent/US11270687B2/en active Active
- 2020-04-28 CN CN202311813671.5A patent/CN117935785A/en active Pending
- 2020-04-28 CN CN202080028777.2A patent/CN113692616B/en active Active
- 2020-04-28 KR KR1020217035448A patent/KR102375115B1/en active IP Right Grant
-
2022
- 2022-02-16 US US17/651,315 patent/US11942076B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2019507362A (en) | 2016-02-05 | 2019-03-14 | グーグル エルエルシー | Speech re-recognition using an external data source |
Non-Patent Citations (1)
Title |
---|
PATEL, Ami et al.，"CROSS-LINGUAL PHONEME MAPPING FOR LANGUAGE ROBUST CONTEXTUAL SPEECH RECOGNITION"，Proc. of the 2018 IEEE ICASSP，2018年04月15日，pp.5924-5928 |
Also Published As
Publication number | Publication date |
---|---|
JP2022523883A (en) | 2022-04-26 |
KR102375115B1 (en) | 2022-03-17 |
CN117935785A (en) | 2024-04-26 |
WO2020226948A1 (en) | 2020-11-12 |
KR20220038514A (en) | 2022-03-28 |
EP3948849A1 (en) | 2022-02-09 |
US11942076B2 (en) | 2024-03-26 |
KR20210138776A (en) | 2021-11-19 |
CN113692616A (en) | 2021-11-23 |
US20200349923A1 (en) | 2020-11-05 |
CN113692616B (en) | 2024-01-05 |
US11270687B2 (en) | 2022-03-08 |
US20220172706A1 (en) | 2022-06-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7092953B2 (en) | Phoneme-based context analysis for multilingual speech recognition with an end-to-end model | |
JP7280382B2 (en) | End-to-end automatic speech recognition of digit strings | |
KR102390940B1 (en) | Context biasing for speech recognition | |
JP2023545988A (en) | Transformer transducer: One model that combines streaming and non-streaming speech recognition | |
EP4078572B1 (en) | Proper noun recognition in end-to-end speech recognition | |
KR20230086737A (en) | Cascade Encoders for Simplified Streaming and Non-Streaming Speech Recognition | |
WO2023059969A1 (en) | Joint unsupervised and supervised training for multilingual automatic speech recognition | |
CN117099157A (en) | Multitasking learning for end-to-end automatic speech recognition confidence and erasure estimation | |
US20210225362A1 (en) | Attention-Based Joint Acoustic and Text On-Device End-to-End Model | |
US20220310067A1 (en) | Lookup-Table Recurrent Language Model | |
JP2024512606A (en) | Reducing streaming ASR model delay using self-alignment | |
US11893349B2 (en) | Systems and methods for generating locale-specific phonetic spelling variations | |
US20220310061A1 (en) | Regularizing Word Segmentation | |
Theis | Learning to detect named entities in bilingual code-mixed open speech corpora | |
WO2024086265A1 (en) | Context-aware end-to-end asr fusion of context, acoustic and text representations | |
CN117378005A (en) | Multilingual re-scoring model for automatic speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20220124 |
|
A80 | Written request to apply exceptions to lack of novelty of invention |
Free format text: JAPANESE INTERMEDIATE CODE: A801Effective date: 20220124Free format text: JAPANESE INTERMEDIATE CODE: A80Effective date: 20220124 |
|
A871 | Explanation of circumstances concerning accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A871Effective date: 20220124 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20220607 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20220616 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7092953Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |