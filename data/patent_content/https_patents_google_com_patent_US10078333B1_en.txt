US10078333B1 - Efficient mapping of robot environment - Google Patents
Efficient mapping of robot environment Download PDFInfo
- Publication number
- US10078333B1 US10078333B1 US15/131,014 US201615131014A US10078333B1 US 10078333 B1 US10078333 B1 US 10078333B1 US 201615131014 A US201615131014 A US 201615131014A US 10078333 B1 US10078333 B1 US 10078333B1
- Authority
- US
- United States
- Prior art keywords
- voxels
- occupied
- voxel
- robot
- processors
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05D—SYSTEMS FOR CONTROLLING OR REGULATING NON-ELECTRIC VARIABLES
- G05D1/00—Control of position, course or altitude of land, water, air, or space vehicles, e.g. automatic pilot
- G05D1/02—Control of position or course in two dimensions
- G05D1/021—Control of position or course in two dimensions specially adapted to land vehicles
- G05D1/0268—Control of position or course in two dimensions specially adapted to land vehicles using internal positioning means
- G05D1/0274—Control of position or course in two dimensions specially adapted to land vehicles using internal positioning means using mapping information stored in a memory device
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/10—Terrestrial scenes
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05D—SYSTEMS FOR CONTROLLING OR REGULATING NON-ELECTRIC VARIABLES
- G05D1/00—Control of position, course or altitude of land, water, air, or space vehicles, e.g. automatic pilot
- G05D1/02—Control of position or course in two dimensions
- G05D1/021—Control of position or course in two dimensions specially adapted to land vehicles
- G05D1/0212—Control of position or course in two dimensions specially adapted to land vehicles with means for defining a desired trajectory
- G05D1/0214—Control of position or course in two dimensions specially adapted to land vehicles with means for defining a desired trajectory in accordance with safety or protection criteria, e.g. avoiding hazardous areas
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B25—HAND TOOLS; PORTABLE POWER-DRIVEN TOOLS; MANIPULATORS
- B25J—MANIPULATORS; CHAMBERS PROVIDED WITH MANIPULATION DEVICES
- B25J9/00—Programme-controlled manipulators
- B25J9/16—Programme controls
- B25J9/1694—Programme controls characterised by use of sensors other than normal servo-feedback from position, speed or acceleration sensors, perception control, multi-sensor controlled systems, sensor fusion
- B25J9/1697—Vision controlled systems
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05D—SYSTEMS FOR CONTROLLING OR REGULATING NON-ELECTRIC VARIABLES
- G05D1/00—Control of position, course or altitude of land, water, air, or space vehicles, e.g. automatic pilot
- G05D1/02—Control of position or course in two dimensions
- G05D1/021—Control of position or course in two dimensions specially adapted to land vehicles
- G05D1/0231—Control of position or course in two dimensions specially adapted to land vehicles using optical position detecting means
- G05D1/0246—Control of position or course in two dimensions specially adapted to land vehicles using optical position detecting means using a video camera in combination with image processing means
- G05D1/0251—Control of position or course in two dimensions specially adapted to land vehicles using optical position detecting means using a video camera in combination with image processing means extracting 3D information from a plurality of images taken from different locations, e.g. stereo vision
-
- G06K9/00664—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/64—Three-dimensional objects
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S901/00—Robots
- Y10S901/01—Mobile robot
Definitions
- a variety of techniques may be performed to determine positions and/or poses of robots. Some robots perform techniques such as simultaneous localization and mapping (“SLAM”). In other instances, techniques such as visual odometry and/or wheel odometry may be employed. Some robots may use an inertial measurement unit (“IMU”) to determine their positions. To perform robot navigation, some robots and/or robot control systems construct and/or maintain a complete three-dimensional (“3D”) model of an environment in which the robot operates. A robot may acquire data from a 3D laser scanner or other 3D vision sensor (e.g., stereographic camera) viewing a portion of the robot's environment and map such data to the complete 3D model.
- 3D three-dimensional
- Some 3D models are formed as so-called “voxel-based” 3D environments in which a 3D grid of voxels are allocated.
- Data points sensed by one or more 3D vision sensors also referred to as a “point cloud”
- 3D vision sensors also referred to as a “point cloud”
- allocating memory for every voxel of the 3D grid, regardless of whether any data points sensed by the 3D sensor occupy those voxels is inefficient both in terms of memory consumption and localization.
- standard ray-tracing techniques for determining whether dynamic objects remain in previously-detected locations may be computationally costly.
- the present disclosure is generally directed to methods, apparatus, and computer-readable media (transitory and non-transitory) for computationally-efficient perception and utilization of data points sensed by a 3D sensor of a robot to map a robot environment.
- a voxel-based 3D model of a robotic environment may be organized logically into columns, with the “floor” being organized into “tiles,” and each tile being associated with a column.
- Data points sensed by a 3D vision sensor in the environment may be projected onto voxels of the model.
- the voxels may be grouped into the columns that contain them, and in many instances may be indexed within the columns by elevation.
- memory is only allocated (e.g., from the heap) for voxels that are “occupied” by at least a threshold number of data points sensed by the 3D vision sensor, and only those occupied voxels are added to columns.
- Other voxels may be deemed “unoccupied,” and may not be allocated memory or added to columns. Consequently, the columns may form sparse data structures that include only occupied voxels.
- This gives rise to various technical advantages, such as reduced memory consumption compared to existing techniques in which memory is allocated for every voxel of the 3D grid, regardless of whether any data points actually occupy the voxel.
- Utilizing sparse features in accordance with various implementations described herein may also facilitate faster localization by robots, particularly compared to localization using comprehensive voxel-based models.
- voxels may be organized into columns using various data structures.
- a linked data structure such as a linked list or binary tree may be generated for each column.
- Each node of the linked data structure may represent (e.g., include data associated with) an occupied voxel.
- each node may include statistics about data points contained within the voxel, which may be analyzed for purposes of surface identification, object detection, etc.
- nodes may be sorted within the linked data structure by elevation.
- Organizing occupied voxels into columnar linked data structures gives rise to various technical advantages. For example, searching for particular occupied voxels becomes particularly efficient, especially where binary trees are employed as the linked data structure.
- a robot searching for a path through the environment may query the columnar linked data structures for occupied voxels within a particular range of elevations, such as voxels occupying space at a height of the robot and lower. The identified voxels may then projected down onto the tiles of the surface over which the robot travels. This enables the robot to efficiently compute and traverse a path through the environment.
- Another technical advantage is that identifying objects that are potentially manipulable by the robot, such as objects on a table that the robot could pick up, becomes relatively inexpensive computationally. For example, if a large number of columns include voxels at the same elevation, and there are no voxels immediately above those voxels, those voxels likely contain data points that define a horizontal surface. If a smaller subset of the same columns include outlier voxels at a slightly higher elevation, those outlier voxels likely contain an object that rests on the surface that may be manipulated by the robot.
- a computer implemented method includes the steps of: receiving from a three-dimensional vision sensor of a robot, a group of data points sensed by the three-dimensional vision sensor; identifying one or more voxels of a three-dimensional voxel model that are occupied by the group of data points; identifying, for each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel; and indexing voxels of the three-dimensional voxel model.
- the indexing may include indexing occupied voxels contained in each column by elevation without indexing one or more unoccupied voxels by elevation.
- the columns of the three-dimensional voxel model do not include unoccupied voxels.
- the indexing includes indexing each column exclusively by one or more elevations of one or more occupied voxels contained in the column.
- the indexing may include generating, for each column of the three-dimensional voxel model that contains one or more occupied voxels, a linked data structure with one or more nodes representing the one or more occupied voxels contained in the column.
- the linked data structure may be a linked list.
- the linked data structure may be a binary tree that may or may not be searchable by voxel elevation.
- the nodes may be linked in order of elevation.
- the method may include searching linked data structures associated with each column to identify voxels within a target elevation range, and operating, by the one or more processors, one or more portions of the robot to move along one or more paths that avoid voxels identified from the searching.
- data representing each voxel may include statistics about one or more data points contained in the voxel.
- the method may include analyzing the statistics to identify a surface formed by the one or more data points contained in the voxel.
- the method may further include identifying one or more of the occupied voxels that satisfy a criterion; projecting the one or more occupied voxels that satisfy the criterion onto a two-dimensional plane representing a surface navigable by the robot; and calculating a path across the surface for the robot to navigate, wherein the path avoids obstacles represented by the projected one or more occupied voxels.
- the criterion may include an elevation range that corresponds to at least a height of the robot.
- the method may include allocating a greater amount of memory to store data representing each occupied voxel than is allocated to store data indicative of any unoccupied voxel of the three-dimensional voxel model. In other implementations, the method may include allocating memory from a heap to each occupied voxel. In some implementations, unoccupied voxels may not be allocated from the heap.
- the method may further include classifying, as manipulable object voxels, one or more occupied voxels containing data points deemed to be associated with a manipulable object; obtaining, from the three-dimensional vision sensor, one or more up-to-date depth values associated with data points within the one or more manipulable object voxels; and determining that the manipulable object has moved based at least in part on the one or more up-to-date depth values.
- implementations may include a non-transitory computer readable storage medium storing instructions executable by a processor to perform a method such as one or more of the methods described above.
- implementations may include a control system including memory and one or more processors operable to execute instructions, stored in the memory, to implement one or more modules or engines that, alone or collectively, perform a method such as one or more of the methods described above.
- FIG. 1 schematically depicts an example environment in which disclosed techniques may be employed, in accordance with various implementations.
- FIG. 2 depicts schematically a simple example of a voxel-based 3D environment, in accordance with various implementations.
- FIG. 3 depicts an example method for mapping a robot environment, in accordance with various implementations.
- FIG. 4 depicts an example of a two-dimensional grid of “tiles” that may be utilized for robot navigation, in accordance with various implementations.
- FIG. 5 depicts an example of how a real world environment may be mapped to a sparse, voxel-based 3D model using techniques described herein.
- FIG. 6 schematically depicts an example architecture of a computer system.
- FIG. 1 is a schematic diagram of an example environment in which selected aspects of the present disclosure may be practiced in accordance with various implementations.
- a robot 100 may be in communication with a control system 150 .
- Robot 100 may take various forms, including but not limited to a telepresence robot (e.g., which may be as simple as a wheeled vehicle equipped with a display and a camera), a robot arm, a humanoid, an animal, an insect, an aquatic creature, a wheeled device, a submersible vehicle, a unmanned aerial vehicle (“UAV”), and so forth.
- robot 100 may include logic 102 .
- Logic 102 may take various forms, such as a real time controller, one or more processors, one or more field-programmable gate arrays (“FPGA”), one or more application-specific integrated circuits (“ASIC”), and so forth. In some implementations, logic 102 may be operably coupled with memory 103 .
- Memory 103 may take various forms, such as random access memory (“RAM”), dynamic RAM (“DRAM”), read-only memory (“ROM”), Magnetoresistive RAM (“MRAM”), resistive RAM (“RRAM”), NAND flash memory, and so forth.
- logic 102 may be operably coupled with one or more operational components 104 1-n , one or more end effectors 106 , and/or one or more sensors 108 1-m , e.g., via one or more buses 110 .
- operational components 104 of a robot may refer to actuators, motors (e.g., servo motors), joints, shafts, gear trains, pumps (e.g., air or liquid), pistons, drives, or other components that may create and/or undergo propulsion, rotation, and/or motion.
- Some operational components such as many joints may be independently controllable, although this is not required.
- the more operational components robot 100 has, the more degrees of freedom of movement it may have.
- end effector 106 may refer to a variety of tools that may be operated by robot 100 in order to accomplish various tasks.
- some robots may be equipped with an end effector 106 that takes the form of a claw with two opposing “fingers” or “digits.”
- Such as claw is one type of “gripper” known as an “impactive” gripper.
- Other types of grippers may include but are not limited to “ingressive” (e.g., physically penetrating an object using pins, needles, etc.), “astrictive” (e.g., using suction or vacuum to pick up an object), or “contigutive” (e.g., using surface tension, freezing or adhesive to pick up object).
- end effectors may include but are not limited to drills, brushes, force-torque sensors, cutting tools, deburring tools, welding torches, containers, trays, and so forth.
- end effector 106 may be removable, and various types of modular end effectors may be installed onto robot 100 , depending on the circumstances.
- Some robots, such as some telepresence robots, may not be equipped with end effectors. Instead, some telepresence robots may include displays to render visual representations of the users controlling the telepresence robots, as well as speakers and/or microphones that facilitate the telepresence robot “acting” like the user.
- Sensors 108 may take various forms, including but not limited to 3D laser scanners or other 3D vision sensors (e.g., stereographic cameras used to perform stereo visual odometry) configured to provide depth measurements, two-dimensional cameras, light sensors (e.g., passive infrared), force sensors, pressure sensors, pressure wave sensors (e.g., microphones), proximity sensors (also referred to as “distance sensors”), depth sensors, torque sensors, bar code readers, radio frequency identification (“RFID”) readers, radars, range finders, accelerometers, gyroscopes, compasses, position coordinate sensors (e.g., global positioning system, or “GPS”), speedometers, edge detectors, and so forth.
- 3D laser scanners or other 3D vision sensors e.g., stereographic cameras used to perform stereo visual odometry
- 3D vision sensors e.g., stereographic cameras used to perform stereo visual odometry
- 3D vision sensors e.g., stereographic cameras used to perform stereo visual
- sensors 108 1-m are depicted as being integral with robot 100 , this is not meant to be limiting. In some implementations, sensors 108 may be located external to, but may be in direct or indirect communication with, robot 100 , e.g., as standalone units or as part of control system 150 .
- Control system 150 may include one or computing systems connected by one or more networks (not depicted) that control operation of robot 100 to various degrees. An example of such a computing system is depicted schematically in FIG. 6 .
- control system 150 may be operated by a user (not depicted) to exert a relatively high level of control over robot 100 , e.g., in real time in response to signals received by a user interface engine 162 and/or one or more readings from one or more sensors 108 . In other implementations, control system 150 exerts less direct control over robot 100 .
- control system 150 may provide robot 100 with a high level task such as “go to location, identify person, follow person.”
- Logic 102 on robot 100 may convert such high level tasks into robot action, e.g., by translating one or more high level tasks into a plurality of motion primitives executable by robot 100 .
- control system 150 may include a display 140 (e.g., CRT, LCD, touchscreen, etc.) on which a graphical user interface 160 operable to remotely control robot 100 may be rendered.
- control system 150 includes a 3D environment model engine 152 , an object attribute engine 156 , and the aforementioned user interface engine 162 .
- 3D environment model engine 152 may be configured to maintain, e.g., in index 154 , one or more 3D models of one or more 3D environments in which robot 100 operates.
- the models stored in index 154 may be so-called “voxel” based 3D models that include a 3D grid of voxels to represent the environment.
- a voxel-based 3D model of an environment may be organized within index 154 by columns.
- voxels may be indexed within each column by elevation, and in some instances the columns may be sparse data structures as described herein.
- Object attribute engine 156 may be configured to store, e.g., in index 158 , records of various objects that may be acted upon by robot 100 .
- object attribute engine 156 may store in index 158 one or more records of voxels classified as “manipulable object” voxels.
- Manipulable object voxels may include occupied voxels that contain data points deemed to be associated with a manipulable object.
- a “manipulable object” may be any object that can potentially be manipulated by robot. “Manipulation” of an object may refer to picking up the object, moving the object, altering one or more physical attributes of the object, altering an orientation of the object, and so forth.
- logic 102 of robot 100 and/or one or more components of control system 150 may determine whether one or more manipulable objects have been moved, rearranged, or otherwise reoriented based at least in part on records in index 158 .
- robot 100 and control system 150 are depicted separately in FIG. 1 , this is not meant to be limiting.
- one or more aspects (e.g., modules, engines, etc.) depicted in FIG. 1 as implemented on one of robot 100 or control system 150 may be implemented on the other, may be distributed across both, and/or may be distributed across one or both in combination with other components not depicted in FIG. 1 .
- robot 100 and control system 150 may communicate over one or more wired or wireless networks (not depicted) or using other wireless technology, such as radio, Bluetooth, infrared, etc.
- control system 150 may be implemented entirely or in part using logic 102 of robot 100 .
- Robot 100 may be configured to make a variety of decisions based on data stored in indices 154 and 158 .
- Data about the environment stored in index 154 may be used by logic 102 to calculate a path through the environment that avoids known obstacles and/or hazards.
- Logic 102 may determine based on data stored in index 158 that an object is on top of the table (at least as far as robot 100 is aware). Using that information, robot 100 may take appropriate action, such as by moving the table in a manner that avoids knocking the object off, by removing the object first before moving the table, or by refusing to move the table and raising an alert to one or more users about the object.
- robot 100 may access data stored in indices 154 and 158 in various ways.
- data stored in index 154 and/or index 158 may be synchronized to mirrored locations in memory 103 of robot 100 periodically (e.g., hourly, daily, every few minutes), e.g., as a batch download.
- data stored in index 154 and/or 158 may be made available to logic 102 of robot 100 on demand.
- robot 100 may maintain (e.g., in memory 103 ) its own indices storing records of the environmental model and any obstacles or hazards.
- FIG. 2 schematically depicts an example voxel-based 3D model 200 of an environment.
- the model 200 consists of a simple cubic structure divided into twenty seven voxels.
- the modeled environment would not necessarily (or even likely) be cubic, and that far more than twenty seven voxels would realistically be used to represent the environment.
- Schematically projected below the cubic structure is a three by three planar grid 202 of flat “tiles,” labeled ⁇ - ⁇ , each corresponding to a column of the model 200 .
- An example column is depicted schematically rising up from the tile labeled “ ⁇ .”
- occupied voxels in each column may be represented as a node in a linked data structure, such as a list of linked list pointers.
- a greater amount of memory may be allocated to store data representing each occupied voxel than is allocated to store data indicative of any unoccupied voxel of the three-dimensional voxel model.
- each of the front three columns/tiles ⁇ - ⁇ may have a linked list allocated in memory.
- the other six columns ⁇ - ⁇ , which contain no occupied voxels may be unpopulated (or from a memory standpoint, unallocated).
- Each of columns/tiles ⁇ and ⁇ may be represented by, for instance, a linked list of three occupied voxels.
- the nodes of the linked list may be linked in order of elevation.
- the linked list may include a single node to represent the single occupied voxel.
- sufficient memory may be pre-allocated to a heap portion of memory 103 local to robot 100 to store voxels for an entire environment, or for a pertinent portion of the environment (e.g., the portion the robot is in or will be within to perform a task). Voxels may be pulled from and returned to the heap as needed.
- linked data structures may be used to represent occupied voxels in each column.
- a tree such as a binary search tree may be allocated for each column that contains one or more occupied voxels.
- the nodes representing the voxels may be searchable by various aspects of the voxel, such as elevation.
- the system may receive, e.g., from a three-dimensional vision sensor of robot 100 , a group of data points sensed by the three-dimensional vision sensor. Each of these data points may include, among other things, a sensed depth and/or other data such as color data (e.g., red, green, and blue values).
- color data e.g., red, green, and blue values
- the system may identify one or more voxels of a three-dimensional voxel model that are occupied by the group of data points. For example, in some implementations, the system may project the group of data points onto voxels of the 3D model to determine which voxels contain at least a threshold number of data points. In some implementations, only a single data point needs be contained in a voxel in order for that voxel to be deemed “occupied.” In other implementations, a voxel may be deemed “occupied” where, for instance, a threshold number of data points are contained in the voxel.
- various statistics about one or more data points in a voxel may be considered when deciding whether the voxel should be deemed occupied.
- the system may allocate a greater amount of memory to store data representing each occupied voxel than is allocated to store data indicative of any unoccupied voxel of the three-dimensional voxel model. For example, as described above, unoccupied voxels may not be allocated any memory, or only enough memory to link to an occupied voxel at a higher elevation.
- a maximum amount of voxels applicable to the local environment may be allocated in the heap of a robot's volatile memory (e.g., RAM).
- volatile memory e.g., RAM
- the voxel may be pulled from the heap.
- a voxel may be returned to the heap.
- Using the heap in this manner enables a robot (or a robot controller) to limit a total amount of memory it uses, which may reduce the likelihood of the robot hanging or crashing due to running out of memory.
- the system may identify, for each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel. This may be based on, for instance, x and y values associated with the data point.
- the system may index occupied voxels contained in each column by elevation. For example, as described above, a linked data structure may be linked in order of elevation.
- memory 103 may be preserved in various ways. For example, and as described above, logic 102 may allocate memory to linked data structures only for those columns that contain one or more occupied voxels. Additionally or alternatively, the system may allocate a linked data structure to each column, regardless of whether the column is occupied, but may only allocate memory for occupied nodes, e.g., such that an empty column only contains a link to NULL.
- Various types of data may be stored in association with each voxel node.
- all data points contained within a voxel may be stored in association with the node.
- storing all data points in all occupied voxels may still put considerable strain on the sometimes limited resources of a robot.
- the robot may not necessarily need such granular level of detail to perform various tasks, such as path planning. It may be enough for the robot to know generally where occupied voxels are located, so that the robot can operate (e.g., in real time) in a manner that avoids colliding with occupied voxels.
- data representing each voxel may include statistics about one or more data points contained in the voxel.
- These statistics may come in various forms and may distill data points contained within a voxel down in various ways.
- the robot may store general statistics about all data points, such as 80% of the data points lie within a certain half or quadrant of the voxel. Statistics such as these may be analyzed, e.g., to identify a horizontal or vertical surface formed by the one or more data points contained in the voxel.
- a histogram of data points may be stored for each voxel.
- robot 100 may be able to quickly search (from a computational standpoint) a voxel-based 3D model of an environment so that a path through the environment can be calculated to avoid obstacles, etc.
- robot 100 may project one or more occupied voxels that satisfy a criterion onto a two-dimensional plane representing a surface navigable by the robot, and may calculate a path across the surface avoids obstacles represented by the projected one or more occupied voxels. Robot 100 may then navigate the calculated path.
- robot 100 may need to avoid columns/tiles ⁇ and ⁇ because they include obstacles that would bar passage by robot 100 .
- robot 100 in addition to tiles ⁇ - ⁇ , which are empty, robot 100 also would not need to avoid tile ⁇ because the only occupied voxel in that column is at elevation three, which is above the robot's height.
- FIG. 4 depicts how the 3D model 200 of FIG. 2 may be projected down onto the two-dimensional grid of tiles in this scenario.
- the criterion that a voxel would need to satisfy in order to be projected downwards is that the voxel occupy space represented by a voxel with an elevation of one or two. Accordingly, only tiles ⁇ and ⁇ are projected downwards (as represented by the shading).
- robot 100 need only take into account those two tiles, instead of considering all voxels of a 3D model of the environment, as might have been the case with conventional techniques.
- Building a voxel-based 3D model of an environment as described above may also make segmentation—the process of identifying distinct objects within the environment—easier and/or more efficient. For example, segmentation may be used to identify objects that can be manipulated or otherwise interacted with by robot 100 , such as objects on a table that robot 100 could pick up. If a large number of columns include voxels at the same elevation, those voxels likely contain data points that define a horizontal surface. If a smaller subset of the same columns include outlier voxels at a slightly (e.g., immediately) higher elevation, those outlier voxels may likely contain an object that rests on the horizontal surface that may be manipulated by robot 100 .
- segmentation may be used to identify objects that can be manipulated or otherwise interacted with by robot 100 , such as objects on a table that robot 100 could pick up. If a large number of columns include voxels at the same elevation, those voxels likely contain data points that define a horizontal surface. If
- an example environment 550 includes a table 552 with a tool 554 placed on top.
- a robot such as robot 100 enters environment 550
- the robot may acquire a group of data points sensed by a three-dimensional vision sensor.
- the robot (or a control system associated with the robot) may then project these data points onto a new or existing voxel-based 3D model of the environment.
- An example voxel-based 3D model 556 demonstrating how the table 552 and tool 554 may be represented using techniques described herein is depicted at the bottom of FIG. 5 .
- the tabletop projects to a horizontal surface 560 of voxels having equal elevation.
- the tool 554 projects onto two outlying voxels that are on top of tabletop 560 .
- the voxels of the model 556 that correspond to tool 554 are grayed in because they contain data points deemed to be associated with a manipulable object, in this instance, tool 554 , and therefore have been classified as “manipulable object voxels.” In some implementations, these voxels may have been classified as manipulable object voxels based at least in part on the fact that they represent two outliers from what otherwise appears to be a horizontal surface 560 .
- voxels may be classified as manipulable object voxels using other criteria. For example, in some robot environments, one or more robots may pass through the environment multiple times, each time conducting collecting new mapping data (e.g., using a 3D camera). As time goes on, the one or more robots may detect that some voxels have been occupied by data points for longer periods of time than others. Voxels may be classified as manipulable objects (or simply as dynamic objects) where, for instance, the voxels are observed less than a threshold amount of time. Other voxels that were observed from the beginning or for at least a threshold amount of time may be considered more permanent or semi-permanent, and hence in some implementations, less likely to be considered manipulable by the one or more robots.
- robots may take advantage of the aforementioned 3D modeling techniques to determine when objects have been removed from, or moved within, an environment.
- robots perform ray tracing across all data points to determine whether objects are still in previously detected positions. Given the large number of potential data points that could be ray traced, this can be a computationally-costly method of ascertaining object removal. Instead, and in accordance with various implementations of the present disclosure, only data points occupying voxels previously deemed to be robot-manipulable objects may be examined.
- “back projection” may be employed to determine whether the manipulable objects underlying these voxels remain.
- a robot may obtain, from a 3D vision sensor, one or more up-to-date depth values associated with data points within one or more voxels previously classified as manipulable object voxels. The robot may then compare these depths with “expected” depths (i.e. depths expected should the object still be present). If a sufficient proportion of the data point depths within the voxel differ from the expected depths, and particularly if the depths (e.g., distance from the robot) associated with these data points are greater than expected, then the robot may determine that the object has been moved and/or removed.
- expected depths i.e. depths expected should the object still be present
- the techniques described herein are not limited to operation of robots. They may be equally applicable to visual interfaces for controlling and/or debugging robots, and more generally, to interfaces for roughly mapping an area.
- disclosed techniques may be used to quickly and efficiently render, on a display device, a sparse 3D map of an environment. The map may have less detail than would be the case if a comprehensive 3D map of the environment was rendered, but the higher level of detail may not always be necessary or worth the computational costs.
- a network communication link between the user and the telepresence robot is spotty, slow, or otherwise unreliable.
- the user may be presented with a simple-to-operate interface that omits many unnecessary details, potentially making navigation simpler for the user and preserving network resources such as bandwidth.
- FIG. 6 is a block diagram of an example computer system 610 .
- Computer system 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via bus subsystem 612 .
- peripheral devices may include a storage subsystem 624 , including, for example, a memory subsystem 625 and a file storage subsystem 626 , user interface output devices 620 , user interface input devices 622 , and a network interface subsystem 616 .
- the input and output devices allow user interaction with computer system 610 .
- Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computer systems.
- User interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computer system 610 or onto a communication network.
- User interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computer system 610 to the user or to another machine or computer system.
- Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 624 may include the logic to perform selected aspects of method 300 , and/or to implement one or more aspects of robot 100 or control system 150 .
- Memory 625 used in the storage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored.
- a file storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain implementations may be stored by file storage subsystem 626 in the storage subsystem 624 , or in other machines accessible by the processor(s) 614 .
- Bus subsystem 612 provides a mechanism for letting the various components and subsystems of computer system 610 communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computer system 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, smart phone, smart watch, smart glasses, set top box, tablet computer, laptop, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computer system 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computer system 610 are possible having more or fewer components than the computer system depicted in FIG. 6 .
Abstract
Methods, apparatus, systems, and computer-readable media are provided for efficient mapping of a robot environment. In various implementations, a group of data points may be sensed by a three-dimensional sensor. One or more voxels of a three-dimensional voxel model that are occupied by the group of data points may be identified. For each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel may be identified. Occupied voxels contained in each column may be indexed by elevation. In various implementations, one or more sparse linked data structures may be used to represent the columns.
Description
A variety of techniques may be performed to determine positions and/or poses of robots. Some robots perform techniques such as simultaneous localization and mapping (“SLAM”). In other instances, techniques such as visual odometry and/or wheel odometry may be employed. Some robots may use an inertial measurement unit (“IMU”) to determine their positions. To perform robot navigation, some robots and/or robot control systems construct and/or maintain a complete three-dimensional (“3D”) model of an environment in which the robot operates. A robot may acquire data from a 3D laser scanner or other 3D vision sensor (e.g., stereographic camera) viewing a portion of the robot's environment and map such data to the complete 3D model. Some 3D models are formed as so-called “voxel-based” 3D environments in which a 3D grid of voxels are allocated. Data points sensed by one or more 3D vision sensors (also referred to as a “point cloud”) are projected onto spatially-corresponding voxels. However, allocating memory for every voxel of the 3D grid, regardless of whether any data points sensed by the 3D sensor occupy those voxels, is inefficient both in terms of memory consumption and localization. In addition, standard ray-tracing techniques for determining whether dynamic objects remain in previously-detected locations may be computationally costly.
The present disclosure is generally directed to methods, apparatus, and computer-readable media (transitory and non-transitory) for computationally-efficient perception and utilization of data points sensed by a 3D sensor of a robot to map a robot environment. In various implementations, a voxel-based 3D model of a robotic environment may be organized logically into columns, with the “floor” being organized into “tiles,” and each tile being associated with a column. Data points sensed by a 3D vision sensor in the environment may be projected onto voxels of the model. The voxels may be grouped into the columns that contain them, and in many instances may be indexed within the columns by elevation. In some implementations, memory is only allocated (e.g., from the heap) for voxels that are “occupied” by at least a threshold number of data points sensed by the 3D vision sensor, and only those occupied voxels are added to columns. Other voxels may be deemed “unoccupied,” and may not be allocated memory or added to columns. Consequently, the columns may form sparse data structures that include only occupied voxels. This gives rise to various technical advantages, such as reduced memory consumption compared to existing techniques in which memory is allocated for every voxel of the 3D grid, regardless of whether any data points actually occupy the voxel. Utilizing sparse features in accordance with various implementations described herein may also facilitate faster localization by robots, particularly compared to localization using comprehensive voxel-based models.
In various implementations, voxels may be organized into columns using various data structures. In some implementations, a linked data structure such as a linked list or binary tree may be generated for each column. Each node of the linked data structure may represent (e.g., include data associated with) an occupied voxel. Thus, for instance, each node may include statistics about data points contained within the voxel, which may be analyzed for purposes of surface identification, object detection, etc. In some implementations, nodes may be sorted within the linked data structure by elevation.
Organizing occupied voxels into columnar linked data structures gives rise to various technical advantages. For example, searching for particular occupied voxels becomes particularly efficient, especially where binary trees are employed as the linked data structure. In some implementations, a robot searching for a path through the environment may query the columnar linked data structures for occupied voxels within a particular range of elevations, such as voxels occupying space at a height of the robot and lower. The identified voxels may then projected down onto the tiles of the surface over which the robot travels. This enables the robot to efficiently compute and traverse a path through the environment.
Another technical advantage is that identifying objects that are potentially manipulable by the robot, such as objects on a table that the robot could pick up, becomes relatively inexpensive computationally. For example, if a large number of columns include voxels at the same elevation, and there are no voxels immediately above those voxels, those voxels likely contain data points that define a horizontal surface. If a smaller subset of the same columns include outlier voxels at a slightly higher elevation, those outlier voxels likely contain an object that rests on the surface that may be manipulated by the robot.
In some implementations, a computer implemented method may be provided that includes the steps of: receiving from a three-dimensional vision sensor of a robot, a group of data points sensed by the three-dimensional vision sensor; identifying one or more voxels of a three-dimensional voxel model that are occupied by the group of data points; identifying, for each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel; and indexing voxels of the three-dimensional voxel model. The indexing may include indexing occupied voxels contained in each column by elevation without indexing one or more unoccupied voxels by elevation.
This method and other implementations of technology disclosed herein may each optionally include one or more of the following features.
In some implementations, the columns of the three-dimensional voxel model do not include unoccupied voxels. In some implementations, the indexing includes indexing each column exclusively by one or more elevations of one or more occupied voxels contained in the column.
In some implementations, the indexing may include generating, for each column of the three-dimensional voxel model that contains one or more occupied voxels, a linked data structure with one or more nodes representing the one or more occupied voxels contained in the column. In some implementations, the linked data structure may be a linked list. In other implementations, the linked data structure may be a binary tree that may or may not be searchable by voxel elevation. In some implementations, the nodes may be linked in order of elevation.
In some implementations, the method may include searching linked data structures associated with each column to identify voxels within a target elevation range, and operating, by the one or more processors, one or more portions of the robot to move along one or more paths that avoid voxels identified from the searching. In some implementations, data representing each voxel may include statistics about one or more data points contained in the voxel. In various implementations, the method may include analyzing the statistics to identify a surface formed by the one or more data points contained in the voxel.
In some implementations, the method may further include identifying one or more of the occupied voxels that satisfy a criterion; projecting the one or more occupied voxels that satisfy the criterion onto a two-dimensional plane representing a surface navigable by the robot; and calculating a path across the surface for the robot to navigate, wherein the path avoids obstacles represented by the projected one or more occupied voxels. In some implementations, the criterion may include an elevation range that corresponds to at least a height of the robot.
In some implementations, the method may include allocating a greater amount of memory to store data representing each occupied voxel than is allocated to store data indicative of any unoccupied voxel of the three-dimensional voxel model. In other implementations, the method may include allocating memory from a heap to each occupied voxel. In some implementations, unoccupied voxels may not be allocated from the heap.
In various implementations, the method may further include classifying, as manipulable object voxels, one or more occupied voxels containing data points deemed to be associated with a manipulable object; obtaining, from the three-dimensional vision sensor, one or more up-to-date depth values associated with data points within the one or more manipulable object voxels; and determining that the manipulable object has moved based at least in part on the one or more up-to-date depth values.
Other implementations may include a non-transitory computer readable storage medium storing instructions executable by a processor to perform a method such as one or more of the methods described above. Yet another implementation may include a control system including memory and one or more processors operable to execute instructions, stored in the memory, to implement one or more modules or engines that, alone or collectively, perform a method such as one or more of the methods described above.
It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
In some implementations, logic 102 may be operably coupled with one or more operational components 104 1-n, one or more end effectors 106, and/or one or more sensors 108 1-m, e.g., via one or more buses 110. As used herein, “operational components” 104 of a robot may refer to actuators, motors (e.g., servo motors), joints, shafts, gear trains, pumps (e.g., air or liquid), pistons, drives, or other components that may create and/or undergo propulsion, rotation, and/or motion. Some operational components such as many joints may be independently controllable, although this is not required. In some instances, the more operational components robot 100 has, the more degrees of freedom of movement it may have.
As used herein, “end effector” 106 may refer to a variety of tools that may be operated by robot 100 in order to accomplish various tasks. For example, some robots may be equipped with an end effector 106 that takes the form of a claw with two opposing “fingers” or “digits.” Such as claw is one type of “gripper” known as an “impactive” gripper. Other types of grippers may include but are not limited to “ingressive” (e.g., physically penetrating an object using pins, needles, etc.), “astrictive” (e.g., using suction or vacuum to pick up an object), or “contigutive” (e.g., using surface tension, freezing or adhesive to pick up object). More generally, other types of end effectors may include but are not limited to drills, brushes, force-torque sensors, cutting tools, deburring tools, welding torches, containers, trays, and so forth. In some implementations, end effector 106 may be removable, and various types of modular end effectors may be installed onto robot 100, depending on the circumstances. Some robots, such as some telepresence robots, may not be equipped with end effectors. Instead, some telepresence robots may include displays to render visual representations of the users controlling the telepresence robots, as well as speakers and/or microphones that facilitate the telepresence robot “acting” like the user.
Sensors 108 may take various forms, including but not limited to 3D laser scanners or other 3D vision sensors (e.g., stereographic cameras used to perform stereo visual odometry) configured to provide depth measurements, two-dimensional cameras, light sensors (e.g., passive infrared), force sensors, pressure sensors, pressure wave sensors (e.g., microphones), proximity sensors (also referred to as “distance sensors”), depth sensors, torque sensors, bar code readers, radio frequency identification (“RFID”) readers, radars, range finders, accelerometers, gyroscopes, compasses, position coordinate sensors (e.g., global positioning system, or “GPS”), speedometers, edge detectors, and so forth. While sensors 108 1-m are depicted as being integral with robot 100, this is not meant to be limiting. In some implementations, sensors 108 may be located external to, but may be in direct or indirect communication with, robot 100, e.g., as standalone units or as part of control system 150.
Various modules or engines may be implemented as part of control system 150 as software, hardware, or any combination of the two. For example, in FIG. 1 , control system 150 includes a 3D environment model engine 152, an object attribute engine 156, and the aforementioned user interface engine 162. 3D environment model engine 152 may be configured to maintain, e.g., in index 154, one or more 3D models of one or more 3D environments in which robot 100 operates. In some implementations, the models stored in index 154 may be so-called “voxel” based 3D models that include a 3D grid of voxels to represent the environment. In some implementations, a voxel-based 3D model of an environment may be organized within index 154 by columns. In some implementations, voxels may be indexed within each column by elevation, and in some instances the columns may be sparse data structures as described herein.
While robot 100 and control system 150 are depicted separately in FIG. 1 , this is not meant to be limiting. In various implementations, one or more aspects (e.g., modules, engines, etc.) depicted in FIG. 1 as implemented on one of robot 100 or control system 150 may be implemented on the other, may be distributed across both, and/or may be distributed across one or both in combination with other components not depicted in FIG. 1 . In implementations where robot 100 and control system 150 are separate, they may communicate over one or more wired or wireless networks (not depicted) or using other wireless technology, such as radio, Bluetooth, infrared, etc. In other implementations, control system 150 may be implemented entirely or in part using logic 102 of robot 100.
To make such decisions, robot 100 may access data stored in indices 154 and 158 in various ways. In some implementations, data stored in index 154 and/or index 158 may be synchronized to mirrored locations in memory 103 of robot 100 periodically (e.g., hourly, daily, every few minutes), e.g., as a batch download. In some implementations, data stored in index 154 and/or 158 may be made available to logic 102 of robot 100 on demand. Given the technical benefits attained by employing techniques described herein, such as conservation of memory and/or processor cycles, in some implementations, robot 100 may maintain (e.g., in memory 103) its own indices storing records of the environmental model and any obstacles or hazards.
In this example, seven voxels labeled A-G are pigmented to indicate that they have been deemed to be “occupied” voxels. In some implementations, occupied voxels in each column (i.e., above each tile) may be represented as a node in a linked data structure, such as a list of linked list pointers. In some implementations, a greater amount of memory may be allocated to store data representing each occupied voxel than is allocated to store data indicative of any unoccupied voxel of the three-dimensional voxel model. Thus, for example, in FIG. 2 , each of the front three columns/tiles α-γ may have a linked list allocated in memory. The other six columns δ-ι, which contain no occupied voxels may be unpopulated (or from a memory standpoint, unallocated). Each of columns/tiles α and γ may be represented by, for instance, a linked list of three occupied voxels. In some implementations, the nodes of the linked list may be linked in order of elevation. For the column/tile β (i.e. the column with only the top voxel occupied), the linked list may include a single node to represent the single occupied voxel. In other implementations, sufficient memory may be pre-allocated to a heap portion of memory 103 local to robot 100 to store voxels for an entire environment, or for a pertinent portion of the environment (e.g., the portion the robot is in or will be within to perform a task). Voxels may be pulled from and returned to the heap as needed.
Other types of linked data structures may be used to represent occupied voxels in each column. For example, in some implementations, a tree such as a binary search tree may be allocated for each column that contains one or more occupied voxels. In such a search tree, the nodes representing the voxels may be searchable by various aspects of the voxel, such as elevation.
Referring now to FIG. 3 , an example method 300 of generating a 3D model of a robotic environment is described. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems. For instance, some operations may be performed at robot 100, while other operations may be performed by one or more components of control system 150. Moreover, while operations of method 300 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. At block 302, the system may receive, e.g., from a three-dimensional vision sensor of robot 100, a group of data points sensed by the three-dimensional vision sensor. Each of these data points may include, among other things, a sensed depth and/or other data such as color data (e.g., red, green, and blue values).
At block 304, the system may identify one or more voxels of a three-dimensional voxel model that are occupied by the group of data points. For example, in some implementations, the system may project the group of data points onto voxels of the 3D model to determine which voxels contain at least a threshold number of data points. In some implementations, only a single data point needs be contained in a voxel in order for that voxel to be deemed “occupied.” In other implementations, a voxel may be deemed “occupied” where, for instance, a threshold number of data points are contained in the voxel. In some implementations, various statistics about one or more data points in a voxel, such as how evenly the data points are distributed within the voxel, whether all the data points are skewed towards one side of the voxel, etc., may be considered when deciding whether the voxel should be deemed occupied.
In some implementations, at block 306, the system may allocate a greater amount of memory to store data representing each occupied voxel than is allocated to store data indicative of any unoccupied voxel of the three-dimensional voxel model. For example, as described above, unoccupied voxels may not be allocated any memory, or only enough memory to link to an occupied voxel at a higher elevation.
In other implementations, a maximum amount of voxels applicable to the local environment may be allocated in the heap of a robot's volatile memory (e.g., RAM). When a voxel is needed (e.g., threshold number of data points are detected within the voxel), the voxel may be pulled from the heap. Likewise, when a voxel is no longer needed, it may be returned to the heap. Using the heap in this manner enables a robot (or a robot controller) to limit a total amount of memory it uses, which may reduce the likelihood of the robot hanging or crashing due to running out of memory.
At block 308, the system may identify, for each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel. This may be based on, for instance, x and y values associated with the data point. At block 310, the system may index occupied voxels contained in each column by elevation. For example, as described above, a linked data structure may be linked in order of elevation.
In order for robot 100 to be able to operate in real time or near real time (e.g., at 10 Hz), memory 103 may be preserved in various ways. For example, and as described above, logic 102 may allocate memory to linked data structures only for those columns that contain one or more occupied voxels. Additionally or alternatively, the system may allocate a linked data structure to each column, regardless of whether the column is occupied, but may only allocate memory for occupied nodes, e.g., such that an empty column only contains a link to NULL.
Various types of data may be stored in association with each voxel node. In some implementations, all data points contained within a voxel may be stored in association with the node. However, storing all data points in all occupied voxels may still put considerable strain on the sometimes limited resources of a robot. Moreover, the robot may not necessarily need such granular level of detail to perform various tasks, such as path planning. It may be enough for the robot to know generally where occupied voxels are located, so that the robot can operate (e.g., in real time) in a manner that avoids colliding with occupied voxels.
Accordingly, in various implementations, data representing each voxel (i.e. data stored in association with each node of the linked data structure) may include statistics about one or more data points contained in the voxel. These statistics may come in various forms and may distill data points contained within a voxel down in various ways. For example, in some implementations, rather than storing full information (e.g., x,y,z coordinates) for each data point, the robot may store general statistics about all data points, such as 80% of the data points lie within a certain half or quadrant of the voxel. Statistics such as these may be analyzed, e.g., to identify a horizontal or vertical surface formed by the one or more data points contained in the voxel. In some implementations, a histogram of data points may be stored for each voxel.
Generating voxel-based 3D models of environments as described herein may give rise to a number of technical benefits. For example, robot 100 (or control system 150) may be able to quickly search (from a computational standpoint) a voxel-based 3D model of an environment so that a path through the environment can be calculated to avoid obstacles, etc. For example, in some implementations, robot 100 may project one or more occupied voxels that satisfy a criterion onto a two-dimensional plane representing a surface navigable by the robot, and may calculate a path across the surface avoids obstacles represented by the projected one or more occupied voxels. Robot 100 may then navigate the calculated path.
For example, suppose robot 100 has a height that is less than two voxels high as represented by 3D model 200 in FIG. 2 . Accordingly, when moving through the environment modeled by 3D model 200, robot 100 may need to avoid columns/tiles α and γ because they include obstacles that would bar passage by robot 100. However, in addition to tiles δ-ι, which are empty, robot 100 also would not need to avoid tile β because the only occupied voxel in that column is at elevation three, which is above the robot's height.
Building a voxel-based 3D model of an environment as described above may also make segmentation—the process of identifying distinct objects within the environment—easier and/or more efficient. For example, segmentation may be used to identify objects that can be manipulated or otherwise interacted with by robot 100, such as objects on a table that robot 100 could pick up. If a large number of columns include voxels at the same elevation, those voxels likely contain data points that define a horizontal surface. If a smaller subset of the same columns include outlier voxels at a slightly (e.g., immediately) higher elevation, those outlier voxels may likely contain an object that rests on the horizontal surface that may be manipulated by robot 100.
An example of this is depicted in FIG. 5 , in which an example environment 550 includes a table 552 with a tool 554 placed on top. When a robot such as robot 100 enters environment 550, the robot may acquire a group of data points sensed by a three-dimensional vision sensor. The robot (or a control system associated with the robot) may then project these data points onto a new or existing voxel-based 3D model of the environment. An example voxel-based 3D model 556 demonstrating how the table 552 and tool 554 may be represented using techniques described herein is depicted at the bottom of FIG. 5 . In this example, the tabletop projects to a horizontal surface 560 of voxels having equal elevation. The tool 554 projects onto two outlying voxels that are on top of tabletop 560.
The voxels of the model 556 that correspond to tool 554 are grayed in because they contain data points deemed to be associated with a manipulable object, in this instance, tool 554, and therefore have been classified as “manipulable object voxels.” In some implementations, these voxels may have been classified as manipulable object voxels based at least in part on the fact that they represent two outliers from what otherwise appears to be a horizontal surface 560.
In some implementations, voxels may be classified as manipulable object voxels using other criteria. For example, in some robot environments, one or more robots may pass through the environment multiple times, each time conducting collecting new mapping data (e.g., using a 3D camera). As time goes on, the one or more robots may detect that some voxels have been occupied by data points for longer periods of time than others. Voxels may be classified as manipulable objects (or simply as dynamic objects) where, for instance, the voxels are observed less than a threshold amount of time. Other voxels that were observed from the beginning or for at least a threshold amount of time may be considered more permanent or semi-permanent, and hence in some implementations, less likely to be considered manipulable by the one or more robots.
In some implementations, robots (or control systems) may take advantage of the aforementioned 3D modeling techniques to determine when objects have been removed from, or moved within, an environment. Traditionally, robots perform ray tracing across all data points to determine whether objects are still in previously detected positions. Given the large number of potential data points that could be ray traced, this can be a computationally-costly method of ascertaining object removal. Instead, and in accordance with various implementations of the present disclosure, only data points occupying voxels previously deemed to be robot-manipulable objects may be examined. In some implementations, “back projection” may be employed to determine whether the manipulable objects underlying these voxels remain. For example, a robot may obtain, from a 3D vision sensor, one or more up-to-date depth values associated with data points within one or more voxels previously classified as manipulable object voxels. The robot may then compare these depths with “expected” depths (i.e. depths expected should the object still be present). If a sufficient proportion of the data point depths within the voxel differ from the expected depths, and particularly if the depths (e.g., distance from the robot) associated with these data points are greater than expected, then the robot may determine that the object has been moved and/or removed.
The techniques described herein are not limited to operation of robots. They may be equally applicable to visual interfaces for controlling and/or debugging robots, and more generally, to interfaces for roughly mapping an area. For example, disclosed techniques may be used to quickly and efficiently render, on a display device, a sparse 3D map of an environment. The map may have less detail than would be the case if a comprehensive 3D map of the environment was rendered, but the higher level of detail may not always be necessary or worth the computational costs.
For example, suppose a user is operating a graphical user interface to remotely navigate a telepresence robot through an environment. Suppose further that a network communication link between the user and the telepresence robot is spotty, slow, or otherwise unreliable. By reducing the environment in which the telepresence robot operates to sparse, columnar-based 3D models as described herein, the user may be presented with a simple-to-operate interface that omits many unnecessary details, potentially making navigation simpler for the user and preserving network resources such as bandwidth.
User interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term “input device” is intended to include all possible types of devices and ways to input information into computer system 610 or onto a communication network.
User interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term “output device” is intended to include all possible types of devices and ways to output information from computer system 610 to the user or to another machine or computer system.
While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (22)
1. A method, comprising:
receiving, by one or more processors, from a three-dimensional vision sensor of a robot, a group of data points sensed by the three-dimensional vision sensor;
identifying, by the one or more processors, one or more voxels of a three-dimensional voxel model that are occupied by the group of data points;
determining, by the one or more processors, for each occupied voxel, data representing the occupied voxel, wherein the data includes statistics about how a plurality of data points of the group of data points are spatially distributed within the occupied voxel;
identifying, by the one or more processors, for each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel; and
indexing in memory, by the one or more processors the data representing voxels of the three-dimensional voxel model, the indexing comprising indexing, in the memory, the data representing occupied voxels contained in each column by elevation without indexing one or more unoccupied voxels by elevation.
2. The method of claim 1 , wherein the indexing comprises generating, for each column of the three-dimensional voxel model that contains one or more occupied voxels, a linked data structure with one or more nodes representing the one or more occupied voxels contained in the column.
3. The method of claim 2 , wherein the linked data structure comprises a linked list.
4. The method of claim 2 , wherein the linked data structure is a binary tree.
5. The method of claim 4 , wherein the binary tree is searchable by voxel elevation.
6. The method of claim 2 , wherein the nodes are linked in order of elevation.
7. The method of claim 2 , further comprising:
searching, by the one or more processors, linked data structures associated with each column to identify voxels within a target elevation range; and
operating, by the one or more processors, one or more portions of the robot to move along one or more paths that avoid voxels identified from the searching.
8. The method of claim 1 , further comprising analyzing the statistics to identify a surface formed by the one or more data points contained in the voxel.
9. The method of claim 1 , further comprising:
identifying, by the one or more processors, one or more of the occupied voxels that satisfy a criterion;
projecting, by the one or more processors, the one or more occupied voxels that satisfy the criterion onto a two-dimensional plane representing a surface navigable by the robot; and
calculating, by the one or more processors, a path across the surface for the robot to navigate, wherein the path avoids obstacles represented by the projected one or more occupied voxels.
10. The method of claim 9 , wherein the criterion comprises an elevation range that corresponds to at least a height of the robot.
11. The method of claim 1 , further comprising allocating, by the one or more processors, a greater amount of memory to store data representing each occupied voxel than is allocated to store data indicative of any unoccupied voxel of the three-dimensional voxel model.
12. The method of claim 1 , further comprising allocating, by the one or more processors, memory from a heap to each occupied voxel, wherein unoccupied voxels are not allocated from the heap.
13. The method of claim 1 , further comprising:
classifying, by the one or more processors, as manipulable object voxels, one or more occupied voxels containing data points deemed to be associated with a manipulable object;
obtaining, by the one or more processors, from the three-dimensional vision sensor, one or more up-to-date depth values associated with data points within the one or more manipulable object voxels; and
determining, by the one or more processors, that the manipulable object has moved based at least in part on the one or more up-to-date depth values.
14. A robot comprising:
a three-dimensional vision sensor;
one or more processors operably coupled with the three-dimensional vision sensor; and
memory operably coupled with the one or more processors, the memory storing instructions that, when executed by the one or more processors, cause the one or more processors to:
receive, from the three-dimensional vision sensor, a group of data points sensed by the three-dimensional vision sensor;
identify one or more voxels of a three-dimensional voxel model that are occupied by the group of data points;
determine, for each occupied voxel, data representing the occupied voxel, wherein the data includes statistics about how a plurality of data points of the group of data points are spatially distributed within the occupied voxel;
identify, for each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel; and
index, in the memory, the data representing voxels of the three-dimensional voxel model, wherein in indexing the voxels the one or more processors are to index, in the memory, the data representing occupied voxels contained in each column by elevation without indexing one or more unoccupied voxels by elevation.
15. The robot of claim 14 , wherein the memory further comprises instructions that, when executed by the one or more processors, cause the one or more processors to generate, for each column of the three-dimensional voxel model that contains one or more occupied voxels, a linked data structure with one or more nodes representing the one or more occupied voxels contained in the column.
16. The robot of claim 15 , wherein the linked data structure comprises a linked list.
17. The robot of claim 15 , wherein the linked data structure is a binary tree that is searchable by voxel elevation.
18. The robot of claim 15 , wherein the nodes are linked in order of elevation.
19. The robot of claim 14 , wherein the memory further comprises instructions that, when executed by the one or more processors, cause the one or more processors to:
identify one or more of the occupied voxels that satisfy a criterion;
project the one or more occupied voxels that satisfy the criterion onto a two-dimensional plane representing a surface navigable by the robot; and
calculate a path across the surface for the robot to navigate, wherein the path avoids obstacles represented by the projected one or more occupied voxels.
20. The robot of claim 14 , wherein the memory further comprises instructions that, when executed by the one or more processors, cause the one or more processors to:
classify, as manipulable object voxels, one or more occupied voxels containing data points deemed to be associated with a manipulable object;
obtain, from the three-dimensional vision sensor, one or more up-to-date depth values associated with data points within the one or more manipulable object voxels; and
determine that the manipulable object has moved based at least in part on the one or more up-to-date depth values.
21. The robot of claim 14 wherein the memory further comprises instructions that, when executed by the one or more processors, cause the one or more processors to allocate memory from a heap portion of the memory to each occupied voxel, wherein unoccupied voxels are not allocated from the heap portion of the memory.
22. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by a robot, cause the robot to perform the following operations:
receiving, from a three-dimensional vision sensor of the robot, a group of data points sensed by the three-dimensional vision sensor;
identifying one or more voxels of a three-dimensional voxel model that are occupied by the group of data points;
determining, for each occupied voxel, data representing the occupied voxel, wherein the data includes statistics about how a plurality of data points of the group of data points are spatially distributed within the occupied voxel;
identifying, for each occupied voxel, a column of the three-dimensional voxel model that contains the occupied voxel; and
indexing, in memory, the data representing the occupied voxels contained in each column by elevation without indexing one or more unoccupied voxels by elevation.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/131,014 US10078333B1 (en) | 2016-04-17 | 2016-04-17 | Efficient mapping of robot environment |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/131,014 US10078333B1 (en) | 2016-04-17 | 2016-04-17 | Efficient mapping of robot environment |
Publications (1)
Publication Number | Publication Date |
---|---|
US10078333B1 true US10078333B1 (en) | 2018-09-18 |
Family
ID=63491049
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/131,014 Active 2036-09-16 US10078333B1 (en) | 2016-04-17 | 2016-04-17 | Efficient mapping of robot environment |
Country Status (1)
Country | Link |
---|---|
US (1) | US10078333B1 (en) |
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10215571B2 (en) * | 2016-08-09 | 2019-02-26 | Nauto, Inc. | System and method for precision localization and mapping |
US10303180B1 (en) * | 2017-04-20 | 2019-05-28 | X Development Llc | Generating and utilizing non-uniform volume measures for voxels in robotics applications |
US20190180409A1 (en) * | 2016-08-19 | 2019-06-13 | Movidius Ltd. | Operations using sparse volumetric data |
US10453150B2 (en) | 2017-06-16 | 2019-10-22 | Nauto, Inc. | System and method for adverse vehicle event determination |
CN111338384A (en) * | 2019-12-17 | 2020-06-26 | 北京化工大学 | Self-adaptive path tracking method of snake-like robot |
US11279039B2 (en) * | 2017-02-07 | 2022-03-22 | Veo Robotics, Inc. | Ensuring safe operation of industrial machinery |
US11392131B2 (en) | 2018-02-27 | 2022-07-19 | Nauto, Inc. | Method for determining driving policy |
Citations (29)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5347459A (en) * | 1993-03-17 | 1994-09-13 | National Research Council Of Canada | Real time collision detection |
US20030156112A1 (en) * | 2000-07-13 | 2003-08-21 | Halmshaw Paul A | Method, apparatus, signals and codes for establishing and using a data structure for storing voxel information |
US20030214502A1 (en) * | 2001-11-27 | 2003-11-20 | Samsung Electronics Co., Ltd. | Apparatus and method for depth image-based representation of 3-dimensional object |
US20070116347A1 (en) * | 2005-10-17 | 2007-05-24 | Siemens Corporate Research Inc | Devices, Systems, and Methods for Improving Image Consistency |
US20070280528A1 (en) * | 2006-06-02 | 2007-12-06 | Carl Wellington | System and method for generating a terrain model for autonomous navigation in vegetation |
US20090171627A1 (en) * | 2007-12-30 | 2009-07-02 | Olson Eric S | System and Method for Surface Reconstruction from an Unstructured Point Set |
US20090284537A1 (en) * | 2008-05-19 | 2009-11-19 | Siemens Corporate Research, Inc. | Framework for processing and rendering large volume data |
US7626569B2 (en) * | 2004-10-25 | 2009-12-01 | Graphics Properties Holdings, Inc. | Movable audio/video communication interface system |
US20090295800A1 (en) * | 2008-05-30 | 2009-12-03 | Siemens Corporate Research, Inc. | Method for direct volumetric rendering of deformable bricked volumes |
US20090326711A1 (en) * | 2008-05-21 | 2009-12-31 | Chang Tien L | Multi-arm robot system interference check via three dimensional automatic zones |
US7860211B1 (en) * | 2009-06-26 | 2010-12-28 | Martin Annis | Method of producing a laminography image with a rotating object, fixed x-ray source, and fixed detector columns |
US20120033273A1 (en) * | 2010-08-09 | 2012-02-09 | Xerox Corporation | Patch codes using a proximity array |
US20120215351A1 (en) * | 2008-05-21 | 2012-08-23 | Mcgee H Dean | Method and system for automatically preventing deadlock in multi-robot systems |
US20120281907A1 (en) * | 2011-05-06 | 2012-11-08 | Toyota Motor Engin. & Manufact. N.A.(TEMA) | Real-time 3d point cloud obstacle discriminator apparatus and associated methodology for training a classifier via bootstrapping |
US8553989B1 (en) | 2010-04-27 | 2013-10-08 | Hrl Laboratories, Llc | Three-dimensional (3D) object recognition system using region of interest geometric features |
US20140003695A1 (en) * | 1999-08-11 | 2014-01-02 | Case Western Reserve University | Methods and systems for producing an implant |
US20140270476A1 (en) * | 2013-03-12 | 2014-09-18 | Harris Corporation | Method for 3d object identification and pose detection using phase congruency and fractal analysis |
US8847954B1 (en) * | 2011-12-05 | 2014-09-30 | Google Inc. | Methods and systems to compute 3D surfaces |
US20150228077A1 (en) * | 2014-02-08 | 2015-08-13 | Honda Motor Co., Ltd. | System and method for mapping, localization and pose correction |
US20150371110A1 (en) * | 2014-06-20 | 2015-12-24 | Samsung Electronics Co., Ltd. | Method and apparatus for extracting feature regions from point cloud |
US20160023352A1 (en) * | 2014-07-25 | 2016-01-28 | California Institute Of Technology | SURROGATE: A Body-Dexterous Mobile Manipulation Robot with a Tracked Base |
US20160109284A1 (en) * | 2013-03-18 | 2016-04-21 | Aalborg Universitet | Method and device for modelling room acoustic based on measured geometrical data |
US20160151120A1 (en) * | 2014-12-02 | 2016-06-02 | KB Medical SA | Robot Assisted Volume Removal During Surgery |
US20160300351A1 (en) * | 2015-04-08 | 2016-10-13 | Algotec Systems Ltd. | Image processing of organs depending on organ intensity characteristics |
US9530101B1 (en) * | 2013-11-20 | 2016-12-27 | The United States Of America As Represented By The Secretary Of The Navy | Method for calculating sensor performance of a sensor grid using dynamic path aggregation |
US20170013251A1 (en) * | 2015-07-10 | 2017-01-12 | Samuel Arledge Thigpen | Three-dimensional projection |
US20170154435A1 (en) * | 2015-11-30 | 2017-06-01 | Lexmark International Technology Sa | System and Methods of Segmenting Vessels from Medical Imaging Data |
US9754405B1 (en) * | 2015-08-10 | 2017-09-05 | Ngrain (Canada) Corporation | System, method and computer-readable medium for organizing and rendering 3D voxel models in a tree structure |
US20180058861A1 (en) * | 2016-08-26 | 2018-03-01 | Here Global B.V. | Automatic localization geometry detection |
-
2016
- 2016-04-17 US US15/131,014 patent/US10078333B1/en active Active
Patent Citations (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5347459A (en) * | 1993-03-17 | 1994-09-13 | National Research Council Of Canada | Real time collision detection |
US20140003695A1 (en) * | 1999-08-11 | 2014-01-02 | Case Western Reserve University | Methods and systems for producing an implant |
US20030156112A1 (en) * | 2000-07-13 | 2003-08-21 | Halmshaw Paul A | Method, apparatus, signals and codes for establishing and using a data structure for storing voxel information |
US20030214502A1 (en) * | 2001-11-27 | 2003-11-20 | Samsung Electronics Co., Ltd. | Apparatus and method for depth image-based representation of 3-dimensional object |
US7626569B2 (en) * | 2004-10-25 | 2009-12-01 | Graphics Properties Holdings, Inc. | Movable audio/video communication interface system |
US20070116347A1 (en) * | 2005-10-17 | 2007-05-24 | Siemens Corporate Research Inc | Devices, Systems, and Methods for Improving Image Consistency |
US20070280528A1 (en) * | 2006-06-02 | 2007-12-06 | Carl Wellington | System and method for generating a terrain model for autonomous navigation in vegetation |
US20090171627A1 (en) * | 2007-12-30 | 2009-07-02 | Olson Eric S | System and Method for Surface Reconstruction from an Unstructured Point Set |
US20090284537A1 (en) * | 2008-05-19 | 2009-11-19 | Siemens Corporate Research, Inc. | Framework for processing and rendering large volume data |
US20090326711A1 (en) * | 2008-05-21 | 2009-12-31 | Chang Tien L | Multi-arm robot system interference check via three dimensional automatic zones |
US20120215351A1 (en) * | 2008-05-21 | 2012-08-23 | Mcgee H Dean | Method and system for automatically preventing deadlock in multi-robot systems |
US20090295800A1 (en) * | 2008-05-30 | 2009-12-03 | Siemens Corporate Research, Inc. | Method for direct volumetric rendering of deformable bricked volumes |
US7860211B1 (en) * | 2009-06-26 | 2010-12-28 | Martin Annis | Method of producing a laminography image with a rotating object, fixed x-ray source, and fixed detector columns |
US8553989B1 (en) | 2010-04-27 | 2013-10-08 | Hrl Laboratories, Llc | Three-dimensional (3D) object recognition system using region of interest geometric features |
US20120033273A1 (en) * | 2010-08-09 | 2012-02-09 | Xerox Corporation | Patch codes using a proximity array |
US20120281907A1 (en) * | 2011-05-06 | 2012-11-08 | Toyota Motor Engin. & Manufact. N.A.(TEMA) | Real-time 3d point cloud obstacle discriminator apparatus and associated methodology for training a classifier via bootstrapping |
US8847954B1 (en) * | 2011-12-05 | 2014-09-30 | Google Inc. | Methods and systems to compute 3D surfaces |
US20140270476A1 (en) * | 2013-03-12 | 2014-09-18 | Harris Corporation | Method for 3d object identification and pose detection using phase congruency and fractal analysis |
US20160109284A1 (en) * | 2013-03-18 | 2016-04-21 | Aalborg Universitet | Method and device for modelling room acoustic based on measured geometrical data |
US9530101B1 (en) * | 2013-11-20 | 2016-12-27 | The United States Of America As Represented By The Secretary Of The Navy | Method for calculating sensor performance of a sensor grid using dynamic path aggregation |
US20150228077A1 (en) * | 2014-02-08 | 2015-08-13 | Honda Motor Co., Ltd. | System and method for mapping, localization and pose correction |
US9443309B2 (en) * | 2014-02-08 | 2016-09-13 | Honda Motor Co., Ltd. | System and method for image based mapping, localization, and pose correction of a vehicle with landmark transform estimation |
US20150371110A1 (en) * | 2014-06-20 | 2015-12-24 | Samsung Electronics Co., Ltd. | Method and apparatus for extracting feature regions from point cloud |
US20160023352A1 (en) * | 2014-07-25 | 2016-01-28 | California Institute Of Technology | SURROGATE: A Body-Dexterous Mobile Manipulation Robot with a Tracked Base |
US20160151120A1 (en) * | 2014-12-02 | 2016-06-02 | KB Medical SA | Robot Assisted Volume Removal During Surgery |
US20160300351A1 (en) * | 2015-04-08 | 2016-10-13 | Algotec Systems Ltd. | Image processing of organs depending on organ intensity characteristics |
US20170013251A1 (en) * | 2015-07-10 | 2017-01-12 | Samuel Arledge Thigpen | Three-dimensional projection |
US9754405B1 (en) * | 2015-08-10 | 2017-09-05 | Ngrain (Canada) Corporation | System, method and computer-readable medium for organizing and rendering 3D voxel models in a tree structure |
US20170154435A1 (en) * | 2015-11-30 | 2017-06-01 | Lexmark International Technology Sa | System and Methods of Segmenting Vessels from Medical Imaging Data |
US20180058861A1 (en) * | 2016-08-26 | 2018-03-01 | Here Global B.V. | Automatic localization geometry detection |
Non-Patent Citations (9)
Title |
---|
Bard et al. "Achieving Dextrous Grasping by Integrating Planning and Vision-Based Sensing." The International Journal of Robotics Research 14, No. 5 (1995): 445-464. |
Häne et al. "Stereo Depth Map Fusion for Robot Navigation." In Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on, pp. 1618-1625. IEEE, 2011. |
Lalonde et al. "Data Structure for Efficient Processing in 3-D." Robotics: Science and Systems Conference, 2005. 8 pages. |
Lau, Boris. "Techniques for Robot Navigation in Dynamic Real-world Environments." PhD diss., 2013; 169 pages. |
Marder-Eppstein et al. "The Office Marathon: Robust Navigation in an Indoor Office Environment." (2010). 8 pages. |
Nießner et al. "Real-Time 3D Reconstruction At Scale Using Voxel Hashing." ACM Transactions on Graphics (TOG) 32, No. 6 (2013): 169; 11 pages. |
Ruhnke et al. "Compact RGBD Surface Models Based on Sparse Coding." In AAAI. 2013. 7 pages. |
Teschner et al. "Collision Handling in Dynamic Simulation Environments." Eurographics Tutorials (2005): 79-185. |
Vanneste et al. "3DVFH+: Real-Time Three-Dimensional Obstacle Avoidance Using an Octomap." 2014; 12 pages. |
Cited By (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11175145B2 (en) | 2016-08-09 | 2021-11-16 | Nauto, Inc. | System and method for precision localization and mapping |
US10215571B2 (en) * | 2016-08-09 | 2019-02-26 | Nauto, Inc. | System and method for precision localization and mapping |
US20190180409A1 (en) * | 2016-08-19 | 2019-06-13 | Movidius Ltd. | Operations using sparse volumetric data |
US11965743B2 (en) | 2016-08-19 | 2024-04-23 | Movidius Limited | Operations using sparse volumetric data |
US11920934B2 (en) | 2016-08-19 | 2024-03-05 | Movidius Limited | Path planning using sparse volumetric data |
US11367246B2 (en) * | 2016-08-19 | 2022-06-21 | Movidius Ltd. | Operations using sparse volumetric data |
US11279039B2 (en) * | 2017-02-07 | 2022-03-22 | Veo Robotics, Inc. | Ensuring safe operation of industrial machinery |
US10303180B1 (en) * | 2017-04-20 | 2019-05-28 | X Development Llc | Generating and utilizing non-uniform volume measures for voxels in robotics applications |
US10671081B1 (en) * | 2017-04-20 | 2020-06-02 | X Development Llc | Generating and utilizing non-uniform volume measures for voxels in robotics applications |
US10453150B2 (en) | 2017-06-16 | 2019-10-22 | Nauto, Inc. | System and method for adverse vehicle event determination |
US11164259B2 (en) | 2017-06-16 | 2021-11-02 | Nauto, Inc. | System and method for adverse vehicle event determination |
US11017479B2 (en) | 2017-06-16 | 2021-05-25 | Nauto, Inc. | System and method for adverse vehicle event determination |
US11392131B2 (en) | 2018-02-27 | 2022-07-19 | Nauto, Inc. | Method for determining driving policy |
CN111338384B (en) * | 2019-12-17 | 2021-06-08 | 北京化工大学 | Self-adaptive path tracking method of snake-like robot |
CN111338384A (en) * | 2019-12-17 | 2020-06-26 | 北京化工大学 | Self-adaptive path tracking method of snake-like robot |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10078333B1 (en) | Efficient mapping of robot environment | |
US10955811B2 (en) | Robot interaction with objects based on semantic information associated with embedding spaces | |
US11042783B2 (en) | Learning and applying empirical knowledge of environments by robots | |
US11192250B1 (en) | Methods and apparatus for determining the pose of an object based on point cloud data | |
US10671081B1 (en) | Generating and utilizing non-uniform volume measures for voxels in robotics applications | |
US11027425B1 (en) | Space extrapolation for robot task performance | |
US11170220B2 (en) | Delegation of object and pose detection | |
CN107436148B (en) | Robot navigation method and device based on multiple maps | |
CN107677279A (en) | It is a kind of to position the method and system for building figure | |
US10891484B2 (en) | Selectively downloading targeted object recognition modules | |
US10102629B1 (en) | Defining and/or applying a planar model for object detection and/or pose estimation | |
WO2021104415A1 (en) | Robot autonomous exploration mapping method, device and storage medium | |
US9764470B2 (en) | Selective deployment of robots to perform mapping | |
US10853646B1 (en) | Generating and utilizing spatial affordances for an object in robotics applications | |
US10105847B1 (en) | Detecting and responding to geometric changes to robots | |
US11328182B2 (en) | Three-dimensional map inconsistency detection using neural network | |
De Silva et al. | Comparative analysis of octomap and rtabmap for multi-robot disaster site mapping | |
US10780581B1 (en) | Generation and application of reachability maps to operate robots | |
Smirnova et al. | A technique of natural visual landmarks detection and description for mobile robot cognitive navigation | |
Han et al. | Novel cartographer using an OAK-D smart camera for indoor robots location and navigation | |
US11654550B1 (en) | Single iteration, multiple permutation robot simulation | |
US20240058954A1 (en) | Training robot control policies | |
CN116263598A (en) | Relocation method and equipment for self-mobile equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |