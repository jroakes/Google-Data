CN115605871A - Recommending actions based on an entity or entity type - Google Patents
Recommending actions based on an entity or entity type Download PDFInfo
- Publication number
- CN115605871A CN115605871A CN202180035381.5A CN202180035381A CN115605871A CN 115605871 A CN115605871 A CN 115605871A CN 202180035381 A CN202180035381 A CN 202180035381A CN 115605871 A CN115605871 A CN 115605871A
- Authority
- CN
- China
- Prior art keywords
- user
- entity
- given application
- computing device
- response actions
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
- G06F40/295—Named entity recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
Abstract
Embodiments for recommending actions based on an entity or entity type are described herein. In various implementations, a partial free-form natural language input may be received from a user at an input component of a computing device. The partial free-form natural language input may identify the entity without identifying the responsive action and may be directed by the user to an automated assistant operating at least in part on the computing device. The partial free-form natural language input may be analyzed to identify an entity. Based on the identified entity, a plurality or superset of candidate response actions may be identified, filtered, and/or ordered based on one or more signals. The automated assistant may then provide an output that recommends one or more of the candidate response actions based on the ranking and/or filtering.
Description
Background
Humans may have human-computer conversations with interactive software applications, referred to herein as "automated assistants" (also referred to as "chat robots," "interactive personal assistants," "intelligent personal assistants," "personal voice assistants," "session agents," "virtual assistants," etc.). For example, a human being (which may be referred to as a "user" when interacting with an automated assistant) may provide commands, queries, and/or requests (collectively referred to herein as "queries") using free-form natural language input, which may include voiced utterances and/or typed free-form natural language input, which are converted to text and then processed. The automated assistant can perform various types of processing on the natural language input, such as natural language processing, syntactic processing, semantic processing, and so forth, to identify and respond to the user's intent.
Free-form natural language input is input that is formulated by a user and is not limited to a set of options presented for the user to select. Thus, the user can issue almost any command he or she likes, and the automated assistant will attempt to respond. If the user is unfamiliar with the automated assistant, the user may not be aware of what types of responsive actions are available for the automated assistant to perform. Thus, the user may issue invalid commands, or may be hesitant to issue commands because he or she is unfamiliar with the automated assistant.
Disclosure of Invention
Techniques for recommending actions based on an entity or entity type are described herein. In various implementations, a partial free-form natural language input may be received from a user at an input component of a computing device. The partial free-form natural language input may identify the entity without identifying the responsive action and may be directed by the user to an automated assistant operating at least in part on the computing device. The partial free-form natural language input may be analyzed to identify an entity. Based on the identified entity, a plurality or superset of candidate response actions may be identified, filtered, and/or ordered based on one or more signals. The automated assistant may then provide an output that recommends one or more of the candidate response actions based on the ranking and/or filtering.
In various embodiments, a method may be implemented using one or more processors and may include: receiving, at an input component of a computing device, partial free form natural language input from a user, wherein the partial free form natural language input identifies an entity without identifying a responsive action and is directed by the user to an automated assistant operating at least in part on the computing device; analyzing a portion of the free-form natural language input to identify an entity; identifying a plurality of candidate response actions based on the identified entity; ranking at least some of the plurality of candidate responsive actions based on the one or more signals; and causing the automated assistant to provide an output recommending one or more of the candidate response actions selected based on the ranking.
In various embodiments, the plurality of candidate responsive actions includes a plurality of applications installed on or available through the computing device, and the one or more signals include: a time at which each of the plurality of applications was most recently used by the user; or the frequency with which each of the plurality of applications is used by the user.
In various embodiments, the method may further comprise: one or more candidate response actions of the plurality of candidate response actions are filtered from consideration for the output based on a current context of the user, wherein the current context is determined based on the one or more context signals. In various implementations, the one or more context signals may include a state of a given application executing at least in part on the computing device, and filtering may include: another application of the same application type as the given application or the given application is filtered from consideration for output. In various implementations, the application type may include a ride share application, and the state of a given application may indicate that the user has traveled as part of a ride share.
In various implementations, the one or more context signals may include a state of a given application executing at least in part on the computing device, and filtering may include: the first response actions available through the given application are filtered from consideration for output.
In various implementations, the entity may be a location, the one or more contextual signals may include a distance between a current location of the user and the location, and the filtering may include: one or more of the plurality of candidate responsive actions are filtered from consideration for the output based on the distance.
In various implementations, one or more of a plurality of candidate response actions may be identified or ordered based on a state of a given application executing at least in part on a computing device. In various implementations, the given application may be an exercise application, the state of the given application may indicate that the user is currently exercising, and the one or more of the plurality of candidate response actions identified or ordered based on the state of the given application may include ceasing to monitor the user's exercise. In various implementations, the given application may be a ride share application, the state of the given application may indicate that the user is currently traveling as part of a ride share, and the one or more of the plurality of candidate response actions identified or ordered based on the state of the given application may include: changing a destination of the user; or cause a communication to be sent to another user, where the communication indicates the user's current location or estimated time of arrival.
Systems and computer-readable media (transitory and non-transitory) configured to perform various aspects of the present disclosure, such as the methods described above, are also provided. It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are considered to be part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram of an example environment in which embodiments disclosed herein may be implemented.
FIG. 2 depicts one example of how various components described herein may process data.
Fig. 3, 4, 5, 6, 7, 8, and 9 illustrate various scenarios in which selected aspects of the present disclosure may be implemented, according to various embodiments.
Fig. 10 depicts a flow diagram illustrating another example method in accordance with implementations disclosed herein.
FIG. 11 illustrates an example architecture of a computing device.
Detailed Description
Embodiments are described for recommending actions based on an entity or entity type. In various implementations, a partial free-form natural language input may be received from a user at an input component of a computing device. The partial free-form natural language input may identify an entity or entity type without identifying a responsive action and may be directed by a user to an automated assistant operating at least in part on a computing device. The partial free-form natural language input may be analyzed to identify an entity or entity type. Based on the identified entity or entity type, a plurality or superset of candidate response actions may be identified, filtered, and/or ordered based on one or more signals. The automated assistant may then provide an output recommending one or more candidate response actions based on the ranking and/or filtering.
Various signals (such as context signals) may be used to perform various aspects of the present disclosure. For example, entity or entity type recognition, entity or entity type ranking, identifying candidate response actions associated with an entity or entity type, ranking candidate response actions, and/or filtering candidate response actions may be performed based on the context signals. For example, contextual signals or "cues" associated with a user and/or a client device they operate may include location (e.g., determined using GPS, wireless triangulation, inertial measurement units, etc.), time of day, user preferences, calendar entries of the user, communications to/from the user (e.g., email, direct messages, text messages, etc.), social network activity, current user activity (e.g., exercise, flight, driving, carpooling, etc.), applications that are installed or otherwise accessible to the user at any given time, and state(s) of running the applications, to name a few.
The candidate response action may be any action that the automated assistant is capable of performing and/or invoking. Some candidate response actions may be used to control appliances and/or other internet of things ("IoT") devices, such as lights, locks, thermostats, televisions, speakers, smart blinds, cameras, toys, and so forth. Some candidate response actions may be used to obtain items such as products, tickets, food, etc. (e.g., order pizza). Some candidate response actions may be applications or "apps" that may be invoked by the automated assistant, e.g., having various fields that are pre-populated based on free-form natural language input. Some candidate response actions may be "application-internal" actions that are executable internally to the application, such as starting or stopping a monitoring workout, pausing/playing a media file, sharing a location and/or estimated arrival time (e.g., via a ride share or ride application), changing a destination (e.g., via a ride share application), and so forth. In the latter case, an application programming interface ("API") may be exposed to the automated assistant directly and/or through an operating system ("OS"), which enables the automated assistant to perform actions within the application.
Turning now to fig. 1, an example environment in which the techniques disclosed herein may be implemented is illustrated. The example environment includes one or more client computing devices 106. Each client device 106 may execute a respective instance of an automated assistant client 108, which automated assistant client 108 may also be referred to herein as a "client portion" of the automated assistant. One or more cloud-based automated assistant components 119 (which may also be collectively referred to herein as the "server portion" of the automated assistant) may be implemented on one or more computing systems (collectively referred to as "cloud" computing systems) that are communicatively coupled to the client device 106 through one or more local and/or wide area networks (e.g., the internet), indicated generally at 114.
In various embodiments, an instance of the automated assistant client 108, through its interaction with one or more cloud-based automated assistant components 119, may form an instance that appears from the user's perspective to be a logical instance of the automated assistant 120 with which the user can conduct a human-machine conversation. An example of such an automated assistant 120 is depicted in fig. 1 with dashed lines. Thus, it should be understood that each user engaged with automated assistant client 108 executing on client device 106 may actually engage with a logical instance of his or her own automated assistant 120. For brevity and simplicity, the term "automated assistant" as used herein when "servicing" a particular user refers to the combination of an automated assistant client 108 executing on a client device 106 operated by the user and one or more cloud-based automated assistant components 119 (which may be shared among multiple automated assistant clients 108). It should also be understood that in some implementations, the automated assistant 120 can respond to requests from any user, regardless of whether the user is actually "served" by a particular instance of the automated assistant 120.
For example, one or more client devices 106 may include one or more of the following: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a standalone interactive speaker (which may include a visual sensor in some cases), a smart appliance, such as a smart television (or a standard television equipped with a networked dongle with automated assistant capabilities), and/or a wearable apparatus of a user that includes a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided. Some client devices 106, such as a stand-alone interactive speaker (or "smart speaker"), may take the form of assistant devices that are designed primarily to facilitate a conversation between a user and the automated assistant 120. Some such assistant devices may take the form of a stand-alone interactive speaker with an attached display, which may or may not be a touch screen display.
As described in greater detail herein, the automated assistant 120 conducts human-to-machine dialog sessions with one or more users through user interface input and output devices of one or more client devices 106. In some implementations, the automated assistant 120 can engage in a human-to-machine conversation session with the user in response to user interface input provided by the user through one or more user interface input devices of one of the client devices 106. In some such implementations, the user interface input is explicitly directed to the automated assistant 120. For example, a user can verbally provide (e.g., type, say) a predetermined invocation phrase, such as "good, assistant" or "hey, assistant. When spoken, such spoken input may be captured by the microphone 109 and may cause the automated assistant 120 to begin actively listening or monitoring typed text. Additionally or alternatively, in some implementations, the automated assistant 120 can be invoked based on one or more detected visual cues (alone or in combination with a spoken invocation phrase).
In various implementations, the automated assistant 120 can utilize speech recognition to convert utterances from the user into text and respond to the text accordingly, e.g., by providing search results, general information, and/or taking one or more responsive actions (e.g., playing media, starting a game, ordering food, etc.). In some implementations, the automated assistant 120 can additionally or alternatively respond to the utterance without converting the utterance to text. For example, the automated assistant 120 can convert the speech input into an embedded, entity representation(s) (indicating entities/entity types present in the speech input) and/or other "non-text" representations, and operate in accordance with such non-text representations. Thus, embodiments described herein that operate based on text converted from speech input may additionally and/or alternatively operate directly from speech input and/or operate from other non-text representations of speech input.
Each of the computing device(s) operating cloud-based automated assistant component 119 and client computing device 106 can include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. The operations performed by the client computing device 106 and/or the automated assistant 120 can be distributed across multiple computer systems. For example, the automated assistant 120 can be implemented as computer programs running on one or more computers at one or more locations coupled to each other by a network.
As described above, in various implementations, the client computing device 106 may operate the automated assistant client 108 or a "client portion" of the automated assistant 120. In various implementations, the automated assistant client 108, which may be implemented using any combination of hardware and software, may interface with hardware, such as a microphone (not shown), to capture an audio recording of the user's utterance(s). Various types of processing may be performed on such audio recordings for various purposes.
The client device 106 may also install other applications, such as a web browser 111 and/or a message exchange client 113. Messaging client 113 may come in various forms. In some implementations, the messaging client 113 can appear as a short messaging service ("SMS") and/or multimedia messaging service ("MMS") client, an online chat client (e.g., instant messaging software, internet relay chat or "IRC," etc.), a messaging application associated with a social network, and the like. In some implementations, the message exchange client 113 can be implemented in a web page rendered by the web browser 111. In various implementations, the message exchange client 113 may provide an interface for a user to enter a typed or spoken human-machine conversation with the automated assistant 120, as a one-to-one conversation, or as a multi-participant conversation in which the automated assistant 120 may "participate". In some embodiments, the web browser 111 can be specially designed, for example, with a microphone button or other user interface element operable to invoke the automated assistant 120 so that a user can issue voice commands to assist in the operation of the web browser 111.
In some implementations, the automated assistant client 108 may be further configured to convert the captured audio to text and/or other representations or embedding, for example, using speech-to-text ("STT") processing techniques. Additionally or alternatively, in some implementations, the automated assistant client 108 can be configured to convert the text into computer synthesized speech, e.g., using one or more voice synthesizers. However, in some cases, because the client device 106 may be relatively limited in computing resources (e.g., processor cycles, memory, battery, etc.), the automated assistant client 108 local to the client device 106 may be configured to convert a limited number of different spoken phrases, particularly phrases that invoke the automated assistant 120, into text (or other forms, such as lower dimensionality embedding). Other speech inputs may be sent to a cloud-based automated assistant component 119, which may include a cloud-based text-to-speech ("TTS") module 116 and/or a cloud-based STT module 117.
Cloud-based TTS module 116 may be configured to utilize the nearly unlimited resources of the cloud to convert text data (e.g., natural language responses formulated by automated assistant 120) into computer-generated speech output. In some implementations, the TTS module 116 can provide the computer-generated speech output to the client device 106 for direct output, e.g., using one or more speakers. In other implementations, textual data (e.g., natural language responses) generated by the automated assistant 120 can be converted to computer-generated speech at the client device 106.
The cloud-based STT module 117 may be configured to convert the captured audio data into text using the nearly unlimited resources of the cloud, and then the text is provided to the intent matcher 135. In some implementations, the cloud-based STT module 117 can convert an audio recording of speech to one or more phonemes and then convert the one or more phonemes to text. Additionally or alternatively, in some embodiments, the STT module 117 may employ a state-decoding graph. In some implementations, the STT module 117 can generate a plurality of candidate text interpretations of the user's utterance. In some implementations, the STT module 117 may give a higher weight or bias to a particular candidate text interpretation than to other interpretations depending on whether there are simultaneously detected visual cues.
The automated assistant 120 (particularly the cloud-based automated assistant component 119) can include an intent matcher 135, the TTS module 116 described above, the STT module 117 described above, and other components described in more detail below. In some implementations, the modules and/or one or more of the modules of the automated assistant 120 can be omitted, combined, and/or implemented in a component separate from the automated assistant 120. In some implementations, to protect privacy, one or more components of the automated assistant 120, such as the natural language processor 122, the TTS module 116, the STT module 117, and/or the like, may be implemented at least partially on the client device 106 (e.g., excluding the cloud).
In some implementations, the automated assistant 120 generates response content in response to various inputs generated by a user of one of the client devices 106 during a human-to-machine conversation session with the automated assistant 120. The automated assistant 120 can provide responsive content (e.g., over one or more networks when separate from the user's client device) for presentation to the user as part of a conversation session. For example, the automated assistant 120 can generate response content in response to free-form natural language input provided by the client device 106. As used herein, a free-form natural language input is an input that is formulated by a user and is not limited to a set of options presented for selection by the user. The free-form natural language input may be spoken (and captured by microphone 109) and/or typed (e.g., into one or more interfaces provided by one or more applications, such as message exchange client 113).
As used herein, a "conversation session" may include a logically self-contained exchange of one or more messages between a user and the automated assistant 120 (and in some cases, other human participants). The automated assistant 120 can distinguish between multiple conversation sessions with the user based on various signals, such as passage of time between sessions, changes in user context (e.g., location, before/during/after a scheduled meeting, etc.) between sessions, detecting one or more intervening interactions between the user and a client device rather than a conversation between the user and the automated assistant (e.g., user switching application for a period of time, user leaving, then returning a separate voice control product), locking/hibernating a client device between sessions, changes in a client device for interfacing with one or more instances of the automated assistant 120, and so forth.
The intent matcher 135 may be configured to determine the intent of the user based on input(s) provided by the user (e.g., spoken utterances, visual cues, etc.) and/or based on other signals, such as sensor signals, online signals (e.g., data obtained from a network service), etc. In some implementations, the intent matcher 135 can include the natural language processor 122 and the entity module 112. The natural language processor 122 may be configured to process natural language input generated by the user(s) through the client device 106 and may generate annotated output (e.g., in textual form) for use by one or more other components of the automated assistant 120. For example, the natural language processor 122 may process natural language free-form natural language input generated by a user through one or more user interface input devices of the client device 106. The generated annotated output includes one or more annotations of the natural language input and one or more (e.g., all) terms of the natural language input.
In some implementations, the natural language processor 122 is configured to identify and annotate various types of grammar information in the natural language input. For example, the natural language processor 122 may include a morphology module that may separate individual words into morphemes and/or annotate morphemes, e.g., in their classes. The natural language processor 122 may also include portions of a part-of-speech tagger configured to annotate terms with their grammatical actions. For example, a part-of-speech tagger may tag each term with its part-of-speech (such as "noun," "verb," "adjective," "pronoun," etc.). Also, for example, in some embodiments, the natural language processor 122 may additionally and/or alternatively include a dependency parser (not shown) configured to determine syntactic relationships between terms in the natural language input. For example, the dependency analyzer may determine which terms modify other terms, subjects and verbs of sentences, etc. (e.g., parse trees), and may annotate such dependencies.
In some implementations, the entity module 112 (also referred to as an "entity tagger") can be configured to perform techniques such as named entity recognition to identify entities and/or entity types conveyed by a user's spoken and/or typed input. In some implementations, the entity module 112 can annotate entity/entity type references in one or more segments, such as references to people (e.g., including literary characters, celebrities, public personalities, etc.), organizations, locations (both real and fictional), and so forth. In some implementations, data about entities may be stored in one or more databases, such as in a knowledge graph ("KG") 115. In some implementations, the knowledge graph 115 can include nodes representing known entities (and in some cases entity attributes) and edges connecting the nodes and representing relationships between the entities. For example, a "banana" node may be connected (e.g., as a child node) to a "fruit" node, which in turn may be connected (e.g., as a child node) to a "produce" and/or "food" node. As another example, a restaurant named "Hypothetical Caf" may be represented by a node that also includes attributes such as its address, type of food served, hours, contact information, and the like. In some implementations, a "contextual Caf" node may be connected by an edge (e.g., representing a relationship between a child node and a parent node) to one or more other nodes, such as a "restaurant" node, a "business" node, a node representing a city and/or state in which the restaurant is located, and so on.
The entity module 112 may annotate references to entities or entity types at a high level of granularity (e.g., to enable identification of all references to entity classes (such as people)) and/or a low level of granularity (e.g., to enable identification of all references to a particular entity (such as a particular person)). The entity module 112 may rely on the content of the natural language input to resolve a particular entity and/or may optionally communicate with the knowledge graph 115 or other entity database to resolve a particular entity.
In some implementations, the natural language processor 122 can additionally and/or alternatively include a co-reference parser (not shown) configured to group or "cluster" references to the same entity based on one or more contextual hints. For example, a coreference parser may be used to parse the term "there" into a natural language input "I prefer the Hypothetical cafe," the Hypothetical cafe "in" there that we have last eaten.
In some implementations, one or more components of the natural language processor 122 may rely on annotations from one or more other components of the natural language processor 122. For example, in some implementations, the entity module 112 may rely on annotations from the co-referred parser and/or the dependent parser in annotating all references to a particular entity or entity type. Also, for example, in some implementations, the coreference parser may rely on annotations from the dependency parser when clustering references to the same entity. In some implementations, in processing a particular natural language input, one or more components of the natural language processor 122 may determine one or more annotations using related prior inputs and/or other related data in addition to the particular natural language input.
The intent matcher 135 may use various techniques to determine the intent of the user, e.g., based on output from the natural language processor 122 (which may include annotations and terms of natural language input) and/or based on output from the entity module 112. In some implementations, the intent matcher 135 can access one or more databases (not shown) that include, for example, a plurality of mappings between grammars, visual cues, and responsive actions (or more generally intents). In many cases, these grammars may be selected and/or learned over time and may represent the most common intent of the user. For example, a grammar "Play < artist >" may map to an intent to invoke a response action that causes music < artist > to be played on the client device 106 operated by the user. Another grammar "weather today | forecast" may be able to match user queries, such as "how do weather today" and "how do weather forecast today? ". In some implementations, the mappings may include mappings between entities and candidate response actions that may be performed in association with the entities.
In addition to or instead of grammars, in some embodiments, the intent matcher 135 may employ one or more trained machine learning models, either alone or in combination with one or more grammars and/or visual cues. These trained machine learning models may also be stored in one or more databases and may be trained to identify intents, for example, by embedding data indicative of the user's utterances and/or any detected user-provided visual cues into the underlying space, and then determining which other embeddings are closest (and thus which intents are closest), for example, using techniques such as euclidean distance, cosine similarity, and the like.
As seen in the "play < artist >" example grammar, some grammars have slots (e.g., < artist >) that can be filled with slot values (or "parameters"). The slot value may be determined in various ways. Typically, the user will actively provide the slot value. For example, for the grammar "you've < topping > pizza", the user might say the phrase "you've sausage pizza", in which case the tank < topping > is automatically filled. Additionally or alternatively, if the user invokes a grammar that includes slots to be filled with slot values without the user actively providing the slot values, the automated assistant 120 may ask the user for the slot values (e.g., "what type of crust your pizza wants.
Fulfillment (or "parsing") information may take various forms, as the intent may be fulfilled (or "parsed") in various ways. Suppose that the user requests pure information, such as, "where is the outdoor shot of' flash? ". The user's intent (e.g., via intent matcher 135) may be determined as a search query. The intent and content of the search query may be provided to a fulfillment module 124, as shown in fig. 1, which fulfillment module 124 may be in communication with one or more search modules 150, which search modules 150 are configured to search a corpus of documents and/or other data sources (e.g., knowledge graphs, etc.) for responsive information. The fulfillment module 124 may provide data indicative of the search query (e.g., text of the query, dimension reduction embedding, etc.) to the search module 150. The search module 150 may provide responsive information, such as GPS coordinates, or other more explicit information, such as "Timberline Lodge, mt. Such response information may form part of the fulfillment information generated by fulfillment module 124.
Additionally or alternatively, fulfillment module 124 may be configured to receive the user's intent (e.g., from intent matcher 135) and any slot values provided by or determined using other means (e.g., the user's GPS coordinates, user preferences, etc.) by the user, and trigger responsive actions. For example, the responsive action may include ordering goods/services, starting a timer, setting a reminder, initiating a phone call, playing media, sending a message, and so forth. In some such embodiments, fulfillment information may include slot values associated with fulfillment, confirmation responses (which may be selected from predetermined responses in some cases), and so forth.
The natural language generator 126 may be configured to generate and/or select a natural language output (e.g., words/phrases designed to mimic human speech) based on data obtained from various sources. In some implementations, the natural language generator 126 may be configured to receive as input fulfillment information associated with fulfillment of the intent, and generate a natural language output based on the fulfillment information. Additionally or alternatively, the natural language generator 126 may receive information from other sources, such as third party applications (e.g., required slots), which may be used to compose natural language output for the user.
Various aspects of the disclosure may be implemented in whole or in part by the action recommendation engine 128 and/or the filtering module 110. Any of these components may be implemented in whole or in part in any of client device 106 or cloud-based automated assistant component 119. In general, the action recommendation engine 128 may be configured to receive data (e.g., from the entity module 112) indicative of an entity or entity type conveyed in input provided at the client device 106. The action recommendation engine 128 may then identify a plurality of candidate response actions (e.g., from a database 129 of mappings between entities/entity types and response actions) that are performable based on the entity. In some cases, the action recommendation engine 128 may be part of the intent matcher 135.
In this manner, if the user wishes to know which actions may be performed in association with a particular entity, the user may identify the entity only in spoken and/or typed free form natural language input provided to the automated assistant 120. The automated assistant 120 may identify an entity or entity type (e.g., via the entity module 112), and may then identify a plurality of candidate responsive actions (e.g., via the action recommendation engine 128) that may be performed in association with the identified entity or entity type.
In many cases, at least some of these candidate response actions may not be applicable to a particular situation. For example, the one or more candidate response actions may be invoking an application (or "app") that is not installed on the client device 106. Additionally, the current context of the user of the client device 106 may make at least some candidate response actions inapplicable. For example, if the user is located in san francisco and the identified entity is located in new york, some responsive actions associated with the identified entity (such as calling a car to the location of the entity) may not be applicable. In some implementations, multiple candidate response actions may be potentially applicable, but some candidate response actions are more likely to satisfy the user than others.
Accordingly, in various embodiments, the filtering module 110 may be configured to rank and/or filter the plurality of candidate responsive actions generated by the action recommendation engine 128. In fig. 1, such ranking/filtering is performed on the client device 106, but in other implementations, the ranking and/or filtering may be performed elsewhere, e.g., as part of the cloud-based automated assistant component 119. In various implementations, a plurality of different filters may be applied by the filtering module 110, where each filter is configured to eliminate one or more candidate response actions from consideration for the user recommended to the client device 106. An example of multiple filters being applied is demonstrated in fig. 2.
Referring now to FIG. 2, an example of how data may flow between various components of FIG. 1 is schematically depicted. Starting from the top left, a partial free form natural language input ("FFNLI") may be received from a user at an input component of the client device 106. The partial natural language input may identify the entity without identifying a responsive action (e.g., a command) and may be directed by the user to the automated assistant 120. Data indicative of the entity (e.g., text entered by the user, speech recognition output generated based on an utterance of the user, embedded) can be provided by the client device 106 to the entity module 112.
The entity module 112 can perform techniques such as named entity recognition ("NER") to identify (e.g., from a knowledge graph) one or more entities or entity types corresponding to the user's free-form natural language input. In some implementations, the identified entities or entity types can be ranked based on relevance of the entities to the requesting user, among other factors. Entities or entity types that are more closely consistent with the user's context, interests, location, etc. may be ranked higher than other entities or entity types that are less tied to the user. In some embodiments, these entity rankings may be used downstream, for example, to rank and/or filter candidate response actions.
These identified entity(s) or entity type(s) may be provided to the action recommendation engine 128. The action recommendation engine 128 may query the database 129 based on the identified entity(s) or entity type(s) to identify a superset of candidate responsive actions. Data indicative of a superset of candidate response actions may then be provided (e.g., by the action recommendation engine 128) to the filtering module 110. The filtering module 110 may apply some number of filters 231 1 、231 2 、231 3 A system to filter at least some of the response actions of the superset from consideration for the output, e.g., by the automated assistant 120, as recommended response actions.
Additionally or alternatively, the filtering module 110 (or in some cases, the action recommendation engine 128) may use various different signals to rank all candidate response actions of the superset, or rank those candidate response actions that still exist after filtering. For example, these signals may include the previously described entity rankings, context signals, relevance scores, and the like.
In various embodiments, there are more or less than the three filters 231 depicted in FIG. 2 1 、231 2 、231 3 The filters of the idle. Each filter 231 i The candidate response actions may be filtered for consideration and recommendation based on different context signals and/or cues. For example, the first filter 231 1 May be applied by the filtering module 110 to filter candidate response actions based on the current location of the client device 106. Assume that the client device 106 is located in japan and the user provides the entity "empire building" as a free-form natural language input directed at the automated assistant 120. Because this landmark is located in new york, far from japan, it makes no sense to order a ride share in the empire building. Thus, the candidate response action "booking a ride share to the empire country building" associated with the empire country building may be eliminated from consideration as a recommendation.
Second Filter 231 2 May be applied by the filtering module 110 to filter candidate response actions based on the current state of the application executing on the client device 106. For example, if the user has been traveling in a car pool to the empire country's building, it would make no sense to give a recommendation "reserve car pool to empire country's building", and therefore the candidate response action may be eliminated from consideration.
Third filter 231 3 May be applied by the filtering module 110 to filter candidate response actions based on a library of applications installed on the client device 106 or otherwise available to a user of the client device 106. For example, if streaming app a is not installed on the client device 106, or if the user does not have an account on streaming app a, the candidate response action "streams on streaming app a<Name of movie>"has no meaning. In other implementations, the action recommendation engine 128 may generate the list of candidate responsive actions based on an application installed on the client device 106, in which case the filtering module 110 does notIt is necessary to filter out the uninstalled applications (therefore, the third filter 231 may be omitted 3 )。
The filtered results may then be provided to an automated assistant 120, e.g., automated assistant client 108 and/or cloud-based automated assistant component(s) 119. In either case, the automated assistant 120 (e.g., via the action recommendation engine or filtering module 110) can rank the remaining candidate response actions using various factors described above (e.g., entity rankings, relevance scores, etc.). The automated assistant 120 can then provide an output (computer-generated verbal or textual output) that recommends one or more candidate response actions, such as the top n ranked candidate response actions (n being a positive integer).
Fig. 3-9 depict example scenarios in which techniques described herein are performed to provide recommended response actions to a user based on an entity or entity type provided by the user. Referring now to fig. 3, a client device 306 in the form of a smartphone or tablet computer provides an interface to interact with an automated assistant (e.g., as part of message exchange client 113). The user may provide free-form natural language input to the automated assistant 120 by operating the keyboard 330 (hardware or "soft" keyboard) and/or by speaking into a microphone (not shown) and processing a recording of the utterance into text via the STT.
In this example, the user has provided a partial free form natural language input ("casalb") into input field 332, which input field 332 partially identifies the entity "Casablanca. The user has not provided any command or response action identifier. Thus, the text "Casabl" is automatically completed as "Casablanca" and parsed into the same name movie or moroccan city, e.g., by the entity module 112. In various implementations, the automatic completion of the entity name may be performed based on, for example, an analysis of logs of free-form natural language input that are provided to the automated assistant 120 by a user in general and/or to the automated assistant 120 by a user of the client device 306 in particular.
Based on the movie named "Casablanca" and the northern non-same-name city, a superset of candidate response actions may be identified, for example, by the action recommendation engine 128. These candidate response actions may be ranked and/or filtered, for example, by the action recommendation engine 128 and/or the filtering module 110, until the automated assistant 120 leaves a subset of candidate response actions that may be appropriate for the particular client device 306 and/or the user controlling it.
In this example, the four candidate response actions of the subset (which may include some or all of the subset) are presented to the user as recommendations. The recommendations may be ranked, for example, from most likely to least likely response. In some implementations, each recommendation can be actionable (e.g., clickable) causing the automated assistant 120 to perform the responsive action, e.g., as if the user originally provided the free-form natural language output. Although depicted as text links in fig. 3, in various embodiments, these candidate response actions/recommendations may alternatively be presented as graphical icons, deep links, and the like.
The first candidate response action is to watch the movie "Casablanca" on streaming app a. The second candidate response action is to purchase a ticket for the movie to be shown. A third candidate response action is to view the movie on a different streaming app B that is also installed on the client device 306 (or at least accessible to the user). A fourth candidate response action is to book a flight to city Casablanca.
The order or ranking in which the four candidate responsive actions are presented may be determined based on a variety of different context and/or historical signals. Depending on the context, unless the user is located at or near the city Casablanca (or in/near the country morocco), the most responsive candidate action for that city may be the purchase of a ticket to that city. However, unless there is other evidence that the city Casablanca is relevant to the user (e.g., the user has recently performed a network search on Casablanca, morocco, africa, etc.), the candidate response action may be ranked behind other candidate response actions, particularly, for example, where the user has known preferences or interests in the movie/movie production. On the other hand, if the user is located in or near morocco, other candidate response actions not presented in fig. 4 may instead be presented, such as booking a ride share to/from Casablanca, booking a train ticket to/from Casablanca, and so on.
For the first three candidate response actions, two streaming apps are presented, streaming app a and streaming app B, indicating that both are either installed on the client device 306 or generally accessible to the user of the client device 306. If the movie "Casablanca" is not viewable on one of the streaming apps, the streaming app may not be presented at all. As far as streaming app a is ranked higher than streaming app B, it may be the case that: the frequency with which the user (or generally a plurality of users) uses streaming app a is higher than streaming app B, or the time in which the user uses streaming app a is closer to the time in which streaming app B is used.
The second candidate response action, purchasing a ticket for a "Casablanca," may be ranked where it is located because, for example, the movie is scheduled to be shown soon in a movie theater near the user's residence and/or near the location of the client device (e.g., determined using GPS, wireless triangulation, social media check-in, etc.). The second candidate response action may be presented below the first candidate response action because, for example, watching a movie on streaming app a may be cheaper (e.g., free) than purchasing a ticket.
FIG. 4 depicts what may happen when the user selects the candidate response action in FIG. 3. In this example, the user selects the first recommendation "play" Casablanca "on streaming app a. Thus, the automated assistant 120 replies: "of course, there is no problem. Do you want to view on this device or stream to another device "? The user answers: "Room television (Den TV)". The automated assistant 120 then replies: "good, now streaming. ", and the movie begins streaming on a television set (not shown).
Fig. 5 depicts another example of a presentation using the same client device 306 as depicted in fig. 3. In FIG. 5, the user speaks or types a partial free form natural language input "JFK". As before, the token "JFK" may be resolved (e.g., by the entity module 112) to one or more entities in the knowledge graph 115, such as new york airport, president, and/or a movie of the same name, to name a few. As before, a superset of candidate response actions may be identified, e.g., by the action recommendation engine 128, such as actions related to airports (e.g., booking flights to and fro, booking taxis to and from, etc.), actions related to presidents (e.g., searching for information), actions related to movies (e.g., streaming the movie over various streaming apps), and so forth.
The superset of candidate response actions may be ranked and/or filtered until four candidate response actions are left in fig. 5 to be presented as recommendations. In this example, it may be assumed that the user and/or client device 306 is located near the JFK airport, which would result in the candidate response actions associated with the JFK airport being ranked higher than would otherwise be the case. The first recommendation is to purchase a flight for airline A to take off from JFK, and the second recommendation is to purchase a flight for airline B to take off from JFK. Airline a may be ranked higher than airline B for various reasons, such as the user taking flights of airline a more frequently or taking airline a closer in time. Additionally or alternatively, airline a's flights may be cheaper and/or may be more consistent with the user's schedule (e.g., determined from the user's online calendar).
The third recommendation and the fourth recommendation are to book a ride to JFK using two different ride sharing applications. As before, the user may have used ride share app a more recently in time, or may use it more often than ride share app B. The car pool recommendations are ranked lower than the flight purchase recommendations because, for example, the user may not have scheduled a flight to take off from JFK yet, and the user may generally tend to purchase airline tickets long before the flight takes off, while the user tends to book a car pool to the airport closer to the flight time (e.g., the day). On the other hand, if the user has purchased a flight from JFK, the ticket purchase recommendation may not appear at all (e.g., due to being filtered by the filtering module 110), as shown in fig. 6. Alternatively, the ticket purchase recommendations may be ranked lower than the ride share application recommendations, particularly if an already booked flight is about to take off.
Assume now that the user of client device 306 is in a different context than that of fig. 5-6. For example, assume that the user is not near JFK airport, but is, for example, in san francisco. In this case, entity JFK may be resolved or ordered in a different manner (e.g., by entity module 112) than that shown in fig. 5 and 6, e.g., as shown in fig. 7. In fig. 7, four recommendations are again presented, but the recommendations are different because the user/client device 306 is not near the JFK airport. The first recommendation and the second recommendation are to use airline a and airline B, respectively, to purchase flights to JFK (as opposed to take off from JFK). The two recommendations may again be relatively ranked based on a signal of how often and/or when the user, such as client device 306, used the airlines.
The third and fourth options are to view the movie "JFK" on streaming app a and streaming app B, respectively. These options may appear while they do not appear in fig. 5 or fig. 6, because JFK movies may be more relevant when the user is not near the JFK airport. As with the previous example, in this example, the streaming apps may be relatively ordered based on various signals such as how often the user used them or when they were recently used.
Fig. 8 depicts another client device 806 in the form of a smart watch that may or may not communicate with another device, such as a smartphone (not shown), using a Personal Area Network (PAN), such as bluetooth. Client device 806 may include a variety of different physiological sensors (not shown), such as a heart rate monitor, thermometer, etc., that obtain physiological measurements from a user (not shown). In fig. 8, the user has typed or spoken a partial free form natural input "RIDE". This may mean that the user intends to start exercising, but has not yet started, nor has the app been used to monitor his or her exercise.
In fig. 8, entity type rid is parsed (e.g., by entity module 112) using data and/or named entity recognition in knowledge graph 115 to identify the type of rid entity, such as, for example, cycling, motorcycling, scouting, skateboarding, booking a RIDE, and the like. The entities may be ranked based on various factors, such as knowing that a user has a habit of riding (e.g., through operation of a riding application over a period of time) and/or using a ride share app with a certain frequency.
The entity types "car share" and "ride" may be determined (e.g., by the entity module 112) and passed to the action recommendation engine 128. The action recommendation engine 128 may examine the mapping(s) between these entity types and various candidate response actions to generate a superset of candidate response actions, as previously described. Through the superset, the two most contextually relevant candidate response actions are presented to the user as recommendations. The first is to record the bicycle riding with app a. The second is to book the ride with app B. In contrast to the previous figures, in FIG. 8, the graphical element 866 is recommended as selectable 1-2 Instead of text links. Also, because client device 806 is a watch with less screen space than client device 306, fewer recommendations may be presented to save screen space.
Fig. 9 depicts the same client device 806 as fig. 8 in a slightly different context. In fig. 9, it may be assumed that the user is already exercising, e.g., riding a bicycle. While the same superset of candidate response actions may or may not be identified by the action recommendation engine 128, different candidate response actions may be presented to the user as recommendations (and other recommendations, such as the one shown in FIG. 8, may be filtered) to account for changes in context. For example, the first selectable element 966 1 Recommended, and optional, causes automated assistant 120 to implement a context-dependent command "stop recording bicycle rides with app a". Second selectable element 966 2 Recommended, and optional, causes automated assistant 120 to implement a context-dependent command "pause recording bicycle riding with app a". A third selectable element is recommended, and is selectable,causing the automated assistant 120 to implement the third context-related command "share location using app a".
Fig. 9 illustrates one example of how APIs associated with various apps may be exposed to the automated assistant 120. This allows the automated assistant 120 to execute application-internal commands, such as the commands shown in fig. 9, and in contrast to the previous example, where the automated assistant 120 invoked the entire application.
Fig. 10 depicts an example method 1000 of practicing selected aspects of the present disclosure, in accordance with various embodiments. For convenience, the operations of the flow diagrams are described with reference to a system that performs the operations. The system may include various components of various computer systems, including the automated assistant 120, the action recommendation engine 128, and/or the filtering module 110. Further, while the operations of method 1000 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.
In block 1002, the system may receive a partial natural language input from a user at an input component of a computing device. In various embodiments, the partial natural language input may identify an entity or entity type without identifying a responsive action. The partial free-form natural language input may be directed by a user to an automated assistant (e.g., 120) operating at least in part on a computing device.
In block 1004, the system (e.g., by the natural language processor 122 and/or the entity module 112) may analyze the free-form natural language input to identify an entity or entity type, e.g., identify an entity or entity type in the knowledge graph 115 using techniques such as named entity recognition.
Based on the identified entity or entity type, in block 1006, the system may identify a plurality or superset of candidate response actions. Such a superset of candidate response actions may include candidate response actions that may or may not be context-dependent. Accordingly, in block 1008, the system (e.g., by the filtering module 110) may filter one or more candidate response actions in the superset from consideration for recommendation to the user.
In block 1010, the system (e.g., by the filtering module 110 or the action recommendation engine 128) may rank at least some of the plurality of candidate response actions based on one or more signals, such as entity rankings, contextual hints (e.g., location, time of day, calendar entries, social media status, etc.), and the like. In some implementations, blocks 1006 and 1008 can be reversed to order the superset and then apply the filter. In other implementations, no filter may be applied, instead, the superset may be ranked based on context cues, and the x most relevant/responsive candidate response actions (x being a positive integer, such as 1, 2, 3....). The remainder of the superset of candidate response actions may actually be filtered out as not being within the first x candidate response actions.
In block 1012, the system may cause the automated assistant 120 to provide an output recommending one or more candidate response actions to select based on the ranking. Examples of such outputs are depicted in fig. 3-9.
Where certain embodiments discussed herein may collect or use personal information about a user (e.g., user data extracted from other electronic communications, information about the user's social network, the user's location, the user's time, the user's biometric information, as well as the user's activity and demographic information, relationships between users, etc.), one or more opportunities may be provided to the user to control: whether to collect information, whether to store personal information, whether to use personal information, and how to collect, store, and use information about a user. That is, the systems and methods discussed herein collect, store, and/or use user personal information only upon receiving explicit authorization to do so from an associated user.
For example, the user is provided with control over whether the program or feature collects user information about that particular user or other users related to the program or feature. Each user for whom personal information is to be collected is presented with one or more options to allow control of information collection associated with that user to provide permission or authorization as to whether information is collected and as to what portions of the information are to be collected. For example, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways before it is stored or used, thereby removing personally identifiable information. As one example, the identity of the user may be processed such that personally identifiable information cannot be determined. As another example, the geographic location of a user may be generalized to a larger area such that the specific location of the user cannot be determined.
Fig. 11 is a block diagram of an example computing device 1110, which example computing device 1110 may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing device, the action recommendation engine 128, the automated assistant 120, and/or other component(s) may include one or more components of the example computing device 1110.
The user interface input devices 1122 may include a keyboard, a pointing device (such as a mouse, trackball, touchpad, or graphical tablet), a scanner, a touch screen incorporated into the display, an audio input device (such as a voice recognition system, microphone), and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information into computing device 1110 or onto a communication network.
User interface output devices 1120 may include a display subsystem, a printer, a facsimile machine, or a non-visual display, such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide a non-visual display, such as through an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 1110 to a user or to another machine or computing device.
These software modules are typically executed by the processor 1114 alone or in combination with other processors. Memory 1125 for storage subsystem 1124 may include a number of memories, including a main Random Access Memory (RAM) 1130 for storing instructions and data during program execution and a Read Only Memory (ROM) 1132 which stores fixed instructions. File storage subsystem 1126 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical disk drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by the file storage subsystem 1126 in the storage subsystem 1124 or in other machines accessible by the processor(s) 1114.
The computing device 1110 may be of various types, including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Because the nature of computers and networks vary, the description of computing device 1110 depicted in FIG. 11 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 1110 are possible with more or fewer components than the computing device shown in FIG. 11.
While various embodiments have been described and illustrated herein, various other ways and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teaching(s) is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, embodiments other than those specifically described and claimed may be practiced. Embodiments of the present disclosure are directed to various individual features, systems, articles, materials, kits, and/or methods described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (20)
1. A method of using one or more processors, comprising:
receiving, at an input component of a computing device, partial free form natural language input from a user, wherein the partial free form natural language input identifies an entity without identifying a responsive action and is directed by the user to an automated assistant operating at least in part on the computing device;
analyzing the partial free-form natural language input to identify the entity;
identifying a plurality of candidate response actions based on the identified entity;
ranking at least some of the plurality of candidate responsive actions based on one or more signals; and
cause the automated assistant to provide an output recommending one or more of the candidate response actions selected based on the ranking.
2. The method of claim 1, wherein the plurality of candidate response actions includes a plurality of applications installed on or available through the computing device, and the one or more signals include:
a time at which each of the plurality of applications was most recently used by the user; or alternatively
A frequency with which each of the plurality of applications is used by the user.
3. The method of claim 1 or 2, further comprising: filtering one or more of the plurality of candidate response actions from consideration for output based on a current context of the user, wherein the current context is determined based on one or more context signals.
4. The method of claim 3, wherein the one or more context signals include a state of a given application executing at least in part on the computing device, and the filtering comprises: filtering another application of the same application type as the given application or the given application from consideration for output.
5. The method of claim 4, wherein the application type comprises a ride share application, and the status of the given application indicates that the user has traveled as part of a ride share.
6. The method of any of claims 3 to 5, wherein the one or more context signals include a state of a given application executing at least in part on the computing device, and the filtering comprises: filtering first response actions available through the given application from consideration for output.
7. The method of any of claims 3 to 6, wherein the entity is a location, the one or more context signals include a distance between a current location of the user and the location, and the filtering comprises: filtering one or more of the plurality of candidate responsive actions from consideration for output based on the distance.
8. The method of any of claims 1-7, wherein one or more of the plurality of candidate response actions are identified or ordered based on a state of a given application executing at least in part on the computing device.
9. The method of claim 8, wherein the given application comprises an exercise application, the state of the given application indicates that the user is currently exercising, and the one or more of the plurality of candidate response actions identified or ordered based on the state of the given application comprises ceasing to monitor the user's exercise.
10. The method of claim 8 or 9, wherein the given application comprises a ride share application, the state of the given application indicates that the user is currently traveling as part of a ride share, and the one or more of the plurality of candidate response actions identified or ranked based on the state of the given application comprise:
altering a destination of the user; or
Causing a communication to be sent to another user, wherein the communication indicates a current location or an estimated time of arrival of the user.
11. A system comprising one or more processors and memory storing instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to:
receiving, at an input component of a computing device, partial free form natural language input from a user, wherein the partial free form natural language input identifies an entity without identifying a responsive action and is directed by the user to an automated assistant operating at least in part on the computing device;
analyzing the partial free-form natural language input to identify the entity;
identifying a plurality of candidate response actions based on the identified entity;
ranking at least some of the plurality of candidate response actions based on one or more signals; and
causing the automated assistant to provide an output recommending one or more of the candidate response actions selected based on the ranking.
12. The system of claim 11, wherein the plurality of candidate response actions includes a plurality of applications installed on or available through the computing device, and the one or more signals include:
a time at which each of the plurality of applications was most recently used by the user; or
A frequency with which each of the plurality of applications is used by the user.
13. The system of claim 11 or 12, further comprising instructions to: filtering one or more of the plurality of candidate response actions from consideration for output based on a current context of the user, wherein the current context is determined based on one or more context signals.
14. The system of claim 13, wherein the one or more context signals comprise a state of a given application executing at least in part on the computing device, and the system comprises instructions to: filtering another application of the given application or application type that is the same as the given application from consideration for output.
15. The system of claim 14, wherein the application type comprises a ride share application, and the status of the given application indicates that the user has traveled as part of a ride share.
16. The system of any of claims 13 to 15, wherein the one or more context signals include a state of a given application executing at least in part on the computing device, and the system includes instructions to: filtering first response actions available through the given application from consideration for output.
17. The system of any of claims 13 to 16, wherein the entity is a location, the one or more context signals include a distance between a current location of the user and the location, and the system includes instructions to: filtering one or more of the plurality of candidate responsive actions from consideration for output based on the distance.
18. The system of any of claims 11 to 17, wherein one or more of the candidate response actions are identified or ordered based on a state of a given application executing at least in part on the computing device.
19. The system of claim 18, wherein the given application comprises an exercise application, the state of the given application indicates that the user is currently exercising, and the one or more of the candidate response actions identified or ordered based on the state of the given application comprise ceasing to monitor the user's exercise.
20. A method of using one or more processors, comprising:
receiving, at an input component of a computing device, a partial free form natural language input from a user, wherein the partial free form natural language input identifies an entity without identifying a responsive action and is directed by the user to an automated assistant operating at least in part on the computing device;
analyzing the partial free-form natural language input to identify the entity;
identifying a superset of candidate response actions based on the identified entity;
filtering one or more candidate responsive actions of the superset of candidate responsive actions from consideration for output based on a current context of the user, wherein the current context is determined based on one or more context signals; and
causing the automated assistant to provide an output recommending one or more of the candidate response actions that still exist after the filtering.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063104400P | 2020-10-22 | 2020-10-22 | |
US63/104,400 | 2020-10-22 | ||
US17/082,580 US11790173B2 (en) | 2020-10-22 | 2020-10-28 | Recommending action(s) based on entity or entity type |
US17/082,580 | 2020-10-28 | ||
PCT/US2021/054671 WO2022086765A1 (en) | 2020-10-22 | 2021-10-13 | Recommending action(s) based on entity or entity type |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115605871A true CN115605871A (en) | 2023-01-13 |
Family
ID=81257397
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180035381.5A Pending CN115605871A (en) | 2020-10-22 | 2021-10-13 | Recommending actions based on an entity or entity type |
Country Status (4)
Country | Link |
---|---|
US (2) | US11790173B2 (en) |
EP (1) | EP4128012A1 (en) |
CN (1) | CN115605871A (en) |
WO (1) | WO2022086765A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220317635A1 (en) * | 2021-04-06 | 2022-10-06 | International Business Machines Corporation | Smart ecosystem curiosity-based self-learning |
Family Cites Families (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102483753A (en) * | 2009-05-27 | 2012-05-30 | 谷歌公司 | Computer Application Data In Search Results |
US20140358958A1 (en) | 2013-05-29 | 2014-12-04 | Microsoft Corporation | Surfacing direct app actions |
US9767091B2 (en) * | 2015-01-23 | 2017-09-19 | Microsoft Technology Licensing, Llc | Methods for understanding incomplete natural language query |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US10180965B2 (en) | 2016-07-07 | 2019-01-15 | Google Llc | User attribute resolution of unresolved terms of action queries |
US10552544B2 (en) * | 2016-09-12 | 2020-02-04 | Sriram Chakravarthy | Methods and systems of automated assistant implementation and management |
US10268680B2 (en) * | 2016-12-30 | 2019-04-23 | Google Llc | Context-aware human-to-computer dialog |
US10817517B2 (en) | 2017-01-31 | 2020-10-27 | Boomi, Inc. | System facilitating user access to enterprise related data and methods thereof |
US10142222B1 (en) * | 2017-06-13 | 2018-11-27 | Uber Technologies, Inc. | Customized communications for network systems |
US10782986B2 (en) | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
CN112055857A (en) | 2018-05-02 | 2020-12-08 | 三星电子株式会社 | Contextual recommendation |
KR102523982B1 (en) | 2018-08-21 | 2023-04-20 | 구글 엘엘씨 | Dynamic and/or context-specific hot words to invoke automated assistants |
EP3770702A1 (en) * | 2018-12-03 | 2021-01-27 | Google LLC | Efficient control and/or linking of smart devices |
DK180649B1 (en) * | 2019-05-31 | 2021-11-11 | Apple Inc | Voice assistant discoverability through on-device targeting and personalization |
US11961509B2 (en) * | 2020-04-03 | 2024-04-16 | Microsoft Technology Licensing, Llc | Training a user-system dialog in a task-oriented dialog system |
US11610065B2 (en) * | 2020-06-12 | 2023-03-21 | Apple Inc. | Providing personalized responses based on semantic context |
US20210406738A1 (en) * | 2020-06-29 | 2021-12-30 | International Business Machines Corporation | Methods and systems for providing activity feedback utilizing cognitive analysis |
US11455996B2 (en) * | 2020-07-27 | 2022-09-27 | Google Llc | Automated assistant adaptation of a response to an utterance and/or of processing of the utterance, based on determined interaction measure |
US11410192B2 (en) * | 2020-08-27 | 2022-08-09 | Google Llc | Group action fulfillment across multiple user devices |
-
2020
- 2020-10-28 US US17/082,580 patent/US11790173B2/en active Active
-
2021
- 2021-10-13 WO PCT/US2021/054671 patent/WO2022086765A1/en unknown
- 2021-10-13 CN CN202180035381.5A patent/CN115605871A/en active Pending
- 2021-10-13 EP EP21814954.0A patent/EP4128012A1/en active Pending
-
2023
- 2023-05-08 US US18/144,707 patent/US20230274090A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022086765A1 (en) | 2022-04-28 |
US11790173B2 (en) | 2023-10-17 |
EP4128012A1 (en) | 2023-02-08 |
US20230274090A1 (en) | 2023-08-31 |
US20220129631A1 (en) | 2022-04-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7443407B2 (en) | Automated assistant with conferencing capabilities | |
JP7419485B2 (en) | Proactively incorporating unsolicited content into human-to-computer dialogs | |
JP7263376B2 (en) | Transition between previous interaction contexts with automated assistants | |
JP6942821B2 (en) | Obtaining response information from multiple corpora | |
US11735182B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
US11347801B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
CN112997171A (en) | Analyzing web pages to facilitate automated navigation | |
US11200893B2 (en) | Multi-modal interaction between users, automated assistants, and other computing services | |
CN115004190A (en) | Analyzing graphical user interfaces to facilitate automated interactions | |
US20230274090A1 (en) | Recommending action(s) based on entity or entity type | |
US20220215179A1 (en) | Rendering content using a content agent and/or stored content parameter(s) | |
US11842206B2 (en) | Generating content endorsements using machine learning nominator(s) | |
CN112463104B (en) | Automatic assistant with conference function |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |