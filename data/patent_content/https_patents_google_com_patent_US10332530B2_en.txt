US10332530B2 - Coding of a soundfield representation - Google Patents
Coding of a soundfield representation Download PDFInfo
- Publication number
- US10332530B2 US10332530B2 US15/417,550 US201715417550A US10332530B2 US 10332530 B2 US10332530 B2 US 10332530B2 US 201715417550 A US201715417550 A US 201715417550A US 10332530 B2 US10332530 B2 US 10332530B2
- Authority
- US
- United States
- Prior art keywords
- signal
- representation
- independent
- soundfield
- signals
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/008—Multichannel audio signal coding or decoding using interchannel correlation to reduce redundancy, e.g. joint-stereo, intensity-coding or matrixing
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/04—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using predictive techniques
- G10L19/16—Vocoder architecture
- G10L19/18—Vocoders using multiple modes
- G10L19/20—Vocoders using multiple modes using sound class specific coding, hybrid encoders or object based coding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/04—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using predictive techniques
- G10L19/16—Vocoder architecture
- G10L19/18—Vocoders using multiple modes
- G10L19/24—Variable rate codecs, e.g. for generating different qualities using a scalable representation such as hierarchical encoding or layered encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S3/00—Systems employing more than two channels, e.g. quadraphonic
- H04S3/002—Non-adaptive circuits, e.g. manually adjustable or static, for enhancing the sound image or the spatial distribution
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S3/00—Systems employing more than two channels, e.g. quadraphonic
- H04S3/008—Systems employing more than two channels, e.g. quadraphonic in which the audio signals are in digital form, i.e. employing more than two discrete digital channels
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/04—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using predictive techniques
- G10L19/16—Vocoder architecture
- G10L19/173—Transcoding, i.e. converting between two coded representations avoiding cascaded coding-decoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S2420/00—Techniques used stereophonic systems covered by H04S but not provided for in its groups
- H04S2420/11—Application of ambisonics in stereophonic audio systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04S—STEREOPHONIC SYSTEMS
- H04S7/00—Indicating arrangements; Control arrangements, e.g. balance control
- H04S7/30—Control circuits for electronic adaptation of the sound field
- H04S7/308—Electronic adaptation dependent on speaker or headphone connection
Definitions
- This document relates, generally, to coding a soundfield representation.
- a method includes: receiving a representation of a soundfield, the representation characterizing the soundfield around a point in space; decomposing the received representation into independent signals; and encoding the independent signals, wherein a quantization noise for any of the independent signals has a common spatial profile with the independent signal.
- the independent signals comprise a mono channel and a number of independent source channels.
- Decomposing the received representation comprises transforming the received representation.
- the transformation involves a demixing matrix, the method further comprising accounting for a filtering ambiguity by replacing the demixing matrix with a normalized demixing matrix.
- the representation of the soundfield corresponds to a time-invariant spatial arrangement.
- the method further comprising determining a demixing matrix, and using the demixing matrix in computing a source signal from an ambisonics signal.
- the method further comprising estimating a mixing matrix from observations of the ambisonics signal, and computing the demixing matrix from the estimated mixing matrix.
- the method further comprising normalizing the determined demixing matrix, and using the normalized demixing matrix in computing the source signal.
- the method further comprising performing blind source separation on the received representation of the soundfield.
- Performing the blind source separation comprises using a directional-decomposition map, estimating an RMS power, performing a scale-invariant clustering, and applying a mixing matrix.
- the method further comprising performing a directional decomposition as a pre-processor for the blind source separation.
- Performing the directional decomposition comprises an iterative process that returns time-frequency patch signals corresponding to a location set for loudspeakers.
- the method further comprising making the encoding scalable.
- Making the encoding scalable comprises encoding only a zero-order signal at a lowest bit rate, and with increasing bit rate, adding one or more extracted source signals and retaining the zero-order signal.
- the method further comprising excluding the zero-order signal from a mixing process.
- the method further comprising decoding the independent signals.
- a computer program product is tangibly embodied in a non-transitory storage medium, the computer program product including instructions that when executed cause a processor to perform operations including: receiving a representation of a soundfield, the representation characterizing the soundfield around a point in space; decomposing the received representation into independent signals; and encoding the independent signals, wherein a quantization noise for any of the independent signals has a common spatial profile with the independent signal.
- the independent signals comprise a mono channel and a number of independent source channels.
- a system in a third aspect, includes: a processor; and a computer program product tangibly embodied in a non-transitory storage medium, the computer program product including instructions that when executed cause the processor to perform operations including: receiving a representation of a soundfield, the representation characterizing the soundfield around a point in space; decomposing the received representation into independent signals; and encoding the independent signals, wherein a quantization noise for any of the independent signals has a common spatial profile with the independent signal.
- FIG. 1 shows an example of a system.
- FIGS. 2A-B schematically show examples of spatial profiles.
- FIG. 3 shows an example of a process.
- FIG. 4 shows examples of signals.
- FIG. 5 shows an example of a computer device and a mobile computer device that can be used to implement the techniques described here.
- the ambisonics representation can be decomposed into 1) a mono channel (e.g., the zero-order ambisonics channel) and 2) an arbitrary number of independent source channels. Coding can then be performed on this new signal representation.
- Examples of advantages that can be obtained include: 1) the spatial profile of the quantization noise and the corresponding independent signal are identical, which can maximize the perceptual masking and lead to minimal coding rate requirements; 2) the independent encoding of the independent signals can facilitate a globally optimal encoding of the ambisonics signal; and 3) the mono channel together with the progressive adding-in of individual sources can facilitate scalability, good quality and directionality compromises at high and low rates.
- the conversion of the signal from (N+1) 2 channels to, say, M independent sources involves a multiplication by a demixing matrix.
- the matrices can be time-invariant, which can lead to only little side information being required.
- the rate can vary with the number of independent sources. For each independent source directionality for that source can be added, effectively in the form of the room response described by the rows of the inverses of the demixing matrices for all the frequency bins. In other words, when an extracted source is added, it can go from being in the mono channel to being as it is heard in the context of the recording environment.
- the rate can be essentially independent of the ambisonics order N.
- Implementations can be used in various audio or audio-visual environments, such as immersive ones. Some implementations can involve virtual reality systems and/or video content platforms.
- Ambisonics for example, is a representation of a soundfield using a number of audio channels that characterize the soundfield around a point in space. From another viewpoint, ambisonics can be considered as a Taylor-like expansion of the soundfield around that point.
- the ambisonics representation describes the soundfield around a point (generally the location of the user). It characterizes the field directly, thus differing from methods that describe a set of sources driving the field.
- a first-order ambisonics representation characterizes sound using channels W, X, Y and Z, where W corresponds to a signal from an omnidirectional microphone, and X, Y and Z correspond to signal associated with the three spatial axes, such as might be picked up by figure-of-eight capsules.
- the ambisonics representation is independent of the rendering method, which can use, for example, headphones or a particular loudspeaker arrangement.
- the representation is also scalable: low-order ambisonics representations, which have less directional information, form a subset of high-order descriptions that have more directional information. For example, the scalability and the fact that the representation describes the soundfield around the user directly has made ambisonics a common representation for virtual reality headset applications.
- An ambisonics representation can be generated with a multi-microphone assembly. Some microphone systems are configured for generating the ambisonics representation directly, and in other cases a separate unit can be used for the generation. Ambisonics representations can have different numbers of channels, such as 9, 25 or 36 channels, or in principle any square integer number of channels.
- An ambisonics representation can be visualized as analogous to a sphere: inside the sphere the description of the sound is accurate, and outside the sphere the description is less accurate or inaccurate. With a higher order ambisonics representation, the sphere can be considered to be larger.
- a higher order ambisonics implementation can be used in order to obtain a better resolution of sound, in that the location of sound can be identified with more accuracy, and the sound characterization goes further from the center of the sphere.
- the ambisonics representation can be of sounds coming from sources that are unknown to the user, so the ambisonics channels can be used to discriminate and dissolve between these sources.
- the present disclosure describes that the perception of quantization noise becomes clearer if the quantization noise of an independent signal component signal, and that independent signal component, have different directionalities.
- the term directionality implies the full map that maps the scalar independent signal component into its ambisonics vector signal representation. For a time-invariant spatial arrangement this map is time-invariant and corresponds to a generalized transfer function. If the quantization noise is perceptually clearer, then the coding rate will go up for equal perceived sound field quality. However, the channels of the ambisonics representation each contain mixtures of independent signals, which can make this issue difficult to resolve. On the other hand, it would be advantageous to be able to use existing mono audio coding schemes in the process.
- FIG. 1 shows an example of a system 100 .
- the system 100 includes multiple sound sensors 102 , including, but not limited to, microphones. For example, one or more omnidirectional microphones and/or microphones of other spatial characteristics can be used.
- the sound sensors 102 detect audio in a space 103 .
- the space 103 can be characterized by structures (such as in a recording studio with a particular ambient impulse response) or it can be characterized as being essentially free of surrounding structures (such as in a substantially open space).
- the output of the sound sensors can be provided to a module 104 , such as an ambisonics module. Any processing component can be used that generates a soundfield representation that characterizes the sound directly, as opposed to, say, in terms of one or more sound sources.
- the ambisonics module 104 generates as its output an ambisonics representation of the soundfield detected by the sound sensors 102 .
- the ambisonics representation can be provided from the ambisonics module 104 to a decomposition module 106 .
- the module 106 is configured for decomposing the ambisonics representation into a mono channel and multiple source channels. For example, matrix multiplication can be performed in each frequency bin of the soundfield representation.
- the output of the decomposition module 106 can be provided to an encoding module 108 .
- an existing coding scheme can be used.
- the encoded signal can be stored, forwarded and/or transmitted to another location.
- a channel 110 represents one or more ways that an encoded audio signal can be managed, such as by transmission to another system for playback.
- the system 100 includes a decoding module 112 .
- the decoding module can perform operations in essentially the opposite way than in the respective modules 104 , 106 and 108 .
- an inverse transform can be performed in the decoding module that partially or completely restores the ambisonics representation that was generated by the module 104 .
- the operations of the decomposition module 106 and the encoding module 108 can have their opposite counterparts in the decoding module 112 .
- the resulting audio signals can be stored and/or played depending on the situation.
- the system 100 can include two or more audio playback sources 114 (including, but not limited to, loudspeakers) to which the processed audio signal can be provided for playback.
- the soundfield representation is not associated with a particular way of playing out the audio description.
- the soundfield description can be played out over a headphone, and the system can then compute what should be rendered in the headphones.
- the rendering can be dependent how the user turns his or her head.
- a sensor can be used that informs the system of the head orientation, and the system can then cause the person to hear the sound coming from a direction that is independent of the head orientation.
- the soundfield description can be played out over a set of loudspeakers. That is, first the system can store or transmit the description of the soundfield around the listener.
- a computation can then be made what the individual speakers should produce to create the soundfield around the listener's head, or the impression of that soundfield around the head. That is, the soundfield can be a definition of what the resulting sound around the listener should be, so that the rendering system can process that information and generate the appropriate sound to accomplish that result.
- FIGS. 2A-B schematically show examples of spatial profiles. These examples involve a physical space 200 , such as a room, an outdoors area or any other location.
- a circle 202 schematically represents a listener in each situation. That is, a soundfield representation is going to be played to the listener 202 .
- the soundfield description can correspond to a recording that was made in the space 200 or elsewhere.
- People 204 A-C are schematically illustrated as being in the space 200 .
- the people symbols represent voices (e.g., speech, song or other utterances) that the listener can hear.
- the locations of the people 204 A-C around the listener 202 indicate that the sound of each individual person is here to arrive at the listener 202 from a separate direction.
- the notion of a spatial profile is a generalization of this illustrative example.
- the spatial profile then includes both the direct path and all the reflective paths through which the sound of the source travels to reach the listener 202 .
- the term “direction” can be taken as having a generalized meaning and to be equivalent to a set of directions representing the direct path and all reflective paths.
- Coding of an audio signal may not, however, be a perfect process.
- noise can be generated.
- the encoding/decoding process for an audio representation can be considered a tradeoff between the perceived severity of signal distortion and signal-independent noise on the one hand, and the coded bit rate on the other.
- signal-correlated distortion and signal-independent noise are lumped together.
- a squared error (such as with perceptual weighting) can then be used as a fidelity measure.
- This “lumped” approach can have shortcomings that can also be relevant in the coding of a soundfield representation.
- the human auditory periphery can interpret differently inaccuracy in directional information (e.g., distortion) and signal-independent noise.
- signal-independent signal error resulting from quantization will be referred to as quantization noise.
- noise 206 is schematically illustrated in the space 200 in FIG. 2A . That is, the noise 206 is associated with the encoding of the audio from one or more of the people 204 A-C.
- the noise 206 does not appear to come from the same direction as any of the voices of the people 204 A-C. Rather, the noise 206 appears to come from another direction in the space 200 . Namely, each of the people 204 A-C can be said to have associated with them a corresponding spatial profile 208 A-C.
- the spatial profile corresponds to how the sound from a particular talker is captured: some of it arrives directly from the talker into the microphone, and other sound (generated simultaneously) first bounces on one or more surfaces before being picked up.
- Each talker can therefore have his or her own distinctive spatial profile. That is, the voice of the person 204 A is associated with the spatial profile 208 A, the voice of the person 204 B with the spatial profile 208 B, and so on.
- the noise 206 is associated with a spatial profile 210 that does not coincide with either of the spatial profiles 208 A-C.
- the spatial profile 210 does not even overlap with either of the spatial profiles 208 A-C. This can be perceptually distracting to the listener 202 , such as because they may not expect any sound (whether a voice or noise) to come from the direction associated with the spatial profile 210 .
- the listener 202 can pick up the noise 206 more quickly because it came from a direction that is different from the original sources.
- the example does use decomposition of a soundfield representation according to the present disclosure.
- any noise generated in the audio processing gets essentially the same spatial profile as the sound that was being processed when the noise occurred. That is, in the decomposition process, audio sources are individualized to channels with their respective directions. These can then be coded individually.
- the noise can have the exact same spatial profile as the source of the noise.
- the voices of the people 204 A-C give rise to respective noise signals 212 A-C.
- the noise signal 212 A has the same spatial profile 208 A as does the voice of the person 204 A
- the noise signal 212 B has the same spatial profile 208 B as the person 204 B, and so on.
- none of the noises 212 A-C appears to come from a direction other than that of the voice that caused it.
- none of the noises 212 A-C comes from a direction in the space 200 that is otherwise free of sound sources.
- One way of characterizing this situation is to describe the voices of the persons 204 A-C as masking the respective noise 212 A-C coming from that sound source. As a result, the system can go down in bit rate when operating at the threshold of just noticeable quantization noise.
- each signal can include also a mono signal and a mono noise signal associated with it. These can then become spread over the space 200 , while the noise and the voice (e.g., a talker) have the same spatial profile.
- the description can be a characterization of a soundfield around a point in space. Here, it is assumed that no sources or objects are present in the region of the characterization.
- k ⁇ c is the wavenumber, with c the speed of sound and co the frequency in radians per second.
- Y n ⁇ ⁇ m ⁇ ( ⁇ , ⁇ ) 2 ⁇ n + 1 4 ⁇ ⁇ ⁇ ( n - ⁇ m ⁇ ) ! ( n + ⁇ m ⁇ ) ! ⁇ P n ⁇ ⁇ m ⁇ ⁇ ( cos ⁇ ( ⁇ ) ) ⁇ e im ⁇ ⁇ ⁇ ( 4 ) is a spherical harmonic of order n and mode m, with P nm ( ⁇ ) the associated Legendre function.
- the solution for outgoing waves can be omitted because a space is considered that has no objects and sound sources.
- the soundfield can be specified with the coefficients B n m (k) and this is what is used in the so-called ambisonics B-format.
- the B-format can be provided as a time-frequency transform, for example with the transform being based on a tight-frame representation. For example, a tight frame can imply that squared-error measures are invariant with the transformation, except for scaling.
- the B-format coefficients can then be of the form B n m (l, q), where l is a time index and q is a discrete frequency index that is linearly related to k. Let be the set of discrete frequencies of the representation. Then the time-frequency representation B n m : ⁇ ⁇ can be converted to time-domain signals b n m : ⁇ by way of a sequence of inverse discrete Fourier transforms ⁇ 1 :
- b n m ⁇ l ⁇ T l ⁇ ⁇ ⁇ K ⁇ H ⁇ ⁇ F - 1 ⁇ B n m ⁇ ( l , ⁇ ) , ( 5 )
- ⁇ 1 returns time-domain samples corresponding to the coefficients B n m (l, ⁇ )
- H is an ⁇ diagonal windowing matrix
- T l is an operator that pads the input with zeros to render it an infinite sequence with the support centered at the origin and then advances it by l samples
- a is chosen such that ⁇ is the number of samples time-advance between the blocks of the time-frequency transform.
- the ambisonics B-format coefficient For a spherical sound wave with driving signal S(l, q) originating from a source at distance p in the direction ( ⁇ , ⁇ ) the ambisonics B-format coefficient can be
- Equation (7) includes a dependency
- the near-field effect amplifies the low-order terms. That is, relatively less directional detail may be needed to represent the soundfield component generated by nearby sources. The effect can appear progressively earlier at low frequencies; it is a result of the spherical Bessel function. This can imply that nearby sources are perceived as having a larger effective aperture. At sufficiently low frequencies, the sound directionality can effectively be lost for nearby sources as essentially all signal power resides in the zero-order coefficient B 0 (l, q). For example, consumer audio equipment can use a single loudspeaker for low-frequency sound as it is necessarily generated from nearby. On the other hand, in the animal world, elephants can determine the direction of other elephants by communication at frequencies below the range of human hearing.
- the following exemplifies an ambisonics approach.
- the expansion (3) can be truncated.
- the task can then be to seek the optimal coefficients B n m (k) to describe the soundfield.
- One possible approach is to determine the coefficients that minimize an L2 norm (a least-squares solution) or an L1 norm on a ball of radius r.
- the L2 answer may not be trivial; while the spherical harmonics are orthonormal on the surface of a sphere, the expansion (3) may not be orthonormal inside a ball of given radius as the spherical Bessel functions of different order have no standard orthogonality conditions.
- the ambisonics approach on the other hand, can take a different approach.
- higher-order ambisonics seeks to match the radial derivatives of the soundfield at the origin in all directions up to a certain radial derivative (i.e., the order). In other words, it can be interpreted as being akin to a Taylor series. In its original form, ambisonics seeks to match only the first-order slopes and does so directly from measurements, as will be discussed below. In later forms, higher order terms are also included.
- ambisonics does not attempt to reconstruct the soundfield directly, but rather characterizes the directionality at the origin.
- the representation is inherently scalable: the higher the value of the truncation of n in the equation (3) (i.e., the ambisonics order), the more precise the directionality.
- the soundfield description is accurate over a larger ball for a higher order n.
- the radius of this ball is inversely proportional to the frequency.
- a good measure of the size of the ball may be the location of the first zero of j 0 ( ⁇ ).
- Low order ambisonics signals are embedded in higher-order descriptions.
- the zero-th order spherical harmonic is the mono signal. However, at the zero of the zero-th order Bessel function this “mono” signal component is zero. The location of the zero moves inward with increasing frequency.
- the amplitude modulation of the spherical harmonic is a physical effect; when one creates the right signal at the center of a ball and insists on a spherically symmetric field, then it will vanish at a particular radius.
- the question can arise whether this is perceptible if the soundfield is placed around the human head. The question may be difficult to answer since the presence of the human head changes the soundfield. However, if one replaces the human head with microphones in free space, then the zeros will be observed physically. Hence, it may be difficult to assign a weighting to the B-format coefficients that reflects their perceptual relevance.
- the zero-order spherical harmonic does not contribute at 1700 Hz.
- the first zero for j 1 ( ⁇ ) is at around 2300 Hz.
- the spatial zeros in direct binaural rendering are a direct result of the binaural rendering and would generally not occur when using rendering with loudspeakers.
- the signal When rendered with loudspeakers, the signal consists of a combination of (approximate) plane waves arriving from different angles.
- Binaural rendering based on ambisonics can then be performed using virtual plane waves that provide the correct soundfield near the coordinate origin (even if that approximation is right only within a sphere that is smaller than the human head).
- the approach can be based on equation (6), as mode matching leads to a vector equality that allows conversion of the coefficients into the amplitudes of a set of planes waves given their azimuths and elevations.
- the Moore-Penrose pseudo-inverse approach can compute amplitudes for the set of plane waves that correspond to the lowest total energy that gives rise to the desired soundfield near the origin. In some situations use of a pseudo-inverse may not be motivated. These plane waves can then be converted to the desired binaural signal using an appropriate head-related transfer function (HRTF). If the head is rotated, the azimuth and elevation of the microphones and the associated HRTF are to be adjusted accordingly.
- HRTF head-related transfer function
- a loudspeaker i has an elevation and azimuth ( ⁇ i , ⁇ i ) and produces a signal S i (k) at frequency k. Near the origin, the rendered signal is then, using the equation (6):
- Equation (10) may be a complicated way of writing the mode matching equation that could have been written directly from equation (6):
- B(l, q) be the stacking of the B n m (l, q) and let Y i be the stacking of Y nm ( ⁇ i , ⁇ i ), both over n and m.
- Y [Y 1 , . . . , Y
- ] and S(k) [S 1 (k), . . . , (k)] T .
- one can rewrite equation (11) as B ( k ) YS ( k ). (12)
- the relationship between the ambisonics representation and the signal heard by the listener is linear but may not be straightforward.
- masking levels for the virtual loudspeaker signals depend on head rotation. This can suggest usage of a minimax approach to ensure transparent coding for any head rotation.
- the following description relates to multi-loudspeaker rendering.
- the rendering over physically fixed loudspeakers can be similar to the principle described above for the loudspeakers at infinity. It can be important to account for the phase difference associated with the distance of the loudspeaker.
- Ambisonics describes the soundfield without the physical presence of the listener. This is easily seen when one considers the original ambisonics recording method: it applies a correction to recording for the Bessel functions and the cardioid microphone. If rendered by loudspeakers, the presence of the listener modifies the soundfield but this approximates what would happen in the original sound-field scenario.
- the soundfield at the ear depends on the orientation of the listener and on the physical presence of the listener. In binaural listening the soundfield is corrected for the presence of the listener with the HRTF.
- the HRTF selection depends on the orientation of the listener.
- the rendered audio signal can generally be perceived by both ears of the listener.
- the dichotic condition occurs when the same signal is heard in both ears. If the signal is only heard in one ear, the monotic condition occurs.
- the masking levels for the monotic and dichotic conditions are identical. More complex scenarios generally correspond to the dichotic condition, where the masker and maskee have a different spatial profile.
- An attribute of a dichotic condition is the masking level difference (MLD).
- MLD is the difference in masking level between the dichotic scenario and the corresponding monotic condition. This difference can be large below 1500 Hz, where it can reach 15 dB; above 1500 Hz the MLD decreases to about 4 dB.
- the values of the MLD show that, in general, masking levels can be lower in the binaural case, and signal accuracy must be commensurably higher. For some applications this implies that a high coding rate is required for a dichotic scenario.
- a separate issue is contralateral, or central, masking, which can occur when one hears the signal in one ear and hears simultaneously an interferer in the other ear.
- the masking by the interferer may be very weak. In some implementations, it is so weak that it need not be considered in the audio coding designs. In the following discussions it will not be considered.
- ambisonics coding scheme To construct an ambisonics coding scheme, one can account for the attributes of spatial masking discussed above. Two contrasting paradigms can be considered: i) the direct coding paradigm: code the B-format time frequency coefficients directly and attempt to find a satisfactory mechanism to define the masking levels for the B-format coefficients, ii) a transform coding paradigm: transform the B-format time-frequency coefficients to a time-frequency domain signals where the computation of masking levels is relatively straightforward.
- An apparent advantage of the direct coding paradigm can be that the scalability with respect to directionality would carry over to the coded streams.
- the computation of the masking levels may be difficult and, moreover the paradigm can lead to dichotic masking conditions (spatial profile of quantization noise and signals are not consistent), where the masking level threshold is low and, as a result the rate is high.
- the B-format coefficients can be strongly statistically interdependent, which means vector quantization is required to obtain high efficiency (note that methods for decorrelation of the coefficients would make the method a transform approach).
- An approach to coding the B-format coefficients directly is explored in more detail below, which describes a masking constrained directional coding algorithm.
- both optimal coding and a high masking threshold can be obtained by decomposing the ambisonics representation into independent signals.
- a coding scheme then first transforms the ambisonics coefficient signals. The resulting independent signals are then encoded. They are decoded when or where the signal is needed. Finally the set of decoded signals are added to provide a single ambisonics representation of the acoustic scenario.
- B( ⁇ , k) ⁇ is an N 2 -dimensional vector process and S( ⁇ , k) ⁇ is an
- BSS Blind source separation
- BSS algorithms may have additional drawbacks. Some BSS algorithms suffer from a filtering ambiguity and frequency domain methods generally suffer from the so-called permutation ambiguity. Various methods for addressing the permutation ambiguity exist. As for the filtering ambiguity, it may appear that it is of no consequence if one remixes the signal after decoding to obtain the ambisonics representation. However, it can affect the masking of the coding scheme used to encode the independent signals.
- the coding of the individual dimensions of the time-frequency signals S(l, q) can be performed independently with existing single-channel audio coders and with conventional single-channel masking considerations (as the source and its quantization noise share their spatial profile).
- the individual dimensions of the time-frequency signals S(l, q) can be converted to time-domain signals by equation (5).
- the masking of one source by another source can be ignored in this paradigm, which can be justified from the fact that individual sources may dominate the signal perceived by the listener under a specific orientation of the listener, and the paradigm effectively represents a minimax approach.
- FIG. 3 shows an example of a source-separation process 300 for a particular frequency q.
- a mixing matrix or a demixing matrix can be estimated from observations of B( ⁇ , q). For example, this can be the demixing matrix in equation (14) or the mixing matrix in equation (15).
- the demixing matrix can be computed from the mixing matrix, if necessary.
- the demixing matrix can be normalized. For example, this can be done as shown in equation (17).
- the source signal S(l, q) can be computed from the ambisonics signal B(l, q) using the demixing matrix.
- the resulting scalability replaces the scalability of the ambisonics B format, but is based on a different principle.
- At the lowest bit rate one can encode only the mono (zero-order) signal.
- the mono channels themselves can be varying in rate. With increasing rate one can add additional extracted sources but retain the mono channel. While the mono channel should be used in the estimation of the source signals as it provides useful information, it is not included in the mixing process as it is already complete. That is, the first row of equation (14), which specifies the zero-order ambisonics channel, can be omitted and the coded ambisonics channel is taken instead.
- the coded signal contains progressively more components. Except for the first component signal, which is the mono channel, the component signals each describe an independent sound source.
- FIG. 4 shows examples of signals 400 .
- a signal 410 corresponds to a lowest rate.
- the signal 410 can include a mono signal.
- Signal 420 can correspond to a next order.
- the signal 420 can include a source signal 1 and its ambisonics mixing matrix.
- Signal 430 can correspond to a next order.
- the signal 430 can include a source signal 2 and its ambisonics mixing matrix.
- Signal 440 can correspond to a next order.
- the signal 440 can include a source signal 3 and its ambisonics mixing matrix.
- the ambisonics mixing matrices can be time-invariant for time-invariant spatial arrangements and, therefore, require only a relatively low transmission rate under this condition.
- a directional decomposition method can be used as a pre-processor. For example, this can be the method described below.
- the algorithm relates to independent source extraction for ambisonics and includes:
- ⁇ j 1 ⁇ L ⁇ ⁇ ⁇ l ⁇ L ⁇ ⁇ S j ′ ⁇ ( l , q ) ⁇ 2
- the BSS algorithm can be run per frequency bin k and can assume that the directional signals generally contain only a single source (as they represent a path to that source).
- the directional signals (which form the rows of the vector process consisting of all signals in all loudspeakers) can then be clustered, a cluster i containing the indices to a set of directional signals associated with a particular sound source i ⁇ .
- the clustering must be invariant with a complex scale factor for the signals and can be based on, for example, affinity propagation.
- Single-signal (singleton) clusters consist of multiple source signals may not be considered.
- Equation (22) can be seen as a synthesis operation: it creates the ambisonics representation from the signals in the directional decomposition representation, S with a straightforward matrix multiplication.
- a matching pursuit algorithm to find both the set of S j (k) and the set of ( ⁇ j , ⁇ j ) for that frequency band.
- the algorithm can be stopped at a certain residual error or after a fixed number of iterations.
- the algorithm relates to a directional decomposition matching pursuit and returns time-frequency patch signals S corresponding to location set , where is the set of complex numbers.
- the algorithm can include:
- the above algorithm returns more consistent values for the selected point set for larger time-frequency patches.
- the optimal point set varies with frequency, but depending on the physical arrangement and the frequency, consistency in the loudspeaker locations found may be expected within frequency bands.
- the optimal point set should not vary in time. Hence the time duration of the patch can be made relatively long.
- FIG. 5 shows an example of a generic computer device 500 and a generic mobile computer device 550 , which may be used with the techniques described here.
- Computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, tablets, workstations, personal digital assistants, televisions, servers, blade servers, mainframes, and other appropriate computing devices.
- Computing device 550 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- Computing device 500 includes a processor 502 , memory 504 , a storage device 506 , a high-speed interface 508 connecting to memory 504 and high-speed expansion ports 510 , and a low speed interface 512 connecting to low speed bus 514 and storage device 506 .
- the processor 502 can be a semiconductor-based processor.
- the memory 504 can be a semiconductor-based memory.
- Each of the components 502 , 504 , 506 , 508 , 510 , and 512 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 502 can process instructions for execution within the computing device 500 , including instructions stored in the memory 504 or on the storage device 506 to display graphical information for a GUI on an external input/output device, such as display 516 coupled to high speed interface 508 .
- an external input/output device such as display 516 coupled to high speed interface 508 .
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 504 stores information within the computing device 500 .
- the memory 504 is a volatile memory unit or units.
- the memory 504 is a non-volatile memory unit or units.
- the memory 504 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 506 is capable of providing mass storage for the computing device 500 .
- the storage device 506 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 504 , the storage device 506 , or memory on processor 502 .
- the high speed controller 508 manages bandwidth-intensive operations for the computing device 500 , while the low speed controller 512 manages lower bandwidth-intensive operations.
- the high-speed controller 508 is coupled to memory 504 , display 516 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 510 , which may accept various expansion cards (not shown).
- low-speed controller 512 is coupled to storage device 506 and low-speed expansion port 514 .
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 500 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 520 , or multiple times in a group of such servers. It may also be implemented as part of a rack server system 524 . In addition, it may be implemented in a personal computer such as a laptop computer 522 . Alternatively, components from computing device 500 may be combined with other components in a mobile device (not shown), such as device 550 . Each of such devices may contain one or more of computing device 500 , 550 , and an entire system may be made up of multiple computing devices 500 , 550 communicating with each other.
- Computing device 550 includes a processor 552 , memory 564 , an input/output device such as a display 554 , a communication interface 566 , and a transceiver 568 , among other components.
- the device 550 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- Each of the components 550 , 552 , 564 , 554 , 566 , and 568 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 552 can execute instructions within the computing device 550 , including instructions stored in the memory 564 .
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 550 , such as control of user interfaces, applications run by device 550 , and wireless communication by device 550 .
- Processor 552 may communicate with a user through control interface 558 and display interface 556 coupled to a display 554 .
- the display 554 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 556 may comprise appropriate circuitry for driving the display 554 to present graphical and other information to a user.
- the control interface 558 may receive commands from a user and convert them for submission to the processor 552 .
- an external interface 562 may be provide in communication with processor 552 , so as to enable near area communication of device 550 with other devices. External interface 562 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 564 stores information within the computing device 550 .
- the memory 564 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 574 may also be provided and connected to device 550 through expansion interface 572 , which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 574 may provide extra storage space for device 550 , or may also store applications or other information for device 550 .
- expansion memory 574 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 574 may be provide as a security module for device 550 , and may be programmed with instructions that permit secure use of device 550 .
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 564 , expansion memory 574 , or memory on processor 552 , that may be received, for example, over transceiver 568 or external interface 562 .
- Device 550 may communicate wirelessly through communication interface 566 , which may include digital signal processing circuitry where necessary. Communication interface 566 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 568 . In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 570 may provide additional navigation- and location-related wireless data to device 550 , which may be used as appropriate by applications running on device 550 .
- GPS Global Positioning System
- Device 550 may also communicate audibly using audio codec 560 , which may receive spoken information from a user and convert it to usable digital information. Audio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 550 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 550 .
- Audio codec 560 may receive spoken information from a user and convert it to usable digital information. Audio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 550 . Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 550 .
- the computing device 550 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 580 . It may also be implemented as part of a smart phone 582 , personal digital assistant, or other similar mobile device.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
∇2 u(r,k)+k 2 U(r,k)=0, (2)
where
is the wavenumber, with c the speed of sound and co the frequency in radians per second.
where j=√{square root over (−1)}, jn(⋅) is a spherical Bessel function of the first kind and
is a spherical harmonic of order n and mode m, with Pnm(⋅) the associated Legendre function. In some implementations, the solution for outgoing waves can be omitted because a space is considered that has no objects and sound sources.
where
B n m(l,q)=S(l,q)Y nm(θ, ϕ). (6)
on the radius; for a given frequency, the near-field effect amplifies the low-order terms. That is, relatively less directional detail may be needed to represent the soundfield component generated by nearby sources. The effect can appear progressively earlier at low frequencies; it is a result of the spherical Bessel function. This can imply that nearby sources are perceived as having a larger effective aperture. At sufficiently low frequencies, the sound directionality can effectively be lost for nearby sources as essentially all signal power resides in the zero-order coefficient B0(l, q). For example, consumer audio equipment can use a single loudspeaker for low-frequency sound as it is necessarily generated from nearby. On the other hand, in the animal world, elephants can determine the direction of other elephants by communication at frequencies below the range of human hearing.
Thus, at 0.1 m radius the zero-order spherical harmonic does not contribute at 1700 Hz. Similarly, for r=0.1 m radius the first zero for j1(⋅) is at around 2300 Hz. Thus, if a soundfield that is not spherically symmetric is to be described accurately, other ambisonics terms must provide the signal at those spatial zeros. The ambisonics representation therefore cannot be statistically independent.
where the error ∈ is orthogonal in the elevation-and-azimuthal space to the spherical harmonics below order N.
B(k)=YS(k). (12)
S(k)=Y H(YY H)−1 B(k), (13)
which is just the definition of the Moore-Penrose pseudo-inverse.
-
- 1) The quantization noise and the signal have the same spatial profile, leading to a higher masking threshold and a lower rate.
- 2) The separate coding of independent signals does not incur coding loss.
B(l,q)=M(q)S(l,q) (14)
S(l,q)=A(q)B(l,q). (15)
A(q)=M(q)H(M(q)M(q)H)−1, (16)
as this inverse will remove any energy not lying in the image of M(q).
where S(l, q)=[S1(l, q), . . . ,
Ynm=[Ynm(θ1, ϕ1), . . . , Ynm(
B(l,q)=Y T S(l,q)+∈(N+1)
where, since Σn=0 N2n+1=(N+1)2, one obtains that S(l, q)∈
B=Y T S+∈ (N+1)
where B=[B(L0, K0), . . . , B(L1−1, K1−1)]∈
{circumflex over (B)}=Y T S. (22)
Initialize loudspeaker location set {ϕp, θp}p∈P | |
Set itermax | |
iter = 0 | |
r = B | |
{circumflex over (B)} = 0 | |
while iter < itermax do | |
j = | |
S = | |
{circumflex over (B)} = {circumflex over (B)} + Y(θj, ϕj)S | |
r = r − Y(θj, ϕj)S | |
iter = iter + 1 | |
end while | |
Claims (20)
Priority Applications (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/417,550 US10332530B2 (en) | 2017-01-27 | 2017-01-27 | Coding of a soundfield representation |
PCT/US2017/059723 WO2018140109A1 (en) | 2017-01-27 | 2017-11-02 | Coding of a soundfield representation |
CN201780070855.3A CN109964272B (en) | 2017-01-27 | 2017-11-02 | Coding of sound field representations |
EP17844590.4A EP3523801B1 (en) | 2017-01-27 | 2017-11-02 | Coding of a soundfield representation |
US16/404,076 US10839815B2 (en) | 2017-01-27 | 2019-05-06 | Coding of a soundfield representation |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/417,550 US10332530B2 (en) | 2017-01-27 | 2017-01-27 | Coding of a soundfield representation |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/404,076 Continuation US10839815B2 (en) | 2017-01-27 | 2019-05-06 | Coding of a soundfield representation |
Publications (2)
Publication Number | Publication Date |
---|---|
US20180218740A1 US20180218740A1 (en) | 2018-08-02 |
US10332530B2 true US10332530B2 (en) | 2019-06-25 |
Family
ID=61257091
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/417,550 Active 2037-04-21 US10332530B2 (en) | 2017-01-27 | 2017-01-27 | Coding of a soundfield representation |
US16/404,076 Active US10839815B2 (en) | 2017-01-27 | 2019-05-06 | Coding of a soundfield representation |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/404,076 Active US10839815B2 (en) | 2017-01-27 | 2019-05-06 | Coding of a soundfield representation |
Country Status (4)
Country | Link |
---|---|
US (2) | US10332530B2 (en) |
EP (1) | EP3523801B1 (en) |
CN (1) | CN109964272B (en) |
WO (1) | WO2018140109A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10264386B1 (en) * | 2018-02-09 | 2019-04-16 | Google Llc | Directional emphasis in ambisonics |
CN117809663A (en) * | 2018-12-07 | 2024-04-02 | 弗劳恩霍夫应用研究促进协会 | Apparatus, method for generating sound field description from signal comprising at least two channels |
BR112021020484A2 (en) * | 2019-04-12 | 2022-01-04 | Huawei Tech Co Ltd | Device and method for obtaining a first-order ambisonic signal |
CN111241904B (en) * | 2019-11-04 | 2021-09-17 | 北京理工大学 | Operation mode identification method under underdetermined condition based on blind source separation technology |
JP2024026010A (en) * | 2022-08-15 | 2024-02-28 | パナソニックＩｐマネジメント株式会社 | Sound field reproduction device, sound field reproduction method, and sound field reproduction system |
Citations (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4042779A (en) | 1974-07-12 | 1977-08-16 | National Research Development Corporation | Coincident microphone simulation covering three dimensional space and yielding various directional outputs |
US20060126852A1 (en) * | 2002-09-23 | 2006-06-15 | Remy Bruno | Method and system for processing a sound field representation |
US20100329466A1 (en) * | 2009-06-25 | 2010-12-30 | Berges Allmenndigitale Radgivningstjeneste | Device and method for converting spatial audio signal |
EP2469741A1 (en) | 2010-12-21 | 2012-06-27 | Thomson Licensing | Method and apparatus for encoding and decoding successive frames of an ambisonics representation of a 2- or 3-dimensional sound field |
US20120294446A1 (en) * | 2011-05-16 | 2012-11-22 | Qualcomm Incorporated | Blind source separation based spatial filtering |
EP2800401A1 (en) | 2013-04-29 | 2014-11-05 | Thomson Licensing | Method and Apparatus for compressing and decompressing a Higher Order Ambisonics representation |
US20140358557A1 (en) * | 2013-05-29 | 2014-12-04 | Qualcomm Incorporated | Performing positional analysis to code spherical harmonic coefficients |
US20150154965A1 (en) * | 2012-07-19 | 2015-06-04 | Thomson Licensing | Method and device for improving the rendering of multi-channel audio signals |
US20150248891A1 (en) * | 2012-11-15 | 2015-09-03 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Segment-wise adjustment of spatial audio signal to different playback loudspeaker setup |
US20150332679A1 (en) * | 2012-12-12 | 2015-11-19 | Thomson Licensing | Method and apparatus for compressing and decompressing a higher order ambisonics representation for a sound field |
US20150340044A1 (en) * | 2014-05-16 | 2015-11-26 | Qualcomm Incorporated | Higher order ambisonics signal compression |
US9531998B1 (en) * | 2015-07-02 | 2016-12-27 | Krush Technologies, Llc | Facial gesture recognition and video analysis tool |
US20170148449A1 (en) * | 2014-03-21 | 2017-05-25 | Dolby International Ab | Method for compressing a higher order ambisonics (hoa) signal, method for decompressing a compressed hoa signal, apparatus for compressing a hoa signal, and apparatus for decompressing a compressed hoa signal |
US20170164133A1 (en) * | 2014-07-03 | 2017-06-08 | Dolby Laboratories Licensing Corporation | Auxiliary Augmentation of Soundfields |
US20170180902A1 (en) * | 2014-03-21 | 2017-06-22 | Dolby International Ab | Method for compressing a higher order ambisonics (hoa) signal, method for decompressing a compressed hoa signal, apparatus for compressing a hoa signal, and apparatus for decompressing a compressed hoa signal |
US9813811B1 (en) * | 2016-06-01 | 2017-11-07 | Cisco Technology, Inc. | Soundfield decomposition, reverberation reduction, and audio mixing of sub-soundfields at a video conference endpoint |
US9961475B2 (en) * | 2015-10-08 | 2018-05-01 | Qualcomm Incorporated | Conversion from object-based audio to HOA |
US20180227665A1 (en) * | 2016-06-15 | 2018-08-09 | Mh Acoustics, Llc | Spatial Encoding Directional Microphone Array |
US20180308500A1 (en) * | 2014-06-27 | 2018-10-25 | Dolby Laboratories Licensing Corporation | Method for decoding a higher order ambisonics (hoa) representation of a sound or soundfield |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6711528B2 (en) * | 2002-04-22 | 2004-03-23 | Harris Corporation | Blind source separation utilizing a spatial fourth order cumulant matrix pencil |
CN100433046C (en) * | 2006-09-28 | 2008-11-12 | 上海大学 | Image blind separation based on sparse change |
CN101384105B (en) * | 2008-10-27 | 2011-11-23 | 华为终端有限公司 | Three dimensional sound reproducing method, device and system |
CN104468436A (en) * | 2014-10-13 | 2015-03-25 | 中国人民解放军总参谋部第六十三研究所 | Communication signal wavelet domain blind source separation anti-interference method and device |
-
2017
- 2017-01-27 US US15/417,550 patent/US10332530B2/en active Active
- 2017-11-02 EP EP17844590.4A patent/EP3523801B1/en active Active
- 2017-11-02 CN CN201780070855.3A patent/CN109964272B/en active Active
- 2017-11-02 WO PCT/US2017/059723 patent/WO2018140109A1/en unknown
-
2019
- 2019-05-06 US US16/404,076 patent/US10839815B2/en active Active
Patent Citations (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4042779A (en) | 1974-07-12 | 1977-08-16 | National Research Development Corporation | Coincident microphone simulation covering three dimensional space and yielding various directional outputs |
US20060126852A1 (en) * | 2002-09-23 | 2006-06-15 | Remy Bruno | Method and system for processing a sound field representation |
US20100329466A1 (en) * | 2009-06-25 | 2010-12-30 | Berges Allmenndigitale Radgivningstjeneste | Device and method for converting spatial audio signal |
EP2469741A1 (en) | 2010-12-21 | 2012-06-27 | Thomson Licensing | Method and apparatus for encoding and decoding successive frames of an ambisonics representation of a 2- or 3-dimensional sound field |
US20120294446A1 (en) * | 2011-05-16 | 2012-11-22 | Qualcomm Incorporated | Blind source separation based spatial filtering |
US20150154965A1 (en) * | 2012-07-19 | 2015-06-04 | Thomson Licensing | Method and device for improving the rendering of multi-channel audio signals |
US20150248891A1 (en) * | 2012-11-15 | 2015-09-03 | Fraunhofer-Gesellschaft Zur Foerderung Der Angewandten Forschung E.V. | Segment-wise adjustment of spatial audio signal to different playback loudspeaker setup |
US20150332679A1 (en) * | 2012-12-12 | 2015-11-19 | Thomson Licensing | Method and apparatus for compressing and decompressing a higher order ambisonics representation for a sound field |
EP2800401A1 (en) | 2013-04-29 | 2014-11-05 | Thomson Licensing | Method and Apparatus for compressing and decompressing a Higher Order Ambisonics representation |
US20140358557A1 (en) * | 2013-05-29 | 2014-12-04 | Qualcomm Incorporated | Performing positional analysis to code spherical harmonic coefficients |
US20170148449A1 (en) * | 2014-03-21 | 2017-05-25 | Dolby International Ab | Method for compressing a higher order ambisonics (hoa) signal, method for decompressing a compressed hoa signal, apparatus for compressing a hoa signal, and apparatus for decompressing a compressed hoa signal |
US20170180902A1 (en) * | 2014-03-21 | 2017-06-22 | Dolby International Ab | Method for compressing a higher order ambisonics (hoa) signal, method for decompressing a compressed hoa signal, apparatus for compressing a hoa signal, and apparatus for decompressing a compressed hoa signal |
US20150340044A1 (en) * | 2014-05-16 | 2015-11-26 | Qualcomm Incorporated | Higher order ambisonics signal compression |
US20180308500A1 (en) * | 2014-06-27 | 2018-10-25 | Dolby Laboratories Licensing Corporation | Method for decoding a higher order ambisonics (hoa) representation of a sound or soundfield |
US20170164133A1 (en) * | 2014-07-03 | 2017-06-08 | Dolby Laboratories Licensing Corporation | Auxiliary Augmentation of Soundfields |
US9531998B1 (en) * | 2015-07-02 | 2016-12-27 | Krush Technologies, Llc | Facial gesture recognition and video analysis tool |
US9961475B2 (en) * | 2015-10-08 | 2018-05-01 | Qualcomm Incorporated | Conversion from object-based audio to HOA |
US9813811B1 (en) * | 2016-06-01 | 2017-11-07 | Cisco Technology, Inc. | Soundfield decomposition, reverberation reduction, and audio mixing of sub-soundfields at a video conference endpoint |
US20180227665A1 (en) * | 2016-06-15 | 2018-08-09 | Mh Acoustics, Llc | Spatial Encoding Directional Microphone Array |
Non-Patent Citations (28)
Title |
---|
Abrard, et al., "A time-frequency blind signal separation method applicable to underdetermined mixture of dependent sources", Signal Processing, vol. 85, No. 7, 2005, pp. 1389-1403. |
Bach, et al., "Spectral clustering for speech separation", Automatic Speech and Speaker Recognition, 2009, pp. 221-250. |
Cohen, "Relative transfer function identification using speech signals", IEEE Transactions on Speech and Audio Processing, vol. 12, No. 5, Sep. 2004, pp. 451-459. |
Comon, "Independent component analysis, a new concept?", Signal Processing, vol. 36, No. 3, 1994, pp. 287-314. |
Durlach, "Note on binaural masking-level differences at high frequencies", J. Acoust. Soc. Am., vol. 36, No. 3, Mar. 1964, pp. 576-581. |
Epain, et al., "Blind Source Separation Using Independent Component Analysis in the Spherical Harmonic Domain", Proc. of the 2nd International Symposium on Ambisonics and Spherical Acoustics, May 6-7, 2010, 6 pages. |
Frey, et al., "Clustering by passing messages between data points", Science, vol. 315, Feb. 16, 2007, pp. 972-976. |
Gannot, et al., "Signal enhancement using beamforming and nonstationarity with applications to speech", IEEE Trans Signal Processing, vol. 49, 2001, pp. 1614-1626. |
Girolami, "Mercer kernel based clustering in feature space", IEEE Trans. Neural Networks, vol. 13, No. 3, 2002, pp. 780-784. |
Gribonval, et al., "A survey of sparse component analysis for blind source separation: principles, perspectives and new challenges", 14th European Symposium on Artificial Neural Networks, 2006, pp. 323-330. |
Hirsh, "The Influence of interaural phase on interaural summation and inhibition", J. Acoust. Soc. Am., vol. 20, No. 4, Jul. 1948, pp. 536-544. |
Hoshuyama, et al., "A robust adaptive beam-former for microphone arrays with a blocking matrix using constrained adaptive filters", IEEE Trans. Signal Processing, vol. 47, 1999, pp. 2677-2684. |
Hyvaerinen, et al., "Independent component analysis: algorithms and applications", Neural networks, vol. 13, No. 4, 2000, pp. 411-430. |
International Search Report and Written Opinion for International Application No. PCT/US2017/059723, dated May 18, 2018, 15 pages. |
Li, et al., "Underdetermined blind source separation based on sparse representation", IEEE Transactions on Signal Processing, vol. 54, No. 2, Feb. 2006, pp. 423-437. |
Markovich, et al., "Multichannel eigenspace beamforming in a reverberant noisy environment with multiple interfering speech signals", IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, No. 6, Aug. 2009, pp. 1071-1086. |
Mitianoudis, et al., "Using Beamforming in the Audio Source Separation Problem", Signal Processing and Its Applications, 2003, Jul. 1-4, 2003, 4 pages. |
Ng, et al., "On spectral clustering: analysis and an algorithm", Advances in Neural Information Processing Systems, MIT Press, vol. 14, 2002, 8 pages. |
Ozerov, et al., "Multichannel nonnegative matrix factorization in convolutive mixtures for audio source separation", IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, No. 3, 2010, pp. 550-563. |
Shalvi, et al., "System identification using nonstationary signals", IEEE Trans. Signal Processing, vol. 44, No. 8, Aug. 1996, pp. 2055-2063. |
Shi, et al., "Normalized cuts and image segmentation", IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, No. 8, 2000, pp. 888-905. |
Trevino, et al., "A Spatial Extrapolation Method to Derive High-Order Ambisonics Data From Stereo Sources", Journal of Information Hiding and Multimedia Signal Processing, vol. 6, No. 6, Nov. 1, 2015, pp. 1100-1116. |
Van Dongen, "Graph clustering by flow simulation", Ph.D. dissertation, University of Utrecht, 2001, 175 pages. |
Vlasblom, et al., "Markov clustering versus affinity propagation for the partitioning of protein interaction graphs", BMC Bioinformatics, vol. 10, No. 1, 2009, 14 pages. |
Von Luxburg, et al., "A tutorial on spectral clustering", Statistics and Computing, vol. 17, No. 4, 2007, pp. 395-416. |
Weisstein, "http://mathworld.wolfram.com/BesselFunctionoftheFirstKind.html", from MathWorld-A Wolfram Web Resource, retrieved on Jan. 25, 2017 from http://mathworld.wolfram.com/BesselFunctionoftheFirstKind.html, 6 pages. |
Weisstein, "http://mathworld.wolfram.com/BesselFunctionoftheFirstKind.html", from MathWorld—A Wolfram Web Resource, retrieved on Jan. 25, 2017 from http://mathworld.wolfram.com/BesselFunctionoftheFirstKind.html, 6 pages. |
Yilmaz, et al., "Blind separation of speech mixtures via time-frequency masking", IEEE Transactions on Signal Processing, vol. 52, No. 7, 2004, pp. 1830-1847. |
Also Published As
Publication number | Publication date |
---|---|
WO2018140109A1 (en) | 2018-08-02 |
EP3523801B1 (en) | 2024-04-10 |
US20190259397A1 (en) | 2019-08-22 |
CN109964272A (en) | 2019-07-02 |
US10839815B2 (en) | 2020-11-17 |
CN109964272B (en) | 2023-12-12 |
EP3523801A1 (en) | 2019-08-14 |
US20180218740A1 (en) | 2018-08-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11671781B2 (en) | Spatial audio signal format generation from a microphone array using adaptive capture | |
US10839815B2 (en) | Coding of a soundfield representation | |
US11832080B2 (en) | Spatial audio parameters and associated spatial audio playback | |
US10873814B2 (en) | Analysis of spatial metadata from multi-microphones having asymmetric geometry in devices | |
US11659349B2 (en) | Audio distance estimation for spatial audio processing | |
CN112567763B (en) | Apparatus and method for audio signal processing | |
JP2020500480A5 (en) | ||
US11350213B2 (en) | Spatial audio capture | |
CN112513980A (en) | Spatial audio parameter signaling | |
WO2018234625A1 (en) | Determination of targeted spatial audio parameters and associated spatial audio playback | |
US20230260525A1 (en) | Transform ambisonic coefficients using an adaptive network for preserving spatial direction | |
KR102284811B1 (en) | Incoherent idempotent ambisonics rendering | |
US20230362537A1 (en) | Parametric Spatial Audio Rendering with Near-Field Effect | |
TWI841483B (en) | Method and apparatus for rendering ambisonics format audio signal to 2d loudspeaker setup and computer readable storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KLEIJN, WILLEM BASTIAAN;SKOGLUND, JAN;LIM, SZE CHIE;SIGNING DATES FROM 20170125 TO 20170126;REEL/FRAME:041127/0001 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |