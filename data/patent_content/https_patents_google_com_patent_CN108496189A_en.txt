CN108496189A - The regularization of machine learning model - Google Patents
The regularization of machine learning model Download PDFInfo
- Publication number
- CN108496189A CN108496189A CN201680079738.9A CN201680079738A CN108496189A CN 108496189 A CN108496189 A CN 108496189A CN 201680079738 A CN201680079738 A CN 201680079738A CN 108496189 A CN108496189 A CN 108496189A
- Authority
- CN
- China
- Prior art keywords
- feature
- learning model
- machine learning
- loss
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
Abstract
Include the mthods, systems and devices of the computer program encoded on computer storage media, the feature weight maintained by machine learning model for regularization.This method includes following action：Acquisition includes the training data set of multiple training feature vectors；And the training machine learning model in each training feature vector, include for each feature vector：For each in multiple features of feature vector：Determine the first-loss for the feature vector with feature；Determine the second loss for the feature vector without this feature；And the current fractional return for the feature is updated using the first-loss and the second loss, wherein the fractional return for the feature indicates serviceability of the feature in generating the Accurate Prediction result for training feature vector.
Description
Cross reference to related applications
This application claims entitled " the Regularization of Machine submitted on December 4th, 2015
The equity of the U.S. Provisional Application No. 62/263,340 of Learning Models " is integrally incorporated by reference herein.
Background technology
Machine learning model can be configured as the scheduled label training data set of analysis, then be carried out from training data
Certain deductions.After model is trained to, model can be fed different not labeled data acquisition systems, and be based on instructing
Practice the deduction learnt during the stage to summarize each item in different data set.
In some cases, machine learning model can only be trained to one based on specific predetermined labels training data set
It is secondary.In other cases, machine learning model can be online machine learning model, can be when receiving new data periodically more
Newly.
Machine learning model may be to training data overfitting.Overfitting may be described as machine learning model mirror
Become over-confident in specific training data set.When machine learning model overfitting, it may start to make
Undesirable summary.
Invention content
According to the disclosure novel aspects, a kind of method for regularization machine learning model is disclosed, it is described
Machine learning model is configured as the feature vector that reception includes multiple features and is generated as input and from described eigenvector
Prediction output.The method may include following actions：Acquisition includes the training data set of multiple training feature vectors；And
The training machine learning model in each training feature vector includes for each feature vector：For the multiple of feature vector
Each in feature：Determine the first-loss for the feature vector with feature；It determines for the spy without this feature
Second loss of sign vector；And update the current income point for the feature using the first-loss and the second loss
Number, wherein indicate that the feature is generating the Accurate Prediction knot for training feature vector for the fractional return of the feature
Serviceability in fruit.
According to each novel aspects of the disclosure, the feature weight that can be maintained by machine learning model by regularization come
Reduce overfitting.Can existing feature weight set associated with the feature of feature vector be changed by using fractional return
To generate regularization feature weight.In some implementations, this method may include for each training feature vector selection with
Meet the associated characteristic set of fractional return for the predetermined threshold that can be used for training machine learning model.For each training
Vector only selects those features associated with the fractional return for meeting predetermined threshold to be additionally provided with training machine learning model
The advantages of reducing the size of machine learning model and improving the efficiency of machine learning model.
Other aspects include corresponding system, the device of the action for executing method of the coding on computer memory device
And computer program.One or more system for computer for being configured as executing specific operation or action mean system
Software, firmware, hardware or combinations thereof have been had mounted thereto, system is made to execute operation or action in operation.For wanting
Being configured to one or more computer programs of execution specific operation or action means that one or more programs are included in by counting
Device is set to execute the instruction of operation or action when being executed according to processing unit.
These and other aspects can optionally include one or more following characteristics.For example, update is directed to the feature
Current fractional return may include：It determines the difference between the first-loss and the second loss and uses the difference update
The current fractional return.Determine for feature feature vector first-loss can be based on the feature with tight
It is associated to connect the non-regularization feature weight for the feature determined in preceding trained iteration.It determines for without institute
The second loss for stating the feature vector of feature is based on the feature by reducing the feature to by the machine learning model
The weight of the influence of the result of generation scales.
In some implementations, the method can also include：It is pre- to determine whether the fractional return for feature meets
Determine revenue threshold；And in response to determining that the fractional return for feature meets estimated earnings threshold value, it is based on the fractional return
To scale non-regularization weight associated with the feature.Alternately or in addition, the method can also include：Determine needle
Whether estimated earnings threshold value is met to the fractional return of feature；And in response to determining that it is pre- that the fractional return for feature is unsatisfactory for
Determine revenue threshold, by non-regularization weight be scaled when being predicted by feature from machine learning model the considerations of in exclude
Value.In each corresponding substitute scheme, the fractional return can be used for determining the degree that feature weight is scaled, reduces etc..
In some implementations, the method can also include：It is pre- to determine whether the fractional return for feature meets
Determine revenue threshold；And the estimated earnings threshold value is unsatisfactory for based on the determination fractional return, it is moved from the learning model
Except the feature is without the use of this feature.
In some implementations, the method can also include：Based on income associated with each individual features point
It is several that each feature is ranked up.
In some implementations, the method can also include：Determination will be included in the pre- of the feature in learning model
Fixed number amount；And the feature based on sequencing selection predetermined quantity.
In some implementations, determination to be included in the feature in the learning model predetermined quantity can be based on use
In the amount for the free memory for storing the learning model.
In some implementations, the machine learning model can be on-line study model.
Theme described in this specification can realize in a particular embodiment, so as to realize one in following advantages or
It is multiple.For example, the theme of this specification is convenient for the training with machine learning model to be performed in conjunction with feature selecting.The disclosure
Various aspects may include realizing the performance of raising by reducing model size and improved model precision.Specifically, the explanation
Book discloses the generation of fractional return and uses generated fractional return to select to helping its target of model realization to have
The feature of actively impact.Therefore, as described in this description, by the way that fractional return to be merged into model training processing, model
Size can reduce, while keep high-caliber accuracy and performance.These methods also improve mould by reducing overfitting
Type accuracy.
Description of the drawings
Fig. 1 is the exemplary block diagram of the system for training machine learning model.
Fig. 2A is to provide the exemplary block diagram for the system that regularization is executed based on fractional return.
Fig. 2 B are the examples of the accumulated earnings scores vector of feature vector.
Fig. 3 be for training feature vector each feature to feature weight carry out regularization processing example.
The details of these and other embodiments is illustrated in the the accompanying drawings and the following description.From the description and the appended drawings and power
During profit requires, other feature and advantage will be evident.
Specific implementation mode
Present specification describes be embodied as computer program on the computer of one or more of one or more positions
System how the feature weight that regularization is maintained by machine learning model.Characteristic set associated with machine learning model can
To include the multiple n dimensional feature vectors trained from the example in data acquisition system.
In in one aspect of the present disclosure, machine learning model training system is disclosed, can be configured to determine that machine
The fractional return (benefit score) of each feature of the characteristic set of learning model.Fractional return can be instruction by machine
Whether the special characteristic for the n dimensional feature vectors that device learning model maintains has to Accurate Prediction result training machine learning model
Any calculated value.In in one aspect of the present disclosure, can for example, by by the loss of not special characteristic with being directed to
Difference between the loss of the complete study weight of special characteristic is cumulative, and fractional return is determined to each special characteristic.In any instruction
Practice in example, when all features in addition to just newer special characteristic in model use the canonical from previous training iteration
When changing weight, each corresponding loss is calculated.
Fig. 1 is the exemplary block diagram of the machine learning model training system 100 for training machine learning model 110.Machine
Device learning model can be that for example neural network, linear regression machine learning model, Logic Regression Models, any generalized linear return
Return machine learning model etc..Machine learning model can be online machine learning model, offline or batch machine learning model etc..
Machine learning model training system 100 is that computer journey is embodied as on one or more of one or more positions computer
The example of the system of sequence, wherein systems described below, component and technology may be implemented.Machine learning model training system 100
Training data item using the database (or data acquisition system) 120 from training data item carrys out training machine learning model 110.Instruction
It may include multiple training feature vectors to practice data item.Each training vector may include multiple values, and each value corresponds to resource
Special characteristic.In addition, system maintains the data of the respective weights of specified each feature included in feature vector.
Machine learning model 110 is configured as receiving input training data item 122 and handles input training data item 122
To generate output category score.
Machine learning model 110 can be configured as receive correspond to any kind of numerical data input feature to
Amount, and any kind of score, prediction or classification output are generated based on the input.For example, if arriving machine learning model 110
Input be based on from the feature vector of the feature of image zooming-out, then machine learning model 110 is the feature vector received
The output of generation can be for the score of each in object type set, wherein each fraction representation image includes to belong to
The possibility of the estimation of the image of the object of the category.
As another example, if the input to machine learning model 110 is based on from internet resource, document or document
Extracting section feature feature vector, then be that output that the feature vector received generates can be with by machine learning model 110
It is for the score of each in theme set, wherein each fraction representation internet resource, document or documentation section are to close
In the possibility of the estimation of the theme.
As another example, if the input to machine learning model 110 is based on the personalized recommendation for user
The feature vector of feature, such as the feature of the context of recommendation is characterized, such as characterize the feature for the previous behavior that user carries out, then
Can be point for each in collection of content items by the output that machine learning model 110 is the feature vector generation received
Number, wherein each fraction representation user by advantageously in response to be recommended the content item estimation possibility.In these examples
In some in, machine learning model 110 is a part for the reinforcement learning system for providing a user commending contents.
As another example, if the input to machine learning model 110 is based on a kind of feature of the text of language
It can be for another language that feature vector, which is then the output that the feature vector received generates by machine learning model 110,
The score of each in text fragments set, wherein the text fragments of each fraction representation another kind language are the input texts
To the possibility for the estimation of the another kind language suitably translated.
As another example, if to machine learning model 110 input be the feature based on spoken utterance feature to
It can be for every in text fragments set that amount, which is then the output that the feature vector received generates by machine learning model 110,
One score, each fraction representation text segment are the possibilities of the estimation of the correct transcript of spoken utterance.
In order to enable machine learning model 110 to be that the data item received generates accurately output, machine learning model instruction
Practice 100 training machine learning model 110 of system to adjust the value of the parameter of machine learning model 110, such as with true from initial value
Determine the trained values of parameter.
In training machine learning model 110, machine learning model training system 100 is used from label training data item
Database (data acquisition system) 120 training item.Database 120 stores multiple trained item set, wherein multiple trained item collection
Each of conjunction training item is associated with corresponding label.In general, be used to train the marker recognition training data item of item
Correct classification (or prediction), that is, the classification of training data item should be identified as by the score that machine learning model 110 generates
Classification.In some implementations, the flag data for given training item is the score distribution indicated by vector, the score point
Cloth includes the corresponding scores that each of tag set marks, one or more correct labelings of mid-score reflection training item.
With reference to figure 1, training data item 122 can be associated with training label 124.
100 training machine learning model 110 of machine learning model training system is with optimization object function.Optimization object function
It may include such as minimum loss function 130.In general, loss function 130 is to depend on function below：(i) by engineering
The output 112 that model 110 generates by handling given training data item 122 is practised, and (ii) is used for training data item 122
Label 124, that is, machine learning model 110 is exported by handling the target that should generate of training data item 122.
Regular machinery learning model training system 100 can be normal by being executed to the training data item from database 120
Advise successive ignition (such as stochastic gradient method, the stochastic gradient descent with backpropagation of machine learning model training technique
Deng), come iteratively adjust machine learning model 110 parameter value, carry out training machine learning model 110 with minimum (accumulation)
Loss function 130.It is then possible to which completely trained machine learning model 110 is deployed as can be used for being based on unlabelled input
The prediction model that data are predicted.
The disclosure is changed by using the novel method for contributing to feature selecting to be combined with the training of machine learning model
Into regular machinery learning model training system 100.This method provides the size to the prediction model of machine learning model and deployment
Control, and reduce overfitting (overfitting).In order to reduce overfitting and reduce the size of machine learning model,
The disclosure is by using the feature such as maintained by machine learning model about Fig. 2A, Fig. 2 B and fractional return shown in Fig. 3 adjustment
Weight carrys out the feature weight that regularization is maintained by machine learning model 110.It can be based on the feature ought during the training period with feature
The first-loss that is generated by loss function when vector is handled by machine learning model and ought not have feature during the training period
The second loss generated by loss function when feature vector is handled by machine learning model, for the every of each training feature vector
A feature calculation fractional return.
Fig. 2A is to provide the exemplary block diagram for the system that regularization is executed based on fractional return.In some cases, it is based on
The regularization of fractional return contributes to complete inhibition machine learning model and/or the prediction model of deployment when inferring to use one
Or multiple features.Fig. 2A describes machine learning model training system 200, uses the database 220 from training data item
Training data item training machine learning model 210.Machine learning model training system 200 may include identical with system 100
Feature.In addition, system 200 may include the subsystem 240 for calculating fractional return.Subsystem 240 may include by system
200 are used for calculating one or more software algorithms of fractional return, are used for calculating the one or more of fractional return by system 200
Computer or combinations thereof.
System 200 can be used for executing the processing of the training and the regularization that include machine learning model.Machine learning model
Training and the processing of regularization start from system 200 and obtain training feature vector 222 from the database 220 of training data item.It is special
Sign vector can be connected (joint) by feature value vector.When machine learning model processing feature vector, feature value vector can be with
For changing by each of machine learning model maintenance study weight.
In fig. 2, training feature vector 222 is fed as input to machine learning model by system 200 during the training period
210.Then, for each feature in training feature vector 222,210 parallel processing training feature vector of machine learning model
222, wherein having feature 226 at 232A, and do not have feature 226 at 232B.Such as institute in feature weight set 322A
Show, passes through its for being set as being determined by previous training iteration by the regularization weight 326a for maintaining feature 226 by system
Complete non-regularization weight handles the training feature vector 222 with feature 226 to realize.Complete regularization weight can be with
Including training any value calculated by machine learning model during iteration previous, the 326A of feature is being represented completely not just
Then change weight.Alternatively, the weight 326A of some other forms of feature, such as regularization weight can be used.It can will be special
Residue character weight in sign weight set 322A is set as their current regularization values from previous training iteration.If worked as
Preceding trained iteration is to train iteration, the then non-regularization weight of the feature handled and the regularization of residue character weight for the first time
Value will be " 0 ".
In a similar way, as shown in feature weight set 322B, pass through the spy that will be maintained for feature 226 by system
Sign weight 326b is set as " 0 " to realize training feature vector 222 of the processing without feature 226.It can be by feature weight collection
It closes the residue character weight in 322B and is set as their current regularization values from previous training iteration.Alternatively, can make
With the magnitude (reduced magnitude) of the reduction of feature 326b weights.If currently training iteration is that training changes for the first time
In generation, then the current regularization value of residue character weight will be " 0 ".
Then, machine learning model provides two output 212A and 212B, they indicate there is feature 226 when processing respectively
With the output of machine learning model when feature vector 222 without feature 226.System 200 can use loss function 230
212A and trained label associated with training feature vector 222 224 are exported based on machine learning model to calculate first-loss
232A.In a similar way, system 200 can use loss function 230 to export 212B to be concurrently based on machine learning model
Associated trained label 224 calculates the second loss 232B with training feature vector 222.
First-loss 232A instructions：When the processing of machine learning model 210 has the feature vector 222 of feature 226, exist
The margin of error between output 212A and training label 224.In addition to associated with feature 226 by " 1 ", 326a is represented complete
Non- regularization weight except, calculate first-loss using its regularization weight 322A using residue character.Similarly, second
Lose 232B instructions：When machine learning model 210 processing without feature feature vector 222 when, be present in output 212B and
The margin of error between training label 224.Other than the feature weight 326b for being arranged to " 0 ", in the surplus of feature weight 322B
In the case that remaining part point is arranged to their current regularization value, the second loss is calculated.
Computer 240 be configured as receive (i) based on by machine learning model 210 to the training characteristics with feature 226
The first-loss 232A and (ii) that the processing of vector 222 is calculated by loss function 230 are based on by machine learning model 210 to not
The second loss 232B that the parallel processing of training feature vector 222 with feature 226 is calculated by loss function 230.Computer
240 are configured as generating fractional return 250 based on the losses of first-loss 232A and second 232B.Fractional return 250 may include
Numerical value, the special characteristic which provides such as feature 226 measure the serviceability for improving the performance of machine learning model 210.
Fractional return 250 can be based on the difference between the losses of first-loss 232A and second 232B.
System 200 can be used for being iteratively performed the processing described with reference to figure 2A to each feature of each training vector,
To calculate the fractional return for each feature for being directed to each training feature vector.
Fig. 2 B are the examples of the accumulated earnings scores vector of feature vector.Moreover, Fig. 2 B provide training feature vector
222, the feature weight set 422 of the non-regularization from previous training iteration and the example of accumulated earnings scores vector 522.
System 240 can calculate the fractional return of each feature for each training feature vector, as with reference to figure 2A institutes
Description.It is that such fractional return calculates the result is that accumulated earnings scores vector 522 for each feature vector 222.It is tired
Product fractional return vector 522 is the fractional return value vector for the special characteristic for each corresponding to training feature vector 222.For example,
Fractional return 526 " .75 " can correspond to the feature 226 of feature vector 222.
Fractional return 526 can be used for adjusting the feature weight collection from the previous training iteration for feature vector 222
Close the feature weight 426 of the non-regularization in 422.For example, fractional return 526 can be compared with predetermined threshold.If
It determines that fractional return 526 meets predetermined threshold, then can carry out zoom feature weight 426 using fractional return 526.In some cases
Under, determine that fractional return 526 meets predetermined threshold and system can be caused to scale the regularization multiple of special characteristic use ' 1'
Non- regularization weight.Alternatively, in some cases, it may include reducing spy to carry out zoom feature weight 426 using fractional return
Levy the value of weight 426.
However, if it is determined that fractional return 526 is unsatisfactory for predetermined threshold, then it can reduce feature weight 426.In some feelings
Under condition, feature weight 426 can moderately be reduced to reduce the influence that feature 226 infers machine learning model 210.At it
In the case of him, when fractional return 526 fails to meet predetermined threshold, feature weight 426 can be reduced fully to " 0 ".By feature
226 feature weight 426 be reduced to " 0 " from for prediction 210 characteristic set of machine learning model (but be not from training more
New characteristic set) in remove this feature.Feature weight after adjustment can be maintained regularization by machine learning model 210
Feature weight.Therefore, each feature 226 of each training feature vector 222 can with the feature weight 426 of non-regularization, receive
Beneficial score 526 is related to the regularization feature weight that the function of fractional return 526 determines to as non-regularization feature weight 426
Connection.The prediction model of deployment may ignore all features that its regularization weight reduces or be decreased to " 0 ".
The feature weight of each corresponding non-regularization may include positive value, negative value or zero.Fractional return can also be just
, zero or negative.Negative fractional return means target can be made to degrade using corresponding feature.In some implementations, canonical
It is derived never in regularization weight and fractional return to change weight.Regularization weight cannot reversed non-regularization weight symbol
Number, but can be smaller magnitude or " 0 ".It in some implementations, can be used as the scaling of the function of fractional return
Device never regularization weight exports regularization weight.However, it is possible to use other methods export regularization weight.For example, working as
When creating regularization weight based on non-regularization feature weight and fractional return, the mathematical operation in addition to multiplication can be used.
It is, for example, possible to use the addition of feature weight and income weight.Alternately or in addition, fractional return can be used
Sigmoid functions.It is even possible that with other mathematical functions come the fractional return of non-the regularization weight and feature of feature based
Regularization weight is created for special characteristic.
In the example described about Fig. 2A and Fig. 2 B, multiple vectors, including non-regularization feature weight vector are discussed
422, accumulated earnings scores vector 422 and regularization feature weight vector 322A and 322B.Although these corresponding vectors are being schemed
Be shown as including special value in 2A and Fig. 2 B, but the disclosure should not necessarily be limited by including numerical value, or be present in above-mentioned
Relationship between numerical value.But non-canonical can be calculated or otherwise determined using method disclosed in this specification
Change the value of feature weight, fractional return and regularization feature weight.
Fig. 3 is the example for the processing for carrying out regularization to feature weight using each feature for training feature vector.
For convenience, processing 300 will be described as being executed by being located at one or more of one or more positions system for computer.
For example, the system of such as machine learning model training system 100 can be according to this explanation (used also as the system 200 in Fig. 2A)
Book is suitably programmed to execute processing 300.
System can execute processing 300 by obtaining training feature vector 302 since the database of training item come.It maintains
Training data item in the database of training data item can be obtained from the third party of preceding mark training data item.Training number
May include multiple training feature vectors according to item.In some implementations, the training feature vector obtained is trained with foundation
The label of feature vector correctly classified is associated.The label may be determined by one or more human users.In some realities
In existing mode, feature vector is associated with for the characteristic value of each feature.
Next, system is concurrently handled with special characteristic by using machine learning model and is not had special characteristic
The training feature vector that is obtained start training managing.Being obtained with special characteristic is handled by machine learning model
Training feature vector include：Handle the training feature vector of the specific non-regularization weight with this feature obtained.It replaces
Dai Di can use other functions of regularization weight, such as regularization weight.It is handled by machine learning model and does not have spy
The training feature vector obtained for determining feature includes：Processing special characteristic weight is arranged to being obtained for " 0 " or some small value
The training feature vector obtained.
As the training feature vector handled with special characteristic and without special characteristic as a result, system can use
Loss function come determine the first-loss 304A with special characteristic and also determine without special characteristic second loss
304B.Each corresponding loss is calculated using the feature weight from previous training iteration.There is special characteristic when determining
When first-loss, system uses the regularization feature from the previous training iteration for all features in addition to special characteristic
Weight carrys out counting loss.However, for special characteristic, system use from the previous training iteration for special characteristic not just
Then change feature weight to determine first-loss.Then, when determining the second loss without feature, system is used only from first
The regularization weight of the every other feature of preceding trained iteration determines the loss without this feature.It changes for first time training
In generation, non-regularization weight, regularization weight and the fractional return of each feature will be " 0 ".Alternatively, in some realization methods
In, non-regularization weight, regularization weight and the fractional return of each feature may be set to be the initial value in addition to " 0 ".
System is used in the function of the identified loss with feature calculated at 304A, at the end of each trained iteration more
The non-regularization feature weight 305 newly arrived from the previously model of training iteration.The example of this loss function is the gradient of loss
It is multiplied by learning rate function.
In the stage 306, system may be used at 304A the determining first-loss with special characteristic and at 304B it is true
Second fixed without special characteristic loses to update the accumulated earnings score for special characteristic.For example, update is for spy
The accumulated earnings score for determining feature includes the difference between determining first-loss and the second loss.It is then possible to based on determining
First-loss and the second loss between difference update the accumulated earnings score from previous ones.
In some but the realization method that is not all of, system can determine the income for special characteristic in the stage 308
Whether score meets estimated earnings score threshold.This may include for example will be for the newer accumulated earnings score of special characteristic
It is compared with estimated earnings score threshold.Estimated earnings score threshold can be provided for controlling machine learning model and deployment
Prediction model size and accuracy mechanism.This is because fractional return threshold value can be controlled using machine learning model
The quantity of the feature used when (classification, prediction etc.) for his purpose.In some implementations, receipts can be strategically set
Beneficial score threshold is to ensure when executing prediction with model using only the feature for the prediction of result for improving machine learning model.It substitutes
Ground or additionally can select fractional return threshold value to directly control the size of machine learning model, will such as refer to the stage 310 more
It describes in detail.As byproduct, in some implementations, fractional return can be used for the feature in model by important
Property is ranked up, with improved model performance.
The size of control machine learning model is disposing complete or part training and regularization machine learning model work
To provide significant advantage when prediction model.For example, all aspects of this disclosure allow in a particular manner (for example, less
Feature) the specific machine learning model of customization, the equipment (example of the first kind for being deployed as to have limited storing capacity
Such as, smartwatch, smart phone, tablet computer, a secondary intelligent glasses etc.) in prediction model.Alternatively, for example, when deployment
For that can have the equipment of the Second Type of bigger storage capacity (for example, laptop computer, desktop computer, server meter
Calculation machine etc.) in prediction model when, the disclosure also allows (for example, more features) to customize same machines in different ways
Practise model.Altematively or additionally, smaller more accurate model can also be deployed in the equipment with bigger storage capacity
In.
It is related to special characteristic to adjust that system may then based on the newer accumulated earnings score for special characteristic
The non-regularization feature weight of connection, to create regularization feature weight 310.For example, if the system determine that for special characteristic
Newer accumulated earnings score meets estimated earnings score threshold, then system can be determined that the special characteristic for helping engineering
It is useful to practise the object function of model optimization machine learning model.In this case, system can be by the regularization of bigger
Weight distribution is to special characteristic, to encourage machine learning model to depend on the special characteristic in prediction result.Alternatively, example
Such as, if the system determine that being unsatisfactory for estimated earnings score threshold for the newer accumulated earnings score of special characteristic, then system
It can be determined that this feature is useless for helping machine learning model to optimize the object function of machine learning model.Such
In the case of, system can reduce the magnitude of feature weight associated with the special characteristic, to create regularization feature weight,
Machine learning model is prevented to depend on the special characteristic in prediction result.In some cases, system, which can determine that, to go completely
Except the special characteristic as the feature that can be used by the prediction model of machine learning model or deployment.In this case, system
The magnitude of regularization feature weight associated with special characteristic can be reduced to " 0 ".
In some implementations, practical newer accumulated earnings score can be used for adjusting for special characteristic not just
Then change feature weight to create the feature weight of regularization.For example, newer accumulated earnings score can be used applied to not
The one or more previous fractional return values applied in the mathematical function of such as Sigmoid functions of regularization weight carry out phase
Add, to determine regularization weight.Alternatively, non-regularization weight can be zoomed into some canonicals using accumulated earnings score
Change weight, wherein scaling means value weight being multiplied by between 0 and 1.In some implementations, it is understood that there may be as by tiring out
The dull dependence of the increased result of effect caused by product fractional return.For example, score is bigger, it is included in regularization weight
The ratio (fraction) of non-regularization weight is bigger.In some implementations, it is less than the receipts of estimated earnings score threshold
Beneficial score will exclude the use of individual features from prediction, and the feature that fractional return meets threshold value will have regularization weight,
The regularization weight is determined independently of fractional return never regularization weight.In one example, these weights will be equal to not
Regularization weight.In another example, these weights will start after meeting threshold value for the first time depending on updated fractional return
The quantity for the training example seen.
In the stage 312, system can determine whether the training feature vector obtained in the stage 302 includes another feature.
If training feature vector includes another feature, system can be directed to each feature of training feature vector from stage 304A and
304B starts searching loop processing 300 again.Alternatively, if it is determined that do not include in the training feature vector that the stage 302 obtains
Another feature, then handling 300 can terminate in the stage 316.System can be directed to each training in training feature vector set
Feature vector is iteratively performed processing 300, until all training data items have been trained to.
At any time, currently stored model may be used to the example do not seen of the prediction with some feature vectors
Label.The prediction of label is executed by using the regularization weight of all features in exemplary feature vector.It is this pre-
Model needed for surveying can be deployed as prediction model.The prediction model of deployment can only include the feature that regularization weight is not " 0 ".
This is when training is terminated especially suitable for model.Only need storage that there is the spy different from 0 regularization weight in this stage
The weight of sign.The storage size of the prediction model of deployment can be controlled in this way.
The reduction of the size of the prediction model of the improvement of the accuracy of machine learning model, machine learning model or deployment or
The two can be realized by adjusting the feature weight that is maintained by system.For example, improved model can be carried out in the following manner
Accuracy：(1) suitably regularization have larger fractional return feature feature weight；(2) it excludes to have in prediction negative
The use of the feature of fractional return；Or (3) combination thereof.It alternately or in addition, can be by increasing estimated earnings score
Threshold value reduces the size of model, to exclude more features from model.For example, for drop below estimated earnings point
Those associated features of fractional return of number threshold value, system can be programmed to feature weight being reduced to zero.By feature
Regularization weight, which is reduced to zero, will prevent machine learning model from considering this feature during deduction and prediction, trained to reduce
At when need the dimension of feature stored.The monotonic function of fractional return can determine feature based on used fractional return
Regularization weight amount.Alternately or in addition, which, which can also use, is adjustable as expected useful feature in model
The parameter of ratio, using assist in for by non-regularization weight regularization as the zoom factor of regularization weight.
Alternatively, can using one or more processing steps come only selected that when being inferred machine learning model or
The subset for the feature that the prediction model of deployment relies on.For example, system can the associated income based on each individual features point
It counts to be ranked up to each feature in each corresponding training feature vector.Then, system only selection can have satisfaction pre-
Determine those of the fractional return of fractional return threshold value feature.Then, machine learning model can only use meet estimated earnings score
The feature of the feature vector of threshold value.
In some implementations, can be that each specific feature group establishes corresponding estimated earnings score threshold.It is right
Different threshold values can be established in different feature groups.In such realization method, machine learning model can be customized to
Rely only on the best features set from each specific feature group.Can based on the estimated earnings score threshold that meets respective sets
The features of the associated each respective sets of fractional return of value determines best features.
In some implementations, it may not be necessary to the calculating and storage of actual gain score.On the contrary, system can generate
The mathematical approach of fractional return, the feature weight for the study that can be combined as the counting with the movable data instance of this feature
The function of loss and blip counting with optimization is made, and or come self-learning algorithm storage any other amount.
Theme described in this specification, feature operation and the embodiment of processing can be with Fundamental Digital Circuits, visibly
The computer software or firmware of implementation, computer hardware (including structure disclosed in this specification and its equivalent structures) or it
One or more of combination realize.The embodiment of theme described in this specification may be implemented as one or more
A computer program encodes one or more modules of the computer program instructions on tangible non-volatile program carrier,
Operation for being executed by data processing equipment or being controlled data processing equipment.Alternately or in addition, program instruction can be by
It encodes on manually generated transmitting signal, such as electric signal, optical signal or the electromagnetic signal that machine generates, the signal are generated
Suitable receiver apparatus is used for transmission for data processing equipment execution to carry out coding to information.Computer storage is situated between
Matter can be machine readable storage device, machine readable storage substrate, random or serial access memory equipment or they in
One or more combinations.
Term " data processing equipment " covers device, equipment and the machine of all kinds for handling data, such as wraps
Include programmable processor, computer or multiple processors or computer.The device may include dedicated logic circuit, such as FPGA
(field programmable gate array) or ASIC (application-specific integrated circuit).In addition to hardware, which can also be including being involved
Computer program create performing environment code, such as constitute processor firmware, protocol stack, data base management system, operation
The code of the combination of system or one or more of which.
(it can also be referred to as or be described as program, software, software application, module, software module, foot to computer program
This or code) it can be with including compiling or interpreted language or declaratively or any type of programming language of process programming language comes
It writes, and can be disposed in any form, including or as stand-alone program or as module, component, subroutine be applicable in
In other units of computing environment.Computer program can with but not necessarily correspond to the file in file system.Program can deposit
Store up one in the file (for example, being stored in one or more of marking language document script) for preserving other programs or data
It in point, is exclusively used in the single file of discussed program, or is stored in multiple coordination files (for example, storage one or more
The file of a module, subprogram or code section) in.Computer program can be deployed as in a computer or multiple computers
Upper execution, these computer bits are in a website or are distributed across multiple websites and pass through interconnection of telecommunication network.
Processing described in this specification and logic flow can by execute one of one or more computer programs or
Multiple programmable calculators execute, to execute function by being operated to input data and generating output.Processing and logic
Flow can also be by can be implemented as dedicated logic circuit (for example, FPGA (field programmable gate array) or ASIC are (special integrated
Circuit)) it executes, and device can also be by its realization.
For example, general or specialized microprocessor or two can be based on by being adapted for carrying out the computer of computer program
The central processing unit of person or any other type.In general, central processing unit will be from read-only memory or random access memory
Device or both receives instruction and data.The primary element of computer be performed for or the central processing unit of operating instruction and
For storing instruction with one or more memory devices of data.In general, computer will also include or be operatively coupled to
One or more mass-memory units (for example, disk, magneto-optic disk or CD) for storing data, with from massive store
Equipment receives data or transfers data to mass-memory unit or both.But computer must not necessarily have it is this
Equipment.Furthermore, it is possible to which computer is embedded in such as mobile phone, personal digital assistant (PDA), Mobile audio frequency or video playing
Device, game console, global positioning system (GPS) receiver or portable memory apparatus are (for example, universal serial bus (USB)
Flash drive) etc. another equipment in.
Computer-readable medium suitable for storing computer program instructions and data includes the non-volatile of form of ownership
Memory, medium and memory devices, such as including semiconductor memory devices, such as EPROM, EEPROM and flash memory device；
Disk, such as internal hard drive or removable disk；Magneto-optic disk；And CD-ROM and DVD-ROM disks.Processor and memory can be by
Supplemented is incorporated in.
In order to provide the interaction with user, the embodiment of theme described in this specification can be realized on computers,
The computer has display equipment (for example, CRT (cathode-ray tube) or LCD (liquid crystal display) monitor), is used for user
Show information；And user can provide the keyboard and pointer device of input, such as mouse or trace ball by it to computer.
Other kinds of equipment can be used for providing the interaction with user；For example, it can be any form to be supplied to the feedback of user
Sense feedback, such as visual feedback, audio feedback or touch feedback；And it can be to include sound, voice or sense of touch
Any form receive input from the user.In addition, computer can be sent by the equipment that is used to user document and from
The equipment receives document and is interacted with user；Such as by the request in response to being received from web browser, webpage is sent to use
Web browser on the user equipment at family.
The embodiment of theme described in this specification can realize that the computing system includes rear end group in computing systems
Part, such as data server；Or including middleware component, such as application server；Or including front end assemblies, such as
Client computer with graphic user interface or Web browser, user can pass through the graphic user interface or web browsing
Device is interacted with the realization method of theme described in this specification；Either rear end as one or more, middleware or preceding
Hold any combinations of component.The component of system can pass through any form or the digital data communications (such as communication network) of medium
Interconnection.The example of communication network includes LAN (" LAN ") and wide area network (" WAN "), such as internet.
Computing system may include client and server.Client and server is generally remote from each other and usually passes through
Communication network interacts.Relationship between client and server is by running on the respective computers and having each other
There is the computer program of client-server relation to generate.
Although this specification includes many specific implementation details, what these details were not necessarily to be construed as pair being claimed
The limitation of range, and should be interpreted may be specific to the description of the feature of specific embodiment.Individually implementing in this specification
Certain features described in the context of example can also combine realization in single embodiment.On the contrary, in single embodiment
Various features described in context can also individually or with any suitable sub-portfolio be realized in various embodiments.This
Outside, it although can describe feature as working with certain combinations above and even initially so require, comes from and is wanted
Ask the one or more features of the combination of protection that can be deleted from combination in some cases, and combination claimed
The variant of sub-portfolio or sub-portfolio can be directed to.
Similarly, although depicting operation in the accompanying drawings with certain order, this is understood not to require with shown
The certain order gone out or the such operation of execution in order, or all operations shown are executed to realize desired knot
Fruit.In some cases, it may be advantageous for multitask and parallel processing.In addition, the various system components in above-described embodiment
Separation be understood not to be required for this separation in all embodiments, and it should be understood that described program groups
Part and system usually can together be integrated in single software product or be encapsulated into multiple software product.
The specific embodiment of theme has been described.Other embodiment is in the range of following claims.For example, right
The action enumerated in it is required that can be performed in a different order and still realize desired result.As an example, in attached drawing
The processing of description be not necessarily required to shown in certain order or sequential order to realize desired result.In certain realization methods
In, it may be advantageous for multitask and parallel processing.Other steps or stage can be provided, or can be from described processing
Exclude step or stage.Therefore, other realization methods are in the range of following claims.
Claims (33)
1. a kind of computer implemented method for regularization machine learning model, the machine learning model is configured as connecing
Packet receiving includes the feature vector of multiple features and generates prediction output as input and from described eigenvector, the method includes：
Acquisition includes the training data set of multiple training feature vectors；And
The training machine learning model in each training feature vector includes for each feature vector：
For each in multiple features of feature vector：
Determine the first-loss for the feature vector with feature；
Determine the second loss for the feature vector without the feature；And
The current fractional return for the feature is updated using the first-loss and second loss, wherein is directed to
The fractional return of the feature indicates serviceability of the feature in generating the Accurate Prediction result for training feature vector.
2. according to the method described in claim 1, wherein, update includes for the current fractional return of the feature：Determine institute
State the difference between first-loss and the second loss and using current fractional return described in the difference update.
3. the method according to claim 1 or claim 2, wherein determine for the feature vector with feature
One loss is based on the feature and the non-regularization feature for the feature determining in immediately preceding trained iteration
Weight is associated.
4. according to the method in any one of claims 1 to 3, wherein determine for without the feature feature to
Second loss of amount is the influence based on the feature with the reduction feature to the result generated by the machine learning model
Weight it is associated.
5. according to any method of the preceding claims, the method further includes：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
Meet estimated earnings threshold value for the fractional return of feature in response to determining, is scaled based on the fractional return and feature
The associated non-regularization weight.
6. method according to claim 1 to 4, the method further includes：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
In response to determining that the fractional return for feature is unsatisfactory for estimated earnings threshold value, non-regularization weight is scaled and is being carried out
When prediction by the feature from machine learning model the considerations of in the value that excludes.
7. method according to claim 1 to 4, the method further includes：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
It is unsatisfactory for the estimated earnings threshold value based on the determination fractional return, the feature is removed from the learning model.
8. according to any method of the preceding claims, the method further includes：
Each feature is ranked up based on fractional return associated with each individual features.
9. according to the method described in claim 8, the method further includes：
Determination will be included in the predetermined quantity of the feature in learning model；And
Feature based on sequencing selection predetermined quantity.
10. according to the method described in claim 9, wherein it is determined that the predetermined number for the feature that be included in the learning model
The amount based on the free memory for storing the learning model of measuring.
11. the method according to any one of the preceding claims, wherein the machine learning model is on-line study mould
Type.
12. a kind of system for regularization machine learning model, it includes multiple that the machine learning model, which is configured as receiving,
The feature vector of feature generates prediction output as input and from described eigenvector, the system comprises：
One or more storage devices of one or more computers and store instruction, described instruction is by one or more of
Computer can be operable so that one or more of computers when executing and execute operation, and the operation includes：
Acquisition includes the training data set of multiple training feature vectors；And
The training machine learning model in each training feature vector includes for each feature vector：
For each in multiple features of feature vector：
Determine the first-loss for the feature vector with feature；
Determine the second loss for the feature vector without the feature；And
The current fractional return for the feature is updated using the first-loss and the second loss, wherein for described
The fractional return of feature indicates serviceability of the feature in generating the Accurate Prediction result for training feature vector.
13. system according to claim 12, wherein update includes determining institute for the current fractional return of the feature
State the difference between first-loss and the second loss and using current fractional return described in the difference update.
14. according to the system described in claim 12 or claim 13, wherein determine for the feature vector with feature
First-loss is special based on the feature and the non-regularization for the feature determining in immediately preceding trained iteration
It is associated to levy weight.
15. the system according to any one of claim 12 to 14, wherein determine for the feature without the feature
Second loss of vector is based on the feature by reducing the feature to the result that is generated by the machine learning model
The weight of influence scales.
16. according to system described in any one of claim 12 to 15, the operation further includes：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
Meet estimated earnings threshold value for the fractional return of feature in response to determining, based on the fractional return come scale with it is described
The associated non-regularization weight of feature.
17. according to system described in any one of claim 12 to 15, the operation further includes：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
In response to determining that the fractional return for feature is unsatisfactory for estimated earnings threshold value, non-regularization weight is scaled and is being carried out
When prediction by the feature from machine learning model the considerations of in the value that excludes.
18. according to system described in any one of claim 12 to 15, the operation further includes：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
It is unsatisfactory for the estimated earnings threshold value based on the determination fractional return, the feature is removed from the learning model.
19. the system according to any one of claim 12 to 18, the operation further include：
Each feature is ranked up based on fractional return associated with each individual features.
20. system according to claim 19, the operation further include：
Determination will be included in the predetermined quantity of the feature in learning model；And
Feature based on sequencing selection predetermined quantity.
21. system according to claim 20, wherein determination will be included in the predetermined number of the feature in the learning model
The amount based on the free memory for storing the learning model of measuring.
22. the system according to any one of claim 12 to 21, wherein the machine learning model is on-line study mould
Type.
23. a kind of non-transitory computer-readable medium of storage software, the software includes can be by one or more computers
The instruction of execution, one or more of computers make one or more of computers execute for canonical when executing instruction
Change machine learning model operation, the machine learning model be configured as receive include multiple features feature vector as defeated
Enter and generate prediction output from described eigenvector, the operation includes：
Acquisition includes the training data set of multiple training feature vectors；And
The training machine learning model in each training feature vector includes for each feature vector：
For each in multiple features of feature vector：
Determine the first-loss for the feature vector with feature；
Determine the second loss for the feature vector without the feature；And
The current fractional return for the feature is updated using the first-loss and the second loss, wherein for described
The fractional return of feature indicates serviceability of the feature in generating the Accurate Prediction result for training feature vector.
24. computer-readable medium according to claim 23, wherein current fractional return of the update for the feature
Including the difference between the determination first-loss and the second loss and use current fractional return described in the difference update.
25. according to the computer-readable medium described in claim 23 or claim 24, wherein determine and be directed to feature
The first-loss of feature vector is to be directed to the feature with what is determined in immediately preceding trained iteration based on the feature
Non- regularization feature weight is associated.
26. the computer-readable medium according to any one of claim 23 to 25, wherein determine described for not having
Second loss of the feature vector of feature is based on the feature by reducing the feature to being given birth to by the machine learning model
At result influence weight scaling.
27. the computer-readable medium according to any one of claim 23 to 26, the operation further include：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
Meet estimated earnings threshold value for the fractional return of feature in response to determining, based on the fractional return come scale with it is described
The associated non-regularization weight of feature.
28. the computer-readable medium according to any one of claim 23 to 26, the operation further include：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
In response to determining that the fractional return for feature is unsatisfactory for estimated earnings threshold value, non-regularization weight is scaled and is being carried out
When prediction by the feature from machine learning model the considerations of in the value that excludes.
29. the computer-readable medium according to any one of claim 23 to 26, the operation further include：
Determine whether the fractional return for feature meets estimated earnings threshold value；And
It is unsatisfactory for the estimated earnings threshold value based on the determination fractional return, the feature is removed from the learning model.
30. the computer-readable medium according to any one of claim 23 to 29, the operation further include：
Each feature is ranked up based on fractional return associated with each individual features.
31. computer-readable medium according to claim 30, the operation further include：
Determination will be included in the predetermined quantity of the feature in learning model；And
Feature based on sequencing selection predetermined quantity.
32. computer-readable medium according to claim 31, wherein determination will be included in the spy in the learning model
Amount of the predetermined quantity of sign based on the free memory for storing the learning model.
33. the computer-readable medium according to any one of claim 23 to 32, wherein the machine learning model is
On-line study model.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562263340P | 2015-12-04 | 2015-12-04 | |
US62/263,340 | 2015-12-04 | ||
PCT/US2016/064836 WO2017096312A1 (en) | 2015-12-04 | 2016-12-02 | Regularization of machine learning models |
Publications (2)
Publication Number | Publication Date |
---|---|
CN108496189A true CN108496189A (en) | 2018-09-04 |
CN108496189B CN108496189B (en) | 2022-07-12 |
Family
ID=57590878
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680079738.9A Active CN108496189B (en) | 2015-12-04 | 2016-12-02 | Method, system, and storage medium for regularizing machine learning models |
Country Status (3)
Country | Link |
---|---|
US (1) | US10600000B2 (en) |
CN (1) | CN108496189B (en) |
WO (1) | WO2017096312A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111481411A (en) * | 2020-04-20 | 2020-08-04 | 绍兴启视电子科技有限公司 | Control system of goggles |
CN113159834A (en) * | 2021-03-31 | 2021-07-23 | 支付宝(杭州)信息技术有限公司 | Commodity information sorting method, device and equipment |
Families Citing this family (60)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10586151B1 (en) | 2015-07-31 | 2020-03-10 | Perceive Corporation | Mitigating overfitting in training machine trained networks |
US11475310B1 (en) | 2016-11-29 | 2022-10-18 | Perceive Corporation | Training network to minimize worst-case error |
US10620618B2 (en) * | 2016-12-20 | 2020-04-14 | Palantir Technologies Inc. | Systems and methods for determining relationships between defects |
US10699184B2 (en) * | 2016-12-29 | 2020-06-30 | Facebook, Inc. | Updating predictions for a deep-learning model |
US11017295B1 (en) | 2017-05-01 | 2021-05-25 | Perceive Corporation | Device storing ternary weight parameters for machine-trained network |
US10581953B1 (en) * | 2017-05-31 | 2020-03-03 | Snap Inc. | Real-time content integration based on machine learned selections |
US11037063B2 (en) | 2017-08-18 | 2021-06-15 | Diveplane Corporation | Detecting and correcting anomalies in computer-based reasoning systems |
US11010672B1 (en) | 2017-09-01 | 2021-05-18 | Google Llc | Evolutionary techniques for computer-based optimization and artificial intelligence systems |
US10713570B1 (en) | 2017-10-04 | 2020-07-14 | Diveplane Corporation | Evolutionary programming techniques utilizing context indications |
US11625625B2 (en) | 2018-12-13 | 2023-04-11 | Diveplane Corporation | Synthetic data generation in computer-based reasoning systems |
US11941542B2 (en) | 2017-11-20 | 2024-03-26 | Diveplane Corporation | Computer-based reasoning system for operational situation control of controllable systems |
US11727286B2 (en) | 2018-12-13 | 2023-08-15 | Diveplane Corporation | Identifier contribution allocation in synthetic data generation in computer-based reasoning systems |
US11676069B2 (en) | 2018-12-13 | 2023-06-13 | Diveplane Corporation | Synthetic data generation using anonymity preservation in computer-based reasoning systems |
US11640561B2 (en) | 2018-12-13 | 2023-05-02 | Diveplane Corporation | Dataset quality for synthetic data generation in computer-based reasoning systems |
US11669769B2 (en) | 2018-12-13 | 2023-06-06 | Diveplane Corporation | Conditioned synthetic data generation in computer-based reasoning systems |
US11092962B1 (en) | 2017-11-20 | 2021-08-17 | Diveplane Corporation | Computer-based reasoning system for operational situation vehicle control |
US10916053B1 (en) * | 2019-11-26 | 2021-02-09 | Sdc U.S. Smilepay Spv | Systems and methods for constructing a three-dimensional model from two-dimensional images |
US11270523B2 (en) * | 2017-11-29 | 2022-03-08 | Sdc U.S. Smilepay Spv | Systems and methods for constructing a three-dimensional model from two-dimensional images |
US11403813B2 (en) | 2019-11-26 | 2022-08-02 | Sdc U.S. Smilepay Spv | Systems and methods for constructing a three-dimensional model from two-dimensional images |
US10671888B1 (en) | 2017-12-14 | 2020-06-02 | Perceive Corporation | Using batches of training items for training a network |
US11537870B1 (en) | 2018-02-07 | 2022-12-27 | Perceive Corporation | Training sparse networks with discrete weight values |
US10909604B1 (en) * | 2018-03-07 | 2021-02-02 | Amazon Technologies, Inc. | Artificial intelligence system for automated selection and presentation of informational content |
US11586902B1 (en) | 2018-03-14 | 2023-02-21 | Perceive Corporation | Training network to minimize worst case surprise |
GB201804433D0 (en) * | 2018-03-20 | 2018-05-02 | Microsoft Technology Licensing Llc | Imputation using a neutral network |
US11262742B2 (en) * | 2018-04-09 | 2022-03-01 | Diveplane Corporation | Anomalous data detection in computer based reasoning and artificial intelligence systems |
US11454939B2 (en) | 2018-04-09 | 2022-09-27 | Diveplane Corporation | Entropy-based techniques for creation of well-balanced computer based reasoning systems |
US11385633B2 (en) * | 2018-04-09 | 2022-07-12 | Diveplane Corporation | Model reduction and training efficiency in computer-based reasoning and artificial intelligence systems |
US10817750B2 (en) * | 2018-04-09 | 2020-10-27 | Diveplane Corporation | Data inclusion in computer-based reasoning models |
US10816980B2 (en) * | 2018-04-09 | 2020-10-27 | Diveplane Corporation | Analyzing data for inclusion in computer-based reasoning models |
US10816981B2 (en) * | 2018-04-09 | 2020-10-27 | Diveplane Corporation | Feature analysis in computer-based reasoning models |
US11436496B2 (en) * | 2018-04-20 | 2022-09-06 | Google Llc | Systems and methods for regularizing neural networks |
US11887003B1 (en) * | 2018-05-04 | 2024-01-30 | Sunil Keshav Bopardikar | Identifying contributing training datasets for outputs of machine learning models |
US10733287B2 (en) | 2018-05-14 | 2020-08-04 | International Business Machines Corporation | Resiliency of machine learning models |
US10270644B1 (en) * | 2018-05-17 | 2019-04-23 | Accenture Global Solutions Limited | Framework for intelligent automated operations for network, service and customer experience management |
US11880775B1 (en) | 2018-06-05 | 2024-01-23 | Diveplane Corporation | Entropy-based techniques for improved automated selection in computer-based reasoning systems |
US20190377984A1 (en) * | 2018-06-06 | 2019-12-12 | DataRobot, Inc. | Detecting suitability of machine learning models for datasets |
US11802537B2 (en) * | 2018-08-13 | 2023-10-31 | International Business Machines Corporation | Methods and systems for wave energy generation prediction and optimization |
US10997614B2 (en) * | 2018-10-09 | 2021-05-04 | Oracle International Corporation | Flexible feature regularization for demand model generation |
EP3861487A1 (en) | 2018-10-30 | 2021-08-11 | Diveplane Corporation | Clustering, explainability, and automated decisions in computer-based reasoning systems |
US11494669B2 (en) | 2018-10-30 | 2022-11-08 | Diveplane Corporation | Clustering, explainability, and automated decisions in computer-based reasoning systems |
US11176465B2 (en) | 2018-11-13 | 2021-11-16 | Diveplane Corporation | Explainable and automated decisions in computer-based reasoning systems |
US11087170B2 (en) * | 2018-12-03 | 2021-08-10 | Advanced Micro Devices, Inc. | Deliberate conditional poison training for generative models |
US10769529B2 (en) * | 2018-12-04 | 2020-09-08 | Google Llc | Controlled adaptive optimization |
US11847567B1 (en) | 2018-12-05 | 2023-12-19 | Perceive Corporation | Loss-aware replication of neural network layers |
US11604973B1 (en) | 2018-12-05 | 2023-03-14 | Perceive Corporation | Replication of neural network layers |
US11763176B1 (en) | 2019-05-16 | 2023-09-19 | Diveplane Corporation | Search and query in computer-based reasoning systems |
US11030801B2 (en) | 2019-05-17 | 2021-06-08 | Standard Cyborg, Inc. | Three-dimensional modeling toolkit |
US11443244B2 (en) * | 2019-06-05 | 2022-09-13 | International Business Machines Corportation | Parallel ensemble of machine learning algorithms |
US11836583B2 (en) * | 2019-09-09 | 2023-12-05 | Huawei Cloud Computing Technologies Co., Ltd. | Method, apparatus and system for secure vertical federated learning |
US11341605B1 (en) * | 2019-09-30 | 2022-05-24 | Amazon Technologies, Inc. | Document rectification via homography recovery using machine learning |
US11636387B2 (en) | 2020-01-27 | 2023-04-25 | Microsoft Technology Licensing, Llc | System and method for improving machine learning models based on confusion error evaluation |
US11710042B2 (en) * | 2020-02-05 | 2023-07-25 | Adobe Inc. | Shaping a neural network architecture utilizing learnable sampling layers |
US11514364B2 (en) * | 2020-02-19 | 2022-11-29 | Microsoft Technology Licensing, Llc | Iterative vectoring for constructing data driven machine learning models |
US11636389B2 (en) | 2020-02-19 | 2023-04-25 | Microsoft Technology Licensing, Llc | System and method for improving machine learning models by detecting and removing inaccurate training data |
US11663481B2 (en) | 2020-02-24 | 2023-05-30 | Adobe Inc. | Neural network architecture pruning |
US11768945B2 (en) * | 2020-04-07 | 2023-09-26 | Allstate Insurance Company | Machine learning system for determining a security vulnerability in computer software |
US20220067052A1 (en) * | 2020-09-01 | 2022-03-03 | Roblox Corporation | Providing dynamically customized rankings of game items |
US11321527B1 (en) * | 2021-01-21 | 2022-05-03 | International Business Machines Corporation | Effective classification of data based on curated features |
US20220382424A1 (en) * | 2021-05-26 | 2022-12-01 | Intuit Inc. | Smart navigation |
WO2023147870A1 (en) * | 2022-02-04 | 2023-08-10 | Telefonaktiebolaget Lm Ericsson (Publ) | Response variable prediction in a communication network |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7970718B2 (en) * | 2001-05-18 | 2011-06-28 | Health Discovery Corporation | Method for feature selection and for evaluating features identified as significant for classifying data |
US6571225B1 (en) * | 2000-02-11 | 2003-05-27 | International Business Machines Corporation | Text categorizers based on regularizing adaptations of the problem of computing linear separators |
US9141622B1 (en) * | 2011-09-16 | 2015-09-22 | Google Inc. | Feature weight training techniques |
US9031897B2 (en) * | 2012-03-23 | 2015-05-12 | Nuance Communications, Inc. | Techniques for evaluation, building and/or retraining of a classification model |
US9489373B2 (en) * | 2013-07-12 | 2016-11-08 | Microsoft Technology Licensing, Llc | Interactive segment extraction in computer-human interactive learning |
US9734436B2 (en) * | 2015-06-05 | 2017-08-15 | At&T Intellectual Property I, L.P. | Hash codes for images |
US10529318B2 (en) * | 2015-07-31 | 2020-01-07 | International Business Machines Corporation | Implementing a classification model for recognition processing |
RU2672394C1 (en) * | 2017-07-26 | 2018-11-14 | Общество С Ограниченной Ответственностью "Яндекс" | Methods and systems for evaluation of training objects through a machine training algorithm |
US11694109B2 (en) * | 2017-08-16 | 2023-07-04 | ODH, Inc. | Data processing apparatus for accessing shared memory in processing structured data for modifying a parameter vector data structure |
-
2016
- 2016-12-02 CN CN201680079738.9A patent/CN108496189B/en active Active
- 2016-12-02 WO PCT/US2016/064836 patent/WO2017096312A1/en active Application Filing
- 2016-12-02 US US15/368,447 patent/US10600000B2/en active Active
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111481411A (en) * | 2020-04-20 | 2020-08-04 | 绍兴启视电子科技有限公司 | Control system of goggles |
CN113159834A (en) * | 2021-03-31 | 2021-07-23 | 支付宝(杭州)信息技术有限公司 | Commodity information sorting method, device and equipment |
CN113159834B (en) * | 2021-03-31 | 2022-06-07 | 支付宝(杭州)信息技术有限公司 | Commodity information sorting method, device and equipment |
Also Published As
Publication number | Publication date |
---|---|
WO2017096312A1 (en) | 2017-06-08 |
CN108496189B (en) | 2022-07-12 |
US10600000B2 (en) | 2020-03-24 |
US20170161640A1 (en) | 2017-06-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108496189A (en) | The regularization of machine learning model | |
US20190220781A1 (en) | Training distilled machine learning models | |
US9740680B1 (en) | Computing numeric representations of words in a high-dimensional space | |
WO2018211138A1 (en) | Multitask neural network systems | |
CN107463701B (en) | Method and device for pushing information stream based on artificial intelligence | |
CN110520871A (en) | Training machine learning model | |
CN108140143A (en) | Regularization machine learning model | |
CN108475505A (en) | Using partial condition target sequence is generated from list entries | |
CN109359247B (en) | Content pushing method, storage medium and computer equipment | |
US20230049747A1 (en) | Training machine learning models using teacher annealing | |
US10430825B2 (en) | Recommending advertisements using ranking functions | |
US20150356658A1 (en) | Systems And Methods For Serving Product Recommendations | |
CN113627846A (en) | Inventory adjusting method and device, electronic equipment and computer readable medium | |
CN114240555A (en) | Click rate prediction model training method and device and click rate prediction method and device | |
CN112015990A (en) | Method and device for determining network resources to be recommended, computer equipment and medium | |
CN113343101B (en) | Object ordering method and system | |
AU2014357328B2 (en) | Ranking autocomplete results based on a business cohort | |
CN111489196B (en) | Prediction method and device based on deep learning network, electronic equipment and medium | |
AU2019200721B2 (en) | Online training and update of factorization machines using alternating least squares optimization | |
CN111597430A (en) | Data processing method and device, electronic equipment and storage medium | |
CN111340605A (en) | Method and device for training user behavior prediction model and user behavior prediction | |
CN114331379B (en) | Method for outputting task to be handled, model training method and device | |
CN112348587B (en) | Information pushing method and device and electronic equipment | |
CN117369962A (en) | Workflow execution sequence generation method, device, computer equipment and storage medium | |
CN114817716A (en) | Method, device, equipment and medium for predicting user conversion behaviors and training model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |