CN117043858A - Cyclic neural network-transducer model for performing speech recognition - Google Patents
Cyclic neural network-transducer model for performing speech recognition Download PDFInfo
- Publication number
- CN117043858A CN117043858A CN202180096093.0A CN202180096093A CN117043858A CN 117043858 A CN117043858 A CN 117043858A CN 202180096093 A CN202180096093 A CN 202180096093A CN 117043858 A CN117043858 A CN 117043858A
- Authority
- CN
- China
- Prior art keywords
- embedding
- network
- output
- rnn
- sequence
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000001537 neural effect Effects 0.000 title claims description 8
- 125000004122 cyclic group Chemical group 0.000 title description 4
- 239000013598 vector Substances 0.000 claims abstract description 82
- 239000011159 matrix material Substances 0.000 claims abstract description 25
- 238000009826 distribution Methods 0.000 claims abstract description 14
- 230000007246 mechanism Effects 0.000 claims description 43
- 238000000034 method Methods 0.000 claims description 36
- 238000012545 processing Methods 0.000 claims description 13
- 230000000306 recurrent effect Effects 0.000 claims description 9
- 238000012935 Averaging Methods 0.000 claims description 6
- 238000013528 artificial neural network Methods 0.000 claims description 3
- 230000015654 memory Effects 0.000 description 29
- 238000003860 storage Methods 0.000 description 12
- 230000027455 binding Effects 0.000 description 10
- 238000009739 binding Methods 0.000 description 10
- 238000013518 transcription Methods 0.000 description 9
- 230000035897 transcription Effects 0.000 description 9
- 238000004590 computer program Methods 0.000 description 8
- 238000004891 communication Methods 0.000 description 7
- 238000010586 diagram Methods 0.000 description 5
- 238000003058 natural language processing Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- 230000004044 response Effects 0.000 description 3
- 230000001934 delay Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 238000010606 normalization Methods 0.000 description 2
- 229920002430 Fibre-reinforced plastic Polymers 0.000 description 1
- 230000009471 action Effects 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000002131 composite material Substances 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 239000004744 fabric Substances 0.000 description 1
- 239000011151 fibre-reinforced plastic Substances 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 238000002372 labelling Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000008439 repair process Effects 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/083—Recognition networks
Abstract
The RNN-T model (200) includes a predictive network (300), the predictive network (300) being configured to receive a non-blank symbol sequence (301) at each time step subsequent to an initial time step. For each non-blank symbol, the prediction network is further configured to generate an embedding (306) of the corresponding non-blank symbol using the shared embedding matrix (304), assign a respective position vector (308) to the non-blank symbol, and weight the embedding in proportion to a similarity between the embedding and the respective position vector. The prediction network is further configured to generate a single embedded vector at a corresponding time step (350). The RNN-T model also includes a federated network (230) configured to receive, at each of a plurality of time steps subsequent to the initial time step, a single embedded vector generated as an output from the predictive network at the corresponding time step, and to generate a probability distribution of possible speech recognition hypotheses.
Description
Technical Field
The present disclosure relates to a recurrent neural network-transducer (RNN-T) model of binding (ted) and reduction (reduced).
Background
Modern Automatic Speech Recognition (ASR) systems are focused not only on providing high quality (e.g., low Word Error Rate (WER)), but also on providing low latency (e.g., short latency between user speech and transcription occurrence). Furthermore, when ASR systems are used today, ASR systems are required to decode utterances in streaming fashion corresponding to or even faster than real-time. To illustrate, when an ASR system is deployed on a mobile phone that experiences direct user interactivity, applications on the mobile phone that use the ASR system may require that speech recognition be streamed so that the word appears on the screen as soon as it is spoken. Here, the user of the mobile phone may also have a low tolerance to time delays. Because of this low tolerance, speech recognition efforts are run on mobile devices in a manner that minimizes impact from delays and inaccuracies that may adversely affect the user experience.
Disclosure of Invention
One aspect of the present disclosure provides a recurrent neural network transducer (RNN-T) model, comprising: a predictive network configured to, at each of a plurality of time steps subsequent to an initial time step: a sequence of non-blank symbols output by the final Softmax layer is received as input. The prediction network is further configured to, for each non-blank symbol in the sequence of non-blank symbols received as input at a corresponding time step: generating an embedding of the corresponding non-blank symbol using the shared embedding matrix; assigning the respective position vectors to the corresponding non-blank symbols; and weighting the embedding in proportion to a similarity between the embedding and the corresponding position vector. The prediction network is further configured to generate as output a single embedding vector at the corresponding time step, the single embedding vector being based on a weighted average of the weighted embeddings. The RNN-T model further includes a federated network configured to, at each of the plurality of time steps subsequent to the initial time step: receiving as input the single embedded vector generated as output from the prediction network at corresponding time steps; and generating a probability distribution of possible speech recognition hypotheses at corresponding time steps.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the RNN-T model further includes an audio encoder configured to: receiving as input a sequence of acoustic frames; and at each of the plurality of time steps, generating a high-order feature representation of a corresponding acoustic frame in the sequence of acoustic frames. Here, the joint network is further configured to receive as input, at each of the plurality of time steps, the high-order feature representation generated by the audio encoder at the corresponding time step. In some examples, weighting the embedding in proportion to the similarity between the embedding and the respective position vector includes: the embedding is weighted in proportion to the cosine similarity between the embedding and the corresponding position vector. The sequence of non-blank symbols output by the final Softmax layer includes word chips. The sequence of non-blank symbols output by the final Softmax layer may include graphemes. Each of the embeddings may include the same dimension size as each of the position vectors. In some embodiments, the sequence of non-blank symbols received as input is limited to the N previous non-blank symbols output by the final Softmax layer. In these embodiments, N may be equal to 2. Alternatively, N may be equal to 5.
In some examples, the prediction network includes a multi-headed attention mechanism that shares the shared embedding matrix across each head of the multi-headed attention mechanism. In these examples, at each of the plurality of time steps subsequent to the initial time step, the predictive network is configured to, at each head of the multi-head attention mechanism, and for each non-blank symbol in the sequence of non-blank symbols received as input at the corresponding time step: generating, using the shared embedding matrix, the same embedding of corresponding non-blank symbols as the embedding generated at each other head of the multi-head attention mechanism; assigning to the corresponding non-blank symbols a respective position vector that is different from the respective position vector assigned to the corresponding non-blank symbol at each other head of the multi-head attention mechanism; and weighting the embedding in proportion to the similarity between the embedding and the corresponding position vector. Here, the prediction network is further configured to generate a weighted embedded respective weighted average of the sequence of non-blank symbols as an output from a corresponding head of the multi-head attention mechanism; and generating the single embedded vector as an output at a corresponding time step by averaging respective weighted averages of corresponding head outputs from the multi-head attention mechanism. In these examples, the multi-headed attention mechanism may include four heads. Optionally, the prediction network may bind the dimensions of the shared embedding matrix with the dimensions of the output layer of the federated network.
Another aspect of the present disclosure provides a computer-implemented method that, when executed on data processing hardware, causes the data processing hardware to perform operations. At each of a plurality of time steps subsequent to the initial time step, the operations include: receiving as input to a recurrent neural network-transducer (RNN-T) model a sequence of non-blank symbols output by the final Softmax layer; for each non-blank symbol in the sequence of non-blank symbols received as input at a corresponding time step: generating, by the prediction network, an embedding of the corresponding non-blank symbol using a shared embedding matrix; assigning, by the prediction network, a respective position vector to a corresponding non-blank symbol; and weighting the embedding by the prediction network in proportion to a similarity between the embedding and the corresponding position vector. At each of a plurality of time steps subsequent to the initial time step, the operations further comprise: generating a single embedded vector as an output from the prediction network at a corresponding time step; and generating, by the joint network of RNN-T models, a probability distribution of possible speech recognition hypotheses at corresponding time steps using the single embedded vector generated as output from the prediction network at corresponding time steps. The single embedding vector is based on a weighted average of the weighted embeddings.
Implementations of the disclosure may include one or more of the following optional features. In some embodiments, the operations further comprise: receiving a sequence of acoustic frames as input to an audio encoder; generating, by the audio encoder, a high-order feature representation of a corresponding acoustic frame in the sequence of acoustic frames at each of the plurality of time steps; and receiving the high-order feature representations generated by the audio encoder at corresponding time steps as inputs to the joint network. In some examples, weighting the embedding in proportion to the similarity between the embedding and the respective position vector includes: the embedding is weighted in proportion to cosine similarity between the embedding and the corresponding position vector.
The sequence of non-blank symbols output by the final Softmax layer may include word sheets (wordpieces). Alternatively, the sequence of non-blank symbols output by the final Softmax layer may comprise graphemes. Each of the embeddings may include the same dimension size as each of the position vectors. In some embodiments, the sequence of non-blank symbols received as input is limited to the N previous non-blank symbols output by the final Softmax layer. In these embodiments, N may be equal to 2. Alternatively, N may be equal to 5. Optionally, the prediction network may bind the dimensions of the shared embedding matrix with the dimensions of the output layer of the federated network.
In some examples, the prediction network includes a multi-headed attention mechanism that shares the shared embedding matrix across each head of the multi-headed attention mechanism. The multi-headed gaze mechanism may include four heads. In these examples, at each of the plurality of time steps subsequent to the initial time step, the operations may further include, at each head of the multi-head attention mechanism, and for each non-blank symbol in the sequence of non-blank symbols received as input at the corresponding time step: generating, by the prediction network, using the shared embedding matrix, a corresponding non-blank symbol of the same embedding as that generated at each other head of the multi-head attention mechanism; assigning, by the prediction network, to corresponding non-blank symbols, respective position vectors that are different from respective position vectors assigned to corresponding non-blank symbols at each other head of the multi-head attention mechanism; and weighting the embedding by the prediction network in proportion to a similarity between the embedding and the corresponding position vector. Here, at each of the plurality of time steps subsequent to the initial time step and at each head of the multi-head attention mechanism, the operations may further include generating, by the prediction network, a respective weighted average of the weighted embeddings of the non-blank symbol sequences as output from the corresponding head of the multi-head attention mechanism. Thereafter, at each of the plurality of time steps subsequent to the initial time step, the operations may further include generating the single embedded vector as an output from the prediction network at the corresponding time step by averaging respective weighted averages of corresponding head outputs from the multi-head attention mechanism.
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 is a schematic diagram of an example speech environment for transcribing speech using a recurrent neural network transducer (RNN-T) model.
FIG. 2 is a schematic diagram of an example RNN-T model architecture.
FIG. 3 is a schematic diagram of an example binding and simplified predictive network of the RNN-T model architecture of FIG. 2.
FIG. 4 is a graph depicting word error rate versus size for both bound and unbound prediction and federated networks.
FIG. 5 is a flow chart of an example arrangement of operations of a computer-implemented method of performing a binding and a simplified RNN-T model.
FIG. 6 is a schematic diagram of an example computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
Fig. 1 is an example of a speech environment 100. In the speech environment 100, the manner in which the user 104 interacts with a computing device, such as the user device 10, may be through voice input. User device 10 (also commonly referred to as device 10) is configured to capture sound (e.g., streaming audio data) from one or more users 104 within speech environment 100. Here, streaming audio data may refer to a spoken utterance 106 of a user 104 that is used as an audible query, command, or audible communication captured by the device 10 for the device 10. The speech-enabled system of device 10 may handle the query or command by answering the query and/or causing the command to be executed/fulfilled by one or more downstream applications.
User device 10 may correspond to any computing device associated with user 104 and capable of receiving audio data. Some examples of user devices 10 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, etc.), computers, wearable devices (e.g., smartwatches), smart appliances, internet of things (IoT) devices, vehicle infotainment systems, smart displays, smart speakers, etc. The user device 10 includes data processing hardware 12 and memory hardware 14 in communication with the data processing hardware 12 and storing instructions that, when executed by the data processing hardware 12, cause the data processing hardware 12 to perform one or more operations. The user device 10 further comprises an audio system 16, the audio system 16 having an audio capturing device (e.g., microphone) 16,16a for capturing and converting spoken utterances 106 within the speech environment 100 into electrical signals and a speech output device (e.g., speaker) 16,16b for delivering audible audio signals (e.g., as output audio data from the device 10). Although in the illustrated example the user device 10 implements a single audio capture device 16a, the user device 10 may implement an array of audio capture devices 16a, whereby one or more of the capture devices 16a in the array may not physically reside on the user device 10, but instead communicate with the audio system 16, without departing from the scope of the disclosure.
In the speech environment 100, an Automatic Speech Recognition (ASR) system 118 implementing a recurrent neural network-transducer (RNN-T) model 200 and an optional re-scorer 180 resides on the user device 10 of the user 104 and/or on a remote computing device 60 (e.g., one or more remote servers of a distributed system executing in a cloud computing environment) in communication with the user device 10 via the network 40. The user device 10 and/or the remote computing device 60 also includes an audio subsystem 108, the audio subsystem 108 being configured to receive the utterance 106 spoken by the user 104 and captured by the audio capture device 16a, and to convert the utterance 106 into a corresponding digital format associated with the input acoustic frame 110 that can be processed by the ASR system 118. In the illustrated example, the user speaks the respective utterance 106 and the audio subsystem 108 converts the utterance 106 into corresponding audio data (e.g., acoustic frames) 110 for input to the ASR system 118. Thereafter, the RNN-T model 200 receives as input audio data 110 corresponding to the utterance 106 and generates/predicts as output a corresponding transcription 120 (e.g., recognition result/hypothesis) of the utterance 106. In the illustrated example, the RNN-T model 200 may perform streaming speech recognition to produce the initial speech recognition results 120,120a, and the re-scorer 180 may update (i.e., re-score) the initial speech recognition results 120a to produce the final speech recognition results 120,120b.
The user device 10 and/or the remote computing device 60 also executes a user interface generator 107 configured to present a representation of the transcription 120 of the utterance 106 to the user 104 of the user device 10. As described in more detail below, the user interface generator 107 may display the initial speech recognition result 120a in a streaming manner during time 1 and then display the final speech recognition result 120b during time 2. In some configurations, the transcription 120 output from the ASR system 118 is processed, for example, by a Natural Language Understanding (NLU) module executing on the user device 10 or the remote computing device 60, to execute the user command/query specified by the utterance 106. Additionally or alternatively, a text-to-speech system (not shown) (e.g., executing on any combination of user device 10 or remote computing device 60) may convert the transcription into synthesized speech for audible output by user device 10 and/or another device.
In the illustrated example, the user 104 interacts with a program or application 50 (e.g., a digital assistant application 50) of the user device 10 using the ASR system 118. For example, fig. 1 depicts user 104 in communication with digital assistant application 50, and digital assistant application 50 displays digital assistant interface 18 on a screen of user device 10 to depict a conversation between user 104 and digital assistant application 50. In this example, user 104 asks digital assistant application 50"What time is the concert tonight? (what time is the tonight concert. In this example, the audio system 16 receives the spoken utterance 106 and converts it into an acoustic frame 110 for input to the ASR system 118.
Continuing with the example, the RNN-T model 200 encodes the acoustic frames 110 while receiving acoustic frames 110 corresponding to the utterance 106 when the user 104 is speaking, and then decodes the encoded acoustic frames 110 into the initial speech recognition result 120a. During time 1, user interface generator 107 presents a representation of initial speech recognition results 120a of utterance 106 to user 104 of user device 10 via digital assistant interface 18 in a streaming manner such that words, word fragments, and/or individual characters appear on the screen as soon as they are spoken. In some examples, the first look-ahead audio context is equal to zero.
During time 2, the user interface generator 107 presents a representation of the final speech recognition result 120b of the utterance 106 re-scored by the re-scorer 180 to the user 104 of the user device 10 via the digital assistant interface 18. In some implementations, the user interface generator 107 replaces the representation of the initial speech recognition result 120a presented at time 1 with the representation of the final speech recognition result 120b presented at time 2. Here, time 1 and time 2 may include time stamps corresponding to the time at which the user interface generator 107 presents the corresponding speech recognition result 120. In this example, the timestamp of time 1 indicates that the user interface generator 107 presented the initial speech recognition result 120a at an earlier time than the final speech recognition result 120 b. For example, since the final speech recognition result 120b is assumed to be more accurate than the initial speech recognition result 120a, the final speech recognition result 120b, which is ultimately displayed as a transcription 120, may repair any terms that may be incorrectly recognized in the initial speech recognition result 120a. In this example, the streaming initial speech recognition results 120a output by the RNN-T model 200 are displayed on the screen of the user device 10 at time 1, associated with low latency, and provide the user 104 with responsiveness to his/her query being processed, while the final speech recognition results 120b output by the re-scorer 180 and displayed on the screen at time 2 leverage additional speech recognition models and/or language models to improve speech recognition quality in terms of accuracy, but increased latency. However, since the initial speech recognition result 120a is displayed when the user speaks the speech 106, the higher latency associated with generating and ultimately displaying the final recognition result is not apparent to the user 104.
In the example shown in fig. 1, digital assistant application 50 may use natural language processing to respond to questions posed by user 104. Natural language processing generally refers to the process of interpreting a written language (e.g., initial speech recognition result 120a and/or final speech recognition result 120 b) and determining whether the written language suggests any actions. In this example, the digital assistant application 50 uses natural language processing to recognize that the problem from the user 104 relates to the user's calendar and, more specifically, to music on the user's calendar. By recognizing these details using natural language processing, the automated assistant returns a response 19 to the user query, where the response 19 states "Venue doors open at 6:30PM and concert starts at 8pm (venue open at 6:30pm and concert start at 8 pm)". In some configurations, natural language processing occurs on a remote server 60 in communication with the data processing hardware 12 of the user device 10.
Referring to fig. 2, an example frame-alignment based transducer model 200 includes a recurrent neural network-transducer (RNN-T) model architecture that complies with latency constraints associated with interactive applications. The RNN-T model 200 provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the RNN-T model architecture suitable for performing speech recognition entirely on the user device 102 (e.g., without requiring communication with a remote server). The RNN-T model 200 includes an encoder network 210, a prediction network 300, and a federated network 230. The predictive network 300 and the federated network 230 may collectively provide an RNN-T decoder. The encoder network 210, which is substantially similar to the Acoustic Model (AM) in conventional ASR systems, includes a cyclic network of stacked long-term memory (LSTM) layers. For example, the encoder reads a sequence of d-dimensional feature vectors (e.g., acoustic frames 110 (fig. 1)), x= (x) 1 ,x 2 ,·,x T ) Wherein, the method comprises the steps of, wherein,and a higher order feature representation is generated at each time step. The higher order feature representation is represented as
Similarly, the predictive network 300 is also an LSTM network, which, like the Language Model (LM), will be output by the final Softmax layer 240 so far a sequence y of non-blank symbols 301 0 ,…,y ui-1 Processing into a representationAs described in more detail below, represents Pu i 350 comprises a single embedded vector. Notably, the sequence of non-blank symbols 301 received at the predictive network 300 captures the language dependence between the non-blank symbols 301 predicted so far during the previous time stepTo assist the federated network 230 in predicting the probability of the next output symbol, or blank symbol, during the current time step. As described in more detail below, to facilitate techniques for reducing the size of the predictive network 300 without sacrificing accuracy/performance of the RNN-T model 200, the predictive network 300 may receive a finite historical sequence y of non-blank symbols ui -n,...,y ui-1 Which is limited to the N previous non-blank symbols 301 output by the final Softmax layer 240.
Finally, using the RNN-T model architecture, the representations generated by the encoder and prediction networks 210, 300 are combined by the federated network 230. Then, joint network prediction Which is the distribution over the next output symbol. In other words, the federated network 230 generates a probability distribution over possible speech recognition hypotheses at each output step (e.g., time step). Here, a "possible speech recognition hypothesis" corresponds to a set of output labels, each representing a symbol/character in a specified natural language. For example, when the natural language is english, the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26 letters in the english alphabet and one label for labeling a space. Thus, the federated network 230 may output a set of values that indicate the likelihood of occurrence of each tag in a predetermined set of output tags. The set of values can be vectors and can indicate a probability distribution over the output set of labels. In some cases, the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited. For example, the output tag set can include word pieces and/or whole words in addition to or instead of graphemes. The output profile of the federated network 230 can include a posterior probability value for each of the different output tags. Thus, if there are 100 different output labels representing different graphemes or other symbols, then the output y of network 230 is joined i 100 different probability values can be included, one for each output label. Then, probability scoreThe cloth can be used to select and assign scores to candidate orthographic (e.g., graphemes, word sheets, and/or words) elements in a bundle search process (e.g., by Softmax layer 240) to determine the transcription 120.
The Softmax layer 240 may employ any technique to select the output label/symbol in the distribution with the highest probability as the next output symbol predicted by the RNN-T model 200 at the corresponding output step. In this way, the RNN-T model 200 does not make a condition independence assumption, but rather the predictions for each symbol are conditioned not only on acoustics, but also on tag sequences output so far. The RNN-T model 200 does assume that the output symbols are independent of the future acoustic frames 110, which allows the RNN-T model to be employed in a streaming manner.
In some examples, the encoder network 210 of the RNN-T model 200 is composed of eight 2048-dimensional LSTM layers, each of which is followed by a 640-dimensional projection layer. In other embodiments, encoder network 210 includes a convolutional enhanced transformer (transformer) or a network of transformer layers. The predictive network 220 may have two 2048-dimensional LSTM layers, each of which is then also a 640-dimensional projection layer and a 128-element embedding layer. Finally, the federated network 230 may also have 640 hidden units. The Softmax layer 240 may consist of a unified word segment or grapheme collection generated using all unique word segments or graphemes in the training data. When the output symbol/label includes a word segment, the set of output symbols/labels may include 4,096 different word segments. When the output symbols/labels comprise graphemes, the set of output symbols/labels may comprise less than 100 different graphemes.
FIG. 3 shows a predictive network 300 of the RNN-T model 200 that receives a non-blank symbol sequence y ui-n ,…,y ui-1 As an input, the non-blank symbol sequence is limited to the N previous non-blank symbols 301a-N output by the final Softmax layer 240. In some examples, N is equal to 2. In other examples, N is equal to 5, however, the present disclosure is not limited and N may be equal to any integer. The sequence of non-blank symbols 301a-n indicates the initial speech recognition result 120a (FIG. 1). In some implementations, the predictive network 300 includes multiple attention machinesA system 302 that shares a shared embedding matrix 304 across each head 302A-302H of a multi-head attention mechanism. In one example, the multi-headed attention mechanism 302 includes four heads. However, the multi-headed attention mechanism 302 may employ any number of heads. Notably, the multi-headed attention mechanism significantly improves performance with minimal increase in model size. As described in more detail below, each head 302A-H includes its own row of position vectors 308, and instead of causing an increase in model size by concatenating outputs 318A-H from all heads, the outputs 318A-H are instead averaged by a head averaging module 322.
Referring to a first head 302A of the multi-head attention mechanism 302, the head 302A uses a shared embedding matrix 304 for a sequence y of non-blank symbols received as input at corresponding time steps from a plurality of time steps ui-n ,…,y ui-1 Each non-blank symbol 301 therein generates a corresponding embedded 306,306a-n (e.g.,). Notably, since the shared embedding matrix 304 is shared across all of the heads of the multi-head attention mechanism 302, the other heads 302B-H all generate the same corresponding embeddings 306 for each non-blank symbol. Header 302A also corresponds to sequence y of non-blank symbols ui-n ,…,y ui-1 Each corresponding non-blank symbol in (b) is assigned a respective position vector PV Aa–An 308,308aa-An (e.g.)>). The respective position vector PV 308 assigned to each non-blank symbol indicates a position in the history of the non-blank symbol sequence (e.g., the N previous non-blank symbols output by the final Softmax layer 240). For example, a first position vector PV Aa Assigned to the most recent position in the history, and the last position vector PV An Assigned to the last position in the history of N previous non-blank symbols output by the final Softmax layer 240. Notably, each of the embeddings 306 can include a phase with each of the position vectors PV 308 The same dimension (i.e., dimension size).
In the case of non-blank symbol sequences 301a-n, y for the shared embedding matrix 304 ui -n,…,y ui-1 While the corresponding embedding of each non-blank symbol 301 in the middle is the same at all of the heads 302A-H of the multi-head attention mechanism 302, each head 302A-H defines a different set/row of position vectors 308. For example, the first head 302A defines a position vector PV Aa–An 308Aa-An, the second header 302B defining a position vector PV Ba–Bn 308 Ba–Bn Is defined, and H-th header 302H defines a position vector PV Ha–Hn 308 Ha–Hn Is a different row of the same.
For each non-blank symbol in the received sequence of non-blank symbols 301a-n, the first head 302A also weights the corresponding insert 306 via the weighting layer 310 in proportion to the similarity between the corresponding insert 306 and the respective position vector PV 308 assigned to the corresponding insert 306. In some examples, the similarity may include cosine similarity (e.g., cosine distance). In the example shown, the weighting layer 310 outputs a sequence of weighting inserts 312,312aa-An, each weighting insert associated with a corresponding insert 306 weighted in proportion to the respective position vector PV 308 assigned to that weighting insert. In other words, the weighted embeddings 312 output by the weighting layer 310 for each embedment 306 may correspond to the dot product between the embedment 306 and the corresponding position vector PV 308. Weighted embedding 312 may be interpreted as focusing on the embedding in proportion to the degree of similarity of the embedding to the locations associated with their respective location vectors PV 308. To increase the computational speed, the prediction network 300 includes non-cyclic layers, and thus the sequence of weighted inserts 312Aa-An is not concatenated, but instead averaged by the weighted average module 316 to generate a weighted average 318A of the weighted inserts 312Aa-An as output from the first head 302A, the weighted average 318A of the weighted inserts 312Aa-An being represented by:
At equal intervalsIn equation 1, h represents the index of the header 302, n represents the position in the context, and e represents the embedding dimension. Additionally, in equation 1, H, N and d e Including the size of the corresponding dimension. The position vector PV 308 need not be trainable and may include random values. Notably, even if the weighted embedding 312 is averaged, the location vector PV 308 can potentially hold location history information, alleviating the need to provide a loop connection at each layer of the predictive network 300.
Each of the other heads 302B-H of the multi-head attention mechanism 302 similarly performs the operations described above with respect to the first head 302A. Due to the different set of positioning vectors PV 308 defined by each head 302, the weighting layer 310 outputs a sequence of weighting embeddings 312Ba-Bn,312Ha-Hn at each other head 302B-H that is different from the sequence of weighting embeddings 312Aa-Aa at the first head 302A. Thereafter, the weighted average module 316 generates a respective weighted average 318B-H of the corresponding weighted embeddings 312 of the sequence of non-blank symbols as output from each of the other corresponding headers 302B-H.
In the illustrated example, the prediction network 300 includes a head average module 322, the head average module 322 averaging the weighted averages 318A-H output from the corresponding heads 302A-H. The projection layer 326 with SWISH may receive as input the output 324 from the head average module 322 corresponding to the average of the weighted averages 318A-H and generate as output the projection output 328. The final layer normalization 330 may normalize the projection output 328 to provide a single embedded vector Pu at a corresponding one of a plurality of time steps i 350. The predictive network 300 generates only a single embedded vector Pu at each of a plurality of time steps subsequent to the initial time step i 350。
In some configurations, predictive network 300 does not implement multi-head attention mechanism 302 and only performs the operations described above with respect to first head 302A. In these configurations, the weighted average 318A of the weighted embeddings 312Aa-An is simply passed through the projection layer 326 and the layer normalization 330 to provide a single embedded vector Pu i 350。
Referring back to fig. 2, the federated network 230 receives a single embedded vector from the predictive network 300Pu i 350, and receive the high-order feature representation from encoder 210The joint network 230 generates a probability distribution on possible speech recognition hypotheses at the corresponding time steps +.>Here, the possible speech recognition hypotheses correspond to a set of output labels, each representing a symbolic character in a specified natural language. Probability distribution over possible speech recognition hypothesesIndicating the probability of the final speech recognition result 120b (fig. 1). That is, the federated network 230 uses a single embedded vector 350 that is based on a sequence of non-blank symbols (e.g., the initial speech recognition result 120 a) to determine the probability distribution of the final speech recognition result 120 b. The final Softmax layer 240 receives the probability distribution of the final speech recognition result 120b and selects the output label/symbol with the highest probability to produce the transcription.
The final speech recognition result 120b is assumed to be more accurate than the initial speech recognition result 120a because the RNN-T model 200 determines the initial speech recognition result 120a in a streaming manner and uses the previous non-blank symbols from the initial speech recognition result 120a to determine the final speech recognition result 120b. That is, the final speech recognition result 120b considers the previous non-blank symbols and is therefore assumed to be more accurate because the initial speech recognition result 120a does not consider any previous non-blank symbols. In addition, the re-scorer 180 (fig. 1) may update the initial speech recognition result 120a with the final speech recognition result 120b to provide transcription to the user 104 via the user interface generator 170.
In some embodiments, to further reduce the size of the RNN-T decoders, i.e., the predictive network 300 and the federated network 230, parameter bindings between the predictive network 300 and the federated network 230 are applied. Specifically, for vocabulary size |V| and embedding dimensiond e The shared embedding matrix 304 at the prediction network isMeanwhile, the last hidden layer includes a dimension size d at the federated network 230 h The feed-forward projection weights from the hidden layer to the output logits will be With additional blank tokens in the vocabulary. Thus, the feed-forward layer corresponding to the last layer of the federated network 230 includes a weight matrix [ d ] h ，|V]| a. The invention relates to a method for producing a fibre-reinforced plastic composite. The dimension d is to be embedded by causing the prediction network 300 to e Size of (d) and dimension of the last hidden layer of the federated network 230 h Binding, the feedforward projection weights of the joint network 230 and the shared embedding matrix 304 of the prediction network 300 can share their weights for all non-blank symbols via a simple transpose transformation. Since the two matrices share all of their values, the RNN-T decoder only needs to store the values once on memory instead of storing the two individual matrices. By embedding dimension d e Is set equal to the hidden layer dimension d h The RNN-T decoder reduces the size equal to the embedding dimension d e The number of parameters of the product of the vocabulary size |v|. The weight binding corresponds to a regularization technique.
FIG. 4 is a graph 400 depicting Word Error Rate (WER) versus the number of parameters of an RNN-T decoder. Here, fig. 4 is a graph 400 illustrating the number of parameters for a WER versus bound RNN-T decoder 410 (shown in solid lines), unbound RNN-T decoder 420 (shown in dotted lines), and Long Short Term Memory (LSTM) network 430 (shown in dashed lines). In particular, graph 400 depicts the sizes of predictive network 300 and federated network 230 with and without binding. Graph 400 shows a varying embedding dimension d e To perform a scan over the model size. As shown in fig. 4, the unbound RNN-T decoder 420 includes four metrics, including an embedding dimension d of 64, 320, 640, 960, and 1280 e . Here, the bound RNN-T decoder 410 includes three metrics, including the embedded dimensions of 640, 960, and 1280. In the case of the unbound RNN-T decoder 420, the last hidden layer of the federated network 230 always includes dimension d of size 640 h (dimension d is not shown in graph 400 h ). In the case of the bound RNN-T decoder 410, the graph 400 also shows the dimension d of the last hidden layer of the federated network 230 h (dimension d is not shown in graph 400 h ) Equal to the embedding dimension d of the prediction network 300 e Making the size and performance of the RNN-T decoder more sensitive to changes in this dimension. Thus, the results depicted by graph 400 indicate that the weight binding is more parameter efficient, thereby achieving better performance with fewer parameters. Additionally, for a sufficiently large model using weight binding, the same word error rate is achieved as for a conventional RNN-T decoder using LSTM network 430.
FIG. 5 is a flow chart of an example arrangement of operations of a computer-implemented method 500 for performing the binding and simplified RNN-T model 200. At each of a plurality of time steps subsequent to the initial time step, method 500 performs operations 502-512. At operation 502, the method 500 includes receiving a sequence y of non-blank symbols 301,301a-n output by the final Softmax layer 240 ui-n ,…,y ui-1 As input to the predictive network 300 of the recurrent neural network-transducer (RNN-T) model 200. For each non-blank symbol in the sequence of non-blank symbols received as input during the corresponding time step, method 500 performs operations 504-508. At operation 504, the method 500 includes generating, by the prediction network 300, an embedding 306 of a corresponding non-blank symbol using the shared embedding matrix 304. At operation 506, the method 500 includes mapping, by the predictive network 300, the corresponding location vector PV Aa–An 308aa-An to the corresponding non-blank symbol. At operation 508, the method 500 includes weighting the embeddings 306 by the prediction network 300 in proportion to the similarity between the embeddings 306 and the corresponding location vectors 308.
At operation 510, the method 500 includes generating a single embedded vector 350 as an output from the prediction network 300 at a corresponding time step. Here, the single embedding vector 350 is based on a weighted average 318A-H of the weighted embeddings 312 Aa-An. At operation 512, the method 500 includesThe probability distribution of possible speech recognition hypotheses at corresponding time steps is generated by the federated network 230 of the RNN-T model 200 using a single embedded vector 350 generated as output from the predictive network 300 at the corresponding time step
FIG. 6 is a schematic diagram of an example computing device 600 that may be used to implement the systems and methods described herein. Computing device 600 is intended to represent various forms of digital computers, such as notebook computers, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
Computing device 600 includes a processor 610, memory 620, storage 630, high-speed interface/controller 640 connected to memory 620 and high-speed expansion ports 650, and low-speed interface/controller 660 connected to low-speed bus 670 and storage 630. Each of the components 610, 620, 630, 640, 650, and 660 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 610 is capable of processing instructions for execution within the computing device 600, including instructions stored in the memory 620 or on the storage device 630, to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as a display 680 coupled to the high-speed interface 640. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. In addition, multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multiprocessor system).
Memory 620 non-transitory stores information within computing device 600. Memory 620 may be a computer-readable medium, a volatile memory unit, or a non-volatile memory unit. Non-transitory memory 620 may be a physical device for storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 600 on a temporary or permanent basis. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., commonly used for firmware such as a boot strap). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
The storage device 630 is capable of providing mass storage for the computing device 600. In some implementations, the storage device 630 is a computer-readable medium. In various different implementations, the storage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory, or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional embodiments, the computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that when executed perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 620, the storage device 630, or memory on processor 610.
The high-speed controller 640 manages bandwidth-intensive operations for the computing device 600, while the low-speed controller 660 manages lower bandwidth-intensive operations. Such assignment of tasks is merely exemplary. In some implementations, the high-speed controller 640 is coupled to the memory 620, the display 680 (e.g., via a graphics processor or accelerator), and the high-speed expansion port 650, which high-speed expansion port 650 may accept various expansion cards (not shown). In some implementations, a low-speed controller 660 is coupled to the storage device 630 and the low-speed expansion port 690. The low-speed expansion port 690, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, pointing device, scanner, or networking device, such as a switch or router, for example, through a network adapter.
Computing device 600 may be implemented in a number of different forms, as shown. For example, it may be implemented as a standard server 600a or multiple times in a group of such servers 600a, as a notebook computer 600b, or as part of a rack server system 600 c.
Various implementations of the systems and techniques described here can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments can include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to a storage system, at least one input device, and at least one output device to receive data and instructions therefrom, and to transmit data and instructions thereto.
A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform tasks. In some examples, a software application may be referred to as an "application," app, "or" program. Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. The terms "machine-readable medium" and "computer-readable medium" as used herein refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the present disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor or a touch screen for displaying information to the user and, optionally, a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices can also be used to provide interaction with the user, for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. Further, the computer is capable of interacting with the user by sending and receiving documents to and from a device used by the user, for example, by sending Web pages to a Web browser on the user's client device in response to requests received from the Web browser.
Many embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (26)
1. A recurrent neural network-transducer (RNN-T) model (200), comprising:
a predictive network (300) configured to, at each of a plurality of time steps subsequent to an initial time step:
receiving as input a sequence of non-blank symbols (301) output by a final Softmax layer (240);
for each non-blank symbol (301) in the sequence of non-blank symbols (301) received as input at a corresponding time step:
generating an embedding (306) of the corresponding non-blank symbol (301) using the shared embedding matrix (304);
assigning respective position vectors (308) to corresponding non-blank symbols (301); and
weighting the embedding (306) in proportion to a similarity between the embedding (306) and a corresponding position vector (308); and
generating as output a single embedding vector (350) at a corresponding time step, the single embedding vector (350) being based on a weighted average (318) of the weighted embeddings (312); and
a federated network (230) configured to, at each of the plurality of time steps subsequent to the initial time step:
-receiving as input the single embedded vector (350) generated as output from the prediction network (300) at corresponding time steps; and
Probability distributions of possible speech recognition hypotheses are generated at corresponding time steps.
2. The RNN-T model (200) of claim 1, further comprising:
an audio encoder (210), the audio encoder configured to:
receiving as input a sequence of acoustic frames (110); and
at each of the plurality of time steps, generating a higher-order feature representation of a corresponding acoustic frame (110) in the sequence of acoustic frames (110),
wherein the joint network (230) is further configured to receive as input, at each of the plurality of time steps, the high-order feature representation generated by the audio encoder (210) at the corresponding time step.
3. The RNN-T model (200) of claim 1 or 2, wherein weighting the embedding (306) in proportion to a similarity between the embedding (306) and the corresponding position vector (308) comprises: the embedding (306) is weighted in proportion to cosine similarity between the embedding (306) and the corresponding position vector (308).
4. The RNN-T model (200) of any of claims 1 to 3, wherein the sequence of non-blank symbols (301) output by the final Softmax layer (240) comprises word sheets.
5. The RNN-T model (200) of any of claims 1 to 4, wherein the sequence of non-blank symbols (301) output by the final Softmax layer (240) comprises graphemes.
6. The RNN-T model (200) of any of claims 1 to 5, wherein each of the embeddings comprises the same dimension size as each of the position vectors.
7. The RNN-T model (200) of any of claims 1 to 6, wherein the sequence of non-blank symbols received as input is limited to N previous non-blank symbols output by the final Softmax layer.
8. The RNN-T model (200) of claim 7, wherein N is equal to 2.
9. The RNN-T model (200) of claim 7, wherein N is equal to 5.
10. The RNN-T model (200) of any of claims 1 to 9, wherein the prediction network comprises a multi-headed attention mechanism that shares the shared embedding matrix across each head of the multi-headed attention mechanism.
11. The RNN-T model (200) of claim 10, wherein the predictive network (300) is configured to, at each of the plurality of time steps subsequent to the initial time step:
At each head (302) of the multi-head attention mechanism (302):
for each non-blank symbol (301) in the sequence of non-blank symbols (301) received as input at a corresponding time step:
generating an identical embedding (306) of the corresponding non-blank symbol (301) with the embedding (306) generated at each other head (302) of the multi-head attention mechanism (302) using the shared embedding matrix (304);
assigning to the corresponding non-blank symbol (301) a respective position vector (308) different from the respective position vector (308) assigned to the corresponding non-blank symbol (301) at each other head (302) of the multi-head attention mechanism (302); and
weighting the embedding (306) in proportion to a similarity between the embedding (306) and a corresponding position vector (308); and
generating a respective weighted average (318) of the weighted embedding (312) of the sequence (301) of non-blank symbols as an output from a corresponding head (302) of the multi-head attention mechanism (302); and
the single embedded vector (350) is generated as an output at a corresponding time step by averaging respective weighted averages (318) of outputs from corresponding heads (302) of the multi-head attention mechanism (302).
12. The RNN-T model (200) of claim 10 or 11, wherein the multi-headed attention mechanism (302) comprises four heads (302).
13. The RNN-T model (200) of any of claims 1 to 12, wherein the prediction network (300) binds dimensions of the shared embedding matrix (304) with dimensions of an output layer of the federated network (230).
14. A computer-implemented method (500), when executed on data processing hardware (12), causes the data processing hardware (12) to perform operations comprising:
at each of a plurality of time steps subsequent to the initial time step:
receiving as input to a prediction network (300) of a recurrent neural network transducer (RNN-T) model (200) a sequence of non-blank symbols (301) output by a final Softmax layer (240);
for each non-blank symbol (301) in the sequence of non-blank symbols (301) received as input at a corresponding time step:
generating, by the prediction network (300), an embedding (306) of the corresponding non-blank symbol (301) using a shared embedding matrix (304);
assigning, by the prediction network (300), respective position vectors (308) to corresponding non-blank symbols (301); and
Weighting the embedding (306) by the prediction network (300) in proportion to a similarity between the embedding (306) and a corresponding position vector (308);
generating a single embedding vector (350) as an output from the prediction network (300) at a corresponding time step, the single embedding vector (350) being based on a weighted average (318) of the weighted embeddings (312); and
a probability distribution of possible speech recognition hypotheses is generated at corresponding time steps by a federated network (230) of the RNN-T model (200) using the single embedded vector (350) generated as output from the predictive network (300) at the corresponding time step.
15. The computer-implemented method (500) of claim 14, wherein the operations further comprise:
receiving a sequence of acoustic frames (110) as input to an audio encoder (210) of the RNN-T model (200);
generating, by the audio encoder (210), a high-order feature representation of a corresponding acoustic frame (110) in the sequence of acoustic frames (110) at each of the plurality of time steps; and
the high order feature representations generated by the audio encoder (210) at corresponding time steps are received as input to the joint network (230).
16. The computer-implemented method (500) of claim 14 or 15, wherein weighting the embedding (306) in proportion to a similarity between the embedding (306) and the corresponding position vector (308) comprises: the embedding (306) is weighted in proportion to cosine similarity between the embedding (306) and the corresponding position vector (308).
17. The computer-implemented method (500) of any of claims 14 to 16, wherein the sequence of non-blank symbols (301) output by the final Softmax layer (240) comprises word sheets.
18. The computer-implemented method (500) of any of claims 14 to 17, wherein the sequence of non-blank symbols (301) output by the final Softmax layer (240) comprises graphemes.
19. The computer-implemented method (500) of any of claims 14 to 18, wherein each of the embeddings (306) includes a same dimension size as each of the location vectors (308).
20. The computer-implemented method (500) of any of claims 14 to 19, wherein the sequence of non-blank symbols (301) received as input is limited to N previous non-blank symbols (301) output by the final Softmax layer (240).
21. The computer-implemented method (500) of claim 20, wherein N is equal to 2.
22. The computer-implemented method (500) of claim 20, wherein N is equal to 5.
23. The computer-implemented method (500) of any of claims 14 to 22, wherein the predictive network (300) includes a multi-headed attention mechanism (302), the multi-headed attention mechanism (302) sharing the shared embedding matrix (304) across each head (302) of the multi-headed attention mechanism (302).
24. The computer-implemented method (500) of claim 23, wherein the operations further comprise, at each of the plurality of time steps subsequent to the initial time step:
at each head (302) of the multi-head attention mechanism (302):
for each non-blank symbol (301) in the sequence of non-blank symbols (301) received as input at a corresponding time step:
-generating, by the prediction network (300), an embedding (306) of the corresponding non-blank symbol (301) that is identical to the embedding (306) generated at each other head (302) of the multi-head attention mechanism (302) using the shared embedding matrix (304);
-assigning, by the prediction network (300), to the corresponding non-blank symbol (301), a respective position vector (308) different from the respective position vector (308) assigned to the corresponding non-blank symbol (301) at each other head (302) of the multi-head attention mechanism (302); and
weighting the embedding (306) by the prediction network (300) in proportion to a similarity between the embedding (306) and a corresponding position vector (308); and
generating, by the prediction network (300), a respective weighted average (318) of the weighted embeddings (312) of the non-blank symbol sequences (301) as output from a corresponding header (302) of the multi-header attentiveness mechanism (302); and
the single embedded vector (350) is generated as an output from the prediction network (300) at a corresponding time step by averaging respective weighted averages (318) of the corresponding head (302) outputs from the multi-head attention mechanism (302).
25. The computer-implemented method (500) of claim 23 or 24, wherein the multi-headed attention mechanism (302) comprises four heads (302).
26. The computer-implemented method (500) of any of claims 14 to 25, wherein the prediction network (300) binds dimensions of the shared embedding matrix (304) with dimensions of an output layer of the federated network (230).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163165030P | 2021-03-23 | 2021-03-23 | |
US63/165,030 | 2021-03-23 | ||
PCT/US2021/034121 WO2022203701A1 (en) | 2021-03-23 | 2021-05-26 | Recurrent neural network-transducer model for performing speech recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117043858A true CN117043858A (en) | 2023-11-10 |
Family
ID=76624155
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180096093.0A Pending CN117043858A (en) | 2021-03-23 | 2021-05-26 | Cyclic neural network-transducer model for performing speech recognition |
Country Status (6)
Country | Link |
---|---|
US (2) | US11727920B2 (en) |
EP (1) | EP4292080A1 (en) |
JP (1) | JP2024510816A (en) |
KR (1) | KR20230156427A (en) |
CN (1) | CN117043858A (en) |
WO (1) | WO2022203701A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2022203701A1 (en) * | 2021-03-23 | 2022-09-29 | Google Llc | Recurrent neural network-transducer model for performing speech recognition |
Family Cites Families (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10504010B2 (en) * | 2015-10-02 | 2019-12-10 | Baidu Usa Llc | Systems and methods for fast novel visual concept learning from sentence descriptions of images |
US10170110B2 (en) * | 2016-11-17 | 2019-01-01 | Robert Bosch Gmbh | System and method for ranking of hybrid speech recognition results with neural networks |
US10733380B2 (en) * | 2017-05-15 | 2020-08-04 | Thomson Reuters Enterprise Center Gmbh | Neural paraphrase generator |
US11423883B2 (en) * | 2019-05-06 | 2022-08-23 | Google Llc | Contextual biasing for speech recognition |
US11217231B2 (en) * | 2019-06-19 | 2022-01-04 | Google Llc | Contextual biasing for speech recognition using grapheme and phoneme data |
US11210474B2 (en) * | 2019-08-15 | 2021-12-28 | Advanced New Technologies Co., Ltd. | Language processing using a neural network |
US11367431B2 (en) * | 2020-03-13 | 2022-06-21 | Amazon Technologies, Inc. | Synthetic speech processing |
US11132988B1 (en) * | 2020-10-22 | 2021-09-28 | PolyAI Limited | Dialogue system, a dialogue method, and a method of training |
WO2022203701A1 (en) * | 2021-03-23 | 2022-09-29 | Google Llc | Recurrent neural network-transducer model for performing speech recognition |
US20220319506A1 (en) * | 2021-03-31 | 2022-10-06 | Chief Chief Technologies Oy | Method and system for performing domain adaptation of end-to-end automatic speech recognition model |
-
2021
- 2021-05-26 WO PCT/US2021/034121 patent/WO2022203701A1/en active Application Filing
- 2021-05-26 KR KR1020237035610A patent/KR20230156427A/en active Search and Examination
- 2021-05-26 US US17/330,446 patent/US11727920B2/en active Active
- 2021-05-26 JP JP2023558608A patent/JP2024510816A/en active Pending
- 2021-05-26 CN CN202180096093.0A patent/CN117043858A/en active Pending
- 2021-05-26 EP EP21735010.7A patent/EP4292080A1/en active Pending
-
2023
- 2023-07-06 US US18/347,842 patent/US20230352006A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP2024510816A (en) | 2024-03-11 |
US11727920B2 (en) | 2023-08-15 |
KR20230156427A (en) | 2023-11-14 |
WO2022203701A1 (en) | 2022-09-29 |
EP4292080A1 (en) | 2023-12-20 |
US20230352006A1 (en) | 2023-11-02 |
US20220310071A1 (en) | 2022-09-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11741947B2 (en) | Transformer transducer: one model unifying streaming and non-streaming speech recognition | |
US20220122622A1 (en) | Cascaded Encoders for Simplified Streaming and Non-Streaming ASR | |
WO2022182769A1 (en) | Learning word-level confidence for subword end-to-end automatic speech recognition | |
US20220310073A1 (en) | Mixture Model Attention for Flexible Streaming and Non-Streaming Automatic Speech Recognition | |
US11715458B2 (en) | Efficient streaming non-recurrent on-device end-to-end model | |
US20230352006A1 (en) | Tied and reduced rnn-t | |
US20230130634A1 (en) | Optimizing Inference Performance for Conformer | |
CN117083668A (en) | Reducing streaming ASR model delay using self-alignment | |
US20230107695A1 (en) | Fusion of Acoustic and Text Representations in RNN-T | |
US20230298570A1 (en) | Rare Word Recognition with LM-aware MWER Training | |
US20240135923A1 (en) | Universal Monolingual Output Layer for Multilingual Speech Recognition | |
US20230306958A1 (en) | Streaming End-to-end Multilingual Speech Recognition with Joint Language Identification | |
US20230107493A1 (en) | Predicting Word Boundaries for On-Device Batching of End-To-End Speech Recognition Models | |
US20230298569A1 (en) | 4-bit Conformer with Accurate Quantization Training for Speech Recognition | |
US20230326461A1 (en) | Unified Cascaded Encoder ASR model for Dynamic Model Sizes | |
US20230109407A1 (en) | Transducer-Based Streaming Deliberation for Cascaded Encoders | |
US20230107248A1 (en) | Deliberation of Streaming RNN-Transducer by Non-Autoregressive Decoding | |
US20230298565A1 (en) | Using Non-Parallel Voice Conversion for Speech Conversion Models | |
US20240029719A1 (en) | Unified End-To-End Speech Recognition And Endpointing Using A Switch Connection | |
US20220310061A1 (en) | Regularizing Word Segmentation | |
US20240029718A1 (en) | Flickering Reduction with Partial Hypothesis Re-ranking for Streaming ASR | |
US20240153495A1 (en) | Multi-Output Decoders for Multi-Task Learning of ASR and Auxiliary Tasks | |
WO2024081332A1 (en) | Universal monolingual output layer for multilingual speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |