CN115605840A - Automated assistant with audio presentation interaction - Google Patents
Automated assistant with audio presentation interaction Download PDFInfo
- Publication number
- CN115605840A CN115605840A CN202080100658.3A CN202080100658A CN115605840A CN 115605840 A CN115605840 A CN 115605840A CN 202080100658 A CN202080100658 A CN 202080100658A CN 115605840 A CN115605840 A CN 115605840A
- Authority
- CN
- China
- Prior art keywords
- audio presentation
- user query
- audio
- response
- presentation
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/65—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/165—Management of the audio stream, e.g. setting of volume, audio stream path
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
- G06F40/35—Discourse or dialogue representation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
Abstract
User interaction may be supported by an audio presentation of the automated assistant, and in particular, utilizing spoken content of such audio presentation presented at a particular point within the audio presentation. Analysis of the audio presentation may be performed to identify one or more entities presented by, mentioned by, or otherwise associated with the audio presentation, and utterance classification may be performed to determine whether an utterance received during playback of the audio presentation is directed to the audio presentation, and in some instances, to a particular entity and/or playback point in the audio presentation, thereby enabling generation of an appropriate response to the utterance.
Description
Background
Humans may engage in human conversations with interactive software applications, referred to herein as "automated assistants" (also referred to as "chat robots," "interactive personal assistants," "intelligent personal assistants," "personal voice assistants," "session agents," etc.). For example, a human being (which may be referred to as a "user" when they interact with the automated assistant) may provide commands and/or requests to the automated assistant using spoken natural language input (i.e., an utterance) -in some cases, the spoken natural language input may be converted to text and then processed-and/or by providing textual (e.g., typed) natural language input. Automated assistants typically respond to commands or requests by providing responsive user interface outputs, which may include audible and/or visual user interface outputs.
Automated assistants enable users to obtain information, access services, and/or perform various tasks. For example, a user can perform a search, obtain directions, and in some cases, interact with third party computing services. The user may also be able to perform various operations, such as calling a car from a ride share application, ordering goods or services (e.g., pizza), controlling smart devices (e.g., light switches), making reservations, and so forth.
Automated assistants may talk to users using speech recognition and natural language processing, some also utilize machine learning and other artificial intelligence techniques, for example, to predict a user's intent. Automated assistants may be adept at conducting conversations with users in a natural, intuitive manner, in part because they understand the context of the conversation. To utilize the conversation context, the automated assistant can save recent inputs from the user, questions from the user, and/or responses/questions provided by the automated assistant. For example, the user may ask "where is the nearest coffee shop? (Where is the close coffee shop. (Two blocks east.) "the user may then ask," how late it opened? (How is it open).
Many automated assistants are also used to playback audio content, such as music, podcasts, radio stations or streams, audio books, and the like. Automated assistants that run on mobile devices or stand-alone interactive speakers often include speakers, or are connectable to headphones, through which a user can listen to audio content. Conventionally, however, interaction with such audio content has been limited primarily to controlling playback, such as starting playback, pausing, ending playback, jumping forward or backward, muting, or changing playback volume, or querying an automated assistant for overall information about the audio content, such as obtaining a title of a song or information about the artist who recorded the song. Especially for audio presentations containing speech content, the scope of automated assistant interaction is very limited.
Disclosure of Invention
Techniques are described herein for enabling users to interact with audio presentations of automated assistants, and in particular to interact with spoken content of such audio presentations presented at particular points within the audio presentations. Analysis of the audio presentation may be performed to identify one or more entities presented by, mentioned by, or otherwise associated with the audio presentation, and utterance classification may be performed to determine whether an utterance received during playback of the audio presentation is directed to the audio presentation, and in some cases, to a particular entity and/or playback point in the audio presentation, thereby enabling generation of an appropriate response to the utterance.
Thus, consistent with one aspect of the invention, a method may include analyzing spoken audio content associated with an audio presentation to identify one or more entities set forth in the audio presentation, receiving a user query during playback of the audio presentation, and determining whether the user query is for the audio presentation, and if the user query is determined to be for the audio presentation, generating a response to the user query, wherein determining whether the user query is for the audio presentation or generating a response to the user query uses the identified one or more entities.
In some embodiments, analyzing the spoken audio content associated with the audio presentation includes performing speech recognition processing on the spoken audio content to generate transcribed text, and performing natural language processing on the transcribed text to identify the one or more entities. Further, in some embodiments, speech recognition processing, natural language processing, and receiving a user query are performed on the assistant device during playback of the audio presentation by the assistant device.
Further, in some embodiments, receiving the user query is performed on the assistant device during playback of the audio presentation by the assistant device, and at least one of performing speech recognition processing and performing natural language processing is performed prior to playback of the audio presentation. In some embodiments, at least one of performing speech recognition processing and performing natural language processing is performed by a remote service.
Further, some embodiments may also include determining one or more suggestions using the identified one or more entities based on a particular point in the audio presentation. Some embodiments may further include presenting the one or more suggestions on the assistant device during playback of a particular point in the audio presentation by the assistant device. Further, some embodiments may also include preprocessing responses to one or more potential user queries using the identified one or more entities prior to receiving the user query. Further, in some embodiments, generating a response to the user query comprises generating a response to the user query using a preprocessed response of the one or more preprocessed responses.
In some embodiments, determining whether the user query is for an audio presentation includes providing transcribed text from the audio presentation and the user query to a neural network-based classifier trained to output an indication of whether a given user query is likely for a given audio presentation. Some embodiments may also include buffering audio data from the audio presentation prior to receiving the user query, and analyzing the spoken audio content associated with the audio presentation includes analyzing the spoken audio content from the buffered audio data to identify one or more entities presented in the buffered audio data after receiving the user query, and determining whether the user query uses the identified one or more entities presented in the buffered audio data for the audio presentation or generating a response to the user query. Further, in some embodiments, the audio presentation is a podcast.
In some embodiments, determining whether the user query is for an audio presentation includes using the identified one or more entities to determine whether the user query is for an audio presentation. Further, in some embodiments, generating a response to the user query includes generating a response to the user query using the identified one or more entities. In some embodiments, determining whether the user query is for an audio presentation includes determining whether the user query is for a particular point in the audio presentation. Further, in some embodiments, determining whether the user query is for the audio presentation includes determining whether the user query is for a particular entity in the audio presentation.
Further, in some embodiments, receiving the user query is performed on the assistant device, and determining whether the user query is for audio presentation comprises determining whether the user query is for audio presentation, rather than a general query for the assistant device. In some embodiments, receiving the user query is performed on the assistant device, and determining whether the user query is for an audio presentation comprises determining that the user query is for an audio presentation, and not a general query for the assistant device.
Further, in some embodiments, determining whether the user query is for an audio presentation further comprises determining that the user query is for an assistant device rather than a non-query utterance. Further, some embodiments may also include determining whether to pause audio presentation in response to receiving the user query.
Further, in some embodiments, determining whether to pause the audio presentation comprises determining whether the query can be responded to with a visual response, and the method further comprises, in response to determining that the query can be responded to with a visual response, visually presenting the generated response without pausing the audio presentation, and in response to determining that the query cannot be responded to with a visual response, pausing the audio presentation and presenting the generated response while the audio presentation is paused. Further, in some embodiments, determining whether to pause the audio presentation comprises determining whether the audio presentation is playing on a stoppable device, and the method further comprises presenting the generated response without pausing the audio presentation in response to determining that the audio presentation is not playing on the stoppable device, and pausing the audio presentation and presenting the generated response while the audio presentation is paused in response to determining that the audio presentation is playing on the stoppable device.
Consistent with another aspect of the invention, a method may include, during playback of an audio presentation including spoken audio content, receiving a user query and determining whether the user query is for the audio presentation and generating a response to the user query if the user query is determined to be for the audio presentation, wherein determining whether the user query is for the audio presentation or generating the response to the user query uses one or more entities identified from an analysis of the audio presentation.
Consistent with another aspect of the invention, a method may include, during playback of an audio presentation including spoken audio content, buffering audio data from the audio presentation and receiving a user query, after receiving the user query, analyzing the spoken audio content from the buffered audio data to identify one or more entities presented in the buffered audio data and determine whether the user query is for the audio presentation, and if the user query is determined to be for the audio presentation, generating a response to the user query, wherein determining whether the user query uses the identified one or more entities for the audio presentation or generating the response to the user query.
Further, some embodiments may include a system comprising one or more processors and memory operably coupled to the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to perform any one of the aforementioned methods. Some embodiments may also include an automated assistant device comprising an audio input device (e.g., a microphone, a wired-in input, a network or storage interface that receives digital audio data, etc.) and the one or more processors coupled to the audio input device and executing locally stored instructions to cause the one or more processors to perform any of the foregoing methods. Some embodiments also include at least one non-transitory computer-readable medium containing instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform any one of the aforementioned methods.
It should be understood that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
FIG. 1 is a block diagram of an example computing environment in which embodiments disclosed herein may be implemented.
Fig. 2 is a block diagram of an example embodiment of an example machine learning stack in which embodiments disclosed herein may be implemented.
FIG. 3 is a flow diagram illustrating an example sequence of operations for capturing and analyzing audio content from an audio presentation, in accordance with various embodiments.
FIG. 4 is a flow diagram illustrating an example sequence of operations for capturing and analyzing audio content from an audio presentation using a remote service, in accordance with various embodiments.
FIG. 5 is a flow diagram illustrating an example sequence of operations for capturing and analyzing audio content from an audio presentation using audio buffering, in accordance with various embodiments.
FIG. 6 is a flow diagram illustrating an example sequence of operations for presenting suggestions associated with audio presentation, according to various embodiments.
FIG. 7 is a flow diagram illustrating an example sequence of operations for processing an utterance and generating a response to the utterance, in accordance with various embodiments.
Fig. 8 illustrates an example architecture of a computing device.
Detailed Description
Turning now to fig. 1, an example environment 100 is shown in which the techniques disclosed herein may be implemented. The example environment 100 includes an assistant device 102 interfacing with one or more remote and/or cloud-based automated assistant components 104, which one or more remote and/or cloud-based automated assistant components 104 can be implemented on one or more computing systems (collectively, "cloud" computing systems) communicatively coupled to the assistant device 102 via one or more local and/or wide area networks (e.g., the internet), indicated generally at 106. The assistant device 102 and computing devices operating the remote or cloud-based automated assistant component 104 may include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. The operations performed by the assistant device 102 and/or automated assistant component 104 can be distributed across multiple computer systems, e.g., as computer programs running on one or more computers in one or more locations coupled to each other over a network. In various implementations, for example, some or all of the functionality of an automated assistant may be distributed among multiple computer systems, or even to client computing devices. In some implementations, for example, the functionality discussed herein may be performed entirely within the client computing device, e.g., such that such functionality is available to the user even when there is no online connection. As such, in some embodiments, the assistant device may comprise a client device, while in other embodiments, the assistant device may comprise one or more computer systems remote from the client device, or even a combination of the client device and one or more remote computer systems, whereby the assistant device is a distributed combination of devices. Thus, in various embodiments, an assistant device may be considered to include any electronic device that implements any of the functionality of an automated assistant.
The assistant device 102 in the illustrated embodiment is generally a computing device on which an instance of the automated assistant client 108, through its interaction with one or more remote and/or cloud-based automated assistant components 104, can form what appears from the user's perspective to be a logical instance of the automated assistant with which the user can participate in a human-machine conversation. For the sake of brevity and simplicity, the term "automated assistant" used herein as a "service" specific user will refer to a combination of an automated assistant client 108 executing on an assistant device 102 operated by the user and one or more remote and/or cloud-based automated assistant components 104 (which may be shared among multiple automated assistant clients in some embodiments).
The assistant device 102 may also include instances of various applications 110, which applications 110 may interact with or otherwise be supported by the automated assistant in some implementations. Among the various applications 110 that may be supported include, for example, audio applications, such as podcast applications, audio book applications, audio streaming applications, and the like. Further, from a hardware perspective, the assistant device 102 may include, for example, one or more of the following: a desktop computing device, a notebook computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart appliance such as a smart television, and/or a wearable apparatus of a user that includes a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). In other embodiments additional and/or alternative computing devices may be used, and it will be understood that the assistant device in various embodiments may utilize assistant functionality as its sole functionality, while in other embodiments the assistant functionality may be a feature of a computing device that performs a multitude of other functions.
As described in more detail herein, the automated assistant engages the assistant device 102 in a human-machine conversation session with one or more users via the user interface input and output devices. Further, various additional components reside in the assistant device 102 in connection with supporting such sessions, and in particular, supporting user interaction with audio presentations on the assistant device.
For example, audio playback module 112 may be used to control playback of various audio presentations, such as one or more audio presentations residing in audio presentation store 114 or one or more audio presentations streamed from a remote service. For example, audio playback may be presented to the user using one or more speakers of assistant device 102, or alternatively, using one or more speakers in communication with assistant device 102, e.g., in a headset, earset, car stereo, home stereo, television, etc. In addition to or instead of audio playback, the audio recording module 116 may be used to capture at least a portion of an audio presentation played back by another device in the same environment as the assistant device, such as a radio playing near the assistant device.
In this regard, an audio presentation may be considered to be any presentation of audio content, and in many cases, a presentation of audio content in which at least a portion of the audio content is spoken audio content containing human speech. Although in some embodiments the audio content in the audio presentation may include music and/or singing, in many embodiments discussed below, the focus is on audio presentations that include non-vocal spoken audio content with which the user may wish to interact, such as podcasts, audio books, radio programs, talk shows, news programs, sports programs, educational programs, and the like. In some implementations, the audio presentation may be directed to fictional and/or non-fictional subject matter, and in some implementations, visual or graphical content in addition to audio content, although in many implementations, the audio presentation may be limited to only audio content.
In the embodiments discussed below, spoken audio content associated with an audio presentation may be analyzed to identify one or more entities presented in the audio presentation, and such analysis may be used to perform various operations, such as generating suggestions associated with the spoken audio content for display or presentation to a user, and/or in response to user queries presented by the user during playback of the audio presentation. In some implementations, for example, a user query can be received during audio presentation playback, and it can be determined whether the user query is for audio presentation, such that if the user query is determined to be for audio presentation, an appropriate response to the user query can be generated and presented to the user. As will become more apparent below, entities identified by the analysis may be used, for example, when attempting to determine whether a user query is directed to an audio presentation and/or when generating a response to a user query.
To support such functionality, the assistant device 102 may include various additional modules or components 118 through 132. The speech recognition module 118 may be used, for example, to generate or transcribe text (and/or other suitable representations or embeddings) from audio data, while the natural language processing module 120 may be used to generate one or more entities. The module 118 may, for example, receive an audio recording of the speech input (e.g., in the form of digital audio data) and convert the digital audio data into one or more text words or phrases (also referred to herein as tokens). In some implementations, the speech recognition module 118 is also a streaming module, such that speech input is converted to text in real-time or near real-time on a token-by-token basis, such that tokens can be output from the module 118 effectively simultaneously with the user's speech, and thus before the user issues a complete verbal request. The speech recognition module 118 may rely on one or more acoustic and/or language models that together model the relationship between the audio signal and phonetic units in the language and word sequences in the language. In some embodiments, a single model may be used, while in other embodiments, multiple models may be supported, e.g., to support multiple languages, multiple speakers, etc.
The speech recognition module 118 converts speech into text, and the natural language processing module 120 attempts to discern the semantics or meaning of the text output by the module. For example, the natural language processing module 120 may rely on one or more grammar models to map action text to particular computer-based actions and identify entity text and/or other text that constrains the performance of those actions. In some embodiments, a single model may be used, while in other embodiments, multiple models may be supported, for example, to support different computer-based actions or computer-based action domains (i.e., a collection of related actions such as communication-related actions, search-related actions, audio/video-related actions, calendar-related actions, device control-related actions, etc.). As an example, the grammar model (stored on the assistant device 102 and/or the remote computing device) may map computer-based actions to action terms of the voice-based action query, such as the action term "tell me more about … … (tell)", "directions to … … (directions to)", "navigate to (navigation to)", "watch (watch)", "call (call)", "email (email)", "contact (contact)", and so on.
Further, while modules 118 and 120 may be used in some implementations to process speech input or queries from a user, in the implementation shown, modules 118 and 120 are also used to process spoken audio content from an audio presentation. In particular, audio presentation analysis module 122 can analyze the audio presentation, in part by utilizing modules 118 and 120, to generate various entities associated with the audio presentation. Alternatively, speech recognition and natural language processing may be performed using separate functionality from modules 118, 120, e.g., embedded within module 122. In this regard, an entity may refer to virtually any logical or semantic concept incorporated into spoken audio content, including, for example, but not limited to, a subject, person, place, thing, event, perspective, fact, organization, date, time, address, URL, email address, measurement, etc., associated with the spoken audio content in an audio presentation. Either of the modules 118, 120 may also use additional content metadata (e.g., podcast titles, descriptions, etc.) to assist in identifying and/or disambiguating entities. The entity may also be logically associated with a particular point in the audio presentation, for example, the entity of the topic "Battle of Verdun" includes an associated timestamp indicating that the topic is referenced in a podcast for the first world war at 13. By associating entities with particular points in an audio presentation, entities may be useful for responding to more ambiguous user queries such as "what year this occurred (what is one)" or "tell me more information about this (all me about this)" because in many cases, knowing what entities are being presented when a user issues a query at a particular point during playback may assist in resolving ambiguous aspects of the query.
In the illustrated embodiment, the audio presentation analysis module 122 may be used to provide feedback to the user in at least two ways, although the invention is not limited in this respect. In some implementations, it may be desirable, for example, for an automated assistant to provide suggestions to a user during playback of an audio presentation, for example, by displaying interesting facts or suggestions of queries that the user may want to issue at different points of the audio presentation. As such, the module 122 may include a suggestion generator 124, which suggestion generator 124 can generate suggestions based on entities identified in the audio presentation (e.g., "Tap here to learn more about < celebrities > (Tap here to left more out < personal > interactive in progress) interview in the podcast" or "Tap here to view offers from < service being advertised > (Tap to check-out the of from < service meeting >"). In some implementations, it may also be desirable for the automated assistant to respond to a particular query issued by a user, and as such, the module 122 may also include a response generator 126 for generating a response to the particular query. As will become more apparent below, either of the generators 124, 126 may be used to generate suggestions and/or responses on demand (i.e., during playback and/or in response to a particular query), and in some implementations, either of the generators may be used to generate pre-processed suggestions and/or responses prior to playback of the audio presentation to reduce processing overhead during playback and/or query processing.
To support module 122, entity and action store 128 may store entities identified in the audio presentation and any actions (e.g., suggestions, responses, etc.) that may be triggered in response to user input associated with any stored entities. Although the invention is not limited in this regard, in some embodiments, the action is similar to a verb and the entity is similar to a noun or pronoun, such that the query may identify or otherwise be associated with the action to be performed and the entity or entities that are the focus of the action. Thus, when executed, a user query may cause the performance of a computer-based action in view of one or more entities involved in the query (directly or indirectly via the surrounding environment) (e.g., "which year this occurred" may map to a Web search on the start date of the vandal campaign when issuing the query during the vandal campaign discussion).
It should be understood that in some implementations, the storage 114, 128 may reside locally in the assistant device 102. However, in other embodiments, storage 114, 128 may reside partially or completely in one or more remote devices.
As described above, the suggestion generator 124 of the audio presentation analysis module 122 may generate suggestions for presentation to the user of the assistant device 102. In some implementations, the assistant device 102 can include a display, and as such, it may be desirable to include a visual rendering module 130 to render a visual representation of the suggestion on the integrated display. Further, where visual responses to queries are supported, the module 130 may also be adapted to generate textual and/or graphical responses to queries.
Yet another module used by automated assistant 108 is a speech classifier module 132 that is used to classify any speech utterances detected by automated assistant 108. Module 132 is generally used to detect speech-based queries from utterances spoken within the environment in which the assistant device 102 is located, as well as to attempt to determine an intent (if any) associated with the utterances. As will become more apparent below, in the context of the present disclosure, module 132 may be used to determine, for example, whether an utterance is a query, whether an utterance is directed to an automated assistant, whether an utterance is directed to an audio presentation, or even whether an utterance is directed to a particular entity and/or point in an audio presentation.
It will be understood that in other embodiments, some or all of the functionality of any of the foregoing modules and components shown as residing in the assistant device 102 may be implemented in a remote automation assistant component. Thus, the present invention is not limited to the particular allocation of functions shown in FIG. 1.
While the functionality described herein may be implemented in a number of different ways in different embodiments, FIG. 2 next illustrates an example embodiment that utilizes an end-to-end audio understanding stack 150 that includes four stages 152, 154, 156, 158 adapted to support audio presentation interactions with an automated assistant.
In this embodiment, the first speech recognition stage 152 generates or transcribes text from the audio presentation 160, which is then processed by the second natural language processing stage 154 to annotate the text with appropriate entities or metadata. The third stage 156 includes two distinct components, a suggestion generation component 162 that generates suggestions from annotated text, and a query processing component 164 that detects and determines intent of a query issued by a user (e.g., a query provided as an utterance 166). Each component 162, 164 may also utilize contextual information 168, e.g., previous user queries or preferences, dialog information, etc., which may further be used to determine the intent of the user and/or generate useful and informative suggestions for a particular user. The fourth feedback generation stage 158 may incorporate a ranking system that takes the cumulative options from the components 162, 164 and presents the most likely action to be performed to the user as user feedback 170 based on the passively listened-to input or context of the action. In some implementations, stages 152, 154 may be implemented using a neural network similar to that used in an assistant stack to process speech recognition pipelines and text annotations used to process user utterances, and in many cases may run locally on the assistant device, or alternatively at least partially on one or more remote devices. Similarly, in some embodiments, stages 156, 158 may be implemented as an extension of the assistant stack, or alternatively using a custom machine learning stack separate from the assistant stack, and implemented locally on the assistant device, or partially or wholly on one or more remote devices.
In some implementations, the suggestion generation component 162 can be configured to generate suggestions that match content that the user has or is currently listening to, and can perform actions such as performing searches or integrating with other applications, for example by surfacing deep links related to entities (or other application functions). Further, the query processing component 164 may determine the intent of a query or utterance made by the user. Further, as will become more apparent below, the query processing component 164 may also be capable of interrupting or pausing playback of an audio presentation in response to a particular query, and may include a machine learning model that can classify whether the query is related to an auxiliary command that is not generally related to the audio presentation, and in some instances, whether the query is related to a particular point in the audio presentation or to a particular entity referenced in the audio presentation. In some implementations, the machine learning model can be a multi-layered neural network classifier trained to output an indication of whether a given user query is likely to be for a given audio presentation, and to use both, e.g., transcribed audio content, and the user query as input embedding layers, and to return one or more available actions if the user query is determined to be relevant to the audio content.
In some implementations, the feedback generation stage can combine the outputs from both components 162, 164 and rank what is presented and not presented to the user. For example, it may be the case that a user query is deemed relevant to an audio presentation, but no action is returned, but one or more suggestions may still be desired to be presented to the user.
Turning now to fig. 3-5, as described above, in some embodiments, spoken audio content from an audio presentation may be analyzed to identify various entities referenced by or otherwise associated with the audio presentation in some implementations. For example, fig. 3 illustrates one example sequence of operations 200 that may be performed on an audio presentation, such as may be performed by the audio presentation analysis module 122 of fig. 1. In some implementations, the sequence 200 can be performed in a real-time manner, e.g., during audio presentation playback. However, in other embodiments, the sequence 200 may be performed prior to audio presentation playback, e.g., as part of a pre-processing operation performed on the audio presentation. For example, in some implementations, it may be desirable to pre-process multiple audio presentations in a batch process and store entities, timestamps, metadata, pre-processed suggestions, and/or pre-processed responses for later retrieval during audio presentation playback. Doing so may reduce the processing overhead associated with supporting a user interacting with the audio presentation via the assistant device during playback. In this regard, it may be desirable to perform such batch processing by a remote or cloud-based service rather than by a single user device.
Thus, as shown at block 202, to analyze the audio presentation, the audio presentation may be presented, retrieved, or captured. In this regard, being presented generally refers to playing back an audio presentation on an assistant device, which may include a local stream or a stream from a remote device or service, and it will be understood that spoken audio content from the audio presentation is generally obtained as a result of presenting the audio presentation. In this regard, retrieved generally refers to retrieval of the audio presentation from storage. In some instances, the retrieval may be combined with the playback, while in other cases, the retrieval may be separate from any playback of the audio presentation, for example, when the audio presentation is pre-processed as part of a batch process. In this regard, capture generally refers to obtaining audio content from an audio presentation while the audio presentation is being presented by a device other than an assistant device. In some implementations, for example, the assistant device can include a microphone that can be used to capture audio played back by another device in the same environment as the assistant device, e.g., audio that might be played by a radio, television, home or business audio system, or another user's device.
Regardless of the source of the spoken audio content in the audio presentation, automated speech recognition may be performed on the audio content to generate transcribed text for the audio presentation (block 204), and the transcribed text may then be used to perform natural language processing to identify one or more entities that are proposed or otherwise associated with the audio presentation (block 206). Further, in some embodiments, one or more pre-processed responses and/or suggestions may be generated and stored based on the identified entities (block 208) in a manner similar to the manner in which responses and suggestions are generated as described elsewhere herein. Further, as indicated by the arrow from block 208 to block 202, in some embodiments, the sequence 200 may be performed incrementally on the audio presentation, whereby playback and analysis occur effectively in parallel during audio presentation playback.
As described above, various functions associated with the analysis of the audio presentation may be distributed among multiple computing devices. For example, fig. 4 shows an operational sequence 210 for performing real-time streaming and analysis of an audio presentation in dependence on an assistant device (left column) communicating with a remote or cloud-based service (right column). In particular, in block 212, the assistant device may present the audio presentation to the user, e.g., by playing back audio from the audio presentation to the user of the assistant device. During such playback, the audio presentation may also be streamed to the remote service (block 214), which performs automated speech recognition and natural language processing (blocks 216 and 218) in much the same manner as blocks 204 and 206 of fig. 3. Further, in some implementations, one or more pre-processed responses and/or suggestions may be generated and stored based on the identified entities (block 220), and the entities (and optionally the responses and/or suggestions) may be returned to the assistant device (block 222), in a manner similar to that of block 208 of fig. 3. The assistant device then stores the received information (block 224), and presentation of the audio presentation continues until the presentation ends or is prematurely paused or stopped.
Turning now to fig. 5, in some implementations, it may be desirable to defer or delay analysis of the audio presentation until required by a user query, an example of which is shown in sequence 230. Thus, in some implementations, rather than continuously performing speech recognition and natural language processing for the entire audio presentation during playback, it may be desirable to only buffer or otherwise store audio data from the audio presentation until an appropriate user query is received, and then analyze a portion of the audio presentation (e.g., the last N seconds) near the particular point in the audio presentation at which the user query was received. Thus, in one representative example, if the user utters a message such as "what year did this happen? (what is year di is happy), "The last approximately 30 seconds of The audio presentation may be analyzed to determine that The current subject in question is The Verwent Battle (The Battle of Verdun), and a search may be performed to generate a representative response, such as" The Verwent Battle occurred in 1916 (The Battle of Verdun secured in 1916) ".
Accordingly, as indicated at block 232, the audio presentation may be presented or captured by the assistant device (if presented by another device), with the last N seconds of the audio presentation being buffered (block 234). In some cases, buffering can include storing the captured audio data, while in other cases, e.g., where the presentation is initiated by the assistant device and the audio presentation is currently stored on the assistant device, buffering can include simply maintaining a reference to the extent of the audio presentation prior to the current playback point, thereby enabling the appropriate extent to be retrieved from storage when analysis is needed.
Playback continues in this manner until a user query is received (block 236), and once the user query is received, automated speech recognition and natural language processing are performed on the buffered audio or range of audio presentations in a manner similar to that discussed above in connection with fig. 3-4 (blocks 238 and 240). The user query is then processed (block 242) in a manner similar to that described below in connection with fig. 7, but primarily based on analysis of the buffered portion of the audio presentation.
Turning now to fig. 6, in some implementations it may be desirable to actively display suggestions on the assistant device during presentation of an audio presentation, an example of which is illustrated by operational sequence 250. In particular, during presentation or capture of an audio presentation (block 252), it may be determined whether any stored suggestions are associated with a current playback point in the audio presentation. If not, playback of the audio presentation continues and control returns to block 252. However, if any of the suggestions are relevant to the current playback point, control passes to block 256 to temporarily display one or more of the suggestions on the assistant device, e.g., using a card, notification, or tab displayed on the assistant device. In some implementations, the suggestions can also be associated with actions, such that user interaction with the suggestions can trigger a particular action. Thus, for example, if a particular celebrity is being discussed in a Podcast, a suitable suggestion may state "Tap here to learn more about < celebrity > (Tap to left more out < personalitiy > interviewed in Podcast) that was interviewed in the Podcast, whereby selection of the suggestion may open a browser tab with additional information about the celebrity.
In different embodiments, the suggestions may be generated in a number of ways. As described above, for example, the suggestions may be generated during analysis of the audio presentation, whether in real-time during playback of the audio presentation, or generated in advance, for example as a result of pre-processing of the audio presentation. In this way, the stored suggestions may have been stored only recently while the audio presentation is being analyzed. Further, in some implementations, suggestions can be associated with a particular entity, such that whenever a particular entity is referenced at a particular point during an audio presentation and identified as a result of an analysis of the audio presentation, any suggestions associated with that particular entity can be retrieved, optionally ranked, and displayed to a user. The suggestions may also or alternatively be associated with particular points in the audio presentation, such that retrieval of the suggestions may be based on the current playback point of the audio presentation, rather than what entity in the audio presentation is currently being presented.
Fig. 7 next illustrates an example sequence of operations 260 suitable for processing utterances or user queries, such as may be performed at least in part by the utterance classifier module 132 of fig. 1, and in some implementations, using a neural network-based classifier trained to output an indication of whether a given user query is likely to be for a given audio presentation. The sequence begins with receiving an utterance during presentation of an audio presentation, such as an utterance captured by a microphone of an assistant device (block 262). The utterance may then be processed (block 264) to determine an utterance intent. In some instances, the utterance intent may be based at least in part on one or more identified entities from the audio presentation currently being presented and/or the current playback point in the audio presentation. In some implementations, the classification of the dialogs may be multi-tiered, and an attempt may be made to determine one or more of the following: whether the utterance is directed to an automated assistant (1) (e.g., not specifically to anyone, as opposed to a non-query utterance to another person in the environment), or to background noise), (2) whether the utterance is directed to an audio presentation (e.g., as opposed to a general query to an automated assistant), (3) whether the utterance is directed to a particular point in the audio presentation, or (4) whether the utterance is directed to a particular entity associated with the audio presentation. The response of the assistant device to the utterance may thus vary based on the classification. Furthermore, in some implementations, acoustic echo cancellation may be utilized such that the audio presentation itself is not processed as an utterance.
In one example embodiment, the classification of the utterance is based on whether the utterance is directed to an assistant (block 266); if so, whether the utterance is more specifically directed to an audio presentation that is currently being presented (block 268); and if so, whether the utterance is directed to a particular point in the audio presentation (block 270) or to a particular entity in the audio presentation (block 272), even more specifically.
If the utterance is determined not to be directed to an assistant, the utterance may be ignored, or alternatively, a response may be generated such as "I did not understand, please repeat once again" (block 274). If the utterance is determined to be for an assistant, but not specifically for an audio presentation (e.g., "how tomorrow is tomorrow"), a response may be generated in a conventional manner (block 276). Similarly, if the utterance is determined to be for an audio presentation, rather than for any particular point or entity in the audio presentation (e.g., "pause podcast" or "what is this podcast (what.
However, if the utterance classification determines that the utterance is directed to a particular point and/or entity in the audio presentation, control may pass to block 278 to optionally determine whether any pre-processed responses are available (e.g., as discussed above in connection with fig. 3-4). If response preprocessing is not used, block 278 may be omitted. If no preprocessed response is available, a response may be generated (block 280). In preparing the response, one or more identified entities in the audio presentation and/or the current playback point may optionally be used to determine an appropriate response.
Next, if a pre-processed response is available, or a new response has been generated, the response may be presented to the user. In the illustrated embodiment, the response may be presented in a number of different ways, depending on the type of response and the context in which the response has been generated. Specifically, in the illustrated embodiment, a determination is made as to whether the response is a visual response (block 282), meaning that the response can be presented to the user by visual means (e.g., via a display of the assistant device). If so, the response may be presented without pausing playback of the audio presentation (block 284). An example of such a response may be a notification, card, or tab displayed on The assistant device in response to querying "which year this occurred (What year)" during The discussion of The valve campaign stating "The valve campaign occurred in 1916 (The bag of product ok workplace in 1916)". However, in other embodiments, any visual response may not be supported (e.g., for non-display assistant devices), and thus block 282 may be omitted.
If the response is not a visual response, a determination may be made as to whether playback is stoppable (block 286). For example, if the playback is on a radio or on a device other than an assistant device and/or is thus not controllable, or if the audio presentation is a live stream that is expected not to be paused, the playback may not be stoppable. In this case, control may pass to block 284 to present the response without pausing playback. However, if playback is stoppable, control may pass to block 288 to temporarily pause playback and present the response, and generally continue playback of the audio presentation when presentation of the response is complete.
FIG. 8 is a block diagram of an example computing device 300 suitable for implementing all or part of the functionality described herein. Computing device 300 typically includes at least one processor 302, which communicates with a number of peripheral devices via a bus subsystem 304. These peripheral devices may include a storage subsystem 306 including, for example, a memory subsystem 308 and a file storage subsystem 310, a user interface input device 312, a user interface output device 314, and a network interface subsystem 316. The input and output devices allow a user to interact with the computing device 300. Network interface subsystem 316 provides an interface to external networks and is coupled to corresponding interface devices in other computing devices.
The user interface input devices 312 may include a keyboard, a pointing device such as a mouse, trackball, touchpad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways to input information to computing device 300 or over a communication network.
User interface output devices 314 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or some other mechanism for creating a visual image. The display subsystem may also provide non-visual displays, such as via audio output devices. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 300 to a user or to another machine or computing device.
The storage subsystem 306 stores programming and data structures that provide the functionality of some or all of the modules described herein. For example, the storage subsystem 306 may include logic to perform selected aspects of the various sequences shown in fig. 5, 7, and/or 10.
These software modules are typically executed by the processor 302 alone or in combination with other processors. The memory 308 used in the storage subsystem 306 may include a number of memories, including a main Random Access Memory (RAM) 318 for storage of instructions and data during program execution and a Read Only Memory (ROM) 420 that stores fixed instructions. File storage subsystem 310 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 310 in storage subsystem 306, or in other machines accessible to processor 302.
Where the systems described herein collect or may utilize personal information about a user, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social actions or activities, profession, the user's preferences, or the user's current geographic location), or whether and/or how to receive content from a content server that may be more relevant to the user. In addition, certain data may be processed in one or more ways to remove personally identifiable information before it is stored or used. For example, the identity of the user may be processed such that no personal identity information can be determined for the user, or the user's geographic location may be summarized (such as to a city, zip code, or state level) if geographic location information is obtained, such that a particular geographic location of the user cannot be determined. Thus, the user may have control over how information about the user is collected and/or used.
Although several embodiments have been described and illustrated herein, various other means and/or structures for performing the function and/or obtaining the result and/or one or more of the advantages described herein may be utilized and each of such variations and/or modifications is considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary, and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (27)
1. A computer-implemented method, comprising:
analyzing spoken audio content associated with an audio presentation to identify one or more entities presented in the audio presentation;
receiving a user query during playback of the audio presentation; and
determining whether the user query is for the audio presentation, and if the user query is determined to be for the audio presentation, generating a response to the user query, wherein determining whether the user query is for the audio presentation or generating a response to the user query uses the identified one or more entities.
2. The method of claim 1, wherein analyzing the spoken audio content associated with the audio presentation comprises:
performing speech recognition processing on the spoken audio content to generate transcribed text; and
natural language processing is performed on the transcribed text to identify the one or more entities.
3. The method of claim 2, wherein performing the speech recognition processing, performing the natural language processing, and receiving the user query are performed on an assistant device during playback of the audio presentation by the assistant device.
4. The method of claim 2 or claim 3, wherein receiving the user query is performed on the assistant device during playback of the audio presentation by the assistant device, and wherein at least one of performing the speech recognition processing and performing the natural language processing is performed prior to playback of the audio presentation.
5. The method of any of claims 2 to 4, wherein at least one of performing the speech recognition processing and performing the natural language processing is performed by a remote service.
6. The method of any of the preceding claims, further comprising determining one or more suggestions using the identified one or more entities based on a particular point in the audio presentation.
7. The method of claim 6, further comprising presenting the one or more suggestions on an assistant device during playback of the particular point in the audio presentation by the assistant device.
8. The method of any of the preceding claims, further comprising pre-processing responses to one or more potential user queries using the identified one or more entities prior to receiving the user query.
9. The method of claim 8, wherein generating the response to the user query comprises generating the response to the user query using one of one or more pre-processed responses.
10. The method of any of the preceding claims, wherein determining whether the user query is for the audio presentation comprises providing transcribed text from the audio presentation and the user query to a neural network-based classifier trained to output an indication of whether a given user query is likely for a given audio presentation.
11. The method of any of the preceding claims, further comprising buffering audio data from the audio presentation prior to receiving the user query, wherein analyzing the spoken audio content associated with the audio presentation comprises analyzing the spoken audio content from the buffered audio data after receiving the user query to identify one or more entities presented in the buffered audio data, and wherein determining whether the user query is for the audio presentation or generating the response to the user query uses the identified one or more entities presented in the buffered audio data.
12. The method of any of the preceding claims, wherein the audio presentation is a podcast.
13. The method of any of the preceding claims, wherein determining whether the user query is for the audio presentation comprises using the identified one or more entities to determine whether the user query is for the audio presentation.
14. The method of any of the preceding claims, wherein generating the response to the user query comprises generating the response to the user query using the identified one or more entities.
15. The method of any of the preceding claims, wherein determining whether the user query is for the audio presentation comprises determining whether the user query is for a particular point in the audio presentation.
16. The method of any of the preceding claims, wherein determining whether the user query is for the audio presentation comprises determining whether the user query is for a particular entity in the audio presentation.
17. The method of any of the preceding claims, wherein receiving the user query is performed on an assistant device, and wherein determining whether the user query is for the audio presentation comprises determining whether the user query is for the audio presentation, and not a general query for the assistant device.
18. The method of any of the preceding claims, wherein receiving the user query is performed on an assistant device, and wherein determining whether the user query is for the audio presentation comprises determining that the user query is for the audio presentation and not a general query for the assistant device.
19. The method of claim 18, wherein determining whether the user query is for the audio presentation further comprises determining that the user query is for the assistant device and not a non-query utterance.
20. The method of any of the preceding claims, further comprising determining whether to pause the audio presentation in response to receiving the user query.
21. The method of claim 20, wherein determining whether to pause the audio presentation comprises determining whether the query can be responded to with a visual response, the method further comprising:
in response to determining that the query can be responded to with a visual response, visually presenting the generated response without pausing the audio presentation;
and in response to determining that the query cannot be responded to with a visual response, pausing the audio presentation and presenting the generated response while the audio presentation is paused.
22. The method of claim 20, wherein determining whether to pause the audio presentation comprises determining whether the audio presentation is playing on a pause-able device, the method further comprising:
in response to determining that the audio presentation is not playing on a paused device, presenting the generated response without pausing the audio presentation; and
in response to determining that the audio presentation is playing on a pausing enabled device, pausing the audio presentation and presenting the generated response while the audio presentation is paused.
23. A computer-implemented method, comprising:
receiving a user query during playback of an audio presentation comprising spoken audio content; and
determining whether the user query is for the audio presentation, and if the user query is determined to be for the audio presentation, generating a response to the user query, wherein determining whether the user query is for the audio presentation or generating a response to the user query uses one or more entities identified from the analysis of the audio presentation.
24. A computer-implemented method, comprising:
buffering audio data from an audio presentation comprising spoken audio content and receiving a user query during playback of the audio presentation;
after receiving the user query, analyzing spoken audio content from the buffered audio data to identify one or more entities set forth in the buffered audio data; and
determining whether the user query is for the audio presentation, and if the user query is determined to be for the audio presentation, generating a response to the user query, wherein determining whether the user query is for the audio presentation or generating the response to the user query uses the identified one or more entities.
25. A system comprising one or more processors and memory operably coupled to the one or more processors, wherein the memory stores instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to perform the method of any of claims 1-24.
26. An assistant device, comprising:
an audio input device; and
one or more processors coupled to the audio input device and executing locally stored instructions to cause the one or more processors to perform the method of any of claims 1-24.
27. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform the method of any one of claims 1-24.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/947,030 US20220020365A1 (en) | 2020-07-15 | 2020-07-15 | Automated assistant with audio presentation interaction |
US16/947,030 | 2020-07-15 | ||
PCT/US2020/064929 WO2022015356A1 (en) | 2020-07-15 | 2020-12-14 | Automated assistant with audio presentation interaction |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115605840A true CN115605840A (en) | 2023-01-13 |
Family
ID=74181328
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080100658.3A Pending CN115605840A (en) | 2020-07-15 | 2020-12-14 | Automated assistant with audio presentation interaction |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220020365A1 (en) |
EP (1) | EP4127897A1 (en) |
JP (1) | JP7481488B2 (en) |
KR (1) | KR20230025708A (en) |
CN (1) | CN115605840A (en) |
WO (1) | WO2022015356A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11776534B1 (en) * | 2020-12-08 | 2023-10-03 | Amazon Technologies, Inc. | Natural language understanding intent adjustment |
Family Cites Families (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP4612903B2 (en) * | 2006-10-24 | 2011-01-12 | キヤノン株式会社 | Video playback apparatus and control method thereof |
US8433577B2 (en) * | 2011-09-27 | 2013-04-30 | Google Inc. | Detection of creative works on broadcast media |
JP2013250490A (en) | 2012-06-01 | 2013-12-12 | Ricoh Co Ltd | Processing apparatus, processing system, and voice recognition method and program |
US10659851B2 (en) * | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US10204104B2 (en) * | 2015-04-14 | 2019-02-12 | Google Llc | Methods, systems, and media for processing queries relating to presented media content |
US10606950B2 (en) * | 2016-03-16 | 2020-03-31 | Sony Mobile Communications, Inc. | Controlling playback of speech-containing audio data |
US10373612B2 (en) * | 2016-03-21 | 2019-08-06 | Amazon Technologies, Inc. | Anchored speech detection and speech recognition |
US10271093B1 (en) * | 2016-06-27 | 2019-04-23 | Amazon Technologies, Inc. | Systems and methods for routing content to an associated output device |
US10192551B2 (en) * | 2016-08-30 | 2019-01-29 | Google Llc | Using textual input and user state information to generate reply content to present in response to the textual input |
US10205794B2 (en) * | 2016-09-08 | 2019-02-12 | International Business Machines Corporation | Enhancing digital media with supplemental contextually relevant content |
US10679608B2 (en) * | 2016-12-30 | 2020-06-09 | Google Llc | Conversation-aware proactive notifications for a voice interface device |
US10839795B2 (en) * | 2017-02-15 | 2020-11-17 | Amazon Technologies, Inc. | Implicit target selection for multiple audio playback devices in an environment |
KR102529262B1 (en) * | 2017-03-20 | 2023-05-08 | 삼성전자주식회사 | Electronic device and controlling method thereof |
JP7026449B2 (en) | 2017-04-21 | 2022-02-28 | ソニーグループ株式会社 | Information processing device, receiving device, and information processing method |
US10564928B2 (en) | 2017-06-02 | 2020-02-18 | Rovi Guides, Inc. | Systems and methods for generating a volume- based response for multiple voice-operated user devices |
GB2579554A (en) * | 2018-12-03 | 2020-07-01 | Audiogum Uk Ltd | Content playback system |
-
2020
- 2020-07-15 US US16/947,030 patent/US20220020365A1/en active Pending
- 2020-12-14 CN CN202080100658.3A patent/CN115605840A/en active Pending
- 2020-12-14 JP JP2022564435A patent/JP7481488B2/en active Active
- 2020-12-14 KR KR1020237001956A patent/KR20230025708A/en unknown
- 2020-12-14 WO PCT/US2020/064929 patent/WO2022015356A1/en unknown
- 2020-12-14 EP EP20839471.8A patent/EP4127897A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4127897A1 (en) | 2023-02-08 |
WO2022015356A1 (en) | 2022-01-20 |
JP2023525232A (en) | 2023-06-15 |
KR20230025708A (en) | 2023-02-22 |
JP7481488B2 (en) | 2024-05-10 |
US20220020365A1 (en) | 2022-01-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7443407B2 (en) | Automated assistant with conferencing capabilities | |
JP7419485B2 (en) | Proactively incorporating unsolicited content into human-to-computer dialogs | |
JP6710740B2 (en) | Providing suggested voice-based action queries | |
US11727220B2 (en) | Transitioning between prior dialog contexts with automated assistants | |
US11238854B2 (en) | Facilitating creation and playback of user-recorded audio | |
KR20190005194A (en) | Automated growth of message exchange threads based on message classification | |
US20180012595A1 (en) | Simple affirmative response operating system | |
US11789695B2 (en) | Automatic adjustment of muted response setting | |
JP7481488B2 (en) | Automated Assistants Using Audio Presentation Dialogue | |
EP4049270B1 (en) | Using video clips as dictionary usage examples | |
US11756533B2 (en) | Hot-word free pre-emption of automated assistant response presentation | |
CN115858601A (en) | Conducting collaborative search sessions through automated assistant |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |