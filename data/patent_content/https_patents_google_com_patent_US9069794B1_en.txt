US9069794B1 - Determining location information for images using landmark, caption, and metadata location data - Google Patents
Determining location information for images using landmark, caption, and metadata location data Download PDFInfo
- Publication number
- US9069794B1 US9069794B1 US14/052,157 US201314052157A US9069794B1 US 9069794 B1 US9069794 B1 US 9069794B1 US 201314052157 A US201314052157 A US 201314052157A US 9069794 B1 US9069794 B1 US 9069794B1
- Authority
- US
- United States
- Prior art keywords
- location
- image
- landmark
- caption
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/587—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5838—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using colour
-
- G06F17/30256—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G06K9/46—
Definitions
- This specification relates to determining location information for images.
- the Internet has enabled access to a wide variety of resources, e.g., video and/or audio files, web pages for particular subjects, news articles, images, etc.
- the Internet also provides Internet users with access to a variety of systems for interacting with, creating, and managing resources, such as cloud storage systems for storing data and social network sites for uploading and/or publishing images, videos, and messages.
- Various resource management systems allow Internet users access to a variety of resource management services, such as image management systems that facilitate organization, display, searching, and sharing of image files.
- This specification describes technologies relating to determining location information for images.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of obtaining landmark location data for an image, the landmark location data specifying one or more landmark locations derived from content depicted in the image and, for each landmark location, a landmark confidence score that indicates a measure of confidence that the image depicts content associated with the landmark location; obtaining caption location data for the image, the caption location data specifying one or more caption locations derived from user input and, for each caption location, a caption confidence score that indicates a measure of confidence that the image depicts content associated with the caption location; obtaining metadata location data for the image, the metadata location data specifying a metadata location derived from data provided by an image capturing device that captured the image; identifying location pairs from the landmark location data, caption location data, and metadata location data, each location pair including two of a landmark location, caption location, and metadata location; generating, for each location pair, a geographic consistency score that indicates a measure of consistency between locations specified in the location pair; selecting a location pair based at least in part on
- the method may further comprise ranking each location pair according to a pre-determined hierarchy of location types, and wherein selecting a location pair based at least in part on the geographic consistency scores further comprises selecting a location pair based at least in part on the location pair rankings.
- the method may further comprise: identifying a confidence score for a location specified in the selected location pair; identifying a location type for the location; and generating a topicality score for the image based on the identified confidence score and the identified location type.
- Identifying a confidence score for a location specified in the selected location pair may comprise identifying a confidence score for a location specified in the selected location pair based on the pre-determined hierarchy of location types.
- Selecting an image location for the image from the locations specified in the selected location pair may comprise selecting an image location for the image from the locations specified in the selected location pair based on the pre-determined hierarchy of location types.
- Generating a topicality score for the image may comprise: inputting the identified confidence score into a function that corresponds to the identified location type, wherein the function is one of three different functions that each correspond to a different location type; and obtaining the topicality score for the image as output from the function.
- the landmark data may be obtained from a landmark processor that i) identifies landmark images that are visually similar to the image and ii) provides, as landmark locations for the image, landmark locations that are associated with the identified landmark images;
- the caption location data may be obtained from a caption processor that derives one or more caption locations from user input that includes one or more of an image caption, an album title, and an image tag;
- the metadata location data may be obtained from a metadata processor that derives a metadata location from EXIF data for the image.
- Determining the location of an image may be useful for organizing images according to location.
- images with associated locations may be more likely to be identified during an image search, and image location confidence scores may be used to affect image search result rankings, providing users with images that are likely to satisfy the user's informational need.
- a topicality score for an image may be used to organize images for a particular location based on a topicality of the image, which may result in displaying images in a topically relevant order for a given location.
- topicality scores may be used to identify images that are relevant to a location based query and to affect image search result rankings, providing users with image search results that are likely to satisfy the user's informational need. Topicality scores may also be used to prune search results, providing only the most relevant and/or topical image search results to users, which can decrease resource and bandwidth usage in a networked search environment.
- FIG. 1 is a block diagram of an example environment in which location information for images is determined.
- FIG. 2 is a block diagram of an example process for determining image location information for in image.
- FIG. 3 is a data flow of an example process for determining image location information for an image.
- FIG. 4 is a flow diagram of an example process in which location information is determined for an image.
- FIG. 5 is a block diagram of an example data processing apparatus.
- An image management system identifies locations and generates topicality scores for images.
- Various information sources of location information for an image are analyzed to identify a location to associate with the image, and the location information may also be used to generate a topicality score for an image that indicates how representative the image is of the identified location.
- Location data for images, including topicality scores, may be stored in an index and used for information retrieval.
- an image management system obtains three different types of location data for an image—landmark location data, caption location data, and metadata.
- Landmark location data specifies landmark locations and corresponding confidence scores.
- a landmark processor may compare the image to other images that depict landmarks or other iconic locations, and provide the image management system with locations that are associated with those other images.
- Caption location data specifies caption locations and corresponding confidence scores.
- a caption processor may analyze user-supplied image captions, album names, and image tags to identify locations referenced by the data and provide the image management system with the locations and corresponding confidence scores.
- Metadata location data specifies a geographic location for an image. For example, Exif data for an image may indicate geographic coordinates for the image.
- the image management system pairs locations of each location type to the locations of each other location type. For example, each landmark location is paired with each caption location and metadata location, and each caption location is paired with each metadata location.
- the image management system generates geographic consistency scores for the location pairs. The geographic consistency score indicates how consistent one location of the pair is with the other location of the pair. For example, if a landmark location specifies the geographic area associated with the Empire State Building (in New York City, N.Y., United States), and it is paired with a caption location that specifies New York City, the locations would be consistent, though not identical. However, if the same landmark location were paired with a caption location that specified Paris, France, the locations would be inconsistent.
- the geographic consistency score could be binary (e.g., either consistent or inconsistent) or could have a range of consistency, e.g., a 0.0 to 1.0 scale, where 0.0 is completely inconsistent and 1.0 is completely consistent/identical).
- One location pair is selected based at least in part on the geographic consistency scores.
- a location pair is also selected based on a pre-determined hierarchy of location types, where some location types are ranked higher than others. For example, the image management system may select the location pair with the highest geographic consistency score among location pairs that include, according to the pre-determined hierarchy, the highest ranked location types.
- an image location is selected for the image.
- the location of the highest ranked location type specified in the selected location pair may be the location selected by the image management system for the image.
- the selected location pair may include a landmark location and a caption location, and the landmark location may be ranked higher than the caption location based on the pre-determined hierarchy, in which case the landmark location would be selected as the location for the image.
- the image management system also determines an image location score for the selected image location. For example, if a landmark location is selected as the location for the image, the image location score may be the previously obtained confidence score for the selected landmark location.
- the image management system may optionally generate a topicality score for the image in addition to selecting a location and determining a location score for the image.
- the topicality score indicates, for example, how representative the image is of the selected location. Thus, if the image location is determined to be the Empire State Building, the topicality score indicates how well the image represents the Empire State Building.
- the image management system may associate that information with the image in an index.
- FIG. 1 is a block diagram of an example environment 100 in which location information for images is determined.
- a computer network 102 such as a local area network (LAN), wide area network (WAN), the Internet, or a combination thereof, connects publisher resources 104 , user devices 106 , and an image management system 108 .
- the online environment 100 may include many thousands of publisher resources 104 and user devices 106 .
- a resource 104 is data that can be provided by a publisher over the network 102 and that is associated with a resource address.
- Resources include HTML pages, word processing documents, and portable document format (PDF) documents, images, video, and feed sources, to name just a few.
- PDF portable document format
- the resources can include content, such as words, phrases, pictures, and so on, and may include embedded information (such as meta information and hyperlinks) and/or embedded instructions (such as scripts).
- a user device 106 is an electronic device capable of requesting and receiving resources 104 over the network 102 .
- Example user devices 106 include personal computers, mobile communication devices, and other devices that can send and receive data over the network 102 .
- a user device 106 typically includes a user application, such as a web browser, to facilitate the sending and receiving of data over the network 102 .
- the web browser can enable a user to display and interact with text, images, videos, music and other information typically located on a web page at a website.
- User devices 106 may communicate with the image management system 108 to search for and manage image resources.
- the image management system 108 may receive image resources from user devices 106 and facilitate organization of the image resources.
- the image management system 108 may also provide user devices 106 with a front end user interface through which the user devices 106 can manage image resources.
- the image management system 108 may provide image management services for a social networking website, where users can, for example, upload images, create captions for images, create photo albums, tag images with relevant people or topics, and organize images. Images uploaded by user devices 106 to the image management system may be stored in a data storage device, such as the index 110 , along with image data, such as the user provided captions, tags, album titles, and image metadata.
- Access to images stored in the index 110 may be private, public, or accessible to subset of users. For example, a user may upload images that are only accessible to that user, while another user may upload images that are viewable by a subset of the user's social networking affiliates, and yet another user may upload images that are viewable to any network or social network user. Users may query the image management system to identify images that were either previously uploaded by the user, by the user's social network affiliates, and/or by users of the network.
- the image management system 108 may also identify image resources by crawling publisher web sites and indexing the image resources provided by the publisher web sites. The indexed and, optionally, cached copies of the resources, may also be stored in the index 110 . In some implementations, the image management system 108 may have multiple indices, such as one index for images provided to the image management system 108 directly by users, and a second index for images obtained from publisher websites.
- the image management system 108 may also determine image location data for images, which can be stored in the index 110 and used for labeling images and/or for retrieving images in response to user queries. As described in further detail below, the image management system 108 may obtain location information for images from various sources to determine locations relevant to an image. The determined locations may be used, for example, to organize images and/or to provide images relevant to particular locations in response to a user query.
- FIG. 2 is a block diagram of an example process 200 for determining image location information for in image.
- a user device 202 provides an image 204 and caption data 206 to the image management system 108 .
- further processing of the image 204 and caption data 206 to determine image location information may be performed in response to receipt of the image 204 and caption data 206 , or at a later time, e.g., in a batch process performed at a particular time or in response to an affirmative request to determine location information.
- the user device 202 may be a smart phone with a camera
- the image 204 may be an image of the Empire State Building in New York City, which was captured by the smart phone's camera.
- the caption data 206 may include a user-provided caption, e.g., “The Empire State Building,” and the image may be designated as part of a user image album entitled, “Summer trip to Weehawken, N.J.”
- the image management system 108 provides the image 204 to a landmark processor 210 , which uses landmark data 212 to identify landmark location data 214 for the image.
- the landmark location data 214 specifies one or more locations that are derived from the content depicted in the image 204 and, for each location, a confidence score that indicates a measure of confidence that the image 204 depicts content associated with the location.
- the landmark processor 210 may compare the image 204 to images with known locations to identify similar images, and identify the locations of those similar images as a landmark location for the image 204 .
- the landmark data 212 may include an index of images with known locations associated with them.
- the location e.g., latitude/longitude or zip code
- several images of the Washington Monument, also from various angles and perspectives, during different seasons, and during the day and at night may be stored in the landmark data 212 and associated with the location of the Washington Monument.
- the landmark processor 210 uses various image feature detection and comparison techniques to identify images that are visually similar to the provided image 204 . If, for example, the image 204 is a relatively clear image of the Empire State Building, the landmark processor 210 may determine that the image 204 is similar to one or more images of the Empire State Building, and identify the location of the Empire State Building as a landmark location for the image 204 .
- the landmark location confidence score may depend on, for example, how close the image 204 matches one or more images with a known location for the Empire State Building, or how strong an association is between the similar images and the location of the Empire State Building.
- the image 204 of the Empire State Building may also be visually similar to one or more images of the New York-New York Hotel and Casino in Las Vegas, Nev., which has a large replica of the Empire State Building.
- a confidence score for the location of the hotel may be lower than that of the location for the Empire State Building in New York City if, for example, the image 204 does not match the images of the hotel as well as it matches the images of the real Empire State Building.
- the landmark data 212 may include other types of images with associated locations.
- street level images such as an image of a storefront taken from the street, may be stored in the landmark data 212 and associated with the address of the store.
- images of art such as an image of the Mona Lisa painting, which may be associated with a location of the artwork, e.g., The Louvre, in Paris, France.
- any location with a relatively large number of images associated with that location may be stored in the landmark data 212 , such as a department store or restaurant in which many images have been captured.
- the landmark processor 210 After identifying landmark locations for the image 204 and corresponding landmark confidence scores, the landmark processor 210 provides the image management system 108 with the landmark location data 214 .
- the image management system 108 provides the caption data 206 to a caption processor 220 , which uses caption data 222 to identify caption location data 224 for the image 204 .
- the caption location data 224 specifies one or more locations derived from user input, such as an image caption and album title, and a corresponding confidence score for each location that indicates a measure of confidence that the image depicts content associated with the location.
- the caption processor 220 parses user supplied text and interprets textual references to particular locations. For example, the caption processor 220 may compare the text of an image album, caption, or tag to an index of locations and/or location nicknames.
- the caption processor 220 may derive one or more image locations from the caption, “The Empire State Building,” and the image album title, “Summer trip to Weehawken, N.J.”
- the caption may be associated with multiple locations, such as New York City, Midtown Manhattan, and the Empire State Building.
- the confidence score for each location may reflect, for example, how well the caption matches the location, how precise the geographic location is, or how important a caption is relative to an album title or image tag.
- the caption processor 220 may also account for correct spelling errors, ambiguities, partial names, and nicknames. For example, a caption of “The Big Apple” can be identified as matching New York City, and the caption “New York” can be associated with both the state of New York and New York City.
- a single caption location is selected.
- the single caption location may be, for example, the highest scoring caption location.
- the caption “The Empire State Building,” may match three different locations, but the location of the Empire State Building is the best semantic match and the most precise geographic match, so its confidence score may be the highest of three matching locations. Accordingly, other caption locations for that image caption may be discarded.
- the image album title “Summer trip to Weehawken, N.J.,” may match the location of Weehawken, N.J. and the state of New Jersey. As the city location is a more precise match, it may be the caption location selected for the image album title.
- the caption processor 220 After identifying caption locations for the image 204 and corresponding caption confidence scores, the caption processor 220 provides the image management system 108 with the caption location data 224 .
- the image management system 108 provides the image 204 to a metadata processor 230 , which uses metadata data 232 to identify metadata location data 234 for the image 204 .
- the metadata location data 234 specifies a location derived from data that is provided for the image 204 by an image capturing device.
- a camera used to capture an image may associate metadata with the image, such as a date/time the image was captured, the type of camera, the size of the image, the location of the camera at the time the image was captured, and other information.
- the smart phone used to capture the image 204 may have included Exchangeable image file format (“Exif”) data with the image that specifies the latitude and longitude of the smart phone when the image was captured.
- the image 204 may include Exif data that specifies the location at 40°46′08′′ N, 74°00′56′′W, which the metadata processor 230 may identify as a location in Weehawken, N.J.
- a confidence score may be associated with a metadata location.
- a confidence score may simply be binary, e.g., a 1 or 0, indicating that the location data either exists, or does not.
- a confidence score for a metadata location may have a range of confidence that depends on the source of the location data, if known. For example, a location derived from cell phone tower triangulation may be less precise then a location derived from Wifi hotspot location, which may in turn be less precise than a location provided by a global positioning system (GPS).
- GPS global positioning system
- the metadata processor 230 After identifying a metadata location for the image 204 , the metadata processor 230 provides the image management system 108 with the metadata location data 234 .
- the image management system uses the landmark location data 214 , the caption location data 224 , and the metadata location data 234 to select an image location 240 for the image, determine an image location score 242 for the image 204 , and, in some implementations, generate a topicality score 244 for the image 244 .
- the image location 240 , image location score 242 , and optional topicality score 244 are associated with the image 204 and stored in the index 110 . Selecting an image location 240 , determining an image location score 242 , and generating a topicality score 244 are described in further detail with respect to FIG. 3 .
- landmark processor 210 While the landmark processor 210 , caption processor 220 , and metadata processor 230 are depicted separately in the example process 200 , they may be part of the same system, and may also be included in the image management system 108 . Similarly, the landmark location data 212 , caption location data 222 , and metadata location data 232 may be separate data storage devices, separate partitions of the same data storage device, or included in a single data storage device, such as the index 110 .
- FIG. 3 is a data flow of an example process 300 for determining image location information for an image.
- the process 300 may be performed, for example, by an image management system.
- the image management system obtains landmark data 302 , caption data 304 , and metadata data 306 for an image.
- the example landmark location data 302 includes two landmark locations: LL1, which is the location of the Empire State Building in New York City, and LL2, which is the location of the New York-New York Hotel and Casino (the “Hotel”).
- the example caption location data 304 includes two caption locations: CL1, which is the location of Weehawken, N.J., and CL2, which is the location of the Empire State Building in New York City.
- the example metadata location data 306 includes one metadata location: ML, which is at 40°46′08′′ N, 74°00′56′′W—coordinates in Weehawken, N.J.
- Each landmark location and caption location has a corresponding confidence score.
- the example confidence scores are represented on a scale from 0.0 to 1.0, with 0.0 being no confidence, and 1.0 being a maximum confidence.
- LCS1 is the landmark confidence score for LL1 and, at 0.9, LCS1 indicates a relatively high confidence that the image depicts something that matches an image associated with the location of the Empire State Building.
- LCS2 is the landmark confidence score for LL2 and, at 0.4, LCS2 indicates a moderate level of confidence that the image depicts something that matches an image associated with the location of the Hotel.
- CCS1 is the caption confidence score for CL1 and, at 1.0, CCS1 indicates a maximum confidence that the image is associated with Weehawken, N.J.
- CCS2 is the caption confidence score for CL2 and, at 1.0, it CCS2 also indicates a maximum confidence that the image is associated with the location of the Empire State Building.
- the confidence score scale of 0.0 to 1.0 is an example, and other value ranges, including binary values, can be used for a confidence score scale.
- Each example location in the example process 300 is shown with a corresponding geographic hierarchy.
- the geographic hierarchy begins with the actual location identified and expands to successively less geographically precise locations, e.g., the Empire State Building has a location (possibly associated with a zip code or geographic coordinates defining an physical area), which is within Midtown Manhattan, which is within New York County, which is within New York City, which is within New York State, which is in the United States.
- This geographic hierarchy may be identified by the image management system or by separate location processors.
- Location pairs 308 are identified from the landmark location data 302 , the caption location data 304 , and the metadata location data 306 .
- Each location pair includes a location from two different sources. For example, each landmark location is paired with each caption location and the metadata location, and each caption location is paired with the metadata location.
- Geographic consistency scores 310 are generated for each location pair. Each geographic consistency score indicates a measure of consistency between the locations specified by the corresponding location pair. As with the example confidence scores, example geographic consistency scores are represented on a scale from 0.0 to 1.0, with 0.0 being no confidence, and 1.0 being a maximum confidence. For example, the location pair LL1, CL1 has a geographic consistency score of 0.1 which indicates a relatively low confidence that the locations are consistent. While New York State and New Jersey are geographically adjacent to one another, neither geographic location overlaps with the other, and in some implementations they are not likely to be considered highly consistent. LL1 and CL2, on the other hand, are exact matches, and the geographic consistency score of 1.0 indicates that there is a high confidence that the two locations are consistent.
- CL1 (Weehawken, N.J.) and ML (Lat. Long. in Weehawken, N.J.) are not exact location matches, because ML is more precise than CL1, but they are geographically consistent because both one location includes the other at a relatively precise level. If, for example, a caption location indicated the location of Jersey City, N.J., this may be relatively consistent with Weehawken, N.J., because both cities are geographically located within Hudson County in New Jersey. However, because they do not overlap, or include one another, a geographic consistency score between Weehawken, N.J. and Jersey City, N.J., may be lower than the geographic consistency score between CL1 and ML and, in some implementations, may also be lower than a geographic consistency score between Weehawken, N.J. and Hudson County.
- geographic consistency scores may weigh precision, or granularity, more than geographic inclusiveness. For example, two geographic locations within different boroughs of New York City may be more geographically consistent that two geographic locations that both indicate New York State. In some implementations, geographic consistency scores may weigh geographic inclusiveness more than precision, or granularity. For example, Albany, N.Y. may be more geographically consistent with New York State than two geographic locations within different boroughs of New York City.
- the geographic consistency score for a location pair may be based, in part, on the confidence scores that correspond to the locations of the pair. For example, a location pair that includes two locations with high confidence scores may be more geographically consistent than a location pair that includes two similar locations with low confidence scores.
- Example measurements include a geographic distance between points centered within a location, a geographic distance between location edges, and geographic area within one location of a pair relative to the position of the geographic area of the other location of the pair.
- location pairs are ranked 312 according to a pre-determined hierarchy of location types.
- a location hierarchy may indicate that landmark locations have a higher priority than caption locations, which in turn have a higher priority than metadata locations.
- the location pairs are first ranked according to the geographic consistency scores, and then according to the hierarchy. For example, location pairs that include a landmark location may be ranked higher than any other location pair, and within the location pairs that include a landmark location, the location pairs are ranked according to geographic consistency scores.
- location pairs that include a landmark location and caption location are ranked higher than location pairs that include a landmark location and a metadata location, which are in turn ranked higher than location pairs that include a caption location and metadata location.
- the location pairs are further ranked according to geographic consistency scores.
- the location pair, LL1, CL2 is in the top ranked hierarchy, and has the highest geographic consistency score, so it is ranked higher than the other location pairs.
- CL1, ML has a geographic consistency score of 1.0, it is ranked lower than every other location pair with a higher priority according to the hierarchy of location types.
- a hierarchy of locations may specify that metadata locations are the highest priority, and that other location types are treated equally.
- the hierarchy of location types may be determined empirically, based on user feedback regarding which location type or location pair produces the most favorable location results.
- the hierarchy of location types may also change if, for example, one type of location becomes more relevant in determining a location than another type.
- the location pair rankings may be based, at least in part, on the location confidence scores. For example, location confidence scores for locations of a pair can be used as a multiplier for geographic consistency scores. As another example, the location confidence score or the highest priority location of a pair may be used to rank location pairs. In some implementations, location pairs including a location with a corresponding confidence score that is less than a pre-determined confidence score threshold may be discarded or demoted in location pair rankings.
- the location pair may be selected based at least in part on the location pair rankings and/or geographic consistency scores.
- the example process 300 includes the selected location pair 314 of LL1, CL2 because it is the location pair with the highest geographic consistency score among the top tier of location pair types.
- An image location and image location score are selected 316 based on the selected location pair 314 .
- the image location may be the location of the selected location pair 314 that has the highest priority in the pre-determined hierarchy.
- the landmark location has a higher priority than the caption location, so the location LL1 is selected.
- the image location may indicate, for example, a location depicted in the corresponding image.
- the image location may be the location of the selected location pair 314 that has the highest location confidence score. For example, the confidence score of CL2 is higher than the confidence score of LL1, so location CL2 is selected.
- the image location score is the location confidence score of the image location.
- the image location score for LL1 is 0.9, so if LL1 is selected as the image location, the image location score is also 0.9.
- the image location score may be based on the confidence scores of both locations of the selected location pair 314 and/or the geographic consistency scores.
- the image location score may be the geographic consistency score, which in the example process is 1.0.
- the image location and image location score 316 for an image can be associated with the image in an index and used for image search and organization purposes.
- the image location score may indicate, for example, a confidence that the image depicts content associated with the image location.
- a topicality score 318 is generated for an image.
- the topicality score indicates a measure of confidence that the image is representative of its image location. For example, if the image location is identified as the location of the Empire State Building, the topicality score may specifically quantify how representative the image is of the Empire State Building. As another example, the topicality score may indicate how relevant the image is to the Empire State Building, or how iconic the image is with respect to the Empire State Building.
- the topicality score for an image may be based on the source of the image location and the image location score.
- the image location is a landmark location.
- the example image location score (e.g., 0.9) may be used as input into a topicality score function specific to landmark locations.
- topicality score functions may exist for each type of location pair. For example, a topicality function for a landmark location—caption location pair may be different from a topicality function for a landmark location—metadata location pair. The topicality function used may depend on the locations of the selected location pair 314 .
- Each topicality score function may be generated based on empirical feedback regarding results of the various functions. For example, user feedback may indicate that landmark locations, which are identified based on visual similarity to other images that are associated with a location, are a strong indicator of how representative an image is of the landmark location.
- metadata locations which are identified based on the location of the user device at the time the image was captured, may not provide a strong indication of what the image actually depicts. For example, an image that is visually similar to other images of the Empire State Building may be more likely to be representative of the Empire State Building than an image having metadata indicating its geographic coordinates are in the same location as the Empire State Building.
- a picture taken in the Empire State Building could be a picture of the interior of the building, or a picture taken from the observation deck of the building, which may be less likely to be representative or iconic of the Empire State Building.
- topicality scores for images may be associated with the images in an index.
- Image locations, image location scores, and topicality scores may be used, for example, for image organization and image retrieval. For example, when a user issues the query, “The Empire State Building,” to an image management system, the image management system can use image locations, image location scores, and topicality scores in an index to provide images likely to be of interest to the user. As another example, the image management system may automatically organize a user's image collection according to location and, within each location, according to topicality scores. Thus, a user's image collection may be presented to the user in a manner that places the most iconic images, e.g, the images with the highest topicality scores, first.
- FIG. 4 is a flow diagram of an example process 400 in which location information is determined for an image.
- the process 400 may be implemented by one or more data processing apparatus, such as an image management system.
- Landmark location data is obtained for an image ( 402 ).
- the landmark location data specifies one or more landmark locations derived from content depicted in the image and, for each landmark location, a landmark confidence score that indicates a measure of confidence that the image depicts content associated with the landmark location.
- landmark location data for an image of the Mona Lisa may indicate The Louvre, in Paris, France.
- the landmark data is obtained from a landmark processor that i) identifies landmark images that are visually similar to the image and ii) provides, as landmark locations for the image, landmark locations that are associated with the identified landmark images.
- Caption location data is obtained for the image ( 404 ).
- the caption location data specifies one or more caption locations derived from user input and, for each caption location, a caption confidence score that indicates a measure of confidence that the image depicts content associated with the caption location. For example, the image of the Mona Lisa may be in an image album that a user has entitled, “Vacation in France,” from which the caption location, France, may be obtained.
- the caption location data is obtained from a caption processor that derives one or more caption locations from user input that includes an image caption, an album title, and/or an image tag.
- Metadata location data is obtained for the image ( 406 ).
- the metadata location data specifies a metadata location derived from data provided by an image capturing device that captured the image. For example, the camera used to take the picture of the Mona Lisa may have created Exif data for the image that indicates geographic coordinates of the camera, which may indicate that the image was captured in The Louvre.
- Location pairs are identified from the landmark location data, caption location data, and metadata location data ( 408 ). Each location pair includes two different location types from the landmark location, caption location, and metadata location. For example, one pair for the image of the Mona Lisa may be the landmark location of The Louvre and the caption location of France, while another may be the landmark location of The Louvre and the metadata location of The Louvre.
- a geographic consistency score is generated for each location pair ( 410 ).
- Each geographic consistency score indicates a measure of consistency between locations specified in a location pair. For example, the landmark location of the Mona Lisa image and the metadata location are identical, and may therefore have a maximum geographic consistency score.
- the landmark location (The Louvre) is within the caption location (France), which is consistent, but the lack of precision for the caption location may result in a relatively low geographic consistency score for the landmark location—caption location pair.
- a location pair is selected based at least in part on the geographic consistency scores ( 412 ). For example, the location pair with the highest geographic consistency score may be selected.
- each location pair is ranked according to a pre-determined hierarchy of location types, and a location pair is selected based at least in part on the location pair rankings and geographic consistency scores. For example, location pairs may first be ranked according to geographic consistency scores and then ranked according to the pre-determined hierarchy.
- An image location is selected for the image from the locations specified in the selected location pair ( 414 ).
- an image location for the image is selected from the locations specified in the selected location pair based on the pre-determined hierarchy of association types. For example, if the landmark location—metadata location pair was selected, and the metadata location has the highest priority according to the pre-determined hierarchy, the metadata location may be selected as the image location for the image.
- An image location score is determined for the image based on a confidence score for one of the locations specified in the selected location pair ( 416 ).
- the image location score may be the confidence score of the location included in the selected pair with the highest priority according to the pre-determined hierarchy, e.g., the metadata location.
- the process 400 includes identifying a confidence score for a location specified in the selected location pair, identifying a location type for the location, and generating a topicality score for the image based on the identified confidence score and the identified location type. For example, the process 400 may identify a confidence score for a location specified in the selected location pair based on the pre-determined hierarchy. The location with the higher priority according to the pre-determined hierarchy may be the location whose corresponding confidence score is identified.
- generating a topicality score for the image includes inputting the identified confidence score into a function that corresponds to the identified location type.
- the function may be one of three different functions that each correspond to a different location type, and the topicality score may be obtained as output from the function. For example, if the metadata location has the highest priority according to the pre-determined hierarchy, and it is in the selected location pair, its corresponding metadata location confidence score may be provided, as input, to a topicality function specific to metadata location scores.
- the metadata location topicality score function provides a topicality score for the image as output.
- the image location and image location score are associated with the image ( 418 ).
- the associations may be recorded in an index used for image organization and image search.
- the topicality score may also be associated with the image and stored in an index.
- FIG. 5 is a block diagram of an example data processing apparatus 500 .
- the system 500 includes a processor 510 , a memory 520 , a storage device 530 , and an input/output device 540 .
- Each of the components 510 , 520 , 530 , and 540 can, for example, be interconnected using a system bus 550 .
- the processor 510 is capable of processing instructions for execution within the system 500 .
- the processor 510 is a single-threaded processor.
- the processor 510 is a multi-threaded processor.
- the processor 510 is capable of processing instructions stored in the memory 520 or on the storage device 530 .
- the memory 520 stores information within the system 500 .
- the memory 520 is a computer-readable medium.
- the memory 520 is a volatile memory unit.
- the memory 520 is a non-volatile memory unit.
- the storage device 530 is capable of providing mass storage for the system 500 .
- the storage device 530 is a computer-readable medium.
- the storage device 530 can, for example, include a hard disk device, an optical disk device, or some other large capacity storage device.
- the input/output device 540 provides input/output operations for the system 500 .
- the input/output device 540 can include one or more network interface devices, e.g., an Ethernet card, a serial communication device, e.g., an RS-232 port, and/or a wireless interface device, e.g., an 802.11 card.
- the input/output device can include driver devices configured to receive input data and send output data to other input/output devices, e.g., keyboard, printer and display devices 560 .
- Other implementations, however, can also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, etc.
- Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.
- the computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- the term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., a FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- special purpose logic circuitry e.g., a FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a user computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include users and servers.
- a user and server are generally remote from each other and typically interact through a communication network. The relationship of user and server arises by virtue of computer programs running on the respective computers and having a user-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a user device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the user device).
- Data generated at the user device e.g., a result of the user interaction
Abstract
Description
Claims (17)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/052,157 US9069794B1 (en) | 2013-10-11 | 2013-10-11 | Determining location information for images using landmark, caption, and metadata location data |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/052,157 US9069794B1 (en) | 2013-10-11 | 2013-10-11 | Determining location information for images using landmark, caption, and metadata location data |
Publications (1)
Publication Number | Publication Date |
---|---|
US9069794B1 true US9069794B1 (en) | 2015-06-30 |
Family
ID=53441771
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/052,157 Expired - Fee Related US9069794B1 (en) | 2013-10-11 | 2013-10-11 | Determining location information for images using landmark, caption, and metadata location data |
Country Status (1)
Country | Link |
---|---|
US (1) | US9069794B1 (en) |
Cited By (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9635079B1 (en) * | 2015-11-12 | 2017-04-25 | International Business Machines Corporation | Social media sharing based on video content |
US20180025006A1 (en) * | 2016-07-20 | 2018-01-25 | Nhn Entertainment Corporation | System and method for providing image search result online using device information |
US20180039276A1 (en) * | 2016-08-04 | 2018-02-08 | Canvas Technology, Inc. | System and methods of determining a geometric pose of a camera based on spatial and visual mapping |
US9906921B2 (en) * | 2015-02-10 | 2018-02-27 | Qualcomm Incorporated | Updating points of interest for positioning |
US10013639B1 (en) | 2013-12-16 | 2018-07-03 | Amazon Technologies, Inc. | Analyzing digital images based on criteria |
US20190392418A1 (en) * | 2018-06-21 | 2019-12-26 | Capital One Services, Llc | Systems For Providing and Processing Customized Location-Activated Gifts |
US10650621B1 (en) | 2016-09-13 | 2020-05-12 | Iocurrents, Inc. | Interfacing with a vehicular controller area network |
US10793369B2 (en) | 2017-07-12 | 2020-10-06 | A9.Com, Inc. | Conveyor system for autonomous robot |
US11086328B2 (en) | 2016-08-23 | 2021-08-10 | A9.Com, Inc. | Autonomous cart for manufacturing and warehouse applications |
US11191005B2 (en) * | 2019-05-29 | 2021-11-30 | At&T Intellectual Property I, L.P. | Cyber control plane for universal physical space |
US11760221B2 (en) | 2017-06-27 | 2023-09-19 | A9.Com, Inc. | Charging systems and methods for autonomous carts |
Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20090290812A1 (en) * | 2008-05-23 | 2009-11-26 | Mor Naaman | System to Compile Landmark Image Search Results |
US7660468B2 (en) * | 2005-05-09 | 2010-02-09 | Like.Com | System and method for enabling image searching using manual enrichment, classification, and/or segmentation |
US20120013767A1 (en) | 2010-07-16 | 2012-01-19 | Research In Motion Limited | Apparatus, and associated method, for tagging a captured image with geographical indicia |
US20120020578A1 (en) * | 2010-06-18 | 2012-01-26 | Google Inc. | Identifying Establishments in Images |
US8212834B2 (en) * | 2009-07-30 | 2012-07-03 | Eastman Kodak Company | Artistic digital template for image display |
US20120258776A1 (en) * | 2009-05-01 | 2012-10-11 | Lord John D | Methods and Systems for Content Processing |
US8311556B2 (en) | 2009-01-22 | 2012-11-13 | Htc Corporation | Method and system for managing images and geographic location data in a mobile device |
US8422794B2 (en) * | 2009-07-30 | 2013-04-16 | Intellectual Ventures Fund 83 Llc | System for matching artistic attributes of secondary image and template to a primary image |
US20130138685A1 (en) * | 2008-05-12 | 2013-05-30 | Google Inc. | Automatic Discovery of Popular Landmarks |
US20130202198A1 (en) * | 2009-05-15 | 2013-08-08 | Google Inc. | Landmarks from Digital Photo Collections |
US20130246409A1 (en) | 2011-07-31 | 2013-09-19 | Hanan Polansky | Topography by popularity |
US20140072173A1 (en) * | 2012-09-12 | 2014-03-13 | International Business Machines Corporation | Location determination for an object using visual data |
-
2013
- 2013-10-11 US US14/052,157 patent/US9069794B1/en not_active Expired - Fee Related
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7660468B2 (en) * | 2005-05-09 | 2010-02-09 | Like.Com | System and method for enabling image searching using manual enrichment, classification, and/or segmentation |
US20130138685A1 (en) * | 2008-05-12 | 2013-05-30 | Google Inc. | Automatic Discovery of Popular Landmarks |
US20090290812A1 (en) * | 2008-05-23 | 2009-11-26 | Mor Naaman | System to Compile Landmark Image Search Results |
US8311556B2 (en) | 2009-01-22 | 2012-11-13 | Htc Corporation | Method and system for managing images and geographic location data in a mobile device |
US8437777B2 (en) | 2009-01-22 | 2013-05-07 | Htc Corporation | Method and system for managing images and geographic location data in a mobile device |
US20120258776A1 (en) * | 2009-05-01 | 2012-10-11 | Lord John D | Methods and Systems for Content Processing |
US20130202198A1 (en) * | 2009-05-15 | 2013-08-08 | Google Inc. | Landmarks from Digital Photo Collections |
US8212834B2 (en) * | 2009-07-30 | 2012-07-03 | Eastman Kodak Company | Artistic digital template for image display |
US8422794B2 (en) * | 2009-07-30 | 2013-04-16 | Intellectual Ventures Fund 83 Llc | System for matching artistic attributes of secondary image and template to a primary image |
US20120020578A1 (en) * | 2010-06-18 | 2012-01-26 | Google Inc. | Identifying Establishments in Images |
US20120013767A1 (en) | 2010-07-16 | 2012-01-19 | Research In Motion Limited | Apparatus, and associated method, for tagging a captured image with geographical indicia |
US20130246409A1 (en) | 2011-07-31 | 2013-09-19 | Hanan Polansky | Topography by popularity |
US20140072173A1 (en) * | 2012-09-12 | 2014-03-13 | International Business Machines Corporation | Location determination for an object using visual data |
Cited By (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10013639B1 (en) | 2013-12-16 | 2018-07-03 | Amazon Technologies, Inc. | Analyzing digital images based on criteria |
US9906921B2 (en) * | 2015-02-10 | 2018-02-27 | Qualcomm Incorporated | Updating points of interest for positioning |
US9635079B1 (en) * | 2015-11-12 | 2017-04-25 | International Business Machines Corporation | Social media sharing based on video content |
US20180025006A1 (en) * | 2016-07-20 | 2018-01-25 | Nhn Entertainment Corporation | System and method for providing image search result online using device information |
US11080319B2 (en) * | 2016-07-20 | 2021-08-03 | Nhn Entertainment Corporation | System and method for providing image search result online using device information |
US20180039276A1 (en) * | 2016-08-04 | 2018-02-08 | Canvas Technology, Inc. | System and methods of determining a geometric pose of a camera based on spatial and visual mapping |
US9964955B2 (en) * | 2016-08-04 | 2018-05-08 | Canvas Technology, Inc. | System and methods of determining a geometric pose of a camera based on spatial and visual mapping |
US11086328B2 (en) | 2016-08-23 | 2021-08-10 | A9.Com, Inc. | Autonomous cart for manufacturing and warehouse applications |
US11232655B2 (en) | 2016-09-13 | 2022-01-25 | Iocurrents, Inc. | System and method for interfacing with a vehicular controller area network |
US10650621B1 (en) | 2016-09-13 | 2020-05-12 | Iocurrents, Inc. | Interfacing with a vehicular controller area network |
US11760221B2 (en) | 2017-06-27 | 2023-09-19 | A9.Com, Inc. | Charging systems and methods for autonomous carts |
US10793369B2 (en) | 2017-07-12 | 2020-10-06 | A9.Com, Inc. | Conveyor system for autonomous robot |
US20190392418A1 (en) * | 2018-06-21 | 2019-12-26 | Capital One Services, Llc | Systems For Providing and Processing Customized Location-Activated Gifts |
US11004057B2 (en) * | 2018-06-21 | 2021-05-11 | Capital One Services, Llc | Systems for providing and processing customized location-activated gifts |
US11191005B2 (en) * | 2019-05-29 | 2021-11-30 | At&T Intellectual Property I, L.P. | Cyber control plane for universal physical space |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9069794B1 (en) | Determining location information for images using landmark, caption, and metadata location data | |
JP6266080B2 (en) | Method and system for evaluating matching between content item and image based on similarity score | |
US9727565B2 (en) | Photo and video search | |
JP6196316B2 (en) | Adjusting content distribution based on user posts | |
US9753951B1 (en) | Presenting image search results | |
US8995716B1 (en) | Image search results by seasonal time period | |
US20170270222A1 (en) | Organizing search results based upon clustered content | |
US9179192B1 (en) | Associating video content with geographic maps | |
Shen et al. | Automatic tag generation and ranking for sensor-rich outdoor videos | |
US20170255652A1 (en) | Method for dynamically matching images with content items based on keywords in response to search queries | |
US20120131009A1 (en) | Enhancing personal data search with information from social networks | |
US20170351709A1 (en) | Method and system for dynamically rankings images to be matched with content in response to a search query | |
CN108763244B (en) | Searching and annotating within images | |
US9934283B2 (en) | Social annotations for enhanced search results | |
WO2012138585A2 (en) | Event determination from photos | |
JP6363682B2 (en) | Method for selecting an image that matches content based on the metadata of the image and content | |
US10685073B1 (en) | Selecting textual representations for entity attribute values | |
US11055335B2 (en) | Contextual based image search results | |
US20150227583A1 (en) | Managing search results | |
US10922321B2 (en) | Interpreting user queries based on device orientation | |
Ennis et al. | High-level geospatial information discovery and fusion for geocoded multimedia | |
Deeksha et al. | A spatial clustering approach for efficient landmark discovery using geo-tagged photos | |
GENTILE | Using Flickr geotags to find similar tourism destinations | |
Xie et al. | Technical evaluation for mashing up crowdsourcing images | |
Ennis et al. | A System for Real-Time High-Level Geo-Information Extraction and Fusion for Geocoded Photos |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BANDUKWALA, FARHANA;ADAM, HARTWIG;FLYNN, JOHN;AND OTHERS;SIGNING DATES FROM 20130925 TO 20131010;REEL/FRAME:031782/0318 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20230630 |