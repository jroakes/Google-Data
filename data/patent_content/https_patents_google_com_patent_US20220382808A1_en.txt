US20220382808A1 - Automated product identification within hosted and streamed videos - Google Patents
Automated product identification within hosted and streamed videos Download PDFInfo
- Publication number
- US20220382808A1 US20220382808A1 US17/334,923 US202117334923A US2022382808A1 US 20220382808 A1 US20220382808 A1 US 20220382808A1 US 202117334923 A US202117334923 A US 202117334923A US 2022382808 A1 US2022382808 A1 US 2022382808A1
- Authority
- US
- United States
- Prior art keywords
- video
- embeddings
- product
- product identification
- determining
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 239000000047 product Substances 0.000 claims description 208
- 238000000034 method Methods 0.000 claims description 61
- 238000001514 detection method Methods 0.000 claims description 29
- 230000015654 memory Effects 0.000 claims description 22
- 238000011156 evaluation Methods 0.000 claims description 7
- 239000012467 final product Substances 0.000 claims description 7
- 239000000284 extract Substances 0.000 claims description 5
- 238000005070 sampling Methods 0.000 claims description 5
- 238000012545 processing Methods 0.000 description 56
- 238000010586 diagram Methods 0.000 description 18
- 238000007667 floating Methods 0.000 description 16
- 238000010801 machine learning Methods 0.000 description 15
- 230000008569 process Effects 0.000 description 15
- 238000000605 extraction Methods 0.000 description 14
- 239000013598 vector Substances 0.000 description 6
- 238000012552 review Methods 0.000 description 5
- 238000013518 transcription Methods 0.000 description 5
- 230000035897 transcription Effects 0.000 description 5
- 238000013528 artificial neural network Methods 0.000 description 4
- 238000013459 approach Methods 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 3
- 238000004891 communication Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000002776 aggregation Effects 0.000 description 2
- 238000004220 aggregation Methods 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000013135 deep learning Methods 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 239000003086 colorant Substances 0.000 description 1
- 230000008867 communication pathway Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000003203 everyday effect Effects 0.000 description 1
- 230000002068 genetic effect Effects 0.000 description 1
- 238000002372 labelling Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7837—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using objects detected or recognised in the video content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/71—Indexing; Data structures therefor; Storage structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/783—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/7844—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using original textual content or text extracted from visual content or transcript of audio data
-
- G06K9/00671—
-
- G06K9/00744—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/7715—Feature extraction, e.g. by transforming the feature space, e.g. multi-dimensional scaling [MDS]; Mappings, e.g. subspace methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/80—Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level
- G06V10/803—Fusion, i.e. combining data from various sources at the sensor level, preprocessing level, feature extraction level or classification level of input or preprocessed data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
Definitions
- Digital video streams may represent video using a sequence of frames or still images.
- Digital video can be used for various applications including, for example, video conferencing, high definition video entertainment, video advertisements, or sharing of user generated videos.
- a digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data.
- Various approaches have been proposed to reduce the amount of data in video streams, including encoding or decoding techniques.
- Disclosed herein are, inter alia, systems and techniques for automated product identification within hosted and streamed videos.
- a method comprises receiving a video at an online video platform, detecting a first object within one or more frames selected from the video, determining first embeddings based on pixel information associated with the first object, determining one or more object candidates based on entities extracted from text content associated with the video, detecting a second object within one or more images identified responsive to a search based on the one or more object candidates, determining second embeddings based pixel information associated with the second object, producing a product candidate index based on the second embeddings, determining a number of nearest neighbors based on a comparison of the first embeddings against entries of the product candidate index, determining a product identification representative of a product featured in the video based on the number of nearest neighbors, and outputting an indication of the product identification at the online video platform.
- An apparatus comprises a memory and a processor configured to execute instructions stored in the memory to receive a video at an online video platform, determine first embeddings representative of one or more first candidate products based on video content of the video, determine second embeddings representative of one or more second candidate products based on text content associated with the video, produce a product candidate index based on the second embeddings, determine a product identification representative of a product featured in the video based on a comparison of the first embeddings against entries of the product candidate index, and output an indication of the product identification at the online video platform.
- a non-transitory computer readable storage device includes program instructions that, when executed by a processor, cause the processor to perform operations.
- the operations comprise determining first embeddings representing first candidate products based on video content of a video, determining second embeddings representing second candidate products based on text content associated with the video sequence, determining a product identification representative of a product featured in the video based on a comparison of the first embeddings and the second embeddings, and outputting an indication of the product identification.
- FIG. 1 is a schematic of an example of a video platform system.
- FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station.
- FIG. 3 is a diagram of an example of a video stream which may be uploaded to and processed at an online video platform.
- FIG. 4 is a block diagram of an example of a video stream processing system.
- FIG. 5 is a block diagram of an example of a product identification stage of a video stream processing system.
- FIG. 6 is a block diagram of an example of a video processing pipeline of a product identification stage.
- FIG. 7 is a block diagram of an example of a text processing pipeline of a product identification stage.
- FIG. 8 is a flowchart diagram of an example of a technique for automated product identification within a video.
- FIG. 9 is a flowchart diagram of an example of a technique for determining embeddings from video content associated with a video.
- FIG. 10 is a flowchart diagram of an example of a technique for determining embeddings from video content associated with a video.
- Video product reviews which are videos which feature a product and may be uploaded by social media influencers, website personalities, or the like.
- a video product review may take up an entire video or may be limited to only a portion of a video in which the rest of the video relates to other people or things.
- a video product review typically includes some information about a subject product that the video uploader intends to feature, promote, sell, or otherwise indicate to viewers.
- a video uploader is required to manually label or link products, such as within the text describing the video.
- this manual process requires time, many include typographical or other errors, and is entirely reliant upon the knowledge of the video uploader.
- a video uploader may unknowingly link a different product in a video description, may omit key information about the product (e.g., manufacturer, model type, etc.), or the like. Nevertheless, and given the continuing popularity of video product reviews, it is important to be able to accurately identify a product being promoted in a video.
- One solution which may be available is to leverage existing computer vision technology configured for object detection to identify products, however such a solution may fail for one or more reasons.
- an object detection system may incorrectly identify an object within background imagery of video content and misrepresent that as the subject product.
- user generated video content shows multiple people or things, so the potential for a false positive or false negative remains.
- Another solution may use language processing to identify products, such as based on a transcript of a user generated video.
- this solution may also fail to identify a product for one or more reasons, for example, due to the difficulty in understanding intentionality within text.
- Implementations of this disclosure address problems such as these by automated product identification within hosted and streamed videos performed based on video content of a video received at an online video platform and text content associated with the video.
- First embeddings representative of one or more first candidate products are determined based on video content of the video, such as one or more frames selected from within the video.
- Second embeddings representative of one or more second candidate products are determined based on text content associated with the video, such as a title, description, or transcript of the video.
- a product candidate index is produced based on the second embeddings.
- a product identification representative of a product featured in the video is determined based on a comparison of the first embeddings against entries of the product candidate index, such as including by a nearest neighbor search responsive to the comparison.
- An indication of the product identification is then output at the online video platform.
- the implementations for automated product identification within hosted and streamed videos disclosed herein may be used to automatically identify products in both pre-recorded and livestreamed videos and to improve product-related search and discovery capabilities within an online video platform which hosts those videos and/or through an internet search engine.
- user generated video content may be automatically associated with information associated with a product identified using the implementations of this disclosure, and a person searching for that product or for videos of that product may be presented with search results including the user generated video content, even without manual user labeling or annotation.
- the implementations for automated product identification within hosted and streamed videos disclosed herein may further be used to enhance user generated video content. For example, relevant links to a product identified within a video may be presented to viewers by the online video platform as the product is being discussed within the video.
- FIG. 1 is a schematic of an example of a video platform system 100 .
- a transmitting station 102 can be, for example, a computer having an internal configuration of hardware such as that described in FIG. 2 .
- the processing of the transmitting station 102 can be distributed among multiple devices.
- a network 104 can connect the transmitting station 102 and a receiving station 106 for the uploading, processing, and/or viewing of a video stream. Specifically, the video stream can be uploaded from the transmitting station 102 and viewed at the receiving station 106 after processing.
- the network 104 can be, for example, the Internet.
- the network 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmitting station 102 to, in this example, the receiving station 106 .
- the receiving station 106 in one example, can be a computer having an internal configuration of hardware such as that described in FIG. 2 . However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 can be distributed among multiple devices.
- an implementation can omit the network 104 .
- a video stream can be uploaded from the transmitting station 102 and then stored for transmission at a later time to the receiving station 106 or any other device having memory.
- the receiving station 106 receives (e.g., via the network 104 , a computer bus, and/or some communication pathway) and stores the processed video stream for later viewing.
- a real-time transport protocol RTP
- a transport protocol other than RTP may be used (e.g., a Hypertext Transfer Protocol-based (HTTP-based) video streaming protocol).
- the transmitting station 102 may be a device of a video uploader and the receiving station 106 may be a server of an online video platform. In some cases, the transmitting station 102 may be a server of an online video platform and the receiving station 106 may be a device of a person viewing videos at the online video platform. In some cases, the transmitting station 102 may be a device of a video uploader and the receiving station 106 may be a device of a person viewing videos at an online video platform, such as where a server of the online video platform is intermediary thereto.
- FIG. 2 is a block diagram of an example of a computing device 200 that can implement a transmitting station or a receiving station.
- a computing device such as the computing device 200 can implement the transmitting station 102 and/or the receiving station 106 shown in FIG. 1 .
- the computing device 200 can be in the form of a computing system including multiple computing devices, or in the form of one computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like.
- a processor 202 in the computing device 200 can be a conventional central processing unit.
- the processor 202 can be another type of device, or multiple devices, capable of manipulating or processing information now existing or hereafter developed.
- the disclosed implementations can be practiced with one processor as shown (e.g., the processor 202 ), advantages in speed and efficiency can be achieved by using more than one processor.
- a memory 204 in computing device 200 can be a read only memory (ROM) device or a random access memory (RAM) device in an implementation. However, other suitable types of storage device can be used as the memory 204 .
- the memory 204 can include code and data 206 that is accessed by the processor 202 using a bus 212 .
- the memory 204 can further include an operating system 208 and application programs 210 , the application programs 210 including at least one program that permits the processor 202 to perform the techniques described herein.
- the application programs 210 can include applications 1 through N, which include a product identification application that performs some or all of the techniques disclosed herein.
- the computing device 200 can also include a secondary storage 214 , which can, for example, be a memory card used with a mobile computing device. Because video may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- a secondary storage 214 can, for example, be a memory card used with a mobile computing device. Because video may contain a significant amount of information, they can be stored in whole or in part in the secondary storage 214 and loaded into the memory 204 as needed for processing.
- the computing device 200 can also include one or more output devices, such as a display 218 .
- the display 218 may be, in one example, a touch sensitive display that combines a display with a touch sensitive element that is operable to sense touch inputs.
- the display 218 can be coupled to the processor 202 via the bus 212 .
- Other output devices that permit a user to program or otherwise use the computing device 200 can be provided in addition to or as an alternative to the display 218 .
- the output device is or includes a display
- the display can be implemented in various ways, including by a liquid crystal display (LCD), a cathode-ray tube (CRT) display, or a light emitting diode (LED) display, such as an organic LED (OLED) display.
- LCD liquid crystal display
- CRT cathode-ray tube
- LED light emitting diode
- OLED organic LED
- the computing device 200 can also include or be in communication with an image-sensing device 220 , for example, a camera, or any other image-sensing device 220 now existing or hereafter developed that can sense an image such as the image of a user operating the computing device 200 .
- the image-sensing device 220 can be positioned such that it is directed toward the user operating the computing device 200 .
- the position and optical axis of the image-sensing device 220 can be configured such that the field of vision includes an area that is directly adjacent to the display 218 and from which the display 218 is visible.
- the computing device 200 can also include or be in communication with a sound-sensing device 222 , for example, a microphone, or any other sound-sensing device now existing or hereafter developed that can sense sounds near the computing device 200 .
- the sound-sensing device 222 can be positioned such that it is directed toward the user operating the computing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates the computing device 200 .
- FIG. 2 depicts the processor 202 and the memory 204 of the computing device 200 as being integrated into one unit, other configurations can be utilized.
- the operations of the processor 202 can be distributed across multiple machines (wherein individual machines can have one or more processors) that can be coupled directly or across a local area or other network.
- the memory 204 can be distributed across multiple machines such as a network-based memory or memory in multiple machines performing the operations of the computing device 200 .
- the bus 212 of the computing device 200 can be composed of multiple buses.
- the secondary storage 214 can be directly coupled to the other components of the computing device 200 or can be accessed via a network and can comprise an integrated unit such as a memory card or multiple units such as multiple memory cards.
- the computing device 200 can thus be implemented in a wide variety of configurations.
- FIG. 3 is a diagram of an example of a video stream 300 which may be uploaded to and processed at an online video platform.
- the video stream 300 includes a video sequence 302 .
- the video sequence 302 includes a number of adjacent frames 304 . While three frames are depicted as the adjacent frames 304 , the video sequence 302 can include any number of adjacent frames 304 .
- the adjacent frames 304 can then be further subdivided into individual frames, for example, a frame 306 .
- the frame 306 can be divided into a series of planes or segments 308 .
- the segments 308 can be subsets of frames that permit parallel processing, for example.
- the segments 308 can also be subsets of frames that can separate the video data into separate colors.
- a frame 306 of color video data can include a luminance plane and two chrominance planes.
- the segments 308 may be sampled at different resolutions.
- the frame 306 may be further subdivided into blocks 310 , which can contain data corresponding to, for example, 16 ⁇ 16 pixels in the frame 306 .
- the blocks 310 can also be arranged to include data from one or more segments 308 of pixel data.
- the blocks 310 can also be of any other suitable size such as 4 ⁇ 4 pixels, 8 ⁇ 8 pixels, 16 ⁇ 8 pixels, 8 ⁇ 16 pixels, 16 ⁇ 16 pixels, or larger. Unless otherwise noted, the terms block and macroblock are used interchangeably herein.
- FIG. 4 is a block diagram of an example of a video stream processing system 400 .
- the video stream processing system 400 can be implemented in a server of an online video platform, as described above, such as in the transmitting station 102 or the receiving station 106 shown in FIG. 1 , such as by providing a computer software program stored in memory, for example, the memory 204 shown in FIG. 2 .
- the computer software program can include machine instructions that, when executed by a processor such as the processor 202 shown in FIG. 2 , cause the server to process an input video stream 402 in the manner described in FIG. 4 .
- the video stream processing system 400 can also be implemented as specialized hardware included in, for example, the transmitting station 102 or the receiving station 106 .
- the input video stream 402 is a video stream, such as the video stream 300 shown in FIG. 3 , which includes user generated video content and is received from a device of a video uploader.
- the video stream processing system 400 includes components for processing the input video stream 402 , including a transcoding stage 404 , a transcription stage 406 , and a product identification stage 408 .
- the transcoding stage 404 processes the input video stream 402 to produce a transcoded video 410 .
- the transcoded video 410 represents the user generated video content in a processed form.
- the transcoding stage 404 may produce the transcoded video 410 by one or more of encoding the input video stream 402 , decoding the input video stream 402 , converting the input video stream 402 in an encoded form or otherwise from a first format to a second format or otherwise preparing multiple formats of the video, changing a bitrate for later viewing of the input video stream 402 from a first bitrate to a second bitrate or otherwise preparing multiple bitrate versions of the video, changing a resolution of the input video stream 402 from first resolution to a second resolution or otherwise preparing multiple resolution versions of the video, a combination thereof, or the like.
- the transcription stage 406 produces a transcript 412 of the video of the input video stream 402 .
- the transcription stage 406 may, for example, use an automatic speech recognition processing aspect to produce the transcript 412 .
- the transcription stage 406 may produce the transcript 412 based on the input video stream 402 or the transcoded video 410 . Where the input video stream 402 is a pre-recorded video, the transcript 412 may be produced during playback of the video. Where the input video stream 402 is a livestreamed video, the transcript 412 may be produced in real-time or substantially in real-time with the processing thereof. In some implementations, the operations of the transcription stage 406 may be performed during the transcoding performed at the transcoding stage 404 .
- the product identification stage 408 determines a product identification 414 for a product which is the subject of some or all of the video of the input video stream 402 .
- the product identification 414 is data which identifies a product.
- the product identification 414 may include one or more of a model name, a manufacturer name, a release type, special features such as color or options, or the like for the product.
- the product identification stage 408 processes video and text associated with the input video stream 402 to determine the product identification 414 .
- the product identification stage 408 may use either the input video stream 402 or the transcoded video 410 for the video aspect and the transcript 412 for the text aspect.
- FIG. 5 is a block diagram of an example of a product identification stage of a video stream processing system, which may, for example, be the product identification stage 408 shown in FIG. 4 .
- the product identification stage receives as input video content 500 and text content 502 and produces or otherwise determines as output a product identification 504 .
- the video content 500 may, for example, be the input video stream 402 or the transcoded video 410 shown in FIG. 4 .
- the video content 500 includes one or more frames of a video sequence of a video hosted or streamed at an online video platform and may thus correspond to frame-level content or chunk-level content of the video.
- the text content 502 includes text associated with the video content 500 .
- the text content 502 may include one or more of a transcript generated for the video, for example, the transcript 410 shown in FIG. 4 , a title of the video, a description of the video, metadata or other data associated with the video, or a combination thereof.
- the product identification 504 may, for example, be the product identification 414 shown in FIG. 4 .
- the product identification stage uses separate processing pipelines to process the inputs.
- a video processing pipeline 506 processes the video content 500 to determine embeddings 508
- a text processing pipeline 510 processes the text content 502 to determine embeddings 512 .
- the embeddings 508 and the embeddings 512 are logical elements for representing information output respectively by the video processing pipeline 506 and the text processing pipeline 510 .
- the embeddings 508 and the embeddings 512 may be vectors of floating point values, although other data storage types and elements may be used.
- the embeddings 508 are used to represent one or more candidate products identified based on the processing at the video processing pipeline 506 .
- the embeddings 512 are used to represent one or more candidate products identified based on the processing at the text processing pipeline 510 .
- the candidate products represented by the embeddings 508 and the candidate products represented by the embeddings 512 may all be the same, may be partially the same and partially different, or may all be different.
- the video processing pipeline 506 processes the video content 500 at the frame level including by performing object detection using one or more machine learning models.
- the embeddings 508 represent pixel information from bounding boxes surrounding the detected objects within a given frame of the video content 500 . Implementations and examples of the video processing pipeline 506 are described below with respect to FIG. 6 .
- the text processing pipeline processes the text content 502 by identifying product candidates based on entity information extracted from the text content 502 . Object detection is then performed using one or more machine learning models against images obtained from searching one or more systems for the product candidates.
- the embeddings 512 represent pixel information from bounding boxes surrounding the detected objects within a given image. Implementations and examples of the text processing pipeline 510 are described below with respect to FIG. 7 .
- the video processing pipeline 506 and the text processing pipeline 510 thus use one or more machine learning models to respectively determine the embeddings 508 and the embeddings 512 .
- a machine learning model used at the video processing pipeline 506 and/or at the text processing pipeline 510 may be or include one or more of a neural network (e.g., a convolutional neural network, recurrent neural network, or other neural network), decision tree, support vector machine, Bayesian network, genetic algorithm, deep learning system separate from a neural network, or other machine learning model.
- the object detection performed at the video processing pipeline 506 and the text processing pipeline 510 may use a deep learning convolutional neural network trained for object detection within images and video frames.
- the video processing pipeline 506 and the text processing pipeline 510 may use the same or different one or more machine learning models.
- the product identification stage includes an embedding matching component 514 which processes the embeddings 508 and the embeddings 512 to determine the product identification 504 .
- the embedding matching component 514 first produces an index of the candidate products represented by the embeddings 512 .
- the index may, for example, be a light-weight, on-the-fly index of entries associated with the embeddings 512 .
- An entry within the index may include a key representative of an identifier of the subject candidate product and data associated with the respective embedding 512 , such as a floating point value.
- the index is produced on a per-video basis such that the index is specific to the subject video. This approach emphasizes products which are featured with intentionality within the video and reduces computational load and maintenance burdens on the online video platform system by limiting the pool of indices against which the embedding matching is performed.
- the embedding matching component 514 then performs a nearest neighbor search against the candidate products represented by the embeddings 508 and the embeddings 512 .
- the embedding matching component 514 compares the embeddings 508 against the index to identify entries of the index which include data similar to (e.g., within a threshold variance) or matching data of the embeddings 508 .
- the comparison may be performed between floating point values of the embeddings 508 and floating point values of entries of the index.
- the embedding matching component 514 identifies K nearest neighbors as the K index entries that are most similar to the embeddings 508 , in which K is an integer having a value greater than or equal to 1 .
- the embedding matching component 514 then evaluates those K nearest neighbors to determine the product identification 504 .
- the embedding matching component 514 evaluates the floating point values of the K nearest neighbors to identify the floating point value which has the highest mode amongst them. For example, if K is equal to 5 and 3 of those 5 nearest neighbors share the same floating point value, the candidate product associated with that floating point value (e.g., the candidate product associated with the one or more embeddings 508 having that floating point value) is identified as the product identification 504 .
- the evaluation of the K nearest neighbors is performed to determine a final candidate product for the video frame or chunk represented by the video content 500 .
- a video-level aggregation may be performed against the final candidate products determined at different times during the subject video to determine the product identification 504 .
- performing the video-level aggregation includes determining confidence scores for the final candidate products determined at the frame-level or chunk-level. After some or all of the subject video has been processed, the final candidate product associated with the highest confidence score may be identified as the product identification 504 .
- the particular approach to identifying the product identification 504 may be based on the type of video from which the video content 500 and the text content 502 derive. For example, where the video is a livestreamed video, the product identification 504 may be identified based on the evaluation of the K nearest neighbors for a given frame or chunk. In another example, where the video is a pre-recorded and thus hosted video, the product identification 504 may be identified either based on the evaluation of the K nearest neighbors for a given frame or chunk or based on an evaluation of confidence scores determined at various times during the video.
- a timestamp associated with the product identification 504 representing a time during the subject video in which the subject product is featured may be stored and later used to output the product identification 504 in one or more ways.
- the product identification 504 or information associated therewith may be presented to a viewer watching the video.
- the product identification 504 may include an annotation (e.g., including a product name and/or a website link) overlaying one or more frames of the video.
- the annotation may be located within, adjacent, or nearby to a bounding box within which the subject product is identified, in which the frame or frames corresponding to the bounding box are identified based on a timestamp associated with the product identification 504 .
- the product identification 504 may be used during a search of videos at an online video platform. For example, when a user searches the online video platform for videos associated with the product identified by the product identification 504 , the video may be identified in the search results along with the timestamp within the video at which the product is featured. In some implementations, a product name and/or a website link associated with the product identification 504 may be presented along with the search results.
- FIG. 6 is a block diagram of an example of a video processing pipeline of a product identification stage, which may, for example, be the video processing pipeline 506 shown in FIG. 5 .
- the video processing pipeline receives as input video content 600 and produces as output embeddings 602 , which may, for example, respectively be the video content 500 and the embeddings 508 shown in FIG. 5 .
- the video processing pipeline includes a frame selection component 604 which selects one or more frames 606 of the video content 600 to use to determine the embeddings 602 , an object detection component 608 which performs object detection against the frames 606 to identify objects 610 , and an embedding extraction component 612 which processes the objects 610 to determine the embeddings 602 .
- the frame selection component 604 selects the frames 606 in one or more ways. In some implementations, the frame selection component 604 selects the frames 606 by sampling N frames per second in which N is an integer greater than or equal to one. In some such implementations, the frames 606 sampled by the frame selection component 604 are selected based on those frames 606 being identified as keyframes. In some implementations, the frame selection component 604 selects the frames 606 based on a threshold difference of video data between ones of the frames 606 and frames preceding them in the video sequence, which may, for example, be measured using motion vectors computed during an encoding process of the subject video.
- the particular manner in which the frame selection component 604 selects the frames 606 may be based on computational resource availability for the online video platform and/or a type of video being processed. For example, where computational resources are abundant, the frame selection component 604 can densely sample N frames per second; however, where computational resources are limited, the frame selection component 604 can instead select amongst keyframes instead. In another example, where the subject video is a livestreamed video, the frame selection process may be more quickly performed using motion vectors and/or other motion detection techniques.
- the object detection component 608 uses a machine learning model to detect the objects 610 within the frames 606 .
- the objects 610 may be people or things appearing within the frames 606 .
- the object detection component 608 draws bounding boxes around the objects 610 responsive to their detection.
- the bounding boxes are used to represent locations of the objects 610 within the frames 606 . In some implementations, the bounding boxes themselves may be used as the objects 610 .
- the embedding extraction component 612 extracts features of the objects 610 and uses those features to determine the embeddings 602 .
- the features extracted by the embedding extraction component 612 include image data derived from the pixels located within the bounding boxes of the objects 610 .
- the input for this process is the group of pixels corresponding to the bounding box, which is processed to convert those pixel values to another format represented as the embeddings.
- the embedding extraction component 612 can output a floating point value for the group of pixels.
- the embedding extraction component 612 may use a machine learning model.
- the machine learning model may be the same as or different from the machine learning model used by the object detection component 608 .
- FIG. 7 is a block diagram of an example of a text processing pipeline of a product identification stage, which may, for example, be the text processing pipeline 510 shown in FIG. 5 .
- the text processing pipeline receives as input text content 700 and produces as output embeddings 702 , which may, for example, respectively be the text content 502 and the embeddings 512 shown in FIG. 5 .
- the text processing pipeline includes an entity extraction component 704 which extracts entities from the text content 700 to identify object candidates 706 , a search component 708 which performs a search based on the object candidates 706 to identify images 710 of the object candidates 706 , an object detection component 712 which performs object detection against the images 710 to identify objects 714 , and an embedding extraction component 716 which processes the objects 714 to determine the embeddings 702 .
- the entity extraction component 704 extracts entities in the form of data or metadata from the text content 700 .
- the entities may, for example, be keywords or phrases identified within a transcript of the subject video, the title of the video, a description of the video, or the like. For example, an entity may identify a product name, a manufacturer name, a model type, other information about the product, or the like, or a combination thereof.
- the entity extraction component 704 may use a machine learning model trained for language and context detection to extract the entities from the text content 700 .
- the entity extraction component 704 determines the object candidates 706 based on the entities.
- the machine learning model may be trained for sentiment analysis to evaluate whether a product for which an entity is extracted is featured with intentionality.
- an entity extracted from the text content 700 may fully identify a specific object candidate 706 , such as based on product name and model type. However, in some cases, an entity may omit information used to accurately identify an object candidate 706 . For example, the entity may identify a product name without identifying a specific model type. In some implementations, where an entity omits such information, the entity extraction component 704 may determine multiple potential object candidates 706 based on the entity such as to increase the chances of one of those potential object candidates 706 being the correct product featured in the subject video.
- the search component 708 performs a search for one or more images using an internet search engine based on information associated with the object candidates 706 , such as based on the data and/or metadata extracted as the entities used to determine those object candidates 706 .
- the search component 708 performs the search or searches to identify the images 710 , which are static images depicting one or more objects including a given one of the object candidates 706 . Where multiple objects are depicted in an image 710 , the main object which is the focus of the image 710 is considered to represent the object candidate 706 .
- the internet search engine used for the search may be external to the online video platform. For example, the internet search engine may be considered to be external to the online video platform where uniform resource locators (URLs) of the internet search engine and the online video platform have different domain names or different subdomain names.
- URLs uniform resource locators
- the object detection component 712 and the embedding extraction component 716 may operate in the same or in a substantially similar manner to the object detection component 608 and the embedding extraction component 612 shown in FIG. 6 .
- the object detection component 712 uses a machine learning model to detect the objects 714 within the images 710 including by drawing bounding boxes around those objects 714 within the images 610
- the embedding extraction component 716 extracts features of the objects 714 and uses those features to determine the embeddings 702 such as by converting pixel information representative of those features into floating point values.
- FIG. 8 is a flowchart diagram of an example of a technique 800 for automated product identification within a video.
- FIG. 9 is a flowchart diagram of an example of a technique 900 for determining embeddings from video content associated with a video.
- FIG. 10 is a flowchart diagram of an example of a technique 1000 for determining embeddings from video content associated with a video.
- the technique 800 , the technique 900 , and/or the technique 1000 can be implemented, for example, as a software program that may be executed by computing devices such as the transmitting station 102 or the receiving station 106 .
- the software program can include machine-readable instructions that may be stored in a memory such as the memory 204 or the secondary storage 214 , and that, when executed by a processor, such as the processor 202 , may cause the computing device to perform the technique 800 , the technique 900 , and/or the technique 1000 .
- the technique 800 , the technique 900 , and/or the technique 1000 can be implemented using specialized hardware or firmware.
- a hardware component configured to perform the technique 800 , the technique 900 , and/or the technique 1000 .
- some computing devices may have multiple memories or processors, and the operations described in the technique 800 , the technique 900 , and/or the technique 1000 can be distributed using multiple processors, memories, or both.
- the technique 800 , the technique 900 , and the technique 1000 are each depicted and described herein as a series of steps or operations. However, the steps or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter.
- a video is received.
- the video may, for example, be received at an online video platform.
- the video may be a pre-recorded video which is uploaded to the online video platform for hosting such as through a website or application associated with the online video platform.
- the video may be a livestreamed video which is uploaded in real-time to the online video platform over a real-time streaming protocol connecting a device at which the video is being recorded to the online video platform.
- first embeddings are determined based on video content of the video.
- the first embeddings represent product candidates identified based on the one or more frames selected from the video.
- Determining the first embeddings includes detecting a first object within one or more frames selected from the video and determining first embeddings based on pixel information associated with the first object. Further implementations and examples of techniques for determining the first embeddings are described below with respect to FIG. 9 .
- second embeddings are determined based on text content associated with the video.
- the second embeddings represent product candidates identified based on the entities extracted from the text content.
- Determining the second embeddings includes determining one or more object candidates based on entities extracted from text content associated with the video, detecting a second object within one or more images identified responsive to a search based on the one or more object candidates, and determining second embeddings based pixel information associated with the second object. Further implementations and examples of techniques for determining the first embeddings are described below with respect to FIG. 10 .
- a product identification representative of a product featured in the video is determined. Determining the product identification includes producing a product candidate index based on the second embeddings, determining a number of nearest neighbors based on a comparison of the first embeddings against entries of the product candidate index, and determining a product identification representative of a product featured in the video based on the number of nearest neighbors.
- floating point values determined based on the video content e.g., the first embeddings
- the entries within the product candidate index e.g., floating point values determined based on the text content (e.g., the second embeddings)
- the floating point value with the highest mode amongst the nearest neighbors is identified, and the candidate product associated therewith is determined as the product featured in the video.
- the product identification is thus determined based on that candidate product. In this way, the determination of a product identification representative of a product featured in the video may be made based on a comparison of the first embeddings against the entries of the product candidate index.
- an indication of the product identification is output.
- the indication of the product identification may be output at the online video platform at which the video was received.
- outputting the indication of the product identification at the online video platform may include outputting an annotation including the indication of the product identification proximate to a bounding box associated with an object detected within one or more frames of the video during a playback of the video at the online video platform.
- the indication of the product identification is a representation of the video output at the online video platform responsive to a user search at the online video platform for videos featuring the product.
- the product identification determined based on the comparison of the first embeddings against the entries of the product candidate index is not the final product identification for which an indication is output.
- the product identification may be one of multiple product identifications determined for the video.
- a confidence score may be determined for each of the product identifications, in which each of the product identifications corresponds to one of a variety of times during the video.
- the final product identification may then be determined based on an evaluation of the various confidence scores.
- the indication output may be an indication of the final product identification.
- the technique 900 for determining embeddings from video content associated with a video is shown.
- one or more frames of the video are selected.
- the one or more frames may be selected according to a sampling rate determined for the video.
- the sampling rate may be based on computational resource availability, a defined frame or time interval for sampling, a frame type (e.g., keyframe), or the like.
- the one or more frames may be selected based on motion information determined, such as motion vectors computed between a current frame and a previously encoded frame of the video sequence of the video.
- an object is detected within a bounding box in the one or more frames.
- Detecting the object within the bounding box includes perform object detection against the one or more frames to detect an object within a bounding box in the one or more frames.
- the object detection may, for example, be performed using a machine learning model trained for object detection.
- the bounding box is a generally rectangular area enclosing some or all of the detected object within the one or more frames.
- the object may be in one or more frames by virtue of the object being present in one or more frames of the video, regardless of motion, translation, or warping of the object between frames.
- pixel information is extracted from the bounding box.
- the pixel information refers to RGB values, YUV values, other color values, other luminance and/or chrominance values, and/or other values enclosed within the one or more frames by the bounding box. Extracting the pixel information can include copying the values from within the bounding box to a separate source, such as a buffer, data store, or the like.
- an embedding is determined based on the pixel information. Determining the embedding can include using the pixel information of the bounding box, such as which has been extracted from the bounding box, to compute a floating point value vector for the pixel information as the embedding.
- the embedding is one of multiple embeddings which may be determined for the purpose of identifying a given product featured within the video.
- the technique 1000 for determining embeddings from video content associated with a video is shown.
- entities are extracted from text content associated with the video.
- the entities are extracted using a text parsing software aspect configured to read text in one or more languages.
- the text may be a title of the video, a description of the video, a transcript produced for the video, or the like.
- the entities are extracted to a buffer, data store, or the like for further processing.
- one or more object candidates are determined based on the entities.
- the entities are processed to identify one or more of a product name, a manufacturer name, a model type, and/or other information usable to identify an object candidate which may be a particular product featured in the video.
- the entities are processed to identify multiple object candidates, for example, where there are multiple potential products determined based on the entities.
- the entities may partially represent a potential product such as based on referencing a product name without a model type or a manufacturer name without a product name. In such a case, additional entities may be evaluated to cover multiple possible products which could be extrapolated from the partial representations by the entities.
- a search is performed based on the object candidates to identify one or more images.
- the search is performed using an internet search engine external to the online video platform that receives the video.
- Performing the search includes entering the object candidate information determined based on the entities into a search string for processing by the internet search engine.
- the internet search engine may perform a web search for websites or an image search for images.
- the search is ultimately performed to identify images and so results of the search are parsed for images. Those images are retrieved from the various sources and stored in a buffer, data store, or the like for further processing.
- an object is detected within a bounding box in the one or more images.
- Detecting the object within the bounding box includes perform object detection against the one or more images to detect an object within a bounding box in the one or images frames.
- the object detection may, for example, be performed using a machine learning model trained for object detection.
- the bounding box is a generally rectangular area enclosing some or all of the detected object within the one or more images.
- the object may be in one or more images by virtue of the object being separately included in each of those one or more images.
- pixel information is extracted from the bounding box.
- the pixel information refers to RGB values, YUV values, other color values, other luminance and/or chrominance values, and/or other values enclosed within the one or more images by the bounding box. Extracting the pixel information can include copying the values from within the bounding box to a separate source, such as a buffer, data store, or the like.
- an embedding is determined based on the pixel information. Determining the embedding can include using the pixel information of the bounding box, such as which has been extracted from the bounding box, to compute a floating point value vector for the pixel information as the embedding.
- the embedding is one of multiple embeddings which may be determined for the purpose of identifying a given product featured within the video.
- example is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the word “example” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clearly indicated otherwise by the context, the statement “X includes A or B” is intended to mean any of the natural inclusive permutations thereof. That is, if X includes A; X includes B; or X includes both A and B, then “X includes A or B” is satisfied under any of the foregoing instances.
- Implementations of the transmitting station 102 and/or the receiving station 106 can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit.
- signal processors should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- signals and “data” are used interchangeably. Further, portions of the transmitting station 102 and the receiving station 106 do not necessarily have to be implemented in the same manner.
- the transmitting station 102 or the receiving station 106 can be implemented using a general purpose computer or general purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein.
- a special purpose computer/processor can be utilized which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein.
- implementations of this disclosure can take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium.
- a computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
Abstract
Automated product identification within hosted and streamed videos is performed based on video content of a video received at an online video platform and text content associated with the video. First embeddings representative of one or more first candidate products are determined based on video content of the video, such as one or more frames selected from within the video. Second embeddings representative of one or more second candidate products are determined based on text content associated with the video, such as a title, description, or transcript of the video. A product candidate index is produced based on the second embeddings. A product identification representative of a product featured in the video is determined based on a comparison of the first embeddings against entries of the product candidate index, such as including by a nearest neighbor search responsive to the comparison. An indication of the product identification is then output at the online video platform.
Description
- Digital video streams may represent video using a sequence of frames or still images. Digital video can be used for various applications including, for example, video conferencing, high definition video entertainment, video advertisements, or sharing of user generated videos. A digital video stream can contain a large amount of data and consume a significant amount of computing or communication resources of a computing device for processing, transmission, or storage of the video data. Various approaches have been proposed to reduce the amount of data in video streams, including encoding or decoding techniques.
- Disclosed herein are, inter alia, systems and techniques for automated product identification within hosted and streamed videos.
- A method according to an implementation of this disclosure comprises receiving a video at an online video platform, detecting a first object within one or more frames selected from the video, determining first embeddings based on pixel information associated with the first object, determining one or more object candidates based on entities extracted from text content associated with the video, detecting a second object within one or more images identified responsive to a search based on the one or more object candidates, determining second embeddings based pixel information associated with the second object, producing a product candidate index based on the second embeddings, determining a number of nearest neighbors based on a comparison of the first embeddings against entries of the product candidate index, determining a product identification representative of a product featured in the video based on the number of nearest neighbors, and outputting an indication of the product identification at the online video platform.
- An apparatus according to another implementation of this disclosure comprises a memory and a processor configured to execute instructions stored in the memory to receive a video at an online video platform, determine first embeddings representative of one or more first candidate products based on video content of the video, determine second embeddings representative of one or more second candidate products based on text content associated with the video, produce a product candidate index based on the second embeddings, determine a product identification representative of a product featured in the video based on a comparison of the first embeddings against entries of the product candidate index, and output an indication of the product identification at the online video platform.
- A non-transitory computer readable storage device according to yet another implementation of this disclosure includes program instructions that, when executed by a processor, cause the processor to perform operations. The operations comprise determining first embeddings representing first candidate products based on video content of a video, determining second embeddings representing second candidate products based on text content associated with the video sequence, determining a product identification representative of a product featured in the video based on a comparison of the first embeddings and the second embeddings, and outputting an indication of the product identification.
- The disclosure is best understood from the following detailed description when read in conjunction with the accompanying drawings. It is emphasized that, according to common practice, the various features of the drawings are not to scale. On the contrary, the dimensions of the various features are arbitrarily expanded or reduced for clarity.
-
FIG. 1 is a schematic of an example of a video platform system. -
FIG. 2 is a block diagram of an example of a computing device that can implement a transmitting station or a receiving station. -
FIG. 3 is a diagram of an example of a video stream which may be uploaded to and processed at an online video platform. -
FIG. 4 is a block diagram of an example of a video stream processing system. -
FIG. 5 is a block diagram of an example of a product identification stage of a video stream processing system. -
FIG. 6 is a block diagram of an example of a video processing pipeline of a product identification stage. -
FIG. 7 is a block diagram of an example of a text processing pipeline of a product identification stage. -
FIG. 8 is a flowchart diagram of an example of a technique for automated product identification within a video. -
FIG. 9 is a flowchart diagram of an example of a technique for determining embeddings from video content associated with a video. -
FIG. 10 is a flowchart diagram of an example of a technique for determining embeddings from video content associated with a video. - Online video platforms, such as YouTube, receive years-worth of new user generated videos every day, which are watched by viewers around the world. User generated video content, which may include both pre-recorded videos and livestreamed videos, generally features one or more people or things. One particular example of user generated video content which continues to grow increasingly popular with viewers are video product reviews, which are videos which feature a product and may be uploaded by social media influencers, website personalities, or the like. A video product review may take up an entire video or may be limited to only a portion of a video in which the rest of the video relates to other people or things. A video product review typically includes some information about a subject product that the video uploader intends to feature, promote, sell, or otherwise indicate to viewers.
- Given that viewers of a video product review are likely to want to learn more about or purchase the subject product, it is important that information identifying the subject product be easily available to the viewer. Conventionally, to enable viewer access to such product information, a video uploader is required to manually label or link products, such as within the text describing the video. However, this manual process requires time, many include typographical or other errors, and is entirely reliant upon the knowledge of the video uploader. For example, a video uploader may unknowingly link a different product in a video description, may omit key information about the product (e.g., manufacturer, model type, etc.), or the like. Nevertheless, and given the continuing popularity of video product reviews, it is important to be able to accurately identify a product being promoted in a video.
- One solution which may be available is to leverage existing computer vision technology configured for object detection to identify products, however such a solution may fail for one or more reasons. For example, an object detection system may incorrectly identify an object within background imagery of video content and misrepresent that as the subject product. In many cases, user generated video content shows multiple people or things, so the potential for a false positive or false negative remains. Furthermore, it can be difficult to successfully disambiguate among similar-looking products using visual signals alone. Another solution may use language processing to identify products, such as based on a transcript of a user generated video. However, this solution may also fail to identify a product for one or more reasons, for example, due to the difficulty in understanding intentionality within text. In particular, intention is an important factor for understanding whether a product is being promoted as opposed to just being mentioned in passing. Where a transcript of a video or a written description thereof mentions multiple products, it is difficult to successfully determine by text processing which is the subject product being promoted with intentionality. Hence, existing solutions for video and textual processing for product identification each suffer drawbacks.
- Implementations of this disclosure address problems such as these by automated product identification within hosted and streamed videos performed based on video content of a video received at an online video platform and text content associated with the video. First embeddings representative of one or more first candidate products are determined based on video content of the video, such as one or more frames selected from within the video. Second embeddings representative of one or more second candidate products are determined based on text content associated with the video, such as a title, description, or transcript of the video. A product candidate index is produced based on the second embeddings. A product identification representative of a product featured in the video is determined based on a comparison of the first embeddings against entries of the product candidate index, such as including by a nearest neighbor search responsive to the comparison. An indication of the product identification is then output at the online video platform.
- The implementations for automated product identification within hosted and streamed videos disclosed herein may be used to automatically identify products in both pre-recorded and livestreamed videos and to improve product-related search and discovery capabilities within an online video platform which hosts those videos and/or through an internet search engine. For example, user generated video content may be automatically associated with information associated with a product identified using the implementations of this disclosure, and a person searching for that product or for videos of that product may be presented with search results including the user generated video content, even without manual user labeling or annotation. The implementations for automated product identification within hosted and streamed videos disclosed herein may further be used to enhance user generated video content. For example, relevant links to a product identified within a video may be presented to viewers by the online video platform as the product is being discussed within the video.
- Further details of techniques for automated product identification within hosted and streamed videos are described herein with initial reference to a system in which such techniques can be implemented.
FIG. 1 is a schematic of an example of avideo platform system 100. A transmittingstation 102 can be, for example, a computer having an internal configuration of hardware such as that described inFIG. 2 . However, other implementations of the transmittingstation 102 are possible. For example, the processing of the transmittingstation 102 can be distributed among multiple devices. - A
network 104 can connect the transmittingstation 102 and areceiving station 106 for the uploading, processing, and/or viewing of a video stream. Specifically, the video stream can be uploaded from thetransmitting station 102 and viewed at thereceiving station 106 after processing. Thenetwork 104 can be, for example, the Internet. Thenetwork 104 can also be a local area network (LAN), wide area network (WAN), virtual private network (VPN), cellular telephone network, or any other means of transferring the video stream from the transmittingstation 102 to, in this example, thereceiving station 106. - The
receiving station 106, in one example, can be a computer having an internal configuration of hardware such as that described inFIG. 2 . However, other suitable implementations of the receivingstation 106 are possible. For example, the processing of the receivingstation 106 can be distributed among multiple devices. - Other implementations of the
video platform system 100 are possible. For example, an implementation can omit thenetwork 104. In another implementation, a video stream can be uploaded from the transmittingstation 102 and then stored for transmission at a later time to the receivingstation 106 or any other device having memory. In one implementation, the receivingstation 106 receives (e.g., via thenetwork 104, a computer bus, and/or some communication pathway) and stores the processed video stream for later viewing. In an example implementation, a real-time transport protocol (RTP) is used for transmission of the processed video over thenetwork 104. In another implementation, a transport protocol other than RTP may be used (e.g., a Hypertext Transfer Protocol-based (HTTP-based) video streaming protocol). - In some cases, the transmitting
station 102 may be a device of a video uploader and the receivingstation 106 may be a server of an online video platform. In some cases, the transmittingstation 102 may be a server of an online video platform and the receivingstation 106 may be a device of a person viewing videos at the online video platform. In some cases, the transmittingstation 102 may be a device of a video uploader and the receivingstation 106 may be a device of a person viewing videos at an online video platform, such as where a server of the online video platform is intermediary thereto. -
FIG. 2 is a block diagram of an example of acomputing device 200 that can implement a transmitting station or a receiving station. For example, a computing device such as thecomputing device 200 can implement the transmittingstation 102 and/or the receivingstation 106 shown inFIG. 1 . Thecomputing device 200 can be in the form of a computing system including multiple computing devices, or in the form of one computing device, for example, a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and the like. - A
processor 202 in thecomputing device 200 can be a conventional central processing unit. Alternatively, theprocessor 202 can be another type of device, or multiple devices, capable of manipulating or processing information now existing or hereafter developed. For example, although the disclosed implementations can be practiced with one processor as shown (e.g., the processor 202), advantages in speed and efficiency can be achieved by using more than one processor. - A
memory 204 incomputing device 200 can be a read only memory (ROM) device or a random access memory (RAM) device in an implementation. However, other suitable types of storage device can be used as thememory 204. Thememory 204 can include code anddata 206 that is accessed by theprocessor 202 using abus 212. Thememory 204 can further include anoperating system 208 andapplication programs 210, theapplication programs 210 including at least one program that permits theprocessor 202 to perform the techniques described herein. For example, theapplication programs 210 can includeapplications 1 through N, which include a product identification application that performs some or all of the techniques disclosed herein. - The
computing device 200 can also include asecondary storage 214, which can, for example, be a memory card used with a mobile computing device. Because video may contain a significant amount of information, they can be stored in whole or in part in thesecondary storage 214 and loaded into thememory 204 as needed for processing. - The
computing device 200 can also include one or more output devices, such as adisplay 218. Thedisplay 218 may be, in one example, a touch sensitive display that combines a display with a touch sensitive element that is operable to sense touch inputs. Thedisplay 218 can be coupled to theprocessor 202 via thebus 212. Other output devices that permit a user to program or otherwise use thecomputing device 200 can be provided in addition to or as an alternative to thedisplay 218. When the output device is or includes a display, the display can be implemented in various ways, including by a liquid crystal display (LCD), a cathode-ray tube (CRT) display, or a light emitting diode (LED) display, such as an organic LED (OLED) display. - The
computing device 200 can also include or be in communication with an image-sensingdevice 220, for example, a camera, or any other image-sensingdevice 220 now existing or hereafter developed that can sense an image such as the image of a user operating thecomputing device 200. The image-sensingdevice 220 can be positioned such that it is directed toward the user operating thecomputing device 200. In an example, the position and optical axis of the image-sensingdevice 220 can be configured such that the field of vision includes an area that is directly adjacent to thedisplay 218 and from which thedisplay 218 is visible. - The
computing device 200 can also include or be in communication with a sound-sensing device 222, for example, a microphone, or any other sound-sensing device now existing or hereafter developed that can sense sounds near thecomputing device 200. The sound-sensing device 222 can be positioned such that it is directed toward the user operating thecomputing device 200 and can be configured to receive sounds, for example, speech or other utterances, made by the user while the user operates thecomputing device 200. - Although
FIG. 2 depicts theprocessor 202 and thememory 204 of thecomputing device 200 as being integrated into one unit, other configurations can be utilized. For example, the operations of theprocessor 202 can be distributed across multiple machines (wherein individual machines can have one or more processors) that can be coupled directly or across a local area or other network. In another example, thememory 204 can be distributed across multiple machines such as a network-based memory or memory in multiple machines performing the operations of thecomputing device 200. - Although depicted here as one bus, the
bus 212 of thecomputing device 200 can be composed of multiple buses. Further, thesecondary storage 214 can be directly coupled to the other components of thecomputing device 200 or can be accessed via a network and can comprise an integrated unit such as a memory card or multiple units such as multiple memory cards. Thecomputing device 200 can thus be implemented in a wide variety of configurations. -
FIG. 3 is a diagram of an example of avideo stream 300 which may be uploaded to and processed at an online video platform. Thevideo stream 300 includes avideo sequence 302. At the next level, thevideo sequence 302 includes a number ofadjacent frames 304. While three frames are depicted as theadjacent frames 304, thevideo sequence 302 can include any number ofadjacent frames 304. Theadjacent frames 304 can then be further subdivided into individual frames, for example, aframe 306. - At the next level, the
frame 306 can be divided into a series of planes orsegments 308. Thesegments 308 can be subsets of frames that permit parallel processing, for example. Thesegments 308 can also be subsets of frames that can separate the video data into separate colors. For example, aframe 306 of color video data can include a luminance plane and two chrominance planes. Thesegments 308 may be sampled at different resolutions. - Whether or not the
frame 306 is divided intosegments 308, theframe 306 may be further subdivided intoblocks 310, which can contain data corresponding to, for example, 16×16 pixels in theframe 306. Theblocks 310 can also be arranged to include data from one ormore segments 308 of pixel data. Theblocks 310 can also be of any other suitable size such as 4×4 pixels, 8×8 pixels, 16×8 pixels, 8×16 pixels, 16×16 pixels, or larger. Unless otherwise noted, the terms block and macroblock are used interchangeably herein. -
FIG. 4 is a block diagram of an example of a videostream processing system 400. The videostream processing system 400 can be implemented in a server of an online video platform, as described above, such as in the transmittingstation 102 or the receivingstation 106 shown inFIG. 1 , such as by providing a computer software program stored in memory, for example, thememory 204 shown inFIG. 2 . The computer software program can include machine instructions that, when executed by a processor such as theprocessor 202 shown inFIG. 2 , cause the server to process aninput video stream 402 in the manner described inFIG. 4 . The videostream processing system 400 can also be implemented as specialized hardware included in, for example, the transmittingstation 102 or the receivingstation 106. - The
input video stream 402 is a video stream, such as thevideo stream 300 shown inFIG. 3 , which includes user generated video content and is received from a device of a video uploader. The videostream processing system 400 includes components for processing theinput video stream 402, including atranscoding stage 404, atranscription stage 406, and aproduct identification stage 408. - The
transcoding stage 404 processes theinput video stream 402 to produce a transcodedvideo 410. The transcodedvideo 410 represents the user generated video content in a processed form. Thetranscoding stage 404 may produce the transcodedvideo 410 by one or more of encoding theinput video stream 402, decoding theinput video stream 402, converting theinput video stream 402 in an encoded form or otherwise from a first format to a second format or otherwise preparing multiple formats of the video, changing a bitrate for later viewing of theinput video stream 402 from a first bitrate to a second bitrate or otherwise preparing multiple bitrate versions of the video, changing a resolution of theinput video stream 402 from first resolution to a second resolution or otherwise preparing multiple resolution versions of the video, a combination thereof, or the like. - The
transcription stage 406 produces atranscript 412 of the video of theinput video stream 402. Thetranscription stage 406 may, for example, use an automatic speech recognition processing aspect to produce thetranscript 412. Thetranscription stage 406 may produce thetranscript 412 based on theinput video stream 402 or the transcodedvideo 410. Where theinput video stream 402 is a pre-recorded video, thetranscript 412 may be produced during playback of the video. Where theinput video stream 402 is a livestreamed video, thetranscript 412 may be produced in real-time or substantially in real-time with the processing thereof. In some implementations, the operations of thetranscription stage 406 may be performed during the transcoding performed at thetranscoding stage 404. - The
product identification stage 408 determines aproduct identification 414 for a product which is the subject of some or all of the video of theinput video stream 402. Theproduct identification 414 is data which identifies a product. For example, theproduct identification 414 may include one or more of a model name, a manufacturer name, a release type, special features such as color or options, or the like for the product. Theproduct identification stage 408 processes video and text associated with theinput video stream 402 to determine theproduct identification 414. In particular, theproduct identification stage 408 may use either theinput video stream 402 or the transcodedvideo 410 for the video aspect and thetranscript 412 for the text aspect. -
FIG. 5 is a block diagram of an example of a product identification stage of a video stream processing system, which may, for example, be theproduct identification stage 408 shown inFIG. 4 . The product identification stage receives asinput video content 500 andtext content 502 and produces or otherwise determines as output aproduct identification 504. Thevideo content 500 may, for example, be theinput video stream 402 or the transcodedvideo 410 shown inFIG. 4 . In particular, thevideo content 500 includes one or more frames of a video sequence of a video hosted or streamed at an online video platform and may thus correspond to frame-level content or chunk-level content of the video. Thetext content 502 includes text associated with thevideo content 500. For example, thetext content 502 may include one or more of a transcript generated for the video, for example, thetranscript 410 shown inFIG. 4 , a title of the video, a description of the video, metadata or other data associated with the video, or a combination thereof. Theproduct identification 504 may, for example, be theproduct identification 414 shown inFIG. 4 . - The product identification stage uses separate processing pipelines to process the inputs. In particular, a
video processing pipeline 506 processes thevideo content 500 to determineembeddings 508, and atext processing pipeline 510 processes thetext content 502 to determineembeddings 512. Theembeddings 508 and theembeddings 512 are logical elements for representing information output respectively by thevideo processing pipeline 506 and thetext processing pipeline 510. In one particular example, theembeddings 508 and theembeddings 512 may be vectors of floating point values, although other data storage types and elements may be used. Theembeddings 508 are used to represent one or more candidate products identified based on the processing at thevideo processing pipeline 506. Theembeddings 512 are used to represent one or more candidate products identified based on the processing at thetext processing pipeline 510. The candidate products represented by theembeddings 508 and the candidate products represented by theembeddings 512 may all be the same, may be partially the same and partially different, or may all be different. - To determine the
embeddings 508, thevideo processing pipeline 506 processes thevideo content 500 at the frame level including by performing object detection using one or more machine learning models. Theembeddings 508 represent pixel information from bounding boxes surrounding the detected objects within a given frame of thevideo content 500. Implementations and examples of thevideo processing pipeline 506 are described below with respect toFIG. 6 . - To determine the
embeddings 512, the text processing pipeline processes thetext content 502 by identifying product candidates based on entity information extracted from thetext content 502. Object detection is then performed using one or more machine learning models against images obtained from searching one or more systems for the product candidates. Theembeddings 512 represent pixel information from bounding boxes surrounding the detected objects within a given image. Implementations and examples of thetext processing pipeline 510 are described below with respect toFIG. 7 . - The
video processing pipeline 506 and thetext processing pipeline 510 thus use one or more machine learning models to respectively determine theembeddings 508 and theembeddings 512. A machine learning model used at thevideo processing pipeline 506 and/or at thetext processing pipeline 510 may be or include one or more of a neural network (e.g., a convolutional neural network, recurrent neural network, or other neural network), decision tree, support vector machine, Bayesian network, genetic algorithm, deep learning system separate from a neural network, or other machine learning model. For example, the object detection performed at thevideo processing pipeline 506 and thetext processing pipeline 510 may use a deep learning convolutional neural network trained for object detection within images and video frames. Thevideo processing pipeline 506 and thetext processing pipeline 510 may use the same or different one or more machine learning models. - The product identification stage includes an embedding
matching component 514 which processes theembeddings 508 and theembeddings 512 to determine theproduct identification 504. The embeddingmatching component 514 first produces an index of the candidate products represented by theembeddings 512. The index may, for example, be a light-weight, on-the-fly index of entries associated with theembeddings 512. An entry within the index may include a key representative of an identifier of the subject candidate product and data associated with the respective embedding 512, such as a floating point value. The index is produced on a per-video basis such that the index is specific to the subject video. This approach emphasizes products which are featured with intentionality within the video and reduces computational load and maintenance burdens on the online video platform system by limiting the pool of indices against which the embedding matching is performed. - The embedding
matching component 514 then performs a nearest neighbor search against the candidate products represented by theembeddings 508 and theembeddings 512. To perform the nearest neighbor search, the embeddingmatching component 514 compares theembeddings 508 against the index to identify entries of the index which include data similar to (e.g., within a threshold variance) or matching data of theembeddings 508. For example, the comparison may be performed between floating point values of theembeddings 508 and floating point values of entries of the index. The embeddingmatching component 514 identifies K nearest neighbors as the K index entries that are most similar to theembeddings 508, in which K is an integer having a value greater than or equal to 1. - The embedding
matching component 514 then evaluates those K nearest neighbors to determine theproduct identification 504. In particular, the embeddingmatching component 514 evaluates the floating point values of the K nearest neighbors to identify the floating point value which has the highest mode amongst them. For example, if K is equal to 5 and 3 of those 5 nearest neighbors share the same floating point value, the candidate product associated with that floating point value (e.g., the candidate product associated with the one or more embeddings 508 having that floating point value) is identified as theproduct identification 504. - In some implementations, the evaluation of the K nearest neighbors is performed to determine a final candidate product for the video frame or chunk represented by the
video content 500. For example, after the final candidate product is determined, a video-level aggregation may be performed against the final candidate products determined at different times during the subject video to determine theproduct identification 504. In some such implementations, performing the video-level aggregation includes determining confidence scores for the final candidate products determined at the frame-level or chunk-level. After some or all of the subject video has been processed, the final candidate product associated with the highest confidence score may be identified as theproduct identification 504. - The particular approach to identifying the
product identification 504 may be based on the type of video from which thevideo content 500 and thetext content 502 derive. For example, where the video is a livestreamed video, theproduct identification 504 may be identified based on the evaluation of the K nearest neighbors for a given frame or chunk. In another example, where the video is a pre-recorded and thus hosted video, theproduct identification 504 may be identified either based on the evaluation of the K nearest neighbors for a given frame or chunk or based on an evaluation of confidence scores determined at various times during the video. - In either case, a timestamp associated with the
product identification 504 representing a time during the subject video in which the subject product is featured may be stored and later used to output theproduct identification 504 in one or more ways. In some implementations, theproduct identification 504 or information associated therewith may be presented to a viewer watching the video. For example, theproduct identification 504 may include an annotation (e.g., including a product name and/or a website link) overlaying one or more frames of the video. The annotation may be located within, adjacent, or nearby to a bounding box within which the subject product is identified, in which the frame or frames corresponding to the bounding box are identified based on a timestamp associated with theproduct identification 504. - In some implementations, the
product identification 504 may be used during a search of videos at an online video platform. For example, when a user searches the online video platform for videos associated with the product identified by theproduct identification 504, the video may be identified in the search results along with the timestamp within the video at which the product is featured. In some implementations, a product name and/or a website link associated with theproduct identification 504 may be presented along with the search results. -
FIG. 6 is a block diagram of an example of a video processing pipeline of a product identification stage, which may, for example, be thevideo processing pipeline 506 shown inFIG. 5 . The video processing pipeline receives asinput video content 600 and produces asoutput embeddings 602, which may, for example, respectively be thevideo content 500 and theembeddings 508 shown inFIG. 5 . The video processing pipeline includes aframe selection component 604 which selects one ormore frames 606 of thevideo content 600 to use to determine theembeddings 602, anobject detection component 608 which performs object detection against theframes 606 to identifyobjects 610, and an embeddingextraction component 612 which processes theobjects 610 to determine theembeddings 602. - The
frame selection component 604 selects theframes 606 in one or more ways. In some implementations, theframe selection component 604 selects theframes 606 by sampling N frames per second in which N is an integer greater than or equal to one. In some such implementations, theframes 606 sampled by theframe selection component 604 are selected based on thoseframes 606 being identified as keyframes. In some implementations, theframe selection component 604 selects theframes 606 based on a threshold difference of video data between ones of theframes 606 and frames preceding them in the video sequence, which may, for example, be measured using motion vectors computed during an encoding process of the subject video. - The particular manner in which the
frame selection component 604 selects theframes 606 may be based on computational resource availability for the online video platform and/or a type of video being processed. For example, where computational resources are abundant, theframe selection component 604 can densely sample N frames per second; however, where computational resources are limited, theframe selection component 604 can instead select amongst keyframes instead. In another example, where the subject video is a livestreamed video, the frame selection process may be more quickly performed using motion vectors and/or other motion detection techniques. - The
object detection component 608 uses a machine learning model to detect theobjects 610 within theframes 606. Theobjects 610 may be people or things appearing within theframes 606. Theobject detection component 608 draws bounding boxes around theobjects 610 responsive to their detection. The bounding boxes are used to represent locations of theobjects 610 within theframes 606. In some implementations, the bounding boxes themselves may be used as theobjects 610. - The embedding
extraction component 612 extracts features of theobjects 610 and uses those features to determine theembeddings 602. The features extracted by the embeddingextraction component 612 include image data derived from the pixels located within the bounding boxes of theobjects 610. The input for this process is the group of pixels corresponding to the bounding box, which is processed to convert those pixel values to another format represented as the embeddings. For example, the embeddingextraction component 612 can output a floating point value for the group of pixels. In some implementations, the embeddingextraction component 612 may use a machine learning model. The machine learning model may be the same as or different from the machine learning model used by theobject detection component 608. -
FIG. 7 is a block diagram of an example of a text processing pipeline of a product identification stage, which may, for example, be thetext processing pipeline 510 shown inFIG. 5 . The text processing pipeline receives asinput text content 700 and produces asoutput embeddings 702, which may, for example, respectively be thetext content 502 and theembeddings 512 shown inFIG. 5 . The text processing pipeline includes anentity extraction component 704 which extracts entities from thetext content 700 to identifyobject candidates 706, asearch component 708 which performs a search based on theobject candidates 706 to identifyimages 710 of theobject candidates 706, anobject detection component 712 which performs object detection against theimages 710 to identifyobjects 714, and an embeddingextraction component 716 which processes theobjects 714 to determine theembeddings 702. - The
entity extraction component 704 extracts entities in the form of data or metadata from thetext content 700. The entities may, for example, be keywords or phrases identified within a transcript of the subject video, the title of the video, a description of the video, or the like. For example, an entity may identify a product name, a manufacturer name, a model type, other information about the product, or the like, or a combination thereof. Theentity extraction component 704 may use a machine learning model trained for language and context detection to extract the entities from thetext content 700. Theentity extraction component 704 determines theobject candidates 706 based on the entities. In some implementations, the machine learning model may be trained for sentiment analysis to evaluate whether a product for which an entity is extracted is featured with intentionality. - In some cases, an entity extracted from the
text content 700 may fully identify aspecific object candidate 706, such as based on product name and model type. However, in some cases, an entity may omit information used to accurately identify anobject candidate 706. For example, the entity may identify a product name without identifying a specific model type. In some implementations, where an entity omits such information, theentity extraction component 704 may determine multiplepotential object candidates 706 based on the entity such as to increase the chances of one of thosepotential object candidates 706 being the correct product featured in the subject video. - The
search component 708 performs a search for one or more images using an internet search engine based on information associated with theobject candidates 706, such as based on the data and/or metadata extracted as the entities used to determine thoseobject candidates 706. Thesearch component 708 performs the search or searches to identify theimages 710, which are static images depicting one or more objects including a given one of theobject candidates 706. Where multiple objects are depicted in animage 710, the main object which is the focus of theimage 710 is considered to represent theobject candidate 706. The internet search engine used for the search may be external to the online video platform. For example, the internet search engine may be considered to be external to the online video platform where uniform resource locators (URLs) of the internet search engine and the online video platform have different domain names or different subdomain names. - The
object detection component 712 and the embeddingextraction component 716 may operate in the same or in a substantially similar manner to theobject detection component 608 and the embeddingextraction component 612 shown inFIG. 6 . In particular, theobject detection component 712 uses a machine learning model to detect theobjects 714 within theimages 710 including by drawing bounding boxes around thoseobjects 714 within theimages 610, and the embeddingextraction component 716 extracts features of theobjects 714 and uses those features to determine theembeddings 702 such as by converting pixel information representative of those features into floating point values. - Further details of techniques for automated product identification within hosted and streamed videos are now described.
FIG. 8 is a flowchart diagram of an example of atechnique 800 for automated product identification within a video.FIG. 9 is a flowchart diagram of an example of atechnique 900 for determining embeddings from video content associated with a video.FIG. 10 is a flowchart diagram of an example of atechnique 1000 for determining embeddings from video content associated with a video. - The
technique 800, thetechnique 900, and/or thetechnique 1000 can be implemented, for example, as a software program that may be executed by computing devices such as the transmittingstation 102 or the receivingstation 106. For example, the software program can include machine-readable instructions that may be stored in a memory such as thememory 204 or thesecondary storage 214, and that, when executed by a processor, such as theprocessor 202, may cause the computing device to perform thetechnique 800, thetechnique 900, and/or thetechnique 1000. Thetechnique 800, thetechnique 900, and/or thetechnique 1000 can be implemented using specialized hardware or firmware. For example, a hardware component configured to perform thetechnique 800, thetechnique 900, and/or thetechnique 1000. As explained above, some computing devices may have multiple memories or processors, and the operations described in thetechnique 800, thetechnique 900, and/or thetechnique 1000 can be distributed using multiple processors, memories, or both. - For simplicity of explanation, the
technique 800, thetechnique 900, and thetechnique 1000 are each depicted and described herein as a series of steps or operations. However, the steps or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter. - Referring first to
FIG. 8 , thetechnique 800 for automated product identification within a video is shown. At 802, a video is received. The video may, for example, be received at an online video platform. The video may be a pre-recorded video which is uploaded to the online video platform for hosting such as through a website or application associated with the online video platform. Alternatively, the video may be a livestreamed video which is uploaded in real-time to the online video platform over a real-time streaming protocol connecting a device at which the video is being recorded to the online video platform. - At 804, first embeddings are determined based on video content of the video. The first embeddings represent product candidates identified based on the one or more frames selected from the video. Determining the first embeddings includes detecting a first object within one or more frames selected from the video and determining first embeddings based on pixel information associated with the first object. Further implementations and examples of techniques for determining the first embeddings are described below with respect to
FIG. 9 . - At 806, second embeddings are determined based on text content associated with the video. The second embeddings represent product candidates identified based on the entities extracted from the text content. Determining the second embeddings includes determining one or more object candidates based on entities extracted from text content associated with the video, detecting a second object within one or more images identified responsive to a search based on the one or more object candidates, and determining second embeddings based pixel information associated with the second object. Further implementations and examples of techniques for determining the first embeddings are described below with respect to
FIG. 10 . - At 808, a product identification representative of a product featured in the video is determined. Determining the product identification includes producing a product candidate index based on the second embeddings, determining a number of nearest neighbors based on a comparison of the first embeddings against entries of the product candidate index, and determining a product identification representative of a product featured in the video based on the number of nearest neighbors. In particular, floating point values determined based on the video content (e.g., the first embeddings) are compared against the entries within the product candidate index (e.g., floating point values determined based on the text content (e.g., the second embeddings)) to determine the number of nearest neighbors. The floating point value with the highest mode amongst the nearest neighbors is identified, and the candidate product associated therewith is determined as the product featured in the video. The product identification is thus determined based on that candidate product. In this way, the determination of a product identification representative of a product featured in the video may be made based on a comparison of the first embeddings against the entries of the product candidate index.
- At 810, an indication of the product identification is output. For example, the indication of the product identification may be output at the online video platform at which the video was received. On one example, outputting the indication of the product identification at the online video platform may include outputting an annotation including the indication of the product identification proximate to a bounding box associated with an object detected within one or more frames of the video during a playback of the video at the online video platform. In another example, the indication of the product identification is a representation of the video output at the online video platform responsive to a user search at the online video platform for videos featuring the product.
- In some implementations, the product identification determined based on the comparison of the first embeddings against the entries of the product candidate index is not the final product identification for which an indication is output. For example, the product identification may be one of multiple product identifications determined for the video. In some such implementations, a confidence score may be determined for each of the product identifications, in which each of the product identifications corresponds to one of a variety of times during the video. The final product identification may then be determined based on an evaluation of the various confidence scores. Thus, the indication output may be an indication of the final product identification.
- Referring next to
FIG. 9 , thetechnique 900 for determining embeddings from video content associated with a video is shown. At 902, one or more frames of the video are selected. In one example, the one or more frames may be selected according to a sampling rate determined for the video. For example, the sampling rate may be based on computational resource availability, a defined frame or time interval for sampling, a frame type (e.g., keyframe), or the like. In another example, the one or more frames may be selected based on motion information determined, such as motion vectors computed between a current frame and a previously encoded frame of the video sequence of the video. - At 904, an object is detected within a bounding box in the one or more frames. Detecting the object within the bounding box includes perform object detection against the one or more frames to detect an object within a bounding box in the one or more frames. The object detection may, for example, be performed using a machine learning model trained for object detection. The bounding box is a generally rectangular area enclosing some or all of the detected object within the one or more frames. The object may be in one or more frames by virtue of the object being present in one or more frames of the video, regardless of motion, translation, or warping of the object between frames.
- At 906, pixel information is extracted from the bounding box. The pixel information refers to RGB values, YUV values, other color values, other luminance and/or chrominance values, and/or other values enclosed within the one or more frames by the bounding box. Extracting the pixel information can include copying the values from within the bounding box to a separate source, such as a buffer, data store, or the like.
- At 908, an embedding is determined based on the pixel information. Determining the embedding can include using the pixel information of the bounding box, such as which has been extracted from the bounding box, to compute a floating point value vector for the pixel information as the embedding. The embedding is one of multiple embeddings which may be determined for the purpose of identifying a given product featured within the video.
- Referring finally to
FIG. 10 , thetechnique 1000 for determining embeddings from video content associated with a video is shown. At 1002, entities are extracted from text content associated with the video. The entities are extracted using a text parsing software aspect configured to read text in one or more languages. The text may be a title of the video, a description of the video, a transcript produced for the video, or the like. The entities are extracted to a buffer, data store, or the like for further processing. - At 1004, one or more object candidates are determined based on the entities. In particular, the entities are processed to identify one or more of a product name, a manufacturer name, a model type, and/or other information usable to identify an object candidate which may be a particular product featured in the video. In some cases, the entities are processed to identify multiple object candidates, for example, where there are multiple potential products determined based on the entities. In some cases, the entities may partially represent a potential product such as based on referencing a product name without a model type or a manufacturer name without a product name. In such a case, additional entities may be evaluated to cover multiple possible products which could be extrapolated from the partial representations by the entities.
- At 1006, a search is performed based on the object candidates to identify one or more images. The search is performed using an internet search engine external to the online video platform that receives the video. Performing the search includes entering the object candidate information determined based on the entities into a search string for processing by the internet search engine. The internet search engine may perform a web search for websites or an image search for images. The search is ultimately performed to identify images and so results of the search are parsed for images. Those images are retrieved from the various sources and stored in a buffer, data store, or the like for further processing.
- At 1008, an object is detected within a bounding box in the one or more images. Detecting the object within the bounding box includes perform object detection against the one or more images to detect an object within a bounding box in the one or images frames. The object detection may, for example, be performed using a machine learning model trained for object detection. The bounding box is a generally rectangular area enclosing some or all of the detected object within the one or more images. The object may be in one or more images by virtue of the object being separately included in each of those one or more images.
- At 1010, pixel information is extracted from the bounding box. The pixel information refers to RGB values, YUV values, other color values, other luminance and/or chrominance values, and/or other values enclosed within the one or more images by the bounding box. Extracting the pixel information can include copying the values from within the bounding box to a separate source, such as a buffer, data store, or the like.
- At 1012, an embedding is determined based on the pixel information. Determining the embedding can include using the pixel information of the bounding box, such as which has been extracted from the bounding box, to compute a floating point value vector for the pixel information as the embedding. The embedding is one of multiple embeddings which may be determined for the purpose of identifying a given product featured within the video.
- The word “example” is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” is not necessarily to be construed as being preferred or advantageous over other aspects or designs. Rather, use of the word “example” is intended to present concepts in a concrete fashion. As used in this application, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clearly indicated otherwise by the context, the statement “X includes A or B” is intended to mean any of the natural inclusive permutations thereof. That is, if X includes A; X includes B; or X includes both A and B, then “X includes A or B” is satisfied under any of the foregoing instances. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more,” unless specified otherwise or clearly indicated by the context to be directed to a singular form. Moreover, use of the term “an implementation” or the term “one implementation” throughout this disclosure is not intended to mean the same implementation unless described as such.
- Implementations of the transmitting
station 102 and/or the receiving station 106 (and the algorithms, methods, instructions, etc., stored thereon and/or executed thereby) can be realized in hardware, software, or any combination thereof. The hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit. In the claims, the term “processor” should be understood as encompassing any of the foregoing hardware, either singly or in combination. The terms “signal” and “data” are used interchangeably. Further, portions of the transmittingstation 102 and the receivingstation 106 do not necessarily have to be implemented in the same manner. - Further, in one aspect, for example, the transmitting
station 102 or the receivingstation 106 can be implemented using a general purpose computer or general purpose processor with a computer program that, when executed, carries out any of the respective methods, algorithms, and/or instructions described herein. In addition, or alternatively, for example, a special purpose computer/processor can be utilized which can contain other hardware for carrying out any of the methods, algorithms, or instructions described herein. - Further, all or a portion of implementations of this disclosure can take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium. A computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available.
- The above-described implementations and other aspects have been described in order to facilitate easy understanding of this disclosure and do not limit this disclosure. On the contrary, this disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation as is permitted under the law so as to encompass all such modifications and equivalent arrangements.
Claims (20)
1. A method, comprising:
receiving a video at an online video platform;
detecting a first object within one or more frames selected from the video;
determining first embeddings based on pixel information associated with the first object;
determining one or more object candidates based on entities extracted from text content associated with the video;
detecting a second object within one or more images identified responsive to a search based on the one or more object candidates;
determining second embeddings based pixel information associated with the second object;
producing a product candidate index based on the second embeddings;
determining a number of nearest neighbors based on a comparison of the first embeddings against entries of the product candidate index;
determining a product identification representative of a product featured in the video based on the number of nearest neighbors; and
outputting an indication of the product identification at the online video platform.
2. The method of claim 1 , wherein the first embeddings represent product candidates identified based on the one or more frames selected from the video and the second embeddings represent product candidates identified based on the entities extracted from the text content.
3. The method of claim 1 , further comprising:
determining a confidence score for the product identification; and
determining a final product identification based on an evaluation of confidence scores for product identifications corresponding to various times during the video,
wherein the indication output at the online video platform is an indication of the final product identification.
4. The method of claim 1 , wherein detecting the first object within the one or more frames comprises identifying a bounding box for the first object within the one or more frames, and wherein outputting the indication of the product identification at the online video platform comprises outputting an annotation including the indication of the product identification proximate to the bounding box during a playback of the video at the online video platform.
5. The method of claim 1 , wherein the indication of the product identification is a representation of the video output at the online video platform responsive to a user search at the online video platform for videos featuring the product.
6. An apparatus, comprising:
a memory; and
a processor configured to execute instructions stored in the memory to:
receive a video at an online video platform;
determine first embeddings representative of one or more first candidate products based on video content of the video;
determine second embeddings representative of one or more second candidate products based on text content associated with the video;
produce a product candidate index based on the second embeddings;
determine a product identification representative of a product featured in the video based on a comparison of the first embeddings against entries of the product candidate index; and
output an indication of the product identification at the online video platform.
7. The apparatus of claim 6 , wherein, to determine the first embeddings, the processor is configured to execute the instructions to:
select one or more frames of the video;
perform object detection against the one or more frames to detect a first object within a bounding box in the one or more frames; and
determine an embedding of the first embeddings based on pixel information of the bounding box.
8. The apparatus of claim 7 , the one or more frames are selected according to a sampling rate determined for the video.
9. The apparatus of claim 7 , wherein, to output the indication of the product identification, the processor is configured to execute the instructions to:
output an annotation including the indication of the product identification proximate to the bounding box during a playback of the video at the online video platform.
10. The apparatus of claim 6 , wherein, to determine the second embeddings, the processor is configured to execute the instructions to:
extract one or more entities from the text content;
determine one or more object candidates based on the one or more entities;
perform a search based on the one or more object candidates to identify one or more images;
perform object detection against the one or more images to detect a second object within a bounding box in the one or more images; and
determine an embedding of the second embeddings based on pixel information of the bounding box.
11. The apparatus of claim 10 , wherein the one or more entities include data or metadata corresponding to one or more of a product name, a manufacturer name, or a model type.
12. The apparatus of claim 10 , wherein the text content includes a transcript of the video and the processor is further configured to execute the instructions to produce the transcript.
13. The apparatus of claim 6 , wherein, to determine the product identification, the processor is configured to execute the instructions to:
determine a number of nearest neighbors based on a comparison of the first embeddings against entries of the product candidate index; and
determine the product identification based on a highest mode of the number of nearest neighbors.
14. The apparatus of claim 6 , wherein the indication of the product identification is a representation of the video output at the online video platform responsive to a user search at the online video platform for videos featuring the product.
15. A non-transitory computer readable storage device including program instructions that, when executed by a processor, cause the processor to perform operations, the operations comprising:
determining first embeddings representing first candidate products based on video content of a video;
determining second embeddings representing second candidate products based on text content associated with the video;
determining a product identification representative of a product featured in the video based on a comparison of the first embeddings and the second embeddings; and
outputting an indication of the product identification.
16. The non-transitory computer readable storage device of claim 15 , wherein the operations for determining the first embeddings comprise:
selecting one or more frames of the video;
performing object detection against the one or more frames to detect a first object within a bounding box in the one or more frames; and
determining an embedding of the first embeddings based on pixel information of the bounding box.
17. The non-transitory computer readable storage device of claim 16 , wherein the video is received at an online video platform, and wherein the operations for outputting the indication of the product identification comprise:
outputting an annotation including the indication of the product identification proximate to the bounding box during a playback of the video at the online video platform.
18. The non-transitory computer readable storage device of claim 15 , wherein the operations for determining the second embeddings comprise:
extracting one or more entities from the text content;
determining one or more object candidates based on the one or more entities;
performing a search based on the one or more object candidates to identify one or more images;
performing object detection against the one or more images to detect a second object within a bounding box in the one or more images; and
determining an embedding of the second embeddings based on pixel information of the bounding box.
19. The non-transitory computer readable storage device of claim 18 , wherein the video is received at an online video platform, and wherein the search is performed using an internet search engine external to the online video platform.
20. The non-transitory computer readable storage device of claim 15 , the operations further comprising:
determining a confidence score for the product identification; and
determining a final product identification based on an evaluation of confidence scores for product identifications corresponding to various times during the video,
wherein the indication output is an indication of the final product identification.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/334,923 US20220382808A1 (en) | 2021-05-31 | 2021-05-31 | Automated product identification within hosted and streamed videos |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/334,923 US20220382808A1 (en) | 2021-05-31 | 2021-05-31 | Automated product identification within hosted and streamed videos |
Publications (1)
Publication Number | Publication Date |
---|---|
US20220382808A1 true US20220382808A1 (en) | 2022-12-01 |
Family
ID=84193994
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/334,923 Pending US20220382808A1 (en) | 2021-05-31 | 2021-05-31 | Automated product identification within hosted and streamed videos |
Country Status (1)
Country | Link |
---|---|
US (1) | US20220382808A1 (en) |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130174195A1 (en) * | 2012-01-04 | 2013-07-04 | Google Inc. | Systems and methods of image searching |
US20150341410A1 (en) * | 2014-05-21 | 2015-11-26 | Audible Magic Corporation | Media stream cue point creation with automated content recognition |
US20170195629A1 (en) * | 2016-01-06 | 2017-07-06 | Orcam Technologies Ltd. | Collaboration facilitator for wearable devices |
US20190141410A1 (en) * | 2017-11-08 | 2019-05-09 | Facebook, Inc. | Systems and methods for automatically inserting advertisements into live stream videos |
US20190180108A1 (en) * | 2017-12-12 | 2019-06-13 | International Business Machines Corporation | Recognition and valuation of products within video content |
US20200065607A1 (en) * | 2016-06-23 | 2020-02-27 | Capital One Services, Llc | Systems and methods for automated object recognition |
US20210090449A1 (en) * | 2019-09-23 | 2021-03-25 | Revealit Corporation | Computer-implemented Interfaces for Identifying and Revealing Selected Objects from Video |
US20210321166A1 (en) * | 2018-07-19 | 2021-10-14 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
US20210383579A1 (en) * | 2018-10-30 | 2021-12-09 | Pak Kit Lam | Systems and methods for enhancing live audience experience on electronic device |
US20220318555A1 (en) * | 2021-03-31 | 2022-10-06 | International Business Machines Corporation | Action recognition using limited data |
-
2021
- 2021-05-31 US US17/334,923 patent/US20220382808A1/en active Pending
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130174195A1 (en) * | 2012-01-04 | 2013-07-04 | Google Inc. | Systems and methods of image searching |
US20150341410A1 (en) * | 2014-05-21 | 2015-11-26 | Audible Magic Corporation | Media stream cue point creation with automated content recognition |
US20170195629A1 (en) * | 2016-01-06 | 2017-07-06 | Orcam Technologies Ltd. | Collaboration facilitator for wearable devices |
US20200065607A1 (en) * | 2016-06-23 | 2020-02-27 | Capital One Services, Llc | Systems and methods for automated object recognition |
US20190141410A1 (en) * | 2017-11-08 | 2019-05-09 | Facebook, Inc. | Systems and methods for automatically inserting advertisements into live stream videos |
US20190180108A1 (en) * | 2017-12-12 | 2019-06-13 | International Business Machines Corporation | Recognition and valuation of products within video content |
US20210321166A1 (en) * | 2018-07-19 | 2021-10-14 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
US20210383579A1 (en) * | 2018-10-30 | 2021-12-09 | Pak Kit Lam | Systems and methods for enhancing live audience experience on electronic device |
US20210090449A1 (en) * | 2019-09-23 | 2021-03-25 | Revealit Corporation | Computer-implemented Interfaces for Identifying and Revealing Selected Objects from Video |
US20220318555A1 (en) * | 2021-03-31 | 2022-10-06 | International Business Machines Corporation | Action recognition using limited data |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8515933B2 (en) | Video search method, video search system, and method thereof for establishing video database | |
Choi et al. | Unsupervised and semi-supervised domain adaptation for action recognition from drones | |
TWI443535B (en) | Video search method, system, and method for establishing a database therefor | |
CN112929744B (en) | Method, apparatus, device, medium and program product for segmenting video clips | |
KR102343766B1 (en) | Text detection in video | |
CN109218629B (en) | Video generation method, storage medium and device | |
US9852364B2 (en) | Face track recognition with multi-sample multi-view weighting | |
US9600483B2 (en) | Categorization of digital media based on media characteristics | |
JP2022020647A (en) | Video processing method, apparatus, electronic device, storage medium, and program | |
CN112163122A (en) | Method and device for determining label of target video, computing equipment and storage medium | |
JP2018018504A (en) | Recommendation generation method, program, and server device | |
CN102884538A (en) | Enriching online videos by content detection, searching, and information aggregation | |
US10319095B2 (en) | Method, an apparatus and a computer program product for video object segmentation | |
CN115994230A (en) | Intelligent archive construction method integrating artificial intelligence and knowledge graph technology | |
CN110287375B (en) | Method and device for determining video tag and server | |
CN109583389B (en) | Drawing recognition method and device | |
CN113010703A (en) | Information recommendation method and device, electronic equipment and storage medium | |
US20190068987A1 (en) | Systems and Methods for Embedding Metadata into Video Contents | |
CN111836118A (en) | Video processing method, device, server and storage medium | |
CN113987274A (en) | Video semantic representation method and device, electronic equipment and storage medium | |
CN108229285B (en) | Object classification method, object classifier training method and device and electronic equipment | |
CN113642536B (en) | Data processing method, computer device and readable storage medium | |
CN114022668A (en) | Method, device, equipment and medium for aligning text with voice | |
US20220382808A1 (en) | Automated product identification within hosted and streamed videos | |
CN115937742B (en) | Video scene segmentation and visual task processing methods, devices, equipment and media |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LI, YUANZHEN;REEL/FRAME:056397/0335Effective date: 20210525 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |