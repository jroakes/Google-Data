US8706733B1 - Automated objective-based feature improvement - Google Patents
Automated objective-based feature improvement Download PDFInfo
- Publication number
- US8706733B1 US8706733B1 US13/560,874 US201213560874A US8706733B1 US 8706733 B1 US8706733 B1 US 8706733B1 US 201213560874 A US201213560874 A US 201213560874A US 8706733 B1 US8706733 B1 US 8706733B1
- Authority
- US
- United States
- Prior art keywords
- alternative
- experiment
- feature
- user
- features
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/10—Office automation; Time management
Definitions
- experiments are configured to determine the best features and feature characteristics to provide to users of the computer implemented entity. These experiments involve a series of alternative features for each of one or more features of the computer-implemented entity. The alternative features are presented to users and the behavior of the users with respect to the alternative features is received and analyzed to determine those alternative(s) that maximize the usability and productivity of the computer-implemented entity.
- experiment process is performed manually. This manual process requires relying on human intuition when grouping the users and selecting the alternative features for testing. Furthermore, because the alternative features are selected and analyzed manually, it becomes increasingly difficult to select and analyze experiments with alternative sets for testing joint features (e.g., adjusting content distribution and UI elements provided to a user). Additionally, it is difficult to adjust the experiment elements during the experiment as the selection of alternative features may only be adjusted manually, after the user has viewed the results of the interaction of user groups with respect to their alternative sets.
- the disclosed subject matter relates to a computer-implemented method for facilitating automatic objective-based feature improvement, the method comprising receiving, using one or more computing devices, a request to perform an experiment with respect to a computer-implemented entity to identify an optimal alternative for each of one or more features of the computer-implemented entity.
- the method further comprising determining, using the one or more computing devices, an alternative feature range with respect to each of the one or more features, the alternative feature range defining a range of possible alternative features available with respect to the feature.
- the method further comprising selecting, using the one or more features, one or more alternative features for each of the one or more features, each alternative feature being selected from the alternative feature range of the feature.
- the method further comprising generating, using the one or more computing devices, a plurality of alternative sets, each alternative set including an alternative feature for at least one of the one or more features.
- the method further comprising selecting, using the one or more computing devices, a plurality of user groups from a pool of users and assigning, using the one or more computing devices, each user group of the plurality of users groups to one of the plurality of alternative sets, wherein the assigning is based on user characteristics of each user and historical information regarding the interaction of that user with the computer-implemented entity.
- the disclosed subject matter also relates to a system for facilitating automatic objective-based feature improvement, the system comprising one or more processors and a machine-readable medium comprising instructions stored therein, which when executed by the processors, cause the processors to perform operations comprising receiving an indication of a request to perform an experiment with respect a computer-implemented entity to identify an optimal alternative for each of one or more features of the computer-implemented entity, the optimal alternative comprising an alternative for each feature that provides the highest likelihood of resulting in an objective associated with the experiment.
- the operations further comprising determining, using the one or more computing devices, an alternative feature range with respect to each of the one or more features, the alternative feature range defining a range of possible alternative features available with respect to the feature.
- the operations further comprising selecting, using the one or more computing devices, a plurality of alternative features for each of the one or more features from the alternative feature range associated with the feature and generating a plurality of alternative sets, each alternative set including at least one alternative of the plurality of alternative features for at least one of the one or more features.
- the operations further comprising identifying a pool of users available to participate in the experiment and generating a plurality of user groups by grouping the users of the identified pool of users, the plurality of user groups being generated based on the characteristics of each user of identified pool of users, wherein the number of user groups is equal to the number of alternative sets.
- the operations further comprising assigning, using the one or more computing devices, each user group of the plurality of users groups to one of the plurality of alternative sets, wherein the assigning is based on user characteristics of users of each user group.
- the disclosed subject matter also relates to a machine-readable medium comprising instructions stored therein, which when executed by a machine, cause the machine to perform operations comprising receiving an indication of a request to perform an experiment with respect a computer-implemented entity to identify an optimal alternative for each of one or more features of the computer-implemented entity, the optimal alternative being an alternative having the highest correlation with an objective identified with respect to the computer-implemented entity.
- the operations further comprising automatically generating an experiment to determine the optimal alternative for each of the one or more features, the generating comprising determining, using the one or more computing devices, an alternative feature range with respect to each of the one or more features, the alternative feature range defining a range of possible alternative features available with respect to the feature.
- the generating further comprising selecting, using the one or more computing devices, a plurality of alternative features for each of the one or more features from the alternative feature range associated with the feature and generating a plurality of alternative sets, each alternative set including at least one alternative of the one or more alternative features for at least one of the one or more features.
- the generating further comprising identifying a pool of users available to participate in the experiment.
- the generating further comprising generating a plurality of user groups by grouping the users of the identified pool of users, the user groups being generated based on the characteristics of the users of identified pool of users, wherein the number of user groups is equal to the number of alternative sets and assigning, using the one or more computing devices, each user group of the plurality of users groups to one of the plurality of alternative sets, wherein the assigning is based on user characteristics of the users of each user group.
- the operations further comprising receiving an indication of user activity for each of the users of the plurality user groups, for the duration of the experiment and providing feedback regarding the optimal alternative for each of the one or more features when it is determined that the experiment has terminated based on analyzing the received user activity.
- FIG. 1 illustrates an example block diagram of a system which provides for facilitating automated objective-based feature improvement.
- FIG. 2 illustrates an example process for generating an experiment.
- FIG. 3 illustrates a flow chart of an example process for automatically conducting a feature-based improvement experiment.
- FIG. 4 illustrates an example client-server network environment which provides for implementing an automated objective-based feature improvement system and process.
- FIG. 5 conceptually illustrates an electronic system with which some implementations of the subject technology are implemented.
- the subject disclosure facilitates automating the process of objective-based feature improvement.
- the automated system receives a specific experiment objective (e.g., a result with respect to each of one or more features being tested being tested as part of the experiment) and an alternative feature range for each feature being tested.
- Features of the computer implemented entity may include performance based features such as content variations presented to the user in search results, recommendations and suggestions provided to users (e.g., for content, friends), user interface based features such as variations in graphical user interface elements displayed to users, or other features which improve user satisfaction and further maximize the effectiveness of the content (e.g., click through rate, number of users returning to a website, or users taking other intended actions).
- Each feature being tested provides a variation on the user experience, such that the system reconfigures the user experience with respect to the computer-entity according to the feature variation.
- the objective identifies one or more goals with respect to the performance of the computer-implemented entity and/or one or more features of the computer-implemented entity.
- Each goal represents an objective (e.g., click through rate, time spent on the program, or other similar user activity with respect to the computer-implemented entity) which is desired to be met based on reconfigurations of the user experience with respect to the computer-implemented entity.
- the alternative feature range for each feature defines a space including a set of alternative values or variations for each feature (e.g., a distribution breakdown range, a listing of UI features or similar range of alternative features available with respect to a feature).
- the experiment may also be assigned a priority (e.g., an indication that an objective is important and thus should be prioritized over other experiments regarding other objectives).
- the priority may be assigned manually by the engineer conducting the experiment, similar to the objective and alternative feature ranges.
- the system may determine a priority for the experiment according to historical information regarding the value of each objective in overall maximization of user experience, maximization of profits and/or overall performance of the computer-implemented entity.
- the system automatically generates an experiment according to the objective, features to be tested, the alternative feature range for each feature and/or the priority for each experiment.
- the system selects a number of alternative features from the identified alternative feature range for each feature being tested.
- the selection of the alternative features is based upon the number of users available for the experiment, the number of alternative features likely to provide enough comparison and generate enough data for a determination, and/or prior knowledge of information relating to the experiment, the objectives, the alternative feature range an/or the specific feature being tested.
- the system may have access to information (e.g., data and/or results from prior studies and/or performance logs) regarding each feature being tested and may determine the optimal delineation and number of alternative features within the alternative feature range based on such information.
- the system Once the system has selected alternative features for each feature being tested, the system generates alternative sets, each alternative set including at least one alternative for each feature being tested.
- the alternative sets include the possible combinations for testing all alternative features for the one or more features. In one example, all possible combinations may be tested, while in other embodiments, only certain combinations are tested.
- the system also identifies a pool of users available for the specific experiment.
- the users available for the experiment may vary based on the priority assigned to the experiment.
- the identified pool of users may be grouped to create user groups, based on the number of the alternative sets. In one example, the grouping of the users is based upon an algorithm that minimizes any bias across the groupings and creates sample groups of users that have same or similar characteristics and behavior at least with respect to the feature(s) included as part of the objective.
- the user groups may be determined based on user characteristics of the users (e.g., historically observed user characteristics stored within a user profile). Each user group is assigned to a specific alternative set.
- User groups may be assigned to an alternative set based on user characteristics of the users within the user group (e.g., historically observed user characteristics stored within a user profile). For example, a user group may be assigned to an alternative set based on a determination of a projection of how well the users within the group are likely to respond to the alternative set (e.g., based on various user characteristics and/or historical information). The assignment of each user to a specific alternative set causes the experience of the user with respect to the computer-implemented entity to be reconfigured according to the specific features of the set.
- the experiment is initiated.
- the system receives indication of user activity of the users within each user group with respect to their respective alternative set. That is, the user interacts with the computer-implemented entity having been reconfigured according to the feature alternatives assigned to the user, and the user interaction with respect to the reconfigured experience of the computer-implemented entity is received by the system.
- the user activity is analyzed by the system to determine an improvement score for each alternative set and/or one or more alternative features associated with one or more of the features being tested.
- the improvement score may be calculated based on the effect of the alternative features and/or alternative sets on the user's interaction with the computer-implemented entity (e.g., product or service), and may be further based on the correlation of the user behavior with the desired objectives of the experiment.
- the improvement score is an indication of the correlation of the alternative set to achieving an objective of the experiment.
- the improvement score may be a score given to each alternative set according to the extent to which the user activity with respect to the alternative entity contributes to the objective of the experiment.
- the objective of the experiment may include one or more results (e.g., click-through rate, user engagement, duration of user engagement with the service or product, user rating, user satisfaction, the number of user visits, etc.) where each result has a corresponding value, and where the improvement score is calculated based on the results achieved in view of the alternative set being applied to the computer-implemented entity.
- the calculated improvement score is used to provide results regarding the experiment.
- the calculated improvement score for the various alternative features and/or alternative sets may be used to dynamically adjust the alternative sets, groupings of users and/or the assignment of user groups to alternative sets while the experiment is ongoing.
- the system may determine that with respect to one or more features from the initial set of alternative features and/or alternative sets, a number of the alternative features and/or alternative sets are the most likely to be selected as the optimal alternative.
- the system may then generate adjusted alternative sets, user groupings and/or assignment of user groups to alternative sets in response to determining that adjustments are necessary and/or desirable to improve the experiment.
- the system may expand one or more alternative sets, group one or more alternative sets, removing one or more alternative sets and/or adding one or more additional alternative sets. Similar adjustments may be made with respect to only a single feature.
- the system may also generate adjusted user groups including varying the grouping of users, number of users within each user group or expanding or collapsing one or more users group. In one example the system may adjust the user groups such that the number of user groups after the adjustment is equal to the number of alternative sets once adjustment has occurred.
- the assignment of users to groups may be adjusted such that users are assigned to groups and/or alternative sets based on a prediction of how each user and/or user group is likely to respond to the adjusted alternative set(s).
- the determination of whether an adjustment should be made may be performed periodically or may be triggered as a result of one or more conditions being met.
- the criteria for determining whether adjustments should be made may include a pre-defined degree of separation between the improvement scores of the alternative features for each features and/or among the alternative sets, a certain confidence or improvement score of one or more alternative sets and/or other similar indications derived from the user activity indicating that an adjustment may be beneficial in reaching a decision as to the optimal alternative for each feature.
- the adjustment process may occur one or more times during the experiment.
- the system continues to collect information regarding user behavior with respect to the variations (alternative features) for the one or more features until it determines that the experiment has reached its point of termination, where all data collected during the experiment is provided for analysis.
- the system may periodically perform a check to determine whether the point of termination has been reached.
- the system may determine that the system has reached its point of termination based on one or more termination criteria (e.g., conditions that when met indicate that the experiment should be terminated).
- the termination criteria may include a pre-defined degree of separation between the improvement scores of the alternative features for each feature and/or among the alternative sets, a certain confidence or improvement score of one or more alternative sets and/or other similar indications derived from the user activity indicating that the experiment should be terminated.
- the determination as to whether the experiment should be made may be based on system or user-indicated conditions such as a certain time period having lapsed since the start of the experiment, a pre-defined time or point of termination, a limit on the resources (e.g., money, time, users) allocated to the experiment and/or whether resources allocated to the termination are needed for other experiments (e.g., those with higher priority).
- system or user-indicated conditions such as a certain time period having lapsed since the start of the experiment, a pre-defined time or point of termination, a limit on the resources (e.g., money, time, users) allocated to the experiment and/or whether resources allocated to the termination are needed for other experiments (e.g., those with higher priority).
- the system analyzes all received activity from users with respect to the features of the computer-implemented entity and analyzes the data to provide a determination.
- the results of the experiment may include a decision as to the optimal alternative (e.g., for each feature or for all features being tested collectively), an indication to the user that there is no actual preferred or optimal alternative (e.g., for each feature or for all features being tested collectively), and/or a determination that while it is likely that an optimal alternative exists, further experimentation is necessary to reach a definitive decision.
- the criteria for generating the results and/or identifying the optimal alternative features may be based on various criteria or conditions (e.g., system or user-indicated conditions or thresholds) and may include a required difference between the improvement score of the alternative(s) being tested and the current alternative in use, an error threshold of the determination (e.g., of the improvement score(s) or difference between those improvement scores), and/or a degree of confidence in the determination.
- various criteria or conditions e.g., system or user-indicated conditions or thresholds
- an error threshold of the determination e.g., of the improvement score(s) or difference between those improvement scores
- FIG. 1 illustrates an example block diagram of a system which provides for facilitating automated objective-based feature improvement.
- the system includes an input mechanism 101 communicably coupled to an experiment generation system 102 including an alternative feature selector 103 , a user group selector 104 , and an experiment generator 105 .
- the experiment generation system is further coupled to a data collector and analyzer 106 .
- the system 100 further includes an adjustment generator 107 and a feedback generator 108 .
- the input mechanism 101 receives a set of inputs from a user (e.g., engineer) requesting to perform an automated objective-based feature improvement experiment with respect to one or more features of a computer-implemented entity.
- a user e.g., engineer
- the input received at the input mechanism 101 may include, for example, a specific experiment objective (e.g., a result with respect to each of one or more features being tested being tested as part of the experiment), an alternative feature range for each feature being tested and/or a priority for the experiment.
- a specific experiment objective e.g., a result with respect to each of one or more features being tested being tested as part of the experiment
- an alternative feature range for each feature being tested e.g., a priority for the experiment.
- the input data received at the input mechanism 101 are forwarded to the experiment generation system 102 .
- the experiment generation system 102 generates an experiment based on the received data and conducts the experiment.
- the experiment generation system 102 includes an alternative feature selector 103 , a user group selector 104 and an experiment generator 105 .
- the alternative feature selector 103 is configured to select a number of alternative features for each feature being tested and generate one or more alternative sets from the selected alternative features.
- the user group selector 104 is configured to identify a pool of users available for the experiment and generate user groups from the pool of users (e.g., wherein the number of user groups generated by the user group selector 104 is dependent upon the number of alternative sets generated by the alternative feature selector 103 ).
- the experiment generator 105 is configured to receive the alternative sets generated by the alternative feature selector 103 and the user groups generated by the user group selector 104 and generate an experiment by assigning each user group to a specific alternative set and initialize the experiment.
- the data collector and analyzer 106 is configured to collect and store activity data relating to received indication of user activity of each user participating in the experiment with respect to each alternative set.
- the data collector and analyzer 106 is further configured to analyze the collected user activity to generate feedback regarding the experiment.
- the received activity may be used during the duration of the experiment to determine an improvement score for each alternative set and/or one or more alternative features associated with one or more of the features being tested.
- the calculated improvement score may be used to determine whether the experiment should be adjusted.
- the adjustment generator 107 is configured to dynamically adjust the experiment, including one or more of the alternative sets, groupings of users and/or the assignment of user groups to alternative sets, based on information and/or data received from the data collector and analyzer 106 .
- the determination as to when adjustment is needed may be performed by the data collector and analyzer 106 and/or the adjustment generator 107 .
- the data collector and analyzer is further configured to analyze the data and provide information regarding the collected data (e.g., improvement score for each alternative or alternative set) and/or analysis of the collected data to the feedback provider 108 which provides the user (e.g., engineer) requesting the experiment with feedback or results regarding the experiment.
- the results of the experiment may be generated at the data collector and analyzer 106 and/or the feedback provider 108 and is provided to the user at the termination of the experiment.
- the determination as to when the experiment should be terminated may be performed by the data collector and analyzer 106 and/or the feedback provider 108 .
- FIG. 2 illustrates an example process 200 for generating an experiment.
- the system receives an indication of a request to generate an experiment.
- the request may be a request from a user to generate a new experiment (e.g., an engineer requesting to conduct an automated feature-based improvement experiment) or may be a system request (e.g., from the experiment adjustor 107 ) to generate an adjusted experiment for an ongoing experiment.
- the system determines the features of the computer-implemented entity being tested according to the objective of the experiment.
- the objective of the experiment is provided as an input by a user and includes one or more features that are to be tested.
- the system determines an alternative feature range for each of the one or more features being tested.
- the alternative feature range for each feature being tested is provided as a user input and defines a range of values for alternative features that may be tested with respect to a specific feature.
- the system identifies a pool of user for the experiment.
- the pool of users may be selected from a pool of users of the computer-implemented entity available to take part in an experiment.
- the pool of users may be selected specifically based on the features being tested and the selection may be based on the user characteristics and features, as well as system or user-indicated criteria regarding the users or types of users that are optimal for the experiment.
- the system generates one or more alternative sets for the experiment.
- the alternative sets are generated according to the objective and the alternative feature ranges provided for each feature being tested.
- the system selects a number of alternative features from the identified alternative feature range for each feature being tested.
- Each alternative may include a variation of the feature being tested (e.g., a variation on an existing feature of a computer-implemented service or product being offered to the users).
- the selection of the alternative features may be based upon the number of users available for the experiment, the number of alternative features likely to provide enough comparison and generate enough data for a definitive result, and/or prior knowledge of information relating to the experiment, the objectives, the alternative feature range or the specific feature.
- the system then generates alternative sets, each alternative set including at least one alternative for each feature being tested.
- the pool of users identified in block 204 are grouped together to generate user groups (e.g., based on the number of the alternative sets).
- the grouping of the users is based upon an algorithm that minimizes any bias across the groupings and creates sample groups of users that have same or similar characteristics and behavior at least with respect to the feature(s) included as part of the objective.
- the user groups may be determined based on user characteristics of the users (e.g., historically observed user characteristics stored within a user profile).
- each user group generated in block 206 is assigned to a specific alternative set generated in block 205 .
- User groups may be assigned to an alternative set based on user characteristics of the users within the user group (e.g., historically observed user characteristics stored within a user profile).
- the experiment is ready for initiation and the system begins collecting information regarding user behavior with regard to the variations on the features to determine the best or optimal features that achieve the objectives of the experiment.
- FIG. 3 illustrates a flow chart of an example process 300 for automatically conducting a feature-based improvement experiment.
- the system receives an indication of user activity with respect to the alternative set assigned to the user (i.e., the feature variations presented to the user as part of the user experience with respect to the computer-implemented entity).
- the system analyzes the received user activity to determine an improvement score for each alternative set and/or one or more alternative features associated with one or more of the features being tested. The calculated improvement score may be used when generating the results of the experiment and/or to dynamically adjust the alternative sets, groupings of users and/or the assignment of user groups to alternative sets during the experiment.
- the system may, in block 303 , determine if the experiment has reached a point of adjustment at which it is beneficial or necessary to adjust some variables of the experiment.
- the process continues to block 304 .
- the system generates adjusted alternative sets (e.g., expanding one or more alternative sets, grouping one or more alternative sets, removing one or more alternative sets and/or adding one or more additional alternative sets).
- the user groupings and/or assignment of user groups may also be adjusted.
- assignment of user groups and alternative sets is adjusted to account for the adjustments of the alternative features, alternative sets, users and/or user groups.
- the adjustment process is then completed and the process returns to block 301 and resumes the process of receiving indication of user activity and analyzing the user activity.
- the process continues to block 307 and determine whether the experiment has reached its termination.
- the system may periodically perform a check to determine whether the point of termination has been reached.
- the system may determine that the system has reached its point of termination based one or more conditions indicating that the experiment should be terminated, including whether certain results have been achieved (e.g., improvement scores, confidence, etc.) or whether a termination event has occurred (e.g., time elapsed, resources spent, etc.).
- the process returns to block 301 . Otherwise, if, in block 307 , the system determines that the experiment has reached the point of termination, the process continues to block 308 and determines experiment results. Once the system determines that the experiment has reached its point of termination, all data collected during the experiment is provided for analysis. At the termination of the experiment, the system analyzes all received activity from users with respect to the features of the computer-implemented entity and analyzes the data to provide the user with feed-back regarding the experiment.
- the system may provides the experiment results in the form of one or more determinations relating to the experiment and/or features being tested for display to a user requesting the experiment (e.g., engineer).
- FIG. 4 illustrates an example client-server network environment which provides for implementing an automated objective-based feature improvement system and process.
- a network environment 400 includes a number of electronic devices 402 , 404 and 406 communicably connected to a server 410 by a network 408 .
- Server 410 includes a processing device 412 and a data store 414 .
- Processing device 412 executes computer instructions stored in data store 414 , for example, to assist in implementing an automated objective-based feature improvement system and process at electronic devices 402 , 404 and 406 .
- electronic devices 402 , 404 and 406 can be computing devices such as laptop or desktop computers, smartphones, PDAs, portable media players, tablet computers, televisions or other displays with one or more processors coupled thereto or embedded therein, or other appropriate computing devices that can be used to for displaying a web page or web application.
- electronic device 402 is depicted as a smartphone
- electronic device 404 is depicted as a desktop computer
- electronic device 406 is depicted as a PDA.
- server 410 can be a single computing device such as a computer server. In other embodiments, server 410 can represent more than one computing device working together to perform the actions of a server computer (e.g., cloud computing).
- the server 410 may host the web server communicationally coupled to the browser at the client device (e.g., electronic devices 402 , 404 or 406 ) via network 408 .
- the network 408 can include, for example, any one or more of a personal area network (PAN), a local area network (LAN), a campus area network (CAN), a metropolitan area network (MAN), a wide area network (WAN), a broadband network (BBN), the Internet, and the like. Further, the network 408 can include, but is not limited to, any one or more of the following network topologies, including a bus network, a star network, a ring network, a mesh network, a star-bus network, tree or hierarchical network, and the like.
- PAN personal area network
- LAN local area network
- CAN campus area network
- MAN metropolitan area network
- WAN wide area network
- BBN broadband network
- the Internet and the like.
- the network 408 can include, but is not limited to, any one or more of the following network topologies, including a bus network, a star network, a ring network, a mesh network, a star-bus network, tree or hierarchical network, and the like.
- An experiment may be requested by a user at a client device (e.g., electronic devices 102 , 104 or 106 ).
- the system e.g., hosted at server 110
- the system may generate the experiment, including selecting a pool of users (e.g., users interacting with electronic devices 102 , 104 and 106 ) and assigning a set of alternative features (i.e., variations of features being tested as part of the experiment) to the users, such that the experience of the user with respect to the computer-implemented entity (i.e., product or service) which is the subject of the experiment.
- the system at server 110 receives user activity with respect to the alternative sets and generates feed-back regarding the experiment for display to the user (e.g., user interacting with one or more of the electronic devices 102 , 104 and 106 ).
- Computer readable storage medium also referred to as computer readable medium.
- processing unit(s) e.g., one or more processors, cores of processors, or other processing units
- processing unit(s) e.g., one or more processors, cores of processors, or other processing units
- Examples of computer readable media include, but are not limited to, CD-ROMs, flash drives, RAM chips, hard drives, EPROMs, etc.
- the computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.
- the term “software” is meant to include firmware residing in read-only memory or applications stored in magnetic storage, which can be read into memory for processing by a processor.
- multiple software aspects of the subject disclosure can be implemented as sub-parts of a larger program while remaining distinct software aspects of the subject disclosure.
- multiple software aspects can also be implemented as separate programs.
- any combination of separate programs that together implement a software aspect described here is within the scope of the subject disclosure.
- the software programs when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- FIG. 5 conceptually illustrates an electronic system with which some implementations of the subject technology are implemented.
- Electronic system 500 can be a server, computer, phone, PDA, laptop, tablet computer, television with one or more processors embedded therein or coupled thereto, or any other sort of electronic device.
- Such an electronic system includes various types of computer readable media and interfaces for various other types of computer readable media.
- Electronic system 500 includes a bus 508 , processing unit(s) 512 , a system memory 504 , a read-only memory (ROM) 510 , a permanent storage device 502 , an input device interface 514 , an output device interface 506 , and a network interface 516 .
- processing unit(s) 512 includes a bus 508 , processing unit(s) 512 , a system memory 504 , a read-only memory (ROM) 510 , a permanent storage device 502 , an input device interface 514 , an output device interface 506 , and a network interface 516 .
- Bus 508 collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices of electronic system 500 .
- bus 508 communicatively connects processing unit(s) 512 with ROM 510 , system memory 504 , and permanent storage device 502 .
- processing unit(s) 512 retrieves instructions to execute and data to process in order to execute the processes of the subject disclosure.
- the processing unit(s) can be a single processor or a multi-core processor in different implementations.
- ROM 510 stores static data and instructions that are needed by processing unit(s) 512 and other modules of the electronic system.
- Permanent storage device 502 is a read-and-write memory device. This device is a non-volatile memory unit that stores instructions and data even when electronic system 500 is off. Some implementations of the subject disclosure use a mass-storage device (such as a magnetic or optical disk and its corresponding disk drive) as permanent storage device 502 .
- system memory 504 is a read-and-write memory device. However, unlike storage device 502 , system memory 504 is a volatile read-and-write memory, such a random access memory. System memory 504 stores some of the instructions and data that the processor needs at runtime. In some implementations, the processes of the subject disclosure are stored in system memory 504 , permanent storage device 502 , and/or ROM 510 .
- the various memory units include instructions for facilitating an automated objective-based feature improvement experiment. From these various memory units, processing unit(s) 512 retrieves instructions to execute and data to process in order to execute the processes of some implementations.
- Bus 508 also connects to input and output device interfaces 514 and 506 .
- Input device interface 514 enables the user to communicate information and select commands to the electronic system.
- Input devices used with input device interface 514 include, for example, alphanumeric keyboards and pointing devices (also called “cursor control devices”).
- Output device interfaces 506 enables, for example, the display of images generated by the electronic system 500 .
- Output devices used with output device interface 506 include, for example, printers and display devices, such as cathode ray tubes (CRT) or liquid crystal displays (LCD). Some implementations include devices such as a touchscreen that functions as both input and output devices.
- CTR cathode ray tubes
- LCD liquid crystal displays
- bus 508 also couples electronic system 500 to a network (not shown) through a network interface 516 .
- the computer can be a part of a network of computers (such as a local area network (“LAN”), a wide area network (“WAN”), or an Intranet, or a network of networks, such as the Internet. Any or all components of electronic system 500 can be used in conjunction with the subject disclosure.
- Some implementations include electronic components, such as microprocessors, storage and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media).
- computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable/rewritable DVDs (e.g., DVD-RAM, DVD-RW, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic and/or solid state hard drives, read-only and recordable Blu-Ray® discs, ultra density optical discs, any other optical or magnetic media, and floppy disks.
- CD-ROM compact discs
- CD-R recordable compact discs
- the computer-readable media can store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations.
- Examples of computer programs or computer code include machine code, such as is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.
- ASICs application specific integrated circuits
- FPGAs field programmable gate arrays
- integrated circuits execute instructions that are stored on the circuit itself.
- the terms “computer”, “server”, “processor”, and “memory” all refer to electronic or other technological devices. These terms exclude people or groups of people.
- display or displaying means displaying on an electronic device.
- computer readable medium and “computer readable media” are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral signals.
- implementations of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
- any specific order or hierarchy of blocks in the processes disclosed is an illustration of exemplary approaches. Based upon design preferences, it is understood that the specific order or hierarchy of blocks in the processes may be rearranged, or that some illustrated blocks may not be performed. Some of the blocks may be performed simultaneously. For example, in certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Headings and subheadings are used for convenience only and do not limit the subject disclosure. Features under one heading may be combined with features under one or more other heading and all features under one heading need not be use together. Features under one heading may be combined with features under one or more other heading and all features under one heading need not be use together.
- a phrase such as an “aspect” does not imply that such aspect is essential to the subject technology or that such aspect applies to all configurations of the subject technology.
- a disclosure relating to an aspect may apply to all configurations, or one or more configurations.
- a phrase such as an aspect may refer to one or more aspects and vice versa.
- a phrase such as a “configuration” does not imply that such configuration is essential to the subject technology or that such configuration applies to all configurations of the subject technology.
- a disclosure relating to a configuration may apply to all configurations, or one or more configurations.
- a phrase such as a configuration may refer to one or more configurations and vice versa.
Abstract
Description
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/560,874 US8706733B1 (en) | 2012-07-27 | 2012-07-27 | Automated objective-based feature improvement |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/560,874 US8706733B1 (en) | 2012-07-27 | 2012-07-27 | Automated objective-based feature improvement |
Publications (1)
Publication Number | Publication Date |
---|---|
US8706733B1 true US8706733B1 (en) | 2014-04-22 |
Family
ID=50481960
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/560,874 Expired - Fee Related US8706733B1 (en) | 2012-07-27 | 2012-07-27 | Automated objective-based feature improvement |
Country Status (1)
Country | Link |
---|---|
US (1) | US8706733B1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160379268A1 (en) * | 2013-12-10 | 2016-12-29 | Tencent Technology (Shenzhen) Company Limited | User behavior data analysis method and device |
US20170300823A1 (en) * | 2016-04-13 | 2017-10-19 | International Business Machines Corporation | Determining user influence by contextual relationship of isolated and non-isolated content |
US11204742B1 (en) * | 2018-03-29 | 2021-12-21 | Electronic Arts Inc. | Multi-objective experiments with dynamic group assignment |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060044339A1 (en) * | 2004-08-27 | 2006-03-02 | Toshiyuki Mizutani | Image forming apparatus, image forming method and image forming program |
US20060064339A1 (en) * | 2004-09-13 | 2006-03-23 | Technology Transfer Office, Brigham Young University | Methods and systems for conducting internet marketing experiments |
US20070008044A1 (en) * | 2005-07-05 | 2007-01-11 | Sharp Kabushiki Kaisha | Test circuit, delay circuit, clock generating circuit, and image sensor |
US20070021886A1 (en) * | 2005-07-25 | 2007-01-25 | Aisin Aw Co., Ltd. | Vehicle suspension control system and method |
US20070094198A1 (en) * | 2005-07-18 | 2007-04-26 | Loh Aik K | Product framework for managing test systems, supporting customer relationships management and protecting intellectual knowledge in a manufacturing testing environment |
US20070271511A1 (en) * | 2006-05-22 | 2007-11-22 | Chirag Khopkar | Starting landing page experiments |
US7321883B1 (en) * | 2005-08-05 | 2008-01-22 | Perceptronics Solutions, Inc. | Facilitator used in a group decision process to solve a problem according to data provided by users |
US20080071741A1 (en) * | 2006-09-19 | 2008-03-20 | Kazunari Omi | Method and an apparatus to perform feature weighted search and recommendation |
US20120078908A1 (en) * | 2010-09-29 | 2012-03-29 | Accenture Global Services Limited | Processing a reusable graphic in a document |
US20130191450A1 (en) * | 2010-09-28 | 2013-07-25 | Google Inc. | Optimization guided by connection classification in a web browser extension |
-
2012
- 2012-07-27 US US13/560,874 patent/US8706733B1/en not_active Expired - Fee Related
Patent Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060044339A1 (en) * | 2004-08-27 | 2006-03-02 | Toshiyuki Mizutani | Image forming apparatus, image forming method and image forming program |
US20060064339A1 (en) * | 2004-09-13 | 2006-03-23 | Technology Transfer Office, Brigham Young University | Methods and systems for conducting internet marketing experiments |
US20070008044A1 (en) * | 2005-07-05 | 2007-01-11 | Sharp Kabushiki Kaisha | Test circuit, delay circuit, clock generating circuit, and image sensor |
US20070094198A1 (en) * | 2005-07-18 | 2007-04-26 | Loh Aik K | Product framework for managing test systems, supporting customer relationships management and protecting intellectual knowledge in a manufacturing testing environment |
US7321885B2 (en) * | 2005-07-18 | 2008-01-22 | Agilent Technologies, Inc. | Product framework for managing test systems, supporting customer relationships management and protecting intellectual knowledge in a manufacturing testing environment |
US20070021886A1 (en) * | 2005-07-25 | 2007-01-25 | Aisin Aw Co., Ltd. | Vehicle suspension control system and method |
US7321883B1 (en) * | 2005-08-05 | 2008-01-22 | Perceptronics Solutions, Inc. | Facilitator used in a group decision process to solve a problem according to data provided by users |
US20070271511A1 (en) * | 2006-05-22 | 2007-11-22 | Chirag Khopkar | Starting landing page experiments |
US7844894B2 (en) * | 2006-05-22 | 2010-11-30 | Google Inc. | Starting landing page experiments |
US20080071741A1 (en) * | 2006-09-19 | 2008-03-20 | Kazunari Omi | Method and an apparatus to perform feature weighted search and recommendation |
US20130191450A1 (en) * | 2010-09-28 | 2013-07-25 | Google Inc. | Optimization guided by connection classification in a web browser extension |
US20120078908A1 (en) * | 2010-09-29 | 2012-03-29 | Accenture Global Services Limited | Processing a reusable graphic in a document |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160379268A1 (en) * | 2013-12-10 | 2016-12-29 | Tencent Technology (Shenzhen) Company Limited | User behavior data analysis method and device |
US20170300823A1 (en) * | 2016-04-13 | 2017-10-19 | International Business Machines Corporation | Determining user influence by contextual relationship of isolated and non-isolated content |
US11204742B1 (en) * | 2018-03-29 | 2021-12-21 | Electronic Arts Inc. | Multi-objective experiments with dynamic group assignment |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11080037B2 (en) | Software patch management incorporating sentiment analysis | |
US9952761B1 (en) | System and method for processing touch actions | |
US8249876B1 (en) | Method for providing alternative interpretations of a voice input to a user | |
US10419585B2 (en) | Dynamically surfacing UI controls | |
US9201563B2 (en) | Mobile device friendly window management for remote desktop | |
JP2020514857A (en) | Smart assist for repetitive actions | |
US9692701B1 (en) | Throttling client initiated traffic | |
US10459835B1 (en) | System and method for controlling quality of performance of digital applications | |
WO2015152882A1 (en) | Candidate services for an application | |
WO2018072060A1 (en) | Machine learning based identification of broken network connections | |
US20200111046A1 (en) | Automated and intelligent time reallocation for agenda items | |
US10592606B2 (en) | System and method for detecting portability of sentiment analysis system based on changes in a sentiment confidence score distribution | |
US20160350771A1 (en) | Survey fatigue prediction and identification | |
US8706733B1 (en) | Automated objective-based feature improvement | |
US11663418B2 (en) | Systems and methods for generating dynamic conversational responses using machine learning models | |
US20150205515A1 (en) | Processing a hover event on a touchscreen device | |
US10628475B2 (en) | Runtime control of automation accuracy using adjustable thresholds | |
US20150200829A1 (en) | Task-based state recovery in a web browser | |
US8875308B1 (en) | Privacy selection based on social groups | |
US8677314B1 (en) | Modifying a source code file to reduce dependencies included therein | |
US9058189B1 (en) | Automatic user account selection for launching an application | |
US20160026372A1 (en) | Graph-based approach for dynamic configuration of user interfaces | |
US8856598B1 (en) | Help center alerts by using metadata and offering multiple alert notification channels | |
US9606720B1 (en) | System and method for providing a preview of a digital photo album | |
US9300756B2 (en) | Identity crowd-sourced curation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:YESKEL, ZACH;HUFFAKER, DAVID ANDREW;ROSENTHAL SCHUTT, RACHEL IDA;AND OTHERS;SIGNING DATES FROM 20120626 TO 20120725;REEL/FRAME:028679/0384 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20220422 |