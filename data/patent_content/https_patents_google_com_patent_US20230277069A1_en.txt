US20230277069A1 - Heart Rate and Respiratory Rate Measurements from Imagery - Google Patents
Heart Rate and Respiratory Rate Measurements from Imagery Download PDFInfo
- Publication number
- US20230277069A1 US20230277069A1 US18/011,899 US202218011899A US2023277069A1 US 20230277069 A1 US20230277069 A1 US 20230277069A1 US 202218011899 A US202218011899 A US 202218011899A US 2023277069 A1 US2023277069 A1 US 2023277069A1
- Authority
- US
- United States
- Prior art keywords
- computing system
- computer
- respiratory
- implemented method
- waveform
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000009532 heart rate measurement Methods 0.000 title description 3
- 238000009531 respiratory rate measurement Methods 0.000 title description 3
- 238000000034 method Methods 0.000 claims abstract description 101
- 230000036387 respiratory rate Effects 0.000 claims abstract description 76
- 238000013186 photoplethysmography Methods 0.000 claims description 52
- 230000000241 respiratory effect Effects 0.000 claims description 43
- 230000003287 optical effect Effects 0.000 claims description 28
- 230000033001 locomotion Effects 0.000 claims description 26
- 238000012935 Averaging Methods 0.000 claims description 17
- 238000001228 spectrum Methods 0.000 claims description 17
- 238000004458 analytical method Methods 0.000 claims description 10
- 230000008859 change Effects 0.000 claims description 4
- 230000004931 aggregating effect Effects 0.000 claims description 3
- 238000009499 grossing Methods 0.000 claims description 3
- 230000001131 transforming effect Effects 0.000 claims description 3
- 238000004422 calculation algorithm Methods 0.000 abstract description 34
- 238000005259 measurement Methods 0.000 description 44
- 238000010191 image analysis Methods 0.000 description 25
- 238000010586 diagram Methods 0.000 description 13
- 230000006870 function Effects 0.000 description 12
- 230000001684 chronic effect Effects 0.000 description 11
- 230000008569 process Effects 0.000 description 11
- 238000013480 data collection Methods 0.000 description 10
- 238000004891 communication Methods 0.000 description 6
- 238000011156 evaluation Methods 0.000 description 6
- 238000012545 processing Methods 0.000 description 6
- 238000010200 validation analysis Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 5
- 230000036541 health Effects 0.000 description 5
- 238000004590 computer program Methods 0.000 description 4
- 230000000694 effects Effects 0.000 description 4
- 230000029058 respiratory gaseous exchange Effects 0.000 description 4
- 238000013459 approach Methods 0.000 description 3
- 239000008280 blood Substances 0.000 description 3
- 210000004369 blood Anatomy 0.000 description 3
- 238000013461 design Methods 0.000 description 3
- 238000003745 diagnosis Methods 0.000 description 3
- 238000012544 monitoring process Methods 0.000 description 3
- 229920001451 polypropylene glycol Polymers 0.000 description 3
- 230000002123 temporal effect Effects 0.000 description 3
- 238000010989 Bland-Altman Methods 0.000 description 2
- 208000006545 Chronic Obstructive Pulmonary Disease Diseases 0.000 description 2
- XUMBMVFBXHLACL-UHFFFAOYSA-N Melanin Chemical compound O=C1C(=O)C(C2=CNC3=C(C(C(=O)C4=C32)=O)C)=C2C4=CNC2=C1C XUMBMVFBXHLACL-UHFFFAOYSA-N 0.000 description 2
- GCKMFJBGXUYNAG-HLXURNFRSA-N Methyltestosterone Chemical compound C1CC2=CC(=O)CC[C@]2(C)[C@@H]2[C@@H]1[C@@H]1CC[C@](C)(O)[C@@]1(C)CC2 GCKMFJBGXUYNAG-HLXURNFRSA-N 0.000 description 2
- 230000004075 alteration Effects 0.000 description 2
- 208000006673 asthma Diseases 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 230000001186 cumulative effect Effects 0.000 description 2
- 238000001514 detection method Methods 0.000 description 2
- 201000010099 disease Diseases 0.000 description 2
- 208000037265 diseases, disorders, signs and symptoms Diseases 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000001914 filtration Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000012806 monitoring device Methods 0.000 description 2
- 230000010355 oscillation Effects 0.000 description 2
- 230000037081 physical activity Effects 0.000 description 2
- 238000007619 statistical method Methods 0.000 description 2
- 238000009528 vital sign measurement Methods 0.000 description 2
- 238000000729 Fisher's exact test Methods 0.000 description 1
- 206010044565 Tremor Diseases 0.000 description 1
- 230000001154 acute effect Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000003321 amplification Effects 0.000 description 1
- 230000003466 anti-cipated effect Effects 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000036760 body temperature Effects 0.000 description 1
- 230000002802 cardiorespiratory effect Effects 0.000 description 1
- 230000036996 cardiovascular health Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 206010012601 diabetes mellitus Diseases 0.000 description 1
- 239000003814 drug Substances 0.000 description 1
- 229940079593 drug Drugs 0.000 description 1
- 230000007717 exclusion Effects 0.000 description 1
- 150000004820 halides Chemical class 0.000 description 1
- 230000003862 health status Effects 0.000 description 1
- 230000001788 irregular Effects 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 230000008450 motivation Effects 0.000 description 1
- 238000003199 nucleic acid amplification method Methods 0.000 description 1
- 238000012898 one-sample t-test Methods 0.000 description 1
- 230000003534 oscillatory effect Effects 0.000 description 1
- 238000007427 paired t-test Methods 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 230000002685 pulmonary effect Effects 0.000 description 1
- 230000000541 pulsatile effect Effects 0.000 description 1
- 238000013139 quantization Methods 0.000 description 1
- 238000011084 recovery Methods 0.000 description 1
- 238000011160 research Methods 0.000 description 1
- 230000000284 resting effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 238000012216 screening Methods 0.000 description 1
- 238000001629 sign test Methods 0.000 description 1
- 230000036555 skin type Effects 0.000 description 1
- 230000004580 weight loss Effects 0.000 description 1
- 210000000707 wrist Anatomy 0.000 description 1
Images
Classifications
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/02—Detecting, measuring or recording pulse, heart rate, blood pressure or blood flow; Combined pulse/heart-rate/blood pressure determination; Evaluating a cardiovascular condition not otherwise provided for, e.g. using combinations of techniques provided for in this group with electrocardiography or electroauscultation; Heart catheters for measuring blood pressure
- A61B5/0205—Simultaneously evaluating both cardiovascular conditions and different types of body conditions, e.g. heart and respiratory condition
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/0059—Measuring for diagnostic purposes; Identification of persons using light, e.g. diagnosis by transillumination, diascopy, fluorescence
- A61B5/0077—Devices for viewing the surface of the body, e.g. camera, magnifying lens
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/02—Detecting, measuring or recording pulse, heart rate, blood pressure or blood flow; Combined pulse/heart-rate/blood pressure determination; Evaluating a cardiovascular condition not otherwise provided for, e.g. using combinations of techniques provided for in this group with electrocardiography or electroauscultation; Heart catheters for measuring blood pressure
- A61B5/024—Detecting, measuring or recording pulse rate or heart rate
- A61B5/02416—Detecting, measuring or recording pulse rate or heart rate using photoplethysmograph signals, e.g. generated by infrared radiation
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/02—Detecting, measuring or recording pulse, heart rate, blood pressure or blood flow; Combined pulse/heart-rate/blood pressure determination; Evaluating a cardiovascular condition not otherwise provided for, e.g. using combinations of techniques provided for in this group with electrocardiography or electroauscultation; Heart catheters for measuring blood pressure
- A61B5/024—Detecting, measuring or recording pulse rate or heart rate
- A61B5/02438—Detecting, measuring or recording pulse rate or heart rate with portable devices, e.g. worn by the patient
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/68—Arrangements of detecting, measuring or recording means, e.g. sensors, in relation to patient
- A61B5/6887—Arrangements of detecting, measuring or recording means, e.g. sensors, in relation to patient mounted on external non-worn devices, e.g. non-medical devices
- A61B5/6898—Portable consumer electronic devices, e.g. music players, telephones, tablet computers
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/72—Signal processing specially adapted for physiological signals or for diagnostic purposes
- A61B5/7235—Details of waveform analysis
- A61B5/725—Details of waveform analysis using specific filters therefor, e.g. Kalman or adaptive filters
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/72—Signal processing specially adapted for physiological signals or for diagnostic purposes
- A61B5/7235—Details of waveform analysis
- A61B5/7253—Details of waveform analysis characterised by using transforms
- A61B5/7257—Details of waveform analysis characterised by using transforms using Fourier transforms
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/72—Signal processing specially adapted for physiological signals or for diagnostic purposes
- A61B5/7271—Specific aspects of physiological measurement analysis
- A61B5/7278—Artificial waveform generation or derivation, e.g. synthesising signals from measured signals
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/74—Details of notification to user or communication with user or patient ; user input means
- A61B5/7475—User input or interface means, e.g. keyboard, pointing device, joystick
- A61B5/748—Selection of a region of interest, e.g. using a graphics tablet
- A61B5/7485—Automatic selection of region of interest
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B2576/00—Medical imaging apparatus involving image processing or analysis
- A61B2576/02—Medical imaging apparatus involving image processing or analysis specially adapted for a particular organ or body part
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/08—Detecting, measuring or recording devices for evaluating the respiratory organs
- A61B5/0803—Recording apparatus specially adapted therefor
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/08—Detecting, measuring or recording devices for evaluating the respiratory organs
- A61B5/0816—Measuring devices for examining respiratory frequency
-
- A—HUMAN NECESSITIES
- A61—MEDICAL OR VETERINARY SCIENCE; HYGIENE
- A61B—DIAGNOSIS; SURGERY; IDENTIFICATION
- A61B5/00—Measuring for diagnostic purposes; Identification of persons
- A61B5/72—Signal processing specially adapted for physiological signals or for diagnostic purposes
- A61B5/7203—Signal processing specially adapted for physiological signals or for diagnostic purposes for noise prevention, reduction or removal
- A61B5/7207—Signal processing specially adapted for physiological signals or for diagnostic purposes for noise prevention, reduction or removal of noise induced by motion artifacts
Definitions
- the present disclosure relates generally to heart rate and respiratory rate measurements. More particularly, the present disclosure relates to measuring heart rate and respiratory rate from imagery captured by a camera such as a smartphone camera or other consumer-grade camera.
- HR heart rate
- RR respiratory rate
- body temperature body temperature
- blood pressure blood pressure
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- One general example aspect includes a computer-implemented method to measure heart rate of a subject.
- the computer-implemented method includes obtaining, by a computing system that includes one or more computing devices, a plurality of image frames captured by a camera while a digit of the subject is placed in a field of view of the camera.
- the method also includes measuring, by the computing system, a photoplethysmography (PPG) waveform from the plurality of image frames.
- PPG photoplethysmography
- the method also includes extracting, by the computing system, a dominant frequency from the PPG waveform.
- the method also includes determining, by the computing system, the heart rate of the subject based on the dominant frequency extracted from the PPG waveform.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- measuring, by the computing system, the PPG waveform from the plurality of image frames may include: selecting, by the computing system, regions of interest from the plurality of image frames; averaging, by the computing system, pixels in each of the regions of interest on a per-channel basis (e.g. separately for each of a plurality of color channels included in the image frame; e.g. up to three channels in the case of RGB images) to form a plurality of per-channel waveforms; and computing, by the computing system, the PPG waveform as a weighted average of the plurality of per-channel waveforms.
- a per-channel basis e.g. separately for each of a plurality of color channels included in the image frame; e.g. up to three channels in the case of RGB images
- Extracting, by the computing system, the dominant frequency from the PPG waveform may include: determining, by the computing system, a signal to noise ratio for each region of interest by computing a ratio between a power of a dominant frequency and powers of non-dominant frequencies on a logarithmic scale; identifying, by the computing system, the region of interest with the largest signal to noise ratio; and extracting, by the computing system, the dominant frequency of the region of interest with the largest signal to noise ratio.
- the digit may include a finger.
- the digit of the subject may be placed in physical contact with an outermost lens or cover of the camera during capture of the plurality of image frames.
- the computing system may consist of a user device, such as a user device having an integrated camera (e.g. a camera which is within the same cover (housing) as the one or more computing devices).
- the computer-implemented method includes obtaining, by a computing system which may include one or more computing devices, a plurality of image frames captured by a camera while a subject is placed in a field of view of the camera.
- the method also includes determining, by the computing system, for each pixel of one or more pixels in each frame, a position represented by a phase of spatially localized sinusoids in multiple scales.
- the method also includes transforming, by the computing system, the phase of the spatially localized sinusoids into optical flow data by linearly approximating the position implied by each phase coefficient and averaging across scales.
- the method also includes determining, by the computing system, the respiratory rate of the subject based on the optical flow data.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations may include one or more of the following features.
- the computer-implemented method where determining, by the computing system, the respiratory rate of the subject based on the optical flow data may include: selecting, by the computing system, a plurality of regions of interest; averaging, by the computing system, a vertical component of the optical flow data for each of the regions of interest to generate a plurality of respiratory waveforms respectively for the plurality of regions of interest; and determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms.
- Determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms may include: generating, by the computing system, a power spectrum from a frequency domain representation of each respiratory waveform; and aggregating, by the computing system, the power spectra for the plurality of respiratory waveforms to obtain a final ensembled power spectrum; and determining, by the computing system, the respiratory rate of the subject based on a maximum power frequency of the final ensembled power spectrum.
- the computer-implemented method may include defaulting, by the computing system, to a time-domain estimation of the respiratory waveform when a signal to noise ratio associated with the frequency domain representation falls below a threshold.
- a base of a neck, a shoulder line, and an upper torso of the subject may be depicted by the plurality of image frames.
- Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.
- the computer-implemented method includes obtaining, by a computing system may include one or more computing devices, a plurality of image frames captured by a camera while a subject is placed in a field of view of the camera.
- the method also includes determining, by the computing system, optical flow data for the plurality of image frames.
- the method also includes averaging, by the computing system, a vertical component of the optical flow data to generate a respiratory waveform.
- the method also includes generating, by the computing system, a frequency domain representation of the respiratory waveform.
- the method also includes determining, by the computing system, the respiratory rate of the subject based on the frequency domain representation of the respiratory waveform.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a computing device may include: a camera; one or more processors; and one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing device to perform any of the methods described herein.
- FIG. 1 A shows an example “setup” of how measurements are taken — with the finger over the rear-facing camera for HR according to example embodiments of the present disclosure.
- FIGS. 1 B-C show example user interfaces for guiding a user to perform the HR evaluation according to example embodiments of the present disclosure.
- FIG. 1 D shows the measurements/results for mean absolute percent error (MAPE) for HR according to example embodiments of the present disclosure.
- FIG. 1 E shows an example “setup” of how measurements are taken using a video of the participant via the front-facing camera according to example embodiments of the present disclosure.
- FIGS. 1 F-G show example user interfaces for guiding a user to perform the RR evaluation according to example embodiments of the present disclosure.
- FIG. 1 H shows the measurements/results for mean absolute error (MAE) for RR according to example embodiments of the present disclosure.
- FIG. 2 A depicts an example block diagram of a system for analyzing imagery according to example embodiments of the present disclosure.
- FIG. 2 B depicts an example block diagram of a system for analyzing imagery according to example embodiments of the present disclosure.
- FIG. 2 C depicts an example block diagram of a system for analyzing imagery according to example embodiments of the present disclosure.
- FIG. 3 depicts a block diagram of an example computing system according to example embodiments of the present disclosure.
- FIG. 4 depicts a flow chart diagram of an example method to measure heart rate according to example embodiments of the present disclosure.
- FIG. 5 depicts a flow chart diagram of an example method to measure respiratory rate according to example embodiments of the present disclosure.
- the present disclosure is directed to systems and methods for measuring heart rate and/or respiratory rate using a camera such as, for example, a smartphone camera or other consumer-grade camera.
- a camera such as, for example, a smartphone camera or other consumer-grade camera.
- the present disclosure presents and validates two algorithms that make use of smartphone cameras (or the like) for measuring heart rate (HR) and respiratory rate (RR) for consumer wellness use.
- HR can be measured by placing the finger of a subject over the rear-facing camera and/or front-facing camera.
- RR can be measured via a video of the subject sitting still in front of the front-facing camera and/or rear-facing camera.
- a first example approach for HR measurement leverages photoplethysmography (PPG) acquired using cameras, such as consumer-grade and/or embedded cameras such as those commonly found in laptops, tablets, smartphones, and/or user devices (e.g., front-facing and/or rear-facing cameras).
- PPG signals can be recorded by placing a finger over the camera lens, and the color changes captured in the video can be used to determine the oscillation of blood volume after each heartbeat.
- a second example approach measures RR from upper-torso videos obtained via a camera to track the physical motion of breathing.
- a computing system can determine, for each pixel of one or more pixels in each video frame, a position represented by a phase of spatially localized sinusoids in multiple scales.
- a plurality of sinusoidal functions (“sinusoids”) may be defined, each function describing an oscillatory average motion of respective corresponding portions of the video frames (e.g. one such sinusoidal function may describe oscillations in a top right corner portion of each of the video frames).
- the portions of the video frame corresponding to different ones of the sinusoidal functions overlap and have different corresponding sizes (scales).
- One of the parameters defining each sinusoidal function is a respective phase coefficient.
- Others of the parameters may be an amplitude of the motion, and parameters describing the portion of the video frames described by the sinusoidal function. Determining, for each of the one or more pixels, a position represented by the phase of the sinusoids can be considered as meaning that the one or more pixels are used to determine some or all parameters of the sinusoidal functions (e.g. the amplitude and the phase coefficient of each sinusoidal function), and in particular the phase coefficients of the plurality of sinusoidal functions.
- the sinusoids collectively imply, for each of the one or more pixels, a respective position on the subject which is shown by that pixel in any given one of the video frames.
- the computing system can transform the phase of spatially localized sinusoids into optical flow data by linearly approximating the position implied by each phase coefficient and averaging across scales (e.g. averaging over the sinusoidal functions, to determine the motion collectively implied by them).
- the computing system can determine the respiratory rate of the subject based on the optical flow data.
- Optical flow can refer to the pattern of the apparent motion of the subject in the video frames, caused by the motion of the subject relative to the camera. Optical flow can be assessed, for example, on a pixel-by-pixel basis.
- the present disclosure reports the performance of these two algorithms in prospective clinical validation studies, demonstrating reliable and consistent accuracy on diverse populations (e.g., in terms of objectively-measured skin tones) for HR and health status (e.g., with and without chronic pulmonary conditions) for RR.
- the mean absolute percent error (MAPE) ⁇ standard deviation of the measurement was 1.6% ⁇ 4.3%, which was significantly lower than the pre-specified goal of 5%. No significant differences in the MAPE were present across colorimeter-measured skin-tone subgroups.
- the mean absolute error (MAE) was 0.78 ⁇ 0.61 breaths/min, which was significantly lower than the pre-specified goal of 3 breath/min.
- the MAE was low in both healthy participants (0.70 ⁇ 0.67 breaths/min), and participants with chronic respiratory conditions (0.80 ⁇ 0.60 breaths/min).
- the systems and methods described herein provide a number of technical effects and benefits. More particularly, the systems and methods of the present disclosure provide improved techniques for providing a diagnosis (e.g., measuring heart rate and/or respiratory rate) from imagery captured by a camera such as a smartphone camera or other consumer-grade camera. In addition, measured heart rate and/or respiratory rate can improve the accuracy of diagnoses and patient outcomes. As such, the disclosed system can significantly reduce the cost and time needed to provide diagnostic information and can result in improved medical care for patients.
- a diagnosis e.g., measuring heart rate and/or respiratory rate
- measured heart rate and/or respiratory rate can improve the accuracy of diagnoses and patient outcomes.
- the disclosed system can significantly reduce the cost and time needed to provide diagnostic information and can result in improved medical care for patients.
- the present disclosure provides specific algorithmic approaches for measuring heart rate and/or respiratory rate from imagery captured by a camera such as a smartphone camera or other consumer-grade camera.
- a camera such as a smartphone camera or other consumer-grade camera.
- the example experimental results described herein show that example implementations of the proposed systems were able to accurately measure heart rate and/or respiratory rate from imagery captured by a camera such as a smartphone camera or other consumer-grade camera.
- the proposed techniques have implications for a very large population of patients because they do not, in principle, require specialized equipment. Specifically, to date, measurement of heart rate and/or respiratory rate has required specialized equipment and/or a trained professional. This limits evaluation of a patient’s health to in-person visits. By contrast, the proposed techniques require only imagery of the user’s (e.g., the user’s finger, torso, etc.). Therefore, improved access to medical care is enabled. Similarly, the computational costs associated with obtaining accurate heart rate and/or respiratory rate measurements can be reduced.
- a user may be provided with controls allowing the user to make an election as to both if and when systems, programs, or features described herein may enable collection of user information (e.g., photographs).
- user information e.g., photographs
- certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user’s identity may be treated so that no personally identifiable information can be determined for the user.
- the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
- an entirety of the measurement algorithm(s) is implemented at a single device (e.g., the patient’s device, the medical professional’s device, a server device, etc.).
- some portions of the measurement algorithm(s) can be implemented at a first device (e.g., the patient’s device or the medical professional’s device) while other portions of the algorithm(s) can be implemented at a second device (e.g., a server device).
- certain data such as patient images and/or patient metadata may never leave the local device (e.g., the patient’s device). Instead, in some implementations, only an intermediate data that is less interpretable is transmitted from the local device to the server device. This arrangement can improve patient privacy.
- heart rate and respiratory rate can be measured using only imagery captured by a low-cost, general consumer grade camera (e.g., as included in a smartphone, tablet, laptop, etc.).
- the tool can be used in a home, pharmacy, or primary care setting to improve disease screening and help with diagnosis and management of disease.
- the HR algorithm used PPG signals measured from the study participants placing their finger over the rear camera, and the enrollment for the corresponding validation study was stratified to ensure diversity across skin tones.
- the RR algorithm used video captures of the face and upper torso, and the enrollment for the corresponding validation study was stratified to capture participants with and without chronic respiratory conditions.
- FIG. 1 A shows an example “setup” of how measurements are taken using a user device having an integrated camera, such as a rear-facing camera — with the finger over the rear-facing camera for HR.
- FIGS. 1 B-C show example user interfaces for guiding a user to perform the HR evaluation. Following an onboarding process of FIG. 1 B , a plurality of image frames at a series of respective times, e.g. video frames, are captured.
- the first diagram of FIG. 1 C shows the display on a screen of user device at a moment during this process, and the fourth shows a display of the result.
- FIG. 1 D shows the measurements/results for mean absolute percent error (MAPE) for HR.
- MPE mean absolute percent error
- An example algorithm provided by the present disclosure estimates HR by optically measuring the PPG waveform from participants’ fingertips and then extracting the dominant frequency.
- several rectangular regions of interest (ROI) can be selected (e.g., manually or automatically) from the video frames (e.g., linear RGB at 15 frames per second and at a resolution of 640 ⁇ 480 pixels).
- the chosen ROIs in some examples can be the full frame, the left half, the right half, the top half, and the bottom half of the frames. Since camera pixels are illuminated non-homogeneously, signal strength can have spatial variations across pixels.
- some example implementations of the proposed method simultaneously analyze different ROIs to identify one with the greatest signal-to-noise ratio (SNR).
- SNR signal-to-noise ratio
- Pixels in each ROI can be averaged per-channel (e.g. separately for a plurality of color channels, a respective average is formed, over the pixels of the ROI, of the intensity values for each channel) to reduce the effects of sensor and quantization noise.
- the images are RGB images, this could be done for each of the RGB channels, to obtain a respective intensity value for each channel and each ROI in each of the image frames.
- the corresponding intensity values over the set of image frames form a respective waveform, e.g. a smoothed (temporal) signal.
- the pulsatile blood volume changes were present as the AC components in these smoothed signals.
- the three RGB waveforms can then be weighted to predict a single PPG waveform (e.g., weights 0.67, 0.33, and 0 for RGB respectively were empirically determined via grid search) for each ROI. Note that in the case that one or more of the channels (e.g. blue) is determined to be of low importance, it may be omitted from the per-channel averaging.
- the resulting single PPG waveforms for each ROI can be bandpass filtered, e.g. based on respective high and/or low frequency thresholds, to remove low- and/or highfrequency noise unlikely to be valid HR.
- Example filter cut-off frequencies (thresholds) corresponded to a low of 30 beats/min and high of 360 beats/min.
- large amplitude changes in PPGs due to motion can be suppressed by limiting maximum allowed changes in amplitudes based on an amplitude change threshold, such as for example 3x of moving average values.
- frequency domain representations of PPGs can be computed using the Fast Fourier Transform (FFT), from which the dominant frequencies with maximum power can be identified (e.g. a respective dominant frequency for each ROI).
- FFT Fast Fourier Transform
- a respective power of the frequency may be calculated (the respective amplitude of the FFT), and the frequency with maximum power can be identified.
- the respective power for each at least some of the frequencies (“base frequencies”) can be augmented based on the power of one or more harmonics of that frequency.
- the powers of the base frequencies can be computed by a process comprising summing the powers of their first, second, and third harmonics (e.g. and adding the resulting sum to the power of the base frequency).
- SNRs can be estimated for each ROI by computing the ratio between the power of the dominant frequency and the powers of non-dominant frequencies (e.g. a sum of the respective powers output by the FFT of the frequencies other than the dominant frequency) on a logarithmic scale.
- ROIs can be filtered to only those with a SNR greater than a threshold (e.g. ⁇ 0 dB).
- the dominant frequency of the ROI with the highest SNR can be reported. If no such ROI exists, then the reporting can indicate no HR. In a variant, the reporting can indicate no HR if two or more of the ROI both have the highest SNR and have respective dominant frequencies which differ by more than a threshold amount.
- An example study performed a prospective observational clinical validation study to assess the accuracy of the study algorithm in estimating HR in individuals.
- the study included participants having diverse skin tones. Specifically, study enrollment was stratified into 3 skin-tone subgroups (mapped to Fitzpatrick skin types) to ensure and evaluate broad representation.
- readings from participants’ cheek skin were collected using a Pantone Capsure color matcher colorimeter (X-Rite, Grand Rapids, MI).
- the exercise protocol was modified (with an IRB (Institutional Review Board) amendment) to achieve higher participant HR: participants were encouraged to achieve 75% of their maximal HR, which was calculated by subtracting the participant’s age from 220 beats/min.
- Exercise was completed either when the goal HR was achieved or when the participant asked to stop.
- the data were collected with flash off and under regular ambient light. Lighting conditions were controlled using two overhead and one front light emitting diode (LED) lights.
- the brightness level of the study environment was measured by a Lux meter (LT300 Light Meter, Extech, Nashua, NH) prior to each study. Measured brightness values were between 160 and 200 Lux for regular ambient light, and between 95 and 110 Lux for dim light.
- the absolute error of each paired measurement was calculated as the absolute value of the difference between the algorithm-estimated and reference HR values.
- the mean absolute error (MAE) was the mean value of all absolute errors.
- the absolute error from each paired measurement was divided by the reference value for that measurement and multiplied by 100 to produce the absolute percentage error.
- the mean absolute percentage error (MAPE) was the mean value for all absolute percent error values. The four (three at rest and one post-exercise; fewer if missing data) values from each participant were treated as statistically independent as the clustering effects (intra-participant correlation) were observed to be minimal.
- the MAPE was the primary study performance criterion, as recommended by the current standards for HR monitoring devices. The standard deviation and 95th percentiles were also computed. Sign tests were used to determine whether the absolute percentage errors were significantly ⁇ 5%, both for the entire group of participants and the 3 skin-tone subgroups. Bland-Altman plots were used to visualize the agreement between the estimated values and the reference measurements and assess for any proportional bias (trends in the error with increasing values). The subgroup analysis across the three skin-tone subgroups was pre-specified.
- HR data collection was planned for approximately 100 participants. Enrollment up to a maximum of 150 participants was allowed as it was anticipated that some enrolled participants would be excluded prior to contributing HR data because they failed to meet the required skin tone distribution or because they were not able to exercise.
- Requirements for participant enrollment termination included ⁇ 60 paired HR measurements in the dark skin tone subgroup and ⁇ 20% of the post-exercise reference HR >100 beats/min.
- the study hypothesis was that MAPE was less than 5% in all of the 3 skin-tone subgroups. To estimate the sample size required for the study, an IRB-approved feasibility study was first conducted with a different set of 55 participants and similar measurements both at rest and post-exercise. In that study, the MAPE ⁇ standard deviation was 0.91% ⁇ 3.68%.
- the power to detect a MAPE > 5% was > 0.8.
- the MAPE of the overall study population was 1.63%, which was significantly lower than the pre-specified study target of 5% (p ⁇ 0.001).
- the MAPE of 1.45% at rest and 2.39% post-exercise were also lower than the 5% target (p ⁇ 0.001 for both).
- the MAPE showed a left-skewed distribution with a long tail (median, 1.14%; range, 0.0-50.6%).
- the MAPE by skin-tone subgroup was 1.77% for subgroup 1, 1.32% for subgroup 2, and 1.77% for subgroup 3, all of which met the study target of ⁇ 5% (p ⁇ 0.001 for all subgroups). No significant variation in MAPE was found across the three different lighting conditions.
- FIG. 1 E shows an example “setup” of how measurements are taken using a video of the participant (e.g. a set of image frames captured at a series of corresponding times) via the front-facing camera.
- FIGS. 1 F-G show example user interfaces for guiding a user to perform the RR evaluation. Following an onboarding process of FIG. 1 F , a plurality of image frames at a series of respective times, e.g. video frames, are captured.
- the first diagram of FIG. 1 G shows the display on a screen of user device at a moment during this process, and the fourth shows a display of the result.
- FIG. 1 H shows the measurements/results for mean absolute error (MAE) for RR.
- MAE mean absolute error
- An example contactless method proposed herein can estimate RR by performing motion analysis in a ROI of the video stream that includes the base of the neck, shoulder line and upper torso of the participant.
- One challenge is that variations in video due to respiratory motions are hard to distinguish from noise.
- the proposed techniques can use Eulerian, phase-based motion processing that is particularly suited for analyzing subtle motions.
- the position at each pixel of one or more pixels in the frame can be represented by the phase of spatially localized sinusoids in multiple scales (frequencies).
- the phase-based motion analysis can amplify small motions by modifying local phase variations in a complex steerable pyramid representation of the video.
- a computing system can compute the local phase over time at every spatial scale and orientation of a steerable pyramid. Then, the computing system can temporally bandpass these phases to isolate specific temporal frequencies relevant to a given application and remove any temporal DC component. These temporally bandpassed phases can correspond to motion in different spatial scales and orientations. To synthesize magnified motion, the computing system can multiply the bandpassed phases by an amplification factor. These amplified phase differences can then be used magnify the motion in the sequence by modifying the phases of each coefficient by this amount for each frame.
- the position can be determined using Riesz pyramids as described in Wadhwa et al., Riesz pyramids for fast phase-based video magnification. 2014 IEEE International Conference on Computational Photography (ICCP). Published online 2014. doi:10.1109/iccphot.2014.6831820. Additional example techniques that can be used are described in: Wadhwa et al., Phase-based video motion processing, ACM Transactions on Graphics, Volume 32, Issue 4, July 2013, Article No.: 80, pp 1-10; and Wadhwa et al., Eulerian Video Magnification and Analysis, Communications of the ACM, January 2017, Vol. 60 No. 1, Pages 87-95.
- the spatial phases can then be transformed into optical flow by linearly approximating the position implied by each phase coefficient and averaging across scales.
- the phase and optical flow computation can be sped up to achieve real-time processing (1-4 ms per frame on Pixel 3a and Pixel 4 mobile devices).
- a spectral-spatial ensemble can be built in the following way.
- the respiratory ROI, together with the four quadrants obtained by equally subdividing the ROI into five (or any other plural number) defined regions, over each of which the vertical component of the optical flow (e.g. the component parallel to a “vertical” direction which may be defined as parallel to one of the sides of the image frames, typically a direction in which the torso is spaced from the head) can be averaged.
- This can result in five respiratory waveforms.
- frequency-domain representations for each of these respiratory waveforms can be computed via FFT, from which power spectra can be computed.
- the power spectra can then be aggregated to obtain a final ensembled power spectrum.
- Bandpass-filtering can be performed, e.g. based on respective high and/or low frequency thresholds, to remove low and/or high frequencies unlikely to represent valid RRs.
- Example filter cut-off frequencies (thresholds) corresponded to a low of 6 breaths/min and a high of 60 breaths/min.
- the maximum power frequency and the corresponding SNR value can be computed from the ensembled power spectrum.
- the waveform corresponding to the entire ROI can be used for displaying the breathing pattern to the user in the mobile app.
- the algorithm can fall back on a time domain estimation method based on counting zero crossings of the waveform corresponding to the entire ROI whenever the SNR obtained via the FFT-based method is lower than a certain threshold.
- Two versions of the algorithm were tested, differing only in terms of this threshold: SNR ⁇ -6.0 dB (“version A”) and SNR ⁇ -4.0 dB (“version B”).
- version A SNR ⁇ -6.0 dB
- version B SNR ⁇ -4.0 dB
- the higher value for the threshold in version B invoked the time domain estimation method more often, which was hypothesized to improve accuracy by improving robustness to irregular breathing.
- Chronic respiratory conditions included moderate or severe chronic obstructive pulmonary disease (COPD) and asthma that was not well-controlled based on specific study criteria. Also, participants with significant tremor were excluded.
- COPD chronic obstructive pulmonary disease
- RR was manually counted and recorded by two research coordinators.
- the two observers counted the number of breaths independently and blinded to the algorithm-estimated results.
- the agreement between the two measurements was high (Pearson correlation coefficient: 0.962; mean difference: 0.48 ⁇ 0.88 breaths/min; range, 0-4).
- the present disclosure provides the results of two example prospective clinical studies validating the performance of example smartphone algorithms to estimate HR and RR.
- Both example algorithms showed high accuracy compared to the reference standard vital sign measurements, with HR within 5% and RR within 3 breaths/min (the pre-specified targets).
- the HR estimation was robust across skin tones, and the RR estimation generalized to participants with common chronic respiratory conditions, COPD, and asthma.
- the accuracy of the HR algorithm is especially notable.
- a MAE less than 5 beats/min or a MAPE less than 10% are standard accuracy thresholds for HR monitors.
- the MAE of 1.32 beats/min in HR is lower than that reported for contemporary wearable devices (4.4 to 10.2 beats/min at rest), albeit with several differences in study design and population.
- the MAPE of 1.63% is comparable to the performance of current wearable devices. For example, Shcherbina et al. tested six wrist worn devices to show a median error ⁇ 5% for all across various activities and a median error of 2.0% for the best-performing device.
- FIG. 2 A depicts an example client-server environment according to example embodiments of the present disclosure.
- FIG. 2 A depicts a user computing device and a server system that communicate over a network.
- the computing device can be a personal electronic device such as a smartphone, tablet, laptop, and so on.
- the computing device can include an image capture system, at least a portion of an image analysis system, and user data.
- the image capture system can capture one or more images of a patient.
- the image analysis system can determine a heart rate and/or a respiratory rate according to any of the techniques described herein.
- the computing device can transmit the captured image(s) to the server computing device.
- the image analysis system can generate intermediate data for or from one or more images. In this way, the computing device can transmit intermediate data representing the image, rather than the image itself. This can reduce the amount of bandwidth needed to transmit the images to the server computing system.
- the user data can be stored in a local data storage device and can include user clinical data, user demographic data, and/or user medical history data. This information can be transmitted to the server computing system as needed with user permission.
- the image analysis system at the user computing device can include a context component that generates a feature representation for the user data.
- the image analysis system can combine one or more image intermediate data and the feature representation data for the user data.
- the server computing system can include some or all of an image analysis system.
- the image analysis system can determine a heart rate and/or a respiratory rate according to any of the techniques described herein.
- the server computing system can receive one or more of: image data, one or more intermediate data generated from the image data, user data, or other forms of data. Any and/or all of these types of data can be received at the server computing system and used to generate one or more output such as image analyses (e.g., heart rate and/or respiratory rate) and/or other physical assessment predictions.
- image analyses e.g., heart rate and/or respiratory rate
- the image analysis system outputs can be transmitted to the computing device or to another third-party device as needed and approved by the user.
- FIG. 2 B depicts an example block diagram of a system for providing physical assessment assistance according to example embodiments of the present disclosure.
- the computing device is associated with a medical professional (e.g., a doctor (e.g., optometrist, ophthalmologist, radiologist, dermatologist, etc.), a nurse practitioner, and so on).
- the medical professional can utilize the computing device to obtain aid during their physical assessment process.
- the computing device can include an image capture system (e.g., a camera and associated software), a physical assessment assistance system, and a display.
- the physical assessment assistance system can include some or all of an image analysis system and medical history data.
- the medical professional can use the computing device to capture one or more images of the patient using the image capture system.
- the physical assessment assistance system can process the imagery locally, generate intermediate data locally, or transmit the raw image data to the server computing system.
- medical history data can be processed locally to generate a feature representation or transmitted to the server computing system.
- the physical assessment assistance system includes the full image analysis system and thus can generate image analyses without transmitting data to the server computing system.
- the physical assessment assistance system transmits data to the server computing system.
- the image analysis system at the server computing system can generate one or more outputs such as image analyses or other physical assessment predictions and transmit the data back to the physical assessment assistance system for display to the medical professional in the display at the computing device.
- FIG. 2 C depicts an example block diagram of a system for providing physical assessment assistance according to example embodiments of the present disclosure.
- the patient is not physically present with the medical professional. Instead, the patient uses a computing device with an image capture system to transmit one or more images (and potentially user data) to the computing device associated with the medical professional and/or the server computing system via a network.
- the computing device receives the one or more images from the computing device associated with the patient, the process can proceed as described above with respect to FIGS. 2 A or 2 B .
- the medical professional can then transmit any relevant outputs such as physical assessment information to the computing device of the patient.
- FIG. 3 depicts a block diagram of an example computing system 100 according to example embodiments of the present disclosure.
- the system 100 includes a user computing device 102 and a server computing system 130 that are communicatively coupled over a network 180 .
- the user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
- a personal computing device e.g., laptop or desktop
- a mobile computing device e.g., smartphone or tablet
- a gaming console or controller e.g., a gaming console or controller
- a wearable computing device e.g., an embedded computing device, or any other type of computing device.
- the user computing device 102 includes one or more processors 112 and a memory 114 .
- the one or more processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 114 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 114 can store data 116 and instructions 118 which are executed by the processor 112 to cause the user computing device 102 to perform operations.
- the user computing device 102 can store or include one or more image analysis systems 120 (e.g. software program(s)).
- the image analysis systems 120 can be or can otherwise include various image analysis systems such as heart rate determination systems and/or respiratory rate determination systems.
- the systems 120 can implement any of the methods described herein.
- the one or more image analysis systems 120 can be received from the server computing system 130 over network 180 , stored in the user computing device memory 114 , and then used or otherwise implemented by the one or more processors 112 .
- the user computing device 102 can implement multiple parallel instances of a single image analysis system 120 (e.g., to perform parallel image analysis across multiple instances of imagery).
- one or more image analysis systems 140 can be included in or otherwise stored and implemented by the server computing system 130 that communicates with the user computing device 102 according to a client-server relationship.
- the image analysis systems 140 can be implemented by the server computing system 140 as a portion of a web service (e.g., a physical assessment service).
- the user computing device 102 can also include one or more user input components 122 that receives user input.
- the user input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus).
- the touch-sensitive component can serve to implement a virtual keyboard.
- Other example user input components include a microphone, a traditional keyboard, a camera, or other means by which a user can provide user input.
- the server computing system 130 includes one or more processors 132 and a memory 134 .
- the one or more processors 132 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 134 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 134 can store data 136 and instructions 138 which are executed by the processor 132 to cause the server computing system 130 to perform operations.
- the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances in which the server computing system 130 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof.
- the server computing system 130 can store or otherwise include one or more image analysis systems 140 .
- the image analysis systems 140 can be or can otherwise include various image analysis systems such as heart rate determination systems and/or respiratory rate determination systems.
- the systems 140 can implement any of the methods described herein.
- the network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links.
- communication over the network 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
- FIG. 3 illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can be used as well.
- FIG. 4 depicts a flow chart diagram of an example method to measure heart rate according to example embodiments of the present disclosure.
- FIG. 4 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement.
- the various steps of the method 400 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- the method 400 includes obtaining, by a computing system comprising one or more computing devices, a plurality of image frames captured by a camera while a digit of the subject is placed in a field of view of the camera.
- the digit can be a finger.
- the digit of the subject can be placed in physical contact with an outermost lens or cover of the camera during capture of the plurality of image frames.
- the digit of the subject can be adjacent to, but not in physical contact, with the outermost lens or cover of the camera.
- the method 400 includes measuring, by the computing system, a photoplethysmography (PPG) waveform from the plurality of image frames.
- measuring, by the computing system, the PPG waveform from the plurality of image frames can include selecting, by the computing system, regions of interest (ROIs) from the plurality of image frames; averaging, by the computing system, pixels in each of the regions of interest on a per-channel basis to form a plurality of per-channel waveforms; and computing, by the computing system, the PPG waveform as a weighted average of the plurality of per-channel waveforms.
- ROIs can be manually or automatically selected.
- the ROIs can include an entirety of the frame and each of the four quadrants of the frame.
- the ROIs can include a top half, a bottom half, a left half, and a right half.
- the ROIs can include a bounding box associated with a digit.
- each of the ROIs may be defined as a respective portion of the bounding box (e.g. a top half, a bottom half, a left half, and a right half), or a respective bounding box.
- the bounding box(es) can be generated by an object detection model provided with the frame as input.
- the channels of the frame can include color channels in a color space such as red, green, and blue channels in an RGB color space.
- measuring the PPG waveform can also include applying, by the computing system, one or more bandpass filters to the PPG waveform.
- filter cut-off frequencies can correspond to a low of 30 beats/min and high of 360 beats/min.
- measuring the PPG waveform can also include smoothing, by the computing system, the PPG waveform using a maximum allowed change in amplitude which is a function of a moving average of PPG waveform values.
- a maximum allowed change in amplitude which is a function of a moving average of PPG waveform values.
- large amplitude changes in PPGs due to motion can be suppressed by limiting maximum allowed changes in amplitudes as 3x of moving average values.
- the method 400 includes extracting, by the computing system, a dominant frequency from the PPG waveform.
- extracting, by the computing system, the dominant frequency from the PPG waveform can include: generating, by the computing system, a Fast Fourier Transform representation of the PPG waveform; and extracting, by the computing system, the dominant frequency from the Fast Fourier Transform representation.
- extracting, by the computing system, the dominant frequency from the Fast Fourier Transform representation can include summing, by the computing system, powers of first, second, and third harmonics.
- extracting, by the computing system, the dominant frequency from the PPG waveform can include: determining, by the computing system, a signal to noise ratio for each region of interest by computing a ratio between a power of a dominant frequency and powers of non-dominant frequencies on a logarithmic scale; identifying, by the computing system, the region of interest with the largest signal to noise ratio; and extracting, by the computing system, the dominant frequency of the region of interest with the largest signal to noise ratio.
- the method 400 includes determining, by the computing system, a heart rate of the subject based on the dominant frequency extracted from the PPG waveform.
- the heart rate of the subject can be set equal to the dominant frequency extracted from the PPG waveform expressed in the format of beats per minute.
- the method 400 includes providing, by the computing system, the heart rate of the subject as an output.
- providing the heart rate as an output can include storing data representing the heart rate of the subject in a memory, providing data representing the heart rate of the subject for display (e.g., to the subject or a medical provider), and/or transmitting data representing the heart rate of the subject over a communications network.
- FIG. 5 depicts a flow chart diagram of an example method to measure respiratory rate according to example embodiments of the present disclosure.
- FIG. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement.
- the various steps of the method 500 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- the method 500 includes obtaining, by a computing system comprising one or more computing devices, a plurality of image frames captured by a camera while a subject is placed in a field of view of the camera. For example, a base of a neck, a shoulder line, and/or an upper torso of the subject can be depicted by the plurality of image frames.
- the method 500 includes determining, by the computing system, optical flow data for the plurality of image frames.
- determining, by the computing system, optical flow data for the plurality of image frames can include: performing, by the computing system, a Eulerian phase-based motion analysis on the plurality of image frames to generate phase-based motion data; and generating, by the computing system, the optical flow data for the plurality of image frames based on the phase-based motion data.
- determining the optical flow data can include determining, by the computing system, for each pixel of one or more pixels in each frame, a position represented by a phase of spatially localized sinusoids in multiple scales; and transforming, by the computing system, the phase of spatially localized sinusoids into optical flow data by linearly approximating the position implied by each phase coefficient and averaging across scales.
- the method 500 includes averaging, by the computing system, a vertical component of the optical flow data to generate a respiratory waveform.
- the method 500 includes generating, by the computing system, a frequency domain representation of the respiratory waveform.
- the method 500 includes determining, by the computing system, the respiratory rate of the subject based on the frequency domain representation of the respiratory waveform.
- steps 506 , 508 , and/or 510 can be performed for each of a plurality of regions of interest (ROIs).
- the method 500 can include selecting, by the computing system, a plurality of regions of interest; averaging, by the computing system, a vertical component of the optical flow data for each of the regions of interest to generate a plurality of respiratory waveforms respectively for the plurality of regions of interest; and determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms.
- the ROIs can be manually or automatically selected.
- the ROIs can include an entirety of the frame and each of the four quadrants of the frame.
- the ROIs can include a top half, a bottom half, a left half, and a right half.
- the ROIs can include one or more bounding boxes associated with a body, a neck, a shoulder line, and/or an upper torso of the subject.
- the bounding box(es) can be generated by an object detection model provided with the frame as input.
- determining, by the computing system, the respiratory rate of the subject based on the one or more waveforms can include: generating, by the computing system, a power spectrum from a frequency domain representation of each respiratory waveform; aggregating, by the computing system, the power spectra for the one or more respiratory waveforms to obtain a final ensembled power spectrum; and determining, by the computing system, the respiratory rate of the subject based on a maximum power frequency of the final ensembled power spectrum.
- bandpass-filtering can be performed to remove low and high frequencies unlikely to represent valid RRs.
- Example filter cut-off frequencies include a low of 6 breaths/min and a high of 60 breaths/min.
- the maximum power frequency and the corresponding SNR value can be computed from the ensembled power spectrum.
- the waveform corresponding to the entire ROI can be used for displaying the breathing pattern to the user in the mobile app.
- the method 500 includes providing, by the computing system, the respiratory rate of the subject as an output.
- providing the respiratory rate as an output can include storing data representing the respiratory rate of the subject in a memory, providing data representing the respiratory rate of the subject for display (e.g., to the subject or a medical provider), and/or transmitting data representing the respiratory rate of the subject over a communications network.
- the method 500 can include defaulting, by the computing system, to a time-domain estimation of the respiratory waveform when a signal to noise ratio associated with the frequency domain representation falls below a threshold.
- the technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems.
- the inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components.
- processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
- Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
Abstract
Generally, the present disclosure is directed to systems and methods for measuring heart rate and respiratory rate using a camera such as, for example, a smartphone camera or other consumer-grade camera. Specifically, the present disclosure presents and validates two algorithms that make use of smartphone cameras (or the like) for measuring heart rate (HR) and respiratory rate (RR) for consumer wellness use. As an example, HR can be measured by placing the finger of a subject over the rear-facing camera. As another example, RR can be measured via a video of the subject sitting still in front of the front-facing camera.
Description
- This application claims priority to and the benefit of U.S. Provisional Pat. Application No. 63/156,272, filed Mar. 3, 2021. U.S. Provisional Pat. Application No. 63/156,272 is hereby incorporated by reference in its entirety.
- The present disclosure relates generally to heart rate and respiratory rate measurements. More particularly, the present disclosure relates to measuring heart rate and respiratory rate from imagery captured by a camera such as a smartphone camera or other consumer-grade camera.
- Measurement of heart rate (HR) and respiratory rate (RR), two of the four cardinal vital signs–HR, RR, body temperature, and blood pressure–is often a starting point of physical assessment for both health and wellness. However, taking these standard measurements via a physical examination becomes challenging in telehealth, remote care, remote triage, remote physical assessment, remote monitoring, and consumer wellness settings, which are becoming increasing common. Although specialized measurement devices are commercially available for consumers and have the potential to motivate healthy behaviors, their cost and relatively low adoption limit general usage.
- Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.
- A system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- One general example aspect includes a computer-implemented method to measure heart rate of a subject. The computer-implemented method includes obtaining, by a computing system that includes one or more computing devices, a plurality of image frames captured by a camera while a digit of the subject is placed in a field of view of the camera. The method also includes measuring, by the computing system, a photoplethysmography (PPG) waveform from the plurality of image frames. The method also includes extracting, by the computing system, a dominant frequency from the PPG waveform. The method also includes determining, by the computing system, the heart rate of the subject based on the dominant frequency extracted from the PPG waveform. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations may include one or more of the following features. In some implementations, measuring, by the computing system, the PPG waveform from the plurality of image frames may include: selecting, by the computing system, regions of interest from the plurality of image frames; averaging, by the computing system, pixels in each of the regions of interest on a per-channel basis (e.g. separately for each of a plurality of color channels included in the image frame; e.g. up to three channels in the case of RGB images) to form a plurality of per-channel waveforms; and computing, by the computing system, the PPG waveform as a weighted average of the plurality of per-channel waveforms. Measuring the PPG waveform may include: applying, by the computing system, one or more bandpass filters to the PPG waveform. Measuring the PPG waveform may include: smoothing, by the computing system, the PPG waveform using a maximum allowed change in amplitude which is a function of a moving average of PPG waveform values. Extracting, by the computing system, the dominant frequency from the PPG waveform may include: generating, by the computing system, a fast Fourier transform representation of the PPG waveform; and extracting, by the computing system, the dominant frequency from the fast Fourier transform representation. Extracting, by the computing system, the dominant frequency from the fast Fourier transform representation may include: summing, by the computing system, powers of first, second, and third harmonics. Extracting, by the computing system, the dominant frequency from the PPG waveform may include: determining, by the computing system, a signal to noise ratio for each region of interest by computing a ratio between a power of a dominant frequency and powers of non-dominant frequencies on a logarithmic scale; identifying, by the computing system, the region of interest with the largest signal to noise ratio; and extracting, by the computing system, the dominant frequency of the region of interest with the largest signal to noise ratio. The digit may include a finger. The digit of the subject may be placed in physical contact with an outermost lens or cover of the camera during capture of the plurality of image frames. The computing system may consist of a user device, such as a user device having an integrated camera (e.g. a camera which is within the same cover (housing) as the one or more computing devices).
- Another general example aspect includes a computer-implemented method to measure respiratory rate of a subject. The computer-implemented method includes obtaining, by a computing system which may include one or more computing devices, a plurality of image frames captured by a camera while a subject is placed in a field of view of the camera. The method also includes determining, by the computing system, for each pixel of one or more pixels in each frame, a position represented by a phase of spatially localized sinusoids in multiple scales. The method also includes transforming, by the computing system, the phase of the spatially localized sinusoids into optical flow data by linearly approximating the position implied by each phase coefficient and averaging across scales. The method also includes determining, by the computing system, the respiratory rate of the subject based on the optical flow data. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- Implementations may include one or more of the following features. The computer-implemented method where determining, by the computing system, the respiratory rate of the subject based on the optical flow data may include: selecting, by the computing system, a plurality of regions of interest; averaging, by the computing system, a vertical component of the optical flow data for each of the regions of interest to generate a plurality of respiratory waveforms respectively for the plurality of regions of interest; and determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms. Determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms may include: generating, by the computing system, a power spectrum from a frequency domain representation of each respiratory waveform; and aggregating, by the computing system, the power spectra for the plurality of respiratory waveforms to obtain a final ensembled power spectrum; and determining, by the computing system, the respiratory rate of the subject based on a maximum power frequency of the final ensembled power spectrum. The computer-implemented method may include defaulting, by the computing system, to a time-domain estimation of the respiratory waveform when a signal to noise ratio associated with the frequency domain representation falls below a threshold. In some implementations, a base of a neck, a shoulder line, and an upper torso of the subject may be depicted by the plurality of image frames. Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.
- Another general example aspect includes a computer-implemented method to measure respiratory rate of a subject. The computer-implemented method includes obtaining, by a computing system may include one or more computing devices, a plurality of image frames captured by a camera while a subject is placed in a field of view of the camera. The method also includes determining, by the computing system, optical flow data for the plurality of image frames. The method also includes averaging, by the computing system, a vertical component of the optical flow data to generate a respiratory waveform. The method also includes generating, by the computing system, a frequency domain representation of the respiratory waveform. The method also includes determining, by the computing system, the respiratory rate of the subject based on the frequency domain representation of the respiratory waveform. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- A computing device may include: a camera; one or more processors; and one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing device to perform any of the methods described herein. One or more non-transitory computer-readable media may collectively store instructions that, when executed by one or more processors, cause a computing device to perform any of the methods described herein. Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium. These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, serve to explain the related principles.
- Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:
-
FIG. 1A shows an example “setup” of how measurements are taken — with the finger over the rear-facing camera for HR according to example embodiments of the present disclosure. -
FIGS. 1B-C show example user interfaces for guiding a user to perform the HR evaluation according to example embodiments of the present disclosure. -
FIG. 1D shows the measurements/results for mean absolute percent error (MAPE) for HR according to example embodiments of the present disclosure. -
FIG. 1E shows an example “setup” of how measurements are taken using a video of the participant via the front-facing camera according to example embodiments of the present disclosure. -
FIGS. 1F-G show example user interfaces for guiding a user to perform the RR evaluation according to example embodiments of the present disclosure. -
FIG. 1H shows the measurements/results for mean absolute error (MAE) for RR according to example embodiments of the present disclosure. -
FIG. 2A depicts an example block diagram of a system for analyzing imagery according to example embodiments of the present disclosure. -
FIG. 2B depicts an example block diagram of a system for analyzing imagery according to example embodiments of the present disclosure. -
FIG. 2C depicts an example block diagram of a system for analyzing imagery according to example embodiments of the present disclosure. -
FIG. 3 depicts a block diagram of an example computing system according to example embodiments of the present disclosure. -
FIG. 4 depicts a flow chart diagram of an example method to measure heart rate according to example embodiments of the present disclosure. -
FIG. 5 depicts a flow chart diagram of an example method to measure respiratory rate according to example embodiments of the present disclosure. - Reference numerals that are repeated across plural figures are intended to identify the same features in various implementations.
- Generally, the present disclosure is directed to systems and methods for measuring heart rate and/or respiratory rate using a camera such as, for example, a smartphone camera or other consumer-grade camera. Specifically, the present disclosure presents and validates two algorithms that make use of smartphone cameras (or the like) for measuring heart rate (HR) and respiratory rate (RR) for consumer wellness use. As an example, HR can be measured by placing the finger of a subject over the rear-facing camera and/or front-facing camera. As another example, RR can be measured via a video of the subject sitting still in front of the front-facing camera and/or rear-facing camera.
- More particularly, a first example approach for HR measurement leverages photoplethysmography (PPG) acquired using cameras, such as consumer-grade and/or embedded cameras such as those commonly found in laptops, tablets, smartphones, and/or user devices (e.g., front-facing and/or rear-facing cameras). PPG signals can be recorded by placing a finger over the camera lens, and the color changes captured in the video can be used to determine the oscillation of blood volume after each heartbeat.
- A second example approach measures RR from upper-torso videos obtained via a camera to track the physical motion of breathing. For example, a computing system can determine, for each pixel of one or more pixels in each video frame, a position represented by a phase of spatially localized sinusoids in multiple scales. To express this in another way, a plurality of sinusoidal functions (“sinusoids”) may be defined, each function describing an oscillatory average motion of respective corresponding portions of the video frames (e.g. one such sinusoidal function may describe oscillations in a top right corner portion of each of the video frames). The portions of the video frame corresponding to different ones of the sinusoidal functions overlap and have different corresponding sizes (scales). One of the parameters defining each sinusoidal function is a respective phase coefficient. Others of the parameters may be an amplitude of the motion, and parameters describing the portion of the video frames described by the sinusoidal function. Determining, for each of the one or more pixels, a position represented by the phase of the sinusoids can be considered as meaning that the one or more pixels are used to determine some or all parameters of the sinusoidal functions (e.g. the amplitude and the phase coefficient of each sinusoidal function), and in particular the phase coefficients of the plurality of sinusoidal functions. The sinusoids collectively imply, for each of the one or more pixels, a respective position on the subject which is shown by that pixel in any given one of the video frames. The computing system can transform the phase of spatially localized sinusoids into optical flow data by linearly approximating the position implied by each phase coefficient and averaging across scales (e.g. averaging over the sinusoidal functions, to determine the motion collectively implied by them). The computing system can determine the respiratory rate of the subject based on the optical flow data. Optical flow can refer to the pattern of the apparent motion of the subject in the video frames, caused by the motion of the subject relative to the camera. Optical flow can be assessed, for example, on a pixel-by-pixel basis.
- The present disclosure reports the performance of these two algorithms in prospective clinical validation studies, demonstrating reliable and consistent accuracy on diverse populations (e.g., in terms of objectively-measured skin tones) for HR and health status (e.g., with and without chronic pulmonary conditions) for RR.
- Specifically, in an example HR study of 95 participants (with a protocol that included both measurements at rest and post exercise), the mean absolute percent error (MAPE) ± standard deviation of the measurement was 1.6% ± 4.3%, which was significantly lower than the pre-specified goal of 5%. No significant differences in the MAPE were present across colorimeter-measured skin-tone subgroups.
- In an example RR study of 50 participants, the mean absolute error (MAE) was 0.78 ± 0.61 breaths/min, which was significantly lower than the pre-specified goal of 3 breath/min. The MAE was low in both healthy participants (0.70 ± 0.67 breaths/min), and participants with chronic respiratory conditions (0.80 ± 0.60 breaths/min). These results validate that smartphone camera-based techniques can accurately measure HR and RR across a range of pre-defined subgroups.
- The systems and methods described herein provide a number of technical effects and benefits. More particularly, the systems and methods of the present disclosure provide improved techniques for providing a diagnosis (e.g., measuring heart rate and/or respiratory rate) from imagery captured by a camera such as a smartphone camera or other consumer-grade camera. In addition, measured heart rate and/or respiratory rate can improve the accuracy of diagnoses and patient outcomes. As such, the disclosed system can significantly reduce the cost and time needed to provide diagnostic information and can result in improved medical care for patients.
- In particular, the present disclosure provides specific algorithmic approaches for measuring heart rate and/or respiratory rate from imagery captured by a camera such as a smartphone camera or other consumer-grade camera. The example experimental results described herein show that example implementations of the proposed systems were able to accurately measure heart rate and/or respiratory rate from imagery captured by a camera such as a smartphone camera or other consumer-grade camera.
- The proposed techniques have implications for a very large population of patients because they do not, in principle, require specialized equipment. Specifically, to date, measurement of heart rate and/or respiratory rate has required specialized equipment and/or a trained professional. This limits evaluation of a patient’s health to in-person visits. By contrast, the proposed techniques require only imagery of the user’s (e.g., the user’s finger, torso, etc.). Therefore, improved access to medical care is enabled. Similarly, the computational costs associated with obtaining accurate heart rate and/or respiratory rate measurements can be reduced.
- Further to the descriptions above, a user may be provided with controls allowing the user to make an election as to both if and when systems, programs, or features described herein may enable collection of user information (e.g., photographs). In addition, certain data may be treated in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user’s identity may be treated so that no personally identifiable information can be determined for the user. Thus, the user may have control over what information is collected about the user, how that information is used, and what information is provided to the user.
- In some implementations, an entirety of the measurement algorithm(s) is implemented at a single device (e.g., the patient’s device, the medical professional’s device, a server device, etc.). In other implementations, some portions of the measurement algorithm(s) can be implemented at a first device (e.g., the patient’s device or the medical professional’s device) while other portions of the algorithm(s) can be implemented at a second device (e.g., a server device). In such fashion, in some implementations, certain data such as patient images and/or patient metadata may never leave the local device (e.g., the patient’s device). Instead, in some implementations, only an intermediate data that is less interpretable is transmitted from the local device to the server device. This arrangement can improve patient privacy.
- Thus, the present disclosure demonstrates the surprising result that heart rate and respiratory rate can be measured using only imagery captured by a low-cost, general consumer grade camera (e.g., as included in a smartphone, tablet, laptop, etc.). The tool can be used in a home, pharmacy, or primary care setting to improve disease screening and help with diagnosis and management of disease.
- With reference now to the Figures, example embodiments of the present disclosure will be discussed in further detail.
- Two separate example prospective studies were conducted to validate the performance of smartphone-based HR and RR measurements. The HR algorithm used PPG signals measured from the study participants placing their finger over the rear camera, and the enrollment for the corresponding validation study was stratified to ensure diversity across skin tones. The RR algorithm used video captures of the face and upper torso, and the enrollment for the corresponding validation study was stratified to capture participants with and without chronic respiratory conditions. The following sections detail each of the two studies. While these algorithms used in these example studies demonstrate aspects of the present disclosure, the present disclosure is not limited to the example algorithms used in the studies.
-
FIG. 1A shows an example “setup” of how measurements are taken using a user device having an integrated camera, such as a rear-facing camera — with the finger over the rear-facing camera for HR.FIGS. 1B-C show example user interfaces for guiding a user to perform the HR evaluation. Following an onboarding process ofFIG. 1B , a plurality of image frames at a series of respective times, e.g. video frames, are captured. The first diagram ofFIG. 1C shows the display on a screen of user device at a moment during this process, and the fourth shows a display of the result.FIG. 1D shows the measurements/results for mean absolute percent error (MAPE) for HR. - An example algorithm provided by the present disclosure estimates HR by optically measuring the PPG waveform from participants’ fingertips and then extracting the dominant frequency. First, several rectangular regions of interest (ROI) can be selected (e.g., manually or automatically) from the video frames (e.g., linear RGB at 15 frames per second and at a resolution of 640×480 pixels). The chosen ROIs in some examples can be the full frame, the left half, the right half, the top half, and the bottom half of the frames. Since camera pixels are illuminated non-homogeneously, signal strength can have spatial variations across pixels. Thus, some example implementations of the proposed method simultaneously analyze different ROIs to identify one with the greatest signal-to-noise ratio (SNR).
- Pixels in each ROI can be averaged per-channel (e.g. separately for a plurality of color channels, a respective average is formed, over the pixels of the ROI, of the intensity values for each channel) to reduce the effects of sensor and quantization noise. For example, if the images are RGB images, this could be done for each of the RGB channels, to obtain a respective intensity value for each channel and each ROI in each of the image frames. For each given one of the ROIs and each of the channels, the corresponding intensity values over the set of image frames form a respective waveform, e.g. a smoothed (temporal) signal. The pulsatile blood volume changes were present as the AC components in these smoothed signals. The three RGB waveforms can then be weighted to predict a single PPG waveform (e.g., weights 0.67, 0.33, and 0 for RGB respectively were empirically determined via grid search) for each ROI. Note that in the case that one or more of the channels (e.g. blue) is determined to be of low importance, it may be omitted from the per-channel averaging.
- The resulting single PPG waveforms for each ROI can be bandpass filtered, e.g. based on respective high and/or low frequency thresholds, to remove low- and/or highfrequency noise unlikely to be valid HR. Example filter cut-off frequencies (thresholds) corresponded to a low of 30 beats/min and high of 360 beats/min. Next, large amplitude changes in PPGs due to motion can be suppressed by limiting maximum allowed changes in amplitudes based on an amplitude change threshold, such as for example 3x of moving average values. Then, frequency domain representations of PPGs can be computed using the Fast Fourier Transform (FFT), from which the dominant frequencies with maximum power can be identified (e.g. a respective dominant frequency for each ROI). That is, for each of a plurality of frequencies (e.g. narrow frequency range), for each ROI a respective power of the frequency may be calculated (the respective amplitude of the FFT), and the frequency with maximum power can be identified. Optionally, the respective power for each at least some of the frequencies (“base frequencies”) can be augmented based on the power of one or more harmonics of that frequency. For example, because the PPG signals are periodic with multiple harmonics, optionally the powers of the base frequencies can be computed by a process comprising summing the powers of their first, second, and third harmonics (e.g. and adding the resulting sum to the power of the base frequency). SNRs can be estimated for each ROI by computing the ratio between the power of the dominant frequency and the powers of non-dominant frequencies (e.g. a sum of the respective powers output by the FFT of the frequencies other than the dominant frequency) on a logarithmic scale. ROIs can be filtered to only those with a SNR greater than a threshold (e.g. ≥0 dB). The dominant frequency of the ROI with the highest SNR can be reported. If no such ROI exists, then the reporting can indicate no HR. In a variant, the reporting can indicate no HR if two or more of the ROI both have the highest SNR and have respective dominant frequencies which differ by more than a threshold amount.
- An example study performed a prospective observational clinical validation study to assess the accuracy of the study algorithm in estimating HR in individuals. In an attempt to ensure that the algorithm provides benefits for all potential users from diverse groups of people, the study included participants having diverse skin tones. Specifically, study enrollment was stratified into 3 skin-tone subgroups (mapped to Fitzpatrick skin types) to ensure and evaluate broad representation. In an attempt to objectively measure skin tone, readings from participants’ cheek skin were collected using a Pantone Capsure color matcher colorimeter (X-Rite, Grand Rapids, MI).
- Each participant underwent four 30-second data collection episodes with their index finger (of a hand of their choice) held directly over the study phone camera. Three of the 30-second episodes were collected at rest under various ambient brightness/lighting conditions: (1) with camera flash on and under regular ambient light, (2) with flash off and under regular ambient light, and (3) with flash off and under dim light. The fourth episode was collected post-exercise. In the original protocol, participants were instructed to ride a stationary bicycle for 30 seconds as strenuously as possible against light to medium resistance. After enrolling 37 participants, the exercise protocol was modified (with an IRB (Institutional Review Board) amendment) to achieve higher participant HR: participants were encouraged to achieve 75% of their maximal HR, which was calculated by subtracting the participant’s age from 220 beats/min. Exercise was completed either when the goal HR was achieved or when the participant asked to stop. The data were collected with flash off and under regular ambient light. Lighting conditions were controlled using two overhead and one front light emitting diode (LED) lights. The brightness level of the study environment was measured by a Lux meter (LT300 Light Meter, Extech, Nashua, NH) prior to each study. Measured brightness values were between 160 and 200 Lux for regular ambient light, and between 95 and 110 Lux for dim light.
- The study was conducted using a mobile app deployed to a
Pixel 3 smartphone running Android 10 (Google LLC, Mountain View, CA). HR estimation using the app was generally completed by the study participants following the in-app instructions, with the coordinators providing feedback on usage when needed. The reference HR was measured simultaneously during each data collection episode using a Masimo MightySat® (Masimo, Irvine, CA), which is US Food and Drug Administration-cleared for fingertip measurement of pulse rate. The measurements were conducted in accordance with the vendor’s manual and taken at the end of each episode. - Each participant contributed up to 3 HR measurements at rest (with different lighting conditions), and up to 1 post-exercise. Measurements were paired observations: the algorithm-estimated HR and the reference HR from the pulse oximeter. For each algorithm measurement, up to three tries were allowed, and the number of tries required was recorded. The baseline characteristics (e.g., demographic information, skin characteristic information) of the participants who did not successfully complete algorithm estimation were compared with the rest of the study population using Fisher’s exact test. A paired measurement was dropped if either the algorithm estimation or reference measurement failed.
- The absolute error of each paired measurement was calculated as the absolute value of the difference between the algorithm-estimated and reference HR values. The mean absolute error (MAE) was the mean value of all absolute errors. Similarly, the absolute error from each paired measurement was divided by the reference value for that measurement and multiplied by 100 to produce the absolute percentage error. The mean absolute percentage error (MAPE) was the mean value for all absolute percent error values. The four (three at rest and one post-exercise; fewer if missing data) values from each participant were treated as statistically independent as the clustering effects (intra-participant correlation) were observed to be minimal.
- The MAPE was the primary study performance criterion, as recommended by the current standards for HR monitoring devices. The standard deviation and 95th percentiles were also computed. Sign tests were used to determine whether the absolute percentage errors were significantly <5%, both for the entire group of participants and the 3 skin-tone subgroups. Bland-Altman plots were used to visualize the agreement between the estimated values and the reference measurements and assess for any proportional bias (trends in the error with increasing values). The subgroup analysis across the three skin-tone subgroups was pre-specified.
- HR data collection was planned for approximately 100 participants. Enrollment up to a maximum of 150 participants was allowed as it was anticipated that some enrolled participants would be excluded prior to contributing HR data because they failed to meet the required skin tone distribution or because they were not able to exercise. Requirements for participant enrollment termination included ≥60 paired HR measurements in the dark skin tone subgroup and ≥20% of the post-exercise reference HR >100 beats/min. The study hypothesis was that MAPE was less than 5% in all of the 3 skin-tone subgroups. To estimate the sample size required for the study, an IRB-approved feasibility study was first conducted with a different set of 55 participants and similar measurements both at rest and post-exercise. In that study, the MAPE ± standard deviation was 0.91% ± 3.68%. Assuming double the mean and SD (e.g., 1.82% and 7.36%, respectively), a minimum of two paired measurements per participant, a skin-tone subgroup of ~25 participants, and some dropout from incomplete data, the power to detect a MAPE > 5% was > 0.8.
- A total of 101 participants were enrolled. After excluding one participant who was found to meet exclusion criteria, there were 100 valid enrollees. Among these, 3 were withdrawn due to skin tone distribution requirements, and 2 were withdrawn during data collection due to difficulty in data collection (such as inability to hold a phone properly or to obtain reference HR data). Thus, 95 participants completed data collection. The participants had a mean age of 41.8 years, 75% were female, and skin-tone subgroups were evenly distributed as planned: 33% were subgroup 1 (very light, light and intermediate), 34% were subgroup 2 (tan and brown), and 34% were subgroup 3 (dark).
- From these participants, 379 total recordings were attempted. A valid HR was successfully obtained (see details on SNR in Methods) in 361 cases (95.3%). The success rate increased with retries up to 3 times: 316 measurements (83.4%) were successful on the first try, another 31 measurements (cumulative 91.6%) on the second try, and another 14 measurements (cumulative 95.3%) on the third try. The baseline characteristics of the 14 participants for whom HR values were not successfully reported by the study app for at least one measurement (due to low SNR) did not differ significantly from the remaining participants. In addition, a corresponding valid reference HR was not obtained for 9 recordings from 4 participants. The remaining 352 recordings with paired valid reference HR contributed to the final analysis. The average reference HR was 79.8 ± 14.6 beats/min overall, 75.5 ± 11.2 beats/min at rest, and 92.9 ± 16.6 beats/min post-exercise.
- Compared to the reference HR, the MAPE of the overall study population was 1.63%, which was significantly lower than the pre-specified study target of 5% (p<0.001). The MAPE of 1.45% at rest and 2.39% post-exercise were also lower than the 5% target (p<0.001 for both). The MAPE showed a left-skewed distribution with a long tail (median, 1.14%; range, 0.0-50.6%). The MAPE by skin-tone subgroup was 1.77% for
subgroup 1, 1.32% forsubgroup 2, and 1.77% forsubgroup 3, all of which met the study target of <5% (p<0.001 for all subgroups). No significant variation in MAPE was found across the three different lighting conditions. -
FIG. 1E shows an example “setup” of how measurements are taken using a video of the participant (e.g. a set of image frames captured at a series of corresponding times) via the front-facing camera.FIGS. 1F-G show example user interfaces for guiding a user to perform the RR evaluation. Following an onboarding process ofFIG. 1F , a plurality of image frames at a series of respective times, e.g. video frames, are captured. The first diagram ofFIG. 1G shows the display on a screen of user device at a moment during this process, and the fourth shows a display of the result.FIG. 1H shows the measurements/results for mean absolute error (MAE) for RR. - An example contactless method proposed herein can estimate RR by performing motion analysis in a ROI of the video stream that includes the base of the neck, shoulder line and upper torso of the participant. One challenge is that variations in video due to respiratory motions are hard to distinguish from noise. The proposed techniques can use Eulerian, phase-based motion processing that is particularly suited for analyzing subtle motions. In each video frame, the position at each pixel of one or more pixels in the frame can be represented by the phase of spatially localized sinusoids in multiple scales (frequencies).
- In some examples, the phase-based motion analysis can amplify small motions by modifying local phase variations in a complex steerable pyramid representation of the video. For example, a computing system can compute the local phase over time at every spatial scale and orientation of a steerable pyramid. Then, the computing system can temporally bandpass these phases to isolate specific temporal frequencies relevant to a given application and remove any temporal DC component. These temporally bandpassed phases can correspond to motion in different spatial scales and orientations. To synthesize magnified motion, the computing system can multiply the bandpassed phases by an amplification factor. These amplified phase differences can then be used magnify the motion in the sequence by modifying the phases of each coefficient by this amount for each frame.
- As an example, in some implementations, the position can be determined using Riesz pyramids as described in Wadhwa et al., Riesz pyramids for fast phase-based video magnification. 2014 IEEE International Conference on Computational Photography (ICCP). Published online 2014. doi:10.1109/iccphot.2014.6831820. Additional example techniques that can be used are described in: Wadhwa et al., Phase-based video motion processing, ACM Transactions on Graphics, Volume 32,
Issue 4, July 2013, Article No.: 80, pp 1-10; and Wadhwa et al., Eulerian Video Magnification and Analysis, Communications of the ACM, January 2017, Vol. 60 No. 1, Pages 87-95. - Referring again to the proposed algorithm, to aggregate the information across scales and to obtain an intuitive representation of motion, the spatial phases can then be transformed into optical flow by linearly approximating the position implied by each phase coefficient and averaging across scales. Using the Halide high-performance image library, the phase and optical flow computation can be sped up to achieve real-time processing (1-4 ms per frame on Pixel 3a and
Pixel 4 mobile devices). - Ensembling can then be used to improve the predictive performance. A spectral-spatial ensemble can be built in the following way. The respiratory ROI, together with the four quadrants obtained by equally subdividing the ROI into five (or any other plural number) defined regions, over each of which the vertical component of the optical flow (e.g. the component parallel to a “vertical” direction which may be defined as parallel to one of the sides of the image frames, typically a direction in which the torso is spaced from the head) can be averaged. This can result in five respiratory waveforms. Next, frequency-domain representations for each of these respiratory waveforms can be computed via FFT, from which power spectra can be computed. The power spectra can then be aggregated to obtain a final ensembled power spectrum. Bandpass-filtering can be performed, e.g. based on respective high and/or low frequency thresholds, to remove low and/or high frequencies unlikely to represent valid RRs. Example filter cut-off frequencies (thresholds) corresponded to a low of 6 breaths/min and a high of 60 breaths/min. The maximum power frequency and the corresponding SNR value can be computed from the ensembled power spectrum. The waveform corresponding to the entire ROI can be used for displaying the breathing pattern to the user in the mobile app.
- In certain situations there may be insufficient periodicity in the respiratory waveform (e.g., the participant briefly held their breath or changed their respiratory rate within the time window used for analyzing the waveform). To increase the robustness of RR estimation, the algorithm can fall back on a time domain estimation method based on counting zero crossings of the waveform corresponding to the entire ROI whenever the SNR obtained via the FFT-based method is lower than a certain threshold. Two versions of the algorithm were tested, differing only in terms of this threshold: SNR < -6.0 dB (“version A”) and SNR < -4.0 dB (“version B”). The higher value for the threshold in version B invoked the time domain estimation method more often, which was hypothesized to improve accuracy by improving robustness to irregular breathing.
- An example prospective observational clinical validation study was performed to assess the accuracy of the study algorithm in measuring the RR in healthy adults and patients with chronic respiratory conditions. Chronic respiratory conditions included moderate or severe chronic obstructive pulmonary disease (COPD) and asthma that was not well-controlled based on specific study criteria. Also, participants with significant tremor were excluded.
- Each participant underwent 30 seconds of data collection using a
Pixel 4 smartphone running Android 10 (Google LLC, Mountain View, CA). The two algorithm versions (A and B) were tested sequentially. The participants followed the study protocol via instructions from the study app, without intervention from the study staff. Participants were prompted to prop the study phone on a table using provided common household items, such that the upper body was centered in the video capture (see, e.g.,FIGS. 1E-G ). There were no specific requirements on the type of clothing worn during the study or additional custom lighting equipment. The in-app instructions guided the participants to wait several minutes after any active movement and to stay comfortable and breathe normally during the measurements. - During the data collection, RR was manually counted and recorded by two research coordinators. The two observers counted the number of breaths independently and blinded to the algorithm-estimated results. The agreement between the two measurements was high (Pearson correlation coefficient: 0.962; mean difference: 0.48 ± 0.88 breaths/min; range, 0-4). The mean of the two human-measured RRs, rounded off to the nearest integer, was taken to be the reference RR.
- Each participant contributed a single pair of measurements for each algorithm version, and the MAE was used as the primary evaluation metric. The study hypothesis was that MAE would be < 3 breaths/min. One-sample t-tests were done to determine whether the MAE was statistically significantly < 3 breaths/min. A pre-specified subgroup analysis was also performed, stratified by history of chronic respiratory conditions. In addition, post-hoc subgroup analyses were performed for age and race/ethnicity subgroups. Bland-Altman plots were used to analyze further for any trends in errors. Differences between the two algorithm versions were compared using a paired t-test.
- To estimate the sample size required for the study, an IRB-approved feasibility study with 80 healthy adults was conducted. Based on that MAE ± standard deviation (0.96 ± 0.72 breaths/min), a sample size of 50 participants was estimated to provide a power of > 0.99 to detect an MAE < 3. The power was also >0.99 for both the subgroup of 10 healthy participants and the subgroup of 40 with chronic respiratory conditions. If the MAE and standard deviation were doubled, the power would be >0.99, 0.71, and >0.99, respectively, for the full sample, healthy participants, and those with chronic respiratory conditions.
- A total of 50 participants were enrolled in the RR study, including 10 healthy participants and 40 participants with chronic respiratory conditions. Participants had diverse demographic characteristics. The average reference RR was 15.3 ± 3.7 breaths/min.
- Both versions of the example algorithm successfully estimated RR in all of the study subjects; thus, all of the 50 study participants contributed to the final analysis. The MAE in the overall study population was 0.84 ± 0.97 and 0.78 ± 0.61 breaths/min for algorithm versions A and B, respectively, which were significantly lower than the pre-specified threshold of 3 breaths/min (p<0.001 for both). Each subgroup also showed MAE values significantly lower than the threshold: algorithm version A, 0.60 ± 0.52 breaths/min (p<0.001) for the healthy cohort and 0.90 ± 1.05 breaths/min (p<0.001) for the cohort with chronic respiratory conditions; algorithm version B, 0.70 ± 0.67 breaths/min (p<0.001) and 0.80 ± 0.60 breaths/min (p<0.001), respectively. No significant variations across age and race subgroups were seen.
- Thus, the present disclosure provides the results of two example prospective clinical studies validating the performance of example smartphone algorithms to estimate HR and RR. Both example algorithms showed high accuracy compared to the reference standard vital sign measurements, with HR within 5% and RR within 3 breaths/min (the pre-specified targets). In addition, the HR estimation was robust across skin tones, and the RR estimation generalized to participants with common chronic respiratory conditions, COPD, and asthma.
- The accuracy of the HR algorithm is especially notable. A MAE less than 5 beats/min or a MAPE less than 10% are standard accuracy thresholds for HR monitors. The MAE of 1.32 beats/min in HR is lower than that reported for contemporary wearable devices (4.4 to 10.2 beats/min at rest), albeit with several differences in study design and population. The MAPE of 1.63% is comparable to the performance of current wearable devices. For example, Shcherbina et al. tested six wrist worn devices to show a median error <5% for all across various activities and a median error of 2.0% for the best-performing device. Because skin tone can be a potential source of bias in medical devices, and the accuracy of PPG-based HR estimation can be affected by melanin’s light-absorbing property, participants with diverse skin tones were enrolled to validate the robustness of the example HR estimation algorithm across skin tones.
- For consumer-grade RR monitoring devices, there is no well-accepted accuracy standard. The MAE of 0.78 breaths/min attained in the study is comparable to that of professional healthcare devices, which have reported accuracy of ± 2-3 breaths/min. This could be a helpful reference point for future studies. In this study, two example algorithm versions for RR estimation were studied that differed only in the SNR threshold. The results suggest that this parameter had little impact on the accuracy or error rates.
- This work supports the use of consumer-grade smartphones for measuring HR and RR. One application of these measurements is in fitness and wellness for the general consumer user. Specifically, an elevated resting HR or slower heart rate recovery after exercise has been linked to lower physical fitness and higher risk of all-cause mortality. Evidence suggests use of direct-to-consumer mobile health technologies may enhance positive lifestyle modification such as increased physical activity, more weight loss, and better diabetes control. Tracking one’s own health-related parameters over time by the general public can potentially increase motivation for a healthier lifestyle by providing an objective, quantifiable metric. Additionally, there exists strong evidence that regular physical activity is key to improving one’s health independent of demographics or current fitness level for maintaining cardiovascular health. Monitoring one’s HR is also an easy and effective way to assess and adjust exercise intensity or enable smartphone-based measurement of cardiorespiratory fitness.
- With further clinical validation across broad populations, such smartphone-based measurement could also be useful in various settings, most notably telehealth where vital sign measurement is challenging due to the remote nature of the patient encounter. Though patients can in principle count their own HR or RR, this can be error prone due to factors such as biases that acute awareness of the self-examination can cause. Because the demand for remote triage, diagnosis and monitoring is increasing, there is increased attention being paid to accurate remote physical examination.
-
FIG. 2A depicts an example client-server environment according to example embodiments of the present disclosure. Specifically,FIG. 2A depicts a user computing device and a server system that communicate over a network. The computing device can be a personal electronic device such as a smartphone, tablet, laptop, and so on. The computing device can include an image capture system, at least a portion of an image analysis system, and user data. The image capture system can capture one or more images of a patient. The image analysis system can determine a heart rate and/or a respiratory rate according to any of the techniques described herein. - In some implementations, the computing device can transmit the captured image(s) to the server computing device. Alternatively or additionally, the image analysis system can generate intermediate data for or from one or more images. In this way, the computing device can transmit intermediate data representing the image, rather than the image itself. This can reduce the amount of bandwidth needed to transmit the images to the server computing system.
- The user data can be stored in a local data storage device and can include user clinical data, user demographic data, and/or user medical history data. This information can be transmitted to the server computing system as needed with user permission. In some examples, the image analysis system at the user computing device can include a context component that generates a feature representation for the user data. In some examples, the image analysis system can combine one or more image intermediate data and the feature representation data for the user data.
- The server computing system can include some or all of an image analysis system. The image analysis system can determine a heart rate and/or a respiratory rate according to any of the techniques described herein.
- For example, the server computing system can receive one or more of: image data, one or more intermediate data generated from the image data, user data, or other forms of data. Any and/or all of these types of data can be received at the server computing system and used to generate one or more output such as image analyses (e.g., heart rate and/or respiratory rate) and/or other physical assessment predictions. The image analysis system outputs can be transmitted to the computing device or to another third-party device as needed and approved by the user.
-
FIG. 2B depicts an example block diagram of a system for providing physical assessment assistance according to example embodiments of the present disclosure. In this example, the computing device is associated with a medical professional (e.g., a doctor (e.g., optometrist, ophthalmologist, radiologist, dermatologist, etc.), a nurse practitioner, and so on). The medical professional can utilize the computing device to obtain aid during their physical assessment process. The computing device can include an image capture system (e.g., a camera and associated software), a physical assessment assistance system, and a display. The physical assessment assistance system can include some or all of an image analysis system and medical history data. - The medical professional can use the computing device to capture one or more images of the patient using the image capture system. The physical assessment assistance system can process the imagery locally, generate intermediate data locally, or transmit the raw image data to the server computing system. Similarly, medical history data can be processed locally to generate a feature representation or transmitted to the server computing system. In some examples, the physical assessment assistance system includes the full image analysis system and thus can generate image analyses without transmitting data to the server computing system.
- In some examples, the physical assessment assistance system transmits data to the server computing system. The image analysis system at the server computing system can generate one or more outputs such as image analyses or other physical assessment predictions and transmit the data back to the physical assessment assistance system for display to the medical professional in the display at the computing device.
-
FIG. 2C depicts an example block diagram of a system for providing physical assessment assistance according to example embodiments of the present disclosure. In this example, the patient is not physically present with the medical professional. Instead, the patient uses a computing device with an image capture system to transmit one or more images (and potentially user data) to the computing device associated with the medical professional and/or the server computing system via a network. Once the computing device receives the one or more images from the computing device associated with the patient, the process can proceed as described above with respect toFIGS. 2A or 2B . The medical professional can then transmit any relevant outputs such as physical assessment information to the computing device of the patient. -
FIG. 3 depicts a block diagram of anexample computing system 100 according to example embodiments of the present disclosure. Thesystem 100 includes auser computing device 102 and aserver computing system 130 that are communicatively coupled over anetwork 180. - The
user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device. - The
user computing device 102 includes one ormore processors 112 and amemory 114. The one ormore processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 114 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 114 can storedata 116 andinstructions 118 which are executed by theprocessor 112 to cause theuser computing device 102 to perform operations. - In some implementations, the
user computing device 102 can store or include one or more image analysis systems 120 (e.g. software program(s)). For example, theimage analysis systems 120 can be or can otherwise include various image analysis systems such as heart rate determination systems and/or respiratory rate determination systems. Thesystems 120 can implement any of the methods described herein. - In some implementations, the one or more
image analysis systems 120 can be received from theserver computing system 130 overnetwork 180, stored in the usercomputing device memory 114, and then used or otherwise implemented by the one ormore processors 112. In some implementations, theuser computing device 102 can implement multiple parallel instances of a single image analysis system 120 (e.g., to perform parallel image analysis across multiple instances of imagery). - Additionally or alternatively, one or more
image analysis systems 140 can be included in or otherwise stored and implemented by theserver computing system 130 that communicates with theuser computing device 102 according to a client-server relationship. For example, theimage analysis systems 140 can be implemented by theserver computing system 140 as a portion of a web service (e.g., a physical assessment service). - The
user computing device 102 can also include one or moreuser input components 122 that receives user input. For example, theuser input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus). The touch-sensitive component can serve to implement a virtual keyboard. Other example user input components include a microphone, a traditional keyboard, a camera, or other means by which a user can provide user input. - The
server computing system 130 includes one ormore processors 132 and amemory 134. The one ormore processors 132 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 134 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 134 can storedata 136 andinstructions 138 which are executed by theprocessor 132 to cause theserver computing system 130 to perform operations. - In some implementations, the
server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances in which theserver computing system 130 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof. - As described above, the
server computing system 130 can store or otherwise include one or moreimage analysis systems 140. For example, theimage analysis systems 140 can be or can otherwise include various image analysis systems such as heart rate determination systems and/or respiratory rate determination systems. Thesystems 140 can implement any of the methods described herein. - The
network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links. In general, communication over thenetwork 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL). -
FIG. 3 illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can be used as well. -
FIG. 4 depicts a flow chart diagram of an example method to measure heart rate according to example embodiments of the present disclosure. AlthoughFIG. 4 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of themethod 400 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. - At 402, the
method 400 includes obtaining, by a computing system comprising one or more computing devices, a plurality of image frames captured by a camera while a digit of the subject is placed in a field of view of the camera. For example, the digit can be a finger. As an example, the digit of the subject can be placed in physical contact with an outermost lens or cover of the camera during capture of the plurality of image frames. Alternatively, the digit of the subject can be adjacent to, but not in physical contact, with the outermost lens or cover of the camera. - At 404, the
method 400 includes measuring, by the computing system, a photoplethysmography (PPG) waveform from the plurality of image frames. In some implementations, measuring, by the computing system, the PPG waveform from the plurality of image frames can include selecting, by the computing system, regions of interest (ROIs) from the plurality of image frames; averaging, by the computing system, pixels in each of the regions of interest on a per-channel basis to form a plurality of per-channel waveforms; and computing, by the computing system, the PPG waveform as a weighted average of the plurality of per-channel waveforms. As examples, the ROIs can be manually or automatically selected. As one example, the ROIs can include an entirety of the frame and each of the four quadrants of the frame. As another example, the ROIs can include a top half, a bottom half, a left half, and a right half. As another example, the ROIs can include a bounding box associated with a digit. For example, each of the ROIs may be defined as a respective portion of the bounding box (e.g. a top half, a bottom half, a left half, and a right half), or a respective bounding box. In some implementations, the bounding box(es) can be generated by an object detection model provided with the frame as input. As examples, the channels of the frame can include color channels in a color space such as red, green, and blue channels in an RGB color space. - In some implementations, measuring the PPG waveform can also include applying, by the computing system, one or more bandpass filters to the PPG waveform. As examples, filter cut-off frequencies can correspond to a low of 30 beats/min and high of 360 beats/min.
- In some implementations, measuring the PPG waveform can also include smoothing, by the computing system, the PPG waveform using a maximum allowed change in amplitude which is a function of a moving average of PPG waveform values. As an example, large amplitude changes in PPGs due to motion can be suppressed by limiting maximum allowed changes in amplitudes as 3x of moving average values.
- At 406, the
method 400 includes extracting, by the computing system, a dominant frequency from the PPG waveform. - In some implementations, extracting, by the computing system, the dominant frequency from the PPG waveform can include: generating, by the computing system, a Fast Fourier Transform representation of the PPG waveform; and extracting, by the computing system, the dominant frequency from the Fast Fourier Transform representation. For example, extracting, by the computing system, the dominant frequency from the Fast Fourier Transform representation can include summing, by the computing system, powers of first, second, and third harmonics.
- In some implementations, extracting, by the computing system, the dominant frequency from the PPG waveform can include: determining, by the computing system, a signal to noise ratio for each region of interest by computing a ratio between a power of a dominant frequency and powers of non-dominant frequencies on a logarithmic scale; identifying, by the computing system, the region of interest with the largest signal to noise ratio; and extracting, by the computing system, the dominant frequency of the region of interest with the largest signal to noise ratio.
- At 408, the
method 400 includes determining, by the computing system, a heart rate of the subject based on the dominant frequency extracted from the PPG waveform. For example, the heart rate of the subject can be set equal to the dominant frequency extracted from the PPG waveform expressed in the format of beats per minute. - At 410, the
method 400 includes providing, by the computing system, the heart rate of the subject as an output. As examples, providing the heart rate as an output can include storing data representing the heart rate of the subject in a memory, providing data representing the heart rate of the subject for display (e.g., to the subject or a medical provider), and/or transmitting data representing the heart rate of the subject over a communications network. -
FIG. 5 depicts a flow chart diagram of an example method to measure respiratory rate according to example embodiments of the present disclosure. AlthoughFIG. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of themethod 500 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. - At 502, the
method 500 includes obtaining, by a computing system comprising one or more computing devices, a plurality of image frames captured by a camera while a subject is placed in a field of view of the camera. For example, a base of a neck, a shoulder line, and/or an upper torso of the subject can be depicted by the plurality of image frames. - At 504, the
method 500 includes determining, by the computing system, optical flow data for the plurality of image frames. - In some implementations, determining, by the computing system, optical flow data for the plurality of image frames can include: performing, by the computing system, a Eulerian phase-based motion analysis on the plurality of image frames to generate phase-based motion data; and generating, by the computing system, the optical flow data for the plurality of image frames based on the phase-based motion data.
- As one example, determining the optical flow data can include determining, by the computing system, for each pixel of one or more pixels in each frame, a position represented by a phase of spatially localized sinusoids in multiple scales; and transforming, by the computing system, the phase of spatially localized sinusoids into optical flow data by linearly approximating the position implied by each phase coefficient and averaging across scales.
- At 506, the
method 500 includes averaging, by the computing system, a vertical component of the optical flow data to generate a respiratory waveform. - At 508, the
method 500 includes generating, by the computing system, a frequency domain representation of the respiratory waveform. - At 510, the
method 500 includes determining, by the computing system, the respiratory rate of the subject based on the frequency domain representation of the respiratory waveform. - In some implementations, steps 506, 508, and/or 510 can be performed for each of a plurality of regions of interest (ROIs). Thus, in some implementations, the
method 500 can include selecting, by the computing system, a plurality of regions of interest; averaging, by the computing system, a vertical component of the optical flow data for each of the regions of interest to generate a plurality of respiratory waveforms respectively for the plurality of regions of interest; and determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms. As examples, the ROIs can be manually or automatically selected. As one example, the ROIs can include an entirety of the frame and each of the four quadrants of the frame. As another example, the ROIs can include a top half, a bottom half, a left half, and a right half. As another example, the ROIs can include one or more bounding boxes associated with a body, a neck, a shoulder line, and/or an upper torso of the subject. In some implementations, the bounding box(es) can be generated by an object detection model provided with the frame as input. - Referring still to
FIG. 5 , at 510, determining, by the computing system, the respiratory rate of the subject based on the one or more waveforms can include: generating, by the computing system, a power spectrum from a frequency domain representation of each respiratory waveform; aggregating, by the computing system, the power spectra for the one or more respiratory waveforms to obtain a final ensembled power spectrum; and determining, by the computing system, the respiratory rate of the subject based on a maximum power frequency of the final ensembled power spectrum. - As an example, bandpass-filtering can be performed to remove low and high frequencies unlikely to represent valid RRs. Example filter cut-off frequencies include a low of 6 breaths/min and a high of 60 breaths/min. The maximum power frequency and the corresponding SNR value can be computed from the ensembled power spectrum. The waveform corresponding to the entire ROI can be used for displaying the breathing pattern to the user in the mobile app.
- At 512, the
method 500 includes providing, by the computing system, the respiratory rate of the subject as an output. As examples, providing the respiratory rate as an output can include storing data representing the respiratory rate of the subject in a memory, providing data representing the respiratory rate of the subject for display (e.g., to the subject or a medical provider), and/or transmitting data representing the respiratory rate of the subject over a communications network. - In some implementations, the
method 500 can include defaulting, by the computing system, to a time-domain estimation of the respiratory waveform when a signal to noise ratio associated with the frequency domain representation falls below a threshold. - The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For instance, processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
- While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, upon attaining an understanding of the foregoing, can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present disclosure cover such alterations, variations, and equivalents.
Claims (19)
1. A computer-implemented method to measure heart rate of a subject, the method comprising:
obtaining, by a computing system comprising one or more computing devices, a plurality of image frames captured by a camera while a digit of the subject is placed in a field of view of the camera;
measuring, by the computing system, a photoplethysmography (PPG) waveform from the plurality of image frames;
extracting, by the computing system, a dominant frequency from the PPG waveform; and
determining, by the computing system, the heart rate of the subject based on the dominant frequency extracted from the PPG waveform.
2. The computer-implemented method of claim 1 , wherein measuring, by the computing system, the PPG waveform from the plurality of image frames comprises:
selecting, by the computing system, regions of interest from the plurality of image frames;
averaging, by the computing system, pixels in each of the regions of interest on a per-channel basis to form a plurality of per-channel waveforms; and
computing, by the computing system, the PPG waveform as a weighted average of the plurality of per-channel waveforms.
3. The computer-implemented method claim 1 , wherein measuring the PPG waveform comprises:
applying, by the computing system, one or more bandpass filters to the PPG waveform.
4. The computer-implemented method claim 1 , wherein measuring the PPG waveform comprises:
smoothing, by the computing system, the PPG waveform using a maximum allowed change in amplitude which is a function of a moving average of PPG waveform values.
5. The computer-implemented method of claim 1 , wherein extracting, by the computing system, the dominant frequency from the PPG waveform comprises:
generating, by the computing system, a Fast Fourier Transform representation of the PPG waveform; and
extracting, by the computing system, the dominant frequency from the Fast Fourier Transform representation.
6. The computer-implemented method of claim 5 , wherein extracting, by the computing system, the dominant frequency from the Fast Fourier Transform representation comprises:
summing, by the computing system, powers of first, second, and third harmonics.
7. The computer-implemented method of claim 1 , wherein extracting, by the computing system, the dominant frequency from the PPG waveform comprises:
determining, by the computing system, a signal to noise ratio for each region of interest by computing a ratio between a power of a dominant frequency and powers of non-dominant frequencies on a logarithmic scale;
identifying, by the computing system, the region of interest with the largest signal to noise ratio; and
extracting, by the computing system, the dominant frequency of the region of interest with the largest signal to noise ratio.
8. The computer-implemented method of claim 1 , wherein the digit comprises a finger.
9. The computer-implemented method of claim 1 , wherein the digit of the subject is in physical contact with an outermost lens or cover of the camera during capture of the plurality of image frames.
10. A computer-implemented method to measure respiratory rate of a subject, the method comprising:
obtaining, by a computing system comprising one or more computing devices, a plurality of image frames captured by a camera while a subject is placed in a field of view of the camera;
determining, by the computing system, optical flow data for the plurality of image frames;
averaging, by the computing system, a vertical component of the optical flow data to generate a respiratory waveform;
generating, by the computing system, a frequency domain representation of the respiratory waveform; and
determining, by the computing system, the respiratory rate of the subject based on the frequency domain representation of the respiratory waveform.
11. The computer-implemented method of claim 10 , wherein determining, by the computing system, the optical flow data for the plurality of image frames comprises:
determining, by the computing system, for each pixel of one or more pixels in each image frame, a position represented by a phase of spatially localized sinusoids in multiple scales; and
transforming, by the computing system, the phase of the spatially localized sinusoids into the optical flow data by linearly approximating the position implied by each phase coefficient and averaging across scales.
12. The computer-implemented method of claim 10 , wherein:
averaging, by the computing system, the vertical component of the optical flow data to generate the respiratory waveform comprises:
selecting, by the computing system, a plurality of regions of interest;
averaging, by the computing system, a vertical component of the optical flow data for each of the regions of interest to generate a plurality of respiratory waveforms respectively for the plurality of regions of interest; and
determining, by the computing system, the respiratory rate of the subject based on the frequency domain representation of the respiratory waveform comprises determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms.
13. The computer-implemented method of claim 12 , wherein determining, by the computing system, the respiratory rate of the subject based on the plurality of respiratory waveforms comprises:
generating, by the computing system, a power spectrum from a frequency domain representation of each respiratory waveform; and
aggregating, by the computing system, the power spectra for the plurality of respiratory waveforms to obtain a final ensembled power spectrum; and
determining, by the computing system, the respiratory rate of the subject based on a maximum power frequency of the final ensembled power spectrum.
14. The computer-implemented method of claim 10 , further comprising:
defaulting, by the computing system, to a time-domain estimation of the respiratory waveform when a signal to noise ratio associated with the frequency domain representation falls below a threshold.
15. The computer-implemented method of claim 10 , wherein a base of a neck, a shoulder line, and an upper torso of the subject are depicted by the plurality of image frames.
16. The computer-implemented method of claim 10 , wherein determining, by the computing system, optical flow data for the plurality of image frames comprises:
performing, by the computing system, a Eulerian phase-based motion analysis on the plurality of image frames to generate phase-based motion data; and
generating, by the computing system, the optical flow data for the plurality of image frames based on the phase-based motion data.
17. The computer-implemented method of claim 10 , wherein the computing system consists of a user device.
18. A computing device comprising:
a camera;
one or more processors; and
one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing device to perform the method of claim 1 .
19. One or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause a computing device to perform the method of claim 1 .
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/011,899 US20230277069A1 (en) | 2021-03-03 | 2022-03-03 | Heart Rate and Respiratory Rate Measurements from Imagery |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163156272P | 2021-03-03 | 2021-03-03 | |
PCT/US2022/018721 WO2022187497A1 (en) | 2021-03-03 | 2022-03-03 | Heart rate and respiratory rate measurements from imagery |
US18/011,899 US20230277069A1 (en) | 2021-03-03 | 2022-03-03 | Heart Rate and Respiratory Rate Measurements from Imagery |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230277069A1 true US20230277069A1 (en) | 2023-09-07 |
Family
ID=80999456
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US18/011,899 Pending US20230277069A1 (en) | 2021-03-03 | 2022-03-03 | Heart Rate and Respiratory Rate Measurements from Imagery |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230277069A1 (en) |
WO (1) | WO2022187497A1 (en) |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
GB2565036A (en) * | 2017-05-30 | 2019-02-06 | Bioepic Ltd | Adaptive media for measurement of blood glucose concentration and insulin resistance |
US11259710B2 (en) * | 2018-05-16 | 2022-03-01 | Mitsubishi Electric Research Laboratories, Inc. | System and method for remote measurements of vital signs |
-
2022
- 2022-03-03 WO PCT/US2022/018721 patent/WO2022187497A1/en active Application Filing
- 2022-03-03 US US18/011,899 patent/US20230277069A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022187497A1 (en) | 2022-09-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Fan et al. | Robust blood pressure estimation using an RGB camera | |
JP6219279B2 (en) | Remote monitoring of vital signs | |
JP6615176B2 (en) | Non-interfering skin tissue hydration measuring device and related method | |
EP3052008B1 (en) | Improved signal selection for obtaining a remote photoplethysmographic waveform | |
Hassan et al. | Towards health monitoring using remote heart rate measurement using digital camera: A feasibility study | |
EP3453321A1 (en) | Non-invasive method and system for estimating blood pressure from photoplethysmogram using statistical post-processing | |
Bae et al. | Prospective validation of smartphone-based heart rate and respiratory rate measurement algorithms | |
US20230233091A1 (en) | Systems and Methods for Measuring Vital Signs Using Multimodal Health Sensing Platforms | |
Casalino et al. | Evaluating the robustness of a contact-less mHealth solution for personal and remote monitoring of blood oxygen saturation | |
Villarroel et al. | Non-contact vital-sign monitoring of patients undergoing haemodialysis treatment | |
Colantonio et al. | Computer vision for ambient assisted living: Monitoring systems for personalized healthcare and wellness that are robust in the real world and accepted by users, carers, and society | |
Chari et al. | Diverse R-PPG: Camera-based heart rate estimation for diverse subject skin-tones and scenes | |
Steinman et al. | Smartphones and video cameras: future methods for blood pressure measurement | |
Tabei et al. | A novel diversity method for smartphone camera-based heart rhythm signals in the presence of motion and noise artifacts | |
Gupta et al. | Availability and performance of face based non-contact methods for heart rate and oxygen saturation estimations: A systematic review | |
Alnaggar et al. | Video-based real-time monitoring for heart rate and respiration rate | |
Hu et al. | Contactless blood oxygen estimation from face videos: A multi-model fusion method based on deep learning | |
Karthick et al. | Analysis of vital signs using remote photoplethysmography (RPPG) | |
US20230277069A1 (en) | Heart Rate and Respiratory Rate Measurements from Imagery | |
Gupta et al. | Remote photoplethysmography‐based human vital sign prediction using cyclical algorithm | |
WO2023055862A1 (en) | Systems and methods for platform-agnostic, real-time physiologic vital sign detection from video stream data | |
Stogiannopoulos et al. | A non-contact SpO 2 estimation using video magnification and infrared data | |
Liu et al. | Lightweight and interpretable convolutional neural network for real-time heart rate monitoring using low-cost video camera under realistic conditions | |
Adelabu et al. | Enhancement of a camera-based continuous heart rate measurement algorithm | |
Islam et al. | Extracting heart rate variability: a summary of camera based Photoplethysmograph |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CHEN, LIWEN;KANG, SI-HYUCK;BAE, SEAN KYUNGMOK;AND OTHERS;SIGNING DATES FROM 20220214 TO 20230112;REEL/FRAME:062544/0522 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |