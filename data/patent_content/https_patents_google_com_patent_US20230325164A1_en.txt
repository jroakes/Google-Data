US20230325164A1 - Translating between programming languages independently of sequence-to-sequence decoders - Google Patents
Translating between programming languages independently of sequence-to-sequence decoders Download PDFInfo
- Publication number
- US20230325164A1 US20230325164A1 US17/717,609 US202217717609A US2023325164A1 US 20230325164 A1 US20230325164 A1 US 20230325164A1 US 202217717609 A US202217717609 A US 202217717609A US 2023325164 A1 US2023325164 A1 US 2023325164A1
- Authority
- US
- United States
- Prior art keywords
- source code
- code snippet
- programming language
- embedding
- translation
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013519 translation Methods 0.000 claims abstract description 110
- 238000000034 method Methods 0.000 claims abstract description 65
- 230000009466 transformation Effects 0.000 claims abstract description 14
- 230000014616 translation Effects 0.000 claims description 109
- 239000011159 matrix material Substances 0.000 claims description 43
- 238000009826 distribution Methods 0.000 claims description 31
- 238000005070 sampling Methods 0.000 claims description 23
- 238000012549 training Methods 0.000 claims description 21
- 238000012545 processing Methods 0.000 claims description 19
- 238000010801 machine learning Methods 0.000 claims description 15
- 230000008569 process Effects 0.000 claims description 13
- 230000015654 memory Effects 0.000 claims description 7
- 230000004044 response Effects 0.000 claims 1
- 238000000605 extraction Methods 0.000 description 8
- 230000008901 benefit Effects 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 238000004458 analytical method Methods 0.000 description 4
- 238000013528 artificial neural network Methods 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000002457 bidirectional effect Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000008520 organization Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000000306 recurrent effect Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 238000003491 array Methods 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 239000000796 flavoring agent Substances 0.000 description 1
- 235000019634 flavors Nutrition 0.000 description 1
- 230000010006 flight Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/40—Transformation of program code
- G06F8/51—Source to source
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/23—Clustering techniques
- G06F18/232—Non-hierarchical techniques
- G06F18/2321—Non-hierarchical techniques using statistics or function optimisation, e.g. modelling of probability density functions
- G06F18/23213—Non-hierarchical techniques using statistics or function optimisation, e.g. modelling of probability density functions with fixed number of clusters, e.g. K-means clustering
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/70—Software maintenance or management
- G06F8/73—Program documentation
-
- G06K9/6223—
Definitions
- Computer software programming often requires developers to read and/or write source code (i.e., to program) in a specific language, e.g. Java, C++, C, Python, etc.
- a specific language e.g. Java, C++, C, Python, etc.
- Each programming language has its own strengths, weaknesses, nuances, idiosyncrasies, etc.
- some programming languages are more suitable for certain stages of software development and/or a software life cycle than others.
- scripting languages such as Python, JavaScript, Perl, etc., are often more effectively used near the very beginning of software development because programmers using these languages are able to turn around functional software relatively quickly.
- Most programmers obtain at least a superficial understanding of multiple programming languages, but only master a few. Consequently, each programming language tends to have its own talent pool.
- Transformer networks have become increasingly popular for performing natural language processing.
- Transformer networks were designed in part to mitigate a variety of shortcomings of prior natural language processing models, such as overfitting, the vanishing gradient problem, and exceedingly high computational costs, to name a few.
- transformer networks are still implemented typically as sequence-to-sequence, encoder-decoder models.
- the decoder portions in particular require significant sequential computational processing, and as a consequence, introduce considerable latency during both training and inference.
- implementations are described herein for translating source code between programming languages, independently of sequence-to-sequence decoding, e.g., using only the encoder part of a transformer network.
- sequence-to-sequence decoding one or more layers of a machine learning model configured with selected aspects of the present disclosure may be trained to generate, in parallel, all target language tokens as a single transformation, corresponding to a translation of a source code snippet in a target programming language. For example, a matrix of probability distributions over a vocabulary of the target programming language may be generated. Tokens of the translation of the source code snippet may then be selected based on these probability distributions.
- a method may be provided for translating a source code snippet from a first programming language to a second programming language independently of sequence-to-sequence decoding.
- the method may be implemented by one or more processors and may include: processing the source code snippet written in the first programming language using an encoder portion of a transformer network to generate an embedding of the source code snippet; processing the embedding of the source code snippet using an all-pair attention layer to generate an attended embedding of the source code snippet; and processing the attended embedding of the source code snippet using an output layer to generate, by way of a single transformation of the attended embedding of the source code snippet, data indicative of a translation of the source code snippet in the second programming language.
- the data indicative of the translation may be a matrix of probability distributions.
- the matrix may be an x by y matrix, with x being a non-zero integer equal to a number of tokens extracted from the source code snippet.
- y is a non-zero integer equal to a vocabulary size of the second programming language.
- the method may further include selecting the tokens of the translation of the source code snippet in the second programming language based on maximum probabilities in the matrix of probability distributions.
- the method may further include sampling from the matrix of probability distributions to generate one or more alternative translations of the source code snippet.
- the sampling may include using the Gumbel-max trick, random temperature-based sampling, top-k sampling, or nucleus sampling.
- a method of training a translation model to translate source code from a first programming language to a second programming language, without using a sequence-to-sequence decoder may include: iteratively training the translation model using a pair of source code snippets, including an original source code snippet written in the first programming language and a ground truth translation of the original source code snippet in the second programming language, wherein during each iteration, the training includes: processing the original source code snippet using one or more encoder layers of the translation model to generate a embedding of the original source code snippet; processing data indicative of the embedding using an output layer to generate, by way of a single transformation, data indicative of a predicted translation of the original source code snippet in the second programming language; selecting a different subset of one or more predicted tokens of the predicted translation of the original source code snippet in the second language that has not yet been used to train the translation model; comparing the selected subset of one or more tokens to a corresponding subset
- the training may include processing the embedding of the source code snippet using an all-pair attention layer to generate an attended embedding of the source code snippet, wherein the data indicative of the first embedding comprises the attended embedding.
- the translation model may be a transformer network.
- the output layer may be a softmax layer.
- the data indicative of the predicted translation may be a matrix of probability distributions, such as an x by y matrix, with x being a non-zero integer equal to a number of tokens extracted from the source code snippet and y being a non-zero integer equal to a vocabulary size of the second programming language.
- implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations.
- FIG. 2 is a block diagram showing an example of how source code may be translated, in accordance with various implementations.
- FIG. 3 depicts an example application of techniques described herein, in accordance with various implementations.
- FIG. 4 depicts a flowchart illustrating an example method for practicing selected aspects of the present disclosure.
- FIG. 5 depicts another flowchart illustrating an example method for practicing selected aspects of the present disclosure.
- FIG. 6 illustrates an example architecture of a computing device.
- Implementations are described herein for translating structured textual data between domain-specific languages. More particularly, but not exclusively, implementations are described herein for translating source code between programming languages, independently of sequence-to-sequence decoding, e.g., using only the encoder part of a transformer network.
- sequence-to-sequence decoding one or more layers of a machine learning model configured with selected aspects of the present disclosure may be trained to generate, as a single transformation, data indicative of a translation of a source code snippet in a target programming language. For example, a matrix of probability distributions over a vocabulary of the target programming language may be generated in parallel. Tokens of the translation of the source code snippet may then be selected based on these probability distributions.
- a machine learning model configured with selected aspects of the present disclosure may include part of a transformer network, such as a BERT (Bidirectional Encoder Representations from Transformers) transformer and/or a GPT (Generative Pre-trained Transformer).
- a BERT Bidirectional Encoder Representations from Transformers
- GPT Geneative Pre-trained Transformer
- encoder layers of a BERT transformer may be directly coupled with downstream layers configured with selected aspects of the present disclosure to facilitate source code translation independently of sequence-to-sequence decoding.
- these encoder layers—and/or a whole transformer model from which these layers are obtained— may be trained initially using a corpus of documents and other data that is relevant to structured text in general, or programming languages in particular. These documents may include, for instance, source code examples, programming handbooks or textbooks, general programming documentation, natural language comments embedding in source code, and so forth.
- the encoder layer(s) may be coupled with custom layers described herein to create an “encoder-only,” or “decoder-free,” translation model configured with selected aspects of the present disclosure.
- the encoder layer(s) of the translation model may be applied to a source code snippet in a base programming language to generate a semantically-rich source code embedding.
- this source code embedding may then be processed using an attention layer, such as an “all-pair” attention layer, to generate what will be referred to herein as an “attended” embedding or “summary representation.”
- This attended embedding may then be processed using an output layer (e.g., softmax) of the translation model to generate, as a single transformation, the aforementioned data indicative of a translation of the source code snippet in the target programming language.
- this data may include a matrix of probability distributions over the vocabulary of the target programming language.
- the translation model may be trained using curriculum and/or teacher forcing training.
- the above-described layers of the translation model e.g., encoder(s), attention layer, output layer
- Each probability distribution may be used to select a token of a potential translation of the source code snippet in the target programming language.
- a subset or subsequence of these selected tokens may be compared to corresponding ground truth tokens during a given training iteration. Subsequent tokens of the translation may be ignored for this iteration of training.
- the source code snippet may once again be processed using the translation model.
- a “next” subset (or subsequence) of selected tokens such as the just-considered token and the very next token, may be used to train the translation model. Tokens outside of this next subset (e.g., subsequent tokens) may be ignored. This process may be repeated for each sub-sequence (e.g., each additional token) of the translation.
- training may be a relatively laborious and time-consuming process.
- decoder-based beam searching may no longer be available.
- a benefit of decoder-based beam searching is that alternative translations can be readily identified/generated during decoding, e.g., so that a user can, for instance, toggle through multiple candidate translations to select the best one, or even have multiple alternative tokens suggested for translation.
- various techniques may be used to sample tokens from the matrix of probability distributions.
- techniques such as the Gumbel-max trick, random temperature-based sampling, top-k sampling, or nucleus sampling, may be used to select tokens for alternative translation(s) that appear, at least to the user, similar to alternative translations generated conventionally using decoder-based beam searching.
- FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations.
- Any computing devices depicted in FIG. 1 or elsewhere in the figures may include logic such as one or more microprocessors (e.g., central processing units or “CPUs”, graphical processing units or “GPUs”, tensor processing units or “TPUs”) that execute computer-readable instructions stored in memory, or other types of logic such as application-specific integrated circuits (“ASIC”), field-programmable gate arrays (“FPGA”), and so forth.
- ASIC application-specific integrated circuits
- FPGA field-programmable gate arrays
- Some of the systems depicted in FIG. 1 such as a code knowledge system 102 , may be implemented using one or more server computing devices that form what is sometimes referred to as a “cloud infrastructure,” although this is not required.
- a code knowledge system 102 may be provided for helping clients 110 - 1 to 110 -P manage their respective code bases 112 - 1 to 112 -P.
- Code knowledge system 102 may include, among other things, a code translator 104 that is configured to perform selected aspects of the present disclosure in order to help one or more clients 110 - 1 to 110 -P to manage and/or make changes to one or more corresponding code bases 112 - 1 to 112 -P.
- Each client 110 may be, for example, an entity or organization such as a business (e.g., financial institute, bank, etc.), non-profit, club, university, government agency, or any other organization that operates one or more software systems.
- a bank may operate one or more software systems to manage the money under its control, including tracking deposits and withdrawals, tracking loans, tracking investments, and so forth.
- An airline may operate one or more software systems for booking/canceling/rebooking flight reservations, managing delays or cancellations of flight, managing people associated with flights, such as passengers, air crews, and ground crews, managing airport gates, and so forth.
- Code translator 104 may be configured to leverage knowledge of multiple different programming languages in order to aid clients 110 - 1 to 110 -P in translating between programming languages when editing, updating, re-platforming, migrating, or otherwise acting upon their code bases 112 - 1 to 112 -P.
- code translator 104 may be configured to translate code snippets from one programming language to another, e.g., on the fly or in batches. This may, for instance, enable a developer fluent in a first programming language to view and/or edit source code that was originally written in a second, less-familiar programming language in the first programming language. It may also significantly decrease the time and/or costs associated with migrating code bases between different programming languages.
- code knowledge system 102 may include a machine learning (“ML” in FIG. 1 ) database 105 that includes data indicative of one or more trained machine learning models 106 - 1 to 106 -N.
- These trained machine learning models 106 - 1 to 106 -N may take various forms that will be described in more detail below, including but not limited to BERT (Bidirectional Encoder Representations from Transformers) transformers, GPT (Generative Pre-trained Transformer) transformers, a graph-based network such as a graph neural network (“GNN”), graph attention neural network (“GANN”), or graph convolutional neural network (“GCN”), other types of sequence-to-sequence models and/or encoder-decoders, various flavors of a recurrent neural network (“RNN”, e.g., long short-term memory, or “LSTM”, gate recurrent units, or “GRU”, etc.), and any other type of machine learning model that may be applied to facilitate selected aspects of the present disclosure.
- RNN recurrent neural network
- code knowledge system 102 may also have access to one or more programming-language-specific corpuses 108 - 1 to 108 -M.
- these programming-language-specific corpuses 108 - 1 to 108 -M may be used, for instance, to train one or more of the machine learning models 106 - 1 to 106 -N.
- the programming-language-specific corpuses 108 - 1 to 108 -M may include examples of source code (e.g., entire code bases, libraries, etc.), inline comments, textual metadata associated with source code (e.g., commits), documentation such as textbooks and programming manuals, programming language-specific discussion threads, presentations, academic papers, and so forth.
- a client 110 that wishes to enable manipulation of its code base 112 in programming language(s) other than that/those used originally to write the source code may establish a relationship with an entity (not depicted in FIG. 1 ) that hosts code knowledge system 102 .
- code translator 104 may provide one or more versions of the source code snippet that is translated to a target programming language preferred by the developer. In some such implementations, code translator 104 may generate the translated source code snippet on the fly, e.g., in real time.
- code translator 104 may operate, e.g., in a batch mode, to preemptively translate all or selection portions of an entity's code base 112 into a targeted programming language.
- the edited version may be translated back into the native programming language or left in the new, target programming language, assuming other necessary infrastructure is in place.
- trained translation models may be deployed closer to or at the edge, e.g., at client devices 110 - 1 to 110 -P. Because these trained translation models do not necessarily include sequence-to-sequence decoders, and because encoders tend to be more computationally efficient than decoders, these trained translation models may be effectively applied at the edge, rather than in the cloud. As mentioned previously, edge-based deployment may give rise to a variety of benefits, such as maintenance of privacy, protection of sensitive source code, and so forth.
- FIG. 2 is a block diagram of an example process flow that may be implemented in whole or in part by code translator 104 in order to use a translation machine learning model 220 (also referred to simply as a “translation model 220 ”) to translate source code between programming languages.
- translation machine learning model 220 includes some number of encoder layers 222 - 1 to 222 -X, followed by an attention layer 224 and an output layer 226 .
- encoder layers 222 - 1 to 222 -X may correspond, for instance, to encoder layers of a large language model such as a BERT or GTP transformer.
- an entire transformer network e.g., encoder layers 222 - 1 to 222 -X plus one or more decoder layers (not depicted in FIG. 2 ), may be trained or bootstrapped on any number of programming language-specific corpuses 108 - 1 to 108 -M. Once trained, the decoders (not depicted in FIG. 2 ) may be discarded or decoupled from the encoder layers 222 - 1 to 222 -X. The encoder layers 222 - 1 to 222 -X may then be coupled with attention layer 224 and output layer 226 to form translation model 220 , which can be further trained as described herein.
- Code translator 104 may include a machine learning (“ML” in FIG. 2 ) module 230 that is configured to process various data using machine learning model(s) to translate source between programming languages.
- ML module 230 may extract tokens from first programming language source code snippet 228 and may use a token extraction model 221 to generate token embeddings 223 of those tokens.
- Token extraction model 221 may take various forms, such as that often used as part of the word2vec framework.
- ML module 230 may then apply these token embeddings 223 as input across encoders 222 - 1 to 222 -X to generate a source code embedding 232 of first programming language source code snippet 228 (alternatively referred to herein as an “encoding” of the source code snippet).
- Source code embedding 232 may be a semantically-rich representation/encoding of first programming language source code snippet 228 , including the tokens and relationships/logic between the tokens that is defined by the syntax and structure of first programming language source code snippet 228 .
- Embeddings such as token embeddings 223 generated based on token extraction model 221 and/or source code embedding 232 may take various forms, such as a one dimensional vector or a two-dimensional matrix.
- the resulting token embeddings matrix 223 may be a 512 ⁇ 768 matrix of token embeddings.
- source code embedding 232 may be a 512 ⁇ 2048 matrix.
- ML module 230 may then apply source code embedding 232 as input across attention layer 224 .
- attention layer 224 may be an “all pairs” attention layer that attends across all pairs of features represented in source code embedding 232 .
- the result may be an attended source code embedding 234 .
- attended source code embedding 234 may have various dimensions. If source code embedding 232 is 512 ⁇ 2048, in some implementations, attended source code embedding 234 may also be a 512 ⁇ 2048 matrix.
- Attended source code embedding 234 may then be applied across output layer 226 to generate, in parallel as a single transformation (as opposed to sequence-to-sequence decoding), data indicative of a translation of first programming language source code snippet 228 .
- output layer 226 may be a softmax layer that generates, as a single transformation of attended source code embedding 234 , an output matrix 236 .
- Output matrix 236 may be an x by y (both positive integers) matrix of probability distributions over a vocabulary of the second programming language.
- output matrix 236 may have, as its x dimension, a number of token inputs, and as its y dimension, a vocabulary size. Accordingly, continuing the example used above, if there are 512 input tokens, and the vocabulary size is 20,000, then output matrix 236 may have a dimension of 512 ⁇ 20,000.
- a sampling module 238 may be configured to sample a token from each probability distribution of output matrix 236 .
- each probability distribution corresponds to a column of output matrix 236 .
- sampling module 238 may select the token with the maximum probability from each probability distribution (column). These selected tokens ⁇ TOKEN-1, TOKEN-2, . . . ⁇ may then be used to formulate a second programming language source code snippet 240 that is a translation of first programming language source code snippet 228 .
- translation model 220 forgoes sequence-to-sequence decoding, and instead generates the probability distributions of 236 in parallel, some sequence-to-sequence decoder-specific techniques for generating alternative translations, such as beam searching, may not be available. However, it may still be desirable to provide a programmer with variations of a translation of a first programming language snippet, e.g., so that the programmer can use their judgment to select the best translation (this feedback can also be used continuously to train translation model 220 ).
- sampling module 238 may sample multiple times from output matrix 236 to generate a plurality of candidate second programming language snippets 240 . Rather than simply selecting the token from each probability distribution with the maximum probability, sampling module 238 may use other sampling techniques, including but not limited to the Gumbel-max trick, random temperature-based sampling, top-k sampling, or aureus sampling.
- FIG. 3 depicts an example scenario in which a code snippet written in one programming language may be translated to another programming language.
- the base source code snippet 360 is written in Java and prints the integers one to five.
- GUI graphical user interface
- the code snippet 360 written in Java is converted by code translator 104 into Python and rendered as part of GUI 362 .
- the developer operating GUI 362 may view the source code in a programming language with which he or she is more familiar.
- the developer may be able to edit the translated source code.
- the edits made by the developer may be translated back to Java before being stored and/or more permanently incorporated into the code base.
- the edited Python code may be incorporated into the code base.
- the original source code 360 may be sent to code knowledge system 102 for translation by code translator 104 prior to being sent to the computing device (not depicted) that renders GUI 362 .
- GUI 362 may be part of a software development application that performs the programming language translation locally, e.g., using a plug-in or built-in functionality.
- FIG. 3 is for illustrative purposes only.
- Source code may be translated between programming languages using techniques described herein for any number of applications.
- the source code in the based programming language may be translated into a target programming language en route to the second user, e.g., by code translator 104 .
- the second user's email application (or an email server that stores emails of the second user) may have a plugin configured with selected aspects of the present disclosure.
- a single user may operate a software development application to view multiple different source code snippets written in multiple different programming languages that are unfamiliar to the user.
- multiple respective translation models may be used to translate the source code snippets from the multiple different programming languages to a language (or languages) that are better understood to the user.
- techniques described herein may be used to automatically convert source code written in one programming language into source code in another programming language, without necessarily presenting translated source code to users as described previously.
- a company may decide to replatform an existing code base 112 to a new programming language, e.g., to obtain new functionality and/or technical benefits (e.g., security features, processing speed features, etc.) that were unavailable with the original programming language.
- Such a company may be able to deploy techniques described herein, or request that an entity associated with code knowledge system 102 deploy techniques described herein, to automatically convert all or a portion of a code base 112 from one programming language to another.
- a selectable link is presented (“CLICK HERE TO VIEW NEXT CANDIDATE TRANSLATION”) that a user can select to see an alternative translation of the original source code snippet.
- these candidate translations may be presented to the user in a ranked order. This ranked order may be determined in various ways, such as by how many (or few) errors or warnings are raised when attempts are made to parse and/or compile the candidate translations (e.g., in the background without the user being aware).
- various types of analysis associated with compiling such as lexical analysis, syntax analysis, semantic analysis, and so forth, may be applied to each candidate translation to determine its score (which may be inversely proportional to the number of errors or warnings generated).
- the candidates with the “best” scores may be presented to the programmer first.
- candidate translations may be presented (or at least made available for presentation) until various criteria are met, such as a candidate no longer being capable of being compiled.
- FIG. 4 is a flowchart illustrating an example method 400 of translating a source code snippet from a first programming language to a second programming language independently of sequence-to-sequence decoding, in accordance with implementations disclosed herein.
- This system may include various components of various computer systems, such as one or more components of code knowledge system 102 .
- operations of method 400 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system may process the source code snippet (e.g., 228 in FIG. 2 ) written in the first programming language using an encoder portion (e.g., 222 - 1 to 222 -X in FIG. 2 ) of a transformer network to generate an embedding (e.g., 232 ) of the source code snippet.
- an encoder portion e.g., 222 - 1 to 222 -X in FIG. 2
- ML module 230 may first perform a token extraction operation, e.g., using token extraction model 221 (e.g., word2vec), to generate token embeddings 223 .
- token extraction model 221 e.g., word2vec
- the system may process the embedding of the source code snippet using an all-pair attention layer (e.g., 224 in FIG. 2 ) to generate an attended embedding (e.g., 234 ) of the source code snippet.
- an all-pair attention layer e.g., 224 in FIG. 2
- an attended embedding e.g., 234
- the system may process the attended embedding of the source code snippet using an output layer (e.g., 226 in FIG. 2 ) to generate, by way of a single transformation of the attended embedding of the source code snippet, data indicative of a translation of the source code snippet in the second programming language.
- output layer 226 is a softmax layer that generates, as part of a single transformation, matrix 236 of probability distributions.
- ML module 230 generates each probability distribution of output matrix 236 in parallel with the other probability distributions, as opposed to generating tokens one after another as would be the case with a conventional decoder.
- the system may sample from the probability distributions of output matrix 236 to generate one or more translations of the original source code snippet in the second programming language.
- sampling module 238 may sample output matrix 236 multiple times to create different candidate translations until, for instance, a confidence measure or quality score associated with a new translation candidate falls beneath a threshold, at which point it may not be worth presenting to a programmer.
- FIG. 5 is a flowchart illustrating an example method 500 of iteratively training a translation model (e.g., 220 ) to translate source code from a first programming language to a second programming language, without using a sequence-to-sequence decoder, in accordance with various implementations.
- a translation model e.g., 220
- FIG. 5 is a flowchart illustrating an example method 500 of iteratively training a translation model (e.g., 220 ) to translate source code from a first programming language to a second programming language, without using a sequence-to-sequence decoder, in accordance with various implementations.
- a translation model e.g., 220
- FIG. 5 is a flowchart illustrating an example method 500 of iteratively training a translation model (e.g., 220 ) to translate source code from a first programming language to a second programming language, without using a sequence-to-sequence decoder, in accord
- a training pair of source code snippets is available, including an original source code snippet written in a first programming language and a ground truth translation of the original source code snippet in a second (or target) programming language.
- the system e.g., by way of ML module 230 , may extract tokens from the ground truth translation of the original source code snippet.
- the ground truth translation of the original source code snippet has 512 tokens
- 512 tokens may be extracted.
- These tokens may be referred to herein as “ground truth” tokens because they will be compared to predicted tokens to train the translation model.
- the system may then enter an iterative loop in which different subsets of predicted tokens are compared to different subsets of ground truth tokens to train the model.
- the system may determine whether there are any more tokens that were extracted during block 502 . If the answer is yes, then method 500 may proceed to block 506 .
- the system may select a next extracted token subset of one or more ground truth tokens as the current ground truth subset. Then, method 500 may proceed to block 508 .
- the system may process the original source code snippet (e.g., 228 in FIG. 2 ) using one or more encoder layers (e.g., 222 - 1 to 222 -X in FIG. 2 ) of the translation model ( 220 in FIG. 2 ) to generate an embedding (e.g., 232 in FIG. 2 ) of the original source code snippet.
- the processing of block 508 may include token extraction and encoding, e.g., using token extraction model 221 .
- the system may process the embedding of the original source code snippet using an attention layer (e.g., 224 in FIG. 2 ) to generate an attended embedding (e.g., 234 in FIG. 2 ).
- the system e.g., by way of ML module 230 , may process the attended embedding using an output layer (e.g., 226 in FIG. 2 ) to generate, by way of a single transformation, data indicative of a predicted translation of the original source code snippet in the second programming language.
- this data indicative of a predicted translation may include, for instance, a two-dimensional matrix of probability distributions across the vocabulary of the target or second programming language.
- the system may utilize teaching forcing and/or curriculum learning to train the translation model.
- the system e.g., by way of ML module 230 , may select a different subset of one or more predicted tokens of the predicted translation of the original source code snippet in the second language that has not yet been used to train the translation model.
- this subset may correspond to the subset of ground truth tokens selected at block 506 .
- the subset of ground truth tokens selected at block 506 included the first token of the ground truth translation of the original source code snippet, then the subset of predicted tokens of the predicted translation selected at block 514 may similarly include the first predicted token.
- the subset of ground truth tokens selected at block 506 included the first 136 tokens of the ground truth translation of the original source code snippet
- the subset of predicted tokens of the predicted translation selected at block 514 may similarly include the first 136 predicted tokens.
- the system may compare the subsets of one or more tokens selected at blocks 506 to those selected at block 514 . Based on the comparison, at block 518 , the system, e.g., by way of ML module 230 , may train the translation model, e.g., using techniques such as back propagation, gradient descent, etc. Method 500 may then proceed back to block 504 . If there are more extracted tokens of the ground truth translation of the original source code snippet that have not yet been used to train the translation model, then a subset of those may be selected at block 506 and method 500 may proceed as described above. However, if at block 504 , the system determines that there are no more extracted tokens that haven't been used for training, then method 500 may end.
- FIG. 6 is a block diagram of an example computing device 610 that may optionally be utilized to perform one or more aspects of techniques described herein.
- Computing device 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via bus subsystem 612 .
- peripheral devices may include a storage subsystem 624 , including, for example, a memory subsystem 625 and a file storage subsystem 626 , user interface output devices 620 , user interface input devices 622 , and a network interface subsystem 616 .
- the input and output devices allow user interaction with computing device 610 .
- Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.
- User interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computing device 610 or onto a communication network.
- User interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computing device 610 to the user or to another machine or computing device.
- Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 624 may include the logic to perform selected aspects of the method of FIGS. 4 - 5 , as well as to implement various components depicted in FIGS. 1 - 2 .
- Memory 625 used in the storage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored.
- a file storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 626 in the storage subsystem 624 , or in other machines accessible by the processor(s) 614 .
- Bus subsystem 612 provides a mechanism for letting the various components and subsystems of computing device 610 communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
- Computing device 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 610 are possible having more or fewer components than the computing device depicted in FIG. 6 .
Abstract
Description
- Computer software programming often requires developers to read and/or write source code (i.e., to program) in a specific language, e.g. Java, C++, C, Python, etc. Each programming language has its own strengths, weaknesses, nuances, idiosyncrasies, etc. Additionally, some programming languages are more suitable for certain stages of software development and/or a software life cycle than others. As one example, scripting languages such as Python, JavaScript, Perl, etc., are often more effectively used near the very beginning of software development because programmers using these languages are able to turn around functional software relatively quickly. Most programmers obtain at least a superficial understanding of multiple programming languages, but only master a few. Consequently, each programming language tends to have its own talent pool.
- Large language models such as transformer networks have become increasingly popular for performing natural language processing. Transformer networks were designed in part to mitigate a variety of shortcomings of prior natural language processing models, such as overfitting, the vanishing gradient problem, and exceedingly high computational costs, to name a few. However, transformer networks are still implemented typically as sequence-to-sequence, encoder-decoder models. The decoder portions in particular require significant sequential computational processing, and as a consequence, introduce considerable latency during both training and inference.
- Techniques are described herein for translating source code in a “base” programming language to source code in another programming language, or “target” programming language, using machine learning. Among other things, this allows programmers who might be unfamiliar with a base programming language to nonetheless view and/or edit source code written in the base language by first translating the source code to another, more familiar programming language.
- More particularly, but not exclusively, implementations are described herein for translating source code between programming languages, independently of sequence-to-sequence decoding, e.g., using only the encoder part of a transformer network. In place of sequence-to-sequence decoding, one or more layers of a machine learning model configured with selected aspects of the present disclosure may be trained to generate, in parallel, all target language tokens as a single transformation, corresponding to a translation of a source code snippet in a target programming language. For example, a matrix of probability distributions over a vocabulary of the target programming language may be generated. Tokens of the translation of the source code snippet may then be selected based on these probability distributions.
- In some implementations, a method may be provided for translating a source code snippet from a first programming language to a second programming language independently of sequence-to-sequence decoding. The method may be implemented by one or more processors and may include: processing the source code snippet written in the first programming language using an encoder portion of a transformer network to generate an embedding of the source code snippet; processing the embedding of the source code snippet using an all-pair attention layer to generate an attended embedding of the source code snippet; and processing the attended embedding of the source code snippet using an output layer to generate, by way of a single transformation of the attended embedding of the source code snippet, data indicative of a translation of the source code snippet in the second programming language.
- In various implementations, the data indicative of the translation may be a matrix of probability distributions. In various implementations, the matrix may be an x by y matrix, with x being a non-zero integer equal to a number of tokens extracted from the source code snippet. In various implementations, y is a non-zero integer equal to a vocabulary size of the second programming language.
- In various implementations, the method may further include selecting the tokens of the translation of the source code snippet in the second programming language based on maximum probabilities in the matrix of probability distributions. In various implementations, the method may further include sampling from the matrix of probability distributions to generate one or more alternative translations of the source code snippet. In various implementations, the sampling may include using the Gumbel-max trick, random temperature-based sampling, top-k sampling, or nucleus sampling.
- In another related aspect, a method of training a translation model to translate source code from a first programming language to a second programming language, without using a sequence-to-sequence decoder, may include: iteratively training the translation model using a pair of source code snippets, including an original source code snippet written in the first programming language and a ground truth translation of the original source code snippet in the second programming language, wherein during each iteration, the training includes: processing the original source code snippet using one or more encoder layers of the translation model to generate a embedding of the original source code snippet; processing data indicative of the embedding using an output layer to generate, by way of a single transformation, data indicative of a predicted translation of the original source code snippet in the second programming language; selecting a different subset of one or more predicted tokens of the predicted translation of the original source code snippet in the second language that has not yet been used to train the translation model; comparing the selected subset of one or more tokens to a corresponding subset of one or more tokens of the ground truth translation of the original source code snippet in the second programming language; and training the machine learning model based on the comparison.
- In various implementations, during each iteration, the training may include processing the embedding of the source code snippet using an all-pair attention layer to generate an attended embedding of the source code snippet, wherein the data indicative of the first embedding comprises the attended embedding. In various implementations, the translation model may be a transformer network.
- In various implementations, the output layer may be a softmax layer. In various implementations, the data indicative of the predicted translation may be a matrix of probability distributions, such as an x by y matrix, with x being a non-zero integer equal to a number of tokens extracted from the source code snippet and y being a non-zero integer equal to a vocabulary size of the second programming language.
- In addition, some implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
-
FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations. -
FIG. 2 is a block diagram showing an example of how source code may be translated, in accordance with various implementations. -
FIG. 3 depicts an example application of techniques described herein, in accordance with various implementations. -
FIG. 4 depicts a flowchart illustrating an example method for practicing selected aspects of the present disclosure. -
FIG. 5 depicts another flowchart illustrating an example method for practicing selected aspects of the present disclosure. -
FIG. 6 illustrates an example architecture of a computing device. - Implementations are described herein for translating structured textual data between domain-specific languages. More particularly, but not exclusively, implementations are described herein for translating source code between programming languages, independently of sequence-to-sequence decoding, e.g., using only the encoder part of a transformer network. In place of sequence-to-sequence decoding, one or more layers of a machine learning model configured with selected aspects of the present disclosure may be trained to generate, as a single transformation, data indicative of a translation of a source code snippet in a target programming language. For example, a matrix of probability distributions over a vocabulary of the target programming language may be generated in parallel. Tokens of the translation of the source code snippet may then be selected based on these probability distributions.
- In some implementations, a machine learning model configured with selected aspects of the present disclosure may include part of a transformer network, such as a BERT (Bidirectional Encoder Representations from Transformers) transformer and/or a GPT (Generative Pre-trained Transformer). For example, one or more encoder layers of a BERT transformer may be directly coupled with downstream layers configured with selected aspects of the present disclosure to facilitate source code translation independently of sequence-to-sequence decoding. In some implementations, these encoder layers—and/or a whole transformer model from which these layers are obtained—may be trained initially using a corpus of documents and other data that is relevant to structured text in general, or programming languages in particular. These documents may include, for instance, source code examples, programming handbooks or textbooks, general programming documentation, natural language comments embedding in source code, and so forth.
- After this initial training, the encoder layer(s) may be coupled with custom layers described herein to create an “encoder-only,” or “decoder-free,” translation model configured with selected aspects of the present disclosure. Once trained as described below, the encoder layer(s) of the translation model may be applied to a source code snippet in a base programming language to generate a semantically-rich source code embedding. In some implementations, this source code embedding may then be processed using an attention layer, such as an “all-pair” attention layer, to generate what will be referred to herein as an “attended” embedding or “summary representation.” This attended embedding may then be processed using an output layer (e.g., softmax) of the translation model to generate, as a single transformation, the aforementioned data indicative of a translation of the source code snippet in the target programming language. As noted above, in some implementations, this data may include a matrix of probability distributions over the vocabulary of the target programming language.
- Attempting to train the translation model based on a comparison of an entire predicted translation to an entire ground truth translation may not be practical, and/or may not generate an accurate translation model. Accordingly, in various implementations, the translation model may be trained using curriculum and/or teacher forcing training. For example, the above-described layers of the translation model (e.g., encoder(s), attention layer, output layer) may be iteratively applied to the same source code snippet in a base programming language as described above to generate, during each iteration, a new matrix of probability distributions. Each probability distribution may be used to select a token of a potential translation of the source code snippet in the target programming language. In some implementations, a subset or subsequence of these selected tokens, such as a single token, may be compared to corresponding ground truth tokens during a given training iteration. Subsequent tokens of the translation may be ignored for this iteration of training.
- Once an iteration of training is complete, during a next training iteration, the source code snippet may once again be processed using the translation model. However, during this next iteration, a “next” subset (or subsequence) of selected tokens, such as the just-considered token and the very next token, may be used to train the translation model. Tokens outside of this next subset (e.g., subsequent tokens) may be ignored. This process may be repeated for each sub-sequence (e.g., each additional token) of the translation. As a result, training may be a relatively laborious and time-consuming process. However, the time and computational resources required for inference compared to a conventional translation model that relies on sequence-to-sequence decoding may be reduced dramatically. This may enable the trained translation model to be deployed at the “edge,” e.g., on resource-constrained devices such as programmers' personal computers. Facilitating edge-based translation may provide other benefits as well, such as protecting sensitive source code (e.g., proprietary, trade secrets, etc.) from outside exposure. In addition, this may also enable significant reduction in latency for performing translations, allowing for programming language translation in real-time development environments.
- One consequence of foregoing traditional decoding is that decoder-based beam searching may no longer be available. A benefit of decoder-based beam searching is that alternative translations can be readily identified/generated during decoding, e.g., so that a user can, for instance, toggle through multiple candidate translations to select the best one, or even have multiple alternative tokens suggested for translation. To enable generation of alternative translations without decoder-based beam searching, various techniques may be used to sample tokens from the matrix of probability distributions. For example, rather than naively selecting the maximum probability token from every probability distribution, techniques such as the Gumbel-max trick, random temperature-based sampling, top-k sampling, or nucleus sampling, may be used to select tokens for alternative translation(s) that appear, at least to the user, similar to alternative translations generated conventionally using decoder-based beam searching.
-
FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations. Any computing devices depicted inFIG. 1 or elsewhere in the figures may include logic such as one or more microprocessors (e.g., central processing units or “CPUs”, graphical processing units or “GPUs”, tensor processing units or “TPUs”) that execute computer-readable instructions stored in memory, or other types of logic such as application-specific integrated circuits (“ASIC”), field-programmable gate arrays (“FPGA”), and so forth. Some of the systems depicted inFIG. 1 , such as acode knowledge system 102, may be implemented using one or more server computing devices that form what is sometimes referred to as a “cloud infrastructure,” although this is not required. - A
code knowledge system 102 may be provided for helping clients 110-1 to 110-P manage their respective code bases 112-1 to 112-P.Code knowledge system 102 may include, among other things, acode translator 104 that is configured to perform selected aspects of the present disclosure in order to help one or more clients 110-1 to 110-P to manage and/or make changes to one or more corresponding code bases 112-1 to 112-P. Eachclient 110 may be, for example, an entity or organization such as a business (e.g., financial institute, bank, etc.), non-profit, club, university, government agency, or any other organization that operates one or more software systems. For example, a bank may operate one or more software systems to manage the money under its control, including tracking deposits and withdrawals, tracking loans, tracking investments, and so forth. An airline may operate one or more software systems for booking/canceling/rebooking flight reservations, managing delays or cancellations of flight, managing people associated with flights, such as passengers, air crews, and ground crews, managing airport gates, and so forth. -
Code translator 104 may be configured to leverage knowledge of multiple different programming languages in order to aid clients 110-1 to 110-P in translating between programming languages when editing, updating, re-platforming, migrating, or otherwise acting upon their code bases 112-1 to 112-P. For example,code translator 104 may be configured to translate code snippets from one programming language to another, e.g., on the fly or in batches. This may, for instance, enable a developer fluent in a first programming language to view and/or edit source code that was originally written in a second, less-familiar programming language in the first programming language. It may also significantly decrease the time and/or costs associated with migrating code bases between different programming languages. - In various implementations,
code knowledge system 102 may include a machine learning (“ML” inFIG. 1 )database 105 that includes data indicative of one or more trained machine learning models 106-1 to 106-N. These trained machine learning models 106-1 to 106-N may take various forms that will be described in more detail below, including but not limited to BERT (Bidirectional Encoder Representations from Transformers) transformers, GPT (Generative Pre-trained Transformer) transformers, a graph-based network such as a graph neural network (“GNN”), graph attention neural network (“GANN”), or graph convolutional neural network (“GCN”), other types of sequence-to-sequence models and/or encoder-decoders, various flavors of a recurrent neural network (“RNN”, e.g., long short-term memory, or “LSTM”, gate recurrent units, or “GRU”, etc.), and any other type of machine learning model that may be applied to facilitate selected aspects of the present disclosure. - In some implementations,
code knowledge system 102 may also have access to one or more programming-language-specific corpuses 108-1 to 108-M. In some implementations, these programming-language-specific corpuses 108-1 to 108-M may be used, for instance, to train one or more of the machine learning models 106-1 to 106-N. In some implementations, the programming-language-specific corpuses 108-1 to 108-M may include examples of source code (e.g., entire code bases, libraries, etc.), inline comments, textual metadata associated with source code (e.g., commits), documentation such as textbooks and programming manuals, programming language-specific discussion threads, presentations, academic papers, and so forth. - In some implementations, a
client 110 that wishes to enable manipulation of itscode base 112 in programming language(s) other than that/those used originally to write the source code may establish a relationship with an entity (not depicted inFIG. 1 ) that hostscode knowledge system 102. When a developer wishes to view/edit a source code snippet of the entity'scode base 112 but is unfamiliar with the native programming language,code translator 104 may provide one or more versions of the source code snippet that is translated to a target programming language preferred by the developer. In some such implementations,code translator 104 may generate the translated source code snippet on the fly, e.g., in real time. In other implementations,code translator 104 may operate, e.g., in a batch mode, to preemptively translate all or selection portions of an entity'scode base 112 into a targeted programming language. In some implementations in which the developer then edits the translated source code snippet, the edited version may be translated back into the native programming language or left in the new, target programming language, assuming other necessary infrastructure is in place. - In other implementations, trained translation models may be deployed closer to or at the edge, e.g., at client devices 110-1 to 110-P. Because these trained translation models do not necessarily include sequence-to-sequence decoders, and because encoders tend to be more computationally efficient than decoders, these trained translation models may be effectively applied at the edge, rather than in the cloud. As mentioned previously, edge-based deployment may give rise to a variety of benefits, such as maintenance of privacy, protection of sensitive source code, and so forth.
-
FIG. 2 is a block diagram of an example process flow that may be implemented in whole or in part bycode translator 104 in order to use a translation machine learning model 220 (also referred to simply as a “translation model 220”) to translate source code between programming languages. In this example, translationmachine learning model 220 includes some number of encoder layers 222-1 to 222-X, followed by anattention layer 224 and anoutput layer 226. - In some implementations. encoder layers 222-1 to 222-X may correspond, for instance, to encoder layers of a large language model such as a BERT or GTP transformer. For example, an entire transformer network, e.g., encoder layers 222-1 to 222-X plus one or more decoder layers (not depicted in
FIG. 2 ), may be trained or bootstrapped on any number of programming language-specific corpuses 108-1 to 108-M. Once trained, the decoders (not depicted inFIG. 2 ) may be discarded or decoupled from the encoder layers 222-1 to 222-X. The encoder layers 222-1 to 222-X may then be coupled withattention layer 224 andoutput layer 226 to formtranslation model 220, which can be further trained as described herein. -
Code translator 104 may include a machine learning (“ML” inFIG. 2 )module 230 that is configured to process various data using machine learning model(s) to translate source between programming languages. To start,ML module 230 may extract tokens from first programming languagesource code snippet 228 and may use atoken extraction model 221 to generatetoken embeddings 223 of those tokens.Token extraction model 221 may take various forms, such as that often used as part of the word2vec framework. -
ML module 230 may then apply thesetoken embeddings 223 as input across encoders 222-1 to 222-X to generate a source code embedding 232 of first programming language source code snippet 228 (alternatively referred to herein as an “encoding” of the source code snippet). Source code embedding 232 may be a semantically-rich representation/encoding of first programming languagesource code snippet 228, including the tokens and relationships/logic between the tokens that is defined by the syntax and structure of first programming languagesource code snippet 228. - Embeddings such as
token embeddings 223 generated based ontoken extraction model 221 and/or source code embedding 232 may take various forms, such as a one dimensional vector or a two-dimensional matrix. As one non-limiting example, suppose there are 512 tokens in first programming languagesource code snippet 228, and that the dimension of token embeddings generated based ontoken extraction model 221 is 768. The resultingtoken embeddings matrix 223 may be a 512×768 matrix of token embeddings. Similarly, if source code embedding 232 has 2048 different dimensions, then source code embedding 232 may be a 512×2048 matrix. These dimensions are merely examples, and other dimensions are possible. - In various implementations,
ML module 230 may then apply source code embedding 232 as input acrossattention layer 224. In some implementations,attention layer 224 may be an “all pairs” attention layer that attends across all pairs of features represented in source code embedding 232. The result may be an attended source code embedding 234. As with other embeddings (223, 232) depicted inFIG. 2 , attended source code embedding 234 may have various dimensions. If source code embedding 232 is 512×2048, in some implementations, attended source code embedding 234 may also be a 512×2048 matrix. - Attended source code embedding 234 may then be applied across
output layer 226 to generate, in parallel as a single transformation (as opposed to sequence-to-sequence decoding), data indicative of a translation of first programming languagesource code snippet 228. InFIG. 2 , for instance,output layer 226 may be a softmax layer that generates, as a single transformation of attended source code embedding 234, anoutput matrix 236.Output matrix 236 may be an x by y (both positive integers) matrix of probability distributions over a vocabulary of the second programming language. In some implementations,output matrix 236 may have, as its x dimension, a number of token inputs, and as its y dimension, a vocabulary size. Accordingly, continuing the example used above, if there are 512 input tokens, and the vocabulary size is 20,000, thenoutput matrix 236 may have a dimension of 512×20,000. - In various implementations, a
sampling module 238 may be configured to sample a token from each probability distribution ofoutput matrix 236. InFIG. 2 , each probability distribution corresponds to a column ofoutput matrix 236. In some implementations,sampling module 238 may select the token with the maximum probability from each probability distribution (column). These selected tokens {TOKEN-1, TOKEN-2, . . . } may then be used to formulate a second programming languagesource code snippet 240 that is a translation of first programming languagesource code snippet 228. - Because
translation model 220 forgoes sequence-to-sequence decoding, and instead generates the probability distributions of 236 in parallel, some sequence-to-sequence decoder-specific techniques for generating alternative translations, such as beam searching, may not be available. However, it may still be desirable to provide a programmer with variations of a translation of a first programming language snippet, e.g., so that the programmer can use their judgment to select the best translation (this feedback can also be used continuously to train translation model 220). - Accordingly, in various implementations,
sampling module 238 may sample multiple times fromoutput matrix 236 to generate a plurality of candidate secondprogramming language snippets 240. Rather than simply selecting the token from each probability distribution with the maximum probability,sampling module 238 may use other sampling techniques, including but not limited to the Gumbel-max trick, random temperature-based sampling, top-k sampling, or aureus sampling. -
FIG. 3 depicts an example scenario in which a code snippet written in one programming language may be translated to another programming language. In this example, the basesource code snippet 360 is written in Java and prints the integers one to five. At bottom, a graphical user interface (“GUI”) 362 is depicted that may be presented to a developer who is unfamiliar with Java, but who has expertise in another programming language. In this example, thecode snippet 360 written in Java is converted bycode translator 104 into Python and rendered as part ofGUI 362. In this way, thedeveloper operating GUI 362 may view the source code in a programming language with which he or she is more familiar. In some cases, the developer may be able to edit the translated source code. In some such implementations, the edits made by the developer (i.e. to the Python code inFIG. 3 ) may be translated back to Java before being stored and/or more permanently incorporated into the code base. In other implementations, the edited Python code may be incorporated into the code base. - In some implementations, the
original source code 360 may be sent to codeknowledge system 102 for translation bycode translator 104 prior to being sent to the computing device (not depicted) that rendersGUI 362. In other implementations,GUI 362 may be part of a software development application that performs the programming language translation locally, e.g., using a plug-in or built-in functionality. The scenario ofFIG. 3 is for illustrative purposes only. Source code may be translated between programming languages using techniques described herein for any number of applications. - For example, suppose a first user who is trained in a base programming language sends a source code snippet in the base programming language to a second user, e.g., as an attachment or in the body of an email. In some implementations, the source code in the based programming language may be translated into a target programming language en route to the second user, e.g., by
code translator 104. Additionally or alternatively, in some implementations, the second user's email application (or an email server that stores emails of the second user) may have a plugin configured with selected aspects of the present disclosure. - In some implementations, a single user may operate a software development application to view multiple different source code snippets written in multiple different programming languages that are unfamiliar to the user. In some such examples, multiple respective translation models may be used to translate the source code snippets from the multiple different programming languages to a language (or languages) that are better understood to the user.
- In some implementations, techniques described herein may be used to automatically convert source code written in one programming language into source code in another programming language, without necessarily presenting translated source code to users as described previously. For example, a company may decide to replatform an existing
code base 112 to a new programming language, e.g., to obtain new functionality and/or technical benefits (e.g., security features, processing speed features, etc.) that were unavailable with the original programming language. Such a company may be able to deploy techniques described herein, or request that an entity associated withcode knowledge system 102 deploy techniques described herein, to automatically convert all or a portion of acode base 112 from one programming language to another. - As mentioned previously, it may be desirable to present a programmer with multiple different candidate translations of a source code snippet, e.g., so that the programmer can use their judgment to determine which candidate is best. Accordingly, in
FIG. 3 , a selectable link is presented (“CLICK HERE TO VIEW NEXT CANDIDATE TRANSLATION”) that a user can select to see an alternative translation of the original source code snippet. In some implementations, these candidate translations may be presented to the user in a ranked order. This ranked order may be determined in various ways, such as by how many (or few) errors or warnings are raised when attempts are made to parse and/or compile the candidate translations (e.g., in the background without the user being aware). For example, various types of analysis associated with compiling, such as lexical analysis, syntax analysis, semantic analysis, and so forth, may be applied to each candidate translation to determine its score (which may be inversely proportional to the number of errors or warnings generated). The candidates with the “best” scores may be presented to the programmer first. In some implementations, candidate translations may be presented (or at least made available for presentation) until various criteria are met, such as a candidate no longer being capable of being compiled. -
FIG. 4 is a flowchart illustrating anexample method 400 of translating a source code snippet from a first programming language to a second programming language independently of sequence-to-sequence decoding, in accordance with implementations disclosed herein. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components ofcode knowledge system 102. Moreover, while operations ofmethod 400 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 402, the system, e.g., by way ofcode translator 104 and/orML module 230, may process the source code snippet (e.g., 228 inFIG. 2 ) written in the first programming language using an encoder portion (e.g., 222-1 to 222-X inFIG. 2 ) of a transformer network to generate an embedding (e.g., 232) of the source code snippet. While not depicted inFIG. 4 , in some implementations,ML module 230 may first perform a token extraction operation, e.g., using token extraction model 221 (e.g., word2vec), to generatetoken embeddings 223. Thesetoken embeddings 223 may be what is ultimately processed using encoder layers 222-1 to 222-X. - At
block 404, the system, e.g., by way ofcode translator 104, may process the embedding of the source code snippet using an all-pair attention layer (e.g., 224 inFIG. 2 ) to generate an attended embedding (e.g., 234) of the source code snippet. - At
block 406, the system, e.g., by way ofcode translator 104, may process the attended embedding of the source code snippet using an output layer (e.g., 226 inFIG. 2 ) to generate, by way of a single transformation of the attended embedding of the source code snippet, data indicative of a translation of the source code snippet in the second programming language. InFIG. 2 , for instance,output layer 226 is a softmax layer that generates, as part of a single transformation,matrix 236 of probability distributions. Put another way,ML module 230 generates each probability distribution ofoutput matrix 236 in parallel with the other probability distributions, as opposed to generating tokens one after another as would be the case with a conventional decoder. - At block 408, the system, e.g., by way of
sampling module 238, may sample from the probability distributions ofoutput matrix 236 to generate one or more translations of the original source code snippet in the second programming language. In some implementations,sampling module 238 may sampleoutput matrix 236 multiple times to create different candidate translations until, for instance, a confidence measure or quality score associated with a new translation candidate falls beneath a threshold, at which point it may not be worth presenting to a programmer. -
FIG. 5 is a flowchart illustrating anexample method 500 of iteratively training a translation model (e.g., 220) to translate source code from a first programming language to a second programming language, without using a sequence-to-sequence decoder, in accordance with various implementations. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components ofcode knowledge system 102. Moreover, while operations ofmethod 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At the outset of
method 500, it may be assumed that a training pair of source code snippets is available, including an original source code snippet written in a first programming language and a ground truth translation of the original source code snippet in a second (or target) programming language. Atblock 502, the system, e.g., by way ofML module 230, may extract tokens from the ground truth translation of the original source code snippet. Thus, if the ground truth translation of the original source code snippet has 512 tokens, 512 tokens may be extracted. These tokens may be referred to herein as “ground truth” tokens because they will be compared to predicted tokens to train the translation model. The system may then enter an iterative loop in which different subsets of predicted tokens are compared to different subsets of ground truth tokens to train the model. Atblock 504, the system may determine whether there are any more tokens that were extracted duringblock 502. If the answer is yes, thenmethod 500 may proceed to block 506. Atblock 506, the system may select a next extracted token subset of one or more ground truth tokens as the current ground truth subset. Then,method 500 may proceed to block 508. - At
block 508, the system, e.g., by way ofML module 230, may process the original source code snippet (e.g., 228 inFIG. 2 ) using one or more encoder layers (e.g., 222-1 to 222-X inFIG. 2 ) of the translation model (220 inFIG. 2 ) to generate an embedding (e.g., 232 inFIG. 2 ) of the original source code snippet. While not depicted inFIG. 5 for brevity, the processing ofblock 508 may include token extraction and encoding, e.g., usingtoken extraction model 221. - At
block 510, the system, e.g., by way ofML module 230, may process the embedding of the original source code snippet using an attention layer (e.g., 224 inFIG. 2 ) to generate an attended embedding (e.g., 234 inFIG. 2 ). Atblock 512, the system, e.g., by way ofML module 230, may process the attended embedding using an output layer (e.g., 226 inFIG. 2 ) to generate, by way of a single transformation, data indicative of a predicted translation of the original source code snippet in the second programming language. As noted previously, this data indicative of a predicted translation may include, for instance, a two-dimensional matrix of probability distributions across the vocabulary of the target or second programming language. - At blocks 514-518, the system may utilize teaching forcing and/or curriculum learning to train the translation model. For example, at
block 514, the system, e.g., by way ofML module 230, may select a different subset of one or more predicted tokens of the predicted translation of the original source code snippet in the second language that has not yet been used to train the translation model. In some implementations, this subset may correspond to the subset of ground truth tokens selected atblock 506. For example, if the subset of ground truth tokens selected atblock 506 included the first token of the ground truth translation of the original source code snippet, then the subset of predicted tokens of the predicted translation selected atblock 514 may similarly include the first predicted token. If, during a later iteration, the subset of ground truth tokens selected atblock 506 included the first 136 tokens of the ground truth translation of the original source code snippet, then the subset of predicted tokens of the predicted translation selected atblock 514 may similarly include the first 136 predicted tokens. - At
block 516, the system, e.g., by way ofML module 230, may compare the subsets of one or more tokens selected atblocks 506 to those selected atblock 514. Based on the comparison, atblock 518, the system, e.g., by way ofML module 230, may train the translation model, e.g., using techniques such as back propagation, gradient descent, etc.Method 500 may then proceed back to block 504. If there are more extracted tokens of the ground truth translation of the original source code snippet that have not yet been used to train the translation model, then a subset of those may be selected atblock 506 andmethod 500 may proceed as described above. However, if atblock 504, the system determines that there are no more extracted tokens that haven't been used for training, thenmethod 500 may end. -
FIG. 6 is a block diagram of anexample computing device 610 that may optionally be utilized to perform one or more aspects of techniques described herein.Computing device 610 typically includes at least oneprocessor 614 which communicates with a number of peripheral devices viabus subsystem 612. These peripheral devices may include astorage subsystem 624, including, for example, amemory subsystem 625 and afile storage subsystem 626, userinterface output devices 620, userinterface input devices 622, and anetwork interface subsystem 616. The input and output devices allow user interaction withcomputing device 610.Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices. - User
interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term “input device” is intended to include all possible types of devices and ways to input information intocomputing device 610 or onto a communication network. - User
interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term “output device” is intended to include all possible types of devices and ways to output information fromcomputing device 610 to the user or to another machine or computing device. -
Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, thestorage subsystem 624 may include the logic to perform selected aspects of the method ofFIGS. 4-5 , as well as to implement various components depicted inFIGS. 1-2 . - These software modules are generally executed by
processor 614 alone or in combination with other processors.Memory 625 used in thestorage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored. Afile storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations may be stored byfile storage subsystem 626 in thestorage subsystem 624, or in other machines accessible by the processor(s) 614. -
Bus subsystem 612 provides a mechanism for letting the various components and subsystems ofcomputing device 610 communicate with each other as intended. Althoughbus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses. -
Computing device 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description ofcomputing device 610 depicted inFIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations ofcomputing device 610 are possible having more or fewer components than the computing device depicted inFIG. 6 . - While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/717,609 US20230325164A1 (en) | 2022-04-11 | 2022-04-11 | Translating between programming languages independently of sequence-to-sequence decoders |
PCT/US2023/017719 WO2023200668A1 (en) | 2022-04-11 | 2023-04-06 | Translating between programming languages independently of sequence-to-sequence decoders |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/717,609 US20230325164A1 (en) | 2022-04-11 | 2022-04-11 | Translating between programming languages independently of sequence-to-sequence decoders |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230325164A1 true US20230325164A1 (en) | 2023-10-12 |
Family
ID=86328308
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/717,609 Pending US20230325164A1 (en) | 2022-04-11 | 2022-04-11 | Translating between programming languages independently of sequence-to-sequence decoders |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230325164A1 (en) |
WO (1) | WO2023200668A1 (en) |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220214863A1 (en) * | 2021-01-03 | 2022-07-07 | Microsoft Technology Licensing, Llc. | Multi-lingual code generation with zero-shot inference |
US20220284028A1 (en) * | 2021-03-08 | 2022-09-08 | Microsoft Technology Licensing, Llc | Transformer for encoding text for use in ranking online job postings |
US20220308845A1 (en) * | 2021-03-24 | 2022-09-29 | Bank Of America Corporation | Systems and methods for assisted code development |
US20230161567A1 (en) * | 2021-11-24 | 2023-05-25 | Microsoft Technology Licensing, Llc. | Custom models for source code generation via prefix-tuning |
US20230229912A1 (en) * | 2020-09-21 | 2023-07-20 | Huawei Technologies Co., Ltd. | Model compression method and apparatus |
US20230281318A1 (en) * | 2022-03-07 | 2023-09-07 | Microsoft Technology Licensing, Llc. | Constrained decoding for source code generation |
US20230305824A1 (en) * | 2022-03-24 | 2023-09-28 | Microsoft Technology Licensing, Llc. | Code adaptation through deep learning |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11842174B2 (en) * | 2019-07-09 | 2023-12-12 | Google Llc | Translating between programming languages using machine learning |
-
2022
- 2022-04-11 US US17/717,609 patent/US20230325164A1/en active Pending
-
2023
- 2023-04-06 WO PCT/US2023/017719 patent/WO2023200668A1/en unknown
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230229912A1 (en) * | 2020-09-21 | 2023-07-20 | Huawei Technologies Co., Ltd. | Model compression method and apparatus |
US20220214863A1 (en) * | 2021-01-03 | 2022-07-07 | Microsoft Technology Licensing, Llc. | Multi-lingual code generation with zero-shot inference |
US20220284028A1 (en) * | 2021-03-08 | 2022-09-08 | Microsoft Technology Licensing, Llc | Transformer for encoding text for use in ranking online job postings |
US20220308845A1 (en) * | 2021-03-24 | 2022-09-29 | Bank Of America Corporation | Systems and methods for assisted code development |
US20230161567A1 (en) * | 2021-11-24 | 2023-05-25 | Microsoft Technology Licensing, Llc. | Custom models for source code generation via prefix-tuning |
US20230281318A1 (en) * | 2022-03-07 | 2023-09-07 | Microsoft Technology Licensing, Llc. | Constrained decoding for source code generation |
US20230305824A1 (en) * | 2022-03-24 | 2023-09-28 | Microsoft Technology Licensing, Llc. | Code adaptation through deep learning |
Also Published As
Publication number | Publication date |
---|---|
WO2023200668A1 (en) | 2023-10-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11169786B2 (en) | Generating and using joint representations of source code | |
US11842174B2 (en) | Translating between programming languages using machine learning | |
US20210192321A1 (en) | Generation and utilization of code change intents | |
US11604628B2 (en) | Generation and/or recommendation of tools for automating aspects of computer programming | |
CA3141560A1 (en) | Automated identification of code changes | |
US11481210B2 (en) | Conditioning autoregressive language model to improve code migration | |
US11537797B2 (en) | Hierarchical entity recognition and semantic modeling framework for information extraction | |
US11748065B2 (en) | Learning and using programming styles | |
US11481202B2 (en) | Transformation templates to automate aspects of computer programming | |
US11822909B2 (en) | Adapting existing source code snippets to new contexts | |
US20210397416A1 (en) | Generating a Pseudo-Code from a Text Summarization Based on a Convolutional Neural Network | |
US11775271B1 (en) | Annotations for developers | |
Hong et al. | Knowledge-grounded dialogue modelling with dialogue-state tracking, domain tracking, and entity extraction | |
Helcl et al. | Neural monkey: The current state and beyond | |
US20230325164A1 (en) | Translating between programming languages independently of sequence-to-sequence decoders | |
Kirsch et al. | Noise reduction in distant supervision for relation extraction using probabilistic soft logic | |
US20230350657A1 (en) | Translating large source code using sparse self-attention | |
US20240086164A1 (en) | Generating synthetic training data for programming language translation | |
US11893384B2 (en) | Refactoring and/or rearchitecting source code using machine learning | |
Turakhia et al. | Time Estimation as Critical Factor of Software Failure: A Systematic Literature Review Protocol | |
Deshmukh et al. | Intelligent Sanskrit translator using NLP | |
Liao et al. | BRQG: A BART-Based Retouching Framework for Multi-hop Question Generation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: X DEVELOPMENT LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:ZAHEER, MANZIL;SINGH, RISHABH;REEL/FRAME:060360/0032Effective date: 20220408 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:X DEVELOPMENT LLC;REEL/FRAME:062572/0565Effective date: 20221227 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |