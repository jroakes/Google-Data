CN114450967A - System and method for playback of augmented reality content triggered by image recognition - Google Patents
System and method for playback of augmented reality content triggered by image recognition Download PDFInfo
- Publication number
- CN114450967A CN114450967A CN202080068204.2A CN202080068204A CN114450967A CN 114450967 A CN114450967 A CN 114450967A CN 202080068204 A CN202080068204 A CN 202080068204A CN 114450967 A CN114450967 A CN 114450967A
- Authority
- CN
- China
- Prior art keywords
- computing device
- client computing
- augmented reality
- target entity
- content
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/2343—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements
- H04N21/234318—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements by decomposing into objects, e.g. MPEG-4 objects
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/2343—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements
- H04N21/234345—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving reformatting operations of video signals for distribution or compliance with end-user requests or end-user device requirements the reformatting operation being performed only on part of the stream, e.g. a region of the image or a time segment
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/41—Structure of client; Structure of client peripherals
- H04N21/422—Input-only peripherals, i.e. input devices connected to specially adapted client devices, e.g. global positioning system [GPS]
- H04N21/4223—Cameras
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/431—Generation of visual interfaces for content selection or interaction; Content or additional data rendering
- H04N21/4312—Generation of visual interfaces for content selection or interaction; Content or additional data rendering involving specific graphical features, e.g. screen layout, special fonts or colors, blinking icons, highlights or animations
- H04N21/4316—Generation of visual interfaces for content selection or interaction; Content or additional data rendering involving specific graphical features, e.g. screen layout, special fonts or colors, blinking icons, highlights or animations for displaying supplemental content in a region of the screen, e.g. an advertisement in a separate window
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/47—End-user applications
- H04N21/472—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content
- H04N21/4728—End-user interface for requesting content, additional data or services; End-user interface for interacting with content, e.g. for content reservation or setting reminders, for requesting event notification, for manipulating displayed content for selecting a Region Of Interest [ROI], e.g. for requesting a higher resolution version of a selected region
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/60—Network structure or processes for video distribution between server and client or between remote clients; Control signalling between clients, server and network components; Transmission of management data between server and client, e.g. sending from server to client commands for recording incoming content stream; Communication details between server and client
- H04N21/65—Transmission of management data between client and server
- H04N21/654—Transmission by server directed to the client
- H04N21/6547—Transmission by server directed to the client comprising parameters, e.g. for client setup
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/24—Indexing scheme for image data processing or generation, in general involving graphical user interfaces [GUIs]
Abstract
Systems and methods are provided for relatively fast and relatively easy access to dynamic Augmented Reality (AR) content, including AR content. An enabled target entity may be identified within a camera view of a mobile computing device, and a server computing device may identify the target entity, match the target entity with dynamic AR content, and transmit the dynamic AR content to the client computing device. This may allow a user to consume content-rich information and/or experience real-time interaction with the image and objects surrounding it that would otherwise be difficult and/or impractical to condense into physical media, and/or would otherwise be difficult and/or impractical to provide in a real-time interactive experience.
Description
Technical Field
This relates to the display of image content, in particular to the playback of augmented reality video content.
Background
Users of mobile computing devices (such as, for example, smartphones, tablet computing devices, etc.) may prefer to use their mobile computing devices to better understand objects, entities, etc. about the user and in the environment in which the computing device is operating. Mobile computing devices typically include an image sensor or camera. These cameras may capture images of entities in the environment in which the computing device operates. The availability of content or experiences related to those entities via the mobile computing device may improve the user experience and may enhance the functionality and utility of the mobile computing device to the user.
Disclosure of Invention
In one general aspect, a computer-implemented method can include: capturing an image within a live view finder of a client computing device; transmitting, by a client computing device, a query to a server computing device, the query comprising an image; receiving, by the client computing device, a response to the query from the server computing device, the response including augmented reality content; and triggering display of a user interface screen on the client computing device, the user interface screen including augmented reality content displayed within an outline defined by coordinates included in a response received from the server computing device.
In some implementations, the image may include a target entity captured within a field of view of a live view of the client computing device. In some implementations, receiving a response to the query can include receiving augmented reality video content associated with a target entity included in the image. In some implementations, receiving a response to the query from the server computing device can include receiving coordinates defining an outline for displaying the augmented reality video content on the user interface screen, the outline corresponding to a periphery of the target entity.
In some embodiments, triggering display of the user interface screen may include: displaying a user interface screen within a live view finder of a client computing device; and attaching the display of the augmented reality video content to the target entity at an attachment point corresponding to the coordinates such that the display of the augmented reality video content remains attached to the target entity within the live view of the client computing device.
In some embodiments, the computer-implemented method may further comprise: detecting a first movement of a client computing device; shifting a display of a target entity within a live view of the client computing device in response to the detected first movement; and shifting display of the augmented reality video content to correspond to the shifted display position of the target entity within the live view finder of the client computing device. In some embodiments, detecting the first movement may include: detecting at least one of a change in position, orientation, or distance between a live view of a client computing device and a target entity; and the display of the displaced augmented reality video content may include: at least one of a display position, a display orientation, or a display size of the augmented reality video content is changed to correspond to the changed display position, orientation, or distance of the target entity, respectively. In some embodiments, the computer-implemented method may further comprise: detecting a second movement of the client computing device such that the target entity is not captured within a live view finder of the client computing device; and terminating display of the augmented reality video content in response to the detected second movement. In some embodiments, triggering display of the user interface screen may include: the augmented reality content is cycled until a second movement is detected or until a termination input is received.
In some embodiments, triggering display of the user interface screen may include: triggering display of a display panel and a user input panel; and receiving a response to the query may include: receiving augmented reality video content for display on a display panel of a user interface screen; and receiving auxiliary information associated with the target entity for display on a user input panel of the user interface screen.
In some implementations, capturing images, transmitting, receiving, and triggering within a live view of a client computing device is done within an application running on the client computing device.
In another general aspect, a computer-implemented method can include: receiving, by a server computing device, a query from a client computing device, the query comprising an image; detecting, by a recognition engine of a server computing device, a target entity within an image included in a query; matching, in an index database of a server computing device, a target entity with augmented reality content from an external provider; and transmitting the augmented reality content to the client computing device for output by the client computing device.
In some implementations, receiving a query that includes an image can include: a query is received that includes an image captured by a target entity within a live view finder of a client computing device. In some implementations, detecting a target entity within an image included in a query can include: the target entity is identified based on a target image linked by an external provider with the recognition engine. In some implementations, matching the target entity with the augmented reality content can include: the target entity is matched with content that is linked with the target image by an external provider in an index database. In some embodiments, detecting the target entity may include: detecting a peripheral outline of a target entity in the image; and defining attachment points along the detected peripheral outline to define a peripheral outline of the target entity. In some implementations, transmitting augmented reality content for output by a client computing device can include: the attachment points are transmitted to the client computing device along with the augmented reality content such that the client computing device outputs the augmented reality content within the outline defined by the attachment points.
In another general aspect, a computer-readable storage medium may store instructions that, when executed by one or more processors, may cause the one or more processors to perform the above-described computer-implemented method.
In another general aspect, a client computing device may include a live viewfinder, one or more processors, and memory storing instructions that, when executed by the one or more processors, may cause the one or more processors to perform the computer-implemented method described above.
In another general aspect, a server computing device may include an identification engine, an index database, one or more processors, and memory storing instructions that, when executed by the one or more processors, cause the one or more processors to perform the computer-implemented method described above.
The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a block diagram of an exemplary system according to embodiments described herein.
Fig. 2A and 2B are third person views of an exemplary physical space in which embodiments of an exemplary client computing device of the exemplary system shown in fig. 1 are operable, according to embodiments described herein.
Fig. 3A-3H illustrate a sequence of exemplary user interface screens of the exemplary client computing device 102 shown in fig. 2A and 2B for displaying exemplary augmented reality content associated with a detected target entity, according to embodiments described herein.
Fig. 4A-4J illustrate a sequence of exemplary user interface screens of the exemplary client computing device 102 shown in fig. 2A and 2B for displaying exemplary augmented reality content and ancillary information associated with a detected target entity, according to embodiments described herein.
Fig. 5 is a flow diagram of an exemplary method of accessing and displaying augmented reality content associated with a target entity according to an embodiment described herein.
Fig. 6 is a flow diagram of an example method of processing a visual content query to provide example augmented reality content and auxiliary information associated with a detected target entity, according to an embodiment described herein.
Fig. 7A-7D illustrate identification of target images within various materials according to embodiments described herein.
Fig. 8A-8F illustrate a sequence of exemplary user interface screens of the exemplary client computing device 102 shown in fig. 2A and 2B for displaying exemplary augmented reality content, according to embodiments described herein.
Fig. 9 is a flow diagram of an exemplary method of linking a target image and augmented reality content to an exemplary server computing device according to an embodiment described herein.
FIG. 10 is a schematic diagram of an exemplary computing device and mobile computing device that may be used to implement the techniques described herein.
Reference will now be made in detail to non-limiting examples of the present disclosure, examples of which are illustrated in the accompanying drawings. Examples are described below with reference to the drawings, wherein like reference numerals refer to like elements. When like reference numerals are shown, the corresponding description is not repeated, and the interested reader describes like elements with reference to the previously discussed figures.
Detailed Description
Systems and methods according to embodiments described herein may allow users to access and experience Augmented Reality (AR) content, including dynamic AR content, relatively quickly and relatively easily in response to the identification of an enabling entity in a camera view of a mobile computing device. Systems and methods according to embodiments may allow providers to attach AR content, including dynamic AR content, to identifiable enabling entities in a relatively simple manner. In particular, the present disclosure describes technical improvements that simplify the identification and presentation of AR content, including dynamic AR content, based on visual content captured and/or identified within a camera view of a mobile computing device. In some implementations, the systems and methods described herein can generate an index of AR content (including dynamic AR content) related to an enabled entity identified in a camera view of a mobile computing device. In some implementations, this index of AR content may allow a user to access AR content through a single application running on a mobile computing device via network-accessible resources (e.g., web pages) located around the world. Thus, systems and methods according to embodiments described herein may allow a user to use a mobile computing device to learn more about physical images and objects when encountering them using a single application rather than multiple separate download applications specific to the images, objects, etc. encountered. This may allow a user to consume content-rich information and/or experience real-time interaction with images and their surrounding objects that would otherwise be difficult and/or impractical to condense (condense) into physical media, and/or would otherwise be difficult and/or impractical to provide in a real-time interactive experience.
For example, in some implementations, a client computing device (such as, for example, a smartphone, a tablet computing device, etc.) may capture an image of an entity, for example, within a field of view or viewfinder of an image sensor or camera of the client computing device. The client computing device may transmit a query, such as a visual content query, to the server computing device based on the image. In response to receiving the query, the server computing device may match the image in the query to the index database.
In systems and methods according to embodiments described herein, matching of images to content in an index database is done by a server computing device rather than a client computing device. In this case, server-side image recognition may be performed by the server computing device to identify or recognize images included in the query (received from the client computing device), and then match the identified/identified images with index content maintained in the index database. For example, in some implementations, data from an image that may be used by a server computing device in identifying and/or identifying a target entity may include text extracted from the image using, for example, optical character recognition, a value read from a barcode, QR code, or the like in the image, an identifier or description of the entity, product, or entity type identified in the image, and other such information. In such a case, this type of server-side image recognition may provide advantages over client-side image recognition, which may instead be performed locally by the client computing device via, for example, an application running locally on the client computing device. For example, the number of images that may actually be embedded within an application running on a client computing device may naturally be limited by the amount of available storage space, the computing power available to index and process the images, and so forth. Similarly, in some cases, client-side image recognition may rely on the use of a large number of different applications operating separately on the client computing device, depending on the entity captured in the image (e.g., a particular type of subject, a particular venue, etc.). In contrast, server-side image recognition may provide access to a large number of index images, e.g., a theoretically unlimited number of index images accessible by a server computing device through a single application, regardless of the entities captured in the images.
Based on the matching or identification of the image to the index database, the server computing device may transmit content, such as augmented reality content, to the client computing device. For example, in some implementations, augmented reality content may be provided by a network-accessible resource, such as, for example, a web page indexed by or associated with a target entity that is identified/identified. In some implementations, the augmented reality content can include moving image content or video content to be displayed on a display of the client computing device. Augmented reality content transmitted from the server computing device to the client computing device may be based on identification and/or identification of entities captured in the image and matching the index database such that the content is closely and/or specifically associated with the entities captured in the image.
For example, where client-side image recognition or client-side image detection is involved, images will be pre-downloaded to the client computing device 102, thereby creating a natural maximum in the number of images actually available for use in the identification/recognition process (e.g., driven by the available storage capacity of the client computing device 102 and other such factors). Performing server-side image recognition (i.e., by the server computing device 170) rather than client-side image recognition (i.e., by the client computing device 102) provides more accurate image recognition and the ability to distinguish between images that appear similar, because the size of the image index is much larger than what is actually possible on the client computing device 102 (i.e., measured in billions). In some implementations, storing and maintaining this voluminous image index on the server computing device 170 (rather than locally, on the client computing device 102) can allow the image index to be updated relatively quickly and dynamically without frequently affecting the network, storage, etc. of the client computing device 102 due to synchronization. Completing this task on the server side may allow image recognition to be done accurately, with relatively few cross-triggers and relatively low latency. Further, upon completion of the task on the server side, the server computing device 170 may identify the target entity within the image, identify the corresponding coordinates, and transmit this information to the client computing device 102 such that only the target entity identified by the coordinates is tracked (e.g., rather than the entire image), thereby reducing resource consumption.
In some implementations, in processing the query (transmitted from the client computing device to the server computing device), the server computing device may detect a boundary or peripheral outline or periphery of the target entity captured in the image as part of the recognition/identification process. For example, in some cases, the target entity may have a generally square or generally rectangular periphery or peripheral outline. In this example implementation, the transmission of augmented reality content (e.g., video content) from the server computing device to the client computing device may include coordinates defining a quadrilateral associated with a target entity within the image. Thus, as the video content is played on, for example, a live view of the client computing device along with the target entity captured within the view finder, the video content may appear to be attached to the target entity as the video content is played within the live view of the client computing device. That is, as the user shifts the location of the client computing device, the location of the target entity within the live-view may shift accordingly, and the display location of the augmented reality video content may shift along with the target entity. In this way, the target entity may become animated as the video content is played, thereby enhancing the user's experience with the augmented reality content.
In some implementations, the video content may remain attached to the target entity within the viewfinder of the client computing device even as the user moves and/or changes the position and/or orientation of the client computing device to further enhance the realistic appearance of the augmented reality content. In some implementations, in response to detecting movement of the client computing device such that the target entity is no longer captured within a field of view of the client computing device and/or visible on a viewfinder of the client computing device, playback of the video content may be terminated. In some implementations, the video content may continue to play or loop until the target entity is no longer captured within the field of view of the client computing device and/or visible on the viewfinder of the client computing device, and/or until terminated by the user. In some implementations, augmented reality content provided from a server computing device to a client computing device can provide a user with access to additional information related to a target entity.
Fig. 1 is a block diagram of a system 100 for playing back augmented reality content according to embodiments described herein. In some implementations, the system 100 can include a client computing device 102, a server computing device 170 (e.g., a search server), and a provider resource 180 (e.g., one or more digital supplemental servers). Also shown is a network 190 over which the client computing device 102, the server computing device 170, and the provider resource 180 may communicate.
In some implementations, the system 100 can include a client computing device 102, the client computing device 102 including a processor component 104, a communication module 106, a display device 108, a sensor system 110, and a memory 120. In some implementations, memory 120 may include one or more non-transitory computer-readable storage media. The memory 120 may store instructions and data that may be used by the client computing device 102 to implement the techniques described herein. In some implementations, the processor component 104 may include one or more devices capable of executing instructions, such as instructions stored by the memory 120. For example, in some implementations, the processor component 104 may include a Central Processing Unit (CPU) and/or a Graphics Processor Unit (GPU). In some implementations, the sensor system 110 may include various sensors, such as, for example, a camera assembly 112, an Inertial Measurement Unit (IMU)114, a Global Positioning System (GPS) receiver 116, and other sensors, including, for example, light sensors, audio sensors, image sensors, distance and/or proximity sensors, contact sensors such as capacitive sensors, timers, and/or other sensors and/or different combinations of sensors. In some implementations, the client computing device 102 is a mobile device (e.g., a smartphone, a tablet computing device, etc.).
The camera component 112 may capture images of a physical space proximate to the client computing device 102, including target entities visible within a field of view or viewfinder of the camera component 112, for example. In some implementations, the images captured by the camera component 112 can be used to determine a location and/or orientation of the client computing device 102 within the physical space and/or relative to a target entity or the like.
In some implementations, the IMU 114 may detect motion, movement, and/or acceleration of the client computing device. The IMU 114 may include a variety of different types of sensors, such as, for example, accelerometers, gyroscopes, magnetometers, and other such sensors. The orientation of the client computing device 102 may be detected and tracked based on data provided by the IMU 114 and/or the GPS receiver 116.
In some implementations, the memory 120 can include one or more applications 140 available for execution on the client computing device 102. In some implementations, the device location system 142 can determine the location of the client computing device 102 based on, for example, data provided by the sensor system 110.
In some implementations, the client computing device 102 can communicate with the server computing device 170 over the network 190 (e.g., via the communication module 106 of the client computing device 102 and the communication module 176 of the server computing device 170). For example, the client computing device 102 may send images captured by the camera component 112 to the server computing device 170. The server computing device 170 may identify a target entity within the image using, for example, the recognition engine 172, and may access the index database 174 to identify content associated with the target entity, such as augmented reality content. In some implementations, the index of the identified target entity and associated augmented reality content in the index database 174 may point to content provided by a network-accessible resource 180, such as, for example, a provider website 180 or a provider webpage 180. A number of provider resources 180 (i.e., provider resources 1801 to 180n) may communicate with the server computing device 170 and/or the client computing device 102 via the network 190. Content associated with the target entity, such as augmented reality content or the like (including, for example, augmented reality video content), may be accessed via the provider web page 180 for display on the display device 108 of the client computing device 102.
In some implementations, the client computing device 102 may be implemented in the form of a mobile computing device, such as a smartphone, tablet computing device, handheld controller, laptop computing device, or the like. In some implementations, the client computing device 102 may be implemented in the form of a wearable device, such as, for example, a head mounted display device (HMD). In some implementations, the HMD may be a separate device from the client computing device 102. In some implementations, the client computing device 102 can communicate with the HMD. For example, in some implementations, the client computing device 102 may send video signals and/or audio signals to the HMD for output to the user, and the HMD may transmit motion, position, and/or orientation information to the client computing device 102.
Fig. 2A and 2B are third person views of a user encountering an exemplary target entity 200 or an exemplary target image 200 in an exemplary physical environment 1000. In the example shown in fig. 2A and 2B, the exemplary target entity 200 is a substantially planar target entity 200 or target image 200. In the example shown in fig. 2A and 2B, the exemplary client computing device 102 is a handheld device 102 for ease of discussion and illustration only, and the principles to be described herein may be applied to other types of client computing devices set forth above.
When the user approaches the example target entity 200, the target entity 200 is within the field of view 204 of the camera component 112 of the client computing device 102. An exemplary target entity 200 (in the form of a magazine in this example) is captured within a viewfinder of the camera assembly 112 and is visible to the user via an exemplary user interface screen 206, as shown in FIG. 2B. For example, the user interface screen 206 may be displayed on the display device 108 of the client computing device 102. In some implementations, the user interface screen 206 can include an image display panel 208. In some implementations, the user interface screen 206 can also include a user input panel 210. In some implementations, the user input panel 210 can provide user interaction with content displayed in the image display panel 208. In the example illustrated in fig. 2B, the image display panel 208 displays images corresponding to the live feed from the camera component 112 of the client computing device 102.
Fig. 3A-3H illustrate exemplary user interface screen sequences that may be displayed by the client computing device 102 when detecting a target entity or target image, conducting a visual content search based on the target entity, and displaying augmented reality content (e.g., augmented reality video content) associated with the detected target entity, according to embodiments described herein. In some implementations, the exemplary sequence of user interface screens and exemplary display of augmented reality content (e.g., augmented reality video content) illustrated in fig. 3A-3H can be displayed within an application running on the client computing device 102.
As described with respect to fig. 2A and 2B, when the target entity 200 is within the field of view 204 of the camera component 112 of the client computing device 102, the target entity 200 may be visible to the user via the display panel 208 of the user interface screen 206, as shown in fig. 3A and 3B. The client computing device 102 may transmit image frames to the server computing device 170, such as, for example, the images displayed on the display panel 208 shown in fig. 3B. In response to receiving the image frames from the client computing device 102 (e.g., from an application running on the client computing device 102), the recognition engine 172 of the server computing device 170 may identify or identify the target entity 200. The server computing device 170 may then match the identified target entities 200 with the associated content in the index database 174. In some implementations, matching of the associated content to the target entity 200 can direct the server computing device 170 to the provider resource 180 to access the associated content. The associated content (e.g., augmented reality content, in particular augmented reality video content) may then be transmitted to the client computing device 102 for display on the display panel 208 of the user interface.
As described above, in identifying the target entity 200, in some implementations, the recognition engine 172 of the server computing device 170 may analyze data from the image including, for example, text extracted from the image, values read from barcodes, QR codes, etc., identifiers or descriptions, products and other entities and information, contrast information stored in an index database.
In some implementations, in analyzing the image and identifying the target entity 200, the server computing device 170 may identify or identify a boundary or periphery or outline or shape of the target entity 200. For example, in the exemplary embodiment illustrated in fig. 2A through 3H, the target entity 200 has a substantially rectangular periphery for purposes of discussion and illustration only. In this example, the transmission or streaming of the augmented reality video content to the client computing device 102 may include a definition of one or more attachment points for displaying the augmented reality content.
In the example shown in fig. 3B, target entity 200 is substantially planar and is defined by a substantially rectangular outer periphery or outline. Thus, in this example, the server computing device 170 may identify, detect, or define the boundaries of the target entity 200 through the four exemplary points 220 or attachment points 220(220A, 220B, 220C, and 220D) shown in fig. 3B. In some implementations, the augmented reality content can be transmitted or streamed to the client computing device 102 along with the attachment point 220. This may allow the augmented reality content to be displayed or played on the display panel 208 of the user interface screen 206, where the augmented reality content (e.g., augmented reality video content) remains connected to the target entity 200 at the attachment point 220. In general, for example, the client computing device 102 is adapted to trigger display of augmented reality content within an outline defined by coordinates included in a response received from the server computing device 170 via the user interface screen 206. The coordinates may be, for example, attachment points 220. In some implementations, this may allow the augmented reality content to remain connected to the target entity 200 even as the user moves the client computing device 102, as long as the target entity 200 remains in the field of view or within the viewfinder of the camera component 112 of the client computing device 102. In this way, augmented reality content may remain within the frame of target entity 200 and move with the target entity, appearing to animate target entity 200, or generating an impression that target entity 200 is already animated. In some implementations, the augmented reality experience may be terminated if the client computing device 102 is moved such that the target entity 200 is no longer captured within the field of view or viewfinder of the camera component of the client computing device 102.
For example, returning to fig. 3A-3H, augmented reality content 300 (e.g., augmented reality video content 300) may be transmitted to the client computing device 102 along with the attachment point 220 described above. The augmented reality content 300 may be played within the bounds or frame of the target entity 220 defined by the attachment point 220, as shown in fig. 3C-3E, while the target entity 200 remains within the field of view or viewfinder of the camera component 112 of the client computing device 102. In particular, fig. 3D-3G illustrate the augmented reality video content 300 playing within the confines or frame of the target entity 200 as the relative position, orientation, distance separation, etc., between the camera component 112 of the client computing device 102 and the target entity 200 changes. As shown in fig. 3H, when the target entity 200 is no longer captured within the field of view or viewfinder of the camera component 112 of the client computing device 102, the augmented reality experience is terminated and the real-world elements captured within the field of view of the camera component 112 of the client computing device 102 are visible on the display panel 208, as shown in fig. 3H. That is, when the target entity 200 is no longer captured within the field of view or viewfinder of the camera component 112 of the client computing device 102, the augmented reality content 300 is no longer playing, as shown in fig. 3H.
For purposes of discussion and illustration, in the exemplary embodiment shown in fig. 3A-3H, user interface screen 206 includes a user input panel 210 displayed along with a display panel 208. In some embodiments, the user interface screen 206 may include both a display panel 208 and a user input panel 210 arranged differently than the exemplary arrangement shown in fig. 3A-3H. In some implementations, the user interface screen 206 may include only the display panel 208. In some implementations, other types of annotations or supplemental information can be displayed on the display panel 208.
Figures 3A-3H and their accompanying description present an exemplary embodiment in which augmented reality video content 300 is applied to a cover of a journal (in this example, target entity 200) such that the cover of the journal appears animated, or otherwise made life-like. Animating the cover of the journal in this manner may, for example, draw the user's attention to information, features, etc. included in the journal and/or the story of the subject cover, thereby enabling the user to experience the journal, arouse the user's interest, encourage the user to explore further, etc. Fig. 4A through 4J present different exemplary embodiments in which augmented reality content may be applied to a target entity to engage or immerse a user in, for example, a shopping experience in which the user may interface with the augmented reality content (playing on the target entity in real world space) to take deeper actions in a screen space or virtual space related to the target entity to access additional information related to the target entity, and the like.
Fig. 4A-4J illustrate a sequence of exemplary user interface screens 206 that may be displayed by the client computing device 102 when detecting a target entity 200 or target image 200, conducting a visual content search based on the target entity, and displaying augmented reality content (e.g., augmented reality video content) associated with the detected target entity, according to embodiments described herein. In some implementations, the exemplary sequence of user interface screens 206 and the exemplary display of augmented reality content shown in fig. 4A-4J can be displayed within an application running on the client computing device 102.
Within an application running on the client computing device 102, an exemplary user interface screen 206 shown in fig. 4A may be displayed by the client computing device 102, representing a real-world object captured within a field of view or viewfinder of the camera component 112 of the client computing device 102. In this camera view, the target entity 200 is within the field of view 204 of the camera component 112 of the client computing device 102, and thus the target entity 200 is visible to the user via the display panel 208 of the user interface screen 206. The client computing device 102 may transmit image frames corresponding to the images displayed on the display panel 208 shown in fig. 4A to the server computing device 170. In response to receiving the image frames from the client computing device 102, the recognition engine 172 of the server computing device 170 may identify or identify the target entity 200 and may match the identified target entity 200 with the associated content in the index database 174. In some implementations, matching of the associated content to the target entity 200 can direct the server computing device 170 to the provider resource 180 to access the associated content. The associated content, including the augmented reality content, may be transmitted to the client computing device 102 for display on the display panel 208 of the user interface screen 206.
As described above, in analyzing the image and identifying the target entity 200, the server computing device 170 may identify or identify a boundary or periphery or outline or shape or frame of the target entity 200, and may define one or more attachment points 220, which attachment points 220 may in turn define a frame in which augmented reality content may be displayed.
As shown in fig. 4B, in some implementations, when the image is transmitted from the client computing device 102 to the server computing device 170, the indicator 225 may be displayed on the display panel 208, the server computing device 170 performs the above-described identification and matching, and the augmented reality content is transmitted to the client computing device 102. The indicator 225 may provide a visual indication to the user that the target entity 200 may be an enabled target entity 200 with associated augmented reality content available and that the identification, matching, and transmission processes are being performed. In the example shown in FIG. 4B, indicator 225 is a visual indicator in the form of a bar, such as a progress bar, that progresses along the periphery of target entity 200.
In the example shown in fig. 4A and 4B, the target entity 200 is substantially planar and is defined by a substantially rectangular outer periphery or outline or frame. Thus, in this example, the server computing device 170 may identify or detect or define the boundaries of the target entity 200 through four exemplary attachment points 220, and the augmented reality content may be transmitted or streamed to the client computing device 102 along with the attachment points 220. This may allow the augmented reality content to be displayed or played on the display panel 208 of the user interface screen 206 even as the user moves the client computing device 102, the augmented reality content still connects to the target entity 200 at the attachment point 220 as long as the target entity 200 remains within the field of view or viewfinder of the camera component 112 of the client computing device 102.
Fig. 4C-4G illustrate sequential display of augmented reality video content transmitted to the client computing device 102 and displayed on the display panel 208 of the user interface screen 206 within an application running on the client computing device 102. In this example, the augmented reality video content 300 includes a virtual representation of a product 350 in the form of a shoe associated with the identified or identified target entity 200. As shown in fig. 4C-4G, in this example, a virtual representation of the product 350 enters the display panel 208, moves to a central portion of the display panel 208, and rotates so that a user can, for example, view various features of the product 350. Fig. 4C through 4G are static illustrations intended to represent sequential movement of the product 350 corresponding to animation.
In some implementations, the user can choose to access additional information during or after viewing the presentation of the augmented reality content 300, or move further into the experience. In some implementations, the user can further move into the screen-based experience, for example, by tapping the target entity or tapping a portion of the augmented reality content 300 or interacting with a portion of the user input panel 210. For example, in the exemplary embodiment shown in fig. 4I, tapping the shutter button 211 provided in the user input panel 210 or tapping the rendered image of the product 350 displayed on the display panel 208 may redirect the user to an information page 360 that includes a link to product-related information, as shown in fig. 4J.
As mentioned above, when the target entity 200 is no longer captured within the field of view or viewfinder of the camera component 112 of the client computing device 102, the augmented reality experience may be terminated and real-world elements captured within the field of view of the camera component 112 of the client computing device 102 may be visible on the display panel 208. As also mentioned above, the exemplary embodiment shown in fig. 4A through 4J, for purposes of discussion and illustration, the user interface screen 206 includes a user input panel 210 displayed with a display panel 208. In some embodiments, the user interface screen 206 may include both a display panel 208 and a user input panel 210, arranged differently than in the exemplary embodiment. In some implementations, the user interface screen 206 can include only the display panel 208. In some implementations, other types of annotations or supplemental information can be displayed on the display panel 208.
As mentioned with respect to the above-described exemplary embodiments, the augmented reality content displayed on the display panel 208 may be augmented reality video content 300 that is fit within the outline or frame of the target entity 200 based on the identified attachment points 220, as described above. In some implementations, the augmented reality video content may be played substantially continuously and repeated or looped until the augmented reality experience is terminated. In some implementations, the augmented reality content 300 can be automatically played without a separate user input to initiate playback of the augmented reality content 300. In some implementations, the augmented reality content 300 can automatically cycle without a separate user input selecting playback of the augmented reality content 300. In some implementations, the augmented reality video content 300 can include audio content that can be played with the augmented reality video content 300. In some implementations, playback of the associated audio content can be muted and enabled in response to a user selection to enable playback of the audio content.
In some implementations, the augmented reality content displayed on the display panel 208 can be a Graphics Interchange Format (GIF) rendered and displayed within the outline or frame of the target entity 200 based on the identified attachment points 220. In some implementations, the augmented reality GIF content can be automatically played without a separate user input to initiate playback of the augmented reality GIF content. In some implementations, the augmented reality GIF content can automatically cycle without a separate user input selecting a replay of the augmented reality GIF content.
In some implementations, the augmented reality content displayed on the display panel 208 can be in a hypertext markup language (HTML) format rendered and displayed within the outline or frame of the target entity 200 based on the identified attachment point 220. In some implementations, the augmented reality HTML content can be automatically played without a separate user input to initiate playback of the augmented reality HTML content. In some implementations, the augmented reality HTML content can be automatically looped without a separate user input selecting playback of the augmented reality HTML content.
The above-described exemplary systems and methods may provide users with an engaging augmented reality experience that is tightly coupled to their immediate environment and easily accessible through, for example, a single application running on the client computing device 102, with relatively minimal user input. These engaging augmented reality experiences may be relatively short, but in turn may attract users to interact with products, services, and other types of information for longer and more widely.
Fig. 5 is a flow diagram of an exemplary method 500 performed by a client computing device according to an embodiment described herein. In some implementations, the exemplary method 500 may be performed within an application running on a client computing device. For ease of description, the exemplary method 500 will be described with respect to the client computing device 102 operating within the exemplary system 100, as discussed above with respect to fig. 1-4J.
For example, an image may be captured within a field of view or viewfinder of the camera component 112 of the client computing device 102 (block 502). As described above, the image may include a target entity of interest to the user. A query (e.g., a visual content query including an image) may be transmitted from the client computing device 102 to the server computing device 170 (block 504) for processing by the server computing device 170 (described below with respect to fig. 6). The client computing device 102 may receive a response to the visual content query from the server computing device 170. The response may identify augmented reality content related to the target entity captured within the image included in the visual content query (block 506). In response to receiving the response including the augmented reality content, the client computing device 102 may display a user interface screen displaying the augmented reality content (block 508). In some implementations, in response to receiving the response to the query, the client computing device 102 can display a user interface screen that also includes user-actuatable controls (block 508). In response to detecting user actuation of one of the user-actuatable controls (block 510), the client computing device 102 may access additional information related to the augmented reality content and may output the additional information for consumption by the user (block 510). Exemplary method 500 may continue to be performed until it is determined that the experience has been terminated (block 514). Termination of the experience may be detected in response to, for example, detecting that the target entity is no longer captured within the field of view or viewfinder of the camera component 112 of the client computing device 102, a detected user input, or the like.
Fig. 6 is a flow chart of an exemplary method 600 performed by a server computing device according to embodiments described herein. For ease of description, the exemplary method 600 will be described with respect to the server computing device 170 operating within the exemplary system 100, as discussed above with respect to fig. 1-4J.
The server computing device 170 may receive a query, such as a visual content query, from the client computing device 102 (block 602). The query may include an image captured within a field of view or viewfinder of a camera component of the client computing device 102. The image received in the query may include a target entity of interest to the user of the client computing device 102. The image included in the query may be processed by the recognition engine 172 of the server computing device 170 to identify or identify the target entity captured in the image included in the query (block 604). The index database of the server computing device 170 may be accessed to match the identified or identified target entity with the provider resource 180 associated with the identified target entity (block 606). The provider resource may provide access to, for example, augmented reality content related to the target entity, additional information related to the target entity, and so forth. The augmented reality content and the additional information may be transmitted to the client computing device 102 for output by the client computing device 102 and for consumption by the user (block 610).
Systems and methods according to embodiments described herein may facilitate access to content (e.g., augmented reality content) by simplifying the process for establishing, updating, and maintaining a link between the index database 174 of the server computing device 170 and the provider resource 180, which provides access to the augmented reality content and associated information related to a target entity identified by matching in the index database 174.
In one example, a resource provider 180, for example in the form of a museum, may wish to link content (e.g., augmented reality content) in the index database 174 of the server computing device 170 of the exemplary system 100 shown in fig. 1 so that a user can easily access content and information related to the museum through a single application running on the client computing device 102. In some embodiments, the content and information may include augmented reality video content related to exhibits within a museum, for example, where the exhibits appear to be life-like to arouse the user's interest in the exhibits and to further drill down the user into related information that may also be accessed through a single application, which may result in visiting the museum. In some implementations, the content and information may include, for example, the location of the museum, directions to the museum, parking information, public transportation information, business hours, admission information, travel information, and the like.
In this example, a content creator associated with the museum may instead link the relevant information to the server computing device 170 for access by the user through a single application, rather than creating, maintaining, servicing, and updating the provider (museum in this example) specific application. In some implementations, to accomplish this, a creator associated with the provider may, for example, create a simple web page that is linked to the index database 174 of the server computing device 170. In some implementations, the web pages linked to the index database 174 can include, for example, links to target images, links to augmented reality content (associated with the target images), and links to additional information (if desired). The target images of providers linked in this manner may be matched with images included in the visual content query received from the client computing device 102 to identify or identify the target entities, to locate associated augmented reality content, to locate relevant information, and so forth. In this example, simply uploading these links to the server computing device 170 in this manner may allow the provider's experience to be pushed out through a single application without the need to develop the application specifically for the provider. Similarly, images, content, information, etc. may be updated simply and quickly by simply updating the links included in the provider web page.
For example, the provider may create a web page as described above, or add a relatively simple tag to an existing website to link content to the server computing device 170 described above, so that the content, information, etc. may be accessed by the user through a single application running on the client computing device 102. The creator may designate the image as the target image and may provide a link to the designated target image in a web page linked to the server computing device 170. The creator may then provide a link to the augmented reality content associated with the specified target image. An example of this is shown in fig. 7A through 8G, where an exemplary provider 180 is a natural history museum.
In this example, the provider 180 (i.e., museum) may have an exhibit related to a large feline of the serentine plains. Provider 180 may wish to link content (e.g., augmented reality video content) to one or more target entities (e.g., images in a brochure, poster, etc.) associated with the exhibition, so that the entities are animated as they are encountered by the user, arousing the user's interest, and attracting visitors to the exhibition. As shown in fig. 7A, the target image 710 may be specified by the provider 180. The target image 710 may exist in one or more locations that are accessible to a user having an application available on the client computing device 102. For example, as shown in fig. 7B through 7D, the target image 710 may appear in a brochure 720 (printed or digital) available at various locations outside the museum, a poster 730 displayed at a location outside the museum, a banner 740 displayed inside the museum, and the like. For purposes of discussion and illustration, the materials comprising target image 710 shown in fig. 7B through 7D are merely exemplary in nature. The target image 710 may be included in any number of different types of material that the user may encounter, and this will allow the target image to be captured within the field of view of the camera component 112 running within the application on the client computing device 102.
As mentioned above, the creator (associated with the provider 180, i.e., the museum in this example) may provide a link to the specified target image 710 in a simple web page, or to the server computing device 170. The creator may also provide a link to the augmented reality content associated with the specified target image 710 for storage in the index database and use by the index database 174 of the server computing device 170 in matching the augmented reality content with the identified target entity. Sample scripts for an exemplary web page are shown below.
In this manner, the creator may relatively quickly and relatively easily specify the target image 710 and link the target image 710 to the augmented reality content, such that the server computing device 170 may identify the target image 710 within the image received from the client computing device 102 and match the target image 710 with the linked augmented reality content in the index database 174 for playback on the client computing device without further user intervention.
For example, as shown in fig. 8A, the booklet 720 can be captured within a field of view or viewfinder of the camera component 112 running within an application on the client computing device 102. The client computing device 102 may transmit the image frames of the booklet to the server computing device 170, and the recognition engine 172 may match the target entity 200 within the image with the target image 710 linked by the provider 180 as described above. Based on a match of the target entity 200 detected within the image (of the booklet 720 captured within the viewfinder of the client computing device 102) with the target entity 710 linked by the provider 180, the associated augmented reality content 300 can be identified using the index database 174. The augmented reality content 300 may be played on the user interface screen 206 displayed by the client computing device 102, as shown in fig. 8B through 8F. As described above, in some implementations, the augmented reality content 300 can be transmitted to the client computing device 102 along with designated attachment points that attach the augmented reality content to the target entity 200 within the viewfinder of the client computing device 102, so that the target entity 200 appears animated or animated as the augmented reality content is played.
In this manner, a creator or developer may link a target image and related content (e.g., augmented reality content (on behalf of the resource provider 180)) to the server computing device 170 in a relatively simple and relatively quick manner. The target image and content of the link may be updated relatively quickly and easily by simply replacing the link within the specified web page. This may allow many different resource providers 180 to make content globally accessible via a single application running on the client computing device 102.
In the corresponding exemplary method shown in fig. 9, the target image may be identified (block 902), as described above with respect to fig. 7A-7D. The target image may be linked with a recognition engine of the server computing device (block 904) for recognizing or identifying a target entity within an image frame provided by the client computing device 102. The target image may also be linked to content, such as augmented reality content, through an indexed database of the server computing device (block 906). Upon identifying or identifying the target entity, augmented reality content may be retrieved via the index database and transmitted to the client computing device.
In the above-described systems and methods according to embodiments described herein, augmented reality content may replace (e.g., virtually replace) a corresponding target entity. In the specific example embodiments described above, systems and methods according to embodiments described herein may allow two-dimensional (2D) augmented reality content to virtually replace a 2D planar object image, for example, to create the appearance of an animation of the 2D planar object image such that the planar object image appears lifelike. The augmented reality content may be applied to any enabled target entity in such a way that a match in the index database of the server computing device and any target entity available for access to the associated augmented reality content (e.g., from a provider resource). The systems and methods according to embodiments described herein may be implemented within a single application running on a client computing device. The single application may facilitate providing augmented reality content in this manner for any number of different entities rather than a plurality of separate applications that are otherwise required by each of the different entities.
In some implementations, augmented reality content in the form of video content can be considered an end product for consumption by a user. For example, augmented reality content in the form of a movie trailer provided in response to a visual content query that includes a movie poster (where the movie poster/movie is the target entity) may be the end product desired by the user. In some implementations, the user may seek additional information associated with the movie (i.e., the target entity), such as, for example, reviews, show times, locations, pricing, ticketing options, and the like. Whether augmented reality video content is the end product desired by the user, or merely an immersive tool that attracts the user to primary content, systems and methods according to embodiments described herein may provide simplicity
The various exemplary embodiments described above are provided for ease of discussion and illustration only. Systems and methods according to embodiments described herein may be used in any number of different types of applications, such as, for example, life periodicals (i.e., books, magazines, newspapers, etc.), life product packaging, life advertising, life shopping, life entertainment (i.e., movies, artwork, travel, games, navigation, etc.), life educational materials, life communications, and many other such businesses.
Fig. 10 illustrates examples of a computer device 1300 and a mobile computer device 1350 that can be used with the techniques described herein (e.g., to implement the client computing device 102, the server computing device 170, and the provider resource 180). Computing device 1300 includes a processor 1302, memory 1304, a storage device 1306, a high-speed interface 1308 connecting to memory 1304 and high-speed expansion ports 1310, and a low speed interface 1312 connecting to low speed bus 1314 and storage device 1306. Each of the components 1302, 1304, 1306, 1308, 1310, and 1312, are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 1302 may process instructions for execution within the computing device 1300, including instructions stored in the memory 1304 or on the storage device 1306 to display graphical information for a GUI on an external input/output device, such as a display 1316 coupled to the high-speed interface 1308. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and memory types. Moreover, multiple computing devices 1300 may be connected, with each device providing portions of the necessary operations (e.g., as a server array, a group of blade servers, or a multi-processor system).
The memory 1304 stores information within the computing device 1300. In one implementation, the memory 1304 is a volatile memory unit or units. In another implementation, the memory 1304 is one or more non-volatile memory units. The memory 1304 may also be another form of computer-readable medium, such as a magnetic or optical disk.
The storage device 1306 is capable of providing mass storage for the computing device 1300. In one implementation, the storage device 1306 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices (including devices in a storage area network or other configurations). The computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-readable medium or machine-readable medium, such as the memory 1304, the storage device 1306, or memory on processor 1302.
The high speed controller 1308 manages bandwidth-intensive operations for the computing device 1300, while the low speed controller 1312 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one embodiment, the high-speed controller 1308 is coupled to memory 1304, display 1316 (e.g., through a graphics processor or accelerator), and high-speed expansion ports 1310, which high-speed expansion ports 1310 may accept various expansion cards (not shown). In an embodiment, low-speed controller 1312 is coupled to storage device 1306 and low-speed expansion port 1314. For example, a low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, and wireless ethernet) may be coupled through a network adapter to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a network device (such as a switch or router).
As shown in the figures, computing device 1300 may be implemented in a number of different forms. For example, it may be implemented as a standard server 1320, or multiple times in a group of such servers. It may also be implemented as part of a rack server system 1324. In addition, it may be implemented in a personal computer, such as a laptop computer 1322. Alternatively, components from computing device 1300 may be combined with other components in a mobile device (not shown), such as device 1350. Each of such devices may contain one or more computing devices 1300, 1350, and an entire system may be made up of multiple computing devices 1300, 1350 communicating with each other.
Computing device 1350 includes, among other components, a processor 1352, memory 1364, input/output devices (such as display 1354), a communication interface 1366, and transceiver 1368. The device 1350 may also be provided with a storage device, such as a miniature hard disk or other device, to provide additional storage. Each of components 1350, 1352, 1364, 1354, 1366, and 1368 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
The processor 1352 may execute instructions within the computing device 1350, including instructions stored in the memory 1364. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. For example, the processor may provide coordination of the other components of the device 1350, such as control of user interfaces, applications run by device 1350, and wireless communication by device 1350.
The processor 1352 may communicate with a user through a control interface 1358 and a display interface 1356 coupled to a display 1354. For example, the display 1354 may be a TFT LCD (thin film transistor liquid crystal display) and LED (light emitting diode) or OLED (organic light emitting diode) display or other suitable display technology. The display interface 1356 may include appropriate circuitry for driving the display 1354 to present graphical and other information to a user. The control interface 1358 may receive commands from a user and convert the commands for submission to the processor 1352. Additionally, an external interface 1362 may be provided in communication with the processor 1352, for enabling the device 1350 to communicate with other devices in the vicinity. External interface 1362 may provide, for example, wired communication in some embodiments, or wireless communication in other embodiments, and multiple interfaces may also be used.
The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one embodiment, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 1364, expansion memory 1374, or memory on processor 1352, which may be housed, for example, over transceiver 1368 or external interface 1362.
Device 1350 may communicate wirelessly via communication interface 1366. communication interface 1366 may include digital signal processing circuitry, if desired. Communication interface 1366 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 1368. Additionally, short-range communication may occur, such as using Bluetooth, Wi-Fi, or other such transceivers (not shown). Additionally, GPS (global positioning system) receiver module 1370 may provide additional navigation-or location-related wireless data to device 1350, which may be used by applications running on device 1350, as appropriate.
The device 1350 may also communicate audibly using the audio codec 1360, which may receive spoken information from a user and convert it to usable digital information. The audio codec 1360 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of the device 1350. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 1350.
As shown in the figures, computing device 1350 may be implemented in a number of different forms. For example, it may be implemented as a cellular telephone 1380. It may also be implemented as part of a smartphone 1382, personal digital assistant, or other similar mobile device.
Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications, or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium," "computer-readable medium" refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having: a display device (LED (light emitting diode) or OLED (organic LED) or LCD (liquid crystal display) monitor/screen) for displaying information to a user; and a keyboard and a pointing device (e.g., a mouse or a trackball) by which a user may provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
The systems and techniques described here can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
In some implementations, the computing device depicted in fig. 13 may include sensors that interface with the AR headset/HMD device 1390 to generate an enhanced environment for viewing content inserted within the physical space. For example, one or more sensors included on computing device 1350 or other computing devices depicted in fig. 13 may provide input to AR headset 1390 or, in general, to the AR space. These sensors may include, but are not limited to, touch screens, accelerometers, gyroscopes, pressure sensors, biometric sensors, temperature sensors, humidity sensors, and ambient light sensors. Computing device 1350 may use these sensors to determine an absolute position and/or detected rotation of the computing device in AR space, which may then be used as an input to the AR space. For example, computing device 1350 may be incorporated into AR space as a virtual object (such as a controller, laser pointer, keyboard, weapon, etc.). When incorporated into the AR space, the user's positioning of the computing device/virtual object may allow the user to position the computing device to view the virtual object in some manner in the AR space. For example, if the virtual object represents a laser pointer, the user may manipulate the computing device as if it were an actual laser pointer. The user may move the computing device left and right, up and down, in a circle, etc., and use the device in a manner similar to using a laser pointer. In some embodiments, a user may aim at a target location using a virtual laser pointer.
In some implementations, one or more input devices included on computing device 1350 or connected to computing device 1350 can be used as inputs to the AR space. These input devices may include, but are not limited to, a touch screen, a keyboard, one or more buttons, a track pad, a touch pad, a pointing device, a mouse, a track ball, a joystick, a camera, a microphone, a headset with input functionality or a bluetooth headset, a game controller, or other connectable input devices. When the computing device is incorporated into the AR space, user interaction with an input device included on computing device 1350 may cause a particular action to occur in the AR space.
In some implementations, the touchscreen of computing device 1350 can be rendered as a touchpad in AR space. The user may interact with the touch screen of computing device 1350. For example, these interactions may be rendered in AR headphones 1390 as movements on a touchpad in the rendered AR space. The rendered movement may control a virtual object in the AR space.
In some implementations, one or more output devices included on computing device 1350 can provide output and/or feedback to a user of AR headset 1390 in AR space. These outputs and feedback may be visual, tactical, or audio. These outputs and/or feedback may include, but are not limited to, vibrating, turning on and off or flashing and/or blinking one or more lights or flashes, sounding an alarm, playing a chime, playing a song, and playing an audio file. Output devices may include, but are not limited to, vibration motors, vibration coils, piezoelectric devices, electrostatic devices, Light Emitting Diodes (LEDs), flashlights, and speakers.
In some implementations, the computing device 1350 can appear as another object in a computer-generated 3D environment. User interaction with computing device 1350 (e.g., rotating, shaking, touching to a touch screen, sliding a finger across a touch screen) may be interpreted as interaction with an object in AR space. In the example of a laser pointer in AR space, computing device 1350 appears as a virtual laser pointer in a computer-generated 3D environment. As the user manipulates computing device 1350, the user sees the laser pointer moving in AR space. The user receives feedback on computing device 1350 or AR headset 1390 from interactions with computing device 1350 in the AR environment. User interaction with the computing device may be translated into interaction with a user interface generated in the AR environment of the controllable device.
In some implementations, the computing device 1350 can include a touch screen. For example, a user may interact with a touch screen to interact with a user interface of a controllable device. For example, a touch screen may include user interface elements, such as sliders that may control a characteristic of a controllable device.
Several embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the description.
In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be excluded, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.
While certain features of the described embodiments have been illustrated as described herein, many modifications, substitutions, changes and equivalents will now occur to those skilled in the art. It is, therefore, to be understood that the appended claims are intended to cover all such modifications and changes as fall within the scope of the embodiments. It is to be understood that they have been presented by way of example only, and not limitation, and various changes in form and details may be made. Any portion of the devices and/or methods described herein can be combined in any combination, except mutually exclusive combinations. The embodiments described herein may include various combinations and/or subcombinations of the functions, features and/or properties of the different embodiments described.
Claims (20)
1. A computer-implemented method, comprising:
capturing an image within a live view finder of a client computing device;
transmitting, by the client computing device, a query to a server computing device, the query including the image;
receiving, by the client computing device, a response to the query from the server computing device, the response including augmented reality content; and
triggering display of a user interface screen on the client computing device, the user interface screen including the augmented reality content displayed within an outline defined by coordinates included in the response received from the server computing device.
2. The computer-implemented method of claim 1, wherein the image comprises a target entity captured within a field of view of the live view of the client computing device, and wherein receiving the response to the query comprises: receiving augmented reality video content associated with the target entity included in the image.
3. The computer-implemented method of claim 2, wherein receiving the response to the query from the server computing device comprises:
receiving the coordinates defining the outline for display of the augmented reality video content on the user interface screen, the outline corresponding to a periphery of the target entity.
4. The computer-implemented method of claim 3, wherein triggering display of the user interface screen comprises:
displaying the user interface screen within the live view finder of the client computing device; and
attaching the display of the augmented reality video content to the target entity at an attachment point corresponding to the coordinates such that the display of the augmented reality video content remains attached to the target entity within the live view of the client computing device.
5. The computer-implemented method of claim 4, further comprising:
detecting a first movement of the client computing device;
shifting a display of the target entity within the live view of the client computing device in response to the detected first movement; and
shifting display of the augmented reality video content to correspond to the shifted display position of the target entity within the live view finder of the client computing device.
6. The computer-implemented method of claim 5, wherein,
detecting the first movement comprises detecting at least one of: a change in position, orientation, or distance between the live view of the client computing device and the target entity; and
shifting the display of the augmented reality video content comprises: changing at least one of a display position, a display orientation, or a display size of the augmented reality video content to correspond to the changed display position, orientation, or distance of the target entity, respectively.
7. The computer-implemented method of claim 5 or 6, further comprising:
detecting a second movement of the client computing device such that the target entity is not captured within the live view finder of the client computing device; and
in response to the detected second movement, terminating the display of the augmented reality video content.
8. The computer-implemented method of claim 7, wherein triggering display of the user interface screen comprises: cycling the augmented reality content until the second movement is detected or until a termination input is received.
9. The computer-implemented method of any of claims 2 to 8,
triggering display of the user interface screen includes triggering display of a display panel and a user input panel; and is
Receiving the response to the query comprises:
receiving the augmented reality video content for display on the display panel of the user interface screen; and
receiving auxiliary information related to the target entity for display on the user input panel of the user interface screen.
10. The computer-implemented method of any of claims 1-9, wherein the capturing the image within the live view of the client computing device, the transmitting, the receiving, and the triggering are accomplished within an application running on the client computing device.
11. A computer-implemented method, comprising:
receiving, by a server computing device, a query from a client computing device, the query comprising an image;
detecting, by a recognition engine of the server computing device, a target entity within the image included in the query;
matching, in an index database of the server computing device, the target entity with augmented reality content from an external provider; and
transmitting the augmented reality content to the client computing device for output by the client computing device.
12. The computer-implemented method of claim 11, wherein receiving the query that includes the image comprises: receiving the query, the query comprising capturing the image of the target entity within a live view finder of the client computing device.
13. The computer-implemented method of claim 11 or 12, wherein detecting the target entity within the image included in the query comprises: identifying the target entity based on a target image linked by the external provider with the recognition engine.
14. The computer-implemented method of claim 13, wherein matching the target entity with the augmented reality content comprises: matching the target entity with content linked in the index database to the target image by the external provider.
15. The computer-implemented method of any of claims 11 to 14, wherein detecting the target entity comprises:
detecting a peripheral outline of the target entity within the image; and
defining attachment points along the detected peripheral outline to define the peripheral outline of the target entity.
16. The computer-implemented method of claim 15, wherein transmitting the augmented reality content for output by the client computing device comprises:
transmitting the attachment points with the augmented reality content to the client computing device for output of the augmented reality content by the client computing device within the outline defined by the attachment points.
17. A computer-readable storage medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform the computer-implemented method of any of the preceding claims.
18. A client computing device comprising a live view finder, one or more processors, and memory storing instructions that, when executed by the one or more processors, cause the one or more processors to perform the computer-implemented method of any of claims 1-10.
19. A server computing device comprising an identification engine, an index database, one or more processors, and memory storing instructions that, when executed by the one or more processors, cause the one or more processors to perform the computer-implemented method of any of claims 11 to 16.
20. A system comprising the client computing device of claim 18 and the server computing device of claim 19.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/020319 WO2021173147A1 (en) | 2020-02-28 | 2020-02-28 | System and method for playback of augmented reality content triggered by image recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114450967A true CN114450967A (en) | 2022-05-06 |
Family
ID=70058471
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080068204.2A Pending CN114450967A (en) | 2020-02-28 | 2020-02-28 | System and method for playback of augmented reality content triggered by image recognition |
Country Status (4)
Country | Link |
---|---|
US (1) | US20220335661A1 (en) |
EP (1) | EP4111696A1 (en) |
CN (1) | CN114450967A (en) |
WO (1) | WO2021173147A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220108536A1 (en) * | 2020-10-07 | 2022-04-07 | Samsung Electronics Co., Ltd. | Method of displaying augmented reality and electronic device for performing same |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11861800B2 (en) * | 2020-12-30 | 2024-01-02 | Snap Inc. | Presenting available augmented reality content items in association with multi-video clip capture |
US11924540B2 (en) | 2020-12-30 | 2024-03-05 | Snap Inc. | Trimming video in association with multi-video clip capture |
CN115914716A (en) * | 2021-09-30 | 2023-04-04 | 中兴通讯股份有限公司 | Image display method and device, storage medium and electronic device |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN103426003A (en) * | 2012-05-22 | 2013-12-04 | 腾讯科技（深圳）有限公司 | Implementation method and system for enhancing real interaction |
US20140225924A1 (en) * | 2012-05-10 | 2014-08-14 | Hewlett-Packard Development Company, L.P. | Intelligent method of determining trigger items in augmented reality environments |
US20140267792A1 (en) * | 2013-03-15 | 2014-09-18 | daqri, inc. | Contextual local image recognition dataset |
US20150023650A1 (en) * | 2013-07-19 | 2015-01-22 | Google Inc. | Small-Screen Movie-Watching Using a Viewport |
US20150070347A1 (en) * | 2011-08-18 | 2015-03-12 | Layar B.V. | Computer-vision based augmented reality system |
CN105046213A (en) * | 2015-06-30 | 2015-11-11 | 成都微力互动科技有限公司 | Method for augmenting reality |
US20150348329A1 (en) * | 2013-01-04 | 2015-12-03 | Vuezr, Inc. | System and method for providing augmented reality on mobile devices |
US9552674B1 (en) * | 2014-03-26 | 2017-01-24 | A9.Com, Inc. | Advertisement relevance |
JP2018141970A (en) * | 2018-02-23 | 2018-09-13 | 大日本印刷株式会社 | Video content display device, glasses, video content processing system, and video content display program |
US10339718B1 (en) * | 2017-12-29 | 2019-07-02 | Verizon Patent And Licensing Inc. | Methods and systems for projecting augmented reality content |
Family Cites Families (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7720436B2 (en) * | 2006-01-09 | 2010-05-18 | Nokia Corporation | Displaying network objects in mobile devices based on geolocation |
US8872767B2 (en) * | 2009-07-07 | 2014-10-28 | Microsoft Corporation | System and method for converting gestures into digital graffiti |
US20150040074A1 (en) * | 2011-08-18 | 2015-02-05 | Layar B.V. | Methods and systems for enabling creation of augmented reality content |
US9214137B2 (en) * | 2012-06-18 | 2015-12-15 | Xerox Corporation | Methods and systems for realistic rendering of digital objects in augmented reality |
US9240077B1 (en) * | 2014-03-19 | 2016-01-19 | A9.Com, Inc. | Real-time visual effects for a live camera view |
US10037465B2 (en) * | 2016-03-18 | 2018-07-31 | Disney Enterprises, Inc. | Systems and methods for generating augmented reality environments |
US10191541B2 (en) * | 2016-06-30 | 2019-01-29 | Sony Interactive Entertainment Inc. | Augmenting virtual reality content with real world content |
US20180300917A1 (en) * | 2017-04-14 | 2018-10-18 | Facebook, Inc. | Discovering augmented reality elements in a camera viewfinder display |
US20180349946A1 (en) * | 2017-05-31 | 2018-12-06 | Telefonaktiebolaget Lm Ericsson (Publ) | System, method and architecture for real-time native advertisement placement in an augmented/mixed reality (ar/mr) environment |
WO2019064704A1 (en) * | 2017-09-29 | 2019-04-04 | オリンパス株式会社 | Endoscopic image observation assistance system, endoscopic image observation assistance device, and endoscopic image observation assistance method |
JP7420741B2 (en) * | 2018-05-07 | 2024-01-23 | グーグル エルエルシー | Real-time object detection and tracking |
US10776933B2 (en) * | 2018-12-06 | 2020-09-15 | Microsoft Technology Licensing, Llc | Enhanced techniques for tracking the movement of real-world objects for improved positioning of virtual objects |
CA3134424A1 (en) * | 2019-03-18 | 2020-09-24 | Geomagical Labs, Inc. | Virtual interaction with three-dimensional indoor room imagery |
US11158128B2 (en) * | 2019-04-26 | 2021-10-26 | Google Llc | Spatial and semantic augmented reality autocompletion in an augmented reality environment |
JP2022539313A (en) * | 2019-06-24 | 2022-09-08 | マジック リープ， インコーポレイテッド | Choosing a virtual location for virtual content |
US11836977B2 (en) * | 2019-12-05 | 2023-12-05 | W.W. Grainger, Inc. | System and method for hybrid visual searches and augmented reality |
-
2020
- 2020-02-28 US US17/753,425 patent/US20220335661A1/en active Pending
- 2020-02-28 CN CN202080068204.2A patent/CN114450967A/en active Pending
- 2020-02-28 EP EP20716008.6A patent/EP4111696A1/en active Pending
- 2020-02-28 WO PCT/US2020/020319 patent/WO2021173147A1/en unknown
Patent Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150070347A1 (en) * | 2011-08-18 | 2015-03-12 | Layar B.V. | Computer-vision based augmented reality system |
US20140225924A1 (en) * | 2012-05-10 | 2014-08-14 | Hewlett-Packard Development Company, L.P. | Intelligent method of determining trigger items in augmented reality environments |
CN103426003A (en) * | 2012-05-22 | 2013-12-04 | 腾讯科技（深圳）有限公司 | Implementation method and system for enhancing real interaction |
US20150348329A1 (en) * | 2013-01-04 | 2015-12-03 | Vuezr, Inc. | System and method for providing augmented reality on mobile devices |
US20140267792A1 (en) * | 2013-03-15 | 2014-09-18 | daqri, inc. | Contextual local image recognition dataset |
US20150023650A1 (en) * | 2013-07-19 | 2015-01-22 | Google Inc. | Small-Screen Movie-Watching Using a Viewport |
US20170178690A1 (en) * | 2013-07-19 | 2017-06-22 | Google Inc. | Small-Screen Movie-Watching Using a Viewport |
US9552674B1 (en) * | 2014-03-26 | 2017-01-24 | A9.Com, Inc. | Advertisement relevance |
US20170168559A1 (en) * | 2014-03-26 | 2017-06-15 | A9.Com, Inc. | Advertisement relevance |
CN105046213A (en) * | 2015-06-30 | 2015-11-11 | 成都微力互动科技有限公司 | Method for augmenting reality |
US10339718B1 (en) * | 2017-12-29 | 2019-07-02 | Verizon Patent And Licensing Inc. | Methods and systems for projecting augmented reality content |
JP2018141970A (en) * | 2018-02-23 | 2018-09-13 | 大日本印刷株式会社 | Video content display device, glasses, video content processing system, and video content display program |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220108536A1 (en) * | 2020-10-07 | 2022-04-07 | Samsung Electronics Co., Ltd. | Method of displaying augmented reality and electronic device for performing same |
Also Published As
Publication number | Publication date |
---|---|
WO2021173147A1 (en) | 2021-09-02 |
EP4111696A1 (en) | 2023-01-04 |
US20220335661A1 (en) | 2022-10-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20200342681A1 (en) | Interaction system for augmented reality objects | |
US11043031B2 (en) | Content display property management | |
CN109952610B (en) | Selective identification and ordering of image modifiers | |
US9836929B2 (en) | Mobile devices and methods employing haptics | |
US20220335661A1 (en) | System and method for playback of augmented reality content triggered by image recognition | |
US20190102946A1 (en) | Systems, methods and apparatuses for deployment and targeting of context-aware virtual objects and behavior modeling of virtual objects based on physical principles | |
US20160041981A1 (en) | Enhanced cascaded object-related content provision system and method | |
US11651022B2 (en) | Video generation system to render frames on demand using a fleet of servers | |
US9671941B1 (en) | Graphical behaviors for recognition interfaces | |
US20210243487A1 (en) | Video generation system to render frames on demand using a fleet of gpus | |
US20130076788A1 (en) | Apparatus, method and software products for dynamic content management | |
EP3850591B1 (en) | Loading indicator in augmented reality environment | |
US11651539B2 (en) | System for generating media content items on demand | |
KR20230003388A (en) | Digital supplement association and retrieval for visual search | |
US11302079B2 (en) | Systems and methods for displaying and interacting with a dynamic real-world environment | |
Siltanen et al. | User interaction for mobile devices | |
CN113614794A (en) | Managing content in augmented reality | |
CN103593876A (en) | Electronic device, and method for controlling object in virtual scene in the electronic device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |