CN113312235A - Service quality early warning power capping system with optimized throughput - Google Patents
Service quality early warning power capping system with optimized throughput Download PDFInfo
- Publication number
- CN113312235A CN113312235A CN202110586000.4A CN202110586000A CN113312235A CN 113312235 A CN113312235 A CN 113312235A CN 202110586000 A CN202110586000 A CN 202110586000A CN 113312235 A CN113312235 A CN 113312235A
- Authority
- CN
- China
- Prior art keywords
- power
- machines
- threshold
- tasks
- limits
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/26—Power supply means, e.g. regulation thereof
- G06F1/32—Means for saving power
- G06F1/3203—Power management, i.e. event-based initiation of a power-saving mode
- G06F1/3234—Power saving characterised by the action undertaken
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/3003—Monitoring arrangements specially adapted to the computing system or computing system component being monitored
- G06F11/3006—Monitoring arrangements specially adapted to the computing system or computing system component being monitored where the computing system is distributed, e.g. networked systems, clusters, multiprocessor systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/26—Power supply means, e.g. regulation thereof
- G06F1/32—Means for saving power
- G06F1/3203—Power management, i.e. event-based initiation of a power-saving mode
- G06F1/3234—Power saving characterised by the action undertaken
- G06F1/329—Power saving characterised by the action undertaken by task scheduling
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/26—Power supply means, e.g. regulation thereof
- G06F1/28—Supervision thereof, e.g. detecting power-supply failure by out of limits supervision
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/26—Power supply means, e.g. regulation thereof
- G06F1/32—Means for saving power
- G06F1/3203—Power management, i.e. event-based initiation of a power-saving mode
- G06F1/3206—Monitoring of events, devices or parameters that trigger a change in power modality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/26—Power supply means, e.g. regulation thereof
- G06F1/32—Means for saving power
- G06F1/3203—Power management, i.e. event-based initiation of a power-saving mode
- G06F1/3234—Power saving characterised by the action undertaken
- G06F1/324—Power saving characterised by the action undertaken by lowering clock frequency
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/30—Monitoring
- G06F11/3051—Monitoring arrangements for monitoring the configuration of the computing system or of the computing system component, e.g. monitoring the presence of processing resources, peripherals, I/O links, software programs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
- G06F9/4881—Scheduling strategies for dispatcher, e.g. round robin, multi-level priority queues
- G06F9/4893—Scheduling strategies for dispatcher, e.g. round robin, multi-level priority queues taking into account power or heat criteria
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5094—Allocation of resources, e.g. of the central processing unit [CPU] where the allocation takes into account power or heat criteria
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/54—Interprogram communication
- G06F9/547—Remote procedure calls [RPC]; Web services
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2209/00—Indexing scheme relating to G06F9/00
- G06F2209/50—Indexing scheme relating to G06F9/50
- G06F2209/5021—Priority
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y02—TECHNOLOGIES OR APPLICATIONS FOR MITIGATION OR ADAPTATION AGAINST CLIMATE CHANGE
- Y02D—CLIMATE CHANGE MITIGATION TECHNOLOGIES IN INFORMATION AND COMMUNICATION TECHNOLOGIES [ICT], I.E. INFORMATION AND COMMUNICATION TECHNOLOGIES AIMING AT THE REDUCTION OF THEIR OWN ENERGY USE
- Y02D10/00—Energy efficient computing, e.g. low power processors, power management or thermal management
Abstract
The present disclosure relates to a quality of service early warning power capping system with throughput optimization. A method is described for minimizing interruption of throughput-oriented jobs in a power overbooking service through dynamic control. The mechanism controls power in a hardware independent manner, and the strategy employs a multi-threshold approach that balances power safety and workload impact. Furthermore, when power measurements are not available, alternative control mechanisms may ensure that the system is operating properly.
Description
Cross Reference to Related Applications
This application claims benefit of the filing date of U.S. provisional patent application No.63/030,639, filed on 27/5/2020, the disclosure of which is incorporated herein by reference.
Technical Field
The present disclosure relates to a quality of service early warning power capping system with throughput optimization.
Background
Data centers form a backbone of popular online services such as search, streaming video, email, social networking, online shopping, and the cloud. The increasing demand for online services has forced very large providers to invest large amounts of capital to continually expand their data center fleet. Most of these investments are allocated for purchasing and building infrastructure, such as buildings, power delivery and refrigeration, to host servers that make up the warehouse-level computers.
If all servers are 100% utilized, power overbooking is the practice of deploying more servers in a data center than the data center nominally supports. It can increase the power capacity of existing data centers locally and reduce future data center setups. On the other hand, power overbooking risks overloading during power peaks, and therefore protection systems such as power capping are often attached. The power capping system enables safe power overbooking by preventing overloading during power emergencies. Power capping actions include suspending low priority tasks, throttling (threaded) CPU voltage and frequency using techniques such as Dynamic Voltage and Frequency Scaling (DVFS) and Running Average Power Limit (RAPL), or packing threads in a subset of available cores. The action needs to be compatible with the workload and meet the Service Level Objective (SLO). However, this is a challenge for clusters where throughput-oriented workloads are collocated with delay-sensitive workloads.
Throughput-oriented tasks represent an important class of computational workloads. Examples include Web indexing, log processing, and machine learning model training. These workload pairs are often time to compute, deadlines are usually in hours, making them ideal candidates for performance throttling when clusters face a power emergency due to power overbooking. However, missing deadlines can lead to serious consequences such as loss of revenue and degradation of quality, making them less prone to interruption.
Delay sensitive workloads are another class. They need to complete the requested calculations in milliseconds to seconds. One typical example is handling jobs requested by a user. High latency can lead to poor user experience, ultimately resulting in loss of users and revenue. Unlike throughput-oriented pipelines, such jobs are not suitable for performance throttling. They are generally considered high priority and need to be free of power capping. Throughput-oriented and delay-sensitive jobs are typically co-located on the same server to improve resource utilization. This presents a significant challenge to power capping since a fine-grained power capping mechanism is required.
Disclosure of Invention
One aspect of the present disclosure provides a method comprising: receiving, by one or more processors, power measurements for a plurality of machines in a data center, the plurality of machines performing one or more tasks; identifying power limits for a plurality of machines; comparing, by the one or more processors, the received power measurements to power limits of the plurality of machines; based on the comparison, determining whether to withdraw (shed) power consumed by the plurality of machines; and instructing, by the one or more processors, the plurality of machines to operate according to the one or more restrictions to reduce power consumption.
According to some examples, the method may further comprise: identifying a first threshold; and identifying a second threshold value that is higher than the first threshold value; wherein determining whether to withdraw power comprises determining whether the received power measurement meets or exceeds a second threshold, and wherein commanding the plurality of machines to operate in accordance with the one or more limits comprises sending a command to reduce power consumption by a first predetermined percentage. Determining whether to withdraw power may include determining whether the received power measurement exceeds the first threshold only during a predetermined time period after the second threshold has been met or exceeded. When the first threshold is met or exceeded during the predetermined period of time, the method may further include sending a second command to the plurality of machines to reduce power consumption by a second predetermined percentage, the second predetermined percentage being lower than the first predetermined percentage.
According to some examples, measurements may be received from one or more power meters coupled to a plurality of machines.
Commanding the plurality of machines to operate according to the one or more restrictions may include: for all machines in the power domain, the processing time of the tasks within the machine-level scheduler is defined. Defining the processing time may include applying a first multiplier to the low priority task, wherein the first multiplier is selected to prevent the low priority task from running and consuming power. In some examples, the method may further include applying a second multiplier to the low priority task. One or more high priority tasks may be exempted from bounds.
According to some examples, the method may further comprise limiting a number of available schedulable processor entities to control power when the power measurement becomes unavailable.
Commanding the plurality of machines to operate according to the one or more restrictions may include making a portion of the processing components of each of the plurality of machines unavailable for the task.
Another aspect of the disclosure provides a system comprising one or more processors in communication with a plurality of machines that perform one or more tasks. The one or more processors may be configured to receive power measurements for a plurality of machines; identifying power limits for a plurality of machines; comparing the received power measurements to power limits of the plurality of machines; based on the comparison, determining whether to remove power consumed by the plurality of machines; and instructing, by the one or more processors, the plurality of machines to operate according to the one or more restrictions to reduce power consumption.
The one or more processors may be further configured to: identifying a first threshold; and identifying a second threshold value that is higher than the first threshold value; wherein determining whether to withdraw power comprises determining whether the received power measurement meets or exceeds a second threshold, and wherein commanding the plurality of machines to operate in accordance with the one or more limits comprises sending a command to reduce power consumption by a first predetermined percentage. Determining whether to withdraw power may include determining whether the received power measurement exceeds the first threshold only during a predetermined time period after the second threshold has been met or exceeded. When the first threshold is met or exceeded during the predetermined period of time, the one or more processors may be further configured to send a second command to the plurality of machines to reduce power consumption by a second predetermined percentage, the second predetermined percentage being lower than the first predetermined percentage.
According to some examples, measurements may be received from one or more power meters coupled to a plurality of machines.
Commanding the plurality of machines to operate according to the one or more restrictions may include: for all machines in the power domain, the processing time of the tasks within the machine-level scheduler is defined. In defining the processing time, the one or more processors may be configured to apply a first multiplier to the low priority task, wherein the first multiplier is selected to prevent the low priority task from running and consuming power. The one or more processors may be configured to exempt the one or more high priority tasks from bounds.
In some examples, instructing the plurality of machines to operate according to the one or more restrictions may include making a portion of the processing components of each of the plurality of machines unavailable for the task.
Yet another aspect of the disclosure provides a non-transitory computer-readable medium storing instructions executable by one or more processors to perform a method comprising: receiving power measurements for a plurality of machines in a data center, the plurality of machines performing one or more tasks; identifying power limits for a plurality of machines; comparing the received power measurements to power limits of the plurality of machines; based on the comparison, determining whether to remove power consumed by the plurality of machines; and instructing the plurality of machines to operate according to the one or more limits to reduce power consumption.
Drawings
Fig. 1A is a block diagram illustrating an exemplary software architecture of a system according to aspects of the present disclosure.
Fig. 1B is a block diagram illustrating an exemplary system in accordance with aspects of the present disclosure.
Fig. 2A-B are graphs illustrating example load shaping policies in accordance with aspects of the present disclosure.
3A-C illustrate exemplary results of running a workload according to aspects of the present disclosure.
FIG. 4 provides a graph illustrating that throttling is triggered by various combinations of parameters, in accordance with aspects of the present disclosure.
Fig. 5 provides a graph illustrating total power consumption and CPU usage, in accordance with aspects of the present disclosure.
Fig. 6 is a flow chart illustrating an exemplary method according to aspects of the present disclosure.
Detailed Description
The present disclosure provides a system and method for throttling CPU shares of throughput-oriented workloads to slow them sufficiently to keep power below a specified budget, while not impacting latency-sensitive jobs. The system architecture enables oversubscription (oversubscription) over large power domains. Power pooling (pooling) and statistical multiplexing across machines in a large power domain maximizes the likelihood of power overbooking. Moreover, the system implements mechanisms and strategies for power throttling with broad applicability. The system may be designed to rely solely on established Linux kernel functionality and is therefore independent of the hardware platform. This allows flexibility in introducing various platforms into the data center without compromising the efficiency of the power capping.
The task level mechanism allows for differentiated quality of service (QoS). In particular, the system does not impact the servicing of delay sensitive workloads collocated with throughput oriented workloads and has the ability to apply different CPU capping to workloads with different SLOs. Platform independence and QoS pre-warning features allow the system to be customized for a wide range of hardware platforms and software applications.
The advantage of this system is that it minimizes performance impact while achieving power safety. The two threshold scheme enables the lowest performance impact while ensuring that a large amount of power can be removed to avoid power overload in emergency situations. The system focuses on availability and introduces a fail-over subsystem to maintain power safety guarantees in the face of power telemetry failures.
The system is capable of performing two end-to-end power capping actuations tailored for throughput-oriented workloads: a main mechanism called reactive (reactive) capping, and a fail-over mechanism called active (reactive) capping. Reactive capping monitors the real-time power signal read from the power meter and reacts to high power measurements by throttling the workload. When the power signal becomes unavailable (e.g., due to meter down time), the active capping will take over and assess the risk of the circuit breaker tripping. The evaluation is based on factors such as the most recent power and how long the signal is not available. If the risk is considered high, it will actively throttle the task.
Architecture and implementation
FIG. 1A illustrates an exemplary software architecture of a system. As shown, the architecture includes a meter observer module 124, a power notifier module 126, a risk evaluator module 122, and a machine manager module 128. The meter observer module 144 polls for power readings from the meters 114. For example, the meter observer module 144 may poll at a rate of one reading per second, multiple readings per second, one reading per second, and so forth. The meter observer module 144 passes the readings to the power notifier module 126 and also stores a copy in the power history database 112.
The power notifier 126 is a central module that implements the control logic for reactive and active capping. When a power reading is available, it uses the reading for reactive capping logic. When the reading is not available, it queries the risk evaluator module 122 for active capping logic.
The risk evaluator 122 uses historical power information from the power history database 112 to evaluate the risk of circuit breaker tripping. If either logic determines capping, the power notifier 126 passes the appropriate capping parameters to the machine manager module 128. For example, the power capping parameters may be received from the power limit database 116. In response, the machine manager module 128 sends load shaping requests to one or more node controllers 130 associated with the respective machines. The load shaping request may be sent in the form of a Remote Procedure Call (RPC) or any other message or communication format. According to some examples, RPCs may be sent to multiple node controllers 130 simultaneously. The RPC may include commands that cause the respective machine to power down.
According to some examples, the load-shaping request may include fields such as a maximum priority of the job to be throttled, a multiplier for use of hard-capped (hardcap) lower priority jobs, and a duration for capped jobs. The request may also include an instruction whether the multiplier should be applied only to the CPU limit, or to the sustained CPU rate.
When a new load-shaping request is received, the node controller 130 checks whether there is already a load-shaping event in progress. If there are no ongoing events, a new request may be accepted. If there is already an ongoing event, the new request may be accepted if it is more aggressive with respect to its power reduction effect. For example, if a new RPC has a higher maximum throttling priority or a lower hard-cap multiplier, or if switching from applying only the multiplier from the CPU limit to applying for a sustained CPU rate, the new PRC may be more aggressive.
The size of the power domain may vary from several megawatts to tens of megawatts, depending on the power architecture of the data center. One instance of the system may be deployed for each protected power domain. The instance may be replicated for fault tolerance. For example, there are 4 copies (replicas) in a 2-master, 2-slave configuration. The master replica may read the power meter and issue a power-down RPC. The power-fallback RPC service is designed to be idempotent and can handle duplicate RPCs from different master copies. Two identical master copies may be used to ensure that power withdrawal is available even during master elections. When the master becomes unavailable, the slave takes over.
FIG. 1B illustrates an exemplary system that includes a server device (such as controller 190) on which the system may be implemented. Controller 190 may include hardware configured to manage load shaping and throttling of devices in data center 180. According to one example, the controller 190 may reside in and control a particular data center. According to other examples, controller 190 may be coupled to one or more data centers 180, such as over a network, and may manage the operation of multiple data centers.
In some examples, controller 190 may be in communication with computing devices in data center 180 and may facilitate execution of programs. For example, controller 190 may track the capabilities, status, workload, or other information of each computing device and use such information to assign tasks. Controller 190 may include a processor 198 and memory 192, including data 194 and instructions 196. In other examples, such operations may be performed by one or more computing devices in the data center 180, and a separate controller may be omitted from the system.
The controller 190 may include a processor 198, memory 192, and other components typically found in a server computing device. The memory 192 may store information accessible by the processor 198, including instructions 196 that may be executed by the processor 198. The memory may also include data 194 that may be retrieved, manipulated, or stored by the processor 198. The memory 192 may be a non-transitory computer readable medium capable of storing information accessible by the processor 198, such as a hard disk drive, solid state drive, tape drive, optical storage, memory card, ROM, RAM, DVD, CD-ROM, writable, and read-only memory. The processor 198 may be a well known processor or other less well known type of processor. Alternatively, the processor 198 can be a dedicated controller, such as an ASIC.
The instructions 196 may be a set of instructions (such as machine code) that are directly executed by the processor 198 or a set of instructions (such as script) that are indirectly executed by the processor 198. In this regard, the terms "instructions," "steps," and "programs" may be used interchangeably herein. The instructions 196 may be stored in an object code format for direct processing by the processor 198 or in other types of computer languages, including a collection of separate source code modules or scripts that are interpreted or pre-compiled as needed.
Although fig. 1B functionally shows the processor 198 and the memory 192 as being within the same block, the processor 198 and the memory 192 may actually comprise multiple processors and memories, which may or may not be stored within the same physical housing. For example, some instructions 196 and data 194 may be stored on a removable CD-ROM, while other instructions may be stored within a read-only computer chip. Some or all of the instructions and data may be stored in a location that is physically remote from the processor 198 but still accessible to the processor 198. Similarly, processor 198 can actually comprise a collection of processors, which may or may not operate in parallel.
Reactive capping with CPU bandwidth control
CPU usage is a good proxy for CPU power consumed by running tasks. According to some examples, the CPU bandwidth control function of the linux (r) full fair scheduler (CFS) kernel scheduler may be used to precisely control the CPU usage of tasks running in a node, thereby allowing control of the power consumed by the node.
Each task may be running in its own control group (cgroup). The scheduler provides two parameters in cgroup, namely quota and period. quotata controls the amount of CPU time a workload gets to run during a period of time. It is shared between the CPUs in the system. Quota and period can be set for each cgroup, typically specified at millisecond granularity. The individual (per cgroup and per CPU), accumulated runtime _ remaining variables are saved in the kernel. The accumulated runtime _ remaining will be consumed while the thread is running. When it reaches zero, the thread that is running will be rescheduled (scheduled) and any thread in cgroup cannot run until the run time is replenished at the beginning of the next slot. Historical CPU usage of all workloads running in the machine may be tracked. During a power event, each node in the power domain may receive an RPC call to throttle throughput-oriented workloads.
The RPC call contains a parameter on how much the CPU usage of the task should be reduced. The node controller receiving the RPC call uses the historical CPU usage of all throughput-oriented workloads to determine the throttled CPU time. Then, new values of quota and period are calculated for each cgroup and written to the cgroup in the machine. QoS differentiation is achieved by grouping tasks of different priorities into different cgroups. In our embodiment, throughput-oriented tasks are assigned a lower priority, while delay-sensitive tasks are assigned a higher priority. CPU bandwidth control is applied to the cgroup of low priority tasks, freeing the cgroup of high priority tasks.
The advantage of CPU bandwidth control over DVFS and RAPL is that it is platform independent and has control in cgroup. To ensure that the CPU bandwidth control achieves the desired power reduction, it can be used in conjunction with power metering and negative feedback. The relationship between CPU bandwidth throttling and power consumption remains monotonic, thus stabilizing the feedback loop. CPU throttling (jailing) is another mechanism that may be used to limit power consumption during power down of a power meter. CPU throttling has a more predictable throttling power relationship suitable for open loop control.
And (3) control strategy: load shaping
A variety of different load shaping control strategies are available. According to a first strategy, shown in fig. 2A, a multiplier is applied when the thresholds are reached for both the low and high thresholds. For example, when the power reaches a low threshold, a soft multiplier will be applied. If the power continues to increase and reaches a high threshold, a hard multiplier will be applied.
The multiplier may be, for example, a number between 0 and 1 that is applied to reduce the power consumed by the single machine, such as by reducing the number of jobs executed by the single machine at a given time. For example, a multiplier close to 0 (such as 0.01) may quickly reduce power by reducing 99% of the tasks on a single machine at a given time. Such multipliers may be referred to as "hard" or "high impact" multipliers. Conversely, a multiplier close to 1 (such as 0.9 or 0.75) may reduce power and also minimize performance impact by reducing the lower percentage of tasks performed by a single machine. Such multipliers may be referred to as "soft" or "low impact" multipliers.
The multiplier may be applied to CPU limits, sustained CPU rates, number of low priority jobs, etc. For example, the multiplier may define the processing time of a task inside the machine-level scheduler for all machines in the power domain. For example, the multiplier may define the time during which a task may access the CPU. The multiplier may be applied only to lower priority tasks, while higher priority tasks may be exempt.
Fig. 2B illustrates a second load shaping control strategy that uses power meter readings as input to determine when to throttle workloads and how much to throttle workloads. A multiplier parameter is predetermined for the throttling action. At the start of throttling, the node controller receives an RPC call containing the multiplier and sets a CPU cap for each affected task, which is calculated by the following formula:
capping multiplier CPUavg
Where CPUavg is the average CPU usage of tasks over the past 10 seconds. The cap is updated every second for the entire period that the throttling is active. The effect is that if the task consumes the CPU to reach the cap, the CPU cap of the task decreases exponentially over time at a rate approximately equal to the multiplier. This is proportional to their CPU utilization and is fairly penalizing to the task.
This strategy aims to balance between power safety and performance impact. It does so by maintaining two power thresholds, a higher threshold near the equipment limit for system protection, such as 98% of the limit, and a lower threshold, such as at 96% of the limit. The higher threshold is associated with a "hard" or "high impact" multiplier close to 0 (such as 0.01) intended to quickly reduce power consumption to ensure safety. The lower threshold is associated with a "soft" or "low impact" multiplier close to 1, such as 0.9 or 0.75, intended to minimize performance impact even though power may temporarily exceed the lower threshold.
According to some examples, the lower threshold is not activated before the throttling is triggered and is deactivated when the throttling is active for a period of time. In this way, power is allowed to reach a range between the two thresholds without throttling. More specifically, the higher threshold is active and the lower threshold is not active until any throttling occurs. The first throttling event will only occur when the power reaches a higher threshold. The lower threshold is activated once the power reaches the upper threshold and expires after a specified duration when the power is below the lower threshold and no throttling occurs.
Throttling ceases when power drops below the minimum activity threshold. If throttling of all tasks is cancelled simultaneously, the power may surge and reach the threshold quickly again, causing unnecessary oscillations, or even reaching dangerous levels before load shaping takes effect. To avoid these undesirable effects, the system is designed to stop in a gradual manner. Each machine is assigned a random throttle timeout in the range [ timeout min, timeout max ]. When throttling stops, the machine will progressively cancel throttling according to its timeout. This smoothes the power rise curve. In one exemplary embodiment, an RPC call may be sent every second to extend the timeout until the power is below the threshold. Instead of stopping the throttling of RPCs, a timeout of a repeated refresh may be used, as RPCs may be lost to reach the machine.
Load shaping parameters (such as an upper threshold and associated hard multiplier, a lower threshold and associated soft multiplier, a lower threshold expiration, and a throttle timeout range) may be predetermined. The threshold and multiplier may be selected to balance power safety and performance impact. The higher threshold may be close to the protected limit so that throttling is not triggered too often, but not so close that the system has enough time to react to the event. The hard multiplier may be close to 0 to quickly reduce the bulk of the power. The lower threshold may similarly be set to have a balance between throttling frequency and sufficient guard band for possible power peaks. The soft multiplier may be close to 1 to minimize performance impact. The expiration of the lower threshold may be long enough to make the lower threshold useful, but not so long as to significantly increase the throttling frequency. A throttle timeout range may be set for reasonable oscillation modes. For example, the throttle timeout range may be about 1-20 seconds.
Thus, in an embodiment, as power increases, the low threshold will be crossed more frequently. Emphasis is frequently activated because a multiplier is applied to the measured CPU utilization for tasks in the moving window. Keeping above the lower threshold will cause the CPU time to be limited to a percentage corresponding to the soft/low impact multiplier. Thus, as the power of the load fluctuates, the soft multiplier will converge to the effect of a high multiplier as needed. The high threshold is used as the activation point for the soft multiplier. As long as the load does not exceed the high threshold, the load is allowed to remain at or near the soft multiplier region indefinitely. The high threshold is further used to eliminate sudden power spikes that may occur when the low threshold is in an active state.
The load shaping control strategy determines when and how much the actuator should throttle CPU usage to control power. Formally, the power consumption of a power domain can be written as:
where t is (discrete) time, p is total power consumption, N is the number of machines, fiIs the power consumed by machine i as a monotonic function of normalized machine CPU utilization (at [0,1 ]]In the range) c)iIs a CPU used by a controllable task, uiIs an uncontrollable CPU used by the task-free and Linux kernel, and n is the power consumed by the non-machine device. CPU used by the capped controllable task (c)i) So that for the power limits l, p<l. Can prevent overload (p) in priority>l) while when p is<Keeping p close to l for l may improve efficiency.
According to some examples, a random un-throttled/multiplicative Reduction (RUMD) algorithm may be used. If p (t) > l, then capping is applied to the CPU utilization of each controllable task. The cap is equal to the previous usage of the task multiplied by a multiplier m in the range (0, 1). Then, the power consumption of the next time step is:
the caps may be updated frequently, such as once per second, and ciWill decrease exponentially with time until p<l. Due to uiAnd n, so that p (t +1) cannot be secured<p (t). However, embodiments may provide a high degree of confidence that the irrevocable power is less than the power limit, i.e.
Therefore, the power will eventually drop below this limit. For example, if the system is configured to completely cancel the throttling of all machines within 5 seconds, then 20% of the random non-overlapping set of machines will be cancelled per second.
In some examples, the capping may be progressively lifted in an additive manner for each machine simultaneously, resulting in a sum-add/product-subtract (AIMD) algorithm. Similar to AIMD, RUMD algorithms also have the desirable property of being partially distributed. There is a central component and distributed components that function primarily independently. In addition to the total power, the central policy controller does not require detailed system state such as CPU usage and task allocation for each machine. The distributed node controllers may make independent decisions based on only some of the parameters sent by the policy controller to all of the node controllers.
A failover mechanism: active capping
Reactive capping systems rely on power signals provided by power meters installed in proximity to protected power equipment, such as circuit breakers. This is different from the more widely adopted method of collecting power measurements from individual compute nodes and aggregating them at a higher layer. It has the advantage of simplicity, avoiding aggregation and related challenges such as time misalignment and partial collection failures, and avoiding the need to estimate power consumption of non-computing devices that do not provide power measurements, such as data center air conditioners.
Transient network problems can result in power signal interruptions of seconds to minutes, while meter downtime can be as long as days to weeks before being repaired. Without a power signal, the load shaping unit cannot determine how much to throttle each task under a strong power guarantee. One method of backup (referred to herein as CPU throttling) may operate without power measurement.
According to some examples, the signal may also be collected from an auxiliary source (such as a power supply unit of the machine) or from a power model.
CPU-guarded node level mechanism
CPU throttling reduces CPU execution of machines available for tasks by modifying the CPU affinity mask. A parameter jailing _ fraction is predetermined, which is a fraction of "disabled" CPUs, so that they are not available for a task. jailing _ fraction may be a fraction of the CPU of each machine that is made unavailable to the task.
It may happen that cluster operators deliberately overuse resources and raise machine utilization. When resource overuse is compounded by CPU throttling, intensive CPU resource contention is expected. Each task has a CPU limit request that translates to a share value in the Linux CFS. When the available CPU decreases due to throttling, the CFS may be utilized to maintain the CPU ratio between tasks. Some privileged processes (such as critical system daemons) are explicitly exempt from being monitored and can still run on the monitored CPU. This is because their CPU usage is very low compared to conventional tasks, but the risks and consequences of privileged system processes due to CPU starvation are very high. For example, a risk and consequence may be that the machine is not functioning properly.
According to some examples, each duress request is of a duration and can be updated by request. After the expiration of the throttling, the previously unavailable CPU becomes available for all tasks immediately.
CPU throttling immediately caps the peak power consumption because it effectively limits the maximum CPU utilization on each individual machine to (1-jailing _ fraction). Therefore, it sets an upper limit to power consumption, so that it can be safely operated for a long time without a power signal.
jailing _ fraction can be applied uniformly to a single machine regardless of their CPU utilization. Thus, a machine with a low utilization rate is less affected, while a machine with a high utilization rate is affected very much. When machine CPU usage is much lower than (1-jailing _ fraction), CPU throttling does not substantially affect the tasks on these machines. A second effect is that the disabled CPU is more likely to enter a deep sleep state due to increased idleness, which helps to further reduce power. Note that not all disabled CPUs may enter deep sleep states all the time, as exempt privileged processes and kernel threads may still use them from time to time.
In some instances, CPU throttling may result in a loose ability to differentiate QoS. For example, the delay of a service task may be affected disproportionately compared to the throughput of a batch task. The fact that delay sensitive tasks run at a higher priority and can preempt lower priority throughput-oriented tasks may diminish this effect. In some embodiments, CPU throttling may be used only if load shaping is not applicable.
The jailing _ fraction may be determined based on one or more factors, such as the performance SLO of the workload, CPU power relationships, and power overbooking. Because the CPU throttles QoS that does not differentiate workloads, there is a potential for high priority delay sensitive jobs to suffer performance degradation under high CPU contention. Applying the jailing _ fraction at the expected frequency specified by the control policy should not harm the SLO of these jobs.
For power safety, the power should be reduced to a safe level after some parts of the watchdog. The jailing _ fraction value may be calculated based on the power overbooking level and the CPU-power relationship for a given set of hardware in the power domain.
J＝1-Ucpu＝1-fpower cpu(1/(1+osr))
Wherein J is jailing _ fraction, UcpuIs the highest allowable CPU utilization, fpower cpuIs a function of converting power utilization to CPU utilization, and osr is a function of additional overbooked powerCapacity (as a fraction of nominal capacity) defined overbooking rate. 1/(1+ osr) gives the maximum safe power utilization, which can be converted to U assuming that the CPU power relationship is monotoniccpu. An exemplary jailing _ fraction may be 20% -50%.
As a backup method, CPU throttling may be triggered when power measurements from the meter are lost and the risk of power overload is high. The risk is determined by two factors, namely predicted power consumption and duration of meter unavailability. Higher predicted power consumption and longer meter unavailability mean higher risk. Given recent power consumption, the likelihood of power reaching the protected equipment limits during certain meter outages may be predicted based on historical data. If the probability is high due to recent high power consumption and long enough down time, CPU throttling may be triggered.
Comparison of node level mechanisms
3A-C illustrate exemplary results of running a workload that stresses the CPU and memory to maximize power consumption. Fig. 3A illustrates the result of using CPU bandwidth control, fig. 3B illustrates the result of using DVFS, and fig. 3C illustrates the result of using RAPL to limit CPU power. CPU power is normalized to the highest power observed when no power management mechanism is enabled. Fig. 3A shows that with CPU bandwidth control, the CPU power can be reduced to 34% of maximum power due to the significant CPU idle time of the bandwidth control. In contrast, fig. 3B shows that with DVFS, the power consumption is still relatively high, 57%, when the lowest frequency limit is applied. The clock frequency is normalized to the base frequency of the processor. The normalized frequency may be higher than 1.0 because the CPU clock frequency may be higher than the base frequency when thermal and other conditions permit. Only a part of the possible frequency range is shown in the diagram. Due to the stress testing nature of the workload, the highest observed clock frequency is close to the fundamental frequency without imposing limitations. The upper right hand corner of the graph reflects that the frequency limit must be lowered below the actual clock frequency to reduce power consumption.
Fig. 3C shows that of the three, RAPL has the widest range of power reduction. It can reduce the power to 22% of the maximum power. However, we note that as RAPL approaches the lowest power limits, the system management task is unresponsive, indicating that the machine timeout risk is higher if these limits are actually used. In contrast, CPU bandwidth control only throttles throughput-oriented jobs without impacting system management tasks.
An exemplary application: load shaping result
FIG. 4 illustrates the results of an experiment performed in a warehouse-sized data center running a production workload. Throttling is triggered manually using various parameter combinations. Power data is collected from the data center power meters, which happens to be the data that the system also reads. Other metrics are sampled from the various machines and aggregated at the same power domain level corresponding to the power readings. The power measurement data is normalized to the device limit of the power domain. Unless otherwise noted, machine metrics such as CPU utilization are normalized to the total capacity of all machines in the power domain. The task failures are normalized to the total number of affected tasks.
Load shaping is triggered by manually lowering the higher power threshold just below the ongoing power consumption of the power domain. Fig. 4(a1) shows a typical load shaping mode where the power oscillates around a lower threshold. Within a few seconds after the throttling is triggered, the power is greatly reduced due to the hard multiplier. While activating the lower threshold. When the power drops below the lower threshold, the throttle is gradually increased, and then the power is ramped back up until the lower threshold is reached. Then, the power is reduced again, but with less margin due to the soft multiplier. As the throttle is repeatedly opened and closed, the process continues, causing the power to oscillate near the lower threshold. Fig. 4(b1) shows that a soft multiplier close to 1.0 results in a smaller amplitude oscillation as expected compared to (a 1). The delay from the load shaping trigger to the large power reduction is less than 5 seconds. Fig. 4(a2) and (b2) show CPU utilization rates corresponding to (a1) and (b1), respectively. At the indicated CPU utilization level, the CPU utilization needs to be reduced by about 10% to reduce the power by 2%.
When tasks slow down, they should not fail due to CPU starvation or unexpected side effects.
One major advantage of load shaping compared to DVFS is that the quality of service can be differentiated at the cgroup level, allowing jobs with different SLOs to run on the same machine. Jobs may be divided into several groups according to their priority and low priority groups may be load-shaped without the need for high priority groups.
FIG. 5 illustrates the total power consumption and CPU usage for two sets of jobs during an event. The CPU utilization of the shaping group and the exemption group are reduced by about 10% and 3%, respectively. The exempt group is indirectly affected because the jobs in both groups are production jobs with complex interactions. One example is that exempting a high priority master job in the group would coordinate low priority workers in the shaping group, while the master job would be less workload and consume less CPU while throttling the workers. However, the ability of load shaping to differentiate jobs is readily apparent.
Load shaping reduces the power to a safe level just below the threshold and allows the power to oscillate around the threshold. However, in the extreme case where power still exceeds the threshold, the system will need to continually reduce the CPU bandwidth of the job, eventually rendering the job inoperable. For example, power may still be high after throttling is triggered, due to new compute-intensive jobs being scheduled continuously, or many high priority jobs that avoid this mechanism, bursting in their CPU utilization. In this case, stopping the affected job is a correct trade-off in order to prevent power overload.
Fig. 6 is a flow diagram illustrating an exemplary method 600 for load shaping. While operations are illustrated and described in a particular order, it will be understood that the order may be modified or operations may be performed concurrently. Further, operations may be added or omitted.
In block 610, power measurements are received for a plurality of machines performing one or more tasks. The measurements may be received, for example, at a controller or other processing unit or collection of processing units. The measurements may be received directly from the multiple machines or through one or more intermediate devices, such as power meters coupled to the multiple machines. The plurality of machines may be computing devices in a data center, for example. Multiple machines may be performing one or more tasks or jobs.
In block 620, power limits for a plurality of machines are identified. For example, the power limits for each machine may be stored in a database, and the database may be accessed to identify the limits for one or more particular machines.
In block 630, the received power measurement is compared to the identified power limit. For example, the controller may determine whether the power measurement is near a limit, such as whether the power measurement is within a predetermined range of the limit. According to some examples, one or more predefined thresholds may be set, wherein the comparison takes into account whether the one or more thresholds have been reached. For example, the first threshold may be set to a lower level, such as a first percentage of the power limit, and the second threshold may be set to a higher level than the first threshold, such as a second percentage of the power limit higher than the first percentage.
In block 640, it is determined whether to withdraw power consumed by the plurality of machines based on a comparison of the power measurement to the identified power limit. For example, if one or more machines of the plurality of machines exceed one or more predetermined thresholds, it may be determined that power should be withdrawn. According to a first example, such as shown in connection with FIG. 2A, where both a high threshold and a low threshold are set, it may be determined whether either threshold has been reached. According to a second example, such as shown in connection with fig. 2B, where both a high threshold and a low threshold are set, it may first only be determined whether the high threshold is reached. Once the high threshold is reached, a response action is triggered and the low threshold may be activated for a predetermined period of time. Within this time period, it may be determined whether a low threshold has been reached, thereby triggering a second responsive action. Once the time period expires, the low threshold may be deactivated and thus not considered again until the high threshold is reached again.
In block 650, if it is determined that power should be removed, a command may be sent to one or more of the plurality of machines to cause the one or more machines to operate in accordance with the one or more limits to reduce power. For example, an RPC may be sent that includes a multiplier to reduce workload. For example, the multiplier may be a number between 0 and 1 that may be applied to a CPU limit, a sustained CPU rate, or a number of tasks, such as low priority tasks. Thus, when a multiplier is applied, the workload is reduced by the percentage corresponding to the multiplier.
In examples where multiple thresholds are set, different limits may be implemented based on which threshold is triggered. For example, triggering a higher threshold may result in sending a hard multiplier (e.g., a number near 1) such that the workload is greatly reduced. By way of the same example, triggering a lower threshold may result in sending a soft multiplier (such as a number closer to 0) such that the reduction in workload is not as great in magnitude as when a hard multiplier is applied.
Unless otherwise specified, the above-described alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of example rather than by way of limitation of the subject matter defined by the claims. In addition, the provision of examples described herein and phrases such as "such as," "including," and the like should not be construed to limit claimed subject matter to the specific examples; rather, the examples are intended to illustrate only one of many possible embodiments. Further, the same reference numbers in different drawings may identify the same or similar elements.
Claims (21)
1. A method, comprising:
receiving, by one or more processors, power measurements for a plurality of machines in a data center, the plurality of machines performing one or more tasks;
identifying power limits for the plurality of machines;
comparing, by the one or more processors, the received power measurements to power limits of the plurality of machines;
determining whether to withdraw power consumed by the plurality of machines based on the comparison; and
commanding, by the one or more processors, the plurality of machines to operate according to one or more limits to reduce power consumption.
2. The method of claim 1, further comprising:
identifying a first threshold; and
identifying a second threshold value that is higher than the first threshold value;
wherein determining whether to relinquish power comprises determining whether the received power measurement meets or exceeds the second threshold, an
Wherein commanding the plurality of machines to operate in accordance with one or more limits comprises sending a command to reduce the power consumption by a first predetermined percentage.
3. The method of claim 2, wherein determining whether to tear down power comprises: determining whether the received power measurement meets or exceeds the first threshold only during a predetermined period of time after the second threshold has been met or exceeded.
4. The method of claim 3, wherein when the first threshold is met or exceeded during the predetermined period of time, the method further comprises sending a second command to the plurality of machines to reduce power consumption by a second predetermined percentage, the second predetermined percentage being lower than the first predetermined percentage.
5. The method of claim 1, wherein the power measurements are received from one or more power meters coupled to the plurality of machines.
6. The method of claim 1, wherein instructing the plurality of machines to operate in accordance with the one or more restrictions comprises: for all machines in the power domain, the processing time of the tasks within the machine-level scheduler is defined.
7. The method of claim 6, wherein defining the processing time comprises applying a first multiplier to low priority tasks, wherein the first multiplier is selected to prevent the low priority tasks from running and consuming power.
8. The method of claim 7, further comprising applying a second multiplier to the low priority task.
9. The method of claim 7, further comprising exempting one or more high priority tasks from the power limit.
10. The method of claim 1, further comprising limiting a plurality of available schedulable processor entities to control power when the power measurement becomes unavailable.
11. The method of claim 1, wherein instructing the plurality of machines to operate according to one or more restrictions comprises making a portion of processing components of each of the plurality of machines unavailable to tasks.
12. A system, comprising:
one or more processors in communication with a plurality of machines performing one or more tasks, the one or more processors configured to:
receiving power measurements for a plurality of machines;
identifying power limits for the plurality of machines;
comparing the received power measurements to power limits of the plurality of machines;
determining whether to withdraw power consumed by the plurality of machines based on the comparison; and
commanding the plurality of machines to operate according to one or more limits to reduce power consumption.
13. The system of claim 12, wherein the one or more processors are further configured to:
identifying a first threshold; and
identifying a second threshold value that is higher than the first threshold value;
wherein determining whether to relinquish power comprises determining whether the received power measurement meets or exceeds the second threshold, an
Wherein commanding the plurality of machines to operate in accordance with one or more limits comprises sending a command to reduce the power consumption by a first predetermined percentage.
14. The system of claim 13, wherein determining whether to withdraw power comprises: determining whether the received power measurement meets or exceeds the first threshold only during a predetermined period of time after the second threshold has been met or exceeded.
15. The system of claim 14, wherein when the first threshold is met or exceeded during the predetermined period of time, the one or more processors are further configured to send a second command to the plurality of machines to reduce power consumption by a second predetermined percentage, the second predetermined percentage being lower than the first predetermined percentage.
16. The system of claim 12, wherein the power measurements are received from one or more power meters coupled to the plurality of machines.
17. The system of claim 12, wherein instructing the plurality of machines to operate in accordance with the one or more restrictions comprises: for all machines in the power domain, the processing time of the tasks within the machine-level scheduler is defined.
18. The system of claim 17, wherein in defining the processing time, the one or more processors are configured to apply a first multiplier to low priority tasks, wherein the first multiplier is selected to prevent the low priority tasks from running and consuming power.
19. The system of claim 18, wherein in defining the processing time, the one or more processors are configured to exempt one or more high priority tasks from the power limit.
20. The system of claim 12, wherein instructing the plurality of machines to operate according to one or more restrictions comprises making a portion of processing components of each machine of the plurality of machines unavailable for tasks.
21. A non-transitory computer-readable medium storing instructions executable by one or more processors to perform operations comprising:
receiving power measurements for a plurality of machines in a data center, the plurality of machines performing one or more tasks;
identifying power limits for the plurality of machines;
comparing the received power measurements to power limits of the plurality of machines;
determining whether to withdraw power consumed by the plurality of machines based on the comparison; and
commanding the plurality of machines to operate according to one or more limits to reduce power consumption.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063030639P | 2020-05-27 | 2020-05-27 | |
US63/030,639 | 2020-05-27 | ||
US17/243,853 US11599184B2 (en) | 2020-05-27 | 2021-04-29 | Throughput-optimized, quality-of-service aware power capping system |
US17/243,853 | 2021-04-29 |
Publications (1)
Publication Number | Publication Date |
---|---|
CN113312235A true CN113312235A (en) | 2021-08-27 |
Family
ID=76098863
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202110586000.4A Pending CN113312235A (en) | 2020-05-27 | 2021-05-27 | Service quality early warning power capping system with optimized throughput |
Country Status (3)
Country | Link |
---|---|
US (2) | US11599184B2 (en) |
EP (1) | EP3916554B1 (en) |
CN (1) | CN113312235A (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11916430B2 (en) * | 2021-01-21 | 2024-02-27 | Dell Products L.P. | Optimization of low power states while AC line power availability is limited |
US20240005971A1 (en) * | 2022-06-29 | 2024-01-04 | Advanced Micro Devices, Inc. | Channel and sub-channel throttling for memory controllers |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7707443B2 (en) * | 2003-07-18 | 2010-04-27 | Hewlett-Packard Development Company, L.P. | Rack-level power management of computer systems |
US8832476B2 (en) * | 2010-09-28 | 2014-09-09 | Google Inc. | Power allotment distribution in a data center |
US9021290B2 (en) * | 2012-04-05 | 2015-04-28 | Oracle International Corporation | Systems and methods for dynamic power management in a blade server |
US9424098B2 (en) * | 2012-08-31 | 2016-08-23 | Silicon Graphics International Corp. | Dynamic resource scheduling |
US9250684B1 (en) | 2015-02-25 | 2016-02-02 | Quanta Computer Inc. | Dynamic power capping of a subset of servers when a power consumption threshold is reached and allotting an amount of discretionary power to the servers that have power capping enabled |
US10481659B2 (en) * | 2016-03-03 | 2019-11-19 | International Business Machines Corporation | Rack resource utilization |
-
2021
- 2021-04-29 US US17/243,853 patent/US11599184B2/en active Active
- 2021-05-25 EP EP21175697.8A patent/EP3916554B1/en active Active
- 2021-05-27 CN CN202110586000.4A patent/CN113312235A/en active Pending
-
2023
- 2023-02-23 US US18/173,293 patent/US11966273B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
US11966273B2 (en) | 2024-04-23 |
US11599184B2 (en) | 2023-03-07 |
EP3916554A1 (en) | 2021-12-01 |
US20210373639A1 (en) | 2021-12-02 |
EP3916554B1 (en) | 2024-05-15 |
US20230305618A1 (en) | 2023-09-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11966273B2 (en) | Throughput-optimized, quality-of-service aware power capping system | |
US8843772B2 (en) | Systems and methods for dynamic power allocation in an information handling system environment | |
US8818989B2 (en) | Memory usage query governor | |
US20140325524A1 (en) | Multilevel load balancing | |
CN111694633A (en) | Cluster node load balancing method and device and computer storage medium | |
US11907762B2 (en) | Resource conservation for containerized systems | |
US11119563B2 (en) | Dynamic power capping of multi-server nodes in a chassis based on real-time resource utilization | |
WO2008016613A2 (en) | Self-monitoring and self-adjusting power consumption computer control system | |
CN110677274A (en) | Event-based cloud network service scheduling method and device | |
CN111880906A (en) | Virtual machine high-availability management method, system and storage medium | |
CN113672345A (en) | IO prediction-based cloud virtualization engine distributed resource scheduling method | |
EP2245518A1 (en) | Changing power states of data handling devices to meet redundancy criterion | |
Cho et al. | A battery lifetime guarantee scheme for selective applications in smart mobile devices | |
CN112887407B (en) | Job flow control method and device for distributed cluster | |
CN113590285A (en) | Method, system and equipment for dynamically setting thread pool parameters | |
US20180167288A1 (en) | Service system and control method of the same | |
CN107872480B (en) | Big data cluster data balancing method and device | |
CN111309480A (en) | Method and equipment for dynamic power consumption capping regulation and control | |
US20160357243A1 (en) | Processor throttle rate limiter | |
US10621006B2 (en) | Method for monitoring the use capacity of a partitioned data-processing system | |
CN110457130B (en) | Distributed resource elastic scheduling model, method, electronic equipment and storage medium | |
CN113542027A (en) | Flow isolation method, device and system based on distributed service architecture | |
Li et al. | Noah: Reinforcement-Learning-Based Rate Limiter for Microservices in Large-Scale E-Commerce Services | |
EP3161586B1 (en) | Method and system for regulating in real time the clock frequencies of at least one cluster of electronic machines | |
CN110989824B (en) | Energy consumption management method, device, equipment and medium for cloud platform |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |