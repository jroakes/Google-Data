US20160322066A1 - Audio Data Classification - Google Patents
Audio Data Classification Download PDFInfo
- Publication number
- US20160322066A1 US20160322066A1 US13/932,198 US201313932198A US2016322066A1 US 20160322066 A1 US20160322066 A1 US 20160322066A1 US 201313932198 A US201313932198 A US 201313932198A US 2016322066 A1 US2016322066 A1 US 2016322066A1
- Authority
- US
- United States
- Prior art keywords
- spectrogram
- spectral
- projection
- audio
- audio data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/683—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L25/81—Detection of presence or absence of voice signals for discriminating voice from music
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10H—ELECTROPHONIC MUSICAL INSTRUMENTS; INSTRUMENTS IN WHICH THE TONES ARE GENERATED BY ELECTROMECHANICAL MEANS OR ELECTRONIC GENERATORS, OR IN WHICH THE TONES ARE SYNTHESISED FROM A DATA STORE
- G10H1/00—Details of electrophonic musical instruments
- G10H1/0008—Associated control or indicating means
- G10H1/0025—Automatic or semi-automatic music composition, e.g. producing random music, applying rules from music theory or modifying a musical piece
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/03—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters
- G10L25/18—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters the extracted parameters being spectral information of each sub-band
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
Definitions
- the present specification relates to classifying audio data, and more specifically, classifying audio data as music audio data or non-music audio data.
- An individual may hear a song on the radio or in a public establishment, and may want to later acquire the song by purchasing the song from an online music distribution service.
- an audio sample is analyzed to determine whether the audio sample includes music.
- a user may be in a coffee house that is playing background music.
- the user may be interested in learning more information about the music, such as a song name or artist title associated with background music.
- the user can interact with a mobile computing device, e.g., a smartphone, to facilitate determining such information.
- the mobile computing device can detect the audio data, e.g., the background music, encode the audio data as waveform data, and provide the waveform data to a server-based computing environment.
- inventions described in this specification may be embodied in methods that include the actions of receiving an audio sample that is associated with audio data, computing a spectrogram of the received audio sample, detecting one or more beats in the spectrogram, detecting one or more sustained pitches in the spectrogram around the beats, determining, for each of the beats, a score based on the sustained pitches around the beat, and determining a beat pitch score that indicates the likelihood that the audio sample contains music audio data, the beat pitch score based on the scores for each of the beats.
- Another aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving an audio sample that is associated with audio data, computing a spectrogram of the received audio sample, determining an average spectral envelope of the spectrogram, determining one or more differences between adjacent values in the average spectral envelope, and determining a spectral fluctuation score that indicates the likelihood that the audio sample contains music audio data, the spectral fluctuation score based on the differences between adjacent values in the average spectral envelope.
- Another aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving an audio sample that is associated with audio data, computing a spectrogram of the received audio sample, the spectrogram including one or more spectral slices, determining, for each of the spectral slices in the spectrogram, one or more peaks, determining, for each of the spectral slices in the spectrogram, a similarity between the spectral slice and the other spectral slices in the spectrogram based on the peaks, determining, for each time shift between slices in the spectrogram, a mean similarity value based on the similarity values associated with the time shift, generating a projection from the mean similarity values, smoothing the projection, determining a density of one or more local peaks in the smoothed projection, and determining a peak repetition score that indicates the likelihood that the audio sample contains music audio data based on the density of the one or more local peaks in the smoothed projection.
- detecting the one or more beats in the spectrogram may comprise determining one or more horizontal peaks in the spectrogram, generating a sparse representation of the spectrogram based on the horizontal peaks, and detecting the one or more beats in the sparse spectrogram.
- detecting the one or more sustained pitches in the spectrogram around the beats comprises determining one or more vertical peaks in the spectrogram, and detecting the one or more sustained pitches in the spectrogram around the beats based on the vertical peaks in the spectrogram around the beats.
- the method further comprises comparing the beat pitch score to a beat pitch threshold, and determining that the audio sample contains music audio data based on the beat pitch score being greater than the beat pitch threshold.
- determining, for each of the beats, the score based on the sustained pitches around the beat comprises determining, for each of the beats, one or more windows associated with the beat, each window centered on the beat, associated with a row in the spectrogram, and having the same predetermined width, determining, for each of the windows, a quantity of vertical peaks in the window, and determining, for each of the beats, a highest score associated with the beat, the highest score based on the highest quantity of vertical peaks in one of the windows associated with the beat, wherein the score for each of the beats comprises the highest score for the beat.
- the method further comprises correcting the spectral tilt of the spectrogram.
- computing the spectrogram comprises creating a plurality of frequency bins for the audio sample, and generating the spectrogram from the frequency bins.
- the method further comprises filtering one or more frequency bands from the spectrogram.
- the method further comprises normalizing one or more intensity values of each spectral slice of the spectrogram to create a normalized spectrogram, wherein determining the average spectral envelope of the spectrogram comprises determining the average spectral envelope of the normalized spectrogram.
- the spectral fluctuation score comprises the mean of the one or more differences.
- the mean of the one or more differences comprises the mean of the absolute values of the differences between adjacent values in the average spectral envelope.
- the method further comprises approximating a first derivative of the average spectral envelope in the frequency dimension, wherein determining the one or more differences between adjacent values in the average spectral envelope comprises determining the one or more differences between adjacent values in the average spectral envelope based on the first derivative of the average spectral envelope.
- the method comprises determining an average squared magnitude of the audio sample, and comparing the average squared magnitude of the audio sample to a threshold value, wherein computing the spectrogram is based on determining that the average squared magnitude of the audio sample is great than the threshold value.
- the method comprises generating a sparse representation of the spectrogram based on the peaks, wherein determining, for each of the spectral slices in the spectrogram, a similarity between the spectral slice and the other spectral slices in the spectrogram comprises determining a similarity between the spectral slice and the other spectral slices in the sparse spectrogram.
- the peaks comprise amplitude peaks.
- the method further comprises filtering one or more mean similarity values from the projection, wherein smoothing the projection comprises smoothing the filtered projection.
- the peak repetition score is based on a maximum of the filtered projection, a sample standard deviation of the filtered projection, and the density of the one or more local peaks in the smoothed projection.
- the one or more mean similarity values are filtered based on the time shifts associated with the one or more mean similarity values.
- a model used by the linear classifier to represent an audio sample may be small to conserve resources.
- the generation of the model might not be computationally expensive to conserver the resources of the device, e.g., battery power.
- classification of audio samples using the model might not be computationally expensive to conserver the resources of the device, e.g., batter power.
- FIG. 1 depicts a system for classifying audio data.
- FIGS. 2, 5, and 12 are flow diagrams of processes for determining whether an audio sample contains music audio data.
- FIGS. 3 and 13-16 are example spectrograms.
- FIGS. 4A-B depict examples of average spectral envelopes.
- FIG. 6 is an example heatmap.
- FIGS. 7-11B are example heatmap projections.
- FIGS. 17A-B are example beat pitch graphs.
- FIG. 18 is a block diagram of a computing system that can be used in connection with computer-implemented methods described in this document.
- FIG. 1 depicts a system 100 for classifying audio data.
- the system 100 can determine whether audio data includes music audio data or non-music audio data.
- the system 100 includes a mobile computing device 102 and a music detector engine 104 .
- the mobile computing device 102 is in communication with the music detector engine 104 over one or more networks.
- the mobile computing device 102 can include a microphone, a camera, or other detection mechanisms for detecting environmental data associated with a user 106 .
- the mobile computing device 102 includes at least a portion, or all, of the music detector engine 104 .
- the mobile computing device 102 implements the music detector engine 104 for classifying audio data.
- the user 106 is sitting at a coffee house that is playing music in the background.
- the user 106 would like to know more information about the music, e.g., the name of the song or the artist of the song.
- the mobile computing device 102 can facilitate determining information about the environmental audio data, e.g., the background music.
- the mobile computing device 102 detects the environmental audio data and processes the detected environmental audio data to generate waveform data 108 .
- the waveform data 108 represents the detected environmental audio data.
- the mobile computing device 102 can transmit the waveform data 108 to the music detector engine 104 , e.g., over a network, during operation (A).
- the waveform data 108 is streamed from the mobile computing device 102 to the music detector engine 104 .
- the environmental audio data can include an utterance that is generated by the user 106 .
- the utterance may include a question posed by the user such as “What is this song?”
- the waveform data 108 can represent the detected utterance and the environmental audio data.
- the mobile computing device 102 detects the environmental audio data after detecting the utterance; detects the environmental audio data concurrently with detecting the utterance; or both.
- the mobile computing device 102 can initiate detection of the environmental audio data in response to interaction by the user with the mobile computing device 102 . Specifically, the mobile computing device 102 can begin detection of the environmental audio data in response to launching an application on the mobile computing device 102 , e.g., through selection of a graphical representation of the application or selection of a physical button of the mobile computing device 102 that is associated with the application, e.g., “double-clicking” a “home” button.
- the mobile computing device 102 detects the environmental audio data, e.g., the background music, continuously. In some examples, the mobile computing device 102 may detect the environmental audio data in the background. For example, the mobile computing device 102 may detect the environment audio data when the user 106 is interacting with a mobile application of the mobile computing device 102 , e.g., the user 106 is checking e-mail. In another example, the mobile computing device 102 may detect the environmental audio data when the mobile computing device 102 is in a “locked” state, e.g., the mobile computing device 102 is powered-on but not being actively used.
- a “locked” state e.g., the mobile computing device 102 is powered-on but not being actively used.
- the mobile computing device 102 detects the environment audio data, e.g., the background music, in response to an instruction from the user 106 , e.g., user-directed interaction. For example, the mobile computing device 102 detects the environmental audio data in response to the user 106 providing instructions through a graphical user interface of the mobile computing device 102 , e.g., the user executing a mobile application of the mobile computing device 102 . Further, the mobile computing device 102 may detect the environmental audio data in response to the user 106 providing instructions that are included by the utterance, i.e., the utterance includes verbal instructions to detect the environmental audio data.
- the environment audio data e.g., the background music
- the mobile computing device 102 detects the environmental audio data in response to the user 106 providing instructions through a graphical user interface of the mobile computing device 102 , e.g., the user executing a mobile application of the mobile computing device 102 .
- the mobile computing device 102 may detect the environmental audio data in
- the mobile computing device 102 can continuously detect utterances, e.g., continuously detect utterances in the background. In some examples, the mobile computing device 102 can detect the utterances in response to the user 106 executing a mobile application for detecting utterances.
- the mobile computing device 102 detects the environmental audio data, e.g., the background music, in a continuous stream of audio samples with low latency. Specifically, the mobile computing device 102 detects the environmental audio data by detecting one or more audio samples of the environmental audio data over one or more time periods, e.g., a “window of length.” For example, the mobile computing device 102 detects a first audio sample of the environmental audio data over a time period t 1 and a second audio sample of the environmental audio data over a time period t 2 .
- the magnitude of the time periods t 1 and t 2 substantially differ, e.g., are of different time lengths. In some examples, the magnitude of the time periods t 1 and t 2 are substantially the same, e.g., are of the same time length.
- a portion, or all, of the second audio sample coincides with, e.g., overlaps, a portion, or all, of the first audio sample.
- a portion, or all, of the time period t 2 coincides with, e.g., overlaps, a portion, or all, of the time period t 1 .
- the second audio sample occurs substantially after the first audio sample.
- the time period t 2 occurs after the time period t 1 .
- the mobile computing device 102 detects multiple audio samples, e.g., four audio samples, each of differing time lengths, e.g., 0.5 seconds, 1.1 seconds, 2.5 seconds, and 5 seconds.
- the mobile computing device 102 provides multiple audio samples to the music detector engine 104 until the music detector engine 104 determines whether the audio samples contain music or non-music or until the music detector engine 104 determines information about music included in the audio samples, e.g., the name of the song in the audio samples, or the artist of the song. For example, the mobile computing device 102 provides a 0.5 second audio sample and a 1.1 second audio sample to the music detector engine 104 and, prior to sending a 2.5 second audio sample, the mobile computing device 102 receives music information from the music detector engine 104 and does not provide the 2.5 second audio sample, or another, e.g., longer, audio sample, to the music detector.
- the mobile computing device 102 provides multiple audio samples to the music detector engine 104 until the music detector engine 104 determines whether the audio samples contain music or non-music or until the music detector engine 104 determines information about music included in the audio samples, e.g., the name of the song in the audio samples, or the artist of the song.
- the mobile computing device 102 detects the environmental audio data, e.g., the background music, utilizing increasing window lengths, i.e., increasing time lengths.
- the increasing window lengths are exponentially increasing window lengths of the environmental audio data.
- the mobile computing device 102 detects multiple audio samples, e.g., four audio samples, each of increasing time lengths, e.g., 0.5 seconds, 1.1 seconds, 2.5 seconds, and 5 seconds.
- the mobile computing device 102 detects the environmental audio data using sliding time windows. That is, the mobile computing device 102 continuously detects audio samples having overlapping time periods. For example, new audio samples of the environmental audio data are detected as the time windows are shifted, e.g., sliding, and a new score, e.g., used to determine whether the environmental audio data includes music audio data or non-music audio data, as described further below, is determined for each audio sample associated with each new time window.
- the sliding window can be 1.1 seconds.
- the music detector engine 104 receives the waveform data 108 from the mobile computing device 102 .
- the music detector engine 104 processes the waveform data 108 , during operation (B). For example, the music detector engine 104 identifies a state of the environmental audio data, i.e., whether the environmental audio data includes music audio data or non-music audio data.
- non-music audio data can include speech, talking, noise, or other ambient non-music noises.
- the music detector engine 104 includes a spectral fluctuation detector 110 , a peak repetition detector 112 , a beat pitch detector 114 , a linear classifier 116 , and/or one or more additional detectors 120 .
- the spectral fluctuation detector 110 , the peak repetition detector 112 , the beat pitch detector 114 , or all three identify the state of the environmental audio data, e.g., whether the environmental audio data includes music audio data or non-music audio data.
- the additional detectors 120 may be used to determine whether the environmental audio data includes music audio data or non-music audio data.
- a frame entropy detector may be used to determine the spectral slice entropy over a time window, as discussed in more detail below.
- the music detector engine 104 may include an energy entropy detector, a spectral centroid detector, and a zero crossing rate detector, to name a few examples of the additional detectors 120 .
- the linear classifier 116 receives input from the detectors, including the spectral fluctuation detector 110 , the peak repetition detector 112 , and/or the beat pitch detector 114 , and determines the state of the environmental audio data.
- the linear classifier 116 may also receive input from one or more music information detectors that the linear classifier 116 uses to determine information about music included in the environmental audio data, e.g., a song name or an artist name.
- the music detector engine 104 compares a time length of the audio sample with a threshold duration. Based upon the comparison, the music detector engine 104 employs the spectral fluctuation detector 110 , the peak repetition detector 112 , the beat pitch detector 114 , another detector, or a combination of detectors, to identify the state of the environment audio data, e.g., whether the environmental audio includes music audio data or non-music audio data.
- the music detector engine 104 may employ the spectral fluctuation detector 110 to identify whether the environmental audio data includes music audio data or non-music audio data.
- the spectral fluctuation detector 110 can be used to determine whether the environmental audio data includes music audio data or non-music audio data when the time length of the audio sample is short, e.g., 0.5 seconds-1.5 seconds.
- the music detector engine 104 employs the peak repetition detector 112 to identify whether the environmental audio data includes music audio data or non-music audio data.
- the peak repetition detector 112 can be used to determine whether the environmental audio data includes music audio data or non-music audio data when the time length of the audio sample is longer, e.g., 1.5 seconds or 2.5 seconds.
- the music detector engine 104 may employ the spectral fluctuation detector 110 , the peak repetition detector 112 , and the beat pitch detector 114 .
- a combination of the spectral fluctuation detector 110 , the peak repetition detector 112 , the beat pitch detector 114 , and/or the additional detectors 120 are used together, e.g., to increase an accuracy of identification of the state of the environmental audio data.
- the music detector engine 104 weights a score output by each of the spectral fluctuation detector 110 , the peak repetition detector 112 , and the beat pitch detector 114 , in addition to any other music detectors, where each of the scores relates to the likelihood that the audio sample is associated with music audio data.
- the weighting can be based on a length of time of the audio sample.
- each of the scores output by the music detectors can be used to determine whether the environmental audio data includes music audio data or non-music audio data.
- the music detector engine 104 , the linear classifier 116 , or both can compare the score to a threshold score. In some examples, when the score is greater than or equal to the threshold score, the audio sample, and the corresponding environmental audio data, can be identified as containing music audio data. In some examples, when the score is not greater than the threshold score, the audio sample, and the corresponding environmental audio data, can be identified as containing non-music audio data.
- the audio sample, and the corresponding environmental audio data can be identified as not containing recognizable music audio data, e.g., when the intensity or quality of any music audio data included in the audio sample is too low.
- one or more of the music detectors computes a score for two or more audio samples of differing time lengths.
- the audio samples can have time lengths of 2.5 seconds and 5 seconds respectively.
- the maximum score of the scores for two or more audio samples can be determined. Thus, the maximum score can be used to determine whether the environmental audio data includes music audio data or non-music audio data.
- the music detector engine 104 provides identification information 118 , related to a state of the environmental audio and based on the analyzed audio samples of the environmental audio data, e.g., a determination of whether the environmental audio data includes music audio data or non-music audio data, to the mobile computing device 102 , e.g., over a network, at operation (C). Specifically, the music detector engine 104 , based on a state determination provided by the linear classifier 116 , provides the state identification information 118 of the environmental audio data to the mobile computing device 102 . For example, the music detector engine 104 determines that the environmental audio data, e.g., the waveform data 108 , corresponds to music audio data. In some examples, the music detector engine 104 can provide such identification information 118 thru an application executed by the mobile computing device 102 , or other communication mechanisms such as a text message or an e-mail.
- the identification information 118 when the identification information 118 includes a determination that the environmental audio data includes music audio data, the identification information 118 can include an option to “recognize” the music audio data, e.g., through an application executed by the mobile computing device, and/or an option to “purchase” the song or album associated with the music audio data. In some examples, the identification information 118 can include recognition information associated with the music audio data, e.g., the song name or the artist name.
- the identification information 118 can be displayed on the GUI of the mobile computing device 102 in response to user interaction.
- the user 106 can launch an application of the mobile computing device 102 , and in response, the application can provide the identification information 118 .
- the identification information 118 can be displayed on the GUI of the mobile computing device 102 automatically, i.e., without input from the user 106 .
- the GUI can include, upon transitioning from the “locked” state to an “active state,” the identification information 118 .
- the GUI can include, in the “locked” state, the identification information 118 , e.g., the “lock” screen of the GUI can include the identification information 118 .
- FIG. 2 is a flow diagram of a process 200 for determining whether an audio sample contains music audio data by measuring the consistency of fluctuations in spectral slices across a time window associated with the time sample.
- the process 200 can be used by the spectral fluctuation detector 110 from the system 100 .
- the spectral fluctuation detector 110 receives an audio sample, e.g., of the environmental audio data, and detects sustained pitches in the audio sample over a period time, where sustained pitches in audio sample are more likely to occur in music audio data as compared to an audio sample including human speech or other types of non-musical noise.
- the spectral fluctuation detector 110 receives an audio sample that is associated with audio data ( 202 ). For example, the spectral fluctuation detector 110 receives t seconds of the audio data as the audio sample. In some examples, the audio sample has a length of 1.5 seconds to 10 seconds; however, in some examples, the spectral fluctuation detector 110 can determine whether the audio sample relates to music audio data with audio samples as short as 0.5 seconds.
- the spectral fluctuation detector 110 determines an average squared magnitude of the audio sample ( 204 ).
- the average squared magnitude of the audio sample represents the intensity of the audio sample and the spectral fluctuation detector 110 uses the average squared magnitude to determine whether the data in the audio sample can be further processed. For example, when the intensity is low, the spectral fluctuation detector 110 determines that the audio sample contains mostly silence and should not be processed further.
- the spectral fluctuation detector 110 compares the average squared magnitude of the audio sample to a threshold value ( 206 ). For example, the spectral fluctuation detector determines whether the average squared magnitude of the audio sample is greater than the threshold value.
- the spectral fluctuation detector 110 determines that the received audio sample is associated with low intensity audio sound ( 208 ). For example, the spectral fluctuation detector 110 determines that the audio sample contains silence, or sounds with very low intensity. In response, the spectral fluctuation detector 110 ceases processing of the received audio sample, e.g., identifying the state of the environmental audio data.
- the spectral fluctuation detector 110 computes a spectrogram of the received audio sample ( 210 ). For example, the spectral fluctuation detector 110 computes a time-varying spectral representation of the received audio sample, such as a spectrogram 300 shown in FIG. 3 .
- a larger number of horizontal lines in the spectrogram 300 indicates that the audio sample more likely includes music audio data than if the spectrogram 300 included fewer horizontal lines.
- the horizontal lines represent sustained tones where music audio data includes more sustained tones than non-music audio data.
- the spectral fluctuation detector 110 computes the spectrogram of the received audio sample utilizing a fast Fourier transform (FFT).
- FFT fast Fourier transform
- an associated window length of the spectrogram is 64 milliseconds and a time step of the spectrogram is 16 milliseconds.
- the spectral fluctuation detector 110 computes a chromogram. In some examples, the spectral fluctuation detector 110 utilizes autocorrelation in the processing of the received audio sample.
- the spectral fluctuation detector 110 corrects spectral tilt of the spectrogram ( 212 ). For example, the spectral fluctuation detector 110 adjusts the magnitude values in the spectrogram 300 to correct the spectral tilt and avoid overweighted lower frequencies of the spectrogram by multiplying the magnitude values associated with higher frequencies in the spectrogram 300 by a high constant value and the magnitude values associated with lower frequencies in the spectrogram 300 by a low constant value, where the magnitude values associated with the middle frequencies are multiplied by intermediate constant values. Specifically, the spectral fluctuation detector 110 may multiply each row of the spectrogram with a square root of the zero-based row index, e.g., the square root of the frequency value. As a result, the spectral tilt and the overweighted lower frequencies of the computer spectrogram are corrected, e.g., providing a corrected spectrogram.
- the spectral fluctuation detector 110 filters one or more frequency bands of the spectrogram associated with human speech ( 214 ). For example, the spectral fluctuation detector 110 filters one or more frequency bands of the corrected spectrogram. In some examples, the spectral fluctuation detector 110 can filter, e.g., remove, frequency bands that are associated with human speech, e.g., mid-range frequencies. For example, the spectral fluctuation detector 110 can filter, e.g., remove, frequency bands in the range of 1400 Hz to 2600 Hz, creating a filtered spectrogram that corresponds to the frequency bands below 1400 Hz and above 2600 Hz.
- the spectral fluctuation detector 110 filters one or more frequency bands that are associated with poor audio quality ( 216 ).
- the mobile computing device 102 may be only able to effectively capture environmental audio data above a certain frequency threshold and below a certain frequency threshold, e.g., based on physical limitations of a microphone associated with the mobile computing device 102 . Therefore, the spectral fluctuation detector 110 filters, e.g., removes, frequency bands below 120 Hz and above 4300 Hz.
- the spectral fluctuation detector 110 may effectively maintain spectrogram rows in the filtered spectrogram that correspond to frequency bands in the ranges of 120 Hz to 1400 Hz and 2600 Hz to 4300 Hz.
- the spectral fluctuation detector 110 normalizes one or more intensity values of each spectral slice of the spectrogram ( 218 ). For example, the spectral fluctuation detector 110 normalizes the intensity values of each spectral slice of the filtered spectrogram to compensate for high and low volume in the captured environmental audio data represented by the intensity values in the spectrogram. Specifically, the spectral fluctuation detector 110 normalizes each slice of the spectrogram by dividing the intensity values associated with each spectral slice by the harmonic mean of the intensity values of the spectral slices of a selected portion of the spectrogram.
- the selected portions of the spectrogram can include slices of the spectrogram having intensity values in a top percentile of the intensity values of the spectral slices of the spectrogram, e.g., the top 5% percentile.
- the spectral fluctuation detector 110 determines the intensity values that have the greatest magnitude in a specific spectral slice and uses the intensity values with the greatest magnitude to normalize the intensity values in the specific spectral slice. Further, the spectral fluctuation detector 110 may determine the harmonic mean of the intensity values with the greatest magnitude and use the harmonic mean value to normalize the intensity values in the specific spectral slice. The spectral fluctuation detector 110 may use alternate methods or values to normalize the intensity values in a specific spectral slice.
- normalizing the intensity values of each spectral slice of the spectrogram based on a top percentile of the intensity values of the spectral slices of the spectrogram filters, e.g., removes, outlying values of spectral slices of the spectrogram, e.g., “outliers.”
- the spectral fluctuation detector 110 normalizes each spectral slice of the spectrogram independently for the lower and the upper spectrogram range, e.g., for the range 120 Hz to 1400 Hz and for the range 2600 Hz to 4300 Hz.
- the spectral fluctuation detector 110 determines an average spectral envelope of the spectrogram ( 220 ). For example, the spectral fluctuation detector 110 determines a curve in the frequency-amplitude plane of the spectrogram. The spectral fluctuation detector 110 may determine the average spectral envelope for each row of the normalized spectrogram and create an average spectral envelope slice or graph from the average spectral envelope for each row. For example, the spectral fluctuation detector 110 averages all of the values for a particular frequency in the normalized spectrogram and uses the average value as the value for the particular frequency in the average spectral envelope graph. The spectral fluctuation detector 110 may then repeat the process for all of the frequencies in the normalized spectrogram, or for a subset of discrete frequencies in the normalized spectrogram.
- FIGS. 4A-B depict examples of average spectral envelopes.
- the spectral fluctuation detector 110 may generate a non-music average spectral envelope 400 a , shown in FIG. 4A , for a non-music audio sample.
- the spectral fluctuation detector 110 may generate a music average spectral envelope 400 b , shown in FIG. 4B , for a music audio sample.
- the average spectral envelopes 400 a - b are examples based on specific audio samples, and the average spectral envelopes for different audio samples would have a similar appearance to one of the average spectral envelopes 400 a - b , depending on whether the different audio samples include music audio data or non-music audio data.
- the spectral fluctuation detector 110 computes the average spectral envelope independently for a lower range and an upper range of the spectrogram 300 .
- the spectral fluctuation detector 110 may determine the average spectral envelope for the frequency bands in the range of 120 Hz to 1400 Hz and then determine the average spectral envelope for the frequency bands in the range of 2600 Hz to 4300 Hz.
- the spectral fluctuation detector 110 determines one or more differences between adjacent values in the average spectral envelope ( 222 ). For example, the spectral fluctuation detector 110 approximates a first derivate of the average spectral envelope in the frequency dimension. In some examples, the spectral fluctuation detector 110 computes the difference between the adjacent values of the average spectral envelope independently for a lower range, e.g., 120 Hz to 1400 Hz, and an upper range of the spectrogram, e.g., 2600 Hz to 4300 Hz.
- the spectral fluctuation detector 110 determines a spectral fluctuation score for the audio sample ( 224 ).
- the spectral fluctuation score is the mean difference based on the differences between adjacent values in the average spectral envelope.
- the spectral fluctuation detector 110 determines the mean absolute difference of the first derivate of the average spectral envelope to determine the flatness of a vector represented by the average spectral envelope.
- the spectral fluctuation detector may determine the mean difference based on the absolute values of the differences between adjacent values in the average spectral envelope.
- the spectral fluctuation detector 110 may use any appropriate algorithm to determine the flatness of the vector represented by the average spectral envelope and use the determined value as a spectral fluctuation score for the audio sample.
- the spectral fluctuation score is a measure of the consistency of fluctuations in spectral slices of the spectrogram in the input time window.
- music audio data is more likely than non-music audio data, e.g., human speech, to comprise sustained pitches and associated harmonics that are repeated in a short time period, e.g., 0.5 seconds-10 seconds.
- the spectral characteristics of music audio data e.g., pitches and harmonics, can accumulate and become emphasized in the average spectral envelope of the spectrogram.
- the average spectral envelope associated with music audio data can be less uniform as compared to an average spectral envelope associated with non-music audio data, e.g., the curve in the frequency-amplitude plane is less uniform for music audio data than non-music audio data.
- the spectral fluctuation score can be used in determining whether the environmental audio data includes music audio data or non-music audio data.
- the music detector engine 104 the spectral fluctuation detector 110 , or both, can compare the spectral fluctuation score to a threshold spectral fluctuation score value.
- the audio sample, and the corresponding environmental audio data can be identified as containing music audio data.
- the audio sample, and the corresponding environmental audio data can be identified as containing non-music audio data.
- the individual spectral slices can “cancel” each other out.
- the flatness of the average spectral envelope is related to the spectral fluctuation score that is indicative of the likelihood that the audio sample is a music audio sample. For example, the flatter the average spectral envelope, the lower the score, and thus the lower the likelihood that the audio sample is a music audio sample.
- the spectral fluctuation detector 110 may compute the spectral fluctuation score for two or more audio samples, e.g. of the received audio data. For example, the spectral fluctuation detector 110 computes two spectral fluctuation scores for audio samples of length 0.5 seconds and 1.1 seconds. In these implementations, the separate spectral fluctuation scores may be combined or the greater spectral fluctuation score value may be used. For example, the spectral fluctuation detector 110 may combine the two spectral fluctuation score values using weights, e.g., based on the duration of the audio sample. Alternatively, the spectral fluctuation detector 110 may determine which spectral fluctuation score has the greater value, and use the greater spectral fluctuation score as the spectral fluctuation score for the received audio sample.
- the output of the spectral fluctuation detector 110 can be combined, e.g., by linear classifier 116 , with a mean absolute difference of the lower spectrogram range, a mean absolute difference of the upper spectrogram range, and a sample standard deviation of all differences.
- the spectral fluctuation detector 110 can combine a spectral fluctuation score for the lower frequency range in the spectrogram with a spectral fluctuation score for the higher frequency range in the spectrogram.
- the output of the spectral fluctuation detector 110 is used to classify an audio sample.
- the linear classifier 116 or the music detector engine 104 may use the spectral fluctuation score to determine information about music included in the environmental audio data, e.g., a song name or an artist name associated with the music.
- the linear classifier 116 or the music detector engine 104 may user other scores or data in addition to the spectral fluctuation score when classifying the environmental audio data.
- the spectral fluctuation detector may filter one or more frequency bands that are associated with poor audio quality prior to filtering one or more frequency bands associated with human speech.
- the process 200 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps.
- the spectral fluctuation detector 110 might perform steps 202 and 210 through 224 without performing steps 204 through 208 , i.e., determining whether the audio sample is associated with low intensity audio sound.
- the spectral fluctuation detector 110 does not filter the spectrogram.
- the spectral fluctuation detector 110 may perform the process 200 without performing step 214 and/or without performing step 216 .
- the spectral fluctuation detector 110 bins the frequencies in the received audio sample when creating the spectrogram 300 shown in FIG. 3 .
- the spectral fluctuation detector 110 creates a plurality of frequency bins where each frequency bin is approximately 10 Hz, such that the frequency values in the spectrogram 300 represent the frequency bins.
- the spectral fluctuation detector 110 creates the frequency bins as part of the fast Fourier Transform process.
- FIG. 5 is a flow diagram of a process 500 for determining whether an audio sample contains music audio data by measuring harmonic repetitions.
- the process 500 can be used by the peak repetition detector 112 from the system 100 .
- the peak repetition detector 112 may determine whether the audio data includes music audio data or non-music audio data by computing a similarity of harmonics of the audio sample.
- repetition of harmonics can represent a similarity of musical notes that are unlikely to occur in background noise associated with non-music audio data.
- the peak repetition detector 112 receives an audio sample that is associated with audio data ( 502 ). For example, the peak repetition detector 112 receives t seconds of the audio data as the audio sample. In some examples, the audio sample is typically of length 1.5 seconds to 10 seconds. In some examples, the audio sample is at least 2.5 seconds long.
- the peak repetition detector 112 receives two or more audio samples with a sliding time window. That is, the peak repetition detector 112 continuously receives audio samples having overlapping time periods.
- the sliding window can be 1.1 seconds.
- the peak repetition detector 112 computes a spectrogram of the received audio sample ( 504 ). For example, the peak repetition detector 112 determines a time-varying spectral representation of the received audio sample, such as the spectrogram 300 shown in FIG. 3 .
- the peak repetition detector 112 computes the spectrogram of the received audio sample utilizing a fast Fourier transform (FFT).
- FFT fast Fourier transform
- an associated window length of the spectrogram is 64 milliseconds and a time step of the spectrogram is 16 milliseconds.
- the peak repetition detector 112 computes a chromogram. In some examples, the peak repetition detector 112 utilizes autocorrelation in processing of the received audio sample.
- the peak repetition detector 112 corrects spectral tilt of the spectrogram ( 506 ). For example, the peak repetition detector 112 corrects the spectral as described above with reference to step 212 . Specifically, the peak repetition detector 112 may multiply each row of the spectrogram with the zero-based row index, e.g., the square root of the frequency value. As a result, the spectral tilt of the spectrogram is corrected providing a corrected spectrogram.
- the peak repetition detector 112 determines one or more peaks for each spectral slice of the spectrogram ( 508 ). For example, the peak repetition detector 112 identifies the peaks of each spectral slice of the corrected spectrogram.
- the peaks of each spectral slice of the spectrogram are peaks of the amplitude of a particular frequency at a particular time for each respective spectral slice.
- the peaks of each spectral slice of the spectrogram 300 may be represented by the color of a point in the spectrogram, i.e., defined by x and y coordinates.
- a peak is a particular point where all the points that surround the particular point are a different color, e.g., a darker color, than the color of the particular point.
- the peaks have a particular value in a single spectrogram column, e.g. slice, that is larger than all neighbor points in the same spectrogram column.
- a peak associated with a frequency or a frequency bin is larger than the vertical neighbors of the peak that are associated with different frequencies or frequency bins and that are in the same slice represented by a predetermined window.
- the window is larger for higher frequencies than lower frequencies, providing a higher density of peaks in lower frequencies.
- the peak repetition detector 112 compares each point in the spectrogram with the neighbors of the point that belong in the same column of the spectrogram. For example, the peak repetition detector 112 compares a particular point associated with a high frequency with ten neighbor values, e.g., five neighbors above and five neighbors below the particular point, to determine whether the particular point is a peak, i.e., if the amplitude of the particular point is greater than the ten neighbor values.
- the peak repetition detector 112 may compare a particular low frequency amplitude value with the amplitude values of four neighbor points, e.g., two neighbors above and two neighbors below, to determine whether the particular point is a peak, i.e., if the amplitude of the particular point is greater than the amplitudes of the four neighbor points.
- the peak repetition detector 112 determines whether a particular point near the edge of the spectrogram is a peak, the peak repetition detector 112 compares the amplitude of the particular point with less than a predetermine quantity of neighbor amplitude values. For example, when the particular point is a high frequency point on the edge of the spectrogram, the peak repetition detector 112 compares the amplitude of the particular point with the amplitudes of the next five lower frequency points and does not compare the amplitude of the particular point with the amplitudes of the next five higher frequency points.
- the amplitude of the particular point is compared with six other amplitudes to determine whether the particular point is a peak, e.g., the amplitude of the other point and the amplitudes of the next five lower frequency points.
- the peak repetition detector 112 generates a sparse representation of the spectrogram ( 510 ). For example, based on identifying the peaks of each spectral slice of the spectrogram, the peak repetition detector 112 adjusts the values associated with each of the peaks. In some implementations, the peak repetition detector 112 adjusts the amplitude values associated with each of the peaks to a value of one. In some implementations, the peak repetition detector 112 adjusts the values of the other points in the spectrogram, e.g., the non-peak amplitudes of the spectral slices, to a value of zero.
- the peak repetition detector 112 generates a binary image of the spectrogram where the peaks are represented by values of one and the non-peaks are represented by values of zero by comparing each point in the spectrogram with the neighbors of the point that belong in the same column of the spectrogram.
- the peak repetition detector 112 filters one or more frequency bands of the spectrogram ( 512 ).
- the peak repetition detector 112 can filter, e.g., remove, frequency bands from the sparse representation of the spectrogram that are associated with poor audio quality.
- the mobile computing device 102 may be only able to effectively capture environmental audio data above a certain frequency, e.g., based on physical limitations of a microphone associated with the mobile computing device 102 . Therefore, in these implementations, the peak repetition detector 112 may filter spectrogram rows that correspond to frequency bands below 150 Hz. The peak repetition detector 112 may filter additional or alternative frequency bands from the spectrogram. For example, the peak repetition detector 112 may filter frequency bands above 4300 Hz. The filtering may be performed based on the performance of the microphone associated with the mobile computing device 102 and/or the requirements of the peak repetition detector 112 , to name a few examples.
- the peak repetition detector 112 determines a similarity between the slices of the sparse representation of the spectrogram ( 514 ). For example, the peak repetition detector 112 may generate a heatmap 600 , shown in FIG. 6 , of the sparse spectrogram that represents the similarity between the slices of the sparse spectrogram.
- the peak repetition detector 112 generates the heatmap 600 by comparing each column, i.e., spectral slice, in the sparse spectrogram to the other columns, i.e., spectral slices, in the sparse spectrogram, e.g., autocorrelation, where the axes in the heatmap represent the column numbers of the sparse spectrogram, e.g., frequency or frequency bin values, and the values represented by the particular points in the heatmap represent the similarity between the two columns associated with the particular point.
- a center diagonal 602 of the heatmap 600 represents the similarity between each column and itself.
- each of the values along the diagonal is a maximum similarity value.
- all of the values along the other diagonals of the heatmap represent the same time shift, i.e., the same time difference between two time slices in the sparse spectrogram.
- each slice of the sparse spectrogram represents the frequency and amplitude of the audio sample at a particular point in time.
- the values in the center diagonal 602 of the heatmap 600 represent the similarity between each slice and itself, where the time shift is zero, i.e., there is no time difference between a slice and itself.
- the values in the diagonal above the center diagonal represent a time shift of one unit of time, e.g., one window, as this diagonal contains the similarity values for each pair of adjacent columns, and so on for the other diagonals in the heatmap 600 .
- the peak repetition detector 112 compares the sparse spectrogram to itself utilizing sample correlations of the sparse spectral slices as a similarity function.
- the peak repetition detector 112 may use any appropriate algorithm to determine the correlation or the similarity between the slices in the spectrogram.
- the peak repetition detector 112 determines a mean similarity value ( 516 ).
- the mean similarity value for a specific time shift is based on all of the similarity values associated with the specific time shift.
- the peak repetition detector 112 generates a projection from the mean similarity values ( 518 ). For example, the peak repetition detector 112 averages the similarity values in each of the diagonals of the heatmap, where a greater mean value represents a greater similarity between the corresponding spectral slices of the spectrogram at a corresponding time shift than a smaller mean value.
- the peak repetition detector 112 may generate a heatmap projection 700 , shown in FIG. 7 , from the mean similarity values for each of the time shifts.
- the x axis represents the different time shifts and the y axis represents the average of the similarity values that correspond to a specific time shift, i.e., the mean similarity value.
- the peak repetition detector 112 identifies all of the heatmap values that correspond to a time shift of five and uses the average of these heatmap values for the mean similarity of a time shift of five in the heatmap projection 700 .
- the mean similarity for a time shift of five is about 0.15.
- the peak repetition detector 112 filters mean similarity values associated with one or more time shifts from the projection ( 520 ).
- the peak repetition detector 112 may remove mean similarity values that are associated with time shifts below a first threshold, above a second threshold, or both, from the heatmap projection 700 to create a filtered heatmap projection.
- the peak repetition detector 112 removes mean similarity values that correspond to time lags below 200 milliseconds and above 3200 milliseconds from the heatmap projection.
- the peak repetition detector 112 effectively maintains heatmap projections that correspond to time shifts between 200 milliseconds and 3200 milliseconds inclusive.
- the first and the second thresholds are based on a time length of the received audio sample.
- FIG. 8 One example of a filtered heatmap projection 800 generated by the peak repetition detector 112 is shown in FIG. 8 .
- the peak repetition detector 112 smooths the filtered projection ( 522 ).
- the peak repetition detector 112 may smooth the filtered heatmap projection with a Gaussian filter mask.
- the peak repetition detector 112 may use other appropriate algorithms to smooth the filtered heatmap projection 800 and create a smoothed heatmap projection.
- One example of a smoothed heatmap projection 900 generated by the peak repetition detector 112 is shown in FIG. 9 .
- the peak repetition detector 112 determines a density of the local peaks in the smoothed projection ( 524 ). For example, the peak repetition detector 112 may compute the density of the local peaks in the smoothed heatmap projection 900 based on the number of local peaks 1002 a - g , shown in FIG. 10 , and the length of the smoothed heatmap projection 900 . For example, the density of the local peaks is determined as the number of local peaks in the smoothed heatmap projection 900 divided by the length of the smoothed heatmap projection 900 .
- the peak repetition detector 112 determines a peak repetition score for the audio sample ( 526 ).
- the peak repetition score is a measure of the presence of consistent salient repetitions in the audio sample that are characteristic of music audio data.
- the peak repetition score is based on a maximum of the filtered heatmap projection, e.g., the maximum value from the filtered heatmap projection, a sample standard deviation of the filtered heatmap projection, i.e., an estimate of the standard deviation of the unfiltered heatmap projection based on the filtered heatmap projection, and the density of the local peaks in the smoothed heatmap projection.
- the peak repetition score may be determined by the equation (A ⁇ B)/C, where A is the maximum of the filtered heatmap projection, B is the sample standard deviation of the filtered heatmap projection, and C is the density of the local peaks in the smoothed heatmap projection.
- A is the maximum of the filtered heatmap projection
- B is the sample standard deviation of the filtered heatmap projection
- C is the density of the local peaks in the smoothed heatmap projection.
- the peak repetition score can be based on other equations.
- the peak repetition detector 112 is more robust to noise, e.g., non-music audio data, and small inessential changes in music audio data as a result of utilizing the sparse spectrogram.
- FIGS. 11A-B depict a non-music heatmap projection 1100 a and a music heatmap projection 1100 b of mean similarity values for non-music audio data and music audio data respectively.
- the mean similarity values associated with the music heatmap projection 1100 b are more similar than the mean similarity values associated with the non-music heatmap projection 1100 a that includes both positive and negative mean similarity values.
- the peak repetition detector 112 may filter time shifts that are below or above a predetermined threshold, e.g., perform step 520 , from the heatmap prior to determining the mean similarity values, e.g., perform step 516 .
- the process 500 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps.
- the music detector engine 104 may perform either the steps 202 and 210 through 212 or the steps 502 through 506 , i.e., the music detector only needs to receive an audio sample, compute a spectrogram, and correct for spectral tilt once.
- the process 500 includes steps 204 through 208 .
- the process 500 can determine whether a received audio sample has a low intensity and does not need further processing based on an average squared magnitude of the audio sample.
- the steps 508 and 510 are performed together.
- the peak repetition detector 112 adjusts the amplitude of the peak to a predetermined value, e.g., one.
- the peak repetition detector 112 adjusts all of the other amplitudes in the spectrogram to different predetermined value, e.g., zero.
- the peak repetition detector 112 adjusts the amplitude of the peak to a first predetermined value and the amplitudes of the neighboring points to a second predetermined value.
- the peak repetition detector 112 generates a triangular heatmap, e.g., the upper right half of the heatmap 600 .
- the peak repetition detector 112 only generates the similarity values for the slices in the spectrogram once, i.e., in step 514 , and uses these similarity values to generate the mean similarity values for each of the time shift values, i.e., in step 516 .
- the peak repetition detector 112 might not compare each slice to itself and generate the center diagonal 602 .
- the peak repetition detector 112 averages the similarity values for time shifts that are greater than a minimum threshold value and/or less than a maximum threshold value and does not discard mean similarity values associated with one or more time shifts. For example, as part of step 516 , the peak repetition detector 112 determines mean similarity values for a subset of the time shifts represented in the heatmap and does not perform step 520 .
- the peak repetition detector may discard, e.g., not compute a mean value for, all the similarity values for a particular time shift, e.g., diagonal in the heatmap 600 , when audio repetitions for the particular time shift, represented by higher similarity values associated with the particular time shift, typically do not correlate with the audio sample containing music audio data.
- the peak repetition detector 112 may discard or otherwise ignore similarity values associated with short time shifts and/or long time shifts.
- the threshold value may be based on a time length of the received audio sample.
- FIG. 12 is a flow diagram of a process 1200 for determining whether an audio sample contains music audio data by finding beats in the audio sample and sustained pitches around each beat.
- the process 1200 can be used by the beat pitch detector 114 from the system 100 .
- the beat pitch detector 114 receives an audio sample, e.g., of the environmental audio data, and detects beats and sustained pitches around each beat in the audio sample, where sustained pitches around the beats are more likely to occur in music audio data as compared to an audio sample that does not include music audio data.
- the beat pitch detector 114 receives an audio sample that is associated with audio data ( 1202 ). For example, the beat pitch detector 114 receives t seconds of the audio data as the audio sample. In some examples, the audio sample has a length of between about 0.5 seconds to about 10 seconds. For example, the audio sample may have a length of about 1.1 seconds or about 2.5 seconds.
- the beat pitch detector 114 receives two or more audio samples with a sliding time window.
- the beat pitch detector 114 continuously receives audio samples having overlapping time periods.
- the sliding window can be 2.5 seconds.
- the beat pitch detector 114 computes a spectrogram of the received audio sample ( 1204 ). For example, the beat pitch detector 114 computes a time-varying spectral representation of the received audio sample, such as a spectrogram 1300 shown in FIG. 13 .
- the beat pitch detector 114 computes the spectrogram of the received audio sample utilizing a fast Fourier transform (FFT).
- FFT fast Fourier transform
- an associated window length of the spectrogram is 128 milliseconds.
- the windows from the received audio sample do not overlap.
- the beat pitch detector 114 computes a chromogram. In some examples, the beat pitch detector 114 utilizes autocorrelation in processing the received audio sample.
- the beat pitch detector 114 determines one or more horizontal peaks in the spectrogram ( 1206 ). For example, the beat pitch detector 114 determines the horizontal peaks in the spectrogram by comparing each point in the spectrogram with the point's neighbors to the left and to the right of the point.
- the beat pitch detector 114 compares each point in the spectrogram 1300 with the point's neighbors that are associated with the same frequency or frequency bin. For example, the beat pitch detector 114 compares a first point with the two points to the left of the first point and the two points to the right of the first point where all of the points are associated with the same frequency bin but represent different instances in time to determine whether the first point is a peak.
- the beat pitch detector 114 generates a sparse representation of the spectrogram ( 1208 ). For example, based on identifying the horizontal peaks in the spectrogram, the beat pitch detector 114 adjusts the values of each of the peaks. In some implementations, the beat pitch detector 114 adjusts the amplitude values associated with each of the peaks to a value of one. In some implementations, the beat pitch detector 114 adjusts the values of the other points of the spectrogram, e.g., the non-peak amplitudes, to a value of zero. For example, the beat pitch detector 114 generates a horizontal peak spectrogram 1400 , shown in FIG. 14 , where the horizontal peaks are represented by values of one, e.g., white, and the non-peaks are represented by values of zero, e.g., black.
- the horizontal peak spectrogram 1400 represents a binary spectrogram of a music audio sample.
- a horizontal peak spectrogram associated with non-music audio data may include a more uniform distribution of horizontal peaks than a horizontal peak spectrogram associated with music audio data.
- the beat pitch detector 114 filters one or more frequency bands of the spectrogram ( 1210 ).
- the beat pitch detector 114 may filter one or more frequencies of the horizontal peak spectrogram 1400 .
- the beat pitch detector 114 removes frequencies below 150 Hz and frequencies above 4000 Hz from the spectrogram.
- the beat pitch detector 114 removes frequencies between about 850 Hz and about 2650 Hz from the spectrogram.
- the beat pitch detector 114 creates an updated spectrogram with frequency ranges between about 150 Hz to about 850 Hz and between about 2650 Hz to about 4000 Hz.
- the beat pitch detector 114 detects one or more beats in the spectrogram ( 1212 ). For example, for each time slice in the horizontal peak spectrogram 1400 , the beat pitch detector 114 determines the number of horizontal peaks in the time slice. If at least a predetermined percentage of the frequency bins for the time slice include horizontal peaks, then the beat pitch detector 114 classifies the time slice as including a beat. For example, the beat pitch detector generates a beat spectrogram 1500 , shown in FIG. 15 based on the beats detected in the horizontal peak spectrogram 1400 .
- the predetermined percentage is based on the type of music included in the audio sample. In certain implementations, the predetermined percentage is based on the types of instruments that generated the audio sample. In some examples, the predetermined percentage is 33%. For example, when the beat pitch detector 114 determines that at least 1 ⁇ 3 of the frequency bins for a particular time slice include horizontal peaks, then the beat pitch detector 114 classifies that particular time slice as including a beat. In other examples, the predetermined percentage is 50%.
- the beat pitch detector 114 determines one or more vertical peaks in the spectrogram ( 1214 ). For example, the beat pitch detector 114 determines the vertical peaks in the in the spectrogram 1300 , shown in FIG. 13 , by comparing each point in the spectrogram with the point's neighbors above and below the point.
- the beat pitch detector 114 compares each point in the spectrogram 1300 with the point's neighbors that are associated with the same time slice. For example, the beat pitch detector 114 compares a first point with the five points above the first point and the five points below the first point where all of the points are associated with the same time slice to determine whether the first point is a vertical peak.
- the beat pitch detector 114 generates a vertical peak spectrogram 1600 , shown in FIG. 16 , from the spectrogram 1300 based on the determined vertical peaks.
- the vertical peak spectrogram 1600 represents the vertical peaks with a value of one, e.g., white, and the non-peaks with a value of zero, e.g., black.
- the beat pitch detector 114 detects one or more sustained pitches around the beats ( 1216 ). For example, the beat pitch detector 114 uses the vertical peaks in the vertical peak spectrogram 1600 to determine the sustained pitches around the beats in the beat spectrogram 1500 .
- a beat pitch graph 1700 a - b shown in FIGS. 17A-B , represents a combination of the horizontal peak spectrogram 1400 and the vertical peak spectrogram 1600 and depicts the horizontal peaks, selected in time, and the vertical peaks, selected in frequency, of the received audio sample.
- the vertical columns in the beat pitch graphs 1700 a - b with a higher density of horizontal peaks correspond to the beats presented in the beat spectrogram 1500 .
- the beat pitch detector 114 finds the vertical peaks that are in the neighboring time slices and the specific time slice. For example, the beat pitch detector 114 determines a quantity of vertical peaks in the neighboring time slices and the specific time slice.
- the beat pitch detector 114 when determining the quantity of vertical peaks in the neighboring time slices and the specific time slice, analyzes each of the frequency bins for the specific time slice separately. For example, the beat pitch detector 114 forms an 11 ⁇ 1 window around a frequency bin where the window is centered on the specific time slice that contains a beat. The beat pitch detector 114 then determines a score representing the quantity of vertical peaks in the window. For example, the beat pitch detector 114 may count the total number of vertical peaks in the window for a maximum score of eleven, i.e., based on the size of the window, such as the width, for each of the frequency bins associated with the specific time slice.
- the beat pitch detector 114 may determine, for each of the beats, one or more windows associated with the beat, each window centered on the beat, associated with a row in the spectrogram, and with the same predetermined width, e.g., eleven.
- the beat pitch detector 114 may determine, for each of the windows, a quantity of vertical peaks in the window.
- the beat pitch detector 114 determines a highest score associated with each of the time slices that contain a beat ( 1218 ). For example, the beat pitch detector 114 compares the scores associated with each of the frequency bins for the specific time slice and selects the highest score as the score for the specific time slice.
- the beat pitch detector 114 may determine, for each of the beats, a highest score associated with the beat, the highest score based on the highest quantity of vertical peaks in one of the windows associated with the beat, such that a score for each of the beats comprises the highest score for the beat.
- the beat pitch detector 114 determines a beat pitch score for the audio sample ( 1220 ). For example, the beat pitch detector 114 combines the scores for each of the time slices that contain a beat to determine the beat pitch score. In one example, the beat pitch detector 114 sums the scores, i.e., the highest scores, associated with each of the time slices that contain a beat to determine the beat pitch score.
- the beat pitch score is a measure of sustained pitches around the beats in the received audio sample.
- a higher beat pitch score for an audio sample indicates a higher likelihood that the audio sample contains music audio data than if the audio sample was associated with a lower beat pitch score.
- the music audio data beat pitch graph 1700 b would be associated with a higher beat pitch score than the non-music audio data beat pitch graph 1700 a based on a higher occurrence of vertical peaks in horizontal lines around the horizontal peaks in the music audio data beat pitch graph 1700 b.
- the beat pitch detector 114 may determine the vertical peaks, e.g., perform step 1214 , prior to determining the horizontal peaks, e.g., perform step 1206 .
- the process 1200 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps.
- the music detector may receive the audio sample, and compute a spectrogram once.
- the process 1200 may include additional steps described above with reference to the process 200 or the process 500 .
- the beat pitch detector 114 may correct the spectrogram 1300 for spectral tilt.
- the process 1200 does not generate the horizontal peak spectrogram 1400 and/or filter one or more frequency bands of the spectrogram.
- the beat pitch detector 114 may determine the horizontal peaks without generating the horizontal peak spectrogram 1400 .
- step 1216 is divided into multiple steps.
- the beat pitch detector 114 may find the vertical peaks around each beat, determine, for each beat, the number of vertical peaks in each frequency bin around the specific beat, select the highest score associated with one of the frequency bins for the specific beat, and combine the highest scores for all of the beats included in the received audio sample to determine a beat pitch score for the received audio sample.
- the linear classifier 116 uses input from a frame entropy engine when determining whether an audio sample contains music audio data or non-music audio data.
- the frame entropy engine is one of the additional detectors 120 and detects music audio data in an audio sample by measuring the median Shannon entropy of a spectral slice.
- the frame entropy engine measures the Shannon entropy of a spectral slice by performing the following steps. For example, the frame entropy engine receives an audio sample and computes a spectrogram of the audio sample. The audio sample may be between about 0.5 seconds and about 5.0 seconds long. In some implementations, the frame entropy engine uses a fast Fourier transform to compute the spectrogram. The frame entropy engine may use a window length of 128 milliseconds when computing the spectrogram. The windows used to compute the spectrogram may overlap. For example, the frame entropy engine may use windows with a 3 ⁇ 4 overlap.
- the frame entropy engine may correct the spectrogram for spectral tilt.
- the frame entropy engine may correct the spectrogram for overweight lower frequencies.
- the frame entropy engine may multiple each row or frequency bin in the spectrogram with the square root of the zero-based row index, e.g., the square root of the frequency value, as described above with reference to FIG. 2 .
- the frame entropy engine may filter low frequencies and high frequencies from the spectrogram. For example, the frame entropy engine may remove the frequencies, or frequency bins associated with frequencies, lower than 170 Hz and higher than 2200 Hz from the spectrogram, creating a filtered spectrogram associated with frequencies between 170 Hz to 2200 Hz, inclusive.
- the frame entropy engine may compute the Shannon entropy of the linear magnitude of the spectral slice.
- the frame entropy engine may use any appropriate algorithm to determine the Shannon entropy of a spectral slice.
- the frame entropy engine computes a frame entropy score for the audio sample based on the Shannon entropy values associated with the spectral slices in the audio sample. For example, the frame entropy score is the negative of the median Shannon entropy value associated with the spectral slices for the audio sample.
- the frame entropy score is a representation of whether the audio sample contains music or non-music audio data. For example, music tends to have a lower spectral slice Shannon entropy than non-music, and higher frame entropy scores may be associated with music audio data compared to non-music audio data when the frame entropy scores are the negative of the median Shannon entropy value associated with the spectral slices for an audio sample.
- the frame entropy engine may use any other appropriate algorithm to determine the entropy of each of the spectral slices for an audio sample and generate a frame entropy score based on the entropy values that is representative of whether the audio sample contains music or non-music audio data.
- the linear classifier 116 uses input from an energy entropy engine when determining whether an audio sample contains music audio data or non-music audio data.
- the energy entropy engine is one of the additional detectors 120 and measures the audio energy entropy of the audio sample.
- the energy entropy engine measures the audio energy entropy of an audio sample by performing the following steps. For example, the energy entropy engine receives an audio sample and creates one or more windows associated with the audio sample. The audio sample may be between about 0.5 and about 5.0 seconds long. In some implementations, the energy entropy engine creates non-overlapping windows for the audio sample. In some examples, the windows are about 90 milliseconds long.
- RMS root mean square
- the energy entropy engine combines the computed energies to determine an energy entropy score. For example, the energy entropy engine determines the Shannon entropy of the obtained energies of each of the windows and uses the determined Shannon entropy as the energy entropy score for the received audio sample.
- the energy entropy score is a representation of whether the audio sample contains music or non-music audio data.
- the energy entropy engine may use any other appropriate algorithm to determine the energy entropy of an audio sample.
- the linear classifier 116 uses input from a spectral centroid engine when determining whether an audio sample contains music audio data or non-music audio data.
- the spectral centroid engine is one of the additional detectors 120 and computes the midpoint of the spectral energy distribution of a sound in an audio sample, e.g., the balance point of the spectrum in the audio sample.
- the spectral centroid engine computes the midpoint of the spectral energy distribution of a sound in an audio sample by performing the following steps. For example, the spectral centroid engine receives an audio sample and computes a spectrogram of the audio sample. The audio sample may be between about 0.5 and about 5.0 seconds long. In some implementations, the spectral centroid engine uses a fast Fourier transform to compute the spectrogram. The spectral centroid engine may use a window length of 64 milliseconds when computing the spectrogram. The spectral centroid engine may use non-overlapping windows when computing the spectrogram.
- the spectral centroid engine may correct the spectrogram for spectral tilt as described with reference to FIG. 2 above.
- the spectral centroid engine may compute the spectral centroid for the respective spectral slice.
- the spectral centroid may be computed as the sum of the frequencies weighted by the linear magnitudes and divided by the sum of the linear magnitudes.
- the spectral centroid engine determines a spectral centroid score for the audio sample based on the spectral centroid values.
- the spectral centroid score may be the median spectral centroid associated with the spectral slices for the audio sample over a time window.
- the spectral centroid score is a representation of whether an audio sample contains music or non-music audio data.
- the spectral centroid engine may use any other appropriate algorithm to determine the spectral centroid of an audio sample.
- the linear classifier 116 uses input from a zero crossing rate engine when determining whether an audio sample contains music audio data or non-music audio data.
- the zero crossing rate engine is one of the additional detectors 120 and may measure the average zero crossing rate of the audio sample.
- the zero crossing rate engine measures the average zero crossing rate of an audio sample by performing the following steps. For example, the zero crossing rate engine receives an audio sample and creates one or more windows associated with the audio sample.
- the audio sample may be between about 0.5 seconds and about 5.0 seconds long. In some implementations, the windows are about 32 milliseconds long.
- the zero crossing rate engine may use non-overlapping windows.
- the zero crossing rate engine determines the zero crossing rate.
- the zero crossing rate may be determined by dividing the quantity of signal zero crossings by the quantity of signal samples. In some examples, the zero crossing rate is a number between 0.0 and 1.0.
- the zero crossing rate engine determines a zero crossing rate score.
- the zero crossing rate score is the median zero crossing rate for the audio sample.
- the zero crossing rate score is a representation of whether an audio sample contains music or non-music audio data.
- the zero crossing rate engine may use any other appropriate algorithm to determine the zero crossing rate of an audio sample.
- the linear classifier 116 receives input from one or more detectors and determines whether an audio sample contains music or non-music audio data based on the received input. For example, the linear classifier 116 receives the spectral fluctuation score, the peak repetition score, the beat pitch score, the frame entropy score, the energy entropy score, the spectral centroid score, and the zero crossing rate score and combines the scores to determine whether the audio sample contains music or non-music audio data.
- the linear classifier 116 receives input from a subset of the spectral fluctuation detector 110 , the peak repetition detector 112 , the beat pitch detector 114 , and the additional detectors 120 , e.g., the frame entropy detector, the energy entropy engine, the spectral centroid engine, and the zero crossing rate engine. In some implementations, the linear classifier 116 receives input from other detectors that generate scores indicative of whether an audio sample contains music or non-music audio data.
- the linear classifier 116 may receive input from one or more detectors that indicate the state of the environmental audio data. For example, the linear classifier engine may use the input to determine information about a song recorded in an audio sample. In one example, the linear classifier 116 may determine the name and/or the artist of the song.
- the linear classifier 116 determines a linear separation in space based on the input from one or more detectors. For example, the linear classifier 116 determines a linear plane in space that represents a received audio sample and uses the linear plane to determine whether the received audio sample contains music audio data or non-music audio data.
- the linear classifier 116 uses scores from the various detectors, and potentially one or more additional detectors not described above, as feature values to determine whether an audio sample contains music audio data or non-music audio data.
- the linear classifier 116 may use quadratic expansion on the feature values to determine whether the audio sample contains music audio data or non-music audio data.
- the linear classifier 116 computes the following quadratic feature vector, shown in Table 1, from the detector scores, where each of the feature values fv represents one of the detector scores.
- the linear classifier 116 may use the result of the quadratic feature vector shown in Table 1 to determine whether an audio sample contains music audio data or non-music audio data.
- quadratic_feature_vector [ fv 0 , fv 1 , ..., fv (n ⁇ 1) , fv n , fv 0 2 , fv 1 2 , .., fv (n ⁇ 1) 2 , fv n 2 , fv 0 *fv 1 , fv 0 *fv 2 , ..., fv 0 *fv (n ⁇ 1) , fv 0 *fv n , fv 1 *fv 2 , fv 1 *fv 3 , ..., fv 1 *fv (n ⁇ 1) , fv 1 *fv n , ... fv (n ⁇ 2) *fv (n ⁇ 1) , fv (n ⁇ 2) *fv n , fv (n ⁇ 1) *fv n ]
- the linear classifier 116 uses a plurality of quadratic feature vectors as training data. For example, for a specific audio sample that has been classified as containing music audio data or non-music audio data, the linear classifier receives the output from the various detectors for the specific audio sample, generates a quadratic feature vector for the specific audio sample, and uses an indication of whether or not the specific audio sample contains music audio data or non-music audio data to learn how to classify other audio samples. During training, the linear classifier 116 may repeat this process for a predetermined number of difference audio samples.
- the linear classifier 116 may use any appropriate algorithm to combine the input from one or more detectors and determine whether an audio sample contains music audio data or non-music audio data. For example, the linear classifier 116 may use the beat pitch score to determine whether the audio sample contains music audio data or non-music audio data without modifying or combining the beat pitch score with another score. In another example, the linear classifier 116 uses another machine learning algorithm to determine whether the audio sample contains music audio data or non-music audio data.
- implementations of the systems and techniques described here may be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations may include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the systems and techniques described here may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the systems and techniques described here, or any combination of such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- FIG. 18 shows a schematic diagram of a generic computer system 1800 .
- the system 1800 can be used for the operations described in association with any of the computer-implement methods described previously, according to one implementation.
- the system 1800 includes a processor 1810 , a memory 1820 , a storage device 1830 , and an input/output device 1840 .
- Each of the components 1810 , 1820 , 1830 , and 1840 are interconnected using a system bus 1850 .
- the processor 1810 is capable of processing instructions for execution within the system 1800 .
- the processor 1810 is a single-threaded processor.
- the processor 1810 is a multi-threaded processor.
- the processor 1810 is capable of processing instructions stored in the memory 1820 or on the storage device 1830 to display graphical information for a user interface on the input/output device 1840 .
- the memory 1820 stores information within the system 1800 .
- the memory 1820 is a computer-readable medium.
- the memory 1820 is a volatile memory unit.
- the memory 1820 is a non-volatile memory unit.
- the storage device 1830 is capable of providing mass storage for the system 1800 .
- the storage device 1830 is a computer-readable medium.
- the storage device 1830 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device.
- the input/output device 1840 provides input/output operations for the system 1800 .
- the input/output device 1840 includes a keyboard and/or pointing device.
- the input/output device 1840 includes a display unit for displaying graphical user interfaces.
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for analyzing an audio sample to determine whether the audio sample includes music audio data. One or more detectors, including a spectral fluctuation detector, a peak repetition detector, and a beat pitch detector, may analyze the audio sample and generate a score that represents whether the audio sample includes music audio data. One or more of the scores may be combined to determine whether the audio sample includes music audio data or non-music audio data.
Description
- This application claims the benefit of U.S. Provisional Application Ser. No. 61/763,668, filed on Feb. 12, 2013, which is incorporated by reference.
- The present specification relates to classifying audio data, and more specifically, classifying audio data as music audio data or non-music audio data.
- An individual may hear a song on the radio or in a public establishment, and may want to later acquire the song by purchasing the song from an online music distribution service.
- According to some innovative aspects of the subject matter described in this specification, an audio sample is analyzed to determine whether the audio sample includes music. For example, a user may be in a coffee house that is playing background music. The user may be interested in learning more information about the music, such as a song name or artist title associated with background music. The user can interact with a mobile computing device, e.g., a smartphone, to facilitate determining such information. The mobile computing device can detect the audio data, e.g., the background music, encode the audio data as waveform data, and provide the waveform data to a server-based computing environment.
- Innovative aspects of the subject matter described in this specification may be embodied in methods that include the actions of receiving an audio sample that is associated with audio data, computing a spectrogram of the received audio sample, detecting one or more beats in the spectrogram, detecting one or more sustained pitches in the spectrogram around the beats, determining, for each of the beats, a score based on the sustained pitches around the beat, and determining a beat pitch score that indicates the likelihood that the audio sample contains music audio data, the beat pitch score based on the scores for each of the beats.
- Another aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving an audio sample that is associated with audio data, computing a spectrogram of the received audio sample, determining an average spectral envelope of the spectrogram, determining one or more differences between adjacent values in the average spectral envelope, and determining a spectral fluctuation score that indicates the likelihood that the audio sample contains music audio data, the spectral fluctuation score based on the differences between adjacent values in the average spectral envelope.
- Another aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving an audio sample that is associated with audio data, computing a spectrogram of the received audio sample, the spectrogram including one or more spectral slices, determining, for each of the spectral slices in the spectrogram, one or more peaks, determining, for each of the spectral slices in the spectrogram, a similarity between the spectral slice and the other spectral slices in the spectrogram based on the peaks, determining, for each time shift between slices in the spectrogram, a mean similarity value based on the similarity values associated with the time shift, generating a projection from the mean similarity values, smoothing the projection, determining a density of one or more local peaks in the smoothed projection, and determining a peak repetition score that indicates the likelihood that the audio sample contains music audio data based on the density of the one or more local peaks in the smoothed projection.
- Other embodiments of these aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- These and other embodiments may each optionally include one or more of the following features. For instance, detecting the one or more beats in the spectrogram may comprise determining one or more horizontal peaks in the spectrogram, generating a sparse representation of the spectrogram based on the horizontal peaks, and detecting the one or more beats in the sparse spectrogram. In some implementations, detecting the one or more sustained pitches in the spectrogram around the beats comprises determining one or more vertical peaks in the spectrogram, and detecting the one or more sustained pitches in the spectrogram around the beats based on the vertical peaks in the spectrogram around the beats. In some implementations, the method further comprises comparing the beat pitch score to a beat pitch threshold, and determining that the audio sample contains music audio data based on the beat pitch score being greater than the beat pitch threshold. In some implementations, determining, for each of the beats, the score based on the sustained pitches around the beat comprises determining, for each of the beats, one or more windows associated with the beat, each window centered on the beat, associated with a row in the spectrogram, and having the same predetermined width, determining, for each of the windows, a quantity of vertical peaks in the window, and determining, for each of the beats, a highest score associated with the beat, the highest score based on the highest quantity of vertical peaks in one of the windows associated with the beat, wherein the score for each of the beats comprises the highest score for the beat.
- In some implementations, the method further comprises correcting the spectral tilt of the spectrogram. In some implementations, computing the spectrogram comprises creating a plurality of frequency bins for the audio sample, and generating the spectrogram from the frequency bins. In some implementations, the method further comprises filtering one or more frequency bands from the spectrogram.
- In some implementations, the method further comprises normalizing one or more intensity values of each spectral slice of the spectrogram to create a normalized spectrogram, wherein determining the average spectral envelope of the spectrogram comprises determining the average spectral envelope of the normalized spectrogram. In some implementations, the spectral fluctuation score comprises the mean of the one or more differences. In some implementations, the mean of the one or more differences comprises the mean of the absolute values of the differences between adjacent values in the average spectral envelope. In some implementations, the method further comprises approximating a first derivative of the average spectral envelope in the frequency dimension, wherein determining the one or more differences between adjacent values in the average spectral envelope comprises determining the one or more differences between adjacent values in the average spectral envelope based on the first derivative of the average spectral envelope. In some implementations, the method comprises determining an average squared magnitude of the audio sample, and comparing the average squared magnitude of the audio sample to a threshold value, wherein computing the spectrogram is based on determining that the average squared magnitude of the audio sample is great than the threshold value.
- In some implementations, the method comprises generating a sparse representation of the spectrogram based on the peaks, wherein determining, for each of the spectral slices in the spectrogram, a similarity between the spectral slice and the other spectral slices in the spectrogram comprises determining a similarity between the spectral slice and the other spectral slices in the sparse spectrogram. In some implementations, the peaks comprise amplitude peaks. In some implementations, the method further comprises filtering one or more mean similarity values from the projection, wherein smoothing the projection comprises smoothing the filtered projection. In some implementations, the peak repetition score is based on a maximum of the filtered projection, a sample standard deviation of the filtered projection, and the density of the one or more local peaks in the smoothed projection. In some implementations, the one or more mean similarity values are filtered based on the time shifts associated with the one or more mean similarity values.
- The subject matter described in this specification may be implemented in various implementations to realize one or more of the following potential advantages. In some implementations when the linear classifier runs on a device with limited resources, a model used by the linear classifier to represent an audio sample may be small to conserve resources. In some implementations when the linear classifier runs on a device with limited resources, the generation of the model might not be computationally expensive to conserver the resources of the device, e.g., battery power. In some implementations, classification of audio samples using the model might not be computationally expensive to conserver the resources of the device, e.g., batter power.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 depicts a system for classifying audio data. -
FIGS. 2, 5, and 12 are flow diagrams of processes for determining whether an audio sample contains music audio data. -
FIGS. 3 and 13-16 are example spectrograms. -
FIGS. 4A-B depict examples of average spectral envelopes. -
FIG. 6 is an example heatmap. -
FIGS. 7-11B are example heatmap projections. -
FIGS. 17A-B are example beat pitch graphs. -
FIG. 18 is a block diagram of a computing system that can be used in connection with computer-implemented methods described in this document. - Like reference numbers and designations in the various drawings indicate like elements.
-
FIG. 1 depicts asystem 100 for classifying audio data. Thesystem 100 can determine whether audio data includes music audio data or non-music audio data. Thesystem 100 includes amobile computing device 102 and amusic detector engine 104. Themobile computing device 102 is in communication with themusic detector engine 104 over one or more networks. Themobile computing device 102 can include a microphone, a camera, or other detection mechanisms for detecting environmental data associated with auser 106. In some implementations, themobile computing device 102 includes at least a portion, or all, of themusic detector engine 104. For example, themobile computing device 102 implements themusic detector engine 104 for classifying audio data. - In some examples, the
user 106 is sitting at a coffee house that is playing music in the background. In the illustrated example, theuser 106 would like to know more information about the music, e.g., the name of the song or the artist of the song. To that end, themobile computing device 102 can facilitate determining information about the environmental audio data, e.g., the background music. - Specifically, the
mobile computing device 102 detects the environmental audio data and processes the detected environmental audio data to generatewaveform data 108. Thewaveform data 108 represents the detected environmental audio data. Themobile computing device 102 can transmit thewaveform data 108 to themusic detector engine 104, e.g., over a network, during operation (A). In some examples, thewaveform data 108 is streamed from themobile computing device 102 to themusic detector engine 104. - In some implementations, the environmental audio data can include an utterance that is generated by the
user 106. For example, the utterance may include a question posed by the user such as “What is this song?” In some examples, when the environmental audio data includes the utterance generated by theuser 106, thewaveform data 108 can represent the detected utterance and the environmental audio data. - In some examples, when the environmental audio data includes the utterance generated by the
user 106, themobile computing device 102 detects the environmental audio data after detecting the utterance; detects the environmental audio data concurrently with detecting the utterance; or both. - In some implementations, the
mobile computing device 102 can initiate detection of the environmental audio data in response to interaction by the user with themobile computing device 102. Specifically, themobile computing device 102 can begin detection of the environmental audio data in response to launching an application on themobile computing device 102, e.g., through selection of a graphical representation of the application or selection of a physical button of themobile computing device 102 that is associated with the application, e.g., “double-clicking” a “home” button. - In some implementations, the
mobile computing device 102 detects the environmental audio data, e.g., the background music, continuously. In some examples, themobile computing device 102 may detect the environmental audio data in the background. For example, themobile computing device 102 may detect the environment audio data when theuser 106 is interacting with a mobile application of themobile computing device 102, e.g., theuser 106 is checking e-mail. In another example, themobile computing device 102 may detect the environmental audio data when themobile computing device 102 is in a “locked” state, e.g., themobile computing device 102 is powered-on but not being actively used. - In some implementations, the
mobile computing device 102 detects the environment audio data, e.g., the background music, in response to an instruction from theuser 106, e.g., user-directed interaction. For example, themobile computing device 102 detects the environmental audio data in response to theuser 106 providing instructions through a graphical user interface of themobile computing device 102, e.g., the user executing a mobile application of themobile computing device 102. Further, themobile computing device 102 may detect the environmental audio data in response to theuser 106 providing instructions that are included by the utterance, i.e., the utterance includes verbal instructions to detect the environmental audio data. In some examples, themobile computing device 102 can continuously detect utterances, e.g., continuously detect utterances in the background. In some examples, themobile computing device 102 can detect the utterances in response to theuser 106 executing a mobile application for detecting utterances. - In some implementations, the
mobile computing device 102 detects the environmental audio data, e.g., the background music, in a continuous stream of audio samples with low latency. Specifically, themobile computing device 102 detects the environmental audio data by detecting one or more audio samples of the environmental audio data over one or more time periods, e.g., a “window of length.” For example, themobile computing device 102 detects a first audio sample of the environmental audio data over a time period t1 and a second audio sample of the environmental audio data over a time period t2. In some examples, the magnitude of the time periods t1 and t2 substantially differ, e.g., are of different time lengths. In some examples, the magnitude of the time periods t1 and t2 are substantially the same, e.g., are of the same time length. - In some examples, a portion, or all, of the second audio sample coincides with, e.g., overlaps, a portion, or all, of the first audio sample. For example, a portion, or all, of the time period t2 coincides with, e.g., overlaps, a portion, or all, of the time period t1. In some examples, the second audio sample occurs substantially after the first audio sample. For example, the time period t2 occurs after the time period t1.
- In some examples, as the length of the audio sample increases, i.e., the time length associated with the audio sample increases, an accuracy of determining a state of the environmental audio, e.g., whether the environmental audio data is associated with music audio data or non-music audio data, also increases. In some examples, the
mobile computing device 102 detects multiple audio samples, e.g., four audio samples, each of differing time lengths, e.g., 0.5 seconds, 1.1 seconds, 2.5 seconds, and 5 seconds. - In some implementations, the
mobile computing device 102 provides multiple audio samples to themusic detector engine 104 until themusic detector engine 104 determines whether the audio samples contain music or non-music or until themusic detector engine 104 determines information about music included in the audio samples, e.g., the name of the song in the audio samples, or the artist of the song. For example, themobile computing device 102 provides a 0.5 second audio sample and a 1.1 second audio sample to themusic detector engine 104 and, prior to sending a 2.5 second audio sample, themobile computing device 102 receives music information from themusic detector engine 104 and does not provide the 2.5 second audio sample, or another, e.g., longer, audio sample, to the music detector. - In some examples, the
mobile computing device 102 detects the environmental audio data, e.g., the background music, utilizing increasing window lengths, i.e., increasing time lengths. In some examples, the increasing window lengths are exponentially increasing window lengths of the environmental audio data. For example, themobile computing device 102 detects multiple audio samples, e.g., four audio samples, each of increasing time lengths, e.g., 0.5 seconds, 1.1 seconds, 2.5 seconds, and 5 seconds. - In some examples, the
mobile computing device 102 detects the environmental audio data using sliding time windows. That is, themobile computing device 102 continuously detects audio samples having overlapping time periods. For example, new audio samples of the environmental audio data are detected as the time windows are shifted, e.g., sliding, and a new score, e.g., used to determine whether the environmental audio data includes music audio data or non-music audio data, as described further below, is determined for each audio sample associated with each new time window. For example, the sliding window can be 1.1 seconds. - The
music detector engine 104 receives thewaveform data 108 from themobile computing device 102. Themusic detector engine 104 processes thewaveform data 108, during operation (B). For example, themusic detector engine 104 identifies a state of the environmental audio data, i.e., whether the environmental audio data includes music audio data or non-music audio data. In some examples, non-music audio data can include speech, talking, noise, or other ambient non-music noises. - Specifically, the
music detector engine 104 includes aspectral fluctuation detector 110, apeak repetition detector 112, abeat pitch detector 114, alinear classifier 116, and/or one or moreadditional detectors 120. In some examples, based on the length of time associated with an audio sample of the environmental audio data, thespectral fluctuation detector 110, thepeak repetition detector 112, thebeat pitch detector 114, or all three, identify the state of the environmental audio data, e.g., whether the environmental audio data includes music audio data or non-music audio data. - The
additional detectors 120 may be used to determine whether the environmental audio data includes music audio data or non-music audio data. For example, a frame entropy detector may be used to determine the spectral slice entropy over a time window, as discussed in more detail below. Further, themusic detector engine 104 may include an energy entropy detector, a spectral centroid detector, and a zero crossing rate detector, to name a few examples of theadditional detectors 120. - The
linear classifier 116 receives input from the detectors, including thespectral fluctuation detector 110, thepeak repetition detector 112, and/or thebeat pitch detector 114, and determines the state of the environmental audio data. Thelinear classifier 116 may also receive input from one or more music information detectors that thelinear classifier 116 uses to determine information about music included in the environmental audio data, e.g., a song name or an artist name. - In some implementations, the
music detector engine 104 compares a time length of the audio sample with a threshold duration. Based upon the comparison, themusic detector engine 104 employs thespectral fluctuation detector 110, thepeak repetition detector 112, thebeat pitch detector 114, another detector, or a combination of detectors, to identify the state of the environment audio data, e.g., whether the environmental audio includes music audio data or non-music audio data. - In some examples, when the time length of the audio sample is less than the threshold duration, the
music detector engine 104 may employ thespectral fluctuation detector 110 to identify whether the environmental audio data includes music audio data or non-music audio data. For example, thespectral fluctuation detector 110 can be used to determine whether the environmental audio data includes music audio data or non-music audio data when the time length of the audio sample is short, e.g., 0.5 seconds-1.5 seconds. - In some examples, when the length of time of the audio sample is greater than the threshold duration, the
music detector engine 104 employs thepeak repetition detector 112 to identify whether the environmental audio data includes music audio data or non-music audio data. For example, thepeak repetition detector 112 can be used to determine whether the environmental audio data includes music audio data or non-music audio data when the time length of the audio sample is longer, e.g., 1.5 seconds or 2.5 seconds. - In some examples, only the
peak repetition detector 112 or thebeat pitch detector 114 is used to determine whether the environmental audio data includes music audio data or non-music audio data when the time length of the audio sample is longer than a threshold duration, e.g., 2.5 seconds. Alternatively, when the threshold duration is a range, e.g., 0.5 seconds to 2.5 seconds, themusic detector engine 104 may employ thespectral fluctuation detector 110, thepeak repetition detector 112, and thebeat pitch detector 114. - In some implementations, a combination of the
spectral fluctuation detector 110, thepeak repetition detector 112, thebeat pitch detector 114, and/or theadditional detectors 120 are used together, e.g., to increase an accuracy of identification of the state of the environmental audio data. For example, themusic detector engine 104 weights a score output by each of thespectral fluctuation detector 110, thepeak repetition detector 112, and thebeat pitch detector 114, in addition to any other music detectors, where each of the scores relates to the likelihood that the audio sample is associated with music audio data. In some examples, the weighting can be based on a length of time of the audio sample. - For example, each of the scores output by the music detectors, e.g., the
spectral fluctuation detector 110, thepeak repetition detector 112, thebeat pitch detector 114, can be used to determine whether the environmental audio data includes music audio data or non-music audio data. For example, themusic detector engine 104, thelinear classifier 116, or both, can compare the score to a threshold score. In some examples, when the score is greater than or equal to the threshold score, the audio sample, and the corresponding environmental audio data, can be identified as containing music audio data. In some examples, when the score is not greater than the threshold score, the audio sample, and the corresponding environmental audio data, can be identified as containing non-music audio data. In certain implementations, when the score is not greater than the threshold score, the audio sample, and the corresponding environmental audio data, can be identified as not containing recognizable music audio data, e.g., when the intensity or quality of any music audio data included in the audio sample is too low. - In some implementations, one or more of the music detectors computes a score for two or more audio samples of differing time lengths. For example, the audio samples can have time lengths of 2.5 seconds and 5 seconds respectively. In some examples, the maximum score of the scores for two or more audio samples can be determined. Thus, the maximum score can be used to determine whether the environmental audio data includes music audio data or non-music audio data.
- The
music detector engine 104 providesidentification information 118, related to a state of the environmental audio and based on the analyzed audio samples of the environmental audio data, e.g., a determination of whether the environmental audio data includes music audio data or non-music audio data, to themobile computing device 102, e.g., over a network, at operation (C). Specifically, themusic detector engine 104, based on a state determination provided by thelinear classifier 116, provides thestate identification information 118 of the environmental audio data to themobile computing device 102. For example, themusic detector engine 104 determines that the environmental audio data, e.g., thewaveform data 108, corresponds to music audio data. In some examples, themusic detector engine 104 can providesuch identification information 118 thru an application executed by themobile computing device 102, or other communication mechanisms such as a text message or an e-mail. - In some examples, when the
identification information 118 includes a determination that the environmental audio data includes music audio data, theidentification information 118 can include an option to “recognize” the music audio data, e.g., through an application executed by the mobile computing device, and/or an option to “purchase” the song or album associated with the music audio data. In some examples, theidentification information 118 can include recognition information associated with the music audio data, e.g., the song name or the artist name. - In some implementations, the
identification information 118 can be displayed on the GUI of themobile computing device 102 in response to user interaction. For example, theuser 106 can launch an application of themobile computing device 102, and in response, the application can provide theidentification information 118. - In some implementations, the
identification information 118 can be displayed on the GUI of themobile computing device 102 automatically, i.e., without input from theuser 106. For example, the GUI can include, upon transitioning from the “locked” state to an “active state,” theidentification information 118. In another example, the GUI can include, in the “locked” state, theidentification information 118, e.g., the “lock” screen of the GUI can include theidentification information 118. -
FIG. 2 is a flow diagram of aprocess 200 for determining whether an audio sample contains music audio data by measuring the consistency of fluctuations in spectral slices across a time window associated with the time sample. Theprocess 200 can be used by thespectral fluctuation detector 110 from thesystem 100. For example, thespectral fluctuation detector 110 receives an audio sample, e.g., of the environmental audio data, and detects sustained pitches in the audio sample over a period time, where sustained pitches in audio sample are more likely to occur in music audio data as compared to an audio sample including human speech or other types of non-musical noise. - The
spectral fluctuation detector 110 receives an audio sample that is associated with audio data (202). For example, thespectral fluctuation detector 110 receives t seconds of the audio data as the audio sample. In some examples, the audio sample has a length of 1.5 seconds to 10 seconds; however, in some examples, thespectral fluctuation detector 110 can determine whether the audio sample relates to music audio data with audio samples as short as 0.5 seconds. - The
spectral fluctuation detector 110 determines an average squared magnitude of the audio sample (204). For example, the average squared magnitude of the audio sample represents the intensity of the audio sample and thespectral fluctuation detector 110 uses the average squared magnitude to determine whether the data in the audio sample can be further processed. For example, when the intensity is low, thespectral fluctuation detector 110 determines that the audio sample contains mostly silence and should not be processed further. - The
spectral fluctuation detector 110 compares the average squared magnitude of the audio sample to a threshold value (206). For example, the spectral fluctuation detector determines whether the average squared magnitude of the audio sample is greater than the threshold value. - Based upon the comparison, when the average squared magnitude of the audio sample is not greater than the threshold value, the
spectral fluctuation detector 110 determines that the received audio sample is associated with low intensity audio sound (208). For example, thespectral fluctuation detector 110 determines that the audio sample contains silence, or sounds with very low intensity. In response, thespectral fluctuation detector 110 ceases processing of the received audio sample, e.g., identifying the state of the environmental audio data. - Based upon the comparison, when the average squared magnitude of the audio sample is greater than the threshold value, the
spectral fluctuation detector 110 computes a spectrogram of the received audio sample (210). For example, thespectral fluctuation detector 110 computes a time-varying spectral representation of the received audio sample, such as aspectrogram 300 shown inFIG. 3 . - In some implementations, a larger number of horizontal lines in the
spectrogram 300, e.g., represented by the light gray and white shading, indicates that the audio sample more likely includes music audio data than if thespectrogram 300 included fewer horizontal lines. For example, the horizontal lines represent sustained tones where music audio data includes more sustained tones than non-music audio data. - In some examples, the
spectral fluctuation detector 110 computes the spectrogram of the received audio sample utilizing a fast Fourier transform (FFT). In some examples, an associated window length of the spectrogram is 64 milliseconds and a time step of the spectrogram is 16 milliseconds. - In some implementations, the
spectral fluctuation detector 110 computes a chromogram. In some examples, thespectral fluctuation detector 110 utilizes autocorrelation in the processing of the received audio sample. - Returning to
FIG. 2 , thespectral fluctuation detector 110 corrects spectral tilt of the spectrogram (212). For example, thespectral fluctuation detector 110 adjusts the magnitude values in thespectrogram 300 to correct the spectral tilt and avoid overweighted lower frequencies of the spectrogram by multiplying the magnitude values associated with higher frequencies in thespectrogram 300 by a high constant value and the magnitude values associated with lower frequencies in thespectrogram 300 by a low constant value, where the magnitude values associated with the middle frequencies are multiplied by intermediate constant values. Specifically, thespectral fluctuation detector 110 may multiply each row of the spectrogram with a square root of the zero-based row index, e.g., the square root of the frequency value. As a result, the spectral tilt and the overweighted lower frequencies of the computer spectrogram are corrected, e.g., providing a corrected spectrogram. - The
spectral fluctuation detector 110 filters one or more frequency bands of the spectrogram associated with human speech (214). For example, thespectral fluctuation detector 110 filters one or more frequency bands of the corrected spectrogram. In some examples, thespectral fluctuation detector 110 can filter, e.g., remove, frequency bands that are associated with human speech, e.g., mid-range frequencies. For example, thespectral fluctuation detector 110 can filter, e.g., remove, frequency bands in the range of 1400 Hz to 2600 Hz, creating a filtered spectrogram that corresponds to the frequency bands below 1400 Hz and above 2600 Hz. - The
spectral fluctuation detector 110 filters one or more frequency bands that are associated with poor audio quality (216). For example, themobile computing device 102 may be only able to effectively capture environmental audio data above a certain frequency threshold and below a certain frequency threshold, e.g., based on physical limitations of a microphone associated with themobile computing device 102. Therefore, thespectral fluctuation detector 110 filters, e.g., removes, frequency bands below 120 Hz and above 4300 Hz. Thus, thespectral fluctuation detector 110 may effectively maintain spectrogram rows in the filtered spectrogram that correspond to frequency bands in the ranges of 120 Hz to 1400 Hz and 2600 Hz to 4300 Hz. - The
spectral fluctuation detector 110 normalizes one or more intensity values of each spectral slice of the spectrogram (218). For example, thespectral fluctuation detector 110 normalizes the intensity values of each spectral slice of the filtered spectrogram to compensate for high and low volume in the captured environmental audio data represented by the intensity values in the spectrogram. Specifically, thespectral fluctuation detector 110 normalizes each slice of the spectrogram by dividing the intensity values associated with each spectral slice by the harmonic mean of the intensity values of the spectral slices of a selected portion of the spectrogram. - In some examples, the selected portions of the spectrogram can include slices of the spectrogram having intensity values in a top percentile of the intensity values of the spectral slices of the spectrogram, e.g., the top 5% percentile. For example, the
spectral fluctuation detector 110 determines the intensity values that have the greatest magnitude in a specific spectral slice and uses the intensity values with the greatest magnitude to normalize the intensity values in the specific spectral slice. Further, thespectral fluctuation detector 110 may determine the harmonic mean of the intensity values with the greatest magnitude and use the harmonic mean value to normalize the intensity values in the specific spectral slice. Thespectral fluctuation detector 110 may use alternate methods or values to normalize the intensity values in a specific spectral slice. - In some examples, normalizing the intensity values of each spectral slice of the spectrogram based on a top percentile of the intensity values of the spectral slices of the spectrogram filters, e.g., removes, outlying values of spectral slices of the spectrogram, e.g., “outliers.” In some examples, the
spectral fluctuation detector 110 normalizes each spectral slice of the spectrogram independently for the lower and the upper spectrogram range, e.g., for therange 120 Hz to 1400 Hz and for the range 2600 Hz to 4300 Hz. - The
spectral fluctuation detector 110 determines an average spectral envelope of the spectrogram (220). For example, thespectral fluctuation detector 110 determines a curve in the frequency-amplitude plane of the spectrogram. Thespectral fluctuation detector 110 may determine the average spectral envelope for each row of the normalized spectrogram and create an average spectral envelope slice or graph from the average spectral envelope for each row. For example, thespectral fluctuation detector 110 averages all of the values for a particular frequency in the normalized spectrogram and uses the average value as the value for the particular frequency in the average spectral envelope graph. Thespectral fluctuation detector 110 may then repeat the process for all of the frequencies in the normalized spectrogram, or for a subset of discrete frequencies in the normalized spectrogram. -
FIGS. 4A-B depict examples of average spectral envelopes. For example, thespectral fluctuation detector 110 may generate a non-music averagespectral envelope 400 a, shown inFIG. 4A , for a non-music audio sample. Alternatively, thespectral fluctuation detector 110 may generate a music averagespectral envelope 400 b, shown inFIG. 4B , for a music audio sample. The averagespectral envelopes 400 a-b are examples based on specific audio samples, and the average spectral envelopes for different audio samples would have a similar appearance to one of the averagespectral envelopes 400 a-b, depending on whether the different audio samples include music audio data or non-music audio data. - In some implementations, the
spectral fluctuation detector 110 computes the average spectral envelope independently for a lower range and an upper range of thespectrogram 300. For example, thespectral fluctuation detector 110 may determine the average spectral envelope for the frequency bands in the range of 120 Hz to 1400 Hz and then determine the average spectral envelope for the frequency bands in the range of 2600 Hz to 4300 Hz. - Returning to
FIG. 2 , thespectral fluctuation detector 110 determines one or more differences between adjacent values in the average spectral envelope (222). For example, thespectral fluctuation detector 110 approximates a first derivate of the average spectral envelope in the frequency dimension. In some examples, thespectral fluctuation detector 110 computes the difference between the adjacent values of the average spectral envelope independently for a lower range, e.g., 120 Hz to 1400 Hz, and an upper range of the spectrogram, e.g., 2600 Hz to 4300 Hz. - The
spectral fluctuation detector 110 determines a spectral fluctuation score for the audio sample (224). For example, the spectral fluctuation score is the mean difference based on the differences between adjacent values in the average spectral envelope. For example, thespectral fluctuation detector 110 determines the mean absolute difference of the first derivate of the average spectral envelope to determine the flatness of a vector represented by the average spectral envelope. The spectral fluctuation detector may determine the mean difference based on the absolute values of the differences between adjacent values in the average spectral envelope. Thespectral fluctuation detector 110 may use any appropriate algorithm to determine the flatness of the vector represented by the average spectral envelope and use the determined value as a spectral fluctuation score for the audio sample. - Specifically, the spectral fluctuation score is a measure of the consistency of fluctuations in spectral slices of the spectrogram in the input time window. For example, music audio data is more likely than non-music audio data, e.g., human speech, to comprise sustained pitches and associated harmonics that are repeated in a short time period, e.g., 0.5 seconds-10 seconds. Thus, in some examples, the spectral characteristics of music audio data, e.g., pitches and harmonics, can accumulate and become emphasized in the average spectral envelope of the spectrogram. As a result, the average spectral envelope associated with music audio data can be less uniform as compared to an average spectral envelope associated with non-music audio data, e.g., the curve in the frequency-amplitude plane is less uniform for music audio data than non-music audio data.
- To that end, the spectral fluctuation score can be used in determining whether the environmental audio data includes music audio data or non-music audio data. For example, the
music detector engine 104, thespectral fluctuation detector 110, or both, can compare the spectral fluctuation score to a threshold spectral fluctuation score value. In some examples, when the spectral fluctuation score is greater than the threshold spectral fluctuation score value, the audio sample, and the corresponding environmental audio data, can be identified as containing music audio data. In some examples, when the spectral fluctuation score is not greater than the threshold spectral fluctuation score value, the audio sample, and the corresponding environmental audio data, can be identified as containing non-music audio data. - Additionally, for example, when averaging the spectral characteristics of non-music audio data of individual spectral slices of the spectrogram, i.e., in
step 220, the individual spectral slices can “cancel” each other out. Thus, the flatness of the average spectral envelope is related to the spectral fluctuation score that is indicative of the likelihood that the audio sample is a music audio sample. For example, the flatter the average spectral envelope, the lower the score, and thus the lower the likelihood that the audio sample is a music audio sample. - In some implementations, the
spectral fluctuation detector 110 may compute the spectral fluctuation score for two or more audio samples, e.g. of the received audio data. For example, thespectral fluctuation detector 110 computes two spectral fluctuation scores for audio samples of length 0.5 seconds and 1.1 seconds. In these implementations, the separate spectral fluctuation scores may be combined or the greater spectral fluctuation score value may be used. For example, thespectral fluctuation detector 110 may combine the two spectral fluctuation score values using weights, e.g., based on the duration of the audio sample. Alternatively, thespectral fluctuation detector 110 may determine which spectral fluctuation score has the greater value, and use the greater spectral fluctuation score as the spectral fluctuation score for the received audio sample. - In some implementations, the output of the
spectral fluctuation detector 110 can be combined, e.g., bylinear classifier 116, with a mean absolute difference of the lower spectrogram range, a mean absolute difference of the upper spectrogram range, and a sample standard deviation of all differences. For example, thespectral fluctuation detector 110 can combine a spectral fluctuation score for the lower frequency range in the spectrogram with a spectral fluctuation score for the higher frequency range in the spectrogram. - In some implementations, the output of the
spectral fluctuation detector 110 is used to classify an audio sample. For example, thelinear classifier 116 or themusic detector engine 104 may use the spectral fluctuation score to determine information about music included in the environmental audio data, e.g., a song name or an artist name associated with the music. Thelinear classifier 116 or themusic detector engine 104 may user other scores or data in addition to the spectral fluctuation score when classifying the environmental audio data. - The order of the steps in the
process 200 described above is illustrative only, and the measuring of the consistency of fluctuations in spectral slices across a time window can be performed in different orders. For example, the spectral fluctuation detector may filter one or more frequency bands that are associated with poor audio quality prior to filtering one or more frequency bands associated with human speech. - In some implementations, the
process 200 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps. For example, thespectral fluctuation detector 110 might performsteps steps 204 through 208, i.e., determining whether the audio sample is associated with low intensity audio sound. In some implementations, thespectral fluctuation detector 110 does not filter the spectrogram. For example, thespectral fluctuation detector 110 may perform theprocess 200 without performingstep 214 and/or without performingstep 216. - In some implementations, the
spectral fluctuation detector 110 bins the frequencies in the received audio sample when creating thespectrogram 300 shown inFIG. 3 . For example, thespectral fluctuation detector 110 creates a plurality of frequency bins where each frequency bin is approximately 10 Hz, such that the frequency values in thespectrogram 300 represent the frequency bins. In one example, thespectral fluctuation detector 110 creates the frequency bins as part of the fast Fourier Transform process. -
FIG. 5 is a flow diagram of aprocess 500 for determining whether an audio sample contains music audio data by measuring harmonic repetitions. Theprocess 500 can be used by thepeak repetition detector 112 from thesystem 100. For example, thepeak repetition detector 112 may determine whether the audio data includes music audio data or non-music audio data by computing a similarity of harmonics of the audio sample. For example, repetition of harmonics can represent a similarity of musical notes that are unlikely to occur in background noise associated with non-music audio data. - The
peak repetition detector 112 receives an audio sample that is associated with audio data (502). For example, thepeak repetition detector 112 receives t seconds of the audio data as the audio sample. In some examples, the audio sample is typically of length 1.5 seconds to 10 seconds. In some examples, the audio sample is at least 2.5 seconds long. - In some implementations, the
peak repetition detector 112 receives two or more audio samples with a sliding time window. That is, thepeak repetition detector 112 continuously receives audio samples having overlapping time periods. For example, the sliding window can be 1.1 seconds. - The
peak repetition detector 112 computes a spectrogram of the received audio sample (504). For example, thepeak repetition detector 112 determines a time-varying spectral representation of the received audio sample, such as thespectrogram 300 shown inFIG. 3 . - In some examples, the
peak repetition detector 112 computes the spectrogram of the received audio sample utilizing a fast Fourier transform (FFT). In some examples, an associated window length of the spectrogram is 64 milliseconds and a time step of the spectrogram is 16 milliseconds. - In some implementations, the
peak repetition detector 112 computes a chromogram. In some examples, thepeak repetition detector 112 utilizes autocorrelation in processing of the received audio sample. - The
peak repetition detector 112 corrects spectral tilt of the spectrogram (506). For example, thepeak repetition detector 112 corrects the spectral as described above with reference to step 212. Specifically, thepeak repetition detector 112 may multiply each row of the spectrogram with the zero-based row index, e.g., the square root of the frequency value. As a result, the spectral tilt of the spectrogram is corrected providing a corrected spectrogram. - The
peak repetition detector 112 determines one or more peaks for each spectral slice of the spectrogram (508). For example, thepeak repetition detector 112 identifies the peaks of each spectral slice of the corrected spectrogram. - In some implementations, the peaks of each spectral slice of the spectrogram are peaks of the amplitude of a particular frequency at a particular time for each respective spectral slice. In these implementations, the peaks of each spectral slice of the
spectrogram 300 may be represented by the color of a point in the spectrogram, i.e., defined by x and y coordinates. For example, a peak is a particular point where all the points that surround the particular point are a different color, e.g., a darker color, than the color of the particular point. - In some implementations, the peaks have a particular value in a single spectrogram column, e.g. slice, that is larger than all neighbor points in the same spectrogram column. For example, a peak associated with a frequency or a frequency bin is larger than the vertical neighbors of the peak that are associated with different frequencies or frequency bins and that are in the same slice represented by a predetermined window. In some implementations, the window is larger for higher frequencies than lower frequencies, providing a higher density of peaks in lower frequencies.
- In certain implementations, the
peak repetition detector 112 compares each point in the spectrogram with the neighbors of the point that belong in the same column of the spectrogram. For example, thepeak repetition detector 112 compares a particular point associated with a high frequency with ten neighbor values, e.g., five neighbors above and five neighbors below the particular point, to determine whether the particular point is a peak, i.e., if the amplitude of the particular point is greater than the ten neighbor values. When thepeak repetition detector 112 identifies peaks for low frequency points, thepeak repetition detector 112 may compare a particular low frequency amplitude value with the amplitude values of four neighbor points, e.g., two neighbors above and two neighbors below, to determine whether the particular point is a peak, i.e., if the amplitude of the particular point is greater than the amplitudes of the four neighbor points. - In some implementations, when the
peak repetition detector 112 determines whether a particular point near the edge of the spectrogram is a peak, thepeak repetition detector 112 compares the amplitude of the particular point with less than a predetermine quantity of neighbor amplitude values. For example, when the particular point is a high frequency point on the edge of the spectrogram, thepeak repetition detector 112 compares the amplitude of the particular point with the amplitudes of the next five lower frequency points and does not compare the amplitude of the particular point with the amplitudes of the next five higher frequency points. Further, if the particular point is associated with a high frequency and is separated from the edge of the spectrogram by one other point, the amplitude of the particular point is compared with six other amplitudes to determine whether the particular point is a peak, e.g., the amplitude of the other point and the amplitudes of the next five lower frequency points. - The
peak repetition detector 112 generates a sparse representation of the spectrogram (510). For example, based on identifying the peaks of each spectral slice of the spectrogram, thepeak repetition detector 112 adjusts the values associated with each of the peaks. In some implementations, thepeak repetition detector 112 adjusts the amplitude values associated with each of the peaks to a value of one. In some implementations, thepeak repetition detector 112 adjusts the values of the other points in the spectrogram, e.g., the non-peak amplitudes of the spectral slices, to a value of zero. - In certain implementations, the
peak repetition detector 112 generates a binary image of the spectrogram where the peaks are represented by values of one and the non-peaks are represented by values of zero by comparing each point in the spectrogram with the neighbors of the point that belong in the same column of the spectrogram. - The
peak repetition detector 112 filters one or more frequency bands of the spectrogram (512). For example, thepeak repetition detector 112 can filter, e.g., remove, frequency bands from the sparse representation of the spectrogram that are associated with poor audio quality. - In some implementations, the
mobile computing device 102 may be only able to effectively capture environmental audio data above a certain frequency, e.g., based on physical limitations of a microphone associated with themobile computing device 102. Therefore, in these implementations, thepeak repetition detector 112 may filter spectrogram rows that correspond to frequency bands below 150 Hz. Thepeak repetition detector 112 may filter additional or alternative frequency bands from the spectrogram. For example, thepeak repetition detector 112 may filter frequency bands above 4300 Hz. The filtering may be performed based on the performance of the microphone associated with themobile computing device 102 and/or the requirements of thepeak repetition detector 112, to name a few examples. - The
peak repetition detector 112 determines a similarity between the slices of the sparse representation of the spectrogram (514). For example, thepeak repetition detector 112 may generate aheatmap 600, shown inFIG. 6 , of the sparse spectrogram that represents the similarity between the slices of the sparse spectrogram. Specifically, thepeak repetition detector 112 generates theheatmap 600 by comparing each column, i.e., spectral slice, in the sparse spectrogram to the other columns, i.e., spectral slices, in the sparse spectrogram, e.g., autocorrelation, where the axes in the heatmap represent the column numbers of the sparse spectrogram, e.g., frequency or frequency bin values, and the values represented by the particular points in the heatmap represent the similarity between the two columns associated with the particular point. - For example, a center diagonal 602 of the
heatmap 600 represents the similarity between each column and itself. In this example, when the similarity is represented by the difference between two columns, each of the values along the diagonal is a maximum similarity value. - Further, all of the values along the other diagonals of the heatmap represent the same time shift, i.e., the same time difference between two time slices in the sparse spectrogram. For example, each slice of the sparse spectrogram represents the frequency and amplitude of the audio sample at a particular point in time. The values in the center diagonal 602 of the
heatmap 600 represent the similarity between each slice and itself, where the time shift is zero, i.e., there is no time difference between a slice and itself. The values in the diagonal above the center diagonal represent a time shift of one unit of time, e.g., one window, as this diagonal contains the similarity values for each pair of adjacent columns, and so on for the other diagonals in theheatmap 600. - In certain implementations, the
peak repetition detector 112 compares the sparse spectrogram to itself utilizing sample correlations of the sparse spectral slices as a similarity function. Thepeak repetition detector 112 may use any appropriate algorithm to determine the correlation or the similarity between the slices in the spectrogram. - Returning to
FIG. 5 , for each time shift in the heatmap, thepeak repetition detector 112 determines a mean similarity value (516). For example, the mean similarity value for a specific time shift is based on all of the similarity values associated with the specific time shift. - The
peak repetition detector 112 generates a projection from the mean similarity values (518). For example, thepeak repetition detector 112 averages the similarity values in each of the diagonals of the heatmap, where a greater mean value represents a greater similarity between the corresponding spectral slices of the spectrogram at a corresponding time shift than a smaller mean value. - For example, the
peak repetition detector 112 may generate aheatmap projection 700, shown inFIG. 7 , from the mean similarity values for each of the time shifts. Here, the x axis represents the different time shifts and the y axis represents the average of the similarity values that correspond to a specific time shift, i.e., the mean similarity value. For example, thepeak repetition detector 112 identifies all of the heatmap values that correspond to a time shift of five and uses the average of these heatmap values for the mean similarity of a time shift of five in theheatmap projection 700. In theheatmap projection 700 the mean similarity for a time shift of five is about 0.15. - Returning to
FIG. 5 , thepeak repetition detector 112 filters mean similarity values associated with one or more time shifts from the projection (520). In some examples, thepeak repetition detector 112 may remove mean similarity values that are associated with time shifts below a first threshold, above a second threshold, or both, from theheatmap projection 700 to create a filtered heatmap projection. For example, thepeak repetition detector 112 removes mean similarity values that correspond to time lags below 200 milliseconds and above 3200 milliseconds from the heatmap projection. Thus, in some examples, thepeak repetition detector 112 effectively maintains heatmap projections that correspond to time shifts between 200 milliseconds and 3200 milliseconds inclusive. In some examples, the first and the second thresholds are based on a time length of the received audio sample. One example of a filteredheatmap projection 800 generated by thepeak repetition detector 112 is shown inFIG. 8 . - Returning to
FIG. 5 , thepeak repetition detector 112 smooths the filtered projection (522). For example, thepeak repetition detector 112 may smooth the filtered heatmap projection with a Gaussian filter mask. Thepeak repetition detector 112 may use other appropriate algorithms to smooth the filteredheatmap projection 800 and create a smoothed heatmap projection. One example of a smoothedheatmap projection 900 generated by thepeak repetition detector 112 is shown inFIG. 9 . - Returning to
FIG. 5 , thepeak repetition detector 112 determines a density of the local peaks in the smoothed projection (524). For example, thepeak repetition detector 112 may compute the density of the local peaks in the smoothedheatmap projection 900 based on the number of local peaks 1002 a-g, shown inFIG. 10 , and the length of the smoothedheatmap projection 900. For example, the density of the local peaks is determined as the number of local peaks in the smoothedheatmap projection 900 divided by the length of the smoothedheatmap projection 900. - Returning to
FIG. 5 , thepeak repetition detector 112 determines a peak repetition score for the audio sample (526). The peak repetition score is a measure of the presence of consistent salient repetitions in the audio sample that are characteristic of music audio data. In some implementations, the peak repetition score is based on a maximum of the filtered heatmap projection, e.g., the maximum value from the filtered heatmap projection, a sample standard deviation of the filtered heatmap projection, i.e., an estimate of the standard deviation of the unfiltered heatmap projection based on the filtered heatmap projection, and the density of the local peaks in the smoothed heatmap projection. For example, the peak repetition score may be determined by the equation (A×B)/C, where A is the maximum of the filtered heatmap projection, B is the sample standard deviation of the filtered heatmap projection, and C is the density of the local peaks in the smoothed heatmap projection. However, in other implementations, the peak repetition score can be based on other equations. - In some implementations, the
peak repetition detector 112 is more robust to noise, e.g., non-music audio data, and small inessential changes in music audio data as a result of utilizing the sparse spectrogram. For example,FIGS. 11A-B depict anon-music heatmap projection 1100 a and amusic heatmap projection 1100 b of mean similarity values for non-music audio data and music audio data respectively. Here the mean similarity values associated with themusic heatmap projection 1100 b are more similar than the mean similarity values associated with thenon-music heatmap projection 1100 a that includes both positive and negative mean similarity values. - The order of steps in the
process 500 described above is illustrative only, and the determination of whether an audio sample contains music by measuring harmonic repetitions can be performed in different orders. For example, thepeak repetition detector 112 may filter time shifts that are below or above a predetermined threshold, e.g., performstep 520, from the heatmap prior to determining the mean similarity values, e.g., performstep 516. - In some implementations, the
process 500 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps. For example, when amusic detector engine 104 performs theprocess 500 in conjunction with theprocess 200 on the same audio sample, themusic detector engine 104 may perform either thesteps steps 502 through 506, i.e., the music detector only needs to receive an audio sample, compute a spectrogram, and correct for spectral tilt once. - In some implementations, the
process 500 includessteps 204 through 208. For example, theprocess 500 can determine whether a received audio sample has a low intensity and does not need further processing based on an average squared magnitude of the audio sample. - In some implementations, the
steps peak repetition detector 112 identifies a peak, thepeak repetition detector 112 adjusts the amplitude of the peak to a predetermined value, e.g., one. Once thepeak repetition detector 112 has identified all of the peaks in the spectrogram and adjusted the amplitude of all of the peaks, thepeak repetition detector 112 adjusts all of the other amplitudes in the spectrogram to different predetermined value, e.g., zero. Alternatively, when thepeak repetition detector 112 identifies a peak, thepeak repetition detector 112 adjusts the amplitude of the peak to a first predetermined value and the amplitudes of the neighboring points to a second predetermined value. - In some implementations, the
peak repetition detector 112 generates a triangular heatmap, e.g., the upper right half of theheatmap 600. For example, thepeak repetition detector 112 only generates the similarity values for the slices in the spectrogram once, i.e., instep 514, and uses these similarity values to generate the mean similarity values for each of the time shift values, i.e., instep 516. In some implementations, thepeak repetition detector 112 might not compare each slice to itself and generate the center diagonal 602. - In some implementations, the
peak repetition detector 112 averages the similarity values for time shifts that are greater than a minimum threshold value and/or less than a maximum threshold value and does not discard mean similarity values associated with one or more time shifts. For example, as part ofstep 516, thepeak repetition detector 112 determines mean similarity values for a subset of the time shifts represented in the heatmap and does not performstep 520. - For example, the peak repetition detector may discard, e.g., not compute a mean value for, all the similarity values for a particular time shift, e.g., diagonal in the
heatmap 600, when audio repetitions for the particular time shift, represented by higher similarity values associated with the particular time shift, typically do not correlate with the audio sample containing music audio data. For example, thepeak repetition detector 112 may discard or otherwise ignore similarity values associated with short time shifts and/or long time shifts. In these implementations, the threshold value may be based on a time length of the received audio sample. -
FIG. 12 is a flow diagram of aprocess 1200 for determining whether an audio sample contains music audio data by finding beats in the audio sample and sustained pitches around each beat. Theprocess 1200 can be used by thebeat pitch detector 114 from thesystem 100. For example, thebeat pitch detector 114 receives an audio sample, e.g., of the environmental audio data, and detects beats and sustained pitches around each beat in the audio sample, where sustained pitches around the beats are more likely to occur in music audio data as compared to an audio sample that does not include music audio data. - The
beat pitch detector 114 receives an audio sample that is associated with audio data (1202). For example, thebeat pitch detector 114 receives t seconds of the audio data as the audio sample. In some examples, the audio sample has a length of between about 0.5 seconds to about 10 seconds. For example, the audio sample may have a length of about 1.1 seconds or about 2.5 seconds. - In some implementations, the
beat pitch detector 114 receives two or more audio samples with a sliding time window. For example, thebeat pitch detector 114 continuously receives audio samples having overlapping time periods. For example, the sliding window can be 2.5 seconds. - The
beat pitch detector 114 computes a spectrogram of the received audio sample (1204). For example, thebeat pitch detector 114 computes a time-varying spectral representation of the received audio sample, such as aspectrogram 1300 shown inFIG. 13 . - In some examples, the
beat pitch detector 114 computes the spectrogram of the received audio sample utilizing a fast Fourier transform (FFT). In some examples, an associated window length of the spectrogram is 128 milliseconds. In some implementations, the windows from the received audio sample do not overlap. - In some implementations, the
beat pitch detector 114 computes a chromogram. In some examples, thebeat pitch detector 114 utilizes autocorrelation in processing the received audio sample. - Returning to
FIG. 12 , thebeat pitch detector 114 determines one or more horizontal peaks in the spectrogram (1206). For example, thebeat pitch detector 114 determines the horizontal peaks in the spectrogram by comparing each point in the spectrogram with the point's neighbors to the left and to the right of the point. - In some implementations, the
beat pitch detector 114 compares each point in thespectrogram 1300 with the point's neighbors that are associated with the same frequency or frequency bin. For example, thebeat pitch detector 114 compares a first point with the two points to the left of the first point and the two points to the right of the first point where all of the points are associated with the same frequency bin but represent different instances in time to determine whether the first point is a peak. - The
beat pitch detector 114 generates a sparse representation of the spectrogram (1208). For example, based on identifying the horizontal peaks in the spectrogram, thebeat pitch detector 114 adjusts the values of each of the peaks. In some implementations, thebeat pitch detector 114 adjusts the amplitude values associated with each of the peaks to a value of one. In some implementations, thebeat pitch detector 114 adjusts the values of the other points of the spectrogram, e.g., the non-peak amplitudes, to a value of zero. For example, thebeat pitch detector 114 generates ahorizontal peak spectrogram 1400, shown inFIG. 14 , where the horizontal peaks are represented by values of one, e.g., white, and the non-peaks are represented by values of zero, e.g., black. - In some implementations, the
horizontal peak spectrogram 1400 represents a binary spectrogram of a music audio sample. For example, a horizontal peak spectrogram associated with non-music audio data may include a more uniform distribution of horizontal peaks than a horizontal peak spectrogram associated with music audio data. - Returning to
FIG. 12 , thebeat pitch detector 114 filters one or more frequency bands of the spectrogram (1210). For example, thebeat pitch detector 114 may filter one or more frequencies of thehorizontal peak spectrogram 1400. In some implementations, thebeat pitch detector 114 removes frequencies below 150 Hz and frequencies above 4000 Hz from the spectrogram. In certain implementations, thebeat pitch detector 114 removes frequencies between about 850 Hz and about 2650 Hz from the spectrogram. For example, thebeat pitch detector 114 creates an updated spectrogram with frequency ranges between about 150 Hz to about 850 Hz and between about 2650 Hz to about 4000 Hz. - The
beat pitch detector 114 detects one or more beats in the spectrogram (1212). For example, for each time slice in thehorizontal peak spectrogram 1400, thebeat pitch detector 114 determines the number of horizontal peaks in the time slice. If at least a predetermined percentage of the frequency bins for the time slice include horizontal peaks, then thebeat pitch detector 114 classifies the time slice as including a beat. For example, the beat pitch detector generates abeat spectrogram 1500, shown inFIG. 15 based on the beats detected in thehorizontal peak spectrogram 1400. - In some implementations, the predetermined percentage is based on the type of music included in the audio sample. In certain implementations, the predetermined percentage is based on the types of instruments that generated the audio sample. In some examples, the predetermined percentage is 33%. For example, when the
beat pitch detector 114 determines that at least ⅓ of the frequency bins for a particular time slice include horizontal peaks, then thebeat pitch detector 114 classifies that particular time slice as including a beat. In other examples, the predetermined percentage is 50%. - Returning to
FIG. 12 , thebeat pitch detector 114 determines one or more vertical peaks in the spectrogram (1214). For example, thebeat pitch detector 114 determines the vertical peaks in the in thespectrogram 1300, shown inFIG. 13 , by comparing each point in the spectrogram with the point's neighbors above and below the point. - In some implementations, the
beat pitch detector 114 compares each point in thespectrogram 1300 with the point's neighbors that are associated with the same time slice. For example, thebeat pitch detector 114 compares a first point with the five points above the first point and the five points below the first point where all of the points are associated with the same time slice to determine whether the first point is a vertical peak. - In some implementations, the
beat pitch detector 114 generates avertical peak spectrogram 1600, shown inFIG. 16 , from thespectrogram 1300 based on the determined vertical peaks. In some implementations, thevertical peak spectrogram 1600 represents the vertical peaks with a value of one, e.g., white, and the non-peaks with a value of zero, e.g., black. - Returning to
FIG. 12 , thebeat pitch detector 114 detects one or more sustained pitches around the beats (1216). For example, thebeat pitch detector 114 uses the vertical peaks in thevertical peak spectrogram 1600 to determine the sustained pitches around the beats in thebeat spectrogram 1500. - For example, a beat pitch graph 1700 a-b, shown in
FIGS. 17A-B , represents a combination of thehorizontal peak spectrogram 1400 and thevertical peak spectrogram 1600 and depicts the horizontal peaks, selected in time, and the vertical peaks, selected in frequency, of the received audio sample. Here, the vertical columns in the beat pitch graphs 1700 a-b with a higher density of horizontal peaks correspond to the beats presented in thebeat spectrogram 1500. - For each specific time slice in the received audio sample that contains a beat, the
beat pitch detector 114 finds the vertical peaks that are in the neighboring time slices and the specific time slice. For example, thebeat pitch detector 114 determines a quantity of vertical peaks in the neighboring time slices and the specific time slice. - In some implementations, when determining the quantity of vertical peaks in the neighboring time slices and the specific time slice, the
beat pitch detector 114 analyzes each of the frequency bins for the specific time slice separately. For example, thebeat pitch detector 114 forms an 11×1 window around a frequency bin where the window is centered on the specific time slice that contains a beat. Thebeat pitch detector 114 then determines a score representing the quantity of vertical peaks in the window. For example, thebeat pitch detector 114 may count the total number of vertical peaks in the window for a maximum score of eleven, i.e., based on the size of the window, such as the width, for each of the frequency bins associated with the specific time slice. - For example, the
beat pitch detector 114 may determine, for each of the beats, one or more windows associated with the beat, each window centered on the beat, associated with a row in the spectrogram, and with the same predetermined width, e.g., eleven. Thebeat pitch detector 114 may determine, for each of the windows, a quantity of vertical peaks in the window. - Returning to
FIG. 12 , thebeat pitch detector 114 determines a highest score associated with each of the time slices that contain a beat (1218). For example, thebeat pitch detector 114 compares the scores associated with each of the frequency bins for the specific time slice and selects the highest score as the score for the specific time slice. - For example, the
beat pitch detector 114 may determine, for each of the beats, a highest score associated with the beat, the highest score based on the highest quantity of vertical peaks in one of the windows associated with the beat, such that a score for each of the beats comprises the highest score for the beat. - The
beat pitch detector 114 determines a beat pitch score for the audio sample (1220). For example, thebeat pitch detector 114 combines the scores for each of the time slices that contain a beat to determine the beat pitch score. In one example, thebeat pitch detector 114 sums the scores, i.e., the highest scores, associated with each of the time slices that contain a beat to determine the beat pitch score. - Specifically, the beat pitch score is a measure of sustained pitches around the beats in the received audio sample. In some implementations, a higher beat pitch score for an audio sample indicates a higher likelihood that the audio sample contains music audio data than if the audio sample was associated with a lower beat pitch score. For example, the music audio data beat
pitch graph 1700 b would be associated with a higher beat pitch score than the non-music audio data beatpitch graph 1700 a based on a higher occurrence of vertical peaks in horizontal lines around the horizontal peaks in the music audio data beatpitch graph 1700 b. - The order of the steps in the
process 1200 described above is illustrative only, and the finding of sustained pitches around each beat can be performed in different orders. For example, thebeat pitch detector 114 may determine the vertical peaks, e.g., performstep 1214, prior to determining the horizontal peaks, e.g., performstep 1206. - In some implementations, the
process 1200 can include additional steps, fewer steps, or some of the steps can be divided into multiple steps. For example, when a music detector performs theprocess 1200 in conjunction with theprocess 200 or theprocess 500 on the same audio sample, the music detector may receive the audio sample, and compute a spectrogram once. - Further, the
process 1200 may include additional steps described above with reference to theprocess 200 or theprocess 500. For example, thebeat pitch detector 114 may correct thespectrogram 1300 for spectral tilt. - In some implementations, the
process 1200 does not generate thehorizontal peak spectrogram 1400 and/or filter one or more frequency bands of the spectrogram. For example, thebeat pitch detector 114 may determine the horizontal peaks without generating thehorizontal peak spectrogram 1400. - In some implementations,
step 1216 is divided into multiple steps. For example, thebeat pitch detector 114 may find the vertical peaks around each beat, determine, for each beat, the number of vertical peaks in each frequency bin around the specific beat, select the highest score associated with one of the frequency bins for the specific beat, and combine the highest scores for all of the beats included in the received audio sample to determine a beat pitch score for the received audio sample. - In some implementations, the
linear classifier 116 uses input from a frame entropy engine when determining whether an audio sample contains music audio data or non-music audio data. For example, the frame entropy engine is one of theadditional detectors 120 and detects music audio data in an audio sample by measuring the median Shannon entropy of a spectral slice. - In certain implementations, the frame entropy engine measures the Shannon entropy of a spectral slice by performing the following steps. For example, the frame entropy engine receives an audio sample and computes a spectrogram of the audio sample. The audio sample may be between about 0.5 seconds and about 5.0 seconds long. In some implementations, the frame entropy engine uses a fast Fourier transform to compute the spectrogram. The frame entropy engine may use a window length of 128 milliseconds when computing the spectrogram. The windows used to compute the spectrogram may overlap. For example, the frame entropy engine may use windows with a ¾ overlap.
- The frame entropy engine may correct the spectrogram for spectral tilt. The frame entropy engine may correct the spectrogram for overweight lower frequencies. For example, the frame entropy engine may multiple each row or frequency bin in the spectrogram with the square root of the zero-based row index, e.g., the square root of the frequency value, as described above with reference to
FIG. 2 . - The frame entropy engine may filter low frequencies and high frequencies from the spectrogram. For example, the frame entropy engine may remove the frequencies, or frequency bins associated with frequencies, lower than 170 Hz and higher than 2200 Hz from the spectrogram, creating a filtered spectrogram associated with frequencies between 170 Hz to 2200 Hz, inclusive.
- For each spectral slice in the spectrogram, the frame entropy engine may compute the Shannon entropy of the linear magnitude of the spectral slice. The frame entropy engine may use any appropriate algorithm to determine the Shannon entropy of a spectral slice.
- The frame entropy engine computes a frame entropy score for the audio sample based on the Shannon entropy values associated with the spectral slices in the audio sample. For example, the frame entropy score is the negative of the median Shannon entropy value associated with the spectral slices for the audio sample.
- The frame entropy score is a representation of whether the audio sample contains music or non-music audio data. For example, music tends to have a lower spectral slice Shannon entropy than non-music, and higher frame entropy scores may be associated with music audio data compared to non-music audio data when the frame entropy scores are the negative of the median Shannon entropy value associated with the spectral slices for an audio sample.
- The frame entropy engine may use any other appropriate algorithm to determine the entropy of each of the spectral slices for an audio sample and generate a frame entropy score based on the entropy values that is representative of whether the audio sample contains music or non-music audio data.
- In some implementations, the
linear classifier 116 uses input from an energy entropy engine when determining whether an audio sample contains music audio data or non-music audio data. For example, the energy entropy engine is one of theadditional detectors 120 and measures the audio energy entropy of the audio sample. - In certain implementations, the energy entropy engine measures the audio energy entropy of an audio sample by performing the following steps. For example, the energy entropy engine receives an audio sample and creates one or more windows associated with the audio sample. The audio sample may be between about 0.5 and about 5.0 seconds long. In some implementations, the energy entropy engine creates non-overlapping windows for the audio sample. In some examples, the windows are about 90 milliseconds long.
- The energy entropy engine computes the energy of each window as the sum of the squared signal. For example, the entropy engine computes the sum of each squared value in a window as sum=s1 2 30 s2 2+s3 2+ . . . . In some examples, the energy entropy engine measures the amplitude of the signal in each window. In some examples, the energy entropy engine uses the root mean square (RMS) to measure the amplitude of the signal in each window from the audio sample.
- The energy entropy engine combines the computed energies to determine an energy entropy score. For example, the energy entropy engine determines the Shannon entropy of the obtained energies of each of the windows and uses the determined Shannon entropy as the energy entropy score for the received audio sample. The energy entropy score is a representation of whether the audio sample contains music or non-music audio data. The energy entropy engine may use any other appropriate algorithm to determine the energy entropy of an audio sample.
- In some implementations, the
linear classifier 116 uses input from a spectral centroid engine when determining whether an audio sample contains music audio data or non-music audio data. For example, the spectral centroid engine is one of theadditional detectors 120 and computes the midpoint of the spectral energy distribution of a sound in an audio sample, e.g., the balance point of the spectrum in the audio sample. - In certain implementations, the spectral centroid engine computes the midpoint of the spectral energy distribution of a sound in an audio sample by performing the following steps. For example, the spectral centroid engine receives an audio sample and computes a spectrogram of the audio sample. The audio sample may be between about 0.5 and about 5.0 seconds long. In some implementations, the spectral centroid engine uses a fast Fourier transform to compute the spectrogram. The spectral centroid engine may use a window length of 64 milliseconds when computing the spectrogram. The spectral centroid engine may use non-overlapping windows when computing the spectrogram.
- The spectral centroid engine may correct the spectrogram for spectral tilt as described with reference to
FIG. 2 above. - For each spectral slice in the spectrogram, the spectral centroid engine may compute the spectral centroid for the respective spectral slice. The spectral centroid may be computed as the sum of the frequencies weighted by the linear magnitudes and divided by the sum of the linear magnitudes.
- The spectral centroid engine determines a spectral centroid score for the audio sample based on the spectral centroid values. For example, the spectral centroid score may be the median spectral centroid associated with the spectral slices for the audio sample over a time window.
- The spectral centroid score is a representation of whether an audio sample contains music or non-music audio data. The spectral centroid engine may use any other appropriate algorithm to determine the spectral centroid of an audio sample.
- In some implementations, the
linear classifier 116 uses input from a zero crossing rate engine when determining whether an audio sample contains music audio data or non-music audio data. For example, the zero crossing rate engine is one of theadditional detectors 120 and may measure the average zero crossing rate of the audio sample. - In certain implementations, the zero crossing rate engine measures the average zero crossing rate of an audio sample by performing the following steps. For example, the zero crossing rate engine receives an audio sample and creates one or more windows associated with the audio sample. The audio sample may be between about 0.5 seconds and about 5.0 seconds long. In some implementations, the windows are about 32 milliseconds long. The zero crossing rate engine may use non-overlapping windows.
- For each window, the zero crossing rate engine determines the zero crossing rate. The zero crossing rate may be determined by dividing the quantity of signal zero crossings by the quantity of signal samples. In some examples, the zero crossing rate is a number between 0.0 and 1.0.
- The zero crossing rate engine determines a zero crossing rate score. For example, the zero crossing rate score is the median zero crossing rate for the audio sample.
- The zero crossing rate score is a representation of whether an audio sample contains music or non-music audio data. The zero crossing rate engine may use any other appropriate algorithm to determine the zero crossing rate of an audio sample.
- The
linear classifier 116 receives input from one or more detectors and determines whether an audio sample contains music or non-music audio data based on the received input. For example, thelinear classifier 116 receives the spectral fluctuation score, the peak repetition score, the beat pitch score, the frame entropy score, the energy entropy score, the spectral centroid score, and the zero crossing rate score and combines the scores to determine whether the audio sample contains music or non-music audio data. - In some implementations, the
linear classifier 116 receives input from a subset of thespectral fluctuation detector 110, thepeak repetition detector 112, thebeat pitch detector 114, and theadditional detectors 120, e.g., the frame entropy detector, the energy entropy engine, the spectral centroid engine, and the zero crossing rate engine. In some implementations, thelinear classifier 116 receives input from other detectors that generate scores indicative of whether an audio sample contains music or non-music audio data. - The
linear classifier 116 may receive input from one or more detectors that indicate the state of the environmental audio data. For example, the linear classifier engine may use the input to determine information about a song recorded in an audio sample. In one example, thelinear classifier 116 may determine the name and/or the artist of the song. - In some implementations when determining whether an audio sample contains music or non-music audio data, the
linear classifier 116 determines a linear separation in space based on the input from one or more detectors. For example, thelinear classifier 116 determines a linear plane in space that represents a received audio sample and uses the linear plane to determine whether the received audio sample contains music audio data or non-music audio data. - For example, the
linear classifier 116 uses scores from the various detectors, and potentially one or more additional detectors not described above, as feature values to determine whether an audio sample contains music audio data or non-music audio data. Thelinear classifier 116 may use quadratic expansion on the feature values to determine whether the audio sample contains music audio data or non-music audio data. - In one example, the
linear classifier 116 computes the following quadratic feature vector, shown in Table 1, from the detector scores, where each of the feature values fv represents one of the detector scores. Thelinear classifier 116 may use the result of the quadratic feature vector shown in Table 1 to determine whether an audio sample contains music audio data or non-music audio data. -
TABLE 1 quadratic_feature_vector = [ fv0, fv1, ..., fv(n−1), fvn, fv0 2, fv1 2, .., fv(n−1) 2, fvn 2, fv0*fv1, fv0*fv2, ..., fv0*fv(n−1), fv0*fvn, fv1*fv2, fv1*fv3, ..., fv1*fv(n−1), fv1*fvn, ... fv(n−2)*fv(n−1), fv(n−2)*fvn, fv(n−1)*fvn] - In some implementations, the
linear classifier 116 uses a plurality of quadratic feature vectors as training data. For example, for a specific audio sample that has been classified as containing music audio data or non-music audio data, the linear classifier receives the output from the various detectors for the specific audio sample, generates a quadratic feature vector for the specific audio sample, and uses an indication of whether or not the specific audio sample contains music audio data or non-music audio data to learn how to classify other audio samples. During training, thelinear classifier 116 may repeat this process for a predetermined number of difference audio samples. - The
linear classifier 116 may use any appropriate algorithm to combine the input from one or more detectors and determine whether an audio sample contains music audio data or non-music audio data. For example, thelinear classifier 116 may use the beat pitch score to determine whether the audio sample contains music audio data or non-music audio data without modifying or combining the beat pitch score with another score. In another example, thelinear classifier 116 uses another machine learning algorithm to determine whether the audio sample contains music audio data or non-music audio data. - Various implementations of the systems and techniques described here may be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations may include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs, also known as programs, software, software applications or code, include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” “computer-readable medium” refers to any computer program product, apparatus and/or device, e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs), used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- To provide for interaction with a user, the systems and techniques described here may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- The systems and techniques described here may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the systems and techniques described here, or any combination of such back end, middleware, or front end components. The components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), and the Internet.
- The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- An example of one such type of computer is shown in
FIG. 18 , which shows a schematic diagram of ageneric computer system 1800. Thesystem 1800 can be used for the operations described in association with any of the computer-implement methods described previously, according to one implementation. Thesystem 1800 includes aprocessor 1810, amemory 1820, astorage device 1830, and an input/output device 1840. Each of thecomponents system bus 1850. Theprocessor 1810 is capable of processing instructions for execution within thesystem 1800. In one implementation, theprocessor 1810 is a single-threaded processor. In another implementation, theprocessor 1810 is a multi-threaded processor. Theprocessor 1810 is capable of processing instructions stored in thememory 1820 or on thestorage device 1830 to display graphical information for a user interface on the input/output device 1840. - The
memory 1820 stores information within thesystem 1800. In one implementation, thememory 1820 is a computer-readable medium. In one implementation, thememory 1820 is a volatile memory unit. In another implementation, thememory 1820 is a non-volatile memory unit. - The
storage device 1830 is capable of providing mass storage for thesystem 1800. In one implementation, thestorage device 1830 is a computer-readable medium. In various different implementations, thestorage device 1830 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device. - The input/output device 1840 provides input/output operations for the
system 1800. In one implementation, the input/output device 1840 includes a keyboard and/or pointing device. In another implementation, the input/output device 1840 includes a display unit for displaying graphical user interfaces. - While this disclosure includes some specifics, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features of example implementations of the disclosure. Certain features that are described in this disclosure in the context of separate implementations can also be provided in combination in a single implementation. Conversely, various features that are described in the context of a single implementation can also be provided in multiple implementations separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Thus, particular implementations of the present disclosure have been described. Other implementations are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. For example, various forms of the flows shown above may be used, with steps re-ordered, added, or removed. Accordingly, other implementations are within the scope of the following claims.
Claims (20)
1. A computer implemented method comprising:
receiving, by an audio classification system, an audio sample that is associated with audio data;
computing, by the audio classification system, a spectrogram of the received audio sample, the spectrogram including one or more spectral slices;
comparing, by the audio classification system for each of the one or more spectral slices in the spectrogram, the respective spectral slice with each of the spectral slices other than the respective spectral slice in the spectrogram;
determining, by the audio classification system for each of the one or more spectral slices in the spectrogram, a plurality of similarity values that each represent a similarity between the respective spectral slice and one of the spectral slices other than the respective spectral slice in the spectrogram using the respective comparison, for each of the one or more spectral slices in the spectrogram, of the respective spectral slice with one of the spectral slices other than the respective spectral slice in the spectrogram;
determining, by the audio classification system for each time shift between slices in the spectrogram, a mean similarity value based on the similarity values associated with the time shift;
generating, by the audio classification system, a projection from the mean similarity values;
smoothing, by the audio classification system, the projection;
determining, by the audio classification system, a density of one or more local peaks in the smoothed projection using a quantity of the local peaks in the smoothed projection and a length of the smoothed projection;
determining, by the audio classification system, a peak repetition score that indicates a likelihood that the audio sample contains music audio data based on the density of the one or more local peaks in the smoothed projection;
determining, by the audio classification system, whether the peak repetition score satisfies a threshold score; and
classifying, by the audio classification system, the audio sample as containing music audio data or not containing music audio data based on determining whether the peak repetition score satisfies the threshold score.
2. The method of claim 1 , further comprising:
determining, for each of the one or more spectral slices in the spectrogram, one or more peaks;
generating a sparse representation of the spectrogram based on the peaks;
wherein determining, for each of the one or more spectral slices in the spectrogram, the plurality of similarity values comprises determining a plurality of similarity values that each represent a similarity between the spectral slice and one of the spectral slices other than the respective spectral slice in the sparse spectrogram.
3. The method of claim 2 , wherein the peaks comprise amplitude peaks.
4. The method of claim 1 , further comprising:
filtering one or more mean similarity values from the projection;
wherein smoothing the projection comprises smoothing the filtered projection.
5. The method of claim 4 , wherein determining the peak repetition score that indicates a likelihood that the audio sample contains music audio data comprises determining the peak repetition score based on a maximum of the filtered projection, a sample standard deviation of the filtered projection, and the density of the one or more local peaks in the smoothed projection.
6. The method of claim 4 , wherein filtering the one or more mean similarity values from the projection comprises filtering the one or more mean similarity values based on the time shifts associated with the one or more mean similarity values.
7. The method of claim 1 , wherein computing the spectrogram comprises:
creating a plurality of frequency bins for the audio sample; and
generating the spectrogram from the frequency bins.
8. The method of claim 1 , further comprising filtering one or more frequency bands from the spectrogram.
9. A non-transitory computer storage medium encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:
receiving, by an audio classification system, an audio sample that is associated with audio data;
computing, by the audio classification system, a spectrogram of the received audio sample, the spectrogram including one or more spectral slices;
comparing, by the audio classification system for each of the one or more spectral slices in the spectrogram, the respective spectral slice with each of the spectral slices other than the respective spectral slice in the spectrogram;
determining, by the audio classification system for each of the one or more spectral slices in the spectrogram, a plurality of similarity values that each represent a similarity between the respective spectral slice and one of the spectral slices other than the respective spectral slice in the spectrogram using the respective comparison, for each of the one or more spectral slices in the spectrogram, of the respective spectral slice with one of the spectral slices other than the respective spectral slice in the spectrogram;
determining, by the audio classification system for each time shift between slices in the spectrogram, a mean similarity value based on the similarity values associated with the time shift;
generating, by the audio classification system, a projection from the mean similarity values;
smoothing, by the audio classification system, the projection;
determining, by the audio classification system, a density of one or more local peaks in the smoothed projection using a quantity of the local peaks in the smoothed projection and a length of the smoothed projection;
determining, by the audio classification system, a peak repetition score that indicates a likelihood that the audio sample contains music audio data based on the density of the one or more local peaks in the smoothed projection;
determining, by the audio classification system, whether the peak repetition score satisfies a threshold score; and
classifying, by the audio classification system, the audio sample as containing music audio data or not containing music audio data based on determining whether the peak repetition score satisfies the threshold score.
10. The computer storage medium of claim 9 , the operations further comprising:
determining, for each of the one or more spectral slices in the spectrogram, one or more peaks;
generating a sparse representation of the spectrogram based on the peaks;
wherein determining, for each of the one or more spectral slices in the spectrogram, the plurality of similarity values comprises determining a plurality of similarity values that each represent a similarity between the spectral slice and one of the spectral slices other than the respective spectral slice in the sparse spectrogram.
11. The computer storage medium of claim 10 , wherein the peaks comprise amplitude peaks.
12. The computer storage medium of claim 9 , the operations further comprising:
filtering one or more mean similarity values from the projection;
wherein smoothing the projection comprises smoothing the filtered projection.
13. The computer storage medium of claim 12 , determining the peak repetition score that indicates a likelihood that the audio sample contains music audio data comprises determining the peak repetition score based on a maximum of the filtered projection, a sample standard deviation of the filtered projection, and the density of the one or more local peaks in the smoothed projection.
14. The computer storage medium of claim 12 , wherein filtering the one or more mean similarity values from the projection comprises filtering the one or more mean similarity values based on the time shifts associated with the one or more mean similarity values.
15. An audio classification system comprising:
one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:
receiving an audio sample that is associated with audio data;
computing a spectrogram of the received audio sample, the spectrogram including one or more spectral slices;
comparing, for each of the one or more spectral slices in the spectrogram, the respective spectral slice with each of the spectral slices other than the respective spectral slice in the spectrogram;
determining, for each of the one or more spectral slices in the spectrogram, a plurality of similarity values that each represent a similarity between the respective spectral slice and one of the spectral slices other than the respective spectral slice in the spectrogram using the respective comparison, for each of the one or more spectral slices in the spectrogram, of the respective spectral slice with one of the spectral slices other than the respective spectral slice in the spectrogram;
determining, for each time shift between slices in the spectrogram, a mean similarity value based on the similarity values associated with the time shift;
generating a projection from the mean similarity values;
smoothing the projection;
determining a density of one or more local peaks in the smoothed projection using a quantity of the local peaks in the smoothed projection and a length of the smoothed projection;
determining a peak repetition score that indicates a likelihood that the audio sample contains music audio data based on the density of the one or more local peaks in the smoothed projection;
determining whether the peak repetition score satisfies a threshold score; and
classifying the audio sample as containing music audio data or not containing music audio data based on determining whether the peak repetition score satisfies the threshold score.
16. The system of claim 15 , the operations further comprising:
determining, for each of the one or more spectral slices in the spectrogram, one or more peaks;
generating a sparse representation of the spectrogram based on the peaks;
wherein determining, for each of the one or more spectral slices in the spectrogram, the plurality of similarity values comprises determining a plurality of similarity values that each represent a similarity between the spectral slice and one of the spectral slices other than the respective spectral slice in the sparse spectrogram.
17. The system of claim 15 , wherein the peaks comprise amplitude peaks.
18. The system of claim 15 , the operations further comprising:
filtering one or more mean similarity values from the projection;
wherein smoothing the projection comprises smoothing the filtered projection.
19. The system of claim 18 , wherein determining the peak repetition score that indicates a likelihood that the audio sample contains music audio data comprises determining the peak repetition score based on a maximum of the filtered projection, a sample standard deviation of the filtered projection, and the density of the one or more local peaks in the smoothed projection.
20. The system of claim 18 , wherein filtering the one or more mean similarity values from the projection comprises filtering the one or more mean similarity values based on the time shifts associated with the one or more mean similarity values.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/932,198 US20160322066A1 (en) | 2013-02-12 | 2013-07-01 | Audio Data Classification |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361763668P | 2013-02-12 | 2013-02-12 | |
US13/932,198 US20160322066A1 (en) | 2013-02-12 | 2013-07-01 | Audio Data Classification |
Publications (1)
Publication Number | Publication Date |
---|---|
US20160322066A1 true US20160322066A1 (en) | 2016-11-03 |
Family
ID=57205556
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/932,158 Active 2037-10-29 US10424321B1 (en) | 2013-02-12 | 2013-07-01 | Audio data classification |
US13/932,198 Abandoned US20160322066A1 (en) | 2013-02-12 | 2013-07-01 | Audio Data Classification |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/932,158 Active 2037-10-29 US10424321B1 (en) | 2013-02-12 | 2013-07-01 | Audio data classification |
Country Status (1)
Country | Link |
---|---|
US (2) | US10424321B1 (en) |
Cited By (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150370892A1 (en) * | 2014-06-23 | 2015-12-24 | Sony Corporation | System and method for audio identification |
CN108831492A (en) * | 2018-05-21 | 2018-11-16 | 广州国音科技有限公司 | A kind of method, apparatus, equipment and readable storage medium storing program for executing handling voice data |
US20190206102A1 (en) * | 2017-12-29 | 2019-07-04 | Facebook, Inc. | Systems and methods for enhancing content |
CN110516109A (en) * | 2019-08-30 | 2019-11-29 | 腾讯科技（深圳）有限公司 | Correlating method, device and the storage medium of music label |
US10761802B2 (en) | 2017-10-03 | 2020-09-01 | Google Llc | Identifying music as a particular song |
US11017774B2 (en) | 2019-02-04 | 2021-05-25 | International Business Machines Corporation | Cognitive audio classifier |
AU2018241963B2 (en) * | 2017-03-31 | 2021-08-12 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V. | Apparatus and method for determining a predetermined characteristic related to a spectral enhancement processing of an audio signal |
CN113257276A (en) * | 2021-05-07 | 2021-08-13 | 普联国际有限公司 | Audio scene detection method, device, equipment and storage medium |
US11138992B2 (en) * | 2017-11-22 | 2021-10-05 | Tencent Technology (Shenzhen) Company Limited | Voice activity detection based on entropy-energy feature |
US11298072B2 (en) * | 2016-07-01 | 2022-04-12 | Bostel Technologies, Llc | Dermoscopy diagnosis of cancerous lesions utilizing dual deep learning algorithms via visual and audio (sonification) outputs |
CN114339392A (en) * | 2021-11-12 | 2022-04-12 | 腾讯科技（深圳）有限公司 | Video editing method and device, computer equipment and storage medium |
US11388343B2 (en) * | 2018-08-17 | 2022-07-12 | SZ DJI Technology Co., Ltd. | Photographing control method and controller with target localization based on sound detectors |
US11484247B2 (en) | 2016-07-01 | 2022-11-01 | Bostel Technologies, Llc | Phonodermoscopy, a medical device system and method for skin diagnosis |
US20220415341A1 (en) * | 2018-05-10 | 2022-12-29 | Nippon Telegraph And Telephone Corporation | Pitch emphasis apparatus, method and program for the same |
US20230215460A1 (en) * | 2022-01-06 | 2023-07-06 | Microsoft Technology Licensing, Llc | Audio event detection with window-based prediction |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2022040282A1 (en) | 2020-08-18 | 2022-02-24 | Dolby Laboratories Licensing Corporation | Audio content identification |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6785645B2 (en) * | 2001-11-29 | 2004-08-31 | Microsoft Corporation | Real-time speech and music classifier |
US7091409B2 (en) * | 2003-02-14 | 2006-08-15 | University Of Rochester | Music feature extraction using wavelet coefficient histograms |
US8428949B2 (en) * | 2008-06-30 | 2013-04-23 | Waves Audio Ltd. | Apparatus and method for classification and segmentation of audio content, based on the audio signal |
JP5282548B2 (en) * | 2008-12-05 | 2013-09-04 | ソニー株式会社 | Information processing apparatus, sound material extraction method, and program |
US8886531B2 (en) | 2010-01-13 | 2014-11-11 | Rovi Technologies Corporation | Apparatus and method for generating an audio fingerprint and using a two-stage query |
JP5907511B2 (en) | 2010-06-09 | 2016-04-26 | アデルフォイ リミテッド | System and method for audio media recognition |
US9099064B2 (en) * | 2011-12-01 | 2015-08-04 | Play My Tone Ltd. | Method for extracting representative segments from music |
US9052991B2 (en) * | 2012-11-27 | 2015-06-09 | Qualcomm Incorporated | System and method for audio sample rate conversion |
-
2013
- 2013-07-01 US US13/932,158 patent/US10424321B1/en active Active
- 2013-07-01 US US13/932,198 patent/US20160322066A1/en not_active Abandoned
Cited By (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150370892A1 (en) * | 2014-06-23 | 2015-12-24 | Sony Corporation | System and method for audio identification |
US10162888B2 (en) * | 2014-06-23 | 2018-12-25 | Sony Interactive Entertainment LLC | System and method for audio identification |
US11484247B2 (en) | 2016-07-01 | 2022-11-01 | Bostel Technologies, Llc | Phonodermoscopy, a medical device system and method for skin diagnosis |
US11298072B2 (en) * | 2016-07-01 | 2022-04-12 | Bostel Technologies, Llc | Dermoscopy diagnosis of cancerous lesions utilizing dual deep learning algorithms via visual and audio (sonification) outputs |
AU2018241963B2 (en) * | 2017-03-31 | 2021-08-12 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e.V. | Apparatus and method for determining a predetermined characteristic related to a spectral enhancement processing of an audio signal |
US10761802B2 (en) | 2017-10-03 | 2020-09-01 | Google Llc | Identifying music as a particular song |
US10809968B2 (en) | 2017-10-03 | 2020-10-20 | Google Llc | Determining that audio includes music and then identifying the music as a particular song |
US11256472B2 (en) | 2017-10-03 | 2022-02-22 | Google Llc | Determining that audio includes music and then identifying the music as a particular song |
US11138992B2 (en) * | 2017-11-22 | 2021-10-05 | Tencent Technology (Shenzhen) Company Limited | Voice activity detection based on entropy-energy feature |
US20190206102A1 (en) * | 2017-12-29 | 2019-07-04 | Facebook, Inc. | Systems and methods for enhancing content |
US20230386498A1 (en) * | 2018-05-10 | 2023-11-30 | Nippon Telegraph And Telephone Corporation | Pitch emphasis apparatus, method and program for the same |
US11749295B2 (en) * | 2018-05-10 | 2023-09-05 | Nippon Telegraph And Telephone Corporation | Pitch emphasis apparatus, method and program for the same |
US20220415341A1 (en) * | 2018-05-10 | 2022-12-29 | Nippon Telegraph And Telephone Corporation | Pitch emphasis apparatus, method and program for the same |
CN108831492A (en) * | 2018-05-21 | 2018-11-16 | 广州国音科技有限公司 | A kind of method, apparatus, equipment and readable storage medium storing program for executing handling voice data |
US11388343B2 (en) * | 2018-08-17 | 2022-07-12 | SZ DJI Technology Co., Ltd. | Photographing control method and controller with target localization based on sound detectors |
US11017774B2 (en) | 2019-02-04 | 2021-05-25 | International Business Machines Corporation | Cognitive audio classifier |
CN110516109A (en) * | 2019-08-30 | 2019-11-29 | 腾讯科技（深圳）有限公司 | Correlating method, device and the storage medium of music label |
CN113257276A (en) * | 2021-05-07 | 2021-08-13 | 普联国际有限公司 | Audio scene detection method, device, equipment and storage medium |
CN114339392A (en) * | 2021-11-12 | 2022-04-12 | 腾讯科技（深圳）有限公司 | Video editing method and device, computer equipment and storage medium |
US20230215460A1 (en) * | 2022-01-06 | 2023-07-06 | Microsoft Technology Licensing, Llc | Audio event detection with window-based prediction |
US11948599B2 (en) * | 2022-01-06 | 2024-04-02 | Microsoft Technology Licensing, Llc | Audio event detection with window-based prediction |
Also Published As
Publication number | Publication date |
---|---|
US10424321B1 (en) | 2019-09-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10424321B1 (en) | Audio data classification | |
EP2828856B1 (en) | Audio classification using harmonicity estimation | |
JP2022173437A (en) | Volume leveler controller and controlling method | |
US9390727B2 (en) | Detecting distorted audio signals based on audio fingerprinting | |
US20220201125A1 (en) | Howl detection in conference systems | |
EP2816550A1 (en) | Audio signal analysis | |
WO2020181824A1 (en) | Voiceprint recognition method, apparatus and device, and computer-readable storage medium | |
US20140358265A1 (en) | Audio Processing Method and Audio Processing Apparatus, and Training Method | |
US20140350923A1 (en) | Method and device for detecting noise bursts in speech signals | |
US9646592B2 (en) | Audio signal analysis | |
US20040216585A1 (en) | Generating a music snippet | |
US8779271B2 (en) | Tonal component detection method, tonal component detection apparatus, and program | |
EP3246920B1 (en) | Method and apparatus for detecting correctness of pitch period | |
Camacho | On the use of auditory models' elements to enhance a sawtooth waveform inspired pitch estimator on telephone-quality signals | |
US20230050565A1 (en) | Audio detection method and apparatus, computer device, and readable storage medium | |
CN107533848B (en) | The system and method restored for speech | |
US8901407B2 (en) | Music section detecting apparatus and method, program, recording medium, and music signal detecting apparatus | |
CN111341333B (en) | Noise detection method, noise detection device, medium, and electronic apparatus | |
Muhammad | Extended average magnitude difference function based pitch detection | |
CN107969164B (en) | Adaptive inter-channel discrimination rescaling filter | |
CN112669797B (en) | Audio processing method, device, electronic equipment and storage medium | |
CN111312287B (en) | Audio information detection method, device and storage medium | |
Slaney | An introduction to auditory model inversion | |
Brandt et al. | Automatic detection of hum in audio signals | |
US9351089B1 (en) | Audio tap detection |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SHARIFI, MATTHEW;ROBLEK, DOMINIK;REEL/FRAME:031338/0340Effective date: 20130628 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044144/0001Effective date: 20170929 |