BR112016009772B1 - METHOD, MACHINE READABLE NON-TRANSITORY STORAGE MEDIA AND SERVER COMPUTER SYSTEM FOR INTERACTIVE AUDIO AND VIDEO PLAYBACK FROM MULTIPLE VIEWPOINTS - Google Patents
METHOD, MACHINE READABLE NON-TRANSITORY STORAGE MEDIA AND SERVER COMPUTER SYSTEM FOR INTERACTIVE AUDIO AND VIDEO PLAYBACK FROM MULTIPLE VIEWPOINTS Download PDFInfo
- Publication number
- BR112016009772B1 BR112016009772B1 BR112016009772-6A BR112016009772A BR112016009772B1 BR 112016009772 B1 BR112016009772 B1 BR 112016009772B1 BR 112016009772 A BR112016009772 A BR 112016009772A BR 112016009772 B1 BR112016009772 B1 BR 112016009772B1
- Authority
- BR
- Brazil
- Prior art keywords
- media items
- video
- media
- audio
- interactive
- Prior art date
Links
Abstract
REPRODUÇÃO INTERATIVA DE ÁUDIO E VÍDEO DE MÚLTIPLOS PONTOS DE VISTA. A presente invenção refere-se a um módulo de múltiplos pontos de vista interativo que identifica uma pluralidade de itens de mídia associados com um evento do mundo real, sendo que cada um dentre a pluralidade de itens de mídia compreende uma porção de vídeo e uma porção de áudio. O módulo de múltiplos pontos de vista interativo sincroniza as porções de áudio de cada um dentre a pluralidade de itens de mídia de acordo com uma linha de tempo de referência comum, determina uma posição geográfica relativa associada com cada um dentre a pluralidade de itens de mídia e apresenta a pluralidade de itens de mídia em uma interface de reprodutor de múltiplos pontos de vista interativa com base pelo menos nas porções de áudio sincronizadas e nas posições geográficas relativas.INTERACTIVE AUDIO AND VIDEO PLAYBACK FROM MULTIPLE POINTS OF VIEW. The present invention relates to an interactive multiview module that identifies a plurality of media items associated with a real-world event, each of the plurality of media items comprising a video portion and an audio portion. The interactive multi-view module synchronizes the audio portions of each of the plurality of media items according to a common reference timeline, determines a relative geographic position associated with each of the plurality of media items, and presents the plurality of media items in an interactive multi-view player interface based on at least the synchronized audio portions and the relative geographic positions.
Description
[001] A presente invenção refere-se ao campo de serviços de visualização de mídia e, em particular, à reprodução interativa de áudio e vídeo de múltiplos pontos de vista.[001] The present invention relates to the field of media visualization services and, in particular, the interactive reproduction of audio and video from multiple points of view.
[002] Na Internet, plataformas de compartilhamento de conteúdo ou outros aplicativos permitem que usuários transfiram por upload, visualizem e compartilhem conteúdo digital tal como itens de mídia. Tais itens de mídia podem incluir clipes de áudio, clipes de filme, clipes de TV e vídeos musicais, assim como conteúdo amador tal como blogs de vídeo, vídeos originais curtos, imagens, fotos, outros conteúdos multimídia, etc. Os usuários podem usar dispositivos de computação (tais como telefones inteligentes, telefones celulares, computadores portáteis, computadores de mesa, computadores do tipo netbook, computadores do tipo tablet) para usar, jogar, e/ou consumir os itens de mídia (por exemplo, assistir vídeos digitais, ouvir música digital).[002] On the Internet, content sharing platforms or other applications allow users to upload, view and share digital content such as media items. Such media items may include audio clips, movie clips, TV clips and music videos, as well as amateur content such as video blogs, short original videos, images, photos, other multimedia content, etc. Users may use computing devices (such as smart phones, cell phones, laptop computers, desktop computers, netbook-type computers, tablet-type computers) to use, play, and/or consume media items (for example, watch digital videos, listen to digital music).
[003] Com a popularidade de compartilhamento de vídeo e plataformas da web sociais, há uma quantidade crescente de vídeos gerados por usuários. Para eventos com diversos participantes, tais como eventos esportivos ou concertos, muitos vídeos são transferidos por upload, cobrindo pontos de vista diferentes e momentos diferentes de tempo. Os sites de compartilhamento de vídeo atuais tentam recomendar e classificar esses vídeos a fim de dar a um usuário todos os vídeos disponíveis para um evento. Entretanto, todo o conteúdo de vídeo tipicamente está disponível em uma lista não organizada de resultados de consulta de busca e o período e ponto de vista precisos do vídeo referente ao evento são perdidos. Desse modo, um usuário irá tender a simplesmente visualizar um único vídeo do evento, que os permite ver um portal pequeno para o evento que foi visível a partir de um único criador de conteúdo.[003] With the popularity of video sharing and social web platforms, there is an increasing amount of user-generated videos. For events with multiple participants, such as sporting events or concerts, many videos are uploaded, covering different points of view and different moments of time. Today's video sharing sites try to recommend and rank these videos in order to give a user all available videos for an event. However, all video content is typically available in an unorganized list of search query results and the precise time period and point of view of the video referring to the event is lost. Thus, a user will tend to simply view a single video of the event, which allows them to see a small portal to the event that was viewable from a single content creator.
[004] O seguinte é um sumário simplificado da revelação a fim de fornecer uma compreensão básica de alguns aspectos da revelação. Esta descrição resumida não é uma visão geral extensiva da revelação. Não se tem por objetivo identificar elementos chave ou críticos da revelação, nem delinear qualquer escopo das implantações particulares da revelação ou qualquer escopo das concretizações. Seu único propósito é apresentar alguns conceitos da revelação de uma forma simplificada como um prelúdio para a descrição mais detalhada que é apresentada posteriormente.[004] The following is a simplified summary of the revelation in order to provide a basic understanding of some aspects of the revelation. This summary description is not an extensive overview of the revelation. It is not intended to identify key or critical elements of the disclosure, nor to delineate any scope of particular deployments of the disclosure or any scope of embodiments. Its sole purpose is to present some concepts from the revelation in a simplified form as a prelude to the more detailed description that is presented later.
[005] Em uma implantação, um módulo de múltiplos pontos de vista interativo identifica uma pluralidade de itens de mídia associados com um evento do mundo real, sendo que cada um dentre a pluralidade de itens de mídia compreende uma porção de vídeo e uma porção de áudio. O módulo de múltiplos pontos de vista interativo sincroniza as porções de áudio de cada um dentre a pluralidade de itens de mídia de acordo com uma linha de tempo de referência comum, determina uma posição geográfica relativa associada com cada um dentre a pluralidade de itens de mídia e apresenta a pluralidade de itens de mídia em uma interface de reprodutor de múltiplos pontos de vista interativa com base pelo menos nas porções de áudio sincronizadas e nas posições geográficas relativas. Em uma implantação, o módulo de múltiplos pontos de vista interativo também gera um único item de mídia editado associado com o evento do mundo real, combinando-se a pluralidade de itens de mídia com base nas porções de áudio sincronizadas.[005] In one deployment, an interactive multiview module identifies a plurality of media items associated with a real-world event, each of the plurality of media items comprising a video portion and an audio portion. The interactive multi-view module synchronizes the audio portions of each of the plurality of media items according to a common reference timeline, determines a relative geographic position associated with each of the plurality of media items, and presents the plurality of media items in an interactive multi-view player interface based on at least the synchronized audio portions and the relative geographic positions. In one deployment, the interactive multiview module also generates a single edited media item associated with the real-world event, combining the plurality of media items based on the synchronized audio portions.
[006] A fim de identificar a pluralidade de itens de mídia associados com o evento do mundo real, o módulo de múltiplos pontos de vista interativo determina itens de mídia que têm metadados que identificam o evento do mundo real. Para sincronizar as porções de áudio de cada um dentre a pluralidade de itens de mídia, o módulo de múltiplos pontos de vista interativo determina um deslocamento temporal para cada um dentre a pluralidade de itens de mídia que maximiza ou de outra forma aumenta a correlação entre o espectrograma de áudio com base em frequência de cada porção de áudio, em que os deslocamentos temporais representam localizações na linha de tempo de referência comum. Determinar a posição geográfica relativa associada com cada um dentre a pluralidade de itens de mídia inclui o módulo de múltiplos pontos de vista interativo que determina as posições relativas de câmeras separadas usadas para capturar cada um dentre a pluralidade de itens de mídia em um período do evento do mundo real com base em uma pluralidade de pontos visíveis em cada um dentre a pluralidade de itens de mídia.[006] In order to identify the plurality of media items associated with the real-world event, the interactive multiple views module determines media items that have metadata that identify the real-world event. To synchronize the audio portions of each of the plurality of media items, the interactive multiview module determines a temporal offset for each of the plurality of media items that maximizes or otherwise increases the correlation between the frequency-based audio spectrogram of each audio portion, where the temporal offsets represent locations on the common reference timeline. Determining the relative geographic position associated with each of the plurality of media items includes the interactive multiple viewpoints module that determines the relative positions of separate cameras used to capture each of the plurality of media items over a period of the real-world event based on a plurality of points visible on each of the plurality of media items.
[007] Em uma implantação, a fim de apresentar a pluralidade de itens de mídia na interface de reprodutor de múltiplos pontos de vista interativa, o módulo de múltiplos pontos de vista interativo exibe um ícone que representa cada um dentre a pluralidade de itens de mídia, em que cada ícone é exibido em uma localização com base na posição geográfica relativa correspondente. O módulo de múltiplos pontos de vista interativo recebe uma seleção de usuário de um ícone que representa um primeiro item de mídia da pluralidade de itens de mídia e inicia a reprodução do primeiro item de mídia. Subsequentemente, o módulo de múltiplos pontos de vista interativo recebe, em um período durante a reprodução do primeiro item de mídia que corresponde a um primeiro ponto na linha de tempo de referência comum, uma seleção de usuário de um ícone que representa um segundo item de mídia da pluralidade de itens de mídia, e inicia a reprodução do segundo item de mídia em um período no segundo item de mídia que corresponde ao primeiro ponto na linha de tempo de referência comum. Em uma implantação, o módulo de múltiplos pontos de vista interativo prevê um item de mídia subsequente, que será selecionado pelo usuário, e armazena em armazenamento temporário o item de mídia subsequente antes de uma seleção de usuário do item de mídia subsequente.[007] In a deployment, in order to present the plurality of media items in the interactive multi-view player interface, the interactive multi-view module displays an icon representing each of the plurality of media items, wherein each icon is displayed in a location based on the corresponding relative geographic position. The interactive multi-view module receives a user selection of an icon representing a first media item of the plurality of media items and initiates playback of the first media item. Subsequently, the interactive multi-view module receives, at a period during playback of the first media item corresponding to a first point on the common reference timeline, a user selection of an icon representing a second media item of the plurality of media items, and initiates playback of the second media item at a period on the second media item corresponding to the first point on the common reference timeline. In one deployment, the interactive multiview module previews a subsequent media item, which will be selected by the user, and caches the subsequent media item prior to a user selection of the subsequent media item.
[008] A presente revelação é ilustrada a título de exemplo, e não de limitação, nas figuras dos desenhos em anexo.[008] The present disclosure is illustrated by way of example, and not limitation, in the figures of the attached drawings.
[009] A Figura 1 é um diagrama de blocos que ilustra uma arquitetura de rede exemplificativa em que implantações da presente invenção podem ser implantadas.[009] Figure 1 is a block diagram illustrating an exemplary network architecture in which deployments of the present invention can be deployed.
[0010] A Figura 2 é um diagrama de blocos que ilustra um módulo de múltiplos pontos de vista interativo, de acordo com algumas implantações.[0010] Figure 2 is a block diagram illustrating an interactive multiple views module, in accordance with some deployments.
[0011] A Figura 3 é um diagrama de blocos que ilustra um fluxo de processamento de múltiplos pontos de vista interativo, de acordo com algumas implantações.[0011] Figure 3 is a block diagram illustrating an interactive multi-view processing flow, as per some deployments.
[0012] A Figura 4 é um fluxograma que ilustra um método para geração de vídeo de múltiplos pontos de vista interativo, de acordo com algumas implantações.[0012] Figure 4 is a flowchart illustrating a method for generating video from multiple interactive viewpoints, in accordance with some deployments.
[0013] A Figura 5A é um diagrama que ilustra espectrogramas de frequência que correspondem a dois itens de mídia referentes a um evento comum, de acordo com uma implantação.[0013] Figure 5A is a diagram illustrating frequency spectrograms that correspond to two media items referring to a common event, according to a deployment.
[0014] A Figura 5B é um diagrama que ilustra um gráfico de linha para a pontuação de correlação entre os espectrogramas de frequência ilustrados na Figura 5A, de acordo com uma implantação.[0014] Figure 5B is a diagram illustrating a line graph for the correlation score between the frequency spectrograms illustrated in Figure 5A, according to a deployment.
[0015] As Figuras 6A e 6B são diagramas que ilustram exemplos de duas possíveis apresentações da interface de reprodução de múltiplos pontos de vista interativa, de acordo com algumas implantações.[0015] Figures 6A and 6B are diagrams illustrating examples of two possible presentations of the interactive multi-view playback interface, according to some deployments.
[0016] A Figura 7 é um diagrama que ilustra uma linha do tempo de previsão e armazenagem em armazenamento temporário, de acordo com algumas implantações.[0016] Figure 7 is a diagram illustrating a timeline of forecasting and staging in staging, according to some deployments.
[0017] A Figura 8 é um diagrama de blocos que ilustra um sistema de computador exemplificativo, de acordo com algumas implantações.[0017] Figure 8 is a block diagram illustrating an exemplary computer system, in accordance with some implementations.
[0018] As implantações são descritas para reprodução interativa de áudio e vídeo de múltiplos pontos de vista. Em uma modalidade, um sistema de múltiplos pontos de vista interativo organiza dados de vídeo tomados do mesmo evento do mundo real em uma experiência interativa, fácil de compreender e intuitiva recuperando-se pelo menos dois pedaços de informações dos vídeos que normalmente são perdidos no processo de aquisição/transferência por upload. Essas informações podem incluir, por exemplo, as posições das câmeras quando capturaram o evento e a sincronização dos vídeos de acordo com uma linha de tempo de referência comum. Após a recuperação da posição e da sincronização de tempo dos vídeos, as informações espaciais permitem que o sistema crie novas interfaces de navegação com base em mapa, ligando vídeos a suas localizações em 3D do mundo real. A sincronização de tempo permite uma transição suave de diferentes pontos de vista de um evento, ao invés do tipo de lista de reprodução típica de transição disponível da maioria de sites de compartilhamento hoje em dia. Ligar os vídeos em espaço e tempo permite que um usuário procure ao longo da linha de tempo do evento além da duração de um único vídeo e adicionalmente pode permitir que um usuário mude interativamente o ponto de vista para ter a sensação do evento de uma localização diferente. Um evento do mundo real pode ser qualquer evento que ocorre no mundo real (por exemplo, não na internet) que pode ser observado e capturado (por exemplo, em fotografias e em vídeo).[0018] Deployments are described for interactive playback of audio and video from multiple viewpoints. In one embodiment, an interactive multi-point-of-view system organizes video data taken from the same real-world event into an interactive, easy-to-understand and intuitive experience by recovering at least two pieces of information from the videos that are normally lost in the acquisition/upload process. This information can include, for example, the positions of the cameras when they captured the event and the synchronization of the videos according to a common reference timeline. After retrieving the position and time synchronization of the videos, the spatial information allows the system to create new map-based navigation interfaces, linking videos to their real-world 3D locations. Time synchronization allows for smooth transition of different views of an event, rather than the typical transitional playlist type available from most sharing sites these days. Linking the videos in space and time allows a user to search along the event timeline beyond the duration of a single video and additionally can allow a user to interactively change the point of view to get the feel of the event from a different location. A real-world event can be any event that occurs in the real world (eg not on the internet) that can be observed and captured (eg in photographs and on video).
[0019] Em uma implantação, o presente sistema de múltiplos pontos de vista interativo supera limitações em largura de banda de transmissão de protocolo de Internet utilizando-se sinais de entrada baseados em interação de usuário, retroalimentações sociais e qualidade de vídeo para prever o próximo ponto de vista de vídeo com a maior probabilidade de ser selecionado pelo usuário, reduzindo assim a largura de banda enquanto garante uma troca de vídeo suave. Tal experiência de reprodução interativa de áudio e vídeo de múltiplos pontos de vista tem aplicações fora de concertos e eventos esportivos. Por exemplo, em outras implantações, vídeos produzidos coletivamente podem ser usados para aprimorar vigilância, o conteúdo gerado por usuário pode ser acoplado à filmagem difundida, ou tutoriais e dicas de truques para aprender uma nova habilidade podem ser apresentados a partir de múltiplos ângulos. A interface pode também ser acoplada a um único vídeo de resumo editado (por exemplo, um corte do diretor, ou uma combinação obtida de todos os vídeos), ou os múltiplos vídeos podem ser usados para melhorar a qualidade de vídeos individuais no conjunto (por exemplo, aprimorando o áudio).[0019] In one deployment, the present interactive multi-view system overcomes Internet Protocol transmission bandwidth limitations by using input signals based on user interaction, social feedback, and video quality to predict the next video view most likely to be selected by the user, thus reducing bandwidth while ensuring smooth video switching. Such an interactive multi-view audio and video playback experience has applications outside of concerts and sporting events. For example, in other deployments, collectively produced videos can be used to enhance surveillance, user-generated content can be coupled with broadcast footage, or tutorials and tricks for learning a new skill can be presented from multiple angles. The interface can either be coupled to a single edited summary video (e.g. a director's cut, or a combination taken from all videos), or the multiple videos can be used to improve the quality of individual videos in the set (e.g. by enhancing the audio).
[0020] Em uma implantação, um sistema completo para organizar e apresentar conteúdo de vídeo de múltiplos pontos de vista é revelado. A reprodução oferecida por tal sistema pode ser denominada como vídeo de ponto de vista livre. Os sistemas existentes para essa tarefa são restritos a equipamentos de captura de laboratório ou estúdios de múltiplos pontos de vista, em que a pose em 3D da câmera e a sincronização são facilmente controladas (por exemplo, métodos com base em bastão para calibração). Gravações difundidas de tais eventos esportivos têm o mesmo benefício de serem facilmente calibradas antecipadamente, o que vem permitindo que tais efeitos de ponto de vista livre e similares a matriz sejam usados em filmagem difundida de tais eventos. O ponto de vista livre também inclui uma síntese de ponto de vista, isto é, gerar um novo ponto de vista sintético com o uso de pontos de vista de câmera físicas existentes. Desse modo, em uma implantação, se os pontos de vista fornecidos por usuário estiverem próximos o bastante, a síntese de ponto de vista pode ser uma possível aplicação em cima da estrutura descrita.[0020] In one deployment, a complete system for organizing and presenting video content from multiple viewpoints is unveiled. The reproduction offered by such a system can be called free point of view video. Existing systems for this task are restricted to laboratory capture equipment or multi-point of view studios, where 3D camera pose and timing are easily controlled (eg, stick-based methods for calibration). Broadcast recordings of such sporting events have the same benefit of being easily calibrated in advance, which has allowed such matrix-like and free point of view effects to be used in broadcast footage of such events. Free viewpoint also includes viewpoint synthesis, that is, generating a new synthetic viewpoint using existing physical camera viewpoints. Thus, in a deployment, if the user-supplied viewpoints are close enough, the viewpoint synthesis can be a possible application on top of the described structure.
[0021] A Figura 1 é um diagrama de blocos que ilustra uma arquitetura de rede 100 exemplificativa em que implantações da presente revelação podem ser implantadas. Em uma implantação, a arquitetura de rede 100 inclui dispositivos-cliente 110A até 110Z, uma rede 105, um armazenamento de dados 106, uma plataforma de compartilhamento de conteúdo 120, um servidor 130, uma plataforma de conexão social 140, uma plataforma de e-mail 150 e uma plataforma de busca 160. Em uma implantação, a rede 105 pode incluir uma rede pública (por exemplo, a Internet) uma rede privada (por exemplo, uma rede local (LAN) ou rede de longa distância (WAN)), uma rede com fio (por exemplo, rede Ethernet), uma rede sem fio (por exemplo, uma rede 802.11 ou uma rede Wi-Fi), uma rede celular (por exemplo, uma rede de Evolução a Longo Prazo (LTE)), roteadores, hubs, chaves, computadores de servidor e/ou uma combinação dos mesmos. Em uma implantação, o armazenamento de dados 106 pode ser uma memória (por exemplo, memória de acesso aleatório), um cache, uma unidade (por exemplo, um disco rígido), uma unidade flash, um sistema de banco de dados, ou outro tipo de componente ou dispositivo com capacidade para armazenar dados. O armazenamento de dados 106 pode também incluir múltiplos componentes de armazenamento (por exemplo, múltiplas unidades ou múltiplos bancos de dados) que podem também abranger múltiplos dispositivos de computação (por exemplo, múltiplos computadores de servidor).[0021] Figure 1 is a block diagram illustrating an exemplary network architecture 100 in which deployments of the present disclosure may be deployed. In one deployment, the network architecture 100 includes client devices 110A through 110Z, a network 105, a data store 106, a content sharing platform 120, a server 130, a social connection platform 140, an email platform 150, and a search platform 160. , a local area network (LAN) or wide area network (WAN)), a wired network (for example, an Ethernet network), a wireless network (for example, an 802.11 network or a Wi-Fi network), a cellular network (for example, a Long Term Evolution (LTE) network), routers, hubs, switches, server computers, and/or a combination thereof. In one deployment, data storage 106 may be memory (e.g., random access memory), a cache, a drive (e.g., hard drive), flash drive, database system, or other type of component or device capable of storing data. Data storage 106 may also include multiple storage components (eg, multiple drives or multiple databases) which may also span multiple computing devices (eg, multiple server computers).
[0022] Os dispositivos-cliente 110A até 110Z podem, cada um, incluir dispositivos de computação tais como computadores pessoais (PCs), computadores portáteis, telefones móveis, telefones inteligentes, computadores do tipo tablet, computadores do tipo netbook, etc. Cada dispositivo-cliente pode incluir um visualizador de mídia 111. Em uma implantação, o visualizador de mídia 111 pode ser um aplicativo que permite que usuários visualizem conteúdo, tal como imagens, vídeos, páginas da web, documentos, etc. Por exemplo, o visualizador de mídia 111 pode ser um navegador da web que pode acessar, reaver, apresentar, e/ou navegar conteúdo (por exemplo, páginas da web tais como páginas de Linguagem de Marcação de Hipertexto (HTML), itens de mídia digital, etc.) servidos por um servidor de web. O visualizador de mídia 111 pode renderizar, exibir e/ou apresentar o conteúdo (por exemplo, uma página da web, um visualizador de mídia) para um usuário. O visualizador de mídia 111 pode também exibir um reprodutor de mídia incorporado (por exemplo, um reprodutor Flash® ou um reprodutor HTML5) que é incorporado em uma página da web (por exemplo, uma página da web que pode fornecer informações sobre um produto vendido por um comerciante online). Em outro exemplo, o visualizador de mídia 111 pode ser um aplicativo autônomo que permite que usuários visualizem itens de mídia digital (por exemplo, vídeos digitais, imagens digitais, livros eletrônicos). O visualizador de mídia 111 pode ser fornecido aos dispositivos-cliente 110A até 110Z pelo servidor 130 e/ou pela plataforma de compartilhamento de conteúdo 120. Por exemplo, o visualizador de mídia 111 pode ser um reprodutor de mídia incorporado que é incorporado em uma página da web fornecida pela plataforma de compartilhamento de conteúdo 120. Em outro exemplo, o visualizador de mídia 111 pode ser um aplicativo que é transferido por download de um servidor 130.[0022] The client devices 110A to 110Z may each include computing devices such as personal computers (PCs), portable computers, mobile phones, smart phones, tablet-type computers, netbook-type computers, etc. Each client device may include a media viewer 111. In one deployment, the media viewer 111 may be an application that allows users to view content such as images, videos, web pages, documents, etc. For example, media viewer 111 may be a web browser that can access, retrieve, display, and/or navigate content (e.g., web pages such as HyperText Markup Language (HTML) pages, digital media items, etc.) served by a web server. Media viewer 111 can render, display and/or present content (e.g., a web page, a media viewer) to a user. Media viewer 111 can also display an embedded media player (e.g., a Flash® player or an HTML5 player) that is embedded in a web page (e.g., a web page that can provide information about a product sold by an online merchant). In another example, media viewer 111 may be a standalone application that allows users to view digital media items (eg, digital videos, digital images, electronic books). Media viewer 111 may be provided to client devices 110A through 110Z by server 130 and/or content sharing platform 120. For example, media viewer 111 may be an embedded media player that is embedded in a web page provided by content sharing platform 120. In another example, media viewer 111 may be an application that is downloaded from a server 130.
[0023] Em uma implantação, a plataforma de compartilhamento de conteúdo 120 pode incluir um ou mais dispositivos de computação (tais como um servidor de montagem em rack, um computador de roteador, um computador de servidor, um computador pessoal, um computador principal, um computador portátil, um computador do tipo tablet, um computador do tipo desktop, etc.), armazenamentos de dados (por exemplo, discos rígidos, memórias, bancos de dados), redes, componentes de software, e/ou componentes de hardware que podem ser usados para fornecer acesso a um usuário a itens de mídia e/ou fornecer os itens de mídia ao usuário. Por exemplo, a plataforma de compartilhamento de conteúdo 120 pode permitir que um usuário consuma, transmita por upload, busque, aprove ("curta"), não curta, e/ou comente em itens de mídia. A plataforma de compartilhamento de conteúdo 120 pode também incluir um site da web (por exemplo, uma página da web) que pode ser usada para fornecer acesso a um usuário aos itens de mídia. A plataforma de compartilhamento de conteúdo 120 pode incluir múltiplas listas de evento (por exemplo, listas de evento de A a Z). Cada lista de eventos pode incluir um ou mais itens de mídia 121. Exemplos de um item de mídia 121 podem incluir e não são limitados a vídeos digitais, filmes digitais, fotos digitais, música digital, conteúdo de site da web, atualizações de mídia social, livros eletrônicos (e-books), revistas eletrônicas, jornais digitais, audiolivros digitais, jornais eletrônicos, blogs da web, feeds de sindicação realmente simples (RSS), revistas em quadrinhos eletrônicas, aplicativos de software e similares. Um item de mídia 121 pode ser consumido por meio da Internet e/ou por meio de um aplicativo de dispositivo móvel. Por questões de brevidade e simplicidade, um vídeo online (também doravante denominado vídeo) é usado como um exemplo de um item de mídia por todo este documento. Conforme usado no presente documento, "mídia", "itens de mídia", "itens de mídia online", "mídia digital" e "itens de mídia digital" podem incluir um arquivo eletrônico que pode ser executado ou carregado com o uso de software, firmware ou hardware configurado para apresentar o item de mídia digital a uma entidade. Em uma implantação, a plataforma de compartilhamento de conteúdo 120 pode armazenar os itens de mídia com o uso do armazenamento de dados 106.[0023] In a deployment, the content sharing platform 120 may include one or more computing devices (such as a rackmount server, a router computer, a server computer, a personal computer, a main computer, a portable computer, a tablet computer, a desktop computer, etc.), data stores (e.g., hard drives, memories, databases), networks, software components, and/or hardware components that can be used to provide a user with access to media items and/or or provide the media items to the user. For example, content sharing platform 120 may allow a user to consume, upload, search, approve ("like"), dislike, and/or comment on media items. Content sharing platform 120 may also include a website (eg, a web page) that may be used to provide a user with access to media items. Content sharing platform 120 may include multiple event listings (e.g., A to Z event listings). Each event listing may include one or more media items 121. Examples of a media item 121 may include and are not limited to digital videos, digital films, digital photos, digital music, website content, social media updates, electronic books (e-books), electronic magazines, digital newspapers, digital audiobooks, electronic newspapers, web blogs, really simple syndication (RSS) feeds, electronic comics, software applications, and the like. A media item 121 may be consumed via the Internet and/or via a mobile device application. For brevity and simplicity, an online video (also referred to as video) is used as an example of a media item throughout this document. As used herein, "media", "media items", "online media items", "digital media" and "digital media items" may include an electronic file that can be played or uploaded using software, firmware or hardware configured to present the digital media item to an entity. In one deployment, the content sharing platform 120 may store the media items using data storage 106.
[0024] A plataforma de conexão social 140 pode incluir um ou mais dispositivos de computação (por exemplo, servidores), armazenamentos de dados, redes, componentes de software, e/ou componentes de hardware que podem ser usados para permitir que usuários se conectem, compartilhem informações, e/ou interajam uns com os outros. A plataforma de conexão social 140 pode apresentar a um usuário uma listagem (por exemplo, feed de atividade, feed, fluxo, mural, etc.) de objetos (tais como postagens, itens de conteúdo (por exemplo, vídeo, imagens, áudio, etc.), atualizações de status, indicações de favorabilidade, identificações, mensagens e assim por diante) gerada por outros usuários de uma rede social. A plataforma de conexão social 140 pode também incluir um aspecto de compartilhamento de conteúdo que permite que usuários transfiram por upload, visualizem, identifiquem e compartilhem conteúdo, tal como conteúdo de texto, conteúdo de vídeo, conteúdo de imagem, conteúdo de áudio e assim por diante. Outros usuários da plataforma de conexão social 140 podem comentar sobre o conteúdo compartilhado, descobrir conteúdo novo, localizar atualizações, compartilhar conteúdo e interagir com o conteúdo fornecido de outras formas. Em uma implantação, a plataforma de compartilhamento de conteúdo 120 pode ser integrada com a plataforma de conexão social 140. Por exemplo, a plataforma de conexão social 140 pode usar uma plataforma de compartilhamento de conteúdo 120 para permitir que usuários transfiram por upload e/ou compartilhem conteúdo. Em outra implantação, a plataforma de conexão social 140 pode ser separada da plataforma de compartilhamento de conteúdo 120. Em uma implantação, a plataforma de conexão social 140 pode também incluir uma funcionalidade de bate-papo (por exemplo, uma plataforma de bate-papo) para permitir que usuários batam papo (por exemplo, trocar mensagens instantâneas) uns com os outros.[0024] The social connection platform 140 may include one or more computing devices (e.g., servers), data stores, networks, software components, and/or hardware components that can be used to allow users to connect, share information, and/or interact with each other. The social connection platform 140 may present a user with a listing (e.g., activity feed, feed, stream, wall, etc.) of objects (such as posts, content items (e.g., video, images, audio, etc.), status updates, favoritism nominations, tags, messages, and so on) generated by other users of a social network. The social connection platform 140 may also include a content sharing aspect that allows users to upload, view, tag and share content, such as text content, video content, image content, audio content and so on. Other users of the social connection platform 140 can comment on the shared content, discover new content, find updates, share content, and interact with the provided content in other ways. In one deployment, content sharing platform 120 may be integrated with social connection platform 140. For example, social connection platform 140 may use content sharing platform 120 to allow users to upload and/or share content. In another deployment, the social connection platform 140 may be separate from the content sharing platform 120. In one deployment, the social connection platform 140 may also include chat functionality (e.g., a chat platform) to allow users to chat (e.g., exchange instant messages) with each other.
[0025] Em uma implantação, a plataforma de e-mail 150 pode ser um ou mais dispositivos de computação (por exemplo, servidores), armazenamentos de dados, redes, componentes de software, e/ou componentes de hardware que podem ser usados para permitir que usuários enviem e/ou recebam correio eletrônico (e-mails) uns aos outros. Por exemplo, um primeiro usuário pode usar a plataforma de email 150 para enviar um e-mail para um segundo usuário referente ao tempo e à localização de um evento particular. O primeiro usuário pode também anexar arquivos (por exemplo, arquivos de vídeo, arquivos de imagem, arquivos de texto, etc.) ao e-mail. Em uma implantação, a plataforma de e-mail 150 pode também incluir uma funcionalidade de bate-papo (por exemplo, uma plataforma de bate- papo) para permitir que usuários batam papo (por exemplo, trocar mensagens instantâneas) uns com os outros. Em outra implantação, a plataforma de busca 160 pode ser um ou mais dispositivos de computação, armazenamentos de dados, redes, componentes de software, e/ou componentes de hardware que podem ser usados para permitir que usuários busquem informações e/ou dados. Por exemplo, a plataforma de busca 160 pode permitir que um usuário busque, na Internet e/ou em outras redes, artigos, blogs, sites da web, páginas da web, imagens, vídeos e/ou outros conteúdos relacionados a um tópico particular (por exemplo, como consertar um carro). A plataforma de busca 160 pode também incluir um mecanismo de busca.[0025] In an implementation, the email platform 150 can be one or more computing devices (e.g., servers), data stores, networks, software components, and/or hardware components that can be used to allow users to send and/or receive electronic mail (emails) to each other. For example, a first user can use email platform 150 to send an email to a second user regarding the time and location of a particular event. The first user can also attach files (eg video files, image files, text files, etc.) to the email. In one deployment, the email platform 150 may also include chat functionality (eg, a chat platform) to allow users to chat (eg, exchange instant messages) with each other. In another implementation, search platform 160 can be one or more computing devices, data stores, networks, software components, and/or hardware components that can be used to allow users to search for information and/or data. For example, search platform 160 may allow a user to search the Internet and/or other networks for articles, blogs, websites, web pages, images, videos and/or other content related to a particular topic (e.g., how to fix a car). Search platform 160 may also include a search engine.
[0026] Em uma implantação, o servidor 130 pode incluir um ou mais dispositivos de computação (por exemplo, um servidor de montagem em rack, um computador de servidor, etc.). Em uma implantação, o servidor 130 pode ser incluído em uma ou mais dentre a plataforma de compartilhamento de conteúdo 120, a plataforma de conexão social 140, a plataforma de e-mail 150 e a plataforma de busca 160. Em outra implantação, o servidor 130 pode ser separado da plataforma de compartilhamento de conteúdo 120, da plataforma de conexão social 140, da plataforma de e-mail 150 e da plataforma de busca 160, mas pode se comunicar (por exemplo, trocar dados) com a plataforma de compartilhamento de conteúdo 120, a plataforma de conexão social 140, a plataforma de e-mail 150 e a plataforma de busca 160. Em uma implantação, o servidor 130 inclui um módulo de múltiplos pontos de vista interativo 135. O módulo de múltiplos pontos de vista interativo 135 pode identificar vídeos ou outros itens de mídia associados com um dado evento e organizar os vídeos de uma maneira para fornecer uma experiência de reprodução de múltiplos pontos de vista interativa. Em uma implantação, o módulo de múltiplos pontos de vista interativo 135 identifica vídeos associados com um dado evento do mundo real (por exemplo, vídeos que retratam pelo menos uma porção da ocorrência do evento) e automaticamente sincroniza os vídeos de acordo com uma linha de tempo de referência compartilhada. Além disso, o módulo de múltiplos pontos de vista interativo 135 pode também determinar as posições relativas das câmeras usadas para capturar cada um dos vídeos a fim de exibir os vídeos em uma interface de reprodutor de múltiplos pontos de vista interativa. O módulo de múltiplos pontos de vista interativo 135 pode gerar um único vídeo de "corte do diretor" representativo combinando- se pelo menos uma porção dos vídeos associados com o evento para reprodução ou pode receber entrada de usuário para controlar quais vídeos são reproduzidos na interface de reprodutor de múltiplos pontos de vista interativa. Detalhes adicionais do módulo de múltiplos pontos de vista interativo 135 serão descritos abaixo.[0026] In one deployment, server 130 may include one or more computing devices (eg, a rackmount server, a server computer, etc.). In one deployment, server 130 may be included in one or more of content sharing platform 120, social connection platform 140, email platform 150, and search platform 160. In another deployment, server 130 may be separate from content sharing platform 120, social connection platform 140, email platform 150, and search platform 160, but can communicate with each other (e.g., , exchanging data) with the content sharing platform 120, the social connection platform 140, the email platform 150, and the search platform 160. In one deployment, the server 130 includes an interactive multi-view module 135. The interactive multi-view module 135 can identify videos or other media items associated with a given event and organize the videos in a manner to provide an interactive multi-view playback experience. In one deployment, the interactive multiview module 135 identifies videos associated with a given real-world event (e.g., videos that depict at least a portion of the event's occurrence) and automatically synchronizes the videos according to a shared reference timeline. In addition, the interactive multiview module 135 can also determine the relative positions of the cameras used to capture each of the videos in order to display the videos in an interactive multiview player interface. Interactive multi-viewpoint module 135 can generate a single representative "director's cut" video by combining at least a portion of the videos associated with the event for playback, or can receive user input to control which videos are played in the interactive multi-viewplayer interface. Additional details of the interactive multiple viewpoints module 135 will be described below.
[0027] A Figura 2 é um diagrama de blocos que ilustra o módulo de múltiplos pontos de vista interativo 135, de acordo com uma implantação. Em uma implantação, o módulo de múltiplos pontos de vista interativo 135 inclui o módulo de descoberta de vídeo 202, o módulo de sincronização de áudio 204, o módulo de determinação de posição 206, o módulo de resumo 208 e o módulo de reprodução de múltiplos pontos de vista 210. Essa disposição de módulos e componentes pode ser uma separação lógica e, em outras implantações, esses módulos ou outros componentes podem ser combinados uns com os outros ou separados em componentes adicionais, de acordo com uma implantação particular. Em uma implantação, o armazenamento de dados 106 é conectado ao módulo de múltiplos pontos de vista interativo 135 e inclui itens de mídia 242, dados de lista de eventos 244 e dados de entrada de usuário 246. Em uma implantação, o servidor 130 pode incluir tanto o módulo de múltiplos pontos de vista interativo 135 quanto o armazenamento de dados 106. Em outra implantação, o armazenamento de dados 106 pode ser externo ao servidor 130 e pode ser conectado ao servidor 130 por uma rede ou outra conexão. Em outras implantações, o servidor 130 pode incluir componentes diferentes e/ou adicionais, que não são mostrados para simplificar a descrição. Os dados armazenados 106 podem incluir um ou mais dispositivos de armazenamento em massa que podem incluir, por exemplo, memória flash, discos magnéticos ou ópticos, ou unidades de fita; memória de somente leitura (ROM); memória de acesso aleatório (RAM); memória programável apagável (por exemplo, EPROM e EEPROM); memória flash; ou qualquer outro tipo de meio de armazenamento.[0027] Figure 2 is a block diagram illustrating the interactive multiple viewpoints module 135, according to a deployment. In one deployment, the interactive multi-view module 135 includes the video discovery module 202, the audio synchronization module 204, the position determination module 206, the summary module 208, and the multi-view playback module 210. In one deployment, data store 106 is connected to interactive multiview module 135 and includes media items 242, event list data 244, and user input data 246. In one deployment, server 130 may include both interactive multiview module 135 and data store 106. In another deployment, data store 106 may be external to server 130 and may be connected to server 130 over a network or other connection. In other deployments, server 130 may include different and/or additional components, which are not shown for simplicity of description. Stored data 106 may include one or more mass storage devices which may include, for example, flash memory, magnetic or optical disks, or tape drives; read-only memory (ROM); random access memory (RAM); erasable programmable memory (eg EPROM and EEPROM); flash memory; or any other type of storage medium.
[0028] Em uma implantação, o módulo de descoberta de vídeo 202 identifica itens de mídia 242 associados com um evento do mundo real, tais como um concerto, evento esportivo, ou outro evento. Em uma implantação, o módulo de descoberta de vídeo 202 varre itens de mídia 242 e identifica itens de mídia que têm metadados ou outros sinais de entrada que identificam o evento do mundo real. Os sinais de entrada podem incluir sinais de entrada de mídia obtidos diretamente dos sinais de áudio e/ou vídeo dos itens de mídia (por exemplo, pedaços comuns de informações visuais ou de áudio que aparecem em múltiplos vídeos) ou sinais de entrada de metadados de metadados associados com os itens de mídia. Os sinais de entrada de metadados podem incluir informações no título ou na descrição do vídeo, identificações ou categorias geradas por sistema ou fornecidas por usuário, informações de data e hora associadas com os itens de mídia, informações de geolocalização (por exemplo, dados de GPS) associadas com os itens de mídia, ou outras informações. Mediante a determinação de que um item de mídia 242 particular é associado com um dado evento do mundo real, o módulo de descoberta de vídeo pode adicionar o item de mídia 242 a uma lista de eventos 244 que corresponde ao evento do mundo real.[0028] In one deployment, the video discovery module 202 identifies media items 242 associated with a real-world event, such as a concert, sporting event, or other event. In one deployment, the video discovery module 202 scans media items 242 and identifies media items that have metadata or other input signals that identify the real world event. Input signals can include media input signals taken directly from the audio and/or video signals of media items (e.g., common pieces of visual or audio information that appear in multiple videos) or metadata input signals from metadata associated with the media items. Metadata input signals may include information in the title or description of the video, system-generated or user-provided tags or categories, date and time information associated with the media items, geolocation information (e.g., GPS data) associated with the media items, or other information. Upon determining that a particular media item 242 is associated with a given real-world event, the video discovery module may add the media item 242 to an event list 244 that corresponds to the real-world event.
[0029] Em uma implantação, o módulo de sincronização de áudio 204 sincroniza as porções de áudio de cada um dos itens de mídia 242 em uma dada lista de eventos 244 de acordo com uma linha de tempo de referência comum. Em uma implantação, o módulo de sincronização de áudio determina um deslocamento temporal para cada um dos itens de mídia que aumenta ou maximiza uma correlação para um espectrograma de áudio com base em frequência de cada porção de áudio. Esse deslocamento temporal representa uma localização na linha de tempo de referência comum que indica a hora em que os itens de mídia associados com o evento começam em relação uns aos outros ou em relação à ocorrência do evento do mundo real. Uma vez que os itens de mídia podem ser capturados por usuário, os itens de mídia podem capturar diferentes porções do evento do mundo real. Desse modo, os itens de mídia podem começar e terminar em períodos diferentes e consequentemente podem ter diferentes deslocamentos temporais associados. Em uma implantação, o módulo de sincronização de áudio 204 armazena os deslocamentos temporais nos dados de lista de eventos 244.[0029] In one deployment, the audio synchronization module 204 synchronizes the audio portions of each of the media items 242 in a given event list 244 according to a common reference timeline. In one deployment, the audio sync module determines a temporal offset for each of the media items that increases or maximizes a correlation for an audio spectrogram based on the frequency of each audio chunk. This temporal offset represents a common reference timeline location that indicates the time at which the media items associated with the event start relative to each other or relative to the real-world occurrence of the event. Since media items can be captured on a per-user basis, media items can capture different portions of the real-world event. Thus, media items can start and end at different periods and consequently can have different associated temporal offsets. In an implementation, the audio synchronization module 204 stores the temporal offsets in event list data 244.
[0030] Em uma implantação, o módulo de determinação de posição 206 determina uma posição geográfica relativa associada com cada um dos itens de mídia 242 na lista de eventos 244. Em uma implantação, o módulo de determinação de posição 206 determina as posições relativas de câmeras separadas usadas para capturar cada um dos itens de mídia no período da ocorrência do evento do mundo real. O módulo de determinação de posição 206 pode usar múltiplos pontos visíveis na porção de vídeo de cada um dos itens de mídia a fim de calcular a posição da câmera usada para capturar cada item, em relação a outras posições. Em uma implantação, o módulo de determinação de posição 206 pode plotar essas posições em relação umas às outras e/ou opcionalmente com o uso de uma geografia da localização onde o evento do mundo real ocorreu (por exemplo, um estádio ou arena). Em uma implantação, o módulo de determinação de posição 206 armazena as informações de posição determinadas nos dados de lista de eventos 244.[0030] In a deployment, the position determination module 206 determines a relative geographic position associated with each of the media items 242 in the event list 244. In a deployment, the position determination module 206 determines the relative positions of separate cameras used to capture each of the media items at the time of the occurrence of the real world event. Position determination module 206 can use multiple visible points on the video portion of each of the media items in order to calculate the position of the camera used to capture each item, relative to other positions. In a deployment, the position determination module 206 may plot these positions relative to each other and/or optionally using a geography of the location where the real world event took place (eg, a stadium or arena). In a deployment, the position determination module 206 stores the position determined information in event list data 244.
[0031] Em uma implantação, o módulo de resumo 208 gera um único item de mídia editado associado com o evento do mundo real. Esse item de mídia editado pode ser denominado "corte do diretor" em algumas implantações. O módulo de resumo 208 pode combinar múltiplos itens de mídia 242 uns com os outros para formar o corte do diretor. Por exemplo, o corte do diretor pode incluir seleções particulares que são editadas para estarem em conjunto para mostrar um resumo, visão geral ou outra forma de apresentação do evento do mundo real. Os itens de mídia incluídos no corte do diretor podem ser selecionados manualmente por um curador de conteúdo ou o módulo de resumo 208 pode selecionar automaticamente os itens de mídia com o uso de sinais de entrada de popularidade para cada vídeo, tais como o número de visualizações, comentários, atividade de compartilhamento, etc.[0031] In a deployment, summary module 208 generates a single edited media item associated with the real-world event. This edited media item may be referred to as a "director's cut" in some deployments. The summary module 208 may combine multiple media items 242 with each other to form the director's cut. For example, the director's cut may include particular selections that are edited together to show a summary, overview, or other presentation of the real-world event. Media items included in the director's cut can be manually selected by a content curator or the summary module 208 can automatically select media items using input popularity signals for each video, such as the number of views, comments, sharing activity, etc.
[0032] Em uma implantação, o módulo de reprodução de múltiplos pontos de vista 210 apresenta os itens de mídia 242 em uma interface de reprodutor de múltiplos pontos de vista interativa com base pelo menos nas porções de áudio sincronizadas e nas posições geográficas relativas. Em uma implantação, o módulo de reprodução de múltiplos pontos de vista 210 exibe um ícone que representa cada um dos itens de mídia em uma localização na interface de reprodutor de múltiplos pontos de vista interativa com base na posição geográfica relativa correspondente determinada pelo módulo de determinação de posição 206. O módulo de reprodução de múltiplos pontos de vista 210 recebe uma seleção de usuário de um ícone que representa um dos itens de mídia e começa a reprodução do item de mídia correspondente. O módulo de reprodução de múltiplos pontos de vista 210 pode armazenar uma indicação da seleção de usuário como dados de entrada de usuário. Durante a reprodução do primeiro item de mídia, um usuário pode selecionar um ícone que representa um item de mídia diferente. O módulo de reprodução de múltiplos pontos de vista 210 pode começar a reprodução do segundo item de mídia em um período que corresponde ao ponto na linha de tempo de referência comum de quando a solicitação foi recebida. Desse modo, o segundo item de mídia pode começar a reprodução em um ponto diferente do começo do arquivo de mídia. Como resultado, a partir da perspectiva do visualizador, o ponto de vista muda, mas o período em relação ao evento do mundo real não muda.[0032] In one deployment, the multi-view player module 210 presents the media items 242 in an interactive multi-view player interface based on at least the synchronized audio portions and relative geographic positions. In one deployment, the multi-view playback module 210 displays an icon representing each of the media items at a location on the interactive multi-view player interface based on the corresponding relative geographic position determined by the position determination module 206. The multi-view playback module 210 receives a user selection of an icon representing one of the media items and begins playback of the corresponding media item. The multi-view playback module 210 may store an indication of the user selection as user input data. During playback of the first media item, a user can select an icon representing a different media item. The multi-view playback module 210 may begin playback of the second media item at a period corresponding to the point on the common reference timeline when the request was received. This way, the second media item can start playing at a different point than the beginning of the media file. As a result, from the viewer's perspective, the point of view changes, but the period in relation to the real-world event does not change.
[0033] A Figura 3 é um diagrama de blocos que ilustra um fluxo de processamento de múltiplos pontos de vista interativo, de acordo com uma implantação da presente revelação. Os vários módulos e componentes podem ser descritos em relação a seus papéis na identificação e sincronização de múltiplos vídeos de um dado evento do mundo real para reprodução de múltiplos pontos de vista interativa. Em uma implantação, o fluxo de processamento 300 começa com um estágio de descoberta de vídeo 320 com o uso de vídeos ou outros itens de mídia do banco de dados de vídeo 310. O estágio de descoberta de vídeo inclui a identificação de vídeos associados com um dado evento do mundo real e pode ser realizado pelo módulo de descoberta de vídeo 202. No bloco 330, a sincronização de áudio é realizada com o uso dos vídeos descobertos durante o estágio de descoberta de vídeo 320 e armazenada no banco de dados de vídeo 310. Em uma implantação, o módulo de sincronização de áudio 204 sincroniza as porções de áudio de cada um dos vídeos descobertos de acordo com uma linha de tempo de referência comum conforme descrito acima. A sincronização de áudio 330 pode usar um algoritmo de propagação de crença para sincronizar os clipes de vídeo quanto ao tempo com o uso dos sinais de áudio. Em uma implantação, isso leva em consideração todas as relações de vídeos em pares, que permite que vídeos que estão próximos uns dos outros tenham influência mais forte na sincronização final. No bloco 340, a determinação de posição é realizada com o uso dos vídeos descobertos durante o estágio de descoberta de vídeo 320 e armazenada no banco de dados de vídeo 310. Em uma implantação, o módulo de determinação de posição 206 determina uma posição geográfica relativa associada com cada um dos vídeos. A geometria de múltiplos pontos de vista considera as posições de câmera (e opcionalmente um mapa em 3D) para navegação com base em mapa e pode ser criada manualmente ou automaticamente. Essas informações podem ser extraídas dos vídeos com o uso de algoritmos de visão de computador. Diferentes pontos de vista a partir da mesma posição de câmera, causados por movimento panorâmico ou trepidação de câmera, comumente encontrados em vídeos de cenários esportivos ou de concerto, podem ser usados para auxiliar a extração de comprimento focal. Com base pelo menos nas porções de áudio sincronizadas e nas posições geográficas relativas, no estágio de resumo 350, um único vídeo editado associado com o evento do mundo real pode ser criado. Em uma implantação, o módulo de resumo 208 pode criar um vídeo de "corte do diretor" combinando-se múltiplos vídeos associados com o evento que foram descobertos durante a descoberta de vídeo 320. No resumo 350, a geometria de múltiplos pontos de vista e as métricas de qualidade de sinal (tais como trepidação ou qualidade de vídeo) podem ser utilizadas para produzir um único vídeo de resumo do evento. O estágio de reprodução 360 inclui a apresentação dos vídeos identificados em uma interface de reprodutor de múltiplos pontos de vista interativa com base pelo menos nas porções de áudio sincronizadas e nas posições geográficas relativas. Em uma implantação, o módulo de reprodução de múltiplos pontos de vista 210 apresenta a interface de reprodutor de múltiplos pontos de vista interativa e recebe uma interação de usuário para selecionar quais vídeos exibir. A reprodução e a entrega 360 usam elementos de UI que apresentam ao usuário uma forma para navegar pela coleta de vídeo. Isso pode incluir navegação em 3D, que permite a troca intuitiva entre pontos de vista e as barras de evento de linha de tempo para denotar períodos-chave no vídeo ou regiões durante o evento que têm a maior parte da filmagem. A fim de garantir transições suaves quando um usuário solicita um novo ponto de vista, o sistema usa uma infraestrutura de vídeo existente e tenta prever transições enquanto mantém o uso de largura de banda em um mínimo apenas transferindo conteúdo que realmente será exibido.[0033] Figure 3 is a block diagram illustrating an interactive multi-view processing flow, in accordance with an implementation of the present disclosure. The various modules and components can be described in terms of their roles in identifying and synchronizing multiple videos of a given real-world event for interactive multi-view playback. In one deployment, the processing flow 300 begins with a video discovery stage 320 using videos or other media items from the video database 310. The video discovery stage includes identifying videos associated with a given real-world event and may be performed by the video discovery module 202. In block 330, audio synchronization is performed using the videos discovered during the video discovery stage 320 and stored in the video database 310 In one deployment, the audio synchronization module 204 synchronizes the audio portions of each of the discovered videos according to a common reference timeline as described above. Audio sync 330 may use a belief propagation algorithm to time-sync the video clips using the audio signals. In a deployment, this takes into account all pairwise video relationships, which allows videos that are close to each other to have the strongest influence on the final sync. At block 340, position determination is performed using the videos discovered during the video discovery stage 320 and stored in the video database 310. In a deployment, the position determination module 206 determines a relative geographic position associated with each of the videos. Multipoint geometry takes into account camera positions (and optionally a 3D map) for map-based navigation and can be created manually or automatically. This information can be extracted from videos using computer vision algorithms. Different viewpoints from the same camera position, caused by panning or camera shake, commonly found in sports or concert video footage, can be used to aid focal length extraction. Based on at least the synchronized audio portions and the relative geographic positions, at the summary stage 350, a single edited video associated with the real world event can be created. In one deployment, the summary module 208 may create a "director's cut" video by combining multiple videos associated with the event that were discovered during video discovery 320. In summary 350, multi-view geometry and signal quality metrics (such as judder or video quality) may be used to produce a single event summary video. The playback stage 360 includes presenting the identified videos in an interactive multi-view player interface based on at least the synchronized audio portions and relative geographic positions. In one deployment, the multi-view player module 210 presents the interactive multi-view player interface and receives user interaction to select which videos to display. 360 playback and delivery uses UI elements that present the user with a way to navigate the video collection. This can include 3D navigation, which allows for intuitive switching between viewpoints, and timeline event bars to denote key periods in the video or regions during the event that have the most footage. In order to ensure smooth transitions when a user requests a new view, the system uses an existing video infrastructure and tries to predict transitions while keeping bandwidth usage to a minimum by only transferring content that will actually be displayed.
[0034] A Figura 4 é um fluxograma que ilustra um método para geração de vídeo de múltiplos pontos de vista interativo, de acordo com uma implantação da presente revelação. O método 400 pode ser realizado processando-se uma lógica que compreende hardware (por exemplo, conjuntos de circuito, lógica dedicada, lógica programável, microcódigo, etc.), software (por exemplo, instruções executadas em um dispositivo de processamento para realizar uma simulação de hardware) ou uma combinação dos mesmos. O método 400 pode identificar vídeos ou outros itens de mídia associados com um dado evento do mundo real e apresentar uma interface para reprodução de múltiplos pontos de vista interativa dos itens de mídia. Para simplicidade de explicação, os métodos desta revelação são apresentados e descritos como uma série de atos. Entretanto, os atos de acordo com esta revelação podem ocorrer em várias ordens e/ou concomitantemente, e com outros atos não apresentados e descritos no presente documento. Adicionalmente, é possível que nem todos os atos ilustrados sejam necessários para implantar os métodos de acordo com a matéria revelada. Além disso, as pessoas versadas na técnica compreenderão e verificarão que os métodos podem ser alternativamente representados como uma série de estados interrelacionados por meio de um diagrama de estado ou eventos. Adicionalmente, deve-se verificar que os métodos revelados neste relatório descritivo têm capacidade para serem armazenados em um artigo de fabricação para facilitar o transporte e a transferência de tais métodos para dispositivos de computação. O termo "artigo de fabricação" conforme usado no presente documento está destinado a abranger um programa de computador acessível a partir de qualquer dispositivo legível por computador ou meio de armazenamento. Em uma implantação, o método 400 pode ser realizado pelo módulo de múltiplos pontos de vista interativo 135 conforme mostrado nas Figuras 1 e 2.[0034] Figure 4 is a flow chart illustrating a method for generating interactive multi-view video, in accordance with an implementation of the present disclosure. Method 400 may be performed by processing logic comprising hardware (e.g., circuit assemblies, dedicated logic, programmable logic, microcode, etc.), software (e.g., instructions executed in a processing device to perform a hardware simulation), or a combination thereof. Method 400 can identify video or other media items associated with a given real-world event and provide an interface for interactive multi-view playback of the media items. For simplicity of explanation, the methods of this revelation are presented and described as a series of acts. However, acts pursuant to this disclosure may occur in various orders and/or concurrently, and with other acts not shown and described herein. Additionally, it is possible that not all illustrated acts are required to implement the methods in accordance with the revealed matter. Furthermore, those skilled in the art will understand and appreciate that the methods may alternatively be represented as a series of interrelated states via a state or event diagram. Additionally, it should be noted that the methods disclosed in this specification are capable of being stored in an article of manufacture to facilitate transport and transfer of such methods to computing devices. The term "article of manufacture" as used herein is intended to encompass a computer program accessible from any computer-readable device or storage medium. In one deployment, method 400 may be performed by interactive multiple viewpoints module 135 as shown in Figures 1 and 2.
[0035] Com referência à Figura 4, no bloco 410, o método 400 identifica itens de mídia associados com um evento do mundo real. Em uma implantação, o módulo de descoberta de vídeo 202 identifica itens de mídia 242 associados com um evento do mundo real, tal como um concerto, evento esportivo, ou outro evento. Em uma implantação, o módulo de descoberta de vídeo 202 varre itens de mídia 242 e identifica itens de mídia que têm metadados ou outros sinais de entrada que identificam o evento do mundo real. Mediante a determinação de que um item de mídia 242 particular é associado com um dado evento do mundo real, o módulo de descoberta de vídeo pode adicionar o item de mídia 242 a uma lista de eventos 244 que corresponde ao evento do mundo real.[0035] Referring to Figure 4, at block 410, method 400 identifies media items associated with a real-world event. In one deployment, the video discovery module 202 identifies media items 242 associated with a real-world event, such as a concert, sporting event, or other event. In one deployment, the video discovery module 202 scans media items 242 and identifies media items that have metadata or other input signals that identify the real world event. Upon determining that a particular media item 242 is associated with a given real-world event, the video discovery module may add the media item 242 to an event list 244 that corresponds to the real-world event.
[0036] No bloco 420, o método 400 sincroniza porções de áudio dos itens de mídia de acordo com uma linha de tempo de referência comum. Em uma implantação, o módulo de sincronização de áudio 204 sincroniza as porções de áudio de cada um dos itens de mídia 242 em uma dada lista de eventos 244 de acordo com uma linha de tempo de referência comum. Em uma implantação, o módulo de sincronização de áudio determina um deslocamento temporal para cada um dos itens de mídia que aumenta ou maximiza uma correlação para um espectrograma de áudio com base em frequência de cada porção de áudio. Esse deslocamento temporal representa uma localização na linha de tempo de referência comum que indica a hora em que os itens de mídia associados com o evento começam em relação uns aos outros ou em relação à ocorrência do evento do mundo real.[0036] In block 420, method 400 synchronizes audio portions of the media items according to a common reference timeline. In one deployment, the audio synchronization module 204 synchronizes the audio portions of each of the media items 242 in a given event list 244 according to a common reference timeline. In one deployment, the audio sync module determines a temporal offset for each of the media items that increases or maximizes a correlation for an audio spectrogram based on the frequency of each audio chunk. This temporal offset represents a common reference timeline location that indicates the time at which the media items associated with the event start relative to each other or relative to the real-world occurrence of the event.
[0037] No bloco 430, o método 400 determina uma posição geográfica relativa associada com cada item de mídia. Em uma implantação, o módulo de determinação de posição 206 determina uma posição geográfica relativa associada com cada um dos itens de mídia 242 na lista de eventos 244. Em uma implantação, o módulo de determinação de posição 206 determina as posições relativas de câmeras separadas usadas para capturar cada um dos itens de mídia no período da ocorrência do evento do mundo real. O módulo de determinação de posição 206 pode usar múltiplos pontos visíveis na porção de vídeo de cada um dos itens de mídia a fim de calcular a posição da câmera usada para capturar cada item, em relação a outras posições.[0037] In block 430, method 400 determines a relative geographic position associated with each media item. In one deployment, the position determination module 206 determines a relative geographic position associated with each of the media items 242 in the event list 244. In one deployment, the position determination module 206 determines the relative positions of separate cameras used to capture each of the media items at the time of the real world event occurrence. Position determination module 206 can use multiple visible points on the video portion of each of the media items in order to calculate the position of the camera used to capture each item, relative to other positions.
[0038] No bloco 440, o método 400 gera um único item de mídia editado combinando-se itens de mídia com base nas porções de áudio sincronizadas. Em uma implantação, o módulo de resumo 208 gera um único item de mídia editado associado com o evento do mundo real. O módulo de resumo 208 pode combinar múltiplos itens de mídia 242 uns com os outros para formar o corte do diretor. Por exemplo, o corte do diretor pode incluir seleções particulares que são editadas para estarem em conjunto para mostrar um resumo, visão geral ou outra forma de apresentação do evento do mundo real. Os itens de mídia incluídos no corte do diretor podem ser selecionados manualmente por um curador de conteúdo ou o módulo de resumo 208 pode selecionar automaticamente os itens de mídia com o uso de sinais de entrada de popularidade para cada vídeo, tais como o número de visualizações, comentários, atividade de compartilhamento, etc.[0038] At block 440, method 400 generates a single edited media item by combining media items based on the synchronized audio portions. In one deployment, summary module 208 generates a single edited media item associated with the real-world event. The summary module 208 may combine multiple media items 242 with each other to form the director's cut. For example, the director's cut may include particular selections that are edited together to show a summary, overview, or other presentation of the real-world event. Media items included in the director's cut can be manually selected by a content curator or the summary module 208 can automatically select media items using input popularity signals for each video, such as the number of views, comments, sharing activity, etc.
[0039] No bloco 450, o método 400 apresenta os itens de mídia em uma interface de reprodutor de múltiplos pontos de vista interativa com base nas porções de áudio sincronizadas e nas posições geográficas relativas. Em uma implantação, o módulo de reprodução de múltiplos pontos de vista 210 apresenta os itens de mídia 242 em uma interface de reprodutor de múltiplos pontos de vista interativa com base pelo menos nas porções de áudio sincronizadas e nas posições geográficas relativas. Em uma implantação, o módulo de reprodução de múltiplos pontos de vista 210 exibe um ícone que representa cada um dos itens de mídia em uma localização na interface de reprodutor de múltiplos pontos de vista interativa com base na posição geográfica relativa correspondente determinada pelo módulo de determinação de posição 206. O módulo de reprodução de múltiplos pontos de vista 210 recebe uma seleção de usuário de um ícone que representa um dos itens de mídia e começa a reprodução do item de mídia correspondente. O módulo de reprodução de múltiplos pontos de vista 210 pode armazenar uma indicação da seleção de usuário como dados de entrada de usuário. Durante a reprodução do primeiro item de mídia, um usuário pode selecionar um ícone que representa um item de mídia diferente. O módulo de reprodução de múltiplos pontos de vista 210 pode começar a reprodução do segundo item de mídia em um período que corresponde ao ponto na linha de tempo de referência comum de quando a solicitação foi recebida. Desse modo, o segundo item de mídia pode começar a reprodução em um ponto diferente do começo do arquivo de mídia. Como resultado, a partir da perspectiva do visualizador, o ponto de vista muda, mas o período em relação ao evento do mundo real não muda.[0039] At block 450, method 400 presents the media items in an interactive multi-view player interface based on the synchronized audio portions and relative geographic positions. In one deployment, the multi-view player module 210 presents the media items 242 in an interactive multi-view player interface based on at least the synchronized audio portions and relative geographic positions. In one deployment, the multi-view playback module 210 displays an icon representing each of the media items at a location on the interactive multi-view player interface based on the corresponding relative geographic position determined by the position determination module 206. The multi-view playback module 210 receives a user selection of an icon representing one of the media items and begins playback of the corresponding media item. The multi-view playback module 210 may store an indication of the user selection as user input data. During playback of the first media item, a user can select an icon representing a different media item. The multi-view playback module 210 may begin playback of the second media item at a period corresponding to the point on the common reference timeline when the request was received. This way, the second media item can start playing at a different point than the beginning of the media file. As a result, from the viewer's perspective, the point of view changes, but the period in relation to the real-world event does not change.
[0040] A Figura 5A é um diagrama que ilustra espectrogramas de frequência que correspondem a dois itens de mídia relacionados a um evento comum. A Figura 5B é um diagrama que ilustra um gráfico de linha para a pontuação de correlação entre os espectrogramas de frequência ilustrados na Figura 5A. Após um conjunto de vídeos ser identificado como vindo do mesmo evento do mundo real, os vídeos podem ser sincronizados para uma linha de tempo comum. Os detalhes desse processo são descritos abaixo no contexto de uma interface de reprodução de múltiplos pontos de vista.[0040] Figure 5A is a diagram illustrating frequency spectrograms that correspond to two media items related to a common event. Figure 5B is a diagram illustrating a line graph for the correlation score between the frequency spectrograms illustrated in Figure 5A. Once a set of videos is identified as coming from the same real-world event, the videos can be synced to a common timeline. The details of this process are described below in the context of a multi-view playback interface.
[0041] Para N vídeos, um objetivo é encontrar um conjunto consistente de deslocamentos, xi:N = (xi, X2, •••, XN) que alinhe os sinais de áudio 510, 520. O sistema pode primeiro correlacionar cada par de sinais para medir os deslocamentos relativos xij 530. Uma solução de consenso é então obtida formulando-se uma distribuição de probabilidade conjunta com o uso de informações em pares e produzindo-se uma inferência com o uso de propagação de crença. A propagação de crença permite que pares próximos de sinais, que compartilham sinais de áudio mais proximamente relacionados, acionem a consistência da solução final.[0041] For N videos, one goal is to find a consistent set of displacements, xi:N = (xi, X2, •••, XN) that align the audio signals 510, 520. The system can first correlate each pair of signals to measure the relative displacements xij 530. A consensus solution is then obtained by formulating a joint probability distribution using pairwise information and producing an inference using belief propagation. Belief propagation allows close pairs of signals, which share more closely related audio signals, to drive the consistency of the final solution.
[0042] O deslocamento temporal 530 que coloca dois sinais de áudio 5i0, 520 em alinhamento pode ser encontrado escolhendo-se recursos a partir dos sinais de áudio e então comparando-se esses recursos em deslocamentos de áudio potenciais. Deslocamentos temporais com as pontuações mais bem correspondentes 540 podem ser usados para um alinhamento temporal de hipótese. Há diversas possibilidades para os recursos dos sinais de áudio usarem, tais como o sinal de áudio bruto, ou métodos espectrais tais como os espectros MEL ou cepstrum comumente usados em processamento de fala. Em uma implantação, o sistema usa um recurso com base em frequência, o espectrograma de áudio, si(t,w) : [0,T - 1] x Q :^R, que mede a potência espectral de comprimento de onda w e Q no período t no vídeo de entrada i, em que Ti é o comprimento do sinal de áudio de entrada i. Para correspondência, o sistema pode usar uma correlação cruzada normalizada de média zero como uma medida de consistência de um par de espectrogramas à medida que a normalização torna a comparação insensível a comprimento de sobreposição. A Figura 5A ilustra dois espectrogramas 510, 520 que estão atualmente desalinhados devido a um deslocamento errôneo e a Figura 5B ilustra a correlação cruzada normalizada de média zero (ZNCC) dos dois espectrogramas que indica o alinhamento correto.[0042] The temporal offset 530 that puts two audio signals 5i0, 520 in alignment can be found by picking features from the audio signals and then comparing those features at potential audio offsets. Time offsets with the best matching scores 540 can be used for a hypothesis time alignment. There are several possibilities for the resources of audio signals to use, such as the raw audio signal, or spectral methods such as MEL or cepstrum spectra commonly used in speech processing. In one deployment, the system uses a frequency-based feature, the audio spectrogram, si(t,w) : [0,T - 1] x Q :^R, which measures the spectral power of wavelength w and Q over period t in input video i, where Ti is the length of input audio signal i. For matching, the system can use a zero-mean normalized cross-correlation as a measure of the consistency of a pair of spectrograms as normalization makes the comparison insensitive to overlap length. Figure 5A illustrates two spectrograms 510, 520 that are currently misaligned due to an erroneous offset, and Figure 5B illustrates the zero mean normalized cross-correlation (ZNCC) of the two spectrograms which indicates correct alignment.
[0043] A função de correlação cruzada normalizada, fj(t) : [1 - Tj, Ti — 1] ^ [-1.1]. é a correlação dos espectrogramas normalizados de zero e de comprimento si, sj e é definida como [0043] The normalized cross-correlation function, fj(t) : [1 - Tj, Ti — 1] ^ [-1.1]. is the correlation of the zero-normalized spectrograms of length si, sj and is defined as
[0044] Em que Tij(t) = [max(0,t), min(Ti - 1, t + Tj - 1)] é a região de sobreposição, e a média e o comprimento do sinal são tomados sobre a mesma região de sobreposição [0044] Where Tij(t) = [max(0,t), min(Ti - 1, t + Tj - 1)] is the overlap region, and the average and signal length are taken over the same overlap region
[0045] Os picos K com maior probabilidade são extraídos encontrando-se a máxima do sinal.[0045] The K peaks with the highest probability are extracted by finding the maximum of the signal.
[0046] As hipóteses extraídas das análises em pares são usadas para construir evidências em pares, [0046] The hypotheses extracted from the paired analyzes are used to build evidence in pairs,
[0047] sendo c uma medida de confiança derivada da correlação fij.[0047] where c is a confidence measure derived from the fij correlation.
[0048] O sistema então modela a distribuição de probabilidade conjunta dos deslocamentos temporais combinando-se a evidência em pares, Φu. dando [0048] The system then models the joint probability distribution of the time shifts by combining the evidence in pairs, Φu. giving
[0049] Como há uma ambiguidade de parâmetro no espaço de solução (isto é, p(x1:N) = p(x1:N +t)), o sistema fixa um nó como uma referência e define seu valor como x1 = 0, o que leva à distribuição de probabilidade conjunta de [0049] Since there is a parameter ambiguity in the solution space (that is, p(x1:N) = p(x1:N +t)), the system fixes a node as a reference and sets its value to x1 = 0, which leads to the joint probability distribution of
[0050] que é uma forma comum vista em modelos de campo aleatórios de Markov.[0050] which is a common shape seen in random field Markov models.
[0051] As margens de x na Equação (6) são então aproximadas através de propagação de crença executada em ciclo, que usa uma série iterativa de mensagens entre nós para propagar evidência através do gráfico. Na iteração t > 1 do algoritmo, a mensagem do nó i para o j é definida com o uso de mensagens da iteração anterior como [0051] The margins of x in Equation (6) are then approximated via belief propagation performed in a loop, which uses an iterative series of messages between nodes to propagate evidence across the graph. In iteration t > 1 of the algorithm, the message from node i to j is defined using messages from the previous iteration as
[0052] com as mensagens em t=0 definidas tanto uniformemente quanto aleatoriamente.[0052] with messages at t=0 defined both uniformly and randomly.
[0053] A crença na iteração t aproxima a margem e é definida com o uso de mensagens propagadas, [0053] The belief in iteration t approximates the margin and is defined using propagated messages,
[0054] Observe que a Equação (7) é uma convolução do fator em pares com a crença parcial, que permite que o sistema compute atualizações de mensagem eficientemente com o uso da transformada de Fourier.[0054] Note that Equation (7) is a convolution of the factor in pairs with the partial belief, which allows the system to efficiently compute message updates using the Fourier transform.
[0055] Após as iterações T, a solução final Xi, pode ser obtida maximizando-se a crença: [0055] After the iterations T, the final solution Xi, can be obtained by maximizing the belief:
[0056] Como não é garantido que a propagação de crença executada em ciclo convirja com as margens corretas, o sistema pode tentar todos os nós possíveis como a referência para obter N soluções de hipótese. O sistema mantém a solução final que maximiza uma pontuação de consistência, Alternativamente, a pontuação de correlação pode ser usada diretamente para medir a consistência geral de uma solução de hipótese: [0056] As it is not guaranteed that the belief propagation performed in cycle converges with the correct margins, the system can try all possible nodes as the reference to obtain N hypothesis solutions. The system maintains the final solution that maximizes a consistency score, Alternatively, the correlation score can be used directly to measure the overall consistency of a hypothesized solution:
[0057] A teoria de geometria de múltiplos pontos de vista fornece as ferramentas matemáticas para fazer reconstrução de poses de câmera e geometria de cena a partir de correspondências de ponto derivadas de imagem. Embora o trabalho tenha sido feito em construções em 3D a partir de múltiplos pontos de vista de câmera, muitas técnicas funcionam somente com pressupostos de que as calibrações internas (por exemplo, comprimentos focais, pontos principais) para as câmeras são conhecidos. Entretanto, com conteúdo gerado por usuário, o sistema não pode presumir que as informações de calibração interna sejam conhecidas. Adicionalmente, diferente de métodos para reconstrução com base em visão em 3D de coleções de foto não estruturadas que dependem de intrínsecos codificados nos cabeçalhos de EXIF, metadados de vídeo frequentemente não contêm essas informações úteis. Desse modo, em uma implantação, o sistema usa a rotação de câmera pura presente nos vídeos gerados por usuário para extrair automaticamente a calibração interna.[0057] Multi-viewpoint geometry theory provides the mathematical tools to make reconstruction of camera poses and scene geometry from image-derived point correspondences. While work has been done on 3D builds from multiple camera viewpoints, many techniques only work with assumptions that the internal calibrations (eg focal lengths, key points) for the cameras are known. However, with user-generated content, the system cannot assume that internal calibration information is known. Additionally, unlike methods for 3D view-based reconstruction of unstructured photo collections that rely on intrinsics encoded in EXIF headers, video metadata often does not contain this useful information. So, in a deployment, the system uses the pure camera rotation present in the user-generated videos to automatically extract the internal calibration.
[0058] Para eventos esportivos e de concerto, a câmera fica frequentemente em uma posição e apenas gira levemente para observar a ação. Como esse é um caso de uso-alvo, o sistema usa um método para fornecer uma estimativa inicial precisa dos comprimentos focais da câmera para esse tipo de movimento de vídeo. O sistema então filtra regiões de imagem que são melhores para fazer extração de pose de câmera e reconstrução de múltiplos pontos de vista com o uso de algum algoritmo de seleção e então usa os comprimentos focais já derivados para fazer extração de pose nessas regiões de imagem.[0058] For sporting and concert events, the camera will often stay in one position and only rotate slightly to observe the action. Since this is a targeted use case, the system uses a method to provide an accurate initial estimate of camera focal lengths for this type of video motion. The system then filters out image regions that are best for doing camera pose extraction and reconstruction from multiple viewpoints with the use of some selection algorithm and then uses the already derived focal lengths to do pose extraction on those image regions.
[0059] No caso em que a câmera foi submetida apenas à rotação (e possivelmente translação desprezível), as coordenadas de imagem referentes à transformação entre as duas imagens podem ser relacionadas por uma matriz de homografia projetiva H 3 x 3.[0059] In the case where the camera was subjected only to rotation (and possibly negligible translation), the image coordinates referring to the transformation between the two images can be related by a projective homography matrix H 3 x 3.
[0060] Sem perda de generalidade, deixe a primeira câmera ser alinhada com a origem; portanto, a matriz de câmera é definida como [0060] Without loss of generality, let the first camera be aligned with the origin; therefore the camera matrix is defined as
[0061] e se o segundo ponto de vista é apenas uma rotação, R da primeira, então [0061] and if the second view is just a rotation, R of the first, then
[0062] Aqui presume-se que os parâmetros de câmera internos tenham zero distorção e presume-se que o ponto de princípio esteja no centro do plano de imagem para a câmera, que é o caso para a maioria das câmeras de consumidor: [0062] Here the internal camera parameters are assumed to have zero distortion and the principle point is assumed to be at the center of the image plane for the camera, which is the case for most consumer cameras:
[0063] Sendo fx e fy o comprimento focal em x e y respectivamente. A homografia referente às imagens é então uma função dos internos da matriz de rotação: [0063] Being fx and fy the focal length in x and y respectively. The homography referring to the images is then a function of the internals of the rotation matrix:
[0064] Uma vez que R é uma matriz de rotação 3x3 ortogonal que satisfaz I3x3 - RRT, com o uso da Equação (15), a restrição pode ser regravada como [0064] Since R is a 3x3 orthogonal rotation matrix satisfying I3x3 - RRT, with the use of Equation (15), the constraint can be rewritten as
[0065] que é uma função de apenas K e a homografia H recuperada. Uma vez que H pode ser estimada diretamente a partir de correspondências de imagem, a restrição Tr(DDT) = 0, dá um problema de quadrados mínimos não lineares nos desconhecidos fx e fy. O sistema soluciona para fx e fy minimizando-se Tr(DDT) no espaço de fx e fy, começando de algum valor inicial para (fx, fy) e então segue computando iterativamente (Δ fx, Δfy) com o uso de métodos de região de confiança como Levenberg-Marquardt até que convirja com um mínimo aceitável.[0065] which is a function of just K and the recovered H homography. Since H can be estimated directly from image matches, the constraint Tr(DDT) = 0, gives a nonlinear least squares problem on the unknowns fx and fy. The system solves for fx and fy by minimizing Tr(DDT) in the space of fx and fy, starting from some initial value for (fx, fy) and then iteratively computing (Δ fx, Δfy) using confidence region methods like Levenberg-Marquardt until it converges to an acceptable minimum.
[0066] O sistema pode descartar automaticamente partes da sequência de imagem com zoom ou rotação insuficiente da câmera inspecionando-se a homografia H e selecionar as porções restantes para estimar os parâmetros internos da câmera.[0066] The system can automatically discard parts of the image sequence with insufficient camera zoom or rotation by inspecting the H-homography and selecting the remaining portions to estimate the camera's internal parameters.
[0067] Seleção de Imagem, Correspondência em Pares & Ajuste de Pacote[0067] Image Selection, Pair Matching & Packet Adjustment
[0068] Para razões computacionais, o sistema pode primeiro reduzir o número de quadros inseridos para a reconstrução, selecionando-se apenas uns poucos quadros salientes de cada sequência de vídeo considerando-se o número de recursos, qualidade de cada quadro e quantidade de movimento temporal. Uma vez que o sistema tem imagens selecionadas para reconstrução, o mesmo pode extrair recursos de SIFT de cada imagem e corresponder pares de imagens com o uso desses recursos. Em uma modalidade, o sistema pode usar correspondência por limite de Lowe para conseguir conjunto inicial de correspondências. Para suprimir correspondências incorretas, o sistema pode refinar adicionalmente essa correspondência apenas selecionando-se correspondências cuja diferença de escala e orientação concorda com a maioria daquelas das correspondências. Isso pode ser feito binando-se o conjunto inicial de correspondências para um histograma em 2D de diferença de escala e orientação através da correspondência. O sistema seleciona as correspondências no compartimento que contém o maior número de correspondências como as correspondências refinadas finais.[0068] For computational reasons, the system can first reduce the number of frames inserted for the reconstruction, selecting only a few salient frames from each video sequence considering the number of features, quality of each frame and amount of temporal movement. Once the system has selected images for reconstruction, it can extract SIFT features from each image and match pairs of images using those features. In one embodiment, the system can use Lowe's limit matching to get an initial set of matches. To suppress incorrect matches, the system can further refine this match by only selecting matches whose difference in scale and orientation agrees with most of those matches. This can be done by matching the initial set of matches to a 2D histogram of scale difference and orientation across the match. The system selects the matches in the bucket that contain the most matches as the final refined matches.
[0069] O sistema pode então computar um modelo de dois pontos de vista, isto é, matriz fundamental e pontos de convergência f, para cada par de imagens com o uso das correspondências. O sistema pode usar as informações de comprimento focal para as imagens computadas acima para computar esses modelos de dois pontos de vista. Uma vez que temos os modelos de dois pontos de vista, o sistema pode adicionar iterativamente modelos de dois pontos de vista juntos e fazer ajuste de pacote, para ter o modelo em 3D final que contém todas as câmeras.[0069] The system can then compute a model from two points of view, ie fundamental matrix and points of convergence f, for each pair of images using the correspondences. The system can use the focal length information for the above computed images to compute these two point of view models. Once we have the two POV models, the system can iteratively add the two POV models together and do package fitting, to have the final 3D model that contains all the cameras.
[0070] Para mapas em 3D estáticos, o sistema tira a posição em 3D final da câmera para ser a posição média sobre todas as posições reconstruídas para essa sequência de vídeo. A nuvem de ponto em 3D reconstruído pode ser usada para derivar um modelo em 3D para a interface com base em mapa 3D, ou alternativamente, as poses de câmera em 3D reconstruídas podem ser manualmente alinhadas a um mapa em 3D criado por artista do ambiente.[0070] For static 3D maps, the system takes the final 3D position of the camera to be the average position over all reconstructed positions for that video sequence. The reconstructed 3D point cloud can be used to derive a 3D model for the 3D map-based interface, or alternatively, the reconstructed 3D camera poses can be manually aligned to an artist-created 3D map of the environment.
[0071] Os vídeos de entrada de câmeras de consumidor sofrem frequentemente de questões de qualidade. Antes de utilizar os fluxos de vídeo, o sistema pode aplicar opcionalmente correção de cor entre os vídeos, reduzir a trepidação através de estabilização de movimento e reduzir artefatos de ruído através de redução de ruído. Os níveis de áudio entre os clipes podem também ser normalizados para níveis comuns.[0071] Input video from consumer cameras often suffers from quality issues. Before using video streams, the system can optionally apply color correction between videos, reduce judder through motion stabilization, and reduce noise artifacts through noise reduction. Audio levels between clips can also be normalized to common levels.
[0072] O sistema pode usar minimização de energia para identificar quando trocar os vídeos. Além da qualidade comum e sinais de entrada de transição, nossos termos de energia podem levar em consideração as posições em 3D e pontos de vista das câmeras. Com o uso das posições e pontos de vista, uma transição pode ser mais contínua se é feita troca em uma direção consistente.[0072] The system can use energy minimization to identify when to switch the displays. In addition to common quality and crossover input signals, our power terms can take into account 3D positions and camera viewpoints. With the use of positions and viewpoints, a transition can be more seamless if you switch in a consistent direction.
[0073] Após solucionar para a otimização, um vídeo editado combinado pode ser criado. Alternativamente, o mecanismo de reprodução recolhe uma lista de pontos de edição, como sequência de pares, indicando períodos em que o vídeo deveria ser trocado. Dessa forma, o usuário pode assistir um corte do diretor, enquanto o mecanismo de reprodução atualiza dinamicamente o ponto de vista com o uso da lista de edição. O usuário pode também trocar do corte do diretor a qualquer momento para ter um efeito de ponto de vista livre.[0073] After solving for optimization, a combined edited video can be created. Alternatively, the playback engine collects a list of edit points, as a string of pairs, indicating periods when the video should be swapped. This way, the user can watch a director's cut, while the playback engine dynamically updates the point of view using the edit list. The user can also switch from the director's cut at any time to get a free point of view effect.
[0074] Os benefícios de ter geometria de cena de aproximação dispersa e de pose de câmera em 3D câmera pose são três: 1) a disposição espacial dos vídeos pode ser apresentada ao usuário, o que permite seleção de ponto de vista interativa intuitiva, 2) pontos de vista virtuais intermediários podem também ser apresentados a um usuário e 3) a disposição espacial pode ser usada para prever prováveis próximos pontos de vista. O reprodutor de múltiplos pontos de vista interativo faz uso dessas vantagens, enquanto apresenta outros elementos de UI que indicam pontos importantes na linha de tempo. Pontos de vista virtuais intermediários provavelmente não terão a fidelidade como os pontos de vista de entrada, mas são bons para cobrir armazenagem em armazenamento temporário e latências inerentes em troca de vídeo.[0074] The benefits of having dispersed zoom scene geometry and camera pose in 3D camera pose are three: 1) the spatial layout of the videos can be presented to the user, which allows intuitive interactive viewpoint selection, 2) intermediate virtual viewpoints can also be presented to a user, and 3) the spatial layout can be used to predict likely upcoming viewpoints. The interactive multi-view player makes use of these advantages, while presenting other UI elements that indicate important points on the timeline. Intermediate virtual viewpoints probably won't have the fidelity as input viewpoints, but are good for covering staging and inherent latencies in video switching.
[0075] As Figuras 6A e 6B são diagramas que ilustram exemplos de duas possíveis apresentações da interface de reprodução de múltiplos pontos de vista interativa, de acordo com algumas implantações da presente revelação. O ponto de vista interativo em 3D na Figura 6A mostra um modelo de gabarito em 3D que corresponde ao evento com ícones/widgets nas poses de vídeo reconstruídas. Esses ícones também ilustram a câmera reproduzida atualmente e quais pontos de vista estão disponíveis (ou no alcance) no período de reprodução atual. Os usuários podem navegar espacialmente pela cena de vídeo clicando em um ponto de vista da câmera desejado no mapa.[0075] Figures 6A and 6B are diagrams illustrating examples of two possible presentations of the interactive multiview playback interface, according to some implementations of the present disclosure. The 3D interactive viewpoint in Figure 6A shows a 3D template model that matches the event with icons/widgets in the reconstructed video poses. These icons also illustrate the currently playing camera and which viewpoints are available (or in range) for the current playback period. Users can spatially navigate through the video scene by clicking on a desired camera viewpoint on the map.
[0076] Como nem todos os vídeos começam ou terminam ao mesmo tempo, uma barra de densidade de vídeo pode ser exibida junto com a barra de procura normal para ilustrar quantos vídeos estão disponíveis em um dado período (consultar Figura 6B). Alternativamente, uma barra de destaque de observação de grande interesse pode ser usada para indicar os destaques do evento. Tal barra de destaque é similar ao que poderia ser usado para reprodução de vídeo único. Essa barra de destaque pode tanto ser curada manualmente, ou pode usar sinais de entrada de mídia social, ou aceitar retroalimentações de interação de usuário registrada em log. No caso de múltiplos pontos de vista, um sinal de entrada é tanto o número quanto a qualidade de vídeos disponível em qualquer dado momento. Adicionalmente, o número de vezes que usuários repetiram um momento no tempo de pontos de vista diferentes pode também ser usado como um sinal de entrada para acionar a densidade da barra de destaque.[0076] Since not all videos start or end at the same time, a video density bar can be displayed along with the normal search bar to illustrate how many videos are available at a given time period (see Figure 6B). Alternatively, a high interest note highlight bar can be used to indicate event highlights. Such a highlight bar is similar to what could be used for single video playback. This highlight bar can either be curated manually, or it can use social media input signals, or accept feedback from logged user interaction. In the case of multiple views, an input signal is both the number and quality of videos available at any given time. Additionally, the number of times users have replayed a point in time from different viewpoints can also be used as an input signal to trigger the highlight bar density.
[0077] Para garantir uma experiência interativa satisfatória, a interface pode apresentar transições suaves entre pontos de vista quando solicitado por um usuário. As descontinuidades abruptas em reprodução devido a rearmazenagem em armazenamento temporário mediante solicitações de troca são minimizadas.[0077] To ensure a satisfactory interactive experience, the interface can present smooth transitions between views when requested by a user. Abrupt outages in playback due to staging upon switch requests are minimized.
[0078] A troca instantânea entre pontos de vista de vídeo ou procura no período utiliza acesso aleatório imediato para todos os dados de vídeo. Garantir tal acesso aleatório necessitaria de pré- armazenagem em armazenamento temporário ou transferência por download de todos os vídeos antes da reprodução, que leva possivelmente a uma experiência de usuário ruim. Relaxar a restrição em procura instantânea, transmitir continuamente todos os vídeos simultaneamente permitiria troca de ponto de vista instantânea, mas devido a restrições de largura de banda tal transmissão contínua provavelmente não será possível. Ao invés destes extremos, um reprodutor ideal apenas transfere por download as porções de vídeo que serão assistidas, mas precisa trocar pré-armazenagem em armazenamento temporário de algumas regiões do vídeo a fim de garantir uma troca interativa.[0078] Instant switching between video views or searching in period utilizes immediate random access to all video data. Ensuring such random access would require pre-storing in temporary storage or downloading all videos prior to playback, possibly leading to a poor user experience. Relaxing the restriction on instantaneous browsing, continuously streaming all videos simultaneously would allow instant point-of-view switching, but due to bandwidth constraints such continuous streaming will likely not be possible. Rather than these extremes, an ideal player only downloads the portions of video that will be watched, but needs to swap pre-storage of some regions of the video in order to ensure interactive switching.
[0079] Uma solução para a troca entre largura de banda e interatividade é para usar uma forma de armazenagem em armazenamento temporário de fundo. O vídeo principal selecionado atualmente é reproduzido no primeiro plano e para tentar garantir uma troca interativa rápida, um segundo vídeo armazenamento temporário de fundo será transmitido continuamente, armazenado em armazenamento temporário e reproduzido no plano de fundo. A transmissão contínua do vídeo no armazenamento temporário de fundo pode monitorar o comportamento de usuário, tal como passar o mouse sobre um ponto de vista virtual para prever qual vídeo será selecionado. Alternativamente, o histórico de todos os usuários pode ser usado como um antecedente para determinar qual será o próximo ponto de vista mais provável, ou algum pré-armazenagem em armazenamento temporário de todos os vídeos por volta de pontos no tempo chave pode ser realizado.[0079] One solution to the tradeoff between bandwidth and interactivity is to use a form of temporary storage in the background. The currently selected main video plays in the foreground and to try to ensure a quick interactive switch, a second video in the background buffer will be streamed continuously, buffered and played in the background. Streaming video in the background buffer can monitor user behavior, such as hovering over a virtual viewpoint to predict which video will be selected. Alternatively, the history of all users can be used as an antecedent to determine what the next most likely viewpoint will be, or some pre-storage of all videos around key time points can be performed.
[0080] Após o vídeo a ser armazenado em armazenamento temporário de fundo ser identificado, o vídeo pode ser armazenado em armazenamento temporário e reproduzido no plano de fundo. A Figura 7 ilustra uma linha de tempo para tal evento, em que tp indica que o tempo em que a previsão foi feita, ta indica o período em que um usuário realmente selecionou o ponto de vista e ts é o período em que o sistema completou de trocar para o novo vídeo. No momento de tp, o reprodutor de vídeo armazenado em armazenamento temporário de fundo precisa obter dados de vídeo o suficiente para iniciar a reprodução (período de armazenagem em armazenamento temporário). Há também um leve atraso após os dados estarem completamente disponíveis para inicialização do reprodutor. Se o período de seleção de usuário, ta, ocorre após a inicialização ter concluído, o vídeo armazenado em armazenamento temporário de fundo pode simplesmente ser trocados para o primeiro plano.[0080] After the video to be stored in background temporary storage is identified, the video can be stored in temporary storage and played in the background. Figure 7 illustrates a timeline for such an event, where tp indicates the time the prediction was made, ta indicates the period when a user actually selected the view, and ts is the period the system completed switching to the new video. At the time of tp, the video player stored in background staging needs to get enough video data to start playing (storage period in staging). There is also a slight delay after the data is completely available for the player to start. If the user selection period, ta, occurs after initialization has completed, the video stored in the background buffer can simply be switched to the foreground.
[0081] A Figura 7 ilustra dois exemplos de uma linha de tempo de previsão & armazenagem em armazenamento temporário. No período tp, o sistema prevê que um usuário irá trocar em breve para o vídeo 2, então o vídeo 2 é armazenado em armazenamento temporário e inicia a reprodução no plano de fundo. O usuário então realmente solicita a troca no período ta após o qual usamos uma animação de atraso de comprimento ts - ta para permitir que armazenagem em armazenamento temporário/ reprodução esteja pronto. Quando a previsão está correta (esquerda), essa animação de atraso pode ser curta. Se o vídeo é armazenado em armazenamento temporário (direita), o atraso apenas precisa ser longo o bastante para cobrir o tempo de inicialização da reprodução.[0081] Figure 7 illustrates two examples of a prediction & storage timeline in temporary storage. In the tp period, the system predicts that a user will soon switch to video 2, so video 2 is stored in temporary storage and starts playing in the background. The user then actually requests the swap at time ta after which we use a delay animation of length ts - ta to allow staging/playback to be ready. When the prediction is correct (left), this delay animation can be short. If the video is stored in temporary storage (right), the delay just needs to be long enough to cover the playback startup time.
[0082] Entretanto, para garantir uma experiência consistente quando a previsão de vídeo é incorreta ou os dados de vídeo não concluíram a armazenagem em armazenamento temporário, o sistema usa um atraso de animação ts - ta > tempo de inicialização. Essa animação de atraso deve pelo menos cobrir o período até que o vídeo armazenado em armazenamento temporário de fundo esteja pronto para reprodução. Dessa forma, o usuário ainda é com capacidade para assistir o vídeo primário atual até que o ponto de vista armazenado em armazenamento temporário de fundo secundária esteja disponível. Adicionalmente, no caso de o vídeo de armazenamento temporário de fundo já estar sendo reproduzido no plano de fundo, o atraso de animação permite uma sincronização final do período de reprodução antes do vídeo armazenado em armazenamento temporário de fundo ser trocado para o plano de fundo.[0082] However, to ensure a consistent experience when the video preview is incorrect or the video data has not completed buffering, the system uses an animation delay ts - ta > startup time. This delay animation should at least cover the period until the video stored in background staging is ready for playback. This way, the user is still able to watch the current primary video until the view stored in secondary background temporary storage is available. Additionally, in case the background buffered video is already playing in the background, the animation delay allows for a final sync of the playback period before the background buffered video is switched to the background.
[0083] A estratégia acima pode ser implantada com o uso de tecnologia de transmissão contínua de vídeo atual. Novos padrões em transmissão contínua de vídeo, tal como MPEG-DASH, também permitem mudanças de qualidade em tempo real suaves, se há largura de banda disponível. Com o uso de MPEG-DASH, o esquema de armazenagem em armazenamento temporário acima poderia usar vídeo de baixa qualidade no armazenamento temporário de fundo, garantindo largura de banda inferior e, portanto, períodos de armazenagem em armazenamento temporário mais rápidos do armazenamento temporário de fundo. Após o vídeo ser trazido para o primeiro plano, a qualidade poderia então ser automaticamente aprimorada de acordo com a largura de banda disponível. Similarmente, métodos de codificação de vídeo escalonável (SVC) poderiam ser usados para derivar uma política com base em utilidade para transmitir os dados de pontos de vista de plano de fundo se há largura de banda disponível. Por exemplo, uma política pode ser definida para tentar preferir ter algum vídeo de baixa qualidade de todos os pontos de vista disponíveis, o que permite tanto troca rápida quanto reutilização dos dados de imagem para exibição em miniaturas no ponto de vista de mapa em 3D.[0083] The above strategy can be implemented with the use of current streaming video technology. New standards in streaming video, such as MPEG-DASH, also allow smooth real-time quality changes, if bandwidth is available. With the use of MPEG-DASH, the above caching scheme could use lower quality video in the background caching, ensuring lower bandwidth and therefore faster caching times than the background caching. After the video was brought to the foreground, the quality could then be automatically upscaled according to the available bandwidth. Similarly, scalable video coding (SVC) methods could be used to derive a utility-based policy for transmitting data from background views if bandwidth is available. For example, a policy can be set to try and prefer to have some low-quality video from all available viewpoints, which allows for both quick switching and reuse of image data for thumbnail display in the 3D map viewpoint.
[0084] A estratégia de armazenagem em armazenamento temporário descrita acima depende de um leve atraso após o usuário escolher um novo vídeo para cobrir latências na preparação do vídeo de plano de fundo para reprodução. Para dar a aparência de uma transição de reprodução mais responsiva, uma animação ou efeito pode ser sobreposto no vídeo durante esse período de transição.[0084] The staging strategy described above relies on a slight delay after the user chooses a new video to cover latencies in preparing the background video for playback. To give the appearance of a more responsive playback transition, an animation or effect can be superimposed on the video during this transition period.
[0085] Um tal efeito seria animar um movimento panorâmico, varredura e esmaecimento da miniatura de uma seleção de vídeo do elemento de interação de usuário para o vídeo principal atual. Se disponíveis, as informações em 3D recuperadas do componente de visão de computador do sistema podem ser usadas para animar uma troca de ponto de vista livre. Tal transição também dá ao usuário uma sensação melhor do espaço do ambiente, à medida que a renderização parece uma navegação em 3D da cena. Tal transição pode ser renderizada com o uso apenas de uma geometria de cena de aproximação granulada e textura projetiva que mapeia a textura de vídeo atual para a cena. Em uma implantação, isso pode ser denominado como renderização de ponto de vista livre. Uma única imagem de entrada e uma geometria de cena granulada podem ser usadas para sintetizar pontos de vista sobre regiões próximas. O ponto de vista de cima para baixo mostra a textura sendo projetada na cena.[0085] One such effect would animate a pan, sweep, and fade thumbnail of a video selection from the user interaction element to the current main video. If available, 3D information retrieved from the system's computer vision component can be used to animate a free viewpoint switch. Such a transition also gives the user a better sense of the space of the environment, as the rendering looks like a 3D navigation of the scene. Such a transition can be rendered using only a grainy zoom scene geometry and projective texture that maps the current video texture to the scene. In a deployment, this can be referred to as Free Point of View rendering. A single input image and grainy scene geometry can be used to synthesize views of nearby regions. The top-down viewpoint shows the texture being projected onto the scene.
[0086] Os efeitos de transição adicionais podem também incluir câmera lenta ou um modo de repetição, em que o período é retrocedido mediante a troca de vídeo de modo que o usuário possa visualizar o mesmo evento a partir de pontos de vista diferentes. O ponto no tempo de retrocesso pode usar informações da barra de densidade de vídeo para identificar automaticamente um ponto de retrocesso dinâmico.[0086] Additional transition effects may also include slow motion or a repeat mode, where the period is rewinded upon switching video so that the user can view the same event from different points of view. The rewind point in time can use video density bar information to automatically identify a dynamic rewind point.
[0087] A descrição acima apresenta um sistema e método completo para alcançar uma reprodução de vídeo de múltiplos pontos de vista a partir de conteúdo gerado por usuário. O sistema foca em extrair, utilizar e apresentar todas as informações disponíveis, incluindo tempo de início de vídeo e localizações de câmera em 3D. Ao fazê-lo, o sistema alcança uma experiência de navegação de múltiplos pontos de vista, em que o usuário é guiado visualmente para pontos de interesse em tempo e espaço considerando-se os sinais de entrada de qualidade de vídeo disponíveis, informações de pose em 3D e histórico de interação de usuário. Esses mesmos sinais de entrada também são explorados em uma estratégia de armazenagem em armazenamento temporário de fundo preditiva durante a reprodução, o que garante interatividade enquanto reduz restrições de largura de banda em transmissão de Protocolo Internet.[0087] The above description presents a complete system and method for achieving multi-view video playback from user-generated content. The system focuses on extracting, utilizing and presenting all available information, including video start times and 3D camera locations. In doing so, the system achieves a multipoint navigation experience, where the user is visually guided to points of interest in time and space considering available video quality input signals, 3D pose information, and user interaction history. These same input signals are also exploited in a predictive background caching strategy during playback, which ensures interactivity while reducing bandwidth constraints on Internet Protocol transmission.
[0088] A Figura 8 ilustra uma representação diagramática de uma máquina na forma exemplificativa de um sistema de computador 800 dentro do qual um conjunto de instruções, para fazer com que a máquina realize uma ou mais das metodologias discutidas no presente documento, pode ser executado. Em implantações alternativas, a máquina pode ser conectada (por exemplo, em rede) a outras máquinas em uma rede local (LAN), uma intranet, uma extranet, ou a Internet. A máquina pode operar na qualidade de um servidor ou uma máquina de cliente em um ambiente de rede de servidor de cliente ou como uma máquina de ponto em um ambiente de rede de ponto a ponto (ou distribuído). A máquina pode ser um computador pessoal (PC), um computador do tipo tablet PC, um decodificador de sinais (STB), um Assistente Digital Pessoal (PDA), um telefone celular, um aplicativo de web, um servidor, um roteador de rede, comutador ou bridge ou qualquer máquina com a capacidade para executar um conjunto de instruções (sequencial ou de outra forma) que especifica as ações a serem tomadas por tal máquina. Adicionalmente, embora apenas uma única single máquina seja ilustrada, o termo "máquina" deverá também ser tomado como incluindo qualquer coleção de máquinas que individualmente ou conjuntamente executam um conjunto (ou múltiplos conjuntos) de instruções para realizar qualquer uma ou mais das metodologias discutidas no presente documento.[0088] Figure 8 illustrates a diagrammatic representation of a machine in exemplary form of a computer system 800 within which a set of instructions to cause the machine to perform one or more of the methodologies discussed herein can be executed. In alternative deployments, the machine can be connected (for example, networked) to other machines on a local area network (LAN), an intranet, an extranet, or the Internet. The machine can operate as a server or a client machine in a server-client network environment or as a peer machine in a peer-to-peer (or distributed) network environment. The machine can be a personal computer (PC), a tablet PC-type computer, a set-top box (STB), a Personal Digital Assistant (PDA), a cell phone, a web application, a server, a network router, switch or bridge, or any machine with the capability to execute a set of instructions (sequential or otherwise) that specifies the actions to be taken by that machine. Additionally, although only a single machine is illustrated, the term "machine" should also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed in this document.
[0089] O sistema de computador exemplificativo 800 inclui um dispositivo de processamento 802, uma memória principal 804 (por exemplo, memória apenas de leitura (ROM), memória flash, memória de acesso aleatório dinâmica (DRAM) (como DRAM síncrona (SDRAM) ou DRAM Rambus (RDRAM), etc.), uma memória estática 806 (por exemplo, memória flash, memória de acesso aleatório estática (SRAM), etc.) e um dispositivo de armazenamento de dados 818, que se comunicam uns com os outros por meio de um barramento 830. Qualquer um dos sinais fornecidos através de vários barramentos descritos no presente documento pode ser multiplexado no tempo com outros sinais e fornecidos através de um ou mais barramentos comuns. Adicionalmente, a interconexão entre componentes ou blocos de circuito pode ser mostrada como barramentos ou como linhas de sinal único. Cada um dos barramentos pode ser alternativamente um ou mais linhas de sinal único e cada uma das linhas de sinal único pode ser alternativamente barramentos.[0089] The exemplary computer system 800 includes a processing device 802, main memory 804 (e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) (such as synchronous DRAM (SDRAM) or Rambus DRAM (RDRAM), etc.), static memory 806 (e.g., flash memory, static random access memory (SRAM), etc.), and a data storage device 818, which communicate with each other via a bus 830. Any of the signals supplied via the various buses described herein may be time-multiplexed with other signals and supplied via one or more common buses. Additionally, the interconnection between components or circuit blocks may be shown as buses or as single-signal lines. Each of the buses may alternatively be one or more single-signal lines, and each of the single-signal lines may alternatively be buses.
[0090] O dispositivo de processamento 802 represente um ou mais dispositivos de processamento de propósito geral tal como um microprocessador, unidade de processamento central ou similares. Mais particularmente, o dispositivo de processamento 202 pode ser um microprocessador de computação de conjunto de instrução complexo (CISC), microprocessador de computação de conjunto de instrução reduzido (RISC), microprocessador de palavra de instrução muito longa (VLIW), ou processador que implanta outros conjuntos de instrução ou processadores que implantam uma combinação de conjuntos de instruções. O dispositivo de processamento 802 pode também ser um ou mais dispositivos de processamento de propósito especial tal como um circuito integrado de aplicação específica (ASIC), uma matriz de porta de campo programável (FPGA), um processador de sinal digital (DSP), processador de rede ou similares. O dispositivo de processamento 802 é configurado para executar uma lógica de processamento 826 para realizar as operações e etapas discutidas no presente documento.[0090] The processing device 802 represents one or more general purpose processing devices such as a microprocessor, central processing unit or the like. More particularly, processing device 202 may be a complex instruction set computing (CISC) microprocessor, reduced instruction set computing (RISC) microprocessor, very long instruction word (VLIW) microprocessor, or processor that implements other instruction sets or processors that implement a combination of instruction sets. Processing device 802 may also be one or more special purpose processing devices such as an application specific integrated circuit (ASIC), field programmable gate array (FPGA), digital signal processor (DSP), network processor, or the like. Processing device 802 is configured to execute processing logic 826 to perform the operations and steps discussed herein.
[0091] O sistema de computador 800 pode incluir adicionalmente um dispositivo de interface de rede 808. O sistema de computador 800 também pode incluir uma unidade de exibição de vídeo 810 (por exemplo, uma tela de cristal líquido (LCD) ou um tudo de raio de catodo (CRT)), um dispositivo de inserção alfanumérica 812 (por exemplo, um teclado), um dispositivo de controle de cursor 814 (por exemplo, um mouse) e um dispositivo de geração de sinal 816 (por exemplo, um alto-falante).[0091] The computer system 800 may further include a network interface device 808. The computer system 800 may also include a video display unit 810 (for example, a liquid crystal display (LCD) or cathode beam tube (CRT)), an alphanumeric input device 812 (for example, a keyboard), a cursor control device 814 (for example, a mouse) and a signal generation device 816 (for example, a loudspeaker).
[0092] O dispositivo de armazenamento de dados 818 pode incluir um meio de armazenamento legível por máquina 828, no qual é armazenado um ou mais conjuntos de instruções 822 (por exemplo, software) que incorporam qualquer uma ou mais das metodologias de funções descritas no presente documento. As instruções 822 podem também residir, completa ou pelo menos parcialmente, na memória principal 804 e/ou no dispositivo de processamento 802 durante a execução da mesma pelo sistema de computador 800; em que a memória principal 804 e o dispositivo de processamento 802 também constituem os meios de armazenamento legíveis por computador. As instruções 822 podem ser adicionalmente transmitidas ou recebidas através de uma rede 820 por meio do dispositivo de interface de rede 808.[0092] Data storage device 818 may include a machine-readable storage medium 828 on which is stored one or more sets of instructions 822 (e.g., software) that embody any one or more of the function methodologies described herein. Instructions 822 may also reside, completely or at least partially, in main memory 804 and/or processing device 802 during execution thereof by computer system 800; wherein main memory 804 and processing device 802 also constitute the computer-readable storage medium. Instructions 822 may additionally be transmitted or received over a network 820 via network interface device 808.
[0093] O meio de armazenamento legível por máquina 828 pode também ser usado para armazenar instruções para realizar um método para reprodução interativa de áudio e vídeo de múltiplos pontos de vista, conforme descrito no presente documento. Embora o meio de armazenamento legível por máquina 828 seja mostrado em uma modalidade exemplificativa para ser um meio único, o termo "meio de armazenamento legível por máquina" deve ser tomado para incluir um meio único ou múltiplos meios (por exemplo, um banco de dados centralizada ou distribuída e/ou caches e servidores associados) que armazenam o um ou mais conjuntos de instruções. Um meio legível por máquina inclui qualquer mecanismo para armazenar informações em uma forma (por exemplo, software, aplicativo de processamento) legível por uma máquina (por exemplo, um computador). O meio legível por máquina pode incluir, mas sem limitação, um meio de armazenamento magnético (por exemplo, disquete); um meio de armazenamento óptico (por exemplo, CD-ROM); um meio de armazenamento magneto-óptico; uma memória de somente leitura (ROM); uma memória de acesso aleatório (RAM); uma memória programável apagável (por exemplo, EPROM e EEPROM); uma memória flash; ou outro tipo de meio adequado para armazenar instruções eletrônicas.[0093] The machine-readable storage medium 828 may also be used to store instructions for realizing a method for interactive playback of audio and video from multiple viewpoints, as described herein. Although machine-readable storage medium 828 is shown in an exemplary embodiment to be a single medium, the term "machine-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database and/or associated caches and servers) that store the one or more sets of instructions. A machine-readable medium includes any mechanism for storing information in a form (eg, software, processing application) readable by a machine (eg, a computer). The machine-readable medium may include, but is not limited to, a magnetic storage medium (eg, floppy disk); an optical storage medium (eg CD-ROM); a magneto-optical storage medium; a read-only memory (ROM); a random access memory (RAM); an erasable programmable memory (eg EPROM and EEPROM); a flash memory; or other suitable medium for storing electronic instructions.
[0094] A descrição antecedente define vários detalhes específicos tais como exemplos de sistemas, componentes, métodos específicos e assim por diante, a fim de fornecer uma compreensão satisfatória de diversas implantações da presente revelação. Será aparente para uma pessoa versada na técnica, entretanto, que pelo menos algumas implantações da presente revelação podem ser praticadas sem esses detalhes específicos. Em outros exemplos, componentes ou métodos bem conhecidos não são descritos em detalhes ou são apresentados em formato de diagrama de bloco simples a fim de evitar obscurecer desnecessariamente a presente revelação. Desse modo, os detalhes específicos estabelecidos são meramente exemplificativos. As implantações particulares podem variar a partir desses detalhes exemplificativos e ainda serem contemplados como no escopo da presente revelação.[0094] The foregoing description defines various specific details such as examples of systems, components, specific methods and so on, in order to provide a satisfactory understanding of various implementations of the present disclosure. It will be apparent to a person skilled in the art, however, that at least some implementations of the present disclosure can be practiced without these specific details. In other examples, well known components or methods are not described in detail or are presented in simple block diagram format in order to avoid unnecessarily obscuring the present disclosure. Accordingly, the specific details set forth are merely exemplary. Particular implantations may vary from these exemplary details and still be contemplated as being within the scope of the present disclosure.
[0095] Em situações em que os sistemas discutidos no presente documento coletam informações pessoais sobre usuários, ou podem fazer uso de informações pessoais, os usuários podem ser fornecidos com uma oportunidade de controlar se programas ou recursos coletam informações de usuário (por exemplo, informações sobre a rede social de um usuário, ações sociais ou atividades, profissão, as preferências de um usuário, ou a localização atual de um usuário), ou controlar se e/ou como recebem conteúdo do servidor de mídia que pode ser mais relevante para o usuário. Além disso, certos dados podem ser tratados de uma ou mais formas antes de serem armazenados ou usados, de modo que informações pessoalmente identificáveis sejam removidas. Por exemplo, a identidade de um usuário pode ser tratada de modo que nenhuma informação pessoalmente identificável possa ser determinada para o usuário, ou a localização geográfica de um usuário pode ser generalizada em que as informações de localização sejam obtidas (tal como uma cidade, o CEP, ou nível estadual), de modo que uma localização particular de um usuário não possa ser determinada. Desse modo, o usuário pode ter controle sobre como informações são coletadas sobre o usuário e usadas pelo servidor de web ou servidor de mídia.[0095] In situations where the systems discussed in this document collect personal information about users, or may make use of personal information, users may be provided with an opportunity to control whether programs or features collect user information (for example, information about a user's social network, social actions or activities, occupation, a user's preferences, or a user's current location), or to control whether and/or how they receive content from the media server that may be more relevant to the user. In addition, certain data may be processed in one or more ways before being stored or used, so that personally identifiable information is removed. For example, a user's identity may be treated in such a way that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as a city, zip code, or state level) so that a user's particular location cannot be determined. In this way, the user can have control over how information is collected about the user and used by the web server or media server.
[0096] A referência por todo este relatório descritivo a "uma (1) implantação" ou "uma implantação" significa que um recurso, estrutura, ou característica particular descrito em combinação com as implantações é incluído em pelo menos uma implantação. Desse modo, os aparecimentos das frases "em uma (1) implantação" ou "em uma implantação" em vários lugares por todo o relatório descritivo não necessariamente faz referência à mesma implantação. Além disso, o termo "Ou" é destinado a significar um "ou" inclusivo ao invés de um "ou" exclusivo.[0096] Reference throughout this specification to "one (1) deployment" or "a deployment" means that a particular feature, structure, or characteristic described in combination with deployments is included in at least one deployment. Accordingly, appearances of the phrases "in one (1) site" or "in one site" in various places throughout the specification do not necessarily refer to the same site. Furthermore, the term "Or" is intended to mean an inclusive "or" rather than an exclusive "or".
[0097] Embora as operações dos métodos no presente documento sejam mostradas e descritas em uma ordem particular, a ordem das operações de cada método pode ser alterada de modo que certas operações possam ser executadas em uma ordem inversa ou de modo que uma certa operação possa ser executada, pelo menos em parte, concomitantemente com outras operações. Em uma outra implantação, instruções ou suboperações de operações distintas podem ser de um modo intermitente e/ou alternativo.[0097] Although the operations of the methods in this document are shown and described in a particular order, the order of operations of each method can be changed so that certain operations can be performed in reverse order or so that a certain operation can be performed, at least in part, concurrently with other operations. In another implementation, instructions or suboperations of distinct operations may be intermittent and/or alternate.
Claims (7)
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361906588P | 2013-11-20 | 2013-11-20 | |
US61/906,588 | 2013-11-20 | ||
US14/323,807 | 2014-07-03 | ||
US14/323,807 US10754511B2 (en) | 2013-11-20 | 2014-07-03 | Multi-view audio and video interactive playback |
PCT/US2014/066713 WO2015077507A1 (en) | 2013-11-20 | 2014-11-20 | Multi-view audio and video interactive playback |
Publications (3)
Publication Number | Publication Date |
---|---|
BR112016009772A2 BR112016009772A2 (en) | 2017-08-01 |
BR112016009772A8 BR112016009772A8 (en) | 2018-01-02 |
BR112016009772B1 true BR112016009772B1 (en) | 2023-05-30 |
Family
ID=
Similar Documents
Publication | Publication Date | Title |
---|---|---|
AU2022204875B2 (en) | Multi-view audio and video interactive playback | |
US10652605B2 (en) | Visual hot watch spots in content item playback | |
US9002175B1 (en) | Automated video trailer creation | |
CN112584086A (en) | Real-time video transformation in video conferencing | |
JP2022536182A (en) | System and method for synchronizing data streams | |
US20160212487A1 (en) | Method and system for creating seamless narrated videos using real time streaming media | |
BR112018009836B1 (en) | METHOD, NON-TRAINER COMPUTER READABLE MEDIUM, AND SYSTEM FOR TOUCH GESTURE CONTROL OF VIDEO PLAYBACK | |
US20150130816A1 (en) | Computer-implemented methods and systems for creating multimedia animation presentations | |
WO2017113717A1 (en) | Video playing method, video player, and electronic device | |
BR112016017260B1 (en) | METHOD TO IMPROVE OFFLINE CONTENT REPRODUCTION | |
WO2013123789A1 (en) | Image processing method and image processing device | |
US20150128040A1 (en) | Generating custom sequences of video streams | |
KR20190096576A (en) | Toys social network service for making user producing and sharing short clip video | |
Li et al. | Emerging technologies and applications on interactive entertainments | |
BR112016009772B1 (en) | METHOD, MACHINE READABLE NON-TRANSITORY STORAGE MEDIA AND SERVER COMPUTER SYSTEM FOR INTERACTIVE AUDIO AND VIDEO PLAYBACK FROM MULTIPLE VIEWPOINTS | |
Grundmann | Computational video: post-processing methods for stabilization, retargeting and segmentation | |
US10296592B2 (en) | Spherical video in a web browser | |
Potetsianakis et al. | SWAPUGC: software for adaptive playback of geotagged UGC | |
US11595636B1 (en) | Enhanced emotive engagement with volumetric content | |
Turnidge | Hypervideo: interface design for collaborative documentaries: exegesis submitted in partial fulfilment of the Master of Design at Massey University, Wellington, New Zealand | |
Uribe et al. | New usability evaluation model for a personalized adaptive media search engine based on interface complexity metrics | |
Taskan | Computational Abstraction of Films for Quantitave Analysis of Cinematography |