JP2023175029A - Attention-based joint acoustic and text on-device end-to-end model - Google Patents
Attention-based joint acoustic and text on-device end-to-end model Download PDFInfo
- Publication number
- JP2023175029A JP2023175029A JP2023183357A JP2023183357A JP2023175029A JP 2023175029 A JP2023175029 A JP 2023175029A JP 2023183357 A JP2023183357 A JP 2023183357A JP 2023183357 A JP2023183357 A JP 2023183357A JP 2023175029 A JP2023175029 A JP 2023175029A
- Authority
- JP
- Japan
- Prior art keywords
- acoustic
- probability distribution
- decoder
- text
- labels
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 239000013598 vector Substances 0.000 claims abstract description 66
- 238000000034 method Methods 0.000 claims abstract description 45
- 230000015654 memory Effects 0.000 claims description 33
- 238000012545 processing Methods 0.000 claims description 33
- 238000009826 distribution Methods 0.000 claims description 28
- 238000013528 artificial neural network Methods 0.000 claims description 15
- 230000007246 mechanism Effects 0.000 claims description 14
- 238000004891 communication Methods 0.000 claims description 11
- 238000013518 transcription Methods 0.000 claims description 11
- 230000035897 transcription Effects 0.000 claims description 11
- 238000003058 natural language processing Methods 0.000 claims description 8
- 230000000306 recurrent effect Effects 0.000 claims description 7
- 230000009471 action Effects 0.000 claims description 4
- 238000012549 training Methods 0.000 abstract description 83
- 238000003062 neural network model Methods 0.000 abstract description 12
- 238000004590 computer program Methods 0.000 description 8
- 230000008569 process Effects 0.000 description 8
- 238000013459 approach Methods 0.000 description 7
- 230000006870 function Effects 0.000 description 7
- 238000010586 diagram Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000009467 reduction Effects 0.000 description 3
- 230000001934 delay Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000002156 mixing Methods 0.000 description 2
- 239000000203 mixture Substances 0.000 description 2
- 230000005236 sound signal Effects 0.000 description 2
- 238000006467 substitution reaction Methods 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 238000003491 array Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 230000007812 deficiency Effects 0.000 description 1
- 238000012217 deletion Methods 0.000 description 1
- 230000037430 deletion Effects 0.000 description 1
- 238000004821 distillation Methods 0.000 description 1
- 238000003780 insertion Methods 0.000 description 1
- 230000037431 insertion Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 238000000275 quality assurance Methods 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
- G10L2015/0635—Training updating or merging of old and new templates; Mean values; Weighting
Abstract
Description
本開示は、２パスのエンド・ツー・エンドの音声認識に関する。 The present disclosure relates to two-pass end-to-end speech recognition.
最新の自動音声認識（ＡＳＲ：ａｕｔｏｍａｔｅｄ ｓｐｅｅｃｈ ｒｅｃｏｇｎｉｔｉｏｎ）システムは、高品質（例えば、低い単語誤り率（ＷＥＲ：ｗｏｒｄ ｅｒｒｏｒ ｒａｔｅ））のみならず、低遅延（例えば、ユーザが話してから文字起こし（ｔｒａｎｓｃｒｉｐｔｉｏｎ）が表示されるまでの短い遅延）を提供することに重点を置いている。さらに、現在ＡＳＲシステムを使用する場合、ＡＳＲシステムは、リアルタイムに対応するか、またはリアルタイムよりもさらに高速に対応するストリーミング方式で発話をデコードすることが要求されている。例えば、ユーザとの直接対話を行う携帯電話にＡＳＲシステムが搭載されている場合、ＡＳＲシステムを使用する携帯電話上のアプリケーションは、単語が話されるとすぐに画面上に表示されるように音声認識がストリーミングされることを必要とする場合がある。ここで、携帯電話のユーザは、遅延に対する許容度が低い可能性もある。この低い許容度により、音声認識は、ユーザエクスペリエンスに悪影響を与える可能性のある遅延および不正確性による影響を最小限に抑えるようにモバイルデバイス上で動作することを目指している。 Modern automated speech recognition (ASR) systems provide not only high quality (e.g., low word error rate (WER)) but also low latency (e.g., the ability to transcribe text after a user speaks). ). Furthermore, when using an ASR system, the ASR system is currently required to decode utterances in a streaming manner that supports real-time or even faster than real-time. For example, if a mobile phone that interacts directly with the user is equipped with an ASR system, an application on the phone that uses the ASR system can use audio to display words on the screen as soon as they are spoken. Recognition may need to be streamed. Here, mobile phone users may also have low tolerance for delays. With this low tolerance, speech recognition aims to operate on mobile devices in a way that minimizes the impact of delays and inaccuracies that can negatively impact the user experience.
本開示の一態様は、データ処理ハードウェア上での実行時に、データ処理ハードウェアに動作を実行させるコンピュータが実施する方法を提供し、動作は、２パスストリーミングニューラルネットワークモデルのリッスン・アテンド・スペル（ＬＡＳ：ｌｉｓｔｅｎ－ａｔｔｅｎｄ－ｓｐｅｌｌ）デコーダのトレーニング例を受信すること、およびトレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのテキストシーケンスに対応しているかを決定することを含む。動作は、トレーニング例が非ペアのテキストシーケンスに対応している場合、トレーニング例のコンテキストベクトルに関連付けられた対数確率（ｌｏｇ ｐｒｏｂａｂｉｌｉｔｙ）に基づいてクロスエントロピー損失（ｃｒｏｓｓ ｅｎｔｒｏｐｙ ｌｏｓｓ）を決定すること、および決定されたクロスエントロピー損失に基づいてＬＡＳデコーダおよびコンテキストベクトルを更新することも含む。 One aspect of the present disclosure provides a computer-implemented method for causing data processing hardware to perform an operation when executing on data processing hardware, the operation comprising: a listen-attend spelling of a two-pass streaming neural network model; listen-attend-spell (LAS) to receive training examples for a decoder and to determine whether the training examples correspond to supervised speech-text pairs or unpaired text sequences. include. The operations include determining a cross entropy loss based on a log probability associated with a context vector of the training example if the training example corresponds to an unpaired text sequence; It also includes updating the LAS decoder and context vector based on the determined cross-entropy loss.
本開示の実施形態は、以下の任意の機能のうちの１つまたは複数を含み得る。いくつかの実施形態では、動作は、２パスストリーミングニューラルネットワークのＬＡＳデコーダに関する第２のトレーニング例を受信すること、第２のトレーニング例が教師付き音声・テキストペアに対応していると決定すること、および音響コンテキストベクトルに関する対数確率に基づいて、ＬＡＳデコーダおよび音響コンテキストベクトルに関連付けられた音響コンテキストベクトルパラメータを更新することも含む。いくつかの例では、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのテキストシーケンスに対応しているかを決定することは、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのテキストシーケンスに対応しているかを示すドメイン識別子を識別することを含む。 Embodiments of the present disclosure may include one or more of any of the following features. In some embodiments, the operations include receiving a second training example for a LAS decoder of a two-pass streaming neural network, and determining that the second training example corresponds to a supervised speech-text pair. , and updating acoustic context vector parameters associated with the LAS decoder and the acoustic context vector based on the log probability for the acoustic context vector. In some examples, determining whether a training example corresponds to a supervised audio-text pair or an unpaired text sequence may be difficult if the training example corresponds to a supervised audio-text pair. the domain identifier that corresponds to the domain name or unpaired text sequence.
追加の実施形態では、ＬＡＳデコーダを更新することにより、ロングテールエンティティに関する２パスストリーミングニューラルネットワークモデルの単語誤り率（ＷＥＲ）を低減させる。対数確率は、音響コンテキストベクトルから生成された第１の個々の対数確率およびテキストコンテキストベクトルから生成された第２の個々の対数確率の補間（ｉｎｔｅｒｐｏｌａｔｉｏｎ）によって定義することができる。さらに、ＬＡＳデコーダは、２パスストリーミングニューラルネットワークモデルの第１のパスを通じてリカレントニューラルネットワークトランスデューサ（ＲＮＮ－Ｔ：ｒｅｃｕｒｒｅｎｔ ｎｅｕｒａｌ ｎｅｔｗｏｒｋ ｔｒａｎｓｄｕｃｅｒ）デコーダによって生成された仮説に基づいてビームサーチモードで動作し得る。いくつかの例では、動作は、エンコードされた音響フレームからエンコーダの特徴を要約する（ｓｕｍｍａｒｉｚｅ）ように構成されたアテンション機構を用いて、トレーニング例のコンテキストベクトルを生成することも含む。 In additional embodiments, updating the LAS decoder reduces the word error rate (WER) of a two-pass streaming neural network model for long tail entities. A log probability may be defined by the interpolation of a first individual log probability generated from an acoustic context vector and a second individual log probability generated from a textual context vector. Additionally, the LAS decoder may operate in a beam search mode based on hypotheses generated by a recurrent neural network transducer (RNN-T) decoder through a first pass of a two-pass streaming neural network model. In some examples, the operations also include generating a context vector of the training example using an attention mechanism configured to summarize features of the encoder from the encoded acoustic frames.
本開示の別の態様は、データ処理ハードウェア上での実行時に、データ処理ハードウェアに動作を実行させるコンピュータが実施する方法を提供し、動作は、２パスストリーミングニューラルネットワークモデルのリッスン・アテンド・スペル（ＬＡＳ）デコーダのトレーニング例を受信すること、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのトレーニングデータに対応しているかを決定すること、トレーニング例が非ペアのトレーニングデータに対応している場合、非ペアのトレーニングデータの欠落部分を生成して、生成音声・テキストペアを形成すること、生成音声・テキストペアに基づいて、ＬＡＳデコーダおよび非ペアのデータに関連付けられたコンテキストベクトルを更新することを含む。 Another aspect of the present disclosure provides a computer-implemented method of causing data processing hardware to perform an operation when executing on data processing hardware, the operation comprising: listening, attending, receiving training examples for a LAS decoder; determining whether the training examples correspond to supervised speech-text pairs or unpaired training data; determining whether the training examples correspond to unpaired training data; If it corresponds to the training data, generating the missing part of the unpaired training data to form a generated speech-text pair, and associating it with the LAS decoder and the unpaired data based on the generated speech-text pair. This includes updating the referenced context vector.
この態様は、以下の任意の特徴のうちの１つまたは複数を含み得る。いくつかの実施形態では、動作は、生成音声・テキストペアに基づいて音響コンテキストベクトルを決定すること、および音響コンテキストベクトルから生成された第１の個々の対数確率およびテキストコンテキストベクトルから生成された第２の個々の対数確率の補間を決定することも含む。これらの実施形態では、ＬＡＳデコーダを更新することは、第１の個々の対数確率および第２の個々の対数確率の補間にさらに基づく。 This aspect may include one or more of the following any features. In some embodiments, the operations include determining an acoustic context vector based on the generated speech-text pair, and a first individual log probability generated from the acoustic context vector and a first individual log probability generated from the text context vector. It also includes determining an interpolation of the individual log probabilities of 2. In these embodiments, updating the LAS decoder is further based on interpolation of the first individual log-probability and the second individual log-probability.
いくつかの例では、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのトレーニングデータに対応しているかを決定することは、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのトレーニングデータに対応しているかを示すドメイン識別子を識別することを含む。さらに、ＬＡＳデコーダを更新することにより、ロングテールエンティティに関する２パスストリーミングニューラルネットワークモデルの単語誤り率（ＷＥＲ）を低減させ得る。いくつかの実施形態では、動作は、エンコードされた音響フレームからエンコーダの特徴を要約するように構成されたアテンション機構を用いて、トレーニング例のコンテキストベクトルを生成することも含む。 In some examples, determining whether a training example corresponds to a supervised audio-text pair or unpaired training data may be difficult if the training example corresponds to a supervised audio-text pair. or unpaired training data. Furthermore, updating the LAS decoder may reduce the word error rate (WER) of the two-pass streaming neural network model for long tail entities. In some embodiments, the operations also include generating a training example context vector using an attention mechanism configured to summarize encoder features from the encoded acoustic frames.
本開示のさらに別の態様は、データ処理ハードウェアと、データ処理ハードウェアと通信し、かつ命令を格納するメモリハードウェアとを備えるシステムを開示し、命令は、データ処理ハードウェアによる実行時に、データ処理ハードウェアに動作を実行させ、動作は、２パスストリーミングニューラルネットワークモデルのリッスン・アテンド・スペル（ＬＡＳ）デコーダに関するトレーニング例を受信すること、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのテキストシーケンスに対応しているかを決定することを含む。動作は、トレーニング例が非ペアのテキストシーケンスに対応している場合、トレーニング例のコンテキストベクトルに関連付けられた対数確率に基づいてクロスエントロピー損失を決定すること、および決定されたクロスエントロピー損失に基づいてＬＡＳデコーダおよびコンテキストベクトルを更新することも含む。 Yet another aspect of the present disclosure discloses a system comprising data processing hardware and memory hardware in communication with the data processing hardware and storing instructions, wherein the instructions, upon execution by the data processing hardware, causing data processing hardware to perform operations, the operations comprising: receiving training examples for a listen-attend-spelling (LAS) decoder of a two-pass streaming neural network model, the training examples corresponding to supervised speech-text pairs; or unpaired text sequences. The operation consists of determining a cross-entropy loss based on the log probability associated with the context vector of the training example, if the training example corresponds to an unpaired text sequence, and based on the determined cross-entropy loss Also includes updating the LAS decoder and context vector.
この態様は、以下の任意の特徴のうちの１つまたは複数を含み得る。いくつかの実施形態では、動作は、２パスストリーミングニューラルネットワークのＬＡＳデコーダの第２のトレーニング例を受信すること、第２のトレーニング例が教師付き音声・テキストペアに対応することを決定すること、および音響コンテキストベクトルの対数確率に基づいてＬＡＳデコーダおよび音響コンテキストベクトルに関連付けられた音響コンテキストベクトルパラメータを更新することも含む。いくつかの例では、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのテキストシーケンスに対応しているかを決定することは、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのテキストシーケンスに対応しているかを示すドメイン識別子を識別することを含む。 This aspect may include one or more of the following any features. In some embodiments, the operations include receiving a second training example of the LAS decoder of the two-pass streaming neural network, determining that the second training example corresponds to a supervised audio-text pair; and updating acoustic context vector parameters associated with the LAS decoder and the acoustic context vector based on the log probability of the acoustic context vector. In some examples, determining whether a training example corresponds to a supervised audio-text pair or an unpaired text sequence may be difficult if the training example corresponds to a supervised audio-text pair. the domain identifier that corresponds to the domain name or unpaired text sequence.
追加の実施形態では、ＬＡＳデコーダを更新することにより、ロングテールエンティティに関する２パスストリーミングニューラルネットワークモデルの単語誤り率（ＷＥＲ）を低減させる。対数確率は、音響コンテキストベクトルから生成された第１の個々の対数確率およびテキストコンテキストベクトルから生成された第２の個々の対数確率の補間によって定義することができる。さらに、ＬＡＳデコーダは、２パスストリーミングニューラルネットワークモデルの第１のパスを通じてリカレントニューラルネットワークトランスデューサ（ＲＮＮ－Ｔ）デコーダによって生成された仮説に基づいてビームサーチモードで動作し得る。いくつかの例では、動作は、エンコードされた音響フレームからエンコーダの特徴を要約するように構成されたアテンション機構を用いて、トレーニング例のコンテキストベクトルを生成することも含む。 In additional embodiments, updating the LAS decoder reduces the word error rate (WER) of a two-pass streaming neural network model for long tail entities. A log probability may be defined by interpolation of a first individual log probability generated from the acoustic context vector and a second individual log probability generated from the textual context vector. Additionally, the LAS decoder may operate in a beam search mode based on hypotheses generated by a recurrent neural network transducer (RNN-T) decoder through a first pass of a two-pass streaming neural network model. In some examples, the operations also include generating a context vector for the training example using an attention mechanism configured to summarize features of the encoder from the encoded acoustic frames.
本開示のさらに別の態様は、データ処理ハードウェアと、データ処理ハードウェアと通信し、かつ命令を格納するメモリハードウェアとを備えるシステムを提供し、命令は、データ処理ハードウェアによる実行時に、データ処理ハードウェアに動作を実行させ、動作は、２パスストリーミングニューラルネットワークモデルのリッスン・アテンド・スペル（ＬＡＳ）デコーダ用のトレーニング例の受信すること、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのトレーニングデータに対応しているかを決定すること、トレーニング例が非ペアのトレーニングデータに対応している場合、非ペアのトレーニングデータの欠落部分を生成して、生成音声・テキストペア形成すること、および生成音声・テキストペアに基づいてＬＡＳデコーダおよび非ペアのデータに関連付けられたコンテキストベクトルを更新することを含む。 Yet another aspect of the present disclosure provides a system comprising data processing hardware and memory hardware in communication with the data processing hardware and storing instructions, wherein the instructions, upon execution by the data processing hardware, causing the data processing hardware to perform an operation, the operation receiving a training example for a listen-attend-spelling (LAS) decoder of a two-pass streaming neural network model, the training example corresponding to a supervised speech-text pair; If the training example corresponds to unpaired training data, then the missing portion of the unpaired training data is generated to generate the generated speech. forming a text pair and updating a LAS decoder and a context vector associated with the unpaired data based on the generated speech-text pair.
この態様は、以下の任意の特徴のうちの１つまたは複数を含み得る。いくつかの実施形態では、動作は、生成音声・テキストペアに基づいて音響コンテキストベクトルを決定すること、および音響コンテキストベクトルから生成された第１の個々の対数確率およびテキストコンテキストベクトルから生成された第２の個々の対数確率の補間を決定することも含む。これらの実施形態では、ＬＡＳデコーダを更新することは、第１の個々の対数確率および第２の個々の対数確率の補間にさらに基づく。 This aspect may include one or more of the following any features. In some embodiments, the operations include determining an acoustic context vector based on the generated speech-text pair, and a first individual log probability generated from the acoustic context vector and a first individual log probability generated from the text context vector. It also includes determining an interpolation of the individual log probabilities of 2. In these embodiments, updating the LAS decoder is further based on interpolation of the first individual log-probability and the second individual log-probability.
いくつかの例では、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのトレーニングデータに対応しているかを決定することは、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのトレーニングデータに対応しているかを示すドメイン識別子を識別することを含む。さらに、ＬＡＳデコーダを更新することにより、ロングテールエンティティに関する２パスストリーミングニューラルネットワークモデルの単語誤り率（ＷＥＲ）を低減させ得る。いくつかの実施形態では、動作は、エンコードされた音響フレームからエンコーダの特徴を要約するように構成されたアテンション機構を用いて、トレーニング例のコンテキストベクトルを生成することも含む。 In some examples, determining whether a training example corresponds to a supervised audio-text pair or unpaired training data may be difficult if the training example corresponds to a supervised audio-text pair. or unpaired training data. Furthermore, updating the LAS decoder may reduce the word error rate (WER) of the two-pass streaming neural network model for long tail entities. In some embodiments, the operations also include generating a training example context vector using an attention mechanism configured to summarize encoder features from the encoded acoustic frames.
本開示の１つまたは複数の実施の詳細は、添付の図面および以下の詳細な説明に記載されている。他の態様、特徴、および利点は、詳細な説明および図面、ならびに特許請求の範囲から明らかになる。 The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the detailed description below. Other aspects, features, and advantages will be apparent from the detailed description and drawings, and from the claims.
様々な図面の同様の参照記号は、同様の構成要素を示す。
音声認識は、モバイル環境の非拘束性および機敏性の要求を満たすために進化し続けている。自動音声認識システム（ＡＳＲ）の品質を向上させるために、新たな音声認識アーキテクチャまたは既存のアーキテクチャの改良が引き続き開発されている。例えば、音声認識は、当初、各モデルが専用の目的を持つ複数のモデルを採用していた。例えば、ＡＳＲシステムは、音響モデル（ＡＭ）、発音モデル（ＰＭ）、および言語モデル（ＬＭ）を含んでいた。音響モデルは、音声のセグメント（即ち、音声のフレーム）を音素（ｐｈｏｎｅｍｅｓ）にマッピングした。発音モデルは、これらの音素をつなぎ合わせて単語を形成し、言語モデルは、所与のフレーズの可能性（即ち、単語のシーケンスの確率）を表現するために使用された。これらの個々のモデルは連携して機能したが、各モデルは個別にトレーニングされ、多くの場合、異なるデータセットに対して手動で設計された。
Like reference symbols in the various drawings indicate similar components.
Speech recognition continues to evolve to meet the demands of freedom and agility in mobile environments. New speech recognition architectures or improvements to existing architectures continue to be developed to improve the quality of automatic speech recognition systems (ASR). For example, speech recognition initially employed multiple models, each with a dedicated purpose. For example, the ASR system included an acoustic model (AM), a pronunciation model (PM), and a language model (LM). The acoustic model mapped segments of speech (ie, frames of speech) to phonemes. A pronunciation model joins these phonemes together to form words, and a language model was used to represent the probabilities of a given phrase (ie, the probabilities of a sequence of words). Although these individual models worked together, each model was trained separately and was often manually designed on different datasets.
個別のモデルのアプローチにより、特に所与のモデルに対するトレーニングコーパス（即ち、トレーニングデータの集合体）がモデルの有効性に対応している場合に、音声認識システムの精度をかなり向上させることが可能になったが、個別のモデルを個別にトレーニングする必要性により、それ自体が複雑になるため、統合モデルを備えたアーキテクチャが採用された。これらの統合モデルは、単一のニューラルネットワークを使用して、音声波形（即ち、入力シーケンス）を出力センテンス（即ち、出力シーケンス）に直接マッピングしようとするものである。これにより、任意の音声特徴のシーケンスが与えられると、単語（または書記素（ｇｒａｐｈｅｍｅｓ））のシーケンスが生成されるシーケンス・ツー・シーケンスのアプローチが実現された。シーケンス・ツー・シーケンスモデルの例には、「アテンションベース」モデルおよび「リッスン・アテンド・スペル」（ＬＡＳ）モデルが含まれる。ＬＡＳモデルは、リスナー（ｌｉｓｔｅｎｅｒ）コンポーネント、アテンダ（ａｔｔｅｎｄｅｒ）コンポーネント、およびスペラー（ｓｐｅｌｌｅｒ）コンポーネントを使用して、音声の発話を文字に変換する。ここで、リスナーは、音声入力（例えば、音声入力の時間周波数表現）を受信し、音声入力をより高レベルの特徴表現にマッピングするリカレントニューラルネットワーク（ＲＮＮ：ｒｅｃｕｒｒｅｎｔ ｎｅｕｒａｌ ｎｅｔｗｏｒｋ）エンコーダである。アテンダは、より高レベルの特徴にアテンションを向けて、入力特徴と予測されるサブワード単位（例えば、書記素または単語ピース）との間のアラインメントを学習する。スペラーは、アテンションベースのＲＮＮデコーダであり、仮説単語のセットに対して確率分布を生成することによって、入力から文字シーケンスを生成する。統合化された構造により、モデルの全てのコンポーネントを単一のエンド・ツー・エンド（Ｅ２Ｅ：ｅｎｄ－ｔｏ－ｅｎｄ）ニューラルネットワークとしてジョイントでトレーニングさせることができる。ここで、Ｅ２Ｅモデルとは、アーキテクチャが全てニューラルネットワークで構成されているモデルを指す。完全なニューラルネットワークは、外部コンポーネントおよび／または手動で設計したコンポーネント（例えば、有限状態トランスデューサ、辞書（ｌｅｘｉｃｏｎ）、またはテキスト正規化モジュール）なしで機能する。さらに、Ｅ２Ｅモデルをトレーニングする場合、これらのモデルは通常、決定木からのブートストラップ、または別のシステムからの時間調整を必要としない。 The individual model approach can significantly improve the accuracy of speech recognition systems, especially when the training corpus (i.e., the collection of training data) for a given model corresponds to the model's effectiveness. However, the need to train separate models separately adds complexity, so an architecture with an integrated model was adopted. These integrated models attempt to directly map speech waveforms (i.e., input sequences) to output sentences (i.e., output sequences) using a single neural network. This has led to a sequence-to-sequence approach in which, given an arbitrary sequence of phonetic features, a sequence of words (or graphemes) is generated. Examples of sequence-to-sequence models include "attention-based" models and "listen-attend-spell" (LAS) models. The LAS model uses listener, attender, and speller components to convert audio utterances to text. Here, the listener is a recurrent neural network (RNN) encoder that receives audio input (eg, a time-frequency representation of the audio input) and maps the audio input to a higher level feature representation. The attender directs attention to higher-level features to learn alignments between input features and predicted subword units (eg, graphemes or word pieces). Speller is an attention-based RNN decoder that generates character sequences from input by generating probability distributions over a set of hypothetical words. The unified structure allows all components of the model to be trained jointly as a single end-to-end (E2E) neural network. Here, the E2E model refers to a model whose architecture is entirely composed of neural networks. A complete neural network functions without external and/or manually designed components (eg, finite state transducers, lexicons, or text normalization modules). Furthermore, when training E2E models, these models typically do not require bootstrapping from decision trees or time adjustments from another system.
初期のＥ２Ｅモデルは正確であり、個別にトレーニングされたモデルよりもトレーニングが改善されたが、ＬＡＳモデルなどのこれらのＥ２Ｅモデルは、出力テキストを生成する前に入力シーケンス全体を確認することによって機能していたため、入力が受信されたときに出力をストリーミングすることはできなかった。ストリーミング機能がないと、ＬＡＳモデルは、リアルタイムの音声文字起こし（ｖｏｉｃｅ ｔｒａｎｓｃｒｉｐｔｉｏｎ）を実行することができない。この欠陥のため、遅延に敏感な、かつ／またはリアルタイムの音声文字起こしを必要とする音声アプリケーションに対してＬＡＳモデルを搭載すると、問題が発生する可能性がある。このため、リアルタイムアプリケーション（例えば、リアルタイム通信アプリケーション）に依存することが多いモバイル技術（例えば、携帯電話）にとって、ＬＡＳモデルだけでは、理想的なモデルではない。 While earlier E2E models were accurate and had improved training over individually trained models, these E2E models, such as the LAS model, work by looking at the entire input sequence before producing output text. Because the input was received, it was not possible to stream the output. Without streaming capabilities, LAS models cannot perform real-time voice transcription. This deficiency can cause problems when implementing the LAS model for speech applications that are delay sensitive and/or require real-time speech transcription. Therefore, the LAS model alone is not an ideal model for mobile technologies (eg, cell phones) that often rely on real-time applications (eg, real-time communication applications).
さらに、音響モデル、発音モデル、および言語モデル、またはそれらが共に構成されているモデルを有する音声認識システムは、これらのモデルに関連する比較的大規模のサーチグラフをサーチする必要があるデコーダに依存し得る。大規模のサーチグラフでは、この種の音声認識システムを完全オンデバイスでホストするのに有利ではない。ここで、音声認識システムが「オンデバイス（ｏｎ－ｄｅｖｉｃｅ）」でホストされている場合、音声入力を受信するデバイスは、そのプロセッサ（単数または複数）を使用して音声認識システムの機能を実行する。例えば、音声認識システムが完全にオンデバイスでホストされている場合、デバイスのプロセッサは、音声認識システムの機能を実行するために、デバイス外のコンピューティングリソースと連携する必要はない。完全にオンデバイスではない音声認識を実行するデバイスは、音声認識システムの少なくとも一部の機能を実行するために、リモートコンピューティング（例えば、リモートコンピューティングシステムまたはクラウドコンピューティング）、従ってオンライン接続に依存している。例えば、音声認識システムは、サーバベースのモデルとのネットワーク接続を使用して、大規模なサーチグラフによりデコーディングを実行する。 Furthermore, speech recognition systems that have acoustic, pronunciation, and language models, or models in which they are constructed together, rely on decoders that have to search relatively large search graphs associated with these models. It is possible. Large search graphs do not favor hosting this type of speech recognition system completely on-device. Here, if the speech recognition system is hosted "on-device", the device that receives the speech input uses its processor(s) to perform the functions of the speech recognition system. . For example, if a speech recognition system is hosted entirely on-device, the device's processor does not need to coordinate with computing resources outside the device to perform the functions of the speech recognition system. Devices that perform speech recognition that are not completely on-device rely on remote computing (e.g., remote computing systems or cloud computing) and therefore online connectivity to perform at least some functions of the speech recognition system. are doing. For example, speech recognition systems use network connections with server-based models to perform decoding through large search graphs.
残念ながら、リモート接続に依存している状態では、音声認識システムは、遅延の問題および／または通信ネットワークに固有の信頼性の低さに対して脆弱になる。これらの問題を回避することによって音声認識の有用性を向上させるために、音声認識システムは、リカレントニューラルネットワークトランスデューサ（ＲＮＮ－Ｔ）として知られるシーケンス・ツー・シーケンス（ｓｅｑｕｅｎｃｅ－ｔｏ－ｓｅｑｕｅｎｃｅ）モデルの形態に再び進化した。ＲＮＮ－Ｔはアテンション機構を採用しておらず、かつ出力（例えば、センテンス）を生成するためにシーケンス全体（例えば、音声波形）を処理する必要がある他のシーケンス・ツー・シーケンスモデルとは異なり、ＲＮＮ－Ｔは、入力サンプルを連続的に処理して、出力シンボルをストリーミングするという、リアルタイム通信にとって特に魅力的な特徴を有している。例えば、ＲＮＮ－Ｔを使用した音声認識では、話した通りに文字が１つずつ出力され得る。ここで、ＲＮＮ－Ｔは、モデルによって予測されたシンボルを自身にフィードバックするフィードバックループを使用して、次のシンボルを予測する。ＲＮＮ－Ｔのデコーディングは、大規模なデコーダグラフではなく、単一のニューラルネットワークを介したビームサーチを含むため、ＲＮＮ－Ｔは、サーバベースの音声認識モデルの数分の１のサイズにスケーリングすることができる。サイズの縮小により、ＲＮＮ－Ｔは完全にオンデバイスで搭載され、オフラインで（即ち、ネットワーク接続なしで）動作させることができるため、通信ネットワークの信頼性の問題を回避することができる。 Unfortunately, relying on remote connections makes voice recognition systems vulnerable to latency issues and/or the inherent unreliability of communication networks. To improve the usefulness of speech recognition by avoiding these problems, speech recognition systems rely on a sequence-to-sequence model known as a recurrent neural network transducer (RNN-T). The form has evolved again. RNN-T does not employ an attention mechanism and unlike other sequence-to-sequence models, which require processing the entire sequence (e.g., speech waveform) to generate an output (e.g., sentence) , RNN-T has a particularly attractive feature for real-time communications: it continuously processes input samples and streams output symbols. For example, in speech recognition using RNN-T, characters can be output one by one as spoken. Here, the RNN-T uses a feedback loop that feeds back the symbols predicted by the model to itself to predict the next symbol. Because RNN-T decoding involves beam searching through a single neural network rather than a large decoder graph, RNN-T scales to a fraction of the size of server-based speech recognition models. can do. Due to the reduced size, the RNN-T can be implemented completely on-device and operated offline (ie, without network connectivity), thus avoiding communication network reliability issues.
音声認識システムが低遅延で動作することに加えて、音声認識システムには、音声を正確に認識することが求められる。音声認識を実行するモデルの場合、モデルの精度を定義するメトリックとして、単語誤り率（ＷＥＲ）が用いられることが多い。ＷＥＲは、実際に話された単語の数と比較して、どれだけ単語が変更されたかを示す尺度である。一般に、これらの単語の変更は、置換（即ち、単語が置き換えられる場合）、挿入（即ち、単語が追加される場合）、および／または削除（即ち、単語が省略される場合）を指す。例えば、話者は「カー（ｃａｒ）」と言っているが、ＡＳＲシステムは、「カー（ｃａｒ）」という単語を「バー（ｂａｒ）」と文字起こしする。これは、音素（ｐｈｏｎｅｔｉｃ）の類似性による置換の例である。他のＡＳＲシステムと比較してＡＳＲシステムの能力を測定する場合、ＷＥＲは、別のシステムまたはあるベースラインと比較して、改善または品質保証能力の尺度を示すことができる。 In addition to operating with low latency, speech recognition systems are required to accurately recognize speech. For models that perform speech recognition, word error rate (WER) is often used as a metric to define the accuracy of the model. WER is a measure of how many words are changed compared to the number of words actually spoken. Generally, these word modifications refer to substitutions (ie, where a word is replaced), insertions (ie, where a word is added), and/or deletions (ie, where a word is omitted). For example, a speaker says "car" and the ASR system transcribes the word "car" as "bar." This is an example of substitution by phonetic similarity. When measuring the capabilities of an ASR system compared to other ASR systems, WER can indicate a measure of improvement or quality assurance ability compared to another system or some baseline.
ＲＮＮ－Ｔモデルは、オンデバイスの音声認識に関する有力な候補モデルとして有望であることを示したが、ＲＮＮ－Ｔモデルのみでは、品質（例えば、音声認識精度）の観点で、大規模な最先端の従来モデル（例えば、別個のＡＭ、ＰＭ、およびＬＭを備えたサーバベースのモデル）に遅れをとっている。しかし、非ストリーミングＥ２Ｅ，ＬＡＳモデルは、大規模な最先端の従来モデルに匹敵する音声認識品質を備えている。非ストリーミングＥ２Ｅ ＬＡＳモデルの品質を活用するために、ＲＮＮ－Ｔネットワークの第１のパスのコンポーネントと、それに続くＬＡＳネットワークの第２のパスのコンポーネントとを含む２パス音声認識システム（例えば、図２Ａに示す）が開発された。この設計により、２パスモデルは、低遅延のＲＮＮ－Ｔモデルのストリーミング特性の恩恵を受け、ＬＡＳネットワークを組み込んだ第２のパスを通じてＲＮＮ－Ｔモデルの精度を向上させている。ＬＡＳネットワークは、ＲＮＮ－Ｔモデルのみと比較して遅延を増加させるが、遅延の増加は、適度にわずかであり、かつオンデバイス動作に関する遅延制約に適合している。精度に関しては、２パスモデルは、ＲＮＮ－Ｔ単独と比較した場合に１７～２２％のＷＥＲ低減を達成し、大規模な従来モデルと比較した場合に同程度のＷＥＲを有している。 Although the RNN-T model has shown promise as a promising candidate model for on-device speech recognition, the RNN-T model alone is not suitable for large-scale state-of-the-art technology in terms of quality (e.g., speech recognition accuracy). traditional models (e.g., server-based models with separate AM, PM, and LM). However, non-streaming E2E,LAS models have speech recognition quality comparable to large-scale state-of-the-art conventional models. To take advantage of the quality of the non-streaming E2E LAS model, a two-pass speech recognition system (e.g., Fig. 2A ) was developed. With this design, the two-pass model benefits from the low-latency streaming characteristics of the RNN-T model and improves the accuracy of the RNN-T model through a second pass that incorporates the LAS network. Although the LAS network increases delay compared to the RNN-T model alone, the increase in delay is reasonably small and meets the delay constraints for on-device operation. In terms of accuracy, the two-pass model achieves a 17-22% WER reduction when compared to RNN-T alone and has comparable WER when compared to large-scale conventional models.
残念ながら、ＲＮＮ－Ｔネットワークの第１のパスとＬＡＳネットワークの第２のパスとを備えたこの２パスモデルには、いくつかの欠点がある。例えば、この種の２パスモデルは、教師付き音声・テキストペアでのみトレーニングされるという問題がある。教師付き音声・テキストペアのみでトレーニングを行うと、２パスモデルは、稀少な単語（ｒａｒｅ ｗｏｒｄｓ）またはロングテールエンティティ（ｌｏｎｇ ｔａｉｌ ｅｎｔｉｔｉｅｓ）に対してパフォーマンスが低下する。ロングテールエンティティとは、テキストコーパスにおいて比較的少ない（即ち、周波数が低い）インスタンスを有する多数のエンティティを指す。言い換えれば、一般的ではない稀少な単語は、それゆえ、小規模なトレーニングデータセットでも本質的に一般的ではない。２パスモデルなどのストリーミングＥ２Ｅモデルをトレーニングする場合、トレーニングは、従来の言語モデル（ＬＭ）のトレーニングに使用されるテキストデータのサイズのごく一部である従来の音響モデル（ＡＭ）用のトレーニングセットを使用して行われる。稀少な単語に対する２パスモデルのパフォーマンスを向上させるために提案された技術があるが、提案された技術の多くは、モデルサイズ（例えば、リカレントニューラルネットワーク言語モデル（ＲＮＮ－ＬＭ）を音声認識システムに組み込む）、トレーニング時間（例えば、教師なし音声・テキストペアを使用したトレーニング）、および／または推論コストを大幅に増加させる。 Unfortunately, this two-path model with a first path in the RNN-T network and a second path in the LAS network has several drawbacks. For example, this type of two-pass model suffers from the fact that it is trained only on supervised speech-text pairs. When trained only on supervised speech-text pairs, the two-pass model performs poorly for rare words or long tail entities. Long-tail entities refer to a large number of entities that have relatively few (ie, low frequency) instances in a text corpus. In other words, uncommon and rare words are therefore inherently uncommon even in small training datasets. When training a streaming E2E model, such as a two-pass model, the training set for a traditional acoustic model (AM) is a small fraction of the size of the text data used to train a traditional language model (LM). is done using. Although there are techniques that have been proposed to improve the performance of two-pass models for rare words, many of the proposed techniques have limited model size (e.g., recurrent neural network language models (RNN-LMs) to speech recognition systems. embedding), training time (e.g., training with unsupervised speech-text pairs), and/or inference costs.
稀少な単語に対する２パスモデルの有効性を高めるために、本明細書の実施形態は、２パスモデルアーキテクチャにジョイント音響・テキストデコーダ（ＪＡＴＤ：ｊｏｉｎｔ ａｃｏｕｓｔｉｃ ａｎｄ ｔｅｘｔ ｄｅｃｏｄｅｒ）を組み込むことに向けられている。ＪＡＴＤ機能は、トレーニング例が教師付き音声・テキストペアに対応しているか、非ペアのデータ（例えば、テキストのみのサンプルまたは音声のみのサンプル）から生成された音声・テキスト例に対応しているかを示すドメイン識別子（ＩＤ）を提供する。非ペアのデータの場合、ペアの欠落している半分は、テキスト読み上げ（ＴＴＳ：ｔｅｘｔ－ｔｏ－ｓｐｅｅｃｈ）システムを使用して欠落している音声部分を生成するか、ＡＳＲシステムを使用して欠落しているテキスト部分を生成することで合成され得る。トレーニング中、エンコーダは、エンコーダがペアの例を受信したときに、デコーダに供給される音響コンテキストベクトルを生成する。ここで、音響コンテキストベクトルの生成の成功によって、ペアのデータを示すドメインＩＤが表されるか、または形成される。一方、デコーダが非ペアの例を受ける場合、エンコーダネットワークを迂回させるために、固定ではあるが学習可能なコンテキストベクトルドメインＩＤが使用される。これらの例は両方とも、デコーダでのトレーニングに使用されるため、モデルサイズを大きくすることなく、ペアのデータと非ペアのデータの両方に対してデコーダを同時にトレーニングさせることができる。さらに、入力ソースに基づいてエンコーダへの入力のパラメータのみを変更する、全てのモードにわたってアテンションおよびデコーダのパラメータを共有するアプローチの代わりに、このＪＡＴＤアプローチは、デコーダのパラメータのみを共有し、かつ異なるアテンションコンテキストパラメータを使用する。様々な固有名詞および稀少な単語のテストセットにおいて、ＪＡＴＤモデルは、ペアのデータのみでトレーニングされた２パスアーキテクチャと比較して、ＷＥＲを３～１０％相対的に削減することを達成した。 To increase the effectiveness of the two-pass model for rare words, embodiments herein are directed to incorporating a joint acoustic and text decoder (JATD) into the two-pass model architecture. . The JATD feature determines whether the training examples correspond to supervised audio-text pairs or audio-text examples generated from unpaired data (e.g., text-only samples or audio-only samples). Provide a domain identifier (ID) to indicate. For unpaired data, the missing half of the pair can be generated using a text-to-speech (TTS) system to generate the missing audio portion or an ASR system to generate the missing audio part. can be synthesized by generating text parts that are During training, the encoder generates acoustic context vectors that are provided to the decoder when the encoder receives paired examples. Here, the successful generation of the acoustic context vector represents or forms a domain ID indicating the paired data. On the other hand, if the decoder receives an unpaired example, a fixed but learnable context vector domain ID is used to bypass the encoder network. Both of these examples are used to train the decoder, so the decoder can be trained on both paired and unpaired data simultaneously without increasing the model size. Furthermore, instead of an approach that shares attention and decoder parameters across all modes, which only changes the parameters of the input to the encoder based on the input source, this JATD approach shares only the decoder parameters and uses different Use attention context parameters. On a test set of various proper nouns and rare words, the JATD model achieved a relative reduction in WER of 3-10% compared to a two-pass architecture trained on paired data only.
図１Ａおよび図１Ｂは、発話環境１００の例である。発話環境１００において、ユーザデバイス１１０などのコンピューティングデバイスと対話するユーザの１０の方法は、音声入力を介するものであり得る。ユーザデバイス１１０（一般にデバイス１１０とも呼ばれる）は、発話対応環境１００内の１人または複数人のユーザ１０からの音（例えば、ストリーミング音声データ）をキャプチャするように構成されている。ここで、ストリーミング音声データ１２は、デバイス１１０によってキャプチャされる可聴の問い合わせ（クエリ）、デバイス１１０に対する命令（コマンド）、または可聴の会話（コミュニケーション）としての役割を持つ、ユーザ１０によって話された発話を指すことができる。デバイス１１０の発話対応システムは、問い合わせに応答することによって、かつ／またはコマンドを実行させることによって、問い合わせまたは命令を処理し得る。 FIGS. 1A and 1B are examples of speech environments 100. FIG. In speech environment 100, ten ways for a user to interact with a computing device, such as user device 110, may be through voice input. User device 110 (also commonly referred to as device 110) is configured to capture sound (eg, streaming audio data) from one or more users 10 within speech-enabled environment 100. Here, streaming audio data 12 includes utterances spoken by user 10 that serve as audible queries, commands to device 110, or audible conversations captured by device 110. can point to. The speech-enabled system of device 110 may process the query or command by responding to the query and/or by causing the command to be executed.
ユーザデバイス１１０は、ユーザ１０に関連付けられ、かつ音声データ１２を受信することが可能な任意のコンピューティングデバイスに対応し得る。ユーザデバイス１１０のいくつかの例は、モバイルデバイス（例えば、携帯電話、タブレット、ラップトップなど）、コンピュータ、ウェアラブルデバイス（例えば、スマートウォッチ）、スマート家電、モノのインターネット（ＩｏＴ）デバイス、スマートスピーカなどを含むが、これらに限定されない。ユーザデバイス１１０は、データ処理ハードウェア１１２と、データ処理ハードウェア１１２と通信し、かつ命令を格納するメモリハードウェア１１４とを含み、命令は、データ処理ハードウェア１１２による実行時に、データ処理ハードウェア１１２に１つまたは複数の動作を実行させる。ユーザデバイス１１０は、発話対応システム１００内で話された発話１２をキャプチャして電気信号に変換するための音声キャプチャデバイス（例えば、マイクロフォン）１１６、１１６ａと、可聴音声信号を（例えば、デバイス１１０からの出力音声データとして）伝達するための発話出力デバイス（例えばスピーカの）１１６、１１６ｂとを有する音声サブシステムをさらに含む。図示される例では、ユーザデバイス１１０は単一の音声キャプチャデバイス１１６ａを実装しているが、ユーザデバイス１１０は、本開示の範囲から逸脱することなく、音声キャプチャデバイス１１６ａのアレイを実装してもよく、それにより、アレイ内の１つまたは複数のキャプチャデバイス１１６ａは、ユーザデバイス１１０上に物理的に存在していないが、音声サブシステム１１６と通信状態になり得る。（例えば、ハードウェア１１２、１１４を使用する）ユーザデバイス１１０は、音声認識器２００を使用して、ストリーミング音声データ１２に対して音声認識処理を実行するようにさらに構成される。いくつかの例では、音声キャプチャデバイス１１６ａを含むユーザデバイス１１０の音声サブシステム１１６は、音声データ１２（例えば、話された発話）を受信し、音声データ１２を音声認識器２００と互換性のあるデジタル形式に変換するように構成される。デジタル形式は、メルフレーム（ｍｅｌ ｆｒａｍｅｓ）などの音響フレーム（例えば、パラメータ化された音響フレーム）に対応し得る。例えば、パラメータ化された音響フレームは、ログメルフィルタバンク（ｌｏｇ－ｍｅｌ ｆｉｌｔｅｒｂａｎｋ）のエネルギーに対応する。 User device 110 may correspond to any computing device associated with user 10 and capable of receiving audio data 12. Some examples of user devices 110 are mobile devices (e.g., cell phones, tablets, laptops, etc.), computers, wearable devices (e.g., smart watches), smart appliances, Internet of Things (IoT) devices, smart speakers, etc. including but not limited to. User device 110 includes data processing hardware 112 and memory hardware 114 that communicates with data processing hardware 112 and stores instructions, where the instructions, when executed by data processing hardware 112, are stored in the data processing hardware 112. 112 to perform one or more operations. The user device 110 includes an audio capture device (e.g., a microphone) 116, 116a for capturing and converting utterances 12 spoken within the speech-enabled system 100 into electrical signals and an audible audio signal (e.g., from the device 110). and a speech output device (e.g., a speaker) 116, 116b for communicating output audio data (as output audio data). Although in the illustrated example, user device 110 implements a single audio capture device 116a, user device 110 may implement an array of audio capture devices 116a without departing from the scope of this disclosure. Often, one or more capture devices 116a in the array may be in communication with the audio subsystem 116 even though they are not physically present on the user device 110. User device 110 (eg, using hardware 112, 114) is further configured to perform speech recognition processing on streaming audio data 12 using speech recognizer 200. In some examples, the audio subsystem 116 of the user device 110, including the audio capture device 116a, receives audio data 12 (e.g., spoken utterances) and converts the audio data 12 into a voice compatible with the speech recognizer 200. Configured to convert to digital format. The digital format may correspond to acoustic frames (eg, parameterized acoustic frames) such as mel frames. For example, the parameterized acoustic frame corresponds to the energy of a log-mel filterbank.
図１Ａなどのいくつかの例では、ユーザ１０は、音声認識器２００を使用するユーザデバイス１１０のプログラムまたはアプリケーション１１８と対話する。例えば、図１Ａは、ユーザ１０が自動アシスタントアプリケーションと通信している状態を示している。この例では、ユーザ１０が自動アシスタントに「今夜のコンサートは何時から？（Ｗｈａｔ ｔｉｍｅ ｉｓ ｔｈｅ ｃｏｎｃｅｒｔ ｔｏｎｉｇｈｔ？）」と尋ねている。ユーザ１０からのこの質問は、音声キャプチャデバイス１１６ａによってキャプチャされ、ユーザデバイス１１０の音声サブシステム１１６によって処理される話された発話１２である。この例では、ユーザデバイス１１０の音声認識器２００は、「今夜のコンサートは何時から」という音声入力２０２を（例えば、音響フレームとして）受信し、音声入力２０２を文字起こし２０４（例えば、「今夜のコンサートは何時から？」というテキスト表現）に転写する。ここで、アプリケーション１１８の自動アシスタントは、自然言語処理を使用して、ユーザ１０によって投げかけられた質問に対して回答し得る。自然言語処理とは、一般に、書き言葉（例えば、文字起こし２０４）を解釈し、書き言葉が何らかのアクションを促しているかどうかを判断するプロセスを指す。この例では、自動アシスタントは、自然言語処理を使用して、ユーザ１０からの質問がユーザのスケジュール、より具体的にはユーザのスケジュールでのコンサートに関するものであることを認識する。自動アシスタントは、自然言語処理でこれらの詳細を認識することによって、ユーザの問い合わせに対して、「今夜のコンサートは午後８時３０分に開場です（Ｄｏｏｒｓ ｏｐｅｎ ａｔ ８：３０ ｐｍ ｆｏｒ ｔｈｅ ｃｏｎｃｅｒｔ ｔｏｎｉｇｈｔ）」という回答を返す。いくつかの構成では、自然言語処理は、ユーザデバイス１１０のデータ処理ハードウェア１１２と通信するリモートシステム上で行われ得る。 In some examples, such as FIG. 1A, user 10 interacts with a program or application 118 on user device 110 that uses speech recognizer 200. For example, FIG. 1A shows a user 10 communicating with an automated assistant application. In this example, user 10 asks the automated assistant, "What time is the concert tonight?" This question from user 10 is a spoken utterance 12 that is captured by audio capture device 116a and processed by audio subsystem 116 of user device 110. In this example, the speech recognizer 200 of the user device 110 receives a speech input 202 (e.g., as an acoustic frame) that says "What time does the concert start tonight?" and transcribes the speech input 202 204 (e.g., "What time does the concert start tonight?" ``What time does the concert start?''). Here, the automated assistant of application 118 may use natural language processing to answer questions posed by user 10. Natural language processing generally refers to the process of interpreting written words (eg, transcription 204) and determining whether the written words prompt some action. In this example, the automated assistant uses natural language processing to recognize that the question from user 10 is related to the user's schedule, and more specifically, a concert on the user's schedule. By recognizing these details using natural language processing, the automated assistant can respond to a user's query with, ``Doors open at 8:30 pm for the concert tonight.'' ” is the answer. In some configurations, natural language processing may occur on a remote system that communicates with data processing hardware 112 of user device 110.
図１Ｂは、音声認識器２００を用いた音声認識の別の例である。この例では、ユーザデバイス１１０に関連付けられたユーザ１０は、通信アプリケーション１１８を用いてジェーン・ドゥという名前の友人と通信している。ここで、テッドという名前のユーザ１０は、音声認識器２００に自身の音声入力を文字起こしさせることによって、ジェーンと通信する。音声キャプチャデバイス１１６は、これらの音声入力をキャプチャし、それらを音声認識器２００にデジタル形式（例えば、音響フレーム）で伝達する。音声認識器２００は、これらの音響フレームを、通信アプリケーション１１８を介してジェーンに送信されるテキストに文字起こしする。この種類のアプリケーション１１８はテキストを介して通信するため、音声認識器２００からの文字起こし２０４は、さらなる処理（例えば、自然言語処理）なしでジェーンに送信され得る。 FIG. 1B is another example of speech recognition using speech recognizer 200. In this example, user 10 associated with user device 110 is communicating with a friend named Jane Doe using communication application 118. Here, a user 10 named Ted communicates with Jane by having the speech recognizer 200 transcribe his speech input. Audio capture device 116 captures these audio inputs and communicates them to speech recognizer 200 in digital form (eg, acoustic frames). Speech recognizer 200 transcribes these acoustic frames into text that is sent to Jane via communication application 118. Because this type of application 118 communicates via text, the transcription 204 from the speech recognizer 200 may be sent to Jane without further processing (eg, natural language processing).
図２Ａおよび図２Ｂなどのいくつかの例では、音声認識器２００は２パスアーキテクチャで構成されている。一般的に、音声認識器２００の２パスアーキテクチャは、少なくとも１つの共有エンコーダ２１０、ＲＮＮ－Ｔデコーダ２２０、およびＬＡＳデコーダ２３０を含む。ここで、図２Ａは従来の２パスアーキテクチャを示し、図２ＢはＪＡＴＤモデル２４０を組み込んだ拡張２パスアーキテクチャを示す。これらの図からわかるように、図２Ｂの拡張２パスアーキテクチャは、図２Ａの基本的な２パス構造に基づいている。２パスデコーディングにおいて、第２のパス２０８（例えば、ＬＡＳデコーダ２３０として示される）は、第１のパス２０６（例えば、ＲＮＮ－Ｔデコーダ２２０として示される）からの初期出力を格子再スコアリング（ｌａｔｔｉｃｅ ｒｅｓｃｏｒｉｎｇ）またはｎベスト再ランク付け（ｎ－ｂｅｓｔ ｒｅ－ｒａｎｋｉｎｇ）などの技術を用いて改善し得る。言い換えれば、ＲＮＮ－Ｔデコーダ２２０はストリーミング予測を生成し、ＬＡＳデコーダ２３０は予測を確定する。ここで、具体的には、ＬＡＳデコーダ２３０は、ＲＮＮ－Ｔデコーダ２２０からのストリーミングされた仮説ｙＲ２２２を再スコアリングする。一般に、ＬＡＳデコーダ２３０は、ＲＮＮ－Ｔデコーダ２２０からストリーミングされた仮説ｙＲ２２２を再スコアリングする再スコアリングモードで機能すると説明されているが、ＬＡＳデコーダ２３０は、設計またはその他の要因（例えば、発話の長さ）に応じてビームサーチモード（ｂｅａｍ ｓｅａｒｃｈ ｍｏｄｅ）などの異なるモードで動作することも可能である。 In some examples, such as FIGS. 2A and 2B, speech recognizer 200 is configured in a two-pass architecture. Generally, the two-pass architecture of speech recognizer 200 includes at least one shared encoder 210, RNN-T decoder 220, and LAS decoder 230. Here, FIG. 2A shows a conventional two-pass architecture, and FIG. 2B shows an enhanced two-pass architecture incorporating the JATD model 240. As can be seen from these figures, the enhanced two-pass architecture of FIG. 2B is based on the basic two-pass structure of FIG. 2A. In two-pass decoding, a second pass 208 (e.g., shown as LAS decoder 230) subjects the initial output from first pass 206 (e.g., shown as RNN-T decoder 220) to lattice rescoring ( techniques such as lattice scoring or n-best re-ranking. In other words, RNN-T decoder 220 generates streaming predictions and LAS decoder 230 finalizes the predictions. Here, specifically, LAS decoder 230 rescores the streamed hypothesis y R 222 from RNN-T decoder 220. Although the LAS decoder 230 is generally described as operating in a rescoring mode in which it rescores the hypothesis y R 222 streamed from the RNN-T decoder 220, the LAS decoder 230 may depend on design or other factors (e.g. It is also possible to operate in different modes, such as beam search mode, depending on the length of the utterance.
少なくとも１つのエンコーダ２１０は、ストリーミング音声データ１２に対応する音響フレームを音声入力２０２として受信するように構成される。音響フレームは、音声サブシステム１１６によってパラメータ化された音響フレーム（例えば、メルフレームおよび／またはスペクトルフレーム）に事前に処理され得る。いくつかの実施形態では、パラメータ化された音響フレームは、ログメル特徴（ｌｏｇ－ｍｅｌ ｆｅａｔｕｒｅｓ）を有するログメルフィルタバンクエネルギーに対応する。例えば、音声サブシステム１１６によって出力され、エンコーダ２１０に入力されるパラメータ化された入力音響フレームは、ｘ＝（ｘ１，．．．，ｘＴ）として表すことができ、ここで、 At least one encoder 210 is configured to receive audio frames corresponding to streaming audio data 12 as audio input 202. The acoustic frames may be pre-processed into parameterized acoustic frames (eg, mel frames and/or spectral frames) by audio subsystem 116. In some embodiments, the parameterized acoustic frames correspond to log-mel filterbank energies having log-mel features. For example, a parameterized input acoustic frame output by audio subsystem 116 and input to encoder 210 can be represented as x = (x 1 , ..., x T ), where
は、ログメルフィルタバンクエネルギーであり、Ｔはｘのフレーム数を示し、ｄはログメル特徴の数を表す。いくつかの例では、各パラメータ化された音響フレームは、短いシフトウィンドウ（例えば、３２ミリ秒、１０ミリ秒ごとにシフト）内で計算された１２８次元のログメル特徴を含む。各特徴は、前のフレーム（例えば、３つ前のフレーム）と重ねられて、より高次元のベクトル（例えば、３つ前のフレームを使用した５１２次元のベクトル）が形成されてもよい。次に、ベクトルを形成する特徴は、（例えば、３０ミリ秒のフレームレートに）ダウンサンプリングされ得る。エンコーダ２１０は、音声入力２０２に基づいて、エンコーディングｅを生成するように構成される。例えば、エンコーダ２１０は、エンコードされた音響フレーム（例えば、エンコードされたメルフレームまたは音響埋め込み（ａｃｏｕｓｔｉｃ ｅｍｂｅｄｄｉｎｇｓ））を生成する。
is the logmel filterbank energy, T denotes the number of frames in x, and d denotes the number of logmel features. In some examples, each parameterized acoustic frame includes 128-dimensional logmel features computed within a short shift window (eg, 32 ms, shifted every 10 ms). Each feature may be superimposed with the previous frame (eg, the 3rd frame before) to form a higher dimensional vector (eg, a 512-dimensional vector using the 3rd frame before). The features forming the vector may then be downsampled (eg, to a 30 ms frame rate). Encoder 210 is configured to generate an encoding e based on audio input 202. For example, encoder 210 generates encoded acoustic frames (eg, encoded mel frames or acoustic embeddings).
エンコーダ２１０の構造は、異なる方法で実施することができるが、いくつかの実施形態では、エンコーダ２１０は、長・短期記憶（ＬＳＴＭ：ｌｏｎｇ－ｓｈｏｒｔ ｔｅｒｍ ｍｅｍｏｒｙ）ニューラルネットワークである。例えば、エンコーダ２１０は、８個のＬＳＴＭ層を含む。ここで、各層は、２０４８個の隠れユニットと、それに続く６４０次元の射影層（ｐｒｏｊｅｃｔｉｏｎ ｌａｙｅｒ）とを含む。いくつかの例では、エンコーダ２１０の第２のＬＳＴＭ層の後に、短縮係数（ｒｅｄｕｃｔｉｏｎ ｆａｃｔｏｒ）Ｎ＝２を有する時間短縮層（ｔｉｍｅ－ｒｅｄｕｃｔｉｏｎ ｌａｙｅｒ）が挿入される。 Although the structure of encoder 210 can be implemented in different ways, in some embodiments encoder 210 is a long-short term memory (LSTM) neural network. For example, encoder 210 includes eight LSTM layers. Here, each layer includes 2048 hidden units followed by a 640-dimensional projection layer. In some examples, a time-reduction layer with a reduction factor N=2 is inserted after the second LSTM layer of encoder 210.
いくつかの構成では、エンコーダ２１０は共有エンコーダネットワークである。言い換えれば、各パスネットワーク２０６、２０８がそれ自体の別個のエンコーダを有する代わりに、各パス２０６、２０８は単一のエンコーダ２１０を共有する。エンコーダを共有することによって、２パスアーキテクチャを使用するＡＳＲ音声認識器２００は、そのモデルサイズおよび／またはその計算コストを削減することができる。ここで、モデルサイズの縮小は、音声認識器２００が完全にオンデバイス（ｏｎ－ｄｅｖｉｃｅ）で良好に機能することを可能にするのに役立ち得る。 In some configurations, encoder 210 is a shared encoder network. In other words, each path 206, 208 shares a single encoder 210 instead of each path network 206, 208 having its own separate encoder. By sharing encoders, ASR speech recognizer 200 using a two-pass architecture can reduce its model size and/or its computational cost. Here, reducing the model size may be helpful in allowing the speech recognizer 200 to function well completely on-device.
いくつかの例では、図２Ａの音声認識器２００はまた、ＬＡＳデコーダ２３０の第２のパス２０８に適したものとなるようにエンコーダ２１０の出力２１２を適応させるための音響エンコーダ２５０などの追加のエンコーダを含む。音響エンコーダ２５０は、出力２１２をエンコードされた出力２５２にさらにエンコードするように構成される。いくつかの実施形態では、音響エンコーダ２５０は、エンコーダ２１０からの出力２１２をさらにエンコードするＬＳＴＭエンコーダ（例えば、２層ＬＳＴＭエンコーダ）である。追加のエンコーダを含むことによって、エンコーダ２１０は、パス２０６、２０８の間の共有エンコーダとして依然として保持され得る。 In some examples, the speech recognizer 200 of FIG. 2A also includes additional components, such as an acoustic encoder 250 to adapt the output 212 of the encoder 210 to be suitable for the second pass 208 of the LAS decoder 230. Contains encoder. Acoustic encoder 250 is configured to further encode output 212 into encoded output 252. In some embodiments, acoustic encoder 250 is an LSTM encoder (eg, a two-layer LSTM encoder) that further encodes output 212 from encoder 210. By including additional encoders, encoder 210 may still be kept as a shared encoder between paths 206, 208.
第１のパス２０６を通じて、エンコーダ２１０は、音声入力２０２の各音響フレームを受信して、出力２１２（例えば、音響フレームのエンコーディングｅとして示される）を生成する。ＲＮＮ－Ｔデコーダ２２０は、各フレームの出力２１２を受信して、仮説ｙＲとして示される出力２２２を各タイムステップにおいてストリーミング方式で生成する。いくつかの実施形態では、ＲＮＮ－Ｔデコーダ２２０は、予測ネットワークおよび結合ネットワークを含む。ここで、予測ネットワークは、２０４８個の隠れユニットおよび６４０次元の射影（層ごと）の２つのＬＳＴＭ層、並びに１２８ユニットの埋め込み層を有し得る。エンコーダ２１０および予測ネットワークの出力２１２は、ソフトマックス予測層を含む結合ネットワークに供給され得る。いくつかの例では、ＲＮＮ－Ｔデコーダ２２０の結合ネットワークは、６４０個の隠れユニットと、それに続く４０９６個の大文字と小文字が混在する単語ピースを予測するソフトマックス層とを含む。 Through a first path 206, an encoder 210 receives each acoustic frame of audio input 202 and produces an output 212 (eg, shown as an encoding of the acoustic frame e). RNN-T decoder 220 receives the output 212 of each frame and produces an output 222, denoted as hypothesis y R , in a streaming manner at each time step. In some embodiments, RNN-T decoder 220 includes a prediction network and a combination network. Here, the prediction network may have two LSTM layers of 2048 hidden units and 640 dimensional projections (per layer), and an embedding layer of 128 units. The encoder 210 and prediction network output 212 may be fed to a combination network that includes a softmax prediction layer. In some examples, the combined network of RNN-T decoder 220 includes 640 hidden units followed by a softmax layer that predicts 4096 mixed case word pieces.
図２Ａの２パスモデルにおいて、第２のパス２０８を通じて、ＬＡＳデコーダ２３０は、各フレームに関してエンコーダ２１０からの出力２１２（またはエンコードされた出力２５２）を受信し、仮説ｙＬとして指定された出力２３２を生成する。ＬＡＳデコーダ２３０がビームサーチモードで動作する場合、ＬＡＳデコーダ２３０は、出力２１２（または出力２５２）のみから出力２３２を生成し、ＲＮＮ－Ｔデコーダ２２０の出力２２２を無視する。ＬＡＳデコーダ２３０が再スコアリングモードで動作する場合、ＬＡＳデコーダ２３０は、ＲＮＮ－Ｔデコーダ２２０からトップＫ仮説を取得し、次いで、ＬＡＳデコーダ２３０は、出力２１２（または出力２５２）にアテンション（ａｔｔｅｎｔｉｏｎ）しつつ、教師強制モードで各シーケンスに対して動作して、スコアを計算する。例えば、スコアは、シーケンスの対数確率とアテンションカバレッジペナルティ（ａｔｔｅｎｔｉｏｎ ｃｏｖｅｒａｇｅ ｐｅｎａｌｔｙ）とを組み合わせたものである。ＬＡＳデコーダ２３０は、最も高いスコアを有するシーケンスを出力２３２として選択する。ここで、再スコアリングモードでは、ＬＡＳデコーダ２３０は、出力２１２（または出力２５２）にアテンションを向けるために、（例えば、４つのヘッドを有する）マルチヘッデドアテンション（ｍｕｌｔｉ－ｈｅａｄｅｄ ａｔｔｅｎｔｉｏｎ）を含み得る。さらに、ＬＡＳデコーダ２３０は、予測のためのソフトマックス層を備えた２層ＬＡＳデコーダ２３０であり得る。例えば、ＬＡＳデコーダ２３０の各層は、２０４８個の隠れユニットと、それに続く６４０次元の射影とを有する。ソフトマックス層は、ＲＮＮ－Ｔデコーダ２２０のソフトマックス層から同じ大文字と小文字が混在する単語ピースを予測するために、４，０９６次元を含み得る。 In the two-pass model of FIG. 2A, through a second pass 208, LAS decoder 230 receives output 212 (or encoded output 252) from encoder 210 for each frame and outputs 232 designated as hypothesis y L generate. When LAS decoder 230 operates in beam search mode, LAS decoder 230 generates output 232 only from output 212 (or output 252) and ignores output 222 of RNN-T decoder 220. When LAS decoder 230 operates in rescoring mode, LAS decoder 230 obtains the top-K hypotheses from RNN-T decoder 220, and then LAS decoder 230 sends attention to output 212 (or output 252). while operating on each sequence in teacher-forced mode to calculate the score. For example, the score is a combination of the log probability of the sequence and the attention coverage penalty. LAS decoder 230 selects the sequence with the highest score as output 232. Here, in rescoring mode, LAS decoder 230 includes multi-headed attention (e.g., with four heads) to direct attention to output 212 (or output 252). obtain. Additionally, LAS decoder 230 may be a two-layer LAS decoder 230 with a softmax layer for prediction. For example, each layer of LAS decoder 230 has 2048 hidden units followed by a 640-dimensional projection. The softmax layer may include 4,096 dimensions to predict the same mixed case word pieces from the softmax layer of RNN-T decoder 220.
いくつかの実施形態では、図２Ａの２パスモデルのトレーニングは２段階で行われる。第１段階の間に、エンコーダ２１０およびＲＮＮ－Ｔデコーダ２２０は、 In some embodiments, training the two-pass model of FIG. 2A is performed in two stages. During the first stage, encoder 210 and RNN-T decoder 220:
を最大化するようにトレーニングされる。第２段階では、エンコーダ２１０が固定され、ＬＡＳデコーダ２３０が
trained to maximize. In the second stage, encoder 210 is fixed and LAS decoder 230 is
を最大化するようにトレーニングされる。２パスモデルが追加のエンコーダ２５０を含む場合、追加のエンコーダ２５０は、エンコーダ２１０が固定されている間に、第２段階で
trained to maximize. If the two-pass model includes an additional encoder 250, the additional encoder 250 is inserted in the second stage while the encoder 210 is fixed.
を最大化するようにトレーニングされる。
trained to maximize.
図２Ｂを参照すると、第１のパス２０６は同じままであるが、第２のパス２０８は、ＬＡＳデコーダ２３０でデコーディングする前に、アテンション機構２４２を含むＪＡＴＤモデル２４０を使用する。ここで、アテンション機構２４２は、エンコードされた出力２１２（または出力２５２）を受信して、各出力ステップに対するエンコードされた特徴を要約するコンテキストベクトルｃ、ｃＡ、ｃＬを決定する。アテンション機構２４２は、入力トレーニングデータの種類に応じて、ＬＡＳデコーダ２３０に渡されるコンテキストベクトルｃを変更するように構成される。言い換えれば、アテンション機構２４２は、入力トレーニングデータ（即ち、特定のトレーニング例）が教師付き音声・テキストペアである場合に音響コンテキストベクトル（ａｃｏｕｓｔｉｃ ｃｏｎｔｅｘｔ ｖｅｃｔｏｒ） ２４４、ｃＡを生成し、入力トレーニングデータ（例えば、トレーニング例２０３）が非ペアのテキストシーケンスである場合に固定言語コンテキストベクトル（ｆｉｘｅｄ ｌｉｎｇｕｉｓｔｉｃ ｃｏｎｔｅｘｔ ｖｅｃｔｏｒ）２４６、ｃＬを生成する。ＪＡＴＤモデル２４０がＬＡＳデコーダ２３０と統合されている場合、ＪＡＴＤモデル２４０は、ペアのデータおよび／または非ペアのデータを利用するようにＬＡＳデコーダ２３０の推論およびトレーニングの両方を変更する。 Referring to FIG. 2B, the first pass 206 remains the same, but the second pass 208 uses the JATD model 240, which includes an attention mechanism 242, before decoding with the LAS decoder 230. Here, attention mechanism 242 receives encoded output 212 (or output 252) and determines context vectors c, c A , c L that summarize the encoded features for each output step. The attention mechanism 242 is configured to modify the context vector c passed to the LAS decoder 230 depending on the type of input training data. In other words, the attention mechanism 242 generates an acoustic context vector 244, c A when the input training data (i.e., a particular training example) is a supervised speech-text pair, and For example, if the training example 203) is an unpaired text sequence, a fixed linguistic context vector 246, cL is generated. When JATD model 240 is integrated with LAS decoder 230, JATD model 240 modifies both the inference and training of LAS decoder 230 to utilize paired and/or unpaired data.
推論の間に、ＬＡＳデコーダ２３０は、以下の式に基づいて対数確率（ｌｏｇ ｐｒｏｂａｂｉｌｉｔｉｅｓ）を計算する。例えば、音響入力を用いて、音響コンテキストベクトルｃａは、各デコーダステップｕにおけるＬＡＳデコーダ２３０に関する対数確率を決定する。ここで、 During inference, LAS decoder 230 calculates log probabilities based on the following equation: For example, using the acoustic input, the acoustic context vector c a determines the log probability for LAS decoder 230 at each decoder step u. here,
は、推論の間に以前にデコードされた単一の仮説のラベルを示す。同様に、テキストベースの入力を用いて、言語コンテキストベクトルｃＬは、各デコーダステップにおけるＬＡＳデコーダ２３０に関する対数確率を決定する。これらの両方の状況では、対数確率は、音声機能が完全に無視されるように、以前のラベルのみに基づいてラベルを予測する。言い換えれば、音響コンテキストベクトルまたは言語コンテキストベクトルｃのいずれかから対数確率を生成することにより、確率は、一般に音響スコアおよび／または言語スコアを示す。従って、各デコーダの時間ステップは、混合重みλ（例えば、混合重みは、音響サンプルと言語サンプルの比率に対応する）を使用した音響ベースの対数確率および言語ベースの対数確率の補間として表すことができる。いくつかの例では、この推論は、ＬＡＳデコーダ２３０が再スコアモードまたはビームサーチモードで動作するときに適用される。様々なデータソースとテストセットを使用した反復に基づいて、０．０５前後の混合重みが推論に最適であり得る。
indicates the label of a single hypothesis previously decoded during inference. Similarly, using text-based input, the linguistic context vector c L determines the log probability for LAS decoder 230 at each decoder step. In both of these situations, log-probability predicts the label based only on previous labels, such that speech features are completely ignored. In other words, by generating log probabilities from either the acoustic context vector or the linguistic context vector c, the probabilities are generally indicative of acoustic and/or linguistic scores. Therefore, each decoder time step can be represented as an interpolation of the acoustic-based log-probability and the language-based log-probability using a mixture weight λ (e.g., the mixture weight corresponds to the ratio of acoustic samples to language samples). can. In some examples, this reasoning applies when LAS decoder 230 operates in rescore mode or beam search mode. Based on iterations using various data sources and test sets, a blending weight around 0.05 may be optimal for inference.
トレーニング中、ＲＮＮ－Ｔデコーダ２２０は、従来の２パスアーキテクチャと同じ方法でトレーニングする。言い換えれば、ＲＮＮ－Ｔデコーダ２２０は、教師付き音声・テキストペアデータでトレーニングする。しかし、ＬＡＳデコーダ２３０をトレーニングする際、複数のトレーニング戦略が使用され得る。例えば、第１のトレーニング戦略は個別のトレーニング戦略であり、第２のトレーニング戦略はジョイントトレーニング戦略である。個別のトレーニング戦略において、音声・テキストペアが使用される場合、ＬＡＳデコーダ２３０は、音響コンテキストベクトルｃａ、２４４を使用して式（２ａ）の決定に基づいて更新する。ここで、ＬＡＳデコーダ２３０の更新に加えて、式（２ａ）の解は、音響コンテキストベクトルパラメータを更新する。一方、非ペアのデータが使用される場合、トレーニング損失は式（２ｂ）から計算されたクロスエントロピー損失に減少する。ここで、ｃＬは、トレーニング可能なコンテキストベクトルである。この状況では、ＬＡＳデコーダ２３０およびコンテキストベクトルのみが更新される。
During training, RNN-T decoder 220 trains in the same manner as a conventional two-pass architecture. In other words, RNN-T decoder 220 is trained with supervised speech-text pair data. However, multiple training strategies may be used in training LAS decoder 230. For example, the first training strategy is an individual training strategy and the second training strategy is a joint training strategy. In the separate training strategy, if speech-text pairs are used, the LAS decoder 230 uses the acoustic context vector ca , 244 to update based on the determination of equation (2a). Now, in addition to updating the LAS decoder 230, the solution to equation (2a) updates the acoustic context vector parameters. On the other hand, if unpaired data is used, the training loss is reduced to the cross-entropy loss calculated from equation (2b). Here, c L is a trainable context vector. In this situation, only the LAS decoder 230 and context vectors are updated.
ジョイントトレーニング戦略を使用して、いくつかの実施形態では、音響および言語コンテキストベクトル２４４、２４６から生成された対数確率の補間は、推論と同様の方法でトレーニング損失を定義する。ここでは、教師付き音声データはｘａとして表される。教師付き音声・テキストペアを含む例では、ＬＡＳデコーダ２３０および音響アテンションパラメータは、
Using a joint training strategy, in some embodiments, interpolation of the log probabilities generated from the acoustic and linguistic context vectors 244, 246 defines the training loss in a manner similar to inference. Here, the supervised audio data is represented as xa . In examples involving supervised speech-text pairs, the LAS decoder 230 and acoustic attention parameters are:
の補間に基づいて更新される。非ペアのデータの場合、適切な対数確率を決定するための音響コンテキストベクトルｃａが不足している。これが発生した場合、２つの潜在的な選択肢がある。第一に、従来のモデルは、実際の音声を取得した後、仮説テキスト（例えば、テキストの文字起こし）を生成し得る。ここで、文字起こしされた音声の使用は、モデルの蒸留（ｍｏｄｅｌ ｄｉｓｔｉｌｌａｔｉｏｎ）に似ている。第２のアプローチでは、第１のアプローチを逆にして、ＴＴＳのようなシステムが実際のテキストから音響信号を合成するようにする。これらのアプローチを使用すると、非ペアのデータは、音響コンテキストベクトルが不足することはなくなる。従って、解決された音響コンテキストベクトルｃａを用いて、音声認識器２００は、非ペアのデータに対する対数確率を補間することができる。この補間に基づいて、音声認識器２００は、ＬＡＳデコーダ２３０および固定コンテキストベクトルパラメータを更新する。いくつかの実施形態では、音声認識器２００は、アテンション機構２４２の音響アテンションパラメータが偏らないように混合重みλを調整する。
is updated based on interpolation. For unpaired data, there is a lack of acoustic context vector ca to determine the appropriate log probability. When this occurs, there are two potential options. First, traditional models may generate hypothetical text (eg, a transcription of the text) after acquiring the actual speech. Here, the use of transcribed audio is similar to model distillation. A second approach reverses the first approach and allows systems like TTS to synthesize audio signals from the actual text. Using these approaches, unpaired data no longer runs out of acoustic context vectors. Therefore, using the resolved acoustic context vector ca , the speech recognizer 200 can interpolate log probabilities for unpaired data. Based on this interpolation, speech recognizer 200 updates LAS decoder 230 and fixed context vector parameters. In some embodiments, speech recognizer 200 adjusts the mixing weights λ so that the acoustic attention parameters of attention mechanism 242 are unbiased.
図３は、自動音声認識（例えば、ＡＳＲ）を実行する方法３００のための例示的な動作の構成のフローチャートである。動作３０２において、方法３００は、２パスストリーミングニューラルネットワークモデルのＬＡＳデコーダ２３０用のトレーニング例を受信する。動作３０４において、方法３００は、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのテキストシーケンスに対応しているかを決定する。トレーニング例が非ペアのテキストシーケンスに対応している場合、動作３０６において、方法３００は、トレーニング例のコンテキストベクトルｃに関連付けられた対数確率に基づいてクロスエントロピー損失を決定する。動作３０８において、方法３００は、決定されたクロスエントロピー損失に基づいて、ＬＡＳデコーダ２３０およびコンテキストベクトルｃを更新する。
FIG. 3 is a flowchart of an example operational configuration for a method 300 of performing automatic speech recognition (eg, ASR). At act 302, method 300 receives a training example for LAS decoder 230 of a two-pass streaming neural network model. At act 304, method 300 determines whether the training example corresponds to a supervised speech-text pair or an unpaired text sequence. If the training example corresponds to an unpaired text sequence, then in operation 306 the method 300 determines a cross-entropy loss based on the log probability associated with the context vector c of the training example. At act 308, method 300 updates LAS decoder 230 and context vector c based on the determined cross-entropy loss.
図４は、自動音声認識（例えば、ＡＳＲ）を実行する方法４００のための例示的な動作の構成の別のフローチャートである。動作４０２において、方法４００は、２パスストリーミングニューラルネットワークモデルのＬＡＳデコーダ２３０用のトレーニング例を受信する。ここで、トレーニング例は、ＬＡＳデコーダ２３０をトレーニングするように構成されている。動作４０４において、方法４００は、トレーニング例が教師付き音声・テキストペアに対応しているか、または非ペアのトレーニングデータに対応しているかを決定する。トレーニング例が非ペアのトレーニングデータに対応している場合、動作４０６において、方法４００は、非ペアのトレーニングデータの欠落部分を生成して、生成音声・テキストペアを形成する。動作４０８において、方法４００は、生成音声・テキストペアに基づいて、ＬＡＳデコーダ２３０および非ペアのデータに関連付けられたコンテキストベクトルｃを更新する。 FIG. 4 is another flowchart of an example operational configuration for a method 400 of performing automatic speech recognition (eg, ASR). At act 402, method 400 receives training examples for LAS decoder 230 of a two-pass streaming neural network model. Here, the training example is configured to train LAS decoder 230. At act 404, method 400 determines whether the training example corresponds to a supervised speech-text pair or unpaired training data. If the training example corresponds to unpaired training data, then in operation 406 the method 400 generates the missing portion of the unpaired training data to form a generated speech-text pair. At act 408, method 400 updates context vector c associated with LAS decoder 230 and unpaired data based on the generated speech-text pair.
図５は、本明細書で説明されるシステム（例えば、音声認識器２００）および方法（例えば、方法３００、４００）を実施するために使用され得る例示的なコンピューティングデバイス５００の概略図である。コンピューティングデバイス５００は、ラップトップ、デスクトップ、ワークステーション、パーソナルデジタルアシスタント、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピュータなどの様々な形態のデジタルコンピュータを代表することが意図されている。本明細書に示された構成要素、それらの接続および関係、およびそれらの機能は、例示的なものに過ぎず、本明細書に記載および／または特許請求の範囲に記載される本発明の実施形態を限定するものではない。 FIG. 5 is a schematic diagram of an example computing device 500 that may be used to implement the systems (e.g., speech recognizer 200) and methods (e.g., methods 300, 400) described herein. . Computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other suitable computers. The components, their connections and relationships, and their functions illustrated herein are exemplary only and may be used to implement the invention as described and/or claimed herein. It does not limit the form.
コンピューティングデバイス５００は、プロセッサ５１０（例えば、データ処理ハードウェア）、メモリ５２０（例えば、メモリハードウェア）、ストレージデバイス５３０、メモリ５２０および高速拡張ポート５４０に接続する高速インタフェース／コントローラ５４０、および低速バス５７０およびストレージデバイス５３０に接続する低速インタフェース／コントローラ５６０を含む。構成要素５１０、５２０、５３０、５４０、５５０、および５６０の各々は、様々なバスを使用して相互接続され、かつ共通のマザーボード上に、または適切な他の方法で搭載され得る。プロセッサ５１０は、メモリ５２０またはストレージデバイス５３０に格納された命令を含むコンピューティングデバイス５００内での実行のための命令を処理して、高速インタフェース５４０に接続されたディスプレイ５８０などの外部入力／出力デバイス上にグラフィカルユーザインタフェース（ＧＵＩ）用のグラフィカル情報を表示する。他の実施形態では、複数のメモリおよび複数のタイプのメモリと共に、複数のプロセッサおよび／または複数のバスが適宜使用されてもよい。また、複数のコンピューティングデバイス５００が接続され、各デバイスが（例えば、サーババンク、ブレードサーバのグループ、またはマルチプロセッサシステムとして）必要な処理の一部を提供してもよい。 The computing device 500 includes a processor 510 (e.g., data processing hardware), a memory 520 (e.g., memory hardware), a storage device 530, a high-speed interface/controller 540 that connects to the memory 520 and a high-speed expansion port 540, and a low-speed bus. 570 and a low speed interface/controller 560 that connects to storage device 530 . Each of the components 510, 520, 530, 540, 550, and 560 may be interconnected using various buses and mounted on a common motherboard or in any other suitable manner. Processor 510 processes instructions for execution within computing device 500, including instructions stored in memory 520 or storage device 530, to external input/output devices such as display 580 connected to high speed interface 540. Displays graphical information for a graphical user interface (GUI) on top. In other embodiments, multiple processors and/or multiple buses may be used, along with multiple memories and multiple types of memory, as appropriate. Also, multiple computing devices 500 may be connected, each device providing a portion of the required processing (eg, as a server bank, group of blade servers, or multiprocessor system).
メモリ５２０は、コンピューティングデバイス５００内に非一時的に情報を記憶する。メモリ５２０は、コンピュータ可読媒体、揮発性メモリユニット、または不揮発性メモリユニットであってもよい。非一時的メモリ５２０は、コンピューティングデバイス５００による使用のための一時的または永久的な基準でプログラム（例えば、命令のシーケンス）またはデータ（例えば、プログラム状態情報）を格納するために使用される物理的デバイスであってもよい。不揮発性メモリの例には、これらに限定されないが、フラッシュメモリおよび読み出し専用メモリ（ＲＯＭ）／プログラム可能読み出し専用メモリ（ＰＲＯＭ）／消去可能プログラム可能読み出し専用メモリ（ＥＰＲＯＭ）／電子消去可能プログラム可能読み出し専用メモリ（ＥＥＰＲＯＭ）（例えば、通常、ブートプログラムなどのファームウェアに使用される）が含まれる。揮発性メモリの例には、これらに限定されないが、ランダムアクセスメモリ（ＲＡＭ）、ダイナミックランダムアクセスメモリ（ＤＲＡＭ）、スタティックランダムアクセスメモリ（ＳＲＡＭ）、相変化メモリ（ＰＣＭ）、およびディスクまたはテープが含まれる。 Memory 520 stores information within computing device 500 on a non-temporary basis. Memory 520 may be a computer readable medium, a volatile memory unit, or a non-volatile memory unit. Non-transitory memory 520 is a physical memory used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by computing device 500. It may be a specific device. Examples of non-volatile memory include, but are not limited to, flash memory and read only memory (ROM)/programmable read only memory (PROM)/erasable programmable read only memory (EPROM)/electronically erasable programmable read only. A dedicated memory (EEPROM) is included (eg, typically used for firmware such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM), and disk or tape. It will be done.
ストレージデバイス５３０は、コンピューティングデバイス５００の大容量ストレージデバイスを提供することができる。いくつかの実施形態では、ストレージデバイス５３０は、コンピュータ可読媒体である。種々の異なる実施形態では、ストレージデバイス５３０は、フロッピーディスク（登録商標）デバイス、ハードディスクデバイス、光ディスクデバイス、またはテープデバイス、フラッシュメモリまたは他の同様のソリッドステートメモリデバイス、またはストレージエリアネットワークまたはその他の構成におけるデバイスを含むデバイスのアレイであり得る。追加の実施形態では、コンピュータプログラム製品は、情報媒体に有形的に具体化される。コンピュータプログラム製品は、実行時に、上記したような１つまたは複数の方法を実行する命令を含む。情報媒体は、メモリ５２０、ストレージデバイス５３０、またはプロセッサ５１０上のメモリなどの、コンピュータ可読媒体または機械可読媒体である。 Storage device 530 may provide a mass storage device for computing device 500. In some embodiments, storage device 530 is a computer readable medium. In various different embodiments, storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or a storage area network or other configuration. may be an array of devices including devices in the . In additional embodiments, the computer program product is tangibly embodied in an information carrier. The computer program product includes instructions that, when executed, perform one or more methods as described above. The information medium is a computer-readable or machine-readable medium, such as memory 520, storage device 530, or memory on processor 510.
高速コントローラ５４０は、コンピューティングデバイス５００の帯域幅を大量に使用する処理を管理し、低速コントローラ５６０は、より低い帯域幅を大量に使用する処理を管理する。このような役割の配分は、例示的なものに過ぎない。いくつかの実施形態では、高速コントローラ５４０は、メモリ５２０、ディスプレイ５８０（例えば、グラフィックプロセッサまたはアクセラレータを介する）、および各種拡張カード（図示せず）を受け入れる高速拡張ポート５５０に接続される。いくつかの実施形態では、低速コントローラ５６０は、ストレージデバイス５３０および低速拡張ポート５９０に接続される。様々な通信ポート（例えば、ＵＳＢ、ブルートゥース（登録商標）、イーサネット（登録商標）、無線イーサネット（登録商標））を含む低速拡張ポート５９０は、キーボード、ポインティングデバイス、スキャナ、または例えばネットワークアダプターを介するスイッチまたはルータなどのネットワークデバイスなどの１つまたは複数の入力／出力デバイスに接続され得る。 A high-speed controller 540 manages bandwidth-intensive processes of the computing device 500, and a low-speed controller 560 manages lower bandwidth-intensive processes. This distribution of roles is exemplary only. In some embodiments, high speed controller 540 is connected to memory 520, display 580 (eg, via a graphics processor or accelerator), and high speed expansion port 550 that accepts various expansion cards (not shown). In some embodiments, low speed controller 560 is connected to storage device 530 and low speed expansion port 590. Low speed expansion ports 590, including various communication ports (e.g., USB, Bluetooth, Ethernet, Wireless Ethernet), can be connected to a keyboard, pointing device, scanner, or switch via a network adapter, for example. or may be connected to one or more input/output devices, such as a network device such as a router.
コンピューティングデバイス５００は、図面に示されるように、いくつかの異なる形態で実施することができる。例えば、標準サーバ５００ａとして、またはそのようなサーバ５００ａのグループ内で複数回、ラップトップコンピュータ５００ｂとして、またはラックサーバシステム５００ｃの一部として実施することができる。 Computing device 500 may be implemented in several different forms as shown in the drawings. For example, it may be implemented as a standard server 500a or multiple times within a group of such servers 500a, as a laptop computer 500b, or as part of a rack server system 500c.
本明細書に記載のシステムおよび技術の様々な実施形態は、デジタル電子回路および／または光回路、集積回路、特別に設計されたＡＳＩＣ（特定用途向け集積回路）、コンピュータハードウェア、ファームウェア、ソフトウェア、および／またはそれらの組み合わせにおいて実現することができる。これらの様々な実施形態は、ストレージシステム、少なくとも１つの入力デバイス、および少なくとも１つの出力デバイスからデータおよび命令を受信し、それらにデータおよび命令を送信するように接続された、特別または一般的な目的であってもよい、少なくとも１つのプログラム可能なプロセッサを含むプログラム可能なシステム上で実行可能および／または解釈可能な１つまたは複数のコンピュータプログラムにおける実施形態を含むことができる。 Various embodiments of the systems and techniques described herein include digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (Application Specific Integrated Circuits), computer hardware, firmware, software, and/or a combination thereof. These various embodiments include specialized or general storage systems connected to receive data and instructions from and transmit data and instructions to the storage system, at least one input device, and at least one output device. The object may include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor.
これらのコンピュータプログラム（プログラム、ソフトウェア、ソフトウェアアプリケーション、またはコードとしても知られている）は、プログラマブルプロセッサ用の機械命令を含み、高水準の手続き型言語および／またはオブジェクト指向のプログラミング言語、および／またはアセンブリ言語／機械語で実施することができる。本明細書で使用する場合、「機械可読媒体」および「コンピュータ可読媒体」という用語は、任意のコンピュータプログラム製品、非一時的なコンピュータ可読媒体、機械命令を機械可読信号として受け取る機械可読媒体を含む、プログラマブルプロセッサに機械命令および／またはデータを提供するために使用される装置および／またはデバイス（例えば、磁気ディスク、光ディスク、メモリ、プログラマブルロジックデバイス（ＰＬＤ））を指す。「機械可読信号」という用語は、機械命令および／またはデータをプログラマブルプロセッサに提供するために使用される任意の信号を指す。 These computer programs (also known as programs, software, software applications, or code) include machine instructions for a programmable processor, and include high-level procedural and/or object-oriented programming languages, and/or Can be implemented in assembly language/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" include any computer program product, non-transitory computer-readable medium, machine-readable medium that receives machine instructions as a machine-readable signal. , refers to an apparatus and/or device (e.g., magnetic disk, optical disk, memory, programmable logic device (PLD)) used to provide machine instructions and/or data to a programmable processor. The term "machine readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
本明細書で説明するプロセスおよび論理フローは、入力データを処理して出力を生成することによって機能を実行する１つまたは複数のコンピュータプログラムを実行する１つまたは複数のプログラマブルプロセッサによって実行することができる。プロセスおよび論理フローは、ＦＰＧＡ（フィールドプログラマブルゲートアレイ）またはＡＳＩＣ（特定用途向け集積回路）などの特定用途論理回路によっても実行することができる。コンピュータプログラムの実行に適したプロセッサは、一例として、汎用マイクロプロセッサおよび専用マイクロプロセッサの両方、および任意の種類のデジタルコンピュータの任意の１つまたは複数のプロセッサを含む。一般に、プロセッサは、読み出し専用メモリまたはランダムアクセスメモリ、あるいはその両方から命令およびデータを受信する。コンピュータの必須要素は、命令を実行するプロセッサと、命令およびデータを格納するための１つまたは複数のメモリデバイスとである。一般に、コンピュータは、データを格納するための１つまたは複数の大容量ストレージデバイス（例えば、磁気ディスク、光磁気ディスク、または光ディスク）からのデータを受信するか、またはデータを転送するか、あるいはその両方を行うように動作可能に結合される。しかしながら、コンピュータはそのようなデバイスを有する必要はない。コンピュータプログラム命令およびデータを格納するのに適したコンピュータ可読媒体には、半導体メモリデバイス（例えば、ＥＰＲＯＭ、ＥＥＰＲＯＭ、およびフラッシュメモリデバイス）、磁気ディスク（例えば、内蔵ハードディスクまたはリムーバブルディスク）、光磁気ディスク、およびＣＤＲＯＭおよびＤＶＤ－ＲＯＭディスクを含む全ての形態の不揮発性メモリ、媒体およびメモリデバイスが含まれる。プロセッサおよびメモリは、特定用途論理回路によって補完または特定用途論理回路に組み込むことができる。 The processes and logic flows described herein may be performed by one or more programmable processors that execute one or more computer programs that perform functions by processing input data and producing output. can. The processes and logic flows can also be performed by special purpose logic circuits such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any type of digital computer. Generally, a processor receives instructions and data from read-only memory and/or random access memory. The essential elements of a computer are a processor for executing instructions, and one or more memory devices for storing instructions and data. Generally, a computer receives data from one or more mass storage devices (e.g., magnetic, magneto-optical, or optical disks) for storing data, transfers data, or operably coupled to do both. However, a computer does not need to have such a device. Computer-readable media suitable for storing computer program instructions and data include semiconductor memory devices (e.g., EPROM, EEPROM, and flash memory devices), magnetic disks (e.g., internal hard disks or removable disks), magneto-optical disks, and all forms of non-volatile memory, media and memory devices, including CD-ROM and DVD-ROM discs. The processor and memory may be supplemented by or incorporated into special purpose logic circuits.
ユーザとのインタラクションを提供するために、本開示の１つまたは複数の態様は、例えば、ＣＲＴ（陰極線管）、ＬＤＣ（液晶ディスプレイ）モニタ、またはタッチスクリーンなどのユーザに情報を表示するためのディスプレイデバイスと、任意選択でユーザがコンピュータに入力を提供するキーボードおよびポインティングデバイス（例えば、マウスやトラックボール）とを有するコンピュータ上で実施することができる。他の種類の装置を使用して、例えば、任意の形態の感覚フィードバック（例えば、視覚フィードバック、聴覚フィードバック、または触覚フィードバック）であり得るユーザに提供されるフィードバックとともにユーザとのインタラクションを提供することもでき、ユーザからの入力は、音響、音声、または触覚入力を含む任意の形態で受信することができる。さらに、コンピュータは、ユーザによって使用されるデバイスとの間でドキュメントを送受信することによって（例えば、ウェブブラウザから受信した要求に応答してユーザのクライアントデバイス上のウェブブラウザにウェブページを送信することによって）、ユーザとインタラクションすることができる。 To provide interaction with a user, one or more aspects of the present disclosure may include a display for displaying information to the user, such as a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen. The method can be implemented on a computer having a device and, optionally, a keyboard and pointing device (eg, a mouse or trackball) for a user to provide input to the computer. Other types of devices may also be used, for example, to provide user interaction with feedback provided to the user, which may be any form of sensory feedback (e.g., visual, auditory, or tactile feedback). Input from the user can be received in any form, including acoustic, audio, or tactile input. In addition, the computer can send and receive documents to and from devices used by the user (e.g., by sending web pages to the web browser on the user's client device in response to requests received from the web browser). ) and can interact with the user.
いくつかの実施形態が説明されている。それにもかかわらず、本開示の技術思想および範囲から逸脱することなく、様々な変更がなされ得ることが理解されるであろう。従って、他の実施形態も以下の特許請求の範囲内にある。 Several embodiments have been described. Nevertheless, it will be understood that various changes may be made without departing from the spirit and scope of the disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (20)
発話を特徴付ける音響フレームのシーケンスを受信するステップと、
共有エンコーダを使用して、前記音響フレームのシーケンスにおける各音響フレームをエンコードして、対応するエンコードされた音響フレームを生成するステップと、
第２のパスのデコーダに関する複数の出力ステップの各出力ステップにおいて、
アテンション機構を使用して、前記エンコードされた音響フレームを要約する音響コンテキストベクトルを決定するステップと、
前記音響コンテキストベクトルを使用して、可能性のある出力ラベルに対する音響ベースの確率分布を決定するステップと、
前記アテンション機構を使用して、前記第２のパスのデコーダによって以前に出力されたデコードされたラベルのシーケンスに基づいて言語コンテキストベクトルを決定するステップと、
前記言語コンテキストベクトルを使用して、可能性のある出力ラベルに対するテキストベースの確率分布を決定するステップと、
可能性のある出力ラベルに対する前記音響ベースの確率分布および可能性のある出力ラベルに対する前記テキストベースの確率分布を補間するステップと、
前記複数の出力ステップの各々における可能性のある出力ラベルに対する前記音響ベースの確率分布および可能性のある出力ラベルに対する前記テキストベースの確率分布の前記補間に基づいて、前記発話の文字起こしを決定するステップと、を含む、方法。 A computer-implemented method that, when executed on data processing hardware, causes the data processing hardware to perform an operation, the operation comprising:
receiving a sequence of acoustic frames characterizing an utterance;
encoding each acoustic frame in the sequence of acoustic frames using a shared encoder to generate a corresponding encoded acoustic frame;
At each output step of the plurality of output steps for the second pass decoder,
determining an acoustic context vector summarizing the encoded acoustic frames using an attention mechanism;
determining an acoustic-based probability distribution for possible output labels using the acoustic context vector;
using the attention mechanism to determine a linguistic context vector based on a sequence of decoded labels previously output by the second pass decoder;
determining a text-based probability distribution for possible output labels using the linguistic context vector;
interpolating the acoustic-based probability distribution for possible output labels and the text-based probability distribution for possible output labels;
determining a transcription of the utterance based on the interpolation of the acoustic-based probability distribution for possible output labels and the text-based probability distribution for possible output labels at each of the plurality of output steps; A method, including steps.
第１のパスのデコーダを使用して、前記エンコードされた音響フレームを処理して、前記発話に関する音声認識仮説のトップのＫ個のリストを生成するステップと、前記音声認識仮説のトップのＫ個のリストにおける各音声認識仮説は、前記発話の候補文字起こしに対応しており、
前記第２のパスのデコーダは、再スコア付けモードで動作して、前記音声認識仮説のトップのＫ個のリストにおける各音声認識仮説を再スコア付けするステップと、をさらに含む、請求項１に記載の方法。 The said operation is
processing the encoded acoustic frames using a first pass decoder to generate a list of the top K speech recognition hypotheses for the utterance; each speech recognition hypothesis in the list corresponds to a candidate transcription of the utterance;
2. The second pass decoder further comprises: operating in a rescoring mode to rescore each speech recognition hypothesis in the top K list of speech recognition hypotheses. Method described.
前記データ処理ハードウェアは、前記ユーザデバイス上に存在している、請求項１に記載の方法。 the utterance characterized by the sequence of acoustic frames is captured in streaming audio by a user device;
2. The method of claim 1, wherein the data processing hardware resides on the user device.
データ処理ハードウェアと、
前記データ処理ハードウェアと通信し、かつ命令を格納するメモリハードウェアと、を備え、前記命令は、前記データ処理ハードウェア上での実行時に、前記データ処理ハードウェアに
発話を特徴付ける音響フレームのシーケンスを受信することと、
共有エンコーダを使用して、前記音響フレームのシーケンスにおける各音響フレームをエンコードして、対応するエンコードされた音響フレームを生成することと、
第２のパスのデコーダに関する複数の出力ステップの各出力ステップにおいて、
アテンション機構を使用して、前記エンコードされた音響フレームを要約する音響コンテキストベクトルを決定することと、
前記音響コンテキストベクトルを使用して、可能性のある出力ラベルに対する音響ベースの確率分布を決定することと、
前記アテンション機構を使用して、前記第２のパスのデコーダによって以前に出力されたデコードされたラベルのシーケンスに基づいて言語コンテキストベクトルを決定することと、
前記言語コンテキストベクトルを使用して、可能性のある出力ラベルに対するテキストベースの確率分布を決定することと、
可能性のある出力ラベルに対する前記音響ベースの確率分布および可能性のある出力ラベルに対する前記テキストベースの確率分布を補間することと、
前記複数の出力ステップの各々における可能性のある出力ラベルに対する前記音響ベースの確率分布および可能性のある出力ラベルに対する前記テキストベースの確率分布の前記補間に基づいて、前記発話の文字起こしを決定することと、を含む動作を実行させる、システム。 A system,
data processing hardware;
memory hardware in communication with the data processing hardware and storing instructions, the instructions, when executed on the data processing hardware, transmitting to the data processing hardware a sequence of acoustic frames characterizing an utterance. and to receive
encoding each acoustic frame in the sequence of acoustic frames using a shared encoder to generate a corresponding encoded acoustic frame;
At each output step of the plurality of output steps for the second pass decoder,
determining an acoustic context vector summarizing the encoded acoustic frame using an attention mechanism;
determining an acoustic-based probability distribution for possible output labels using the acoustic context vector;
using the attention mechanism to determine a linguistic context vector based on a sequence of decoded labels previously output by the second pass decoder;
determining a text-based probability distribution for possible output labels using the linguistic context vector;
interpolating the acoustic-based probability distribution for possible output labels and the text-based probability distribution for possible output labels;
determining a transcription of the utterance based on the interpolation of the acoustic-based probability distribution for possible output labels and the text-based probability distribution for possible output labels at each of the plurality of output steps; A system that causes an action to be performed, including
第１のパスのデコーダを使用して、前記エンコードされた音響フレームを処理して、前記発話に関する音声認識仮説のトップのＫ個のリストを生成することと、前記音声認識仮説のトップのＫ個のリストにおける各音声認識仮説は、前記発話の候補文字起こしに対応しており、
前記第２のパスのデコーダは、再スコア付けモードで動作して、前記音声認識仮説のトップのＫ個のリストにおける各音声認識仮説を再スコア付けすることと、をさらに含む、請求項１１に記載のシステム。 The said operation is
processing the encoded acoustic frames using a first pass decoder to generate a list of the top K speech recognition hypotheses for the utterance; and each speech recognition hypothesis in the list corresponds to a candidate transcription of the utterance;
12. The second pass decoder further comprises: operating in a rescoring mode to rescore each speech recognition hypothesis in the top K list of speech recognition hypotheses. System described.
前記データ処理ハードウェアは、前記ユーザデバイス上に存在している、請求項１１に記載のシステム。 the utterance characterized by the sequence of acoustic frames is captured in streaming audio by a user device;
12. The system of claim 11, wherein the data processing hardware resides on the user device.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062964567P | 2020-01-22 | 2020-01-22 | |
US62/964,567 | 2020-01-22 | ||
JP2022544371A JP7375211B2 (en) | 2020-01-22 | 2021-01-21 | Attention-based joint acoustic and text on-device end-to-end model |
PCT/US2021/014468 WO2021150791A1 (en) | 2020-01-22 | 2021-01-21 | Attention-based joint acoustic and text on-device end-to-end model |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022544371A Division JP7375211B2 (en) | 2020-01-22 | 2021-01-21 | Attention-based joint acoustic and text on-device end-to-end model |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2023175029A true JP2023175029A (en) | 2023-12-08 |
Family
ID=74669520
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022544371A Active JP7375211B2 (en) | 2020-01-22 | 2021-01-21 | Attention-based joint acoustic and text on-device end-to-end model |
JP2023183357A Pending JP2023175029A (en) | 2020-01-22 | 2023-10-25 | Attention-based joint acoustic and text on-device end-to-end model |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022544371A Active JP7375211B2 (en) | 2020-01-22 | 2021-01-21 | Attention-based joint acoustic and text on-device end-to-end model |
Country Status (6)
Country | Link |
---|---|
US (2) | US11594212B2 (en) |
EP (1) | EP4078573A1 (en) |
JP (2) | JP7375211B2 (en) |
KR (1) | KR20220128401A (en) |
CN (1) | CN114981884A (en) |
WO (1) | WO2021150791A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113539273B (en) * | 2021-09-16 | 2021-12-10 | 腾讯科技（深圳）有限公司 | Voice recognition method and device, computer equipment and storage medium |
WO2023059980A1 (en) * | 2021-10-04 | 2023-04-13 | Google Llc | Transducer-based streaming deliberation for cascaded encoders |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9799327B1 (en) * | 2016-02-26 | 2017-10-24 | Google Inc. | Speech recognition with attention-based recurrent neural networks |
US11133011B2 (en) | 2017-03-13 | 2021-09-28 | Mitsubishi Electric Research Laboratories, Inc. | System and method for multichannel end-to-end speech recognition |
US10706840B2 (en) * | 2017-08-18 | 2020-07-07 | Google Llc | Encoder-decoder models for sequence to sequence mapping |
US11017761B2 (en) * | 2017-10-19 | 2021-05-25 | Baidu Usa Llc | Parallel neural text-to-speech |
US10971142B2 (en) * | 2017-10-27 | 2021-04-06 | Baidu Usa Llc | Systems and methods for robust speech recognition using generative adversarial networks |
US10593321B2 (en) * | 2017-12-15 | 2020-03-17 | Mitsubishi Electric Research Laboratories, Inc. | Method and apparatus for multi-lingual end-to-end speech recognition |
-
2021
- 2021-01-21 EP EP21706752.9A patent/EP4078573A1/en active Pending
- 2021-01-21 CN CN202180009937.3A patent/CN114981884A/en active Pending
- 2021-01-21 US US17/155,010 patent/US11594212B2/en active Active
- 2021-01-21 WO PCT/US2021/014468 patent/WO2021150791A1/en unknown
- 2021-01-21 JP JP2022544371A patent/JP7375211B2/en active Active
- 2021-01-21 KR KR1020227027983A patent/KR20220128401A/en unknown
-
2023
- 2023-02-10 US US18/167,454 patent/US20230186901A1/en active Pending
- 2023-10-25 JP JP2023183357A patent/JP2023175029A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
JP2023511390A (en) | 2023-03-17 |
CN114981884A (en) | 2022-08-30 |
US20210225362A1 (en) | 2021-07-22 |
EP4078573A1 (en) | 2022-10-26 |
JP7375211B2 (en) | 2023-11-07 |
WO2021150791A1 (en) | 2021-07-29 |
US11594212B2 (en) | 2023-02-28 |
KR20220128401A (en) | 2022-09-20 |
US20230186901A1 (en) | 2023-06-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR20230003056A (en) | Speech recognition using non-speech text and speech synthesis | |
JP7222153B1 (en) | Derivation model-based two-pass end-to-end speech recognition | |
JP7436760B1 (en) | Learning word-level confidence for subword end-to-end automatic speech recognition | |
JP7351018B2 (en) | Proper noun recognition in end-to-end speech recognition | |
JP2023545988A (en) | Transformer transducer: One model that combines streaming and non-streaming speech recognition | |
JP2023175029A (en) | Attention-based joint acoustic and text on-device end-to-end model | |
KR20230158608A (en) | Multi-task learning for end-to-end automatic speech recognition confidence and erasure estimation. | |
JP2024512606A (en) | Reducing streaming ASR model delay using self-alignment | |
JP7286888B2 (en) | Emitting word timing with an end-to-end model | |
US20230298563A1 (en) | Deliberation by Text-Only and Semi-Supervised Training | |
KR102637025B1 (en) | Multilingual rescoring models for automatic speech recognition | |
KR20240022598A (en) | Inserting text in self-guided speech pretraining |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20231025 |