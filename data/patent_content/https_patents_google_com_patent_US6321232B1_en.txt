US6321232B1 - Method for creating a geometric hash tree in a document processing system - Google Patents
Method for creating a geometric hash tree in a document processing system Download PDFInfo
- Publication number
- US6321232B1 US6321232B1 US09/389,111 US38911199A US6321232B1 US 6321232 B1 US6321232 B1 US 6321232B1 US 38911199 A US38911199 A US 38911199A US 6321232 B1 US6321232 B1 US 6321232B1
- Authority
- US
- United States
- Prior art keywords
- tree
- image
- affine
- geometric
- images
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Lifetime
Links
- 238000000034 method Methods 0.000 title claims abstract description 42
- 238000012545 processing Methods 0.000 title claims abstract description 10
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 claims abstract description 73
- 230000006870 function Effects 0.000 claims abstract description 10
- 238000013139 quantization Methods 0.000 claims description 13
- 238000000638 solvent extraction Methods 0.000 claims description 4
- 239000003086 colorant Substances 0.000 claims description 2
- 230000004807 localization Effects 0.000 description 34
- 238000012549 training Methods 0.000 description 17
- 238000000605 extraction Methods 0.000 description 11
- 238000012795 verification Methods 0.000 description 11
- 238000000926 separation method Methods 0.000 description 10
- 238000010276 construction Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 8
- 230000008569 process Effects 0.000 description 7
- 238000013459 approach Methods 0.000 description 5
- 238000003780 insertion Methods 0.000 description 5
- 230000037431 insertion Effects 0.000 description 5
- 238000007781 pre-processing Methods 0.000 description 5
- 238000001514 detection method Methods 0.000 description 4
- 238000013528 artificial neural network Methods 0.000 description 3
- 230000002452 interceptive effect Effects 0.000 description 3
- 238000010606 normalization Methods 0.000 description 3
- 230000009466 transformation Effects 0.000 description 3
- 238000003491 array Methods 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 230000015556 catabolic process Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 125000004122 cyclic group Chemical group 0.000 description 1
- 230000001351 cycling effect Effects 0.000 description 1
- 238000013479 data entry Methods 0.000 description 1
- 238000006731 degradation reaction Methods 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 238000003909 pattern recognition Methods 0.000 description 1
- 230000000135 prohibitive effect Effects 0.000 description 1
- 230000000717 retained effect Effects 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
- G06F16/5854—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content using shape and object relationship
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/40—Document-oriented image-based pattern recognition
- G06V30/41—Analysis of document content
- G06V30/416—Extracting the logical structure, e.g. chapters, sections or page numbers; Identifying elements of the document, e.g. authors
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99941—Database schema or data structure
- Y10S707/99942—Manipulating data structure, e.g. compression, compaction, compilation
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99941—Database schema or data structure
- Y10S707/99943—Generating database or data structure, e.g. via user interface
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99941—Database schema or data structure
- Y10S707/99944—Object-oriented database structure
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99941—Database schema or data structure
- Y10S707/99944—Object-oriented database structure
- Y10S707/99945—Object-oriented database structure processing
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/99941—Database schema or data structure
- Y10S707/99948—Application of database or data structure, e.g. distributed, multimedia, or image
Definitions
- This invention relates to pattern localization and, more particularly, to a method for using a data structure to facilitate such pattern localization.
- the data structure is particularly suited for locating geometric patterns (including distinguishable features) in unsegmented images.
- Text-based retrieval is becoming a powerful alternative/addition to conventional text annotation-based retrieval. Even so, it has yet to reach the robustness and computational effectiveness of text-based retrieval.
- Text-based retrieval is notoriously lacking in precision, even when boolean combinations of key-words are allowed. It is a common observation with those using popular conventional search that full text indexing of documents (scanned or electronic) causes a large number of irrelevant documents to be retrieved.
- a more productive use of text-based querying is when it is combined with image content-based querying.
- a special case of this occurs when the text strings relevant for indexing documents occur within image structures, such as text in special regions of a news video or text within region fields of a form. Retrieval based on such structured text can yield fewer but more relevant matching documents.
- title blocks In contrast to full-text indexing of pure text documents, automatic full-text indexing using conventional OCR algorithms will not yield useful results for drawing images. Fortunately, useful text information for indexing such drawing images is found in specific image structures called “title blocks”. Typically, a title block will include information pertinent for indexing a corresponding drawing, such as part number, name of the unit being depicted, date of design, and architect name. Indexing keyword extraction from such image structures requires that the image structures themselves be first identified.
- the present invention employs some of the principles underlying a solution for a model indexing problem, namely the principles underlying “Geometric Hashing”.
- Geometric Hashing Referring to articles by Y. Lamdan and H. J. Wolfson (entitled “Geometric hashing: A general and efficient model-based recognition scheme”, in Proceeding of the International Conference on Computer Vision, pages 238-249, 1988, and “Transformation invariant indexing” in Geometric Invariants in Computer Vision, IT Press, pages 334-352, 1992), Geometric Hashing has been used to identify objects in pre-segmented image regions. Another work extending the basic geometric hashing scheme for use with line features includes an article by F. C. D.
- a data structure known as the “geometric hash table” can be used effectively to index handwritten words in a handwriting localization scheme. While the handwriting technique is believed to provide fast search and retrieval in the context of locating and recognizing handwritten word queries in handwritten documents, the same speed for search and retrieval is not obtainable when locating and recognizing two-dimensional patterns in a relatively large document (e.g. engineering drawing document). This degradation of search and retrieval speed is attributable, in substantial part, to the size of the geometric hash table. It would be desirable to provide a system in which localization of two-dimensional patterns could be achieved with a data structure that is considerably more compact than the geometric hash table.
- a geometric hash table for several types of document images, such as the type of document image associated with typical engineering drawing documents, can be quite large. It has been found that a geometric hash table for a group of images from a typical engineering drawing document set can be as large as 40 Gbytes, a size that far exceeds the size of main memory for most computer systems. Thus a database cannot be formed readily for a geometric hash table developed from one of several types of document images. It would be desirable to provide a relatively compact structure that both exploits the principles underlying the geometric hash tree and lends itself readily to searching in databases having images of all sizes.
- a method for creating a geometric hash tree in a document processing system having a memory having a memory.
- a plurality of images are stored in the memory and organized in a database.
- Each image includes curve groups wherein each curve group is corresponded with a feature set.
- the method for creating a geometric hash tree includes the steps of: (1) associating a list of basis triples with an affine coordinate set, the basis triples and the affine coordinate set both varying as a function of the images and their corresponding curve groups; (2) storing both the affine coordinate set and the list of basis triples in the memory; (3) quantizing the affine coordinate set into a plurality of subsets; (4) assigning an order to the plurality of subsets; and (5) creating a geometric hash tree with the quantized affine coordinate set using the order from (4) such that the geometric hash tree is more compact in size than a conventional geometric hash table.
- FIG. 1 is an elevational view of a scanned image of a sample handwritten document
- FIG. 2A is an elevational view of a sample handwritten document image
- FIG. 2B is an elevational view of a handwritten query word
- FIG. 2C is an elevational view of a subject query word projected at candidate locations
- FIG. 3 is a block diagram implementation employable in constructing hash tables
- FIG. 4 is a block diagram implementation employable in performing image indexing of hash tables
- FIG. 5A is an elevational view of curves in the handwritten sample document of FIG. 1, wherein corner features on the curves are shown in circles;
- FIG. 5B is an elevational view of a query pattern including a single curve, wherein corner features of the curve are used for indexing in a hash table;
- FIG. 6 is a schematic, graphical view of a histogram of hits for all basis points in the image of FIG. 5A;
- FIG. 7 is an elevational view representing Hashing results for FIG. 5A;
- FIG. 8 is an elevational view of three reduced engineering drawing document sheets
- FIGS. 9 and 10 are respective elevational views of two title blocks extracted from the reduced engineering drawing document sheets of FIG. 8;
- FIG. 11 is a flow diagram illustrating a process for generating a geometric hash tree from a geometric hash table
- FIG. 12 is a schematic, graphical representation illustrating a manner in which the geometric hash table is partitioned, in accordance with the process of FIG. 11, to from the geometric hash tree;
- FIG. 13 is a flow diagram illustrating a process for localizing two-dimensional (“2d”) patterns in images of databases represented through geometric hash trees;
- FIG. 14 is a schematic, block diagrammatic view of an engineering drawing indexing system
- FIG. 15 is an elevational view of image data corresponding with text extracted from one of the title blocks of FIG. 9.
- FIG. 16 is a graphical view illustrating time performance data for a geometric hash tree plotted as a function of query complexity.
- FIG. 3 the components for implementing a hash table construction technique are illustrated.
- original documents obtained by scanning handwritten pages at high resolution (200 dpi or higher) are obtained.
- Feature Extraction Module 2 Within the Feature Extraction Module 2 , connected component regions of scanned Document Images 1 are formed. Although several methods of finding connected components exist, the following algorithm is used to determine the connected components regions in bitmaps:
- Step A Find the number of “on” neighboring pixels (k′,l′) and their associated run lengths, and
- Step B Merge a given runlength with a neighboring runlength identified with 1. This is recorded by having all merged runlength having the same group identification.
- the above algorithm can be efficiently implemented using a data structure called the union-find data structure as described in a book by Cormen, Leisersen and Rivest entitled “Introduction to algorithms”, MIT Press, 1994, to run in time linear in the number of runlengths in the image.
- Boundary points are determined on the connected component regions as those points that have at least one “off” neighbor.
- a cyclic trace of such boundary pixels is used to yield curves representing the boundaries of the connected component regions.
- the curves are smoothed using a conventional line-segment approximation algorithm.
- corner features are extracted from the curves as those points where significant curvature deviation occurs, i.e., where the angle between two incident lines is greater than a specified threshold.
- Other methods of curve tracing and corner feature detection can be used without significantly affecting the principles of the presently described embodiment. Note that since the images are assumed to be scanned at high resolution, the lines are thick enough so that junctions are also manifested as corners in such images.
- Corner features on a curve are chosen as the basic unit for localization using the rationale that although not all curves come from single words, especially in the presence of occlusions and noise, features generated from within a curve are more likely to point to a single image location than an arbitrary triple of features chosen randomly across the image.
- the pre-processing step of curve extraction and feature generation can be applied uniformly to a document image or to a query word represented as an image pattern, and takes time linear in the size of the image.
- Detection of a line of text in a handwritten page image involves determining which of the individual word regions lie predominantly along a perceivable line of text. In contrast to the case for printed text, deducing lines of text in handwritten document is difficult because handwritten text words are often not written on a straight line. Furthermore, consecutive lines of text may not be parallel as in printed text. Finally, an author may vary the inter-word and intra-word spacing while writing so that different instances of the same word may show writing differences. This makes the task of determining which word segments belong to a group difficult.
- the method of detecting text lines disclosed herein is independent of page orientation, and does not assume that the individual lines of handwritten text are parallel. Furthermore, it does not require that all word regions be aligned with the text line orientation.
- the first operation performed on a bitmap image of a handwritten document is to pre-process the image using the Feature Extraction Module 2 of FIG. 3 to generate connected components of dark regions constituting word segments as well as curves formed from the boundaries of such connected regions.
- This pre-processing stage also records the centroids of the regions.
- the orientation of the word segment regions is determined as the direction of the moment-of-inertia axis of the region.
- the formula for finding the moment of inertia axis is given in Chapter 3 of the book entitled “Robot Vision” by B. K. P. Horn, MIT Press, 1986.
- a histogram of orientations is generated and its peaks automatically selected to represent major word orientations in the image. For each of the dominant orientations selected, a line of the specified orientation is drawn through the centroids of each of the regions. A clustering of these lines is done to determine groups of such lines.
- the resulting data structure, called the Hough transform table is a two-dimensional array that records the number of points (centroids of region here) that lie along or close to a line of specified orientation and position. The highest valued entries in this table are taken to correspond to candidate lines of text. The regions whose centroids contribute to the peak table entries are noted. These word segment regions thus are taken to form the lines of text in the handwritten document image.
- the curve groups capture word segments that form part of the same word. Once the lines of text, and hence the word segments that lie along a line of text, are determined, grouping involves assembling all such word segments that are separated by a distance—characterizing intra-word separation.
- the intra-word separation is estimated as follows:
- the boundaries of the word segment regions lying on the line are used to determine two extremal points per region; that is, all the boundary points of a region are projected onto the line, and the beginning and end points noted.
- a projection of a given point onto a line is the point of intersection of a perpendicular line through the given point with the given line.
- a histogram of such distances is generated. For most handwritten documents such a histogram shows at least two distinct peaks. The peak at the lowest separation distance is noted as intra-word separation.
- curve groups are formed by grouping word segment regions that are separated along the line of text orientation by a distance within a certain bound of the intra-word separation determined above. The grouping of curves separated by intra-word separation (+/ ⁇ a chosen threshold) is done using the union-find data structure mentioned earlier.
- an image hash table is developed within the Hash Table Construction Module 4 and is used to succinctly represent information in the position of features in curves in curve groups in a manner that helps locate a query handwritten word.
- each curve group consists of a single curve.
- the task is to locate a given query curve in an image including this curve.
- the coordinates of any other point P of the curve can be expressed in terms of the coordinates of points (O, P 1 , P 2 ) (called basis triples) as:
- the coordinates ( ⁇ , ⁇ ) are called affine coordinates and they are invariant to affine transformations.
- the corresponding points on the transformed image curve will have the same coordinates with respect to the transformed basis triples in the transformed image curve.
- one way to check if a curve at an image location matches a given curve is to see if enough feature points on the image curve have the same affine coordinates with respect to some image basis triple (O′, P′ 1 , P′ 2 ) on the image curve.
- O′, P′ 1 , P′ 2 image basis triple
- the resulting Image Hash Table 5 (FIG. 3) is a data structure representing a convenient way to express this computed information so that the entries are the basis triples that give rise to a range of affine coordinates.
- the image hash table is constructed within the Hash Table Construction Module 4 using a suitable quantization of the affine coordinates, and recording the basis points that give rise to the respective affine coordinates. That is:
- H( ⁇ 1 ⁇ 2, ⁇ 1 ⁇ 2) ⁇ O′, P′ 1 , P′ 2 >. . . ⁇
- the image hash table is constructed as follows:
- Each triple of consecutive features in a curve is used as a basis triple, and the affine coordinates of all features in the curve group are computed.
- the basis points are taken from a single curve, but the affine coordinates are computed for all features on all curves in a curve group.
- module 4 could be used for constructing the geometric hash tree and the geometric hash tree data structure would result in Geometric Hash Tree 5 .
- a Query Word 6 is given to the system during indexing, and curve groups are generated from the word using the pre-processing steps and requisite modules 7 and 8 for feature generation described in FIG. 3 .
- the word localization is attempted first using curve groups of longer average curve lengths.
- sets of affine coordinates are computed within the Indexing Module 9 and used to index the Image Hash Table 12. Since the number of basis points are linear, this operation can be repeated with respect to all basis points in the curve group for robustness.
- the number of times it was indexed (called a hit) as well as the corresponding query triple are recorded.
- a histogram of the number of hits and the corresponding query word and matching basis points in the document image are recorded within the Histogram Ranking Module 10 .
- the peaks in the histogram are then taken as the candidate locations for the query.
- the indexing of the hash table accounts for the breaking of words into word segments in the image (or query word) by generating a set of affine coordinates as follows:
- k is a number representative of the number of curves in a curve group.
- the value of k is meant to be tuned to the handwriting style of the author (i.e., the way he or she writes words in his or her characteristic style).
- the last step of word localization verifies the word at the candidate locations given in the indexing step. This is conducted by the Pose verification module 11 . This step involves recovering the pose parameters (A,T) by solving the set of linear equations for the matching basis points corresponding to the significant hits.
- indexing or object localization/verification scheme described above could be implemented alternatively with a geometric hash tree instead of a hash table.
- FIG. 1 shows a scanned handwritten document and FIG. 5A shows the result of pre-processing and feature extraction on that image.
- the corner features per curve used for hash table construction are shown as circles in FIG. 5 A.
- FIG. 5B shows a query pattern consisting of a single curve.
- FIG. 6 shows the histogram of hashing based on affine coordinates. Here the image basis points are plotted against the number of hits they obtained from affine coordinates on the query pattern.
- FIG. 7 shows the results of hashing.
- the hashed image basis points corresponding to the three most significant peaks of the histogram are matched to their respective query basis triples to compute candidate poses.
- the query curve is then projected into the image using the pose parameters and shown overlayed on the original image in FIG. 7 .
- the top two matches localize the query pattern correctly at the two places it occurs.
- the third match is however, a false positive which can be removed during pose verification. The false positive occurs in this case because of a merging of the foreground text patterns with the lines of the tabular background in the image.
- FIG. 2A shows a sample document in which a word “database” occurs twice.
- the query word “database” is illustrated in FIG. 2 B.
- the inter-letter spacing between letters of the word is not uniform in the two instances.
- the query pattern used for indexing is shown in FIG. 2 C.
- the top three matches are shown overlayed (after pose solution) on the original image to indicate query localization. Notice that using the indexing scheme, the word has been localized even when its constituent letters are written with different spacings in the two instances in which it occurs in the image.
- the false positive match shown here persisted even after pose verification, because of the similarity with the underlying word based on corner features.
- the above method can be used to identify multiple occurrences of the word in the document without explicitly matching to every single word in the document as is done by other tokenization schemes. Also, by using affine invariant features within curve groups, such a tokenization scheme is robust to changes in orientation, skew, and handwriting variances for a single author.
- location hashing is a technique for indexing arbitrary 2d pattern queries in images of a database, called location hashing, and the technique is applied to the problem of localization of title blocks in engineering drawing images.
- Location hashing is a variation of geometric hashing and determines simultaneously the relevant images in the database and the regions within them that are most likely to contain a 2d pattern.
- An engineering drawing indexing system that localizes title blocks using location hashing, and extracts indexing text keywords from these regions is also described. This enables retrieval of drawing documents using the conventional machinery of text.
- title blocks are 2d patterns that can be of different styles. Referring to FIGS. 9 and 10, it will be noted that different title blocks or patterns may exist across drawing sets. These patterns can be drawn at different locations in a drawing (FIG. 8 ), and they can appear in different orientation, depending on the scanning process. Moreover, title blocks may be confused with other tabular pattern regions appear merged with other contours, and/or exhibit noisy and spurious features due to scanning resolution. Finally, since title block patterns are originally hand-drawn, even patterns of the same basic type may show variations in the relative positions of the contours in addition to variations in field information.
- title block localization is regarded as an instance of the general problem of localizing 2d pattern queries in unsegmented images of a database. This is one of the most difficult problems in content-based retrieval, and is believed to have been addressed by only a relatively few researchers.
- a solution to this problem requires computationally effective approaches that can identity relevant images of the database as well as candidate regions in such images that are likely to contain the pattern. The solution should be achievable without detailed search of either the database or the images themselves.
- location hashing A technique for localization of 2d patterns in image databases, called location hashing is now presented.
- geometric hashing is varied for the purpose of enabling image indexing of databases.
- the basic premise underlying geometric hashing and the development of a hash table H was discussed in detail above.
- Location hashing addresses the problem of image indexing. That is, the goal is to use the hashing technique to not only identity relevant images of a database that contain a pattern query but also localize a region in the image that contains the 2d pattern. Geometric hashing in its original form, is not viewed as being directly applicable to location hashing for two reasons. First, if the hash table were constructed in an entirely analogous manner by considering all possible basis triples, this would be computationally prohibitive. In particular, it can be shown that the size of a hash table for images is expressed as 0(P*N 4 ), where P is the number of images and N is the number of features per image.
- N is of the order of 1000 features (corners, for example), so that even with one image in the database, the size of the hash table can grow to be O(N 4 ) ⁇ 10 12 , an impossible size for a hash table.
- the highest number of hits may be for a basis that comes from any arbitrary triple of features distributed across the image.
- hashing for the largest number of hits need not correspond to a single image location.
- Location hashing takes an intermediate course in building the hash table that is in between the version with fully grouped features, and the version with no grouping that considers all possible basis triples.
- features are selected from structures that are likely to come from single objects, and then features are grouped loosely to provide a small number of groups.
- a curve is chosen as a basic unit for generating features, and consecutive corner features are chosen along the curve for generating the basis points. Curves are chosen based on the rationale that although not all curves come from single objects, (especially in the presence of occlusions and noise) features generated from within a curve are more likely to point to a single image location than an arbitrary triple of features chosen randomly across the image.
- model images are provided to a data base (step 100 ).
- Curves are extracted (step 101 ) from all images of the database, and consecutive corner features are noted.
- a group of curves (curve-group) is formed (step 102 ) using a suitable grouping technique (one such grouping is mentioned in the next section), and features are extracted from curve groups (step 103 ).
- Basis triples are formed from consecutive features (step 104 ) along the curves.
- the affine coordinates of all other features in the curve group are computed (step 105 ). This process is repeated for all curve groups in all images of the database.
- the curve-group R B is noted to serve as a location index.
- the image Im R B to which each basis triple B belongs is noted.
- each affine coordinate ( ⁇ , ⁇ ) is associated with a list of triples,
- the hash table is a quantized version of the affine coordinate space, with the level of quantization being a function of the image size as well as feature sets.
- the sizes of the images can vary in a database, each type of database has its own typical size.
- an engineering drawing database may consist of high resolution scanned engineering drawings of sizes 9000 ⁇ 14000, while a scene image database generated from video camera would typically include images of sizes 512 ⁇ 640.
- the feature sets also determine the hash table quantization, particularly based on the distribution of features.
- the curves tend to be long lines with sharp corners at ends, causing the affine coordinates of consecutive points along curves to be farther apart, allowing a coarser quantization.
- a fine quantization is suitable when precise matches to queries need to be found, but can be expensive in terms of the amount of memory required.
- a coarse quantization allows flexibility in matching a query to a stored model, but can result in an undesirable amount of false matches.
- the resulting array size of a corresponding affine coordinate plane can be quite large.
- an observed range of each affine coordinates is in the range ( ⁇ 10000.0, 10000.0).
- a quantization level of 1.0 a straightforward array implementation would result in an array size of about 4 ⁇ 10 11 or 40 Gbytes, too large a table to fit in the main memory of any existing computer.
- Such a uniform partition of the affine coordinate space is unnecessary, as only a few of the units are non-empty, thus suggesting a more compact representation.
- the location hash table can, therefore, be physically represented through more compact structures such as a balanced binary search tree (steps 107 , 108 ).
- a simple 2d ordering was used on associated affine coordinates, and the active entries were organized in the affine coordinate plane as a binary search tree.
- a red-black tree is a binary search tree with additional color information attached to each tree node, and obeys the following properties: (1) Every node is either red or black; (2) Every leaf (NIL) is black; (3) If a node is red, then both its children are black; and (4) Every simple path from a node to a descendent lead contains the same number of black nodes. These properties ensure that a red-black tree remains balanced. In particular, it can be shown that a red-black tree with n internal nodes has height at most 2log(n+1).
- each non-empty entry in the location hash table is represented by a node N 1 where,
- N i (I ⁇ i ,I ⁇ i ,C(i),Data (i),left(i),right(i)), 1 ⁇ i ⁇ N
- N is the number of occupied cells in the location hash table
- left(i) and right(i) are the left and right pointers.
- (I ⁇ i , I ⁇ i ) are the index of the affine coordinates ( ⁇ , ⁇ ) in the hash table based on the quantization chosen
- C(i) is one of two colors, namely, red or black.
- the set Data(i) represents the collection of information pertaining to all affine coordinates that fall into the cell represented by (I ⁇ , I ⁇ ).
- F j t is a list of basis triple information of basis triples giving rise to affine coordinates of the cell, and k i represents the number of data entries per node i.
- the insertion of the feature involves two steps, namely, tree search and insertion, and tree balancing.
- the tree insertion is done as in conventional binary search, except that when there is a match of keys at a node, the data elements are updated as given below.
- T is a pointer to the root node of the GHT (which is NIL initially).
- Balancing is done as is conventional for a red-black tree, and involves doing search and tree rotations to maintain the balancing properties of the red black tree outlined above.
- the binary search tree form of the GHT is modified by recoloring nodes and performing rotations.
- the code for RIGHT-ROTATE is similar. Both LEFT-ROTATE and RIGHT-ROTATE run in O(1) time.
- the cost of insertion into an n-node GHT can be accomplished in O(logn) time, as the height of the GHT (being a red-black tree) is O(logn).
- step 116 of FIG. 13 a query image or 2d pattern is provided.
- the pattern is processed and affine coordinate features are generated (step 118 ) in a manner identical to the one described in the previous section.
- Steps 116 and 118 are comparable to steps 101 - 105 of the flow diagram in FIG. 11 .
- Affine coordinate features from each pattern curve-group are successively used to index the GHT (step 120 ) to retrieve potential matching basis triples and their associated region and image indices.
- a histogram of the basis triples indexed or “hit” (steps 122 , 124 ) is taken and the peak of the histogram shows the basis triple most likely to match a query basis triple.
- a histogram of regions indexed (taken using affine coordinates generated from several query basis triples), points to the most likely region (and hence the most likely image) to contain the pattern. Since the localization only points to places most likely to contain the object (but not guaranteed to do so), localization is preferably followed by a verification stage (step 126 ).
- R represents a query region (feature group)
- F Q (R) represents the set of affine coordinate features generated from the query group R.
- the indexing of the GHT (LHT (or GHT)-INDEX) is similar to the binary search done prior to TREE-INSERT. On finding a match at a node, the data component of the node is returned. If no match is found, the search bottoms out of a leaf node, and NIL is returned.
- h(i) represent histograms of the respective quantities
- (B M ,R M ,I M ) are the most likely basis triple, the most likely region, and the most likely image, respectively, to contain the object query.
- the first two quantities correspond to the peaks in the histogram of basis triple and region “hits” in the GHT.
- the image index is the image from which the most likely region arises.
- QUERY-INDEX can be accepted with confidence when the h(B M )>T B and h(R M )>T R , for some thresholds T B and T R suitably chosen for the domain (for example, an acceptance criterion may be acceptable if over 95% of the query feature groups affine coordinates can be accounted for by some basis triple).
- Feature groups are used to index the GHT successively, until at least one query region (curve group) succeeds in finding a matching group in the images of a corresponding database. The likely candidate locations must then be verified by projecting the query at the underlying image regions using the transform recovered from a match of the basis.
- Patterns intended for use with the localization are assumed to be available through a training stage and may be obtained by cropping representative regions from sample images.
- the feature extraction process (which is applied identically to title blocks and scanned images) is as follows:
- drawing images can be relatively large in size (e.g., 14000 ⁇ 9000 at 40 Odpi scan resolution), they are first normalized to a fixed size (since location hashing is scale-invariant, image normalization does not affect hashing).
- the scaled and normalized images are further processed to retain only the contour pixels of connected regions.
- the border contours are traced to give the respective curves. Ordering of points along the curve is then done through a simple depth-first in-order tree traversal.
- connected components are extracted from the scaled image.
- Groups of curves are formed by grouping curves that come from a single connected region. This simple grouping ensures a linear number of groups, and can isolate a region that contains the title block (possibly merged with the surrounding contours). Corner features are extracted from all the curves, and consecutive corner points along single curves are used to form basis triples.
- the affine coordinates of all corner features in a group are computed with respect to the basis triples of the group. Similar processing is done for the title block patterns.
- the quantization level for affine coordinates is chosen from a distribution of the affine coordinates of the title blocks used for training, to give a quantization level suitable for localizing such title blocks.
- a GHT of the basis points is created where the region and image index of each basis triple is stored along with the basis triple as explained in the previous section.
- each curve group is successively used to index the GHT of the engineering drawing database, until the recorded number of hits confirm a possible match.
- the resulting matching basis points are used to recover the underlying affine transform and the title block is projected at the indicated location for verification. While corner features are used for location hashing, lines of curve groups are preferably used for verification.
- title block localization was to enable extraction of indexing keywords from localized regions in drawing images. Not all fields of a title block, however, may contain relevant indexing information. For example, in one or both of the title blocks of FIGS. 9 and 10, the values of fields with company label, date, and drawing number, may be deemed important for indexing. Regions containing important indexing information may be marked in a training stage during title block pattern construction. Since the text within a given title block can appear in orientations other than left to right, the orientation of text is preferably noted during training.
- the relevant text information is extracted as follows. Using the original known position and orientation of the text region, and the computed pose of the localized title block, the relevant text field regions are localized within the title block and an appropriate transformation is derived that allows the text information to appear in the preferred left to right order. These regions are then re-scaled back to original resolution (400 dpi), if necessary, to enable accurate text recognition.
- the title block field labels may be typed fonts, the field values are often handwritten (e.g., date or part number).
- the handprint recognizer is a neural network-based character classifier for hand-printed and machine printed characters.
- Digit, alphabetic and alphanumeric recognition can be separately invoked on each text region based on the metadata provided for the corresponding field during the title block training stage.
- the training data for the neural network has been obtained from several machine and handprint databases such as the NIST, UNLV databases. Characters recognized are assembled into words, and a lexicon of domain terminology created from the engineering drawings is used to aid the word recognition.
- a system for extracting indexing keywords was developed for the Windows NT platform. Java-based interfaces were used for easy training on title block patterns, obtained by cropping regions from training drawing images. The metadata (field coordinates, their labels, their orientation) of each indexing field was also noted through dialog boxes. Title block templates could then be used in interactive or batch modes.
- individual users can invoke indexing of the engineering drawing database (currently, a list of tiff files), using a title block template selected from an available menu.
- a list of images containing the title blocks is returned along with the recognized text keywords within the regions. These keywords form the indices of their respective drawing images.
- the batch mode is similar in approach to the interactive mode, except that a set of templates are used successively to obtain the indexing keywords from corresponding images.
- the indexing system can also be integrated with a large format document scanner, to scan and index drawings “on the fly”. Through the use of efficient algorithms, the feature extraction and GHT construction time has been reduced to about 30 sec per image. Text recognition takes another 30 sec.
- the system includes a training module (block 132 ) that learns information necessary to localize and parse the comments of title blocks with the help of an expert user.
- the three main components of the indexing system are (a) a training module for title block training, (b) a title block localization module (block 134 ), and (c) a text recognition module (block 136 ).
- the indexing system 130 is implementable with a variety of image capture devices, such as a large format scanner. Moreover, the system is intended to operate in at least one of two modes, namely a training mode or a recognition/indexing mode.
- the function of the title block training module (block 130 in FIG. 14) is to infer the information from the title block regions of engineering drawings that will be relevant for its later retrieval.
- the training module includes a Java-based user interface in which a scanned and/or pre-processed engineering drawing image (in tiff format) may be displayed. An expert user highlights the title block region in the drawing, and the training module redisplays the zoomed-in region for the user. The displayed region may be used as a reference image for subsequent computations.
- the regions in the title block that contain useful indexing information are similarly highlighted by the user (through a mouse interface).
- a dialog window is subsequently provided for the user to enter metadata about the highlighted region.
- the name of the indexing field its content type (alphabetic, numeric, alphanumeric), text orientation (left to right, top to bottom, etc.), and any additional comments to aid in text recognition are supplied by the user.
- the text orientation may be important to note, as title block regions often depict text in multiple orientations.
- the following information is generated about the title block regions: (1) the image region of the title block, (2) the indexing field information that includes their location in the image, and the metadata entered by the user, (3) a normalized and scaled version of the original title block region for each of recognition.
- the above information may comprise the title block “model” or template information for subsequent use in the indexing stage.
- This template is stored in a chosen directory and a name selected by the user is assigned as an index to the template description. The template name can then appear in a template menu of a scan subsystem for use during scanning and automatic indexing.
- the title block localization module (block 132 in FIG. 14) takes, in one example, a scanned and pre-processed engineering drawing in tiff format and a user-selected title-block template as the input. It detects if a region corresponding to the user-selected template is present in the scanned image.
- the detection employs the above-described technique of location hashing, in which pose-invariant features are extracted from the title block template as well as the given image.
- the features in the image are represented compactly and organized for efficient search using a balanced binary search tree, namely the geometric hash tree.
- object-based coordinates e.g., affine coordinates
- index the GHT to point to candidate locations in images that have a collection of features having the same affine coordinates, and hence the likelihood of the title block region being present.
- the top five hits during this indexing are retained and displayed to the user.
- the top hit points to the location of the title block in the query object or drawing.
- the localization technique described herein establishes the pose or the orientation of the drawing so that the individual field regions within a given title block can be de-rotated, if necessary, to contain text in the appropriate left to right orientation for initiating text recognition.
- the Text Processing module (block 134 ) module takes the indexing keyword containing regions from the localization module (block 132 ), the metadata in the chosen title block template, and possibly a lexicon to aid in text recognition with contextual information.
- the module 134 then performs word recognition (machine and handprint) to give as output keywords to be used for indexing the document.
- word recognition machine and handprint
- this module can also recognize machine-printed text.
- the types of text recognized are: numeric (includes Arabic numerals ranging from 0 through 9), alphabetic (composed of 26 Roman letters from A through Z), and alphanumeric (containing 36 elements from the union of alphabet and numeric classes).
- the image data shown in FIG. 15 represents a binary sample of the scanned field on the document of FIG. 9, where a black sample is a ‘1’ and white sample is a ‘0’.
- the first step is to isolate the characters in the image by determining and encoding the contours of all the contiguous black areas on the image.
- the list of contours is filtered to remove small or unusually large contours that do not represent characters.
- the contours are grouped together to form characters. While different grouping schemes are admissible in this framework, a scheme of grouping that groups contours with centers separated by 0.0125 inches and overlap in vertical extent by 45% was used.
- Grouping is done by cycling through the list of contours and either forming a new group or adding contours to existing groups until there are no free contours left.
- Each group now represents a character which is rendered on a 128 pixel by 128 pixel raster.
- the rendered character is then passed to a normalization module, using algorithms and computer code provided by the National Institute of Technology and Standards, which corrects character slant and resizes the character to a standard 32 pixel by 32 pixel.
- Normalization facilitates classifier training by reducing the intra-class variance of the features.
- a feedforward neural-network having 15 nodes in the hidden layer was trained for each type of data, the input being 72 real numbers and the output being a vector of 10, 26, or 36 real numbers between 0 and 1, depending on whether the data type is numerical, alphabetic or alphanumeric, respectively.
- the output node with the highest value is reported as the recognized class (characters).
- Characters are sorted by position in the original document, left to right, top to bottom. Closely spaced, vertically overlapping recognized characters are collected into words. Vertically separated words are assigned to lines. A string in which words are separated by a single space and two spaces delimit lines is returned as the ASCII representation of the field.
- a database of scanned engineering drawings was assembled by scanning hardcopy drawings through a conventional large format scanner at 400 dpi resolution.
- the database included 1000 drawings. Since quantization levels based on the largest addable image sizes were used, new images can be added to the GHT without recomputation of the affine coordinates for previously present images.
- the performance of the indexing system is a function of the performance of title block localization, as well as character recognition.
- the handprint recognition module was rigorously tested on standard data sets such as the UNLV database, and was shown to recognize characters with an accuracy of 95% for printed text and 68% for hand-printed text.
- the handprint recognition module was also tested to a limited extent in conjunction with the title block localization and indexing region extraction, with results achieved showing similar performance.
- the result of localization is shown by overlaying the projected title block pattern (using the pose computed during location hashing) on the image found to contain the pattern. It has been found that a pattern can be localized accurately, notwithstanding differing positions and orientations between query patterns and model images.
- the number of features is very large for most drawing images, and would have made localization by conventional model-based recognition methods difficult, if not impossible.
- the GHT is a very space-efficient alternative to geometric hashing and represents a straightforward array-based implementation of location hashing.
- the actual CPU time for title block localization was recorded using a C/C++ implementation on a Windows NT platform (200 Mhz and 250M page size). Although the code was not optimized for speed, the CPU time for indexing of a single curve group in a title block pattern is indicative of corresponding performance.
- the average CPU time for 40 title block query patterns was noted as a function of the database size. The result is plotted in FIG. 16 . As can be seen, the time performance is not greatly affected by the complexity of the pattern (as indicated by the number of features), pointing to the indexing efficiency even for complex patterns.
- a data structure referred to as a geometric hash tree (“GHT”), particularly well suited for storing pose-invariant feature information is provided.
- the GHT permits such information to be organized as a color tagged balanced binary tree, the tree permitting efficient storage of data in a manner that facilitates pattern searching/localization.
- the data structure has broad implications as a new geometric search tree for database organization, and as a tool in locating/recognizing arbitrary patterns within document processing systems.
- a system that permits automatic text extraction from title blocks of engineering drawing documents.
- the system can readily be integrated with an image capture device (e.g., a large format scanner) to facilitate automatic drawing content-based document annotation prior to storing corresponding document sheets to a repository.
- the system combines geometric and text recognition capabilities to extract indexing information from engineering drawing images in a way that is invariant to (i) shape and style of title blocks, (ii) their position in drawings , (iii) the skew in the scanned drawing, and (iv) modest amounts of noise due to scanning and paper quality.
- the system can recognize title blocks 138 even when their orientation does not correspond with the orientation of model title block 140 .
Abstract
Description
TREE-INSERT (T,z) | ||
y = NIL | ||
X =root(T) | ||
keymatch = NIL | ||
while x ≠ NIL and keymatch = NIL do | ||
y=x | ||
if key[z] < key[x] | ||
then x = left[x] | ||
else if key[z] > key[x] | ||
then x = right[x] | ||
else data[x] = data[x] ∪ data[z] | ||
color[x] = RED | ||
keymatch = TRUE | ||
if keymatch = NIL | ||
then p[z] = y | ||
if y = NIL | ||
then root[T] = z | ||
else if key[z] < key[y] | ||
then left[y] = z | ||
else right[y] = z | ||
GHT-BALANCE(T,x) | ||
while x ≠ root[T] and color[p[x]] = RED do | ||
if p[x] = left[p[p[x]]] | ||
then y = right[p[p[x]]] | ||
if color[y] = RED | ||
then color[p[x]] = BLACK | ||
color[y] = BLACK | ||
color[p[p[x]]] = RED | ||
x = p[p[x]] | ||
else if x = right[p[x]] | ||
then x = p[x] | ||
LEFT-ROTATE(T,x) | ||
color[p[x]] = BLACK | ||
color[p[p[x]]] = RED | ||
RIGHT-ROTATE(t,p[p[x]]) | ||
else | ||
(same as then clause with “right” and “left” interchanged) | ||
color[root[t]] = BLACK | ||
LEFT-ROTATE(t,x) | ||
y = right[x] | ||
right[x] = left[y] | ||
if left[y] ≠ NIL | ||
then p[left[y]] = x | ||
p[y] = p[x] | ||
if p[x] = NIL | ||
then root[T] = y | ||
else if x = left[p[x]] | ||
then left[p[x]] = y | ||
else right[p[x]] = y | ||
left[y] = x | ||
p[x] = y | ||
Avg. | |||||||
basis | |||||||
per | Geometric | Location | GHT | ||||
S. No. | Features | Groups | group | hash table | | nodes | |
1. | 30305 | 502 | 98.6 | 4.6 × 1010 | 1.87 × 108 | 2.1 × 105 |
2. | 6332 | 90 | 53.4 | 7.10 × 108 | 2.02 × 105 | 1.34 × 104 |
3. | 1810 | 89 | 23.3 | 1.42 × 107 | 3.5 × 104 | 2.3 × 103 |
4. | 2347 | 78 | 30.1 | 6.18 × 101 | 7.02 × 104 | 3.5 × 104 |
5. | 4552 | 101 | 44.06 | 3.78 × 108 | 2.1 × 105 | 2.35 × 104 |
6. | 12617 | 43 | 200.3 | 6.8 × 1010 | 2.3 × 106 | 1.35 × 106 |
Matches in | |||
Query Features | Actual Occurrences | Hits Examined | |
1010 | 8 | 67 | 4 |
1010 | 6 | 123 | 3 |
3746 | 7 | 63 | 5 |
870 | 25 | 35 | 9 |
1345 | 13 | 48 | 5 |
569 | 32 | 67 | 8 |
Claims (10)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US09/389,111 US6321232B1 (en) | 1998-12-18 | 1999-09-02 | Method for creating a geometric hash tree in a document processing system |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US11296698P | 1998-12-18 | 1998-12-18 | |
US09/389,111 US6321232B1 (en) | 1998-12-18 | 1999-09-02 | Method for creating a geometric hash tree in a document processing system |
Publications (1)
Publication Number | Publication Date |
---|---|
US6321232B1 true US6321232B1 (en) | 2001-11-20 |
Family
ID=26810571
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US09/389,111 Expired - Lifetime US6321232B1 (en) | 1998-12-18 | 1999-09-02 | Method for creating a geometric hash tree in a document processing system |
Country Status (1)
Country | Link |
---|---|
US (1) | US6321232B1 (en) |
Cited By (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010014887A1 (en) * | 2000-02-15 | 2001-08-16 | Wong Tin Cheung | Computer automated system for management of engineering drawings |
US20020154778A1 (en) * | 2001-04-24 | 2002-10-24 | Mihcak M. Kivanc | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20020172394A1 (en) * | 2001-04-24 | 2002-11-21 | Ramarathnam Venkatesan | Robust and stealthy video watermarking |
US20020172425A1 (en) * | 2001-04-24 | 2002-11-21 | Ramarathnam Venkatesan | Recognizer of text-based work |
US20020184505A1 (en) * | 2001-04-24 | 2002-12-05 | Mihcak M. Kivanc | Recognizer of audio-content in digital signals |
US20020196976A1 (en) * | 2001-04-24 | 2002-12-26 | Mihcak M. Kivanc | Robust recognizer of perceptually similar content |
US20030169925A1 (en) * | 2002-03-11 | 2003-09-11 | Jean-Pierre Polonowski | Character recognition system and method |
US20040001605A1 (en) * | 2002-06-28 | 2004-01-01 | Ramarathnam Venkatesan | Watermarking via quantization of statistics of overlapping regions |
US20040005097A1 (en) * | 2002-06-28 | 2004-01-08 | Ramarathnam Venkatesan | Content recognizer via probabilistic mirror distribution |
US20040025025A1 (en) * | 1999-10-19 | 2004-02-05 | Ramarathnam Venkatesan | System and method for hashing digital images |
US6691126B1 (en) * | 2000-06-14 | 2004-02-10 | International Business Machines Corporation | Method and apparatus for locating multi-region objects in an image or video database |
US20040073579A1 (en) * | 2002-10-09 | 2004-04-15 | Kirk Snyder | System and method for implementing dynamic set operations on data stored in a sorted array |
US6757686B1 (en) * | 2000-06-14 | 2004-06-29 | International Business Machines Corporation | Method and apparatus for representing database and query information using interval hash tree |
US20040186916A1 (en) * | 2003-03-03 | 2004-09-23 | Bjorner Nikolaj S. | Interval vector based knowledge synchronization for resource versioning |
US20050063545A1 (en) * | 2003-09-19 | 2005-03-24 | Ntt Docomo, Inc | Structured document signature device, structured document adaptation device and structured document verification device |
US20050165690A1 (en) * | 2004-01-23 | 2005-07-28 | Microsoft Corporation | Watermarking via quantization of rational statistics of regions |
US20050257060A1 (en) * | 2004-04-30 | 2005-11-17 | Microsoft Corporation | Randomized signal transforms and their applications |
US20060079156A1 (en) * | 2003-05-02 | 2006-04-13 | Applied Materials, Inc. | Method for processing a substrate using multiple fluid distributions on a polishing surface |
US20080319987A1 (en) * | 2007-06-19 | 2008-12-25 | Daisuke Takuma | System, method and program for creating index for database |
US7831832B2 (en) | 2004-01-06 | 2010-11-09 | Microsoft Corporation | Digital goods representation based upon matrix invariances |
CN102110122A (en) * | 2009-12-24 | 2011-06-29 | 阿里巴巴集团控股有限公司 | Method and device for establishing sample picture index table, method and device for filtering pictures and method and device for searching pictures |
US20110264687A1 (en) * | 2010-04-23 | 2011-10-27 | Red Hat, Inc. | Concurrent linked hashed maps |
US8170372B2 (en) * | 2010-08-06 | 2012-05-01 | Kennedy Michael B | System and method to find the precise location of objects of interest in digital images |
US20120221572A1 (en) * | 2011-02-24 | 2012-08-30 | Nec Laboratories America, Inc. | Contextual weighting and efficient re-ranking for vocabulary tree based image retrieval |
US20140321765A1 (en) * | 2011-11-18 | 2014-10-30 | Nec Corporation | Feature descriptor encoding apparatus, feature descriptor encoding method, and program |
US20150339543A1 (en) * | 2014-05-22 | 2015-11-26 | Xerox Corporation | Method and apparatus for classifying machine printed text and handwritten text |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5465353A (en) * | 1994-04-01 | 1995-11-07 | Ricoh Company, Ltd. | Image matching and retrieval by multi-access redundant hashing |
US5742807A (en) * | 1995-05-31 | 1998-04-21 | Xerox Corporation | Indexing system using one-way hash for document service |
US5799312A (en) * | 1996-11-26 | 1998-08-25 | International Business Machines Corporation | Three-dimensional affine-invariant hashing defined over any three-dimensional convex domain and producing uniformly-distributed hash keys |
US5802525A (en) * | 1996-11-26 | 1998-09-01 | International Business Machines Corporation | Two-dimensional affine-invariant hashing defined over any two-dimensional convex domain and producing uniformly-distributed hash keys |
US5875264A (en) * | 1993-12-03 | 1999-02-23 | Kaman Sciences Corporation | Pixel hashing image recognition system |
US5897637A (en) * | 1997-03-07 | 1999-04-27 | Apple Computer, Inc. | System and method for rapidly identifying the existence and location of an item in a file |
US5953451A (en) | 1997-06-19 | 1999-09-14 | Xerox Corporation | Method of indexing words in handwritten document images using image hash tables |
US6014733A (en) * | 1997-06-05 | 2000-01-11 | Microsoft Corporation | Method and system for creating a perfect hash using an offset table |
US6047283A (en) * | 1998-02-26 | 2000-04-04 | Sap Aktiengesellschaft | Fast string searching and indexing using a search tree having a plurality of linked nodes |
US6067547A (en) * | 1997-08-12 | 2000-05-23 | Microsoft Corporation | Hash table expansion and contraction for use with internal searching |
-
1999
- 1999-09-02 US US09/389,111 patent/US6321232B1/en not_active Expired - Lifetime
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5875264A (en) * | 1993-12-03 | 1999-02-23 | Kaman Sciences Corporation | Pixel hashing image recognition system |
US5465353A (en) * | 1994-04-01 | 1995-11-07 | Ricoh Company, Ltd. | Image matching and retrieval by multi-access redundant hashing |
US5742807A (en) * | 1995-05-31 | 1998-04-21 | Xerox Corporation | Indexing system using one-way hash for document service |
US5799312A (en) * | 1996-11-26 | 1998-08-25 | International Business Machines Corporation | Three-dimensional affine-invariant hashing defined over any three-dimensional convex domain and producing uniformly-distributed hash keys |
US5802525A (en) * | 1996-11-26 | 1998-09-01 | International Business Machines Corporation | Two-dimensional affine-invariant hashing defined over any two-dimensional convex domain and producing uniformly-distributed hash keys |
US5897637A (en) * | 1997-03-07 | 1999-04-27 | Apple Computer, Inc. | System and method for rapidly identifying the existence and location of an item in a file |
US6212525B1 (en) * | 1997-03-07 | 2001-04-03 | Apple Computer, Inc. | Hash-based system and method with primary and secondary hash functions for rapidly identifying the existence and location of an item in a file |
US6014733A (en) * | 1997-06-05 | 2000-01-11 | Microsoft Corporation | Method and system for creating a perfect hash using an offset table |
US5953451A (en) | 1997-06-19 | 1999-09-14 | Xerox Corporation | Method of indexing words in handwritten document images using image hash tables |
US6067547A (en) * | 1997-08-12 | 2000-05-23 | Microsoft Corporation | Hash table expansion and contraction for use with internal searching |
US6047283A (en) * | 1998-02-26 | 2000-04-04 | Sap Aktiengesellschaft | Fast string searching and indexing using a search tree having a plurality of linked nodes |
Non-Patent Citations (6)
Title |
---|
Frank C.D. Tsai, "Geometric Hashing with Line Features", Pattern Recognition, vol. 27, No. 3, pp. 377-389, 1994. |
George Bebis, Michael Georgiopoulos and Niels Da Vitroia Lobo, "Learning Geometric Hashing Functions for Model-Based Object Recognition", in Proceedings International Conference on Computer Vision, pp. 334-339, 1990. |
Haim J. Wolfson and Yehezkel Lamdan, "Transformation Invariant Indexing", in Geometric Invariants in Computer Vision, IT Press, pp. 334-352, 1992. |
Isidore Rigoutsos and Robert Hummel, "Massively Parallel Model Matching: Geometric hashing on the connection machine", in IEEE Computer, pp. 33-41, Feb. 1992. |
W. Eric L. Grimson and Daniel P. Huttenlocher, "On Sensitivity of Geometric Hashing", in Proceedings International Conference on Computer Vision, pp. 334-339, 1990. |
Yehezkel Lamdan and Haim J. Wolfson, "Geometric Hashing: A general and Efficient Model-Based Recognition Scheme", 1/88, pp. 238-249. |
Cited By (89)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040025025A1 (en) * | 1999-10-19 | 2004-02-05 | Ramarathnam Venkatesan | System and method for hashing digital images |
US7421128B2 (en) | 1999-10-19 | 2008-09-02 | Microsoft Corporation | System and method for hashing digital images |
US20010014887A1 (en) * | 2000-02-15 | 2001-08-16 | Wong Tin Cheung | Computer automated system for management of engineering drawings |
US7058623B2 (en) * | 2000-02-15 | 2006-06-06 | Vhsoft Technologies Company Limited | Computer automated system for management of engineering drawings |
US6757686B1 (en) * | 2000-06-14 | 2004-06-29 | International Business Machines Corporation | Method and apparatus for representing database and query information using interval hash tree |
US6691126B1 (en) * | 2000-06-14 | 2004-02-10 | International Business Machines Corporation | Method and apparatus for locating multi-region objects in an image or video database |
US7634660B2 (en) | 2001-04-24 | 2009-12-15 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7636849B2 (en) | 2001-04-24 | 2009-12-22 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7707425B2 (en) | 2001-04-24 | 2010-04-27 | Microsoft Corporation | Recognizer of content of digital signals |
US7657752B2 (en) | 2001-04-24 | 2010-02-02 | Microsoft Corporation | Digital signal watermaker |
US20020196976A1 (en) * | 2001-04-24 | 2002-12-26 | Mihcak M. Kivanc | Robust recognizer of perceptually similar content |
US20020154778A1 (en) * | 2001-04-24 | 2002-10-24 | Mihcak M. Kivanc | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20020184505A1 (en) * | 2001-04-24 | 2002-12-05 | Mihcak M. Kivanc | Recognizer of audio-content in digital signals |
US7617398B2 (en) | 2001-04-24 | 2009-11-10 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20040234099A1 (en) * | 2001-04-24 | 2004-11-25 | Microsoft Corporation | Robust and stealthy video watermarking |
US20040268220A1 (en) * | 2001-04-24 | 2004-12-30 | Microsoft Corporation | Recognizer of text-based work |
US20050066176A1 (en) * | 2001-04-24 | 2005-03-24 | Microsoft Corporation | Categorizer of content in digital signals |
US20050065974A1 (en) * | 2001-04-24 | 2005-03-24 | Microsoft Corporation | Hash value computer of content of digital signals |
US20050066177A1 (en) * | 2001-04-24 | 2005-03-24 | Microsoft Corporation | Content-recognition facilitator |
US7568103B2 (en) | 2001-04-24 | 2009-07-28 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20050076229A1 (en) * | 2001-04-24 | 2005-04-07 | Microsoft Corporation | Recognizer of digital signal content |
US20050084103A1 (en) * | 2001-04-24 | 2005-04-21 | Microsoft Corporation | Recognizer of content of digital signals |
US20050086489A1 (en) * | 2001-04-24 | 2005-04-21 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US6888952B2 (en) | 2001-04-24 | 2005-05-03 | Microsoft Corporation | Robust and stealthy video watermarking |
US20050094847A1 (en) * | 2001-04-24 | 2005-05-05 | Microsoft Corporation | Robust and stealthy video watermarking into regions of successive frames |
US20050097312A1 (en) * | 2001-04-24 | 2005-05-05 | Microsoft Corporation | Recognizer of content of digital signals |
US20050108545A1 (en) * | 2001-04-24 | 2005-05-19 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20050108543A1 (en) * | 2001-04-24 | 2005-05-19 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20050108544A1 (en) * | 2001-04-24 | 2005-05-19 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7072493B2 (en) | 2001-04-24 | 2006-07-04 | Microsoft Corporation | Robust and stealthy video watermarking into regions of successive frames |
US20020172394A1 (en) * | 2001-04-24 | 2002-11-21 | Ramarathnam Venkatesan | Robust and stealthy video watermarking |
US6971013B2 (en) | 2001-04-24 | 2005-11-29 | Microsoft Corporation | Recognizer of content of digital signals |
US6973574B2 (en) | 2001-04-24 | 2005-12-06 | Microsoft Corp. | Recognizer of audio-content in digital signals |
US20050273617A1 (en) * | 2001-04-24 | 2005-12-08 | Microsoft Corporation | Robust recognizer of perceptually similar content |
US6975743B2 (en) | 2001-04-24 | 2005-12-13 | Microsoft Corporation | Robust and stealthy video watermarking into regions of successive frames |
US6996273B2 (en) * | 2001-04-24 | 2006-02-07 | Microsoft Corporation | Robust recognizer of perceptually similar content |
US7406195B2 (en) | 2001-04-24 | 2008-07-29 | Microsoft Corporation | Robust recognizer of perceptually similar content |
US20060059354A1 (en) * | 2001-04-24 | 2006-03-16 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20060059353A1 (en) * | 2001-04-24 | 2006-03-16 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20060059356A1 (en) * | 2001-04-24 | 2006-03-16 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7020777B2 (en) | 2001-04-24 | 2006-03-28 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7020775B2 (en) | 2001-04-24 | 2006-03-28 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7028189B2 (en) | 2001-04-24 | 2006-04-11 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7318157B2 (en) | 2001-04-24 | 2008-01-08 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US20020172425A1 (en) * | 2001-04-24 | 2002-11-21 | Ramarathnam Venkatesan | Recognizer of text-based work |
US7318158B2 (en) | 2001-04-24 | 2008-01-08 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7266244B2 (en) | 2001-04-24 | 2007-09-04 | Microsoft Corporation | Robust recognizer of perceptually similar content |
US7240210B2 (en) | 2001-04-24 | 2007-07-03 | Microsoft Corporation | Hash value computer of content of digital signals |
US7188249B2 (en) | 2001-04-24 | 2007-03-06 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7188065B2 (en) | 2001-04-24 | 2007-03-06 | Microsoft Corporation | Categorizer of content in digital signals |
US7152163B2 (en) | 2001-04-24 | 2006-12-19 | Microsoft Corporation | Content-recognition facilitator |
US7181622B2 (en) | 2001-04-24 | 2007-02-20 | Microsoft Corporation | Derivation and quantization of robust non-local characteristics for blind watermarking |
US7327883B2 (en) | 2002-03-11 | 2008-02-05 | Imds Software Inc. | Character recognition system and method |
US20030169925A1 (en) * | 2002-03-11 | 2003-09-11 | Jean-Pierre Polonowski | Character recognition system and method |
US7136535B2 (en) | 2002-06-28 | 2006-11-14 | Microsoft Corporation | Content recognizer via probabilistic mirror distribution |
US7095873B2 (en) | 2002-06-28 | 2006-08-22 | Microsoft Corporation | Watermarking via quantization of statistics of overlapping regions |
US20040001605A1 (en) * | 2002-06-28 | 2004-01-01 | Ramarathnam Venkatesan | Watermarking via quantization of statistics of overlapping regions |
US7006703B2 (en) | 2002-06-28 | 2006-02-28 | Microsoft Corporation | Content recognizer via probabilistic mirror distribution |
US20040005097A1 (en) * | 2002-06-28 | 2004-01-08 | Ramarathnam Venkatesan | Content recognizer via probabilistic mirror distribution |
US7069272B2 (en) | 2002-10-09 | 2006-06-27 | Blackrock Financial Management, Inc. | System and method for implementing dynamic set operations on data stored in a sorted array |
US20040073579A1 (en) * | 2002-10-09 | 2004-04-15 | Kirk Snyder | System and method for implementing dynamic set operations on data stored in a sorted array |
US20060271541A1 (en) * | 2002-10-09 | 2006-11-30 | Kirk Snyder | System and method for implementing dynamic set operations on data stored in a sorted array |
US7499945B2 (en) | 2002-10-09 | 2009-03-03 | Blackrock Financial Management, Inc. | System and method for implementing dynamic set operations on data stored in a sorted array |
US20040186916A1 (en) * | 2003-03-03 | 2004-09-23 | Bjorner Nikolaj S. | Interval vector based knowledge synchronization for resource versioning |
US7506007B2 (en) * | 2003-03-03 | 2009-03-17 | Microsoft Corporation | Interval vector based knowledge synchronization for resource versioning |
US20060079156A1 (en) * | 2003-05-02 | 2006-04-13 | Applied Materials, Inc. | Method for processing a substrate using multiple fluid distributions on a polishing surface |
US20050063545A1 (en) * | 2003-09-19 | 2005-03-24 | Ntt Docomo, Inc | Structured document signature device, structured document adaptation device and structured document verification device |
US7639818B2 (en) * | 2003-09-19 | 2009-12-29 | Ntt Docomo, Inc. | Structured document signature device, structured document adaptation device and structured document verification device |
US7831832B2 (en) | 2004-01-06 | 2010-11-09 | Microsoft Corporation | Digital goods representation based upon matrix invariances |
US20050165690A1 (en) * | 2004-01-23 | 2005-07-28 | Microsoft Corporation | Watermarking via quantization of rational statistics of regions |
US20050257060A1 (en) * | 2004-04-30 | 2005-11-17 | Microsoft Corporation | Randomized signal transforms and their applications |
US20100228809A1 (en) * | 2004-04-30 | 2010-09-09 | Microsoft Corporation | Randomized Signal Transforms and Their Applications |
US7770014B2 (en) | 2004-04-30 | 2010-08-03 | Microsoft Corporation | Randomized signal transforms and their applications |
US8595276B2 (en) | 2004-04-30 | 2013-11-26 | Microsoft Corporation | Randomized signal transforms and their applications |
US20080319987A1 (en) * | 2007-06-19 | 2008-12-25 | Daisuke Takuma | System, method and program for creating index for database |
US8190613B2 (en) * | 2007-06-19 | 2012-05-29 | International Business Machines Corporation | System, method and program for creating index for database |
CN102110122A (en) * | 2009-12-24 | 2011-06-29 | 阿里巴巴集团控股有限公司 | Method and device for establishing sample picture index table, method and device for filtering pictures and method and device for searching pictures |
WO2011078911A1 (en) * | 2009-12-24 | 2011-06-30 | Alibaba Group Holding Limited | Method and system for sample image index creation and image filtering and search |
CN102110122B (en) * | 2009-12-24 | 2013-04-03 | 阿里巴巴集团控股有限公司 | Method and device for establishing sample picture index table, method and device for filtering pictures and method and device for searching pictures |
US8577153B2 (en) | 2009-12-24 | 2013-11-05 | Alibaba Group Holding Limited | Method and system for sample image index creation and image filtering and search |
US20110264687A1 (en) * | 2010-04-23 | 2011-10-27 | Red Hat, Inc. | Concurrent linked hashed maps |
US8719307B2 (en) * | 2010-04-23 | 2014-05-06 | Red Hat, Inc. | Concurrent linked hashed maps |
US8170372B2 (en) * | 2010-08-06 | 2012-05-01 | Kennedy Michael B | System and method to find the precise location of objects of interest in digital images |
US20120221572A1 (en) * | 2011-02-24 | 2012-08-30 | Nec Laboratories America, Inc. | Contextual weighting and efficient re-ranking for vocabulary tree based image retrieval |
US8892542B2 (en) * | 2011-02-24 | 2014-11-18 | Nec Laboratories America, Inc. | Contextual weighting and efficient re-ranking for vocabulary tree based image retrieval |
US20140321765A1 (en) * | 2011-11-18 | 2014-10-30 | Nec Corporation | Feature descriptor encoding apparatus, feature descriptor encoding method, and program |
US9239850B2 (en) * | 2011-11-18 | 2016-01-19 | Nec Corporation | Feature descriptor encoding apparatus, feature descriptor encoding method, and program |
US20150339543A1 (en) * | 2014-05-22 | 2015-11-26 | Xerox Corporation | Method and apparatus for classifying machine printed text and handwritten text |
US9432671B2 (en) * | 2014-05-22 | 2016-08-30 | Xerox Corporation | Method and apparatus for classifying machine printed text and handwritten text |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US6621941B1 (en) | System of indexing a two dimensional pattern in a document drawing | |
US6321232B1 (en) | Method for creating a geometric hash tree in a document processing system | |
US5953451A (en) | Method of indexing words in handwritten document images using image hash tables | |
US6108444A (en) | Method of grouping handwritten word segments in handwritten document images | |
US6178417B1 (en) | Method and means of matching documents based on text genre | |
US8412710B2 (en) | Searching for handwritten annotations appearing a given distance from document content | |
US5774580A (en) | Document image processing method and system having function of determining body text region reading order | |
US5465353A (en) | Image matching and retrieval by multi-access redundant hashing | |
JP2973944B2 (en) | Document processing apparatus and document processing method | |
US6169998B1 (en) | Method of and a system for generating multiple-degreed database for images | |
EP2364011B1 (en) | Fine-grained visual document fingerprinting for accurate document comparison and retrieval | |
US7548916B2 (en) | Calculating image similarity using extracted data | |
CN109272440B (en) | Thumbnail generation method and system combining text and image content | |
Lu et al. | Information retrieval in document image databases | |
WO2007117334A2 (en) | Document analysis system for integration of paper records into a searchable electronic database | |
US6917708B2 (en) | Handwriting recognition by word separation into silhouette bar codes and other feature extraction | |
Duygulu et al. | A hierarchical representation of form documents for identification and retrieval | |
CN111860524A (en) | Intelligent classification device and method for digital files | |
Sari et al. | A search engine for Arabic documents | |
Dixit et al. | A survey on document image analysis and retrieval system | |
JP3917349B2 (en) | Retrieval device and method for retrieving information using character recognition result | |
Karanje et al. | Survey on text detection, segmentation and recognition from a natural scene images | |
Syeda-Mahmood | Indexing of handwritten document images | |
Marinai et al. | Exploring digital libraries with document image retrieval | |
Syeda-Mahmood | Extracting indexing keywords from image structures in engineering drawings |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: XEROX CORPORATION, CONNECTICUTFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SYEDA-MAHMOOD, TANVEER F.;REEL/FRAME:010219/0247Effective date: 19990821 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: BANK ONE, NA, AS ADMINISTRATIVE AGENT, ILLINOISFree format text: SECURITY INTEREST;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:013153/0001Effective date: 20020621 |
|
AS | Assignment |
Owner name: JPMORGAN CHASE BANK, AS COLLATERAL AGENT, TEXASFree format text: SECURITY AGREEMENT;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:015134/0476Effective date: 20030625Owner name: JPMORGAN CHASE BANK, AS COLLATERAL AGENT,TEXASFree format text: SECURITY AGREEMENT;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:015134/0476Effective date: 20030625 |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: XEROX CORPORATION, NEW YORKFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:JPMORGAN CHASE BANK, N.A.;REEL/FRAME:026958/0249Effective date: 20061204Owner name: XEROX CORPORATION, NEW YORKFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:BANK ONE, NA;REEL/FRAME:026957/0447Effective date: 20030625 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:027728/0811Effective date: 20111110 |
|
FPAY | Fee payment |
Year of fee payment: 12 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044127/0735Effective date: 20170929 |
|
AS | Assignment |
Owner name: XEROX CORPORATION, CONNECTICUTFree format text: RELEASE BY SECURED PARTY;ASSIGNOR:JPMORGAN CHASE BANK, N.A. AS SUCCESSOR-IN-INTEREST ADMINISTRATIVE AGENT AND COLLATERAL AGENT TO JPMORGAN CHASE BANK;REEL/FRAME:066728/0193Effective date: 20220822 |