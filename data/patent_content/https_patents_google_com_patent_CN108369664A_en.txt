CN108369664A - Adjust the size of neural network - Google Patents
Adjust the size of neural network Download PDFInfo
- Publication number
- CN108369664A CN108369664A CN201680070099.XA CN201680070099A CN108369664A CN 108369664 A CN108369664 A CN 108369664A CN 201680070099 A CN201680070099 A CN 201680070099A CN 108369664 A CN108369664 A CN 108369664A
- Authority
- CN
- China
- Prior art keywords
- neural net
- net layer
- size
- neural
- layer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
Abstract
Mthods, systems and devices for the size for adjusting neural net layer, it include the computer program encoded on computer storage media, this method includes obtaining the data for specifying housebroken neural network, and wherein the neural network includes one or more neural net layers；Reduce the size of one or more of neural net layer neural net layer to generate the neural network of adjusted size, including：Selection is for adjusted size of one or more neural net layers；For each selected neural net layer：Determine effective dimensionality reduction of the neural net layer；Based on identified effective dimensionality reduction, the size of the neural net layer is adjusted；And the neural network of the re -training adjusted size.
Description
Background technology
This specification is related to training neural network.
Neural network is the machine learning model exported for received input prediction using one or more layers.It removes
Except output layer, some neural networks further include one or more hidden layers.The output of each hidden layer is used as in network
Next layer of input, next hidden layer or output layer in next layer, that is, network.Each layer of the network is according to phase
The current value of parameter sets is answered to generate output from the input received.
Invention content
Generally, a novel aspects of theme described in this specification can specify housebroken god for obtaining
The method of data through network embodies, wherein the neural network includes one or more neural net layers；Reduce the god
Size through one or more of network layer neural net layer to generate the neural network of adjusted size, including：Selection is used
In adjusted size of one or more neural net layers；For each selected neural net layer：Determine the neural net layer
Effective dimensionality reduction；Based on identified effective dimensionality reduction, the size of the neural net layer is adjusted；And it is adjusted described in re -training
The neural network of size.
Other embodiments in this respect include corresponding computer system, device, and are recorded in one or more calculating
Computer program in machine storage device is both configured to execute the action of this method.A kind of one or more computer
System can be configured as by using installation on the system can to make that the system executes in operation described dynamic
Software, firmware, hardware or the combination thereof of work executes specific operation or action.One or more computer programs can be by
It is configured to execute specific operation or action by including instruction, described instruction makes described when being executed by data processing equipment
Device executes the action.
Optionally, the above and other embodiment can include one or more of following characteristics alone or in combination.
In some embodiments, the size for reducing one or more of neural net layer neural net layer includes described in reduction
The respective number of unit in each in neural net layer.
In some embodiments, determine that effective dimensionality reduction of the neural net layer includes：It is provided to the neural network multiple
Data input；The input, which is handled, through the neural net layer is generated at each neural net layer with being directed to each data input
Equivalent layer encourages；And it is determined for selected neural net layer using the network activation at selected neural net layer
Effective dimensionality reduction.
In other embodiments, it is directed to selected neural network using the layer excitation at selected neural net layer
Layer determines that effective dimensionality reduction includes：The layer is encouraged and executes principal component analysis (PCA) to be directed to the network activation generation spy
Value indicative is composed；Cut-off (cut-off) is selected for the PCA eigenvalue spectrums；And set effective dimensionality reduction to cut-off PCA
The number of characteristic value dimension.
In some cases, include for PCA eigenvalue spectrums selection cut-off：It is tired based on the PCA eigenvalue spectrums
The threshold value of variance is accumulated to select to end.
In other cases, include for PCA eigenvalue spectrums selection cut-off：Based on the flat of the PCA eigenvalue spectrums
Rate (flattening) selects cutoff level.
In some cases, include for PCA eigenvalue spectrums selection cut-off：Based on scheduled minimum PCA variances with
And previously neural net layer read the size of weight to select cutoff level.
In some embodiments, it is directed to selected neural network using the layer excitation at selected neural net layer
Layer determines that effective dimensionality reduction includes：Execute the dimensionality reduction technology for generating variance spectrum (spectrum of variances).
In some embodiments, reduce in one or more of neural net layers in the neural net layer extremely
One size includes less：It is described at least one in one or more of neural net layers in the neural net layer
Unit of the middle removal equal to the number of effective dimensionality reduction.
In other embodiments, the method further includes the neural networks of the adjusted size described in re -training
The neural network of the adjusted size is reinitialized before.
Theme described in this specification can realize to reach one in following advantages with specific embodiment
Or it is multiple.
It is a kind of realize neural network size adjustment nerve network system to the unit in one or more neural net layers
Must (integral) number optimize, therefore reduce required computing resource and reduce and the neural network
The associated calculating cost of system.For example, a kind of realizing that the nerve network system of neural network size adjustment can be to avoid selection
The needs of the excessive neural net layer of size, therefore all reduce in the training stage of neural network and reasoning stage required
Computing resource and calculating cost.
In addition, a kind of nerve network system for realizing the adjustment of neural network size can be to avoid random selection neural net layer
Size needs, therefore improve the accuracy of the nerve network system.In addition, a kind of realizing what neural network size adjusted
The examination in the space that nerve network system can need not carry out network layer size to determine optimal neural net layer size
It tests and is searched for error, such as the search carried out with artificial or programming mode, therefore avoid the demand to a large amount of computing resources.
With and compared with the bigger nerve network system of neural network size adjustment is not implemented, a kind of realization neural network size
The nerve network system of adjustment can reach model error rate that is similar or being reduced.In addition, with may require a large amount of training
Test the nerve network system of the unrealized neural network size adjustment of the correspondence optimal set to determine neural net layer size
It compares, a kind of to realize that the nerve network system of neural network size adjustment require less training experiment --- for example twice
Training experiment --- to determine the optimal set of neural net layer size.
The one or more embodiments of the detail of this specification are presented in the accompanying drawings and the description below.This theme it is other
Feature, aspect and advantage will due to the description, drawings and claims and become apparent.
Description of the drawings
Fig. 1 is the example of neural network size adjustment system.
Fig. 2 is the flow chart of the instantiation procedure of the neural network for generating adjusted size.
Fig. 3 is the flow chart of the instantiation procedure of the size for adjusting neural net layer.
Similar reference numeral and title indicate similar element in each attached drawing.
Specific implementation mode
Fig. 1 shows that exemplary neural network size adjusts system 100.It is to be implemented that neural network size, which adjusts system 100,
One or more of one or more positions that can be realized wherein for system hereinafter described, component and technology calculate
The example of the system of computer program on machine.
Neural network size adjustment system 100 is that the data of housebroken neural network 1 02 are specified in reception and generation refers to
System of the data of the neural network 1 04 of fixed adjusted size as output.
Housebroken neural network 1 02 include multiple neural net layers, for example, neural net layer A, neural net layer B and
Neural net layer C.One or more of the neural net layer can hide neural net layer.The housebroken neural network
In each layer be configured as receiving equivalent layer input, such as output that another layer is generated, to neural network input or
This two of person, and this layer input is handled to generate corresponding layer output from the input, i.e. layer encourages.Each nerve net
Network layers include the unit of respective number, which specifies the size or width of the neural net layer.In neural net layer
Each unit be configured as receiving unit input, such as equivalent layer input some or all of, and according to the input come
Generation unit exports.Corresponding layer excitation is the combination of generated unit output.
Some or all of layer of housebroken neural network is associated with relevant parameter matrix or weight matrix, described
The housebroken value of the parameter or weight of relevant parameter matrix or weight matrix storage neural net layer.For example, neural net layer
In each unit parameter correspond to the neural net layer weight matrix in corresponding line.Neural net layer is according to the nerve
The trained values of the parameter of network layer and from input generate output.For example, generating one of output as according to the input received
Point, the row of weight matrix corresponding with the unit can be inputted with it and be multiplied to generation unit output by corresponding units.One
In a little embodiments, the respective component encouraged with generation layer using excitation function can be exported to unit.
Neural network size adjustment system 100 receives the data for specifying housebroken neural network 1 02 and adjusts nerve
The size of one or more of network layer --- for example, neural net layer A, neural net layer B and neural net layer C ---,
To generate the neural net layer of corresponding adjusted size, for example, neural net layer A ', neural net layer B ' and neural net layer
C’.The neural net layer of corresponding adjusted size constitutes the neural network of adjusted size.
Neural network size adjustment system 100 can be set as by the value of the parameter of the neural network of adjusted size
Initial value reinitializes the neural network of adjusted size, for example randomly selected value of the initial value.The neural network
Size adjustment system can train the neural network of adjusted size using training example, to determine the god of adjusted size
The value of parameter is adjusted to housebroken value by the housebroken value of the parameter through network layer from initial value.For example, in training period
Between, neural network size adjustment system 100 can be handled a batch of training example, and for every in the batch
A training example generates the neural network output of corresponding adjusted size.Then, the neural network of adjusted size exports energy
It is enough used to adjust the value of the parameter of the neural network of adjusted size, such as passes through gradient decline and reverse transmittance nerve network
Training technique.
The neural network 1 04 of adjusted size includes the neural net layer of multiple adjusted sizes, such as neural net layer
A ', neural net layer B ' and neural net layer C '.The neural net layer of each adjusted size includes corresponding adjusted number
Hidden unit, which specifies the size or width of the neural net layer.The adjusted number of hidden unit is small
In or equal to hidden unit in housebroken neural network 1 02 corresponding number.For example, in housebroken neural network 1 02
Neural net layer A can be with the hidden unit of the first number, and corresponding god in the neural network 1 04 of adjusted size
The first number can be less than or equal to the hidden unit of the second number, wherein second number through network layer A '.At some
In embodiment, the number of the hidden unit in each layer in the neural network 1 04 of adjusted size can be identical.
In other embodiment, the number of the hidden unit in each layer in the neural network 1 04 of adjusted size can be become
Change.
Neural network size adjusts system 100 and generates the specified number through re -training, the neural network 1 04 of adjusted size
According to this as output.The neural network 1 04 of adjusted size can be provided with：Such as the nerve according to adjusted size
The housebroken value of the parameter of network 104 handles new neural network by the neural net layer of adjusted size and inputs with needle
The neural network output of new adjusted size is generated to the input.
Housebroken neural network 1 02 and neural network 1 04 through re -training, adjusted size can be configured as connecing
It receives any kind of numerical data input and is exported based on the input to generate any kind of score value or classification.
For example, if to housebroken neural network 1 02 and the neural network 1 04 through re -training, adjusted size
Input is image or the feature extracted from image, then housebroken neural network 1 02 and through re -training, adjusted
The neural network 1 04 of size is for the score value that the output that given image is generated can be each object type set, wherein often
A score value indicates that the image includes the estimation possibility of the image for the object for belonging to the category.
As another example, if being directed to housebroken neural network 1 02 and the god through re -training, adjusted size
Input through network 104 is the feature for the personalized recommendation of user, such as characterizes the feature of the situation of recommendation, such as is characterized
The feature for the prior actions that user is taken, then housebroken neural network 1 02 and the nerve through re -training, adjusted size
The output that network 104 is generated can be the score value for each collection of content items, wherein each score value indicates that user will be for
The recommended estimation possibility energetically made a response with the content item.
As another example, if being directed to housebroken neural network 1 02 and the god through re -training, adjusted size
Input through network 104 is a kind of text of language, then housebroken neural network 1 02 and through re -training, adjusted size
The output that is generated of neural network 1 04 can be another language each text fragments set score value, wherein each point
Value indicates that the text fragments of the other Languages are the estimation possibilities suitably translated for inputting text into the other Languages.
As another example, if to housebroken neural network 1 02 and the nerve through re -training, adjusted size
The input of network 104 be spoken utterance, the sequence of spoken utterance or from this alternative one institute derived from feature, then it is housebroken
The output that neural network 1 02 and neural network 1 04 through re -training, adjusted size are generated can be each text fragments
The score value of set, wherein each score value indicates that text segment is that the speech or the estimation of speech sequence correctly transcribed may
Property.
Fig. 2 is the flow chart of the instantiation procedure of the neural network for generating adjusted size.For convenience, process 200
It will be described as performed by one or more system for computer positioned at one or more positions.For example, according to this explanation
Book and properly programmed neural network size adjusts system --- for example, the neural network size of Fig. 1 adjusts system 100 --- energy
Enough implementation procedures 200.
The system obtains the data for specifying trained nerve net --- for example, housebroken neural network 1 02 of Fig. 1 ---
(step 202).The housebroken neural network includes one or more neural net layers, for example, the neural net layer A-C of Fig. 1.
Each neural net layer has corresponding layer size.For example, each neural net layer in housebroken neural network can wrap
Include thousands of units.In some embodiments, housebroken neural network is cycle (recurrent) neural network.At it
In its embodiment, housebroken neural network is positive feedback neural network.
The system reduces the size of one or more neural net layers to generate the neural network (204) of adjusted size.
The system by reduce in corresponding neural net layer one or more of number of unit reduce corresponding neural net layer
Size.In some embodiments, which can select the one or more neural net layers adjusted for size.Example
Such as, which can receive the specified input for wanting adjusted size of one or more layers.In other examples, which can connect
Receive the input for specifying all neural net layers that will be resized.For each selected neural net layer, which can
To determine effective dimensionality reduction for the neural net layer, and the neural net layer is adjusted based on identified effective dimensionality reduction
Size.In some embodiments, which can determine effective dimensionality reduction using principal component analysis (PCA).In other implementations
In mode, which can use other technologies, such as accidental projection from linear reconstruction or reconstructed error effective to determine
Dimensionality reduction.The size of the Stochastic Networks network layers of adjusted size will than the neural net layer size adjustment before size smaller.
For example, the example more than continuing, the neural net layer of adjusted size can have correspondingly sized N1, N2..., NL, wherein Ni
≤ N, i=1 ..., L, wherein L are the sums of neural net layer.It is more fully described below with reference to Fig. 3 and adjusts god using PCA
Size through network layer.
Neural network (the step 206) of the adjusted size of system re -training.In some embodiments, which exists
The value of the parameter of the neural network of the adjusted size is carried out again just before the neural network of the adjusted size of re -training
Beginningization, such as by specifying randomly selected value for neural network parameter.
Fig. 3 is the flow chart of the instantiation procedure of the size for adjusting the neural net layer in housebroken neural network.
For convenience, process 300 will be described as being held by being located at one or more of one or more positions system for computer
Row.For example, the neural network size properly programmed according to this specification adjusts system --- for example, the neural network of Fig. 1 is big
Small adjustment system 100 --- it is able to carry out process 300.
The input of multiple data is provided to housebroken neural network, such as the housebroken neural network of Fig. 1 by the system
102 (steps 302).For example, multiple data input can be the data input of big batch.
The system handles multiple data by the neural net layer of the housebroken neural network and inputs to be directed to this batch
The equivalent layer that each data in secondary data input input to generate each neural net layer encourages (step 304).For example, needle
To being defined as the data input matrix X of dimension U × BUBData input, the i-th neural net layer in the input data
Layer excitation can be defined as with dimension N × B's
Principal component analysis (PCA) is executed to the layer excitation of selected neural net layer and is generated for the neural net layer
Eigenvalue spectrum (step 306).Then,PCA can be by passing throughThe spy of defined correlation matrix C
Value indicative and feature vector define, wherein feature vector VNNIt is arranged as arranging and eigenvalue spectrum { λ1..., λNThen from maximum to
Minimum arrangement.
The system is PCA eigenvalue spectrums selection cut-off (step 308).Selected by PCA eigenvalue spectrums
The PCA dimension tables of cut-off show the minimum PCA dimensions of serviceability.For example, make the weight of the i-th neural net layer given by W and
It enablesThe estimation of the serviceability of PCA dimensions can be byIt is given, whereinIt is that W is thrown to the jth of V
The norm of shadow.For example, the system can be by numberIt is arranged basic minimum threshold to be selected for PCA eigenvalue spectrums
Cut-off, therefore generate compared with the network trained in the case where all layers both are set to N on the one of network influence minimum
Series layer size.
In some embodiments, which is cut based on the ellipticity of PCA eigenvalue spectrums for the selection of PCA eigenvalue spectrums is arbitrary
Sealing is flat.For example, eigenvalue spectrum can be modeled as the exponential damping with one or more time constants, and eigenvalue spectrum
It can be flat to occur with index speed in tail end.Since tail end may indicate the portion that serviceability may be minimum in state space
Point, so the system can select appropriate cutoff level based on the ellipticity of PCA eigenvalue spectrums.
In other embodiments, the system based on the threshold value of the cumulative variance of PCA eigenvalue spectrums come for PCA eigenvalue spectrums
Select arbitrary cutoff level, the 99% of the threshold value such as cumulative variance.
In other embodiments, which is based on arbitrary minimum PCA variances and previous neural net layer reads power
The size of weight to select cutoff level for PCA eigenvalue spectrums.For example, if the variance of PCA dimensions is small, and previous network layer
It is given size to read weight, then the dimension for including the variance less than the threshold value may not be just useful.
The system encourages to determine effective dimensionality reduction (310) of the neural net layer using the layer at neural net layer.For example,
As above with reference to described in step 308, which can set effective dimensionality reduction of neural net layer to cut-off PCA characteristic values
The number of dimension.
The system reduces the size (step 312) of neural net layer based on effective dimensionality reduction.For example, the system can pass through
The number of layer dimension is set to reduce identified effective dimensionality reduction in step 310 --- it is equal to effectively by removing in neural net layer
The unit of the number of dimensionality reduction, to reduce the size of neural net layer.The size of the neural network of adjusted size --- for example through
Number of unit in adjusted size of neural net layer --- is less than size of the neural net layer before size adjustment ---
Such as the number of unit in the neural net layer.
In some embodiments, such as by being held repeatedly for each neural net layer in housebroken neural network
Row step 306-312, each neural net layer that can be directed in housebroken neural network execute size and adjust process 300.
In other embodiments, size adjustment process can be executed for single Neural layer, and for one or more
Additional neural network layer repeats step 312, i.e., identified identical effective dimensionality reduction can be applied to through instruction in the step 310
One or more of experienced neural network additional neural network layer.
In other embodiments, the valid dimension for the layer excitation generated in measuring process 304 can be used
Alternative technology adjusts process to execute the size.For example, the replacement as PCA, can use the other of same generation variance spectrum
Dimensionality reduction technology determines effective dimensionality reduction.
The embodiment of theme and feature operation described in this specification can be realized with Fundamental Digital Circuit, to have
The existing computer software of body or firmware are realized, in terms of including structure and its equivalent structures disclosed in this specification
Calculation machine hardware realizes, or is realized with combination one or more in them.The implementation of theme disclosed in this specification
Example can be implemented as one or more computer programs, i.e., encoded for by counting on tangible non-transitory memory carrier
One or more computer program instructions modules of its operation are executed or controlled according to processing unit.Alternatively or additionally, should
Program instruction can be encoded on manually generated transmitting signal, such as electricity, optics or the electromagnetic signal that machine generates,
It is generated to be encoded information for being transmitted to appropriate acceptor device and is executed by data processing equipment.Computer
Storage medium can be computer readable storage devices, computer-readable storage substrate, random or serial access memory equipment,
Or one or more combination in them.However, computer storage media is not transmitting signal.
Term " data processing equipment " includes device, equipment and the machine of all kinds for handling data, such as wraps
Include programmable processor, computer or multiple processors or computer.The device may include dedicated logic circuit, such as
FPGA (field programmable gate array) or ASIC (application-specific integrated circuit).In addition to hardware, which can also include for institute
The computer program of discussion creates the code of performing environment, for example, constitute processor firmware, protocol stack, data base management system,
The code of the combination of operating system or one or more of which.
Computer program (is also referred to as or is described as program, software, software application, module, software module, script or generation
Code) can be in any form programming language write, including compiling or interpretative code, or statement or procedural language, and
And it can be disposed in any form, include as stand-alone program or as the mould suitable for using in a computing environment
Block, component, subprogram or other units.Computer program can correspond to the file in file system, but not necessarily such as
This.Program can be stored in the other programs for preserving the one or more scripts stored in such as marking language document or
It in a part for the file of data, is stored in the single file for being exclusively used in discussed program, or is stored in multiple collaboration texts
In part, such as store the file of one or more modules, subprogram or code section.Computer program can be deployed as one
It is executed in platform computer or multiple stage computers, the multiple stage computers are located at one place or are distributed and lead to across multiple places
Communication network is crossed to be interconnected.
As used in this description, " engine " or " software engine " refers to the offer of software realization different from input
The input/output of output.Engine can be the functional block of coding, such as library, platform, software development kit (SDK) or right
As.Each engine can realize on the computing device of any appropriate type, for example, server, mobile phone, tablet computer,
Notebook computer, music player, E-book reader, above-knee or desktop computer, PDA, smart phone, or including one
The other fixed or portable device of a or multiple processors and computer-readable medium.In addition, two or more engines
It can be realized in identical calculations equipment or different computing devices.
Process and logic flow described in this specification can realize by one or more programmable calculators, institute
It states programmable calculator and executes one or more computer programs with by the way that output is operated and generated to input data come real
Row function.The processing and logic flow can also be by such as FPGA (field programmable gate array) or ASIC (special integrated electricity
Road) dedicated logic circuit practiced by, and device can also be implemented as the dedicated logic circuit.
The processor for being adapted for carrying out computer program includes --- can for example be based on --- general and dedicated microprocessor
Device or both of which and any other kind of central processing units.In general, central processing unit will from read-only memory or with
Machine accesses memory or the two receives instruction and data.The necessary component of computer is for being practiced or carried out in instruction
Central Processing Unit and for storing instruction with one or more memory devices of data.In general, computer will also include using
In storage data one or more large storage facilities or be operatively coupled to the large storage facilities to receive from it and
Data or the two, the large storage facilities such as disk, magneto-optic disk or CD are transmitted to it.However, computer is not necessarily to
With such equipment.In addition, computer can be embedded among another equipment, several examples are only enumerated, the equipment is for example
Mobile phone, personal digital assistant (PDA), Mobile audio frequency or video player, game console, global positioning system (GPS)
Receiver or portable memory apparatus, such as universal serial bus (USB) flash drive.
Computer-readable medium suitable for storing computer program instructions and data includes that the non-volatile of form of ownership is deposited
Reservoir, medium and memory devices, such as including semiconductor memory devices, such as EPROM, EEPROM and flash memory device；Magnetic
Disk, such as internal hard drive or removable disk；Magneto-optic disk；And CD ROM and DVD-ROM disks.Processor and memory can be mended
It is filled with dedicated logic circuit or is incorporated into wherein.
In order to provide the interaction with user, the embodiment of theme described in this specification can with display equipment with
And it is realized on keyboard and the computer of indicating equipment, the display equipment such as CRT (cathode-ray tube) or LCD (liquid crystal displays
Device) monitor, for showing information to user, and the indicating equipment such as user can be defeated to computer offer by it
The mouse or trace ball entered.The interaction with user can also be provided using the equipment of other types；Such as it is supplied to user's
Feedback can be any type of sensory feedback, such as visual feedback, audio feedback or touch feedback；And it is from the user defeated
Enter to be received in any form, including sound, voice or sense of touch.In addition, computer can be by being made to user
Equipment sends document and receives document from the equipment to be interacted with user；For example, by response to from user
Client device on web browser receive request and to the web browser send webpage.
The embodiment of theme described by the specification can realize that the computing system includes rear end group in computing systems
Part --- such as data server, either including middleware component --- such as application server or including front group
Part --- it can for example be used by the figure that the embodiment of itself and the theme described in the specification interacts with user
The client computer or the computing system of family interface or Web browser include rear end, centre as one or more
Any combinations of part or front end assemblies.The component of the system can be for example, by any form of communication network or the number of medium
Digital data communication is interconnected.The example of communication network includes LAN (LAN) and wide area network (WAN), such as internet.
Computing system may include client and server.Client and server is generally remote from each other and typically leads to
Communication network is crossed to interact.The relationship of client and server by running and having client each other on the respective computers
The computer program of end-relationship server is established.In some embodiments, for example, for the user that serves as client
User's display data of equipment interaction and purpose input by user is received from the user, server transmits example to user equipment
Such as the data of html web page.The knot for example interacted as user at the user equipment can be received from user equipment in server
Fruit and the data generated.
Although this specification includes many specific implementation mode details, these should not be understood to any hair
The range of content bright and claimed is limited, but as may be specific to the specific embodiment of specific invention
The description of feature.The certain features being described as context (context) using separate embodiments in this specification also can be in list
It is realized in combination in a embodiment.On the contrary, each feature being described as context using single embodiment also can be single
It is realized solely or in various embodiments with any appropriate sub-portfolio.In addition, although feature can hereinbefore be described as with certain
Kind combination carry out work and it is even initially claimed in this way, still existing from the one or more features of combination claimed
The combination can be detached under some cases, and combination claimed can be directed to the variation shape of sub-portfolio or sub-portfolio
Formula.
Similarly, although operation is described in figure with particular order, this should not be understood to require this
The operation of sample with shown particular order or with consecutive order come execute or all illustrated operations will be performed with
Realize desired result.In some cases, it may be advantageous for multitask and parallel processing.In addition, reality described above
Applying the separation of each system module and component in example should not be understood to require such point in all embodiments
From, and it should be understood that described program assembly and system usually integrated in single software product or
It is packaged into multiple software product.
The specific embodiment of the theme is described.Other embodiments are in the scope of the appended claims
Within.For example, action recorded in claims can be executed with different order and still be realized desired knot
Fruit.As an example, discribed process not necessarily requires shown particular order or consecutive order with reality in attached drawing
Existing desired result.In some embodiments, it may be advantageous for multitask and parallel processing.
Claims (12)
1. a kind of method, including：
The data for specifying housebroken neural network are obtained, wherein the neural network includes one or more neural net layers；
Reduce the size of one or more of neural net layer neural net layer to generate the nerve net of adjusted size
Network, including：
Selection is for adjusted size of one or more neural net layers；
For each selected neural net layer：
Determine effective dimensionality reduction of the neural net layer, and
Based on identified effective dimensionality reduction, the size of the neural net layer is adjusted；And
The neural network of adjusted size described in re -training.
2. according to the method described in claim 1, wherein, reducing one or more of neural net layer neural net layer
Size include：Reduce the respective counts of the unit in each neural net layer in one or more of neural net layers
Mesh.
3. method according to claim 1 or 2, wherein determine that effective dimensionality reduction of the neural net layer includes：
Multiple data inputs are provided to the neural network；
The input, which is handled, through the neural net layer is generated accordingly at each neural net layer with being directed to each data input
Layer excitation；And
Effective dimensionality reduction is determined for selected neural net layer using the network activation at selected neural net layer.
4. according to the method described in claim 3, wherein, being directed to using the layer excitation at selected neural net layer selected
Neural net layer include to determine effective dimensionality reduction：
The layer is encouraged and executes principal component analysis (PCA) to be directed to the network activation generation eigenvalue spectrum；
It selects to end for the PCA eigenvalue spectrums；And
Set effective dimensionality reduction to the number of cut-off PCA characteristic value dimensions.
5. according to the method described in claim 4, wherein, including for PCA eigenvalue spectrums selection cut-off：Based on described
The threshold value of the cumulative variance of PCA eigenvalue spectrums selects to end.
6. according to the method described in claim 4, wherein, including for PCA eigenvalue spectrums selection cut-off：Based on described
The ellipticities of PCA eigenvalue spectrums selects cutoff level.
7. according to the method described in claim 4, wherein, including for PCA eigenvalue spectrums selection cut-off：Based on scheduled
Minimum PCA variances and previous neural net layer read the size of weight to select cutoff level.
8. according to the method described in claim 3, wherein, being directed to using the layer excitation at selected neural net layer selected
Neural net layer include to determine effective dimensionality reduction：Execute the dimensionality reduction technology for generating variance spectrum.
9. the method according to any one of claim 3 to 8, wherein reduce described one in the neural net layer
The size of at least one of a or multiple neural net layers neural net layer includes：Described one in the neural net layer
List of the removal equal to the number of effective dimensionality reduction at least one neural net layer in a or multiple neural net layers
Member.
10. according to the method described in any one of aforementioned claim, further comprise：The adjusted size described in re -training
The neural network of the adjusted size is reinitialized before neural network.
11. a kind of system includes the storage device of one or more computers and one or more store instructions, described instruction
It is operable to when being executed by one or more of computers so that one or more of computers execute.
12. a kind of computer program product of coding on one or more non-transitory storage medias, the computer program production
Product include instruction, and described instruction makes one or more of computers execute basis when being executed by one or more computers
Method described in any one of claims 1 to 10.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/954,683 | 2015-11-30 | ||
US14/954,683 US20170154262A1 (en) | 2015-11-30 | 2015-11-30 | Resizing neural networks |
PCT/US2016/062942 WO2017095667A1 (en) | 2015-11-30 | 2016-11-18 | Resizing neural networks |
Publications (1)
Publication Number | Publication Date |
---|---|
CN108369664A true CN108369664A (en) | 2018-08-03 |
Family
ID=57544526
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680070099.XA Pending CN108369664A (en) | 2015-11-30 | 2016-11-18 | Adjust the size of neural network |
Country Status (4)
Country | Link |
---|---|
US (1) | US20170154262A1 (en) |
EP (1) | EP3369046A1 (en) |
CN (1) | CN108369664A (en) |
WO (1) | WO2017095667A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111967570A (en) * | 2019-07-01 | 2020-11-20 | 嘉兴砥脊科技有限公司 | Implementation method, device and machine equipment of mysterious neural network system |
CN112639833A (en) * | 2018-08-30 | 2021-04-09 | 皇家飞利浦有限公司 | Adaptable neural network |
Families Citing this family (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10656962B2 (en) * | 2016-10-21 | 2020-05-19 | International Business Machines Corporation | Accelerate deep neural network in an FPGA |
US10121103B2 (en) * | 2016-12-09 | 2018-11-06 | Cisco Technologies, Inc. | Scalable deep learning video analytics |
US10019668B1 (en) | 2017-05-19 | 2018-07-10 | Google Llc | Scheduling neural network processing |
CN107748809B (en) * | 2017-09-20 | 2021-01-26 | 苏州芯智瑞微电子有限公司 | Semiconductor device modeling method based on neural network technology |
KR102610820B1 (en) * | 2017-09-27 | 2023-12-06 | 삼성전자주식회사 | Neural network system, and Operating method of neural network system |
US10713563B2 (en) * | 2017-11-27 | 2020-07-14 | Technische Universiteit Eindhoven | Object recognition using a convolutional neural network trained by principal component analysis and repeated spectral clustering |
US11734567B2 (en) * | 2018-02-13 | 2023-08-22 | Samsung Electronics Co., Ltd. | Method and system for reducing deep neural network architectures |
US11341034B2 (en) | 2018-08-06 | 2022-05-24 | International Business Machines Corporation | Analysis of verification parameters for training reduction |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO1991002315A1 (en) * | 1989-08-01 | 1991-02-21 | E.I. Du Pont De Nemours And Company | Methods relating to the configuration of a parallel distributed processing network |
-
2015
- 2015-11-30 US US14/954,683 patent/US20170154262A1/en not_active Abandoned
-
2016
- 2016-11-18 EP EP16810532.8A patent/EP3369046A1/en not_active Withdrawn
- 2016-11-18 WO PCT/US2016/062942 patent/WO2017095667A1/en active Application Filing
- 2016-11-18 CN CN201680070099.XA patent/CN108369664A/en active Pending
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO1991002315A1 (en) * | 1989-08-01 | 1991-02-21 | E.I. Du Pont De Nemours And Company | Methods relating to the configuration of a parallel distributed processing network |
Non-Patent Citations (3)
Title |
---|
ASRIEL U LEVIN ET AL: "Fast Pruning Using Principal Components", 《NIPS》 * |
HE TIANXING ET AL: "reshaping deep neural network for fast decoding by node-pruning", 《2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS SPEECH AND SIGNAL PROCESSING》 * |
XIE ET AL.: "Linear Pruning Techniques for Neural Networks一Based on Projection Latent Structuren", 《IEEE SMC CONFERENCE》 * |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112639833A (en) * | 2018-08-30 | 2021-04-09 | 皇家飞利浦有限公司 | Adaptable neural network |
CN111967570A (en) * | 2019-07-01 | 2020-11-20 | 嘉兴砥脊科技有限公司 | Implementation method, device and machine equipment of mysterious neural network system |
CN111967570B (en) * | 2019-07-01 | 2024-04-05 | 北京砥脊科技有限公司 | Implementation method, device and machine equipment of visual neural network system |
Also Published As
Publication number | Publication date |
---|---|
EP3369046A1 (en) | 2018-09-05 |
WO2017095667A1 (en) | 2017-06-08 |
US20170154262A1 (en) | 2017-06-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108369664A (en) | Adjust the size of neural network | |
JP6758406B2 (en) | Wide and deep machine learning model | |
JP6790286B2 (en) | Device placement optimization using reinforcement learning | |
US11954597B2 (en) | Using embedding functions with a deep network | |
US11900232B2 (en) | Training distilled machine learning models | |
JP7440420B2 (en) | Application development platform and software development kit offering comprehensive machine learning services | |
US20220004879A1 (en) | Regularized neural network architecture search | |
CN110520871A (en) | Training machine learning model | |
US11790233B2 (en) | Generating larger neural networks | |
US9449283B1 (en) | Selecting a training strategy for training a machine learning model | |
US9454733B1 (en) | Training a machine learning model | |
CN109564575A (en) | Classified using machine learning model to image | |
CN108140143A (en) | Regularization machine learning model | |
US20200090043A1 (en) | Generating output data items using template data items | |
CN109923560A (en) | Neural network is trained using variation information bottleneck | |
US11797839B2 (en) | Training neural networks using priority queues | |
US11488067B2 (en) | Training machine learning models using teacher annealing | |
JP2019517075A (en) | Categorizing Example Inputs Using Comparison Sets | |
WO2018201151A1 (en) | Neural network optimizer search | |
US20190251419A1 (en) | Low-pass recurrent neural network systems with memory | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
US10755171B1 (en) | Hiding and detecting information using neural networks | |
US20240013769A1 (en) | Vocabulary selection for text processing tasks using power indices | |
CN109685091A (en) | It is determined using the number experience target of Bayes's mode | |
CN110347800B (en) | Text processing method and device, electronic equipment and readable storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
WD01 | Invention patent application deemed withdrawn after publication | ||
WD01 | Invention patent application deemed withdrawn after publication |
Application publication date: 20180803 |