EP2864980A1 - Mixed model speech recognition - Google Patents
Mixed model speech recognitionInfo
- Publication number
- EP2864980A1 EP2864980A1 EP13734936.1A EP13734936A EP2864980A1 EP 2864980 A1 EP2864980 A1 EP 2864980A1 EP 13734936 A EP13734936 A EP 13734936A EP 2864980 A1 EP2864980 A1 EP 2864980A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- transcription
- user
- speech recognizer
- language model
- computing device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/32—Multiple recognisers used in sequence or in parallel; Score combination systems therefor, e.g. voting systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/193—Formal grammars, e.g. finite state automata, context free grammars or word networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
Definitions
- This specification relates to speech recognition.
- a user of a mobile device may enter text by, for example, typing on a keyboard or speaking into a microphone.
- an Automated Search Recognition (ASR) engine may be used to process input in the form of speech.
- one innovative aspect of the subject matter described in this specification can be implemented in methods that include a computer-implemented method for providing sound transcription.
- the method comprises accessing audio data generated by a computing device based on audio input from a user, the audio data encoding one or more user utterances.
- the method further comprises generating a first transcription of the utterances by performing speech recognition on the audio data using a first speech recognizer, wherein the first speech recognizer employs a language model that is based on user-specific data.
- the method further comprises generating a second transcription of the utterances by performing speech recognition on the audio data using a second speech recognizer, wherein the second speech recognizer employs a language model independent of user-specific data.
- the method further comprises determining that the second transcription of the utterances includes a term from a predefined set of one or more terms.
- the method further comprises, based on determining that the second transcription of the utterance includes the term from the predefined set of one or more terms, providing an output of the first transcription of the utterance.
- the set of one or more terms can be associated with one or more actions to be performed by the computing device.
- the first speech recognizer can employ a grammar-based language model.
- the grammar-based language model can include a context free grammar.
- the second speech recognizer can employ a statistics-based language model.
- the user-specific data can include a contact list for the user, an applications list of applications installed on the computing device, or a media list of media stored on the computing device.
- the first speech recognizer can be implemented on the computing device and the second speech recognizer is implemented on one or more server devices.
- the system comprises one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising: accessing audio data generated by a computing device based on audio input from a user, the audio data encoding one or more user utterances; generating a first transcription of the utterances by performing speech recognition on the audio data using a first speech recognizer, wherein the first speech recognizer employs a language model that is developed based on user-specific data; generating a second transcription of the utterances by performing speech recognition on the audio data using a second speech recognizer, wherein the second speech recognizer employs a language model developed independent of user-specific data; determining that the second transcription of the utterances includes a term from a predefined set of one or more terms; and, based on determining that the second transcription of the utterance
- the set of one or more terms can be associated with one or more actions to be performed by the computing device.
- the first speech recognizer can employ a grammar-based language model.
- the grammar-based language model can include a context free grammar.
- the second speech recognizer can employ a statistics-based language model.
- the user-specific data can include a contact list for the user, an applications list of applications installed on the computing device, or a media list of media stored on the computing device.
- the first speech recognizer can be implemented on the computing device and the second speech recognizer can be implemented on one or more server devices.
- FIG. 8 In general, another innovative aspect of the subject matter described in this specification can be implemented in computer-readable medium that include a computer-readable medium storing software comprising instructions executable by one or more computers which, upon such execution, cause the one or more computers to perform operations.
- the operations comprise accessing audio data generated by a computing device based on audio input from a user, the audio data encoding one or more user utterances; determining a first transcription of the utterances by performing speech recognition on the audio data using a first speech recognizer, wherein the first speech recognizer employs a language model that is developed based on user-specific data; determining a second transcription of the utterances by performing speech recognition on the audio data using a second speech recognizer, wherein the second speech recognizer employs a language model developed independent of user-specific data; determining that the second transcription of the utterances includes a term from a predefined set of one or more terms; and, based on determining that the second transcription of the utterance includes the term from the predefined
- the set of one or more terms can be associated with one or more actions to be performed by the computing device.
- the first speech recognizer can employ a grammar-based language model.
- the second speech recognizer can employ a statistics-based language model.
- the user-specific data can include a contact list for the user, an applications list of applications installed on the computing device, or a media list of media stored on the computing device.
- the first speech recognizer can be implemented on the computing device and the second speech recognizer is implemented on one or more server devices.
- FIG. 1 is a diagram of an example of a system that employs the different capabilities of two speech recognizers.
- FIG. 2 is a flowchart showing an example of a process for performing speech recognition.
- FIG. 3 is a flowchart showing an example of another process for performing speech recognition.
- FIG. 4 is a swim lane diagram showing an example of communications and operations while performing speech recognition.
- FIG. 5 is a diagram showing an example of a screen shot.
- FIG. 6 shows an example of a generic computing device and a generic mobile computing device.
- FIG. 1 is a diagram of an example of a system 100 that employs the different capabilities of two speech recognizers to, for example, enhance speech recognition accuracy.
- FIG. 1 also illustrates a flow of data within the system 100 during states (a) to (i), as well as a user interface 101 that is displayed on a mobile device 102 of the system 100 during state (i).
- the system 100 processes user utterances on both a client-side and a server-side speech recognizer, which may help to resolve ambiguities caused by user-specific words or names in the utterances.
- the client-side speech recognizer may use a limited language model that is developed using user specific data, such as the contact names in the user's contact list, while the server-side speech recognizer uses a large vocabulary language model developed independently of such user-specific data.
- comparing or combining the results of both speech recognizers may be used to increase accuracy when the utterances involve user-specific terms.
- the system 100 includes the mobile device 102, which is in communication with a server 104 and an ASR engine 105 over one or more networks 106.
- ASR engine 105 may be implemented on server 104 or on a separate computing device and in communication with server 104 and mobile device 102 over the one or more networks 106.
- the server 104 may be a search engine, a dictation engine, a dialogue system, or any other engine or system that uses transcribed speech or that invokes a software application that uses transcribed speech, to perform some action. In general, the following description uses the example of a search engine for server 104.
- the networks 106 may include a wireless cellular network, a wireless local area network (WLAN) or Wi-Fi network, a Third Generation (3G) or Fourth Generation (4G) mobile telecommunications network, a private network such as an intranet, a public network such as the Internet, or any combination thereof.
- WLAN wireless local area network
- Wi-Fi Wireless Fidelity
- 4G Fourth Generation
- the states (a) through (i) depict a flow of data that occurs when an example process is performed by the system 100.
- the states (a) to (i) may be time-sequenced states, or they may occur in a sequence that is different than the illustrated sequence.
- the mobile device 102 may be, for example, a cellular telephone, a smartphone, a tablet computer, or a personal digital assistant (PDA).
- the mobile device 102 implements a local ASR engine 1 14 including the client-based speech recognizer 126.
- the client-based speech recognizer 126 uses user- specific data 128 saved on the mobile device 102 to develop its language model 127.
- User-specific data 128 may include, for example, contact lists, phone numbers, addresses, applications, digital photographs, audio media, or video media. The following description uses a contact list as an example of the user-specific data 128.
- the client-based speech recognizer 126 may be able to more accurately distinguish unique words and names describing the data such as a friend's name, "Alex
- the language model may be a context-free grammar that can support simple voice actions and incorporate the user-specific data pertaining to a particular action.
- ASR engine 105 incorporates a server-based speech recognizer 124 that uses language model 125.
- Language model 125 may be a large vocabulary statistical language model capable of transcribing complex user dictations, and may be designed to handle transcriptions over a large number of users. However, in some situations, the language model 124 may not have access to the user-specific data and therefore may not perform speech recognition related to such data as well as the ASR engine 1 14. The language model 124 may not have access to user- specific data, for example, because of either privacy concerns or data and bandwidth limitation of the system when storing data associated with millions of users.
- the server-based speech recognizer 124 may generate one or more candidate transcriptions that match the utterance encoded in the audio data 1 12, and speech recognition confidence values for the candidate transcriptions. The transcription with the highest confidence value may be selected as the ASR engine 105 transcription. Other techniques may be used to select which candidate transcription to use as the server transcription.
- ASR engine 105 may incorporate both ASR engine 105 and ASR engine 1 14 on the same computing device.
- a computing device may be one or more servers, a desktop computer, a laptop computer, a tablet computer, a mobile telephone, or a smart phone.
- Some implementations may combine the ASR engines 105 and 1 14 into a single ASR engine having both a large statistical language model and a context-free grammar incorporating user-specific data 128. Further, speech recognition process by each of the described ASR engines may be performed simultaneously, at substantially the same time, or in a parallel manner.
- the system 100 employs speech recognition to determine whether speech input is a voice command or a voice search query, and then act accordingly.
- a voice command may be considered a user utterance that is received by a speech recognition device and causes the device to perform a task.
- the voice command may be recognized by the inclusion, in the utterance, of a word or phrase describing an action that is performable by the device, for example "call.”
- the voice action may be described by multiple variations of such action.
- commands may be to play, pause, or stop audio or video media; open or close an application; send, open, delete or save and e-mail or SMS, get a map, directions or otherwise navigate to a location; open, zoom, save, or capture a digital photograph; set, stop, or start an alarm or timer; or to set or schedule a reminder or a task.
- a voice search query may be considered a user utterance that is received by a speech recognition device and causes the device to perform a search based on a transcription of the utterance.
- the search may be a web search, an address search, a telephone directory search, a map search, or any other type of search.
- Such an utterance may be distinguished from a voice command by the lack of a word or phrase relating to an action performable by a mobile device or by the inclusion of certain words or phrases indicative of a search query, for example "search...
- a “search query” includes one or more query terms that a user submits to a search engine when the user requests the search engine to execute a search query, where a "term” or a “query term” includes one or more whole or partial words, characters, or strings of characters.
- a "result” (or a "search result") of the search query includes a Uniform Resource Identifier (URI) that references a resource that the search engine determines to be responsive to the search query.
- URI Uniform Resource Identifier
- the search result may include other things, such as a title, preview image, user rating, map or directions, description of the corresponding resource, or a snippet of text that has been automatically or manually extracted from, or otherwise associated with, the corresponding resource.
- the system 100 determines transcriptions of the audio data 1 13 using both ASR engine 105 and ASR engine 1 14 and those transcriptions are examined to determine whether to perform a voice command or a voice search. For example, in one case, the transcriptions are examined to determine, for example, that the transcription 1 15a received from ASR engine 105 represents a search query and transcription 1 18 from ASR engine 1 14 represents an action. Transcription 1 15a is then further examined to determine that the text of the search query contains a word or phrase contained in a pre-determined set of terms indicating an action
- Transcription 1 15a is thereby used to confirm the action of transcription 1 18.
- the system 100 may then prefer transcription 1 18 based on the access that language model 127 has to user-specific data 128 required to perform the action.
- the system 100 then either requests user confirmation of the received transcriptions, request confirmation of the action before initiating
- the example described relates to the performance of speech recognition for voice commands and search queries, however, the system 100 may be used to implement speech recognition for voice commands and dictations or voice commands and any other non-command transcriptions.
- FIG. 1 shows a more particular example of the operation of system 100.
- a user 1 10 speaks an utterance 1 12 into the mobile device 102.
- the mobile device 102 receives the utterance 1 12 and records it as audio data 1 13.
- ASR engine 1 14 generates a first transcription 1 18 of the audio data 1 13.
- the mobile device 102 communicates the audio data 1 13 to the ASR engine 105 where a second transcription 1 15a is generated.
- the second transcription 1 15a is communicated to the mobile device 102 where it is compared with the first transcription to determine the proper result.
- a user 1 10 speaks one or more utterances 1 12 to the mobile device 102, which records the utterances 1 12 and generates corresponding audio data 1 13.
- the user 1 10 may wish to call a friend (for example, Alex Gruenstein), who's telephone number is stored in the user-specific data 128 on the user's 1 10 mobile device 102.
- the user-specific data 128 may contain, for example, the user's contacts, applications, and various forms of audio or video media.
- the user 1 10 speaks "call Alex Gruenstein" into his mobile device 102 it records the utterance 1 12 and generates the corresponding audio data 1 13.
- the audio data 1 13 may also include a snippet of environmental audio, such as a two second snippet of audio that was recorded before or after the utterance 1 12 was spoken. While the utterance 1 12 is described as illustrated in FIG. 1 as a voice command, in other example implementations the utterance 1 12 may be a voice input to a search query, to a dictation system, or to a dialog system.
- the audio data 1 13 is provided to the ASR engine 1 14 on the mobile device 102 to be processed. Additionally, during state (c), the mobile device 102 communicates the audio data 1 13 to ASR engine 105 over the networks 106 to be processed.
- the client-based speech recognizer 126 of the client-based ASR engine 1 14 performs speech recognition on the audio data 1 13 using language model 127 to determine a client transcription of the audio data 1 13.
- the first transcription 1 18 is an action ("call Alex Gruenstein") to be carried out by the mobile device 102.
- the language model 127 may be a context-free grammar that can support simple voice actions including variations on the voice actions and incorporates user-specific data 128.
- variations on the action "call” incorporating user data may be “call Alex Gruenstein,” “call Alex Gruenstein at home,” or “please dial 555-321 -4567.”
- Certain terms in the voice actions and variations may be considered a set of predefined terms, recognizable by the client-based speech recognizer 126.
- the server-based speech recognizer 124 performs speech recognition on the audio data 1 13 using language model 125 to determine a second transcription 1 15a of the audio data 1 13.
- the language model 125 may be a large vocabulary statistical language model capable of transcribing complex user dictations.
- the ASR engine 105 may classify the utterance 1 12 as either an action to be performed by the mobile device 102 or a text string to be used as a search query or as a transcribed dictation. After performing the second transcription 1 15a, ASR engine 124 makes the preliminary determination as to whether the produced second transcription 1 15a is an action or a search query. The determination may result in one of three possible classifications.
- the ASR engine 105 may determine that the second transcription 1 15a does contain an action performable by the mobile device 102 and is therefore a voice command and would be communicated to the mobile device 102 as such. Second, the ASR engine 105 may determine that the second transcription 1 15a does not contain an action and is therefore a search query. If this is the case the ASR engine may communicate either the second transcription 1 15a along with search results or only search results to the mobile device 102. Third, the ASR may be unable to determine whether the second transcription is an action or a search query and may communicate either the second transcription 1 15a alone or the second transcription 1 15a and search results to the mobile device 102.
- the classification discussed above may be made using confidence values associated with the produced transcription in association with threshold values for the set of decisions to be made by the ASR engine 105.
- the second transcription 1 15a (“call another pine") generated by the server-based speech recognizer 124 is communicated from the ASR engine 105 to the mobile device 102.
- the server-based speech recognizer 124 has mistakenly transcribed the audio data 1 13 as "call another pine.”
- ASR engine 105 can neither distinguish the second transcription 1 15a as a query nor as an action. Therefore, it is communicated to the mobile device 102 as a text string to be displayed with, for example, search results.
- second transcription 1 15b (“call another pine") generated by the server-based speech recognizer 124 is communicated from the ASR engine 105 to the server 104.
- the candidate transcriptions 1 15 may be used as a search query term by the search engine to execute one or more search queries.
- the ASR engine 105 may provide multiple candidate transcriptions 1 15b and rank the candidate transcriptions 1 15b based at least on their respective speech recognition confidence scores before transmission to the server 104. By transcribing spoken utterances and providing candidate transcriptions to the server 104, the ASR engine 105 may provide a voice search query capability, a dictation capability, or a dialogue system capability to the mobile device 102.
- the server 104 may execute one or more search queries using the candidate query terms, and may generate a file 1 16 that references search result 131 .
- the file 1 16 may be a markup language file, such as an extensible Markup Language (XML) or HyperText Markup Language (HTML) file.
- XML extensible Markup Language
- HTML HyperText Markup Language
- the server 104 may include a web search engine used to find references within the Internet, a phone book type search engine used to find businesses or individuals, or another specialized search engine (e.g., a search engine that provides references to entertainment listings such as restaurants and movie theater information, medical and pharmaceutical information, etc.).
- a web search engine used to find references within the Internet
- a phone book type search engine used to find businesses or individuals
- another specialized search engine e.g., a search engine that provides references to entertainment listings such as restaurants and movie theater information, medical and pharmaceutical information, etc.
- the server 104 provides the file 1 16 that references the search results 131 to the mobile device 102.
- the ASR engine 105 may return a transcription and search query results 1 16 because it is unsure if the second transcription 1 15a is an action or a query, as noted in the third possibility described above, while ASR engine 1 14 on mobile device 102 may return an action (i.e. "call Alex Gruenstein"). The mobile device 102 will examine both results.
- the mobile device 102 will prefer first transcription 1 18, the result from ASR engine 1 14. In doing so the mobile device is employing the comprehensive language model 125 in ASR engine 105 to confirm the action transcribed by ASR engine 1 14 in first transcription 1 18 and employing the user- specific data 128 available to ASR engine 1 14 to produce an accurate transcription of any unique user-specific words such as contact names. Therefore, the result may be an effective use of the attributes of each ASR engine to improve the overall speech recognition accuracy of the mobile device 102.
- the mobile device 102 determines that a portion of the second transcription 1 15a includes a term associated with an action and the associated action is identical to the action returned by ASR engine 1 14 ("call").
- the remaining portion of the second transcription 1 15a (“another pine") is not usable with the action described.
- second transcription 1 15a confirms the type of action returned by ASR engine 1 14, but has mis-transcribed relevant information (for example, contact name) to carry out the action.
- the ASR engine 1 14 returned the same action and matched the action with a contact name contained in user-specific data 128.
- the action returned by the ASR engine 1 14 is performed by mobile device 102. This example is the one depicted in FIG. 1 .
- both ASR engines 105 and 1 14 may return the action "call Alex Gruenstein.” Because both results match either may be chosen. However, the result from ASR engine 105 may generally be preferred out of deference to its complex language model 125.
- ASR engine 105 may return the search query
- the query result may be preferred because the complex language model 125 may be more accurate at distinguishing common speech patterns.
- the mobile device 102 may display the action determined along with a confirmation 130 of that action on user interface 101 .
- search results 131 determined by the search query performed using server transcription 1 15b may be displayed.
- the mobile device 102 may initiate the action without user confirmation. Further, initiating the action without user confirmation may be performed with some but not all actions. For example, “play Lynyrd Skynyrd Sweet Home Alabama” may be initiated once detected as an action by the mobile device 102, but "call Alex Gruenstein” may always require confirmation by the user regardless of a confidence level.
- Such a method may be preferred because any mistake as to playing the incorrect song is a mere inconvenience to the user while calling the wrong person may be embarrassing.
- the mobile device 102 determines with a high confidence that the correct result is a search query it may only display the search results 1 16.
- the user may be provided with a list of the possible transcriptions from both ASR engines 105 1 14 and asked to confirm the proper result.
- FIG. 2 is a flowchart showing an example of a process 200 for performing speech recognition.
- the process 200 includes accessing audio data generated by a mobile device based on audio input from a user, the audio data encoding one or more user utterances, determining a first transcription of the utterances by performing speech recognition on the audio data using a first speech recognizer, wherein the first speech recognizer employs a language model that is developed based on user-specific data, determining a second transcription of the utterances by performing speech recognition on the audio data using a second speech recognizer, wherein the second speech recognizer employs a language model developed independent of user-specific data, determining that the second transcription of the utterances includes a term from a predefined set of one or more terms, and causing an output of the first transcription of the utterance based on determining that the second transcription of the utterance includes the term from the predefined set of one or more terms.
- audio data that encodes an utterance received by a mobile device is generated (202).
- the utterance may include a voice command, a voice search query, or may be an input to a dictation or dialog application or system.
- a first transcription of the utterances is determined by a first speech recognizer performing speech recognition on the audio data (204).
- the first speech recognizer employs a language model developed based on user-specific data.
- user-specific data may be used to aid the first speech recognizer in transcribing unique words associated with the user-specific data.
- the first speech recognizer may employ a grammar-based language model, such as a context-free grammar.
- the grammar-based language model may include a set of one or more terms associated with one or more actions to be performed by the mobile device.
- the user-specific data may include, as an example, a contact list for the user, an applications list of applications installed on the mobile device, or a media list of media stored on the mobile device.
- the first speech recognizer may be implemented on a mobile device, one or more server devices, a personal computer, a tablet computer, or any other computing device.
- a second transcription of the utterances is determined by a second speech recognizer performing speech recognition on the audio data (206).
- the second speech recognizer employs a language model independent of user-specific data stored on the mobile device. As noted above, this may be because of privacy concerns or system limitations.
- the second speech recognizer may employ a statistics-based language model.
- the statistics-based language model may incorporate a large vocabulary.
- the second speech recognizer may be implemented on one or more server devices, personal computers, or any other computing device capable of implementing it.
- action (204) and (206) may be carried out on separate computing devices, such as a mobile device and a server as shown in FIG. 1 , on any combination of computing devices, or on a single computing device.
- the second transcription of the utterances is determined to include a term from a predefined set of one or more terms (208).
- the text of the second transcription may be searched for a term contained in a pre-defined set of one or more terms that describe an action performable by a mobile device.
- An output of the first transcription of the utterance is outputted based on the determination that the second transcription of the utterance includes a term from the predefined set of one or more terms (210).
- the decision to output the first transcription may include determining that the first transcription indicated an action performable by the mobile device and using the term determined in action (208) to be included in the second transcription to confirm the action indicated by the first transcription.
- the first transcription may be used to provide an accurate representation of a word or phrase included in the utterance which is unique to the user-specific data, such as the name of a contact.
- Outputting the first transcription may include, for example, providing the user with the first transcription and requesting user confirmation that the transcription is accurate before initiating performance of the action described by the first transcription.
- outputting the first transcription may include initiating performance of the action described by the first transcription.
- outputting the first transcription may include displaying both the first and second transcription and requesting confirmation from the user of the correct transcription.
- a search may be performed based on the second transcription producing search results. As a result both the first transcription and the search results from the search performed based on the second transcription may be outputted.
- Both the first transcription and the search results from the search performed based on the second transcription may be outputted to the user but displayed in separate interface elements.
- the interface element displaying the output of the first transcription may be configured to receive a user selection of that interface element where such a user selection would cause the mobile device to perform the action described by the first transcription.
- FIG. 3 is a flowchart showing an example of another process 300 for performing speech recognition.
- the process 300 is described as being performed by system 100, but other systems or configurations may perform process 300.
- ASR engine 105 may make a preliminary determination regarding whether transcription 1 15a is an action or a query (state (e) of FIG. 1 ) and may communicate the transcription 1 15a to the mobile device 102 as either a voice command, a query, or as undetermined.
- the mobile device 102 encodes an utterance 1 12 to generate audio data
- the audio data 1 13 is communicated to each of the speech recognizers 124 and 126 incorporated in ASR engine 105 and
- ASR engine 105 and ASR engine 1 14 then each access the audio data (302).
- Client-based speech recognizer 126 incorporated in ASR engine 1 14 determines a first transcription 1 18 of the audio data 1 13 (304).
- the server-based speech recognizer 124 incorporated in ASR engine 105 determines a second transcription 1 15a of the audio data 1 13 (306).
- the mobile device 102 examines the second transcription 1 15a determined by the server-based speech recognizer 124 and determines if the server- based speech recognizer 124 returned an action (308). If so, (310) the mobile device 102 performs the returned action. In implementations, the mobile device 102 may display the action to be performed to the user 1 10 and request confirmation before performing the action. If the second transcription 1 15a is not an action, then the second transcription 1 15a may be considered a search query or simply a transcription with an uncertain preliminary designation as discussed above.
- the mobile device examines the first transcription 1 18 from the client- based speech recognizer 126 to determine if the first transcription is an action (312). If the first transcription 1 18 from the client-based speech recognizer 126 was an action, then the mobile device 102 examines the text of the second transcription 1 18 from the server-based speech recognizer 124 to determine if the search query may be identified as an action (314). To make this determination, the mobile device 102 may search the text of the second transcription 1 15a for words or phrases in a predefined list which indicate an action performable by the mobile device 102. For example, text such as "call,” “please call,” “call ... at home,” or "dial” may be used to indicate the action of initiating a telephone call.
- the mobile device 102 may prefer the first transcription and therefore perform the action of the first transcription 1 18 (316). Doing so may leverage the strengths of the two different types of voice recognizers.
- the language model 125 of the server-based speech recognizer 124 may be used to confirm the action indicated by the client-based 126 speech recognizer while taking advantage of the client-based speech recognizer's 126 access to users-specific data 128 to produce an accurate transcription of words or phrases unique to that data.
- the mobile device 102 may display the action to be performed to the user 1 10 and request confirmation before performing the action.
- the search query does not contain words or phrases that match the pre-defined list of terms, then the user utterance 1 12 is not considered to be an action, but instead considered to be a search query.
- the second transcription 1 15a is preferred and search results 1 16 from a search based on that transcription are presented to the user 1 10 (318).
- the first transcription 1 18 was not an action (312)
- the second transcription 1 15a is preferred and search results 1 16 from a search based on that transcription are presented to the user 1 10 (318).
- search results 1 16 have not yet been received from a server 104, the mobile device 102 may request a search query from a server 104 using the second transcription 1 15b and display the results to the user 1 10.
- FIG. 4 is a swim lane diagram showing an example 400 of
- the process 400 may be implemented by a mobile device 402 with a local ASR engine having access to user-specific data, a second ASR engine 404 independent of user-specific data, and a search engine 406.
- the mobile device 402 may provide audio data that corresponds to an utterance to both its local ASR engine and the second ASR engine 404. Although only one mobile device 402 is illustrated, the mobile device 402 may represent a large quantity of mobile devices 402 contributing audio data.
- the mobile device 402 receives an utterance from a user and generates audio data from the utterance (408).
- the utterance for example, may include a voice command or a search query.
- the recording of the utterance may optionally include a sample of audio, for example recorded briefly before or after the recording of the utterance.
- the mobile device 402 sends the audio data (410) to the second ASR engine 404, which receives the audio data.
- the local ASR engine implemented on the mobile device 402 performs speech recognition on the audio data (412) and, as a result, determines a first transcription (414).
- ASR engine 404 performs speech recognition on the audio data (416) and, as a result, determines a second transcription (418).
- the ASR engine 404 may classify the second transcription as an action relating to a voice command, text for a search query, or leave the transcription unclassified.
- the ASR engine 404 is uncertain as to the proper classification of the utterance and leaves the second transcription unclassified.
- the ASR engine sends the second transcription (420) to the mobile device 402 and sends the second transcription (422) to the search engine 406, each of which receive the second transcription.
- the search engine 406 performs a search using the second transcription as a search query (424).
- the search may be a web search, an address search, a telephone directory search, a map search, or any other type of search.
- the search engine 406 may execute one or more search queries using the second transcription, and may generate a file that references search results.
- the file may be a markup language file, such as an extensible Markup Language (XML) or HyperText Markup Language (HTML) file.
- the search engine 406 then sends the search results (426) to the mobile device 402, which receives the search results.
- the mobile device 402 determines that the second transcription includes a term or phrase that is included in a pre-defined set of terms which describe an action performable by the mobile device 402 (428). In making this determination, the mobile device 402 may also determine that the first transcription indicated an action performable by the mobile device 402. In addition, the mobile device 402 may use the second transcription to confirm the action indicated by the first transcription. Further, the mobile device 402 may use the first transcription to provide an accurate representation of a word or phrase included in the utterance which is unique to the user-specific data.
- the mobile device 402 outputs the appropriate transcription (430). This may include initiating performance of the action as described by the first
- FIG. 5 is a diagram showing an example of a screen shot 500, for example, on mobile device 102.
- the diagram 500 is described as a specific example of an output generated by system 100, but other displays or configurations may be used to display output form system 100 to the user.
- Upper display section 502 displays a transcription to the user. This section 502 may display the first transcription 1 18, the second transcription 1 15a, or a user selectable list of both transcriptions for user confirmation.
- Middle display section 504 is a user selectable interface displaying the action interpreted by the mobile device 102 from the first transcription 1 18 of the user utterance 1 12. It displays the information associated with a contact from the user- specific data 128 stored on the mobile device based on the first transcription 1 18 of the utterance 1 12 and generated by the client-based speech recognizer 126 that has access to the user-specific data 128. As shown, middle display section 504 may include all appropriate data related to the contact, for example, the contact's name, telephone number, and a digital photo. Along with the contact information displayed, a confirmation of the action "call" is displayed in the bottom with a telephone icon prompting the user to touch that portion of the display to initiate performance of the action, in this case calling Bill Byrne.
- Lower display section 506 shows a list of user selectable search results
- FIG. 6 shows an example of a generic computing device 600 and a generic mobile computing device 650, which may be used with the techniques described here.
- the computing devices may support either one or both of the ASR engines described.
- the computing devices may use the ASR engine(s) for multiple purposes, for example, operating as a server to provide transcriptions to other devices or generating transcriptions for use locally on the computing device itself.
- Computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- Computing device 650 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown here, their connections and
- Computing device 600 includes a processor 602, memory 604, a storage device 606, a high-speed interface 608 connecting to memory 604 and high-speed expansion ports 610, and a low speed interface 612 connecting to low speed bus 614 and storage device 606.
- Each of the components 602, 604, 606, 608, 610, and 612 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 602 can process instructions for execution within the computing device 600, including instructions stored in the memory 604 or on the storage device 606 to display graphical information for a GUI on an external input/output device, such as display 616 coupled to high speed interface 608.
- the memory 604 stores information within the computing device 600.
- the memory 604 is a volatile memory unit or units.
- the memory 604 is a non-volatile memory unit or units.
- the memory 604 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 606 is capable of providing mass storage for the computing device 600.
- the storage device 606 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product can be tangibly embodied in an information carrier.
- the computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 604, the storage device 606, memory on processor 602, or a propagated signal.
- the high speed controller 608 manages bandwidth-intensive operations for the computing device 600, while the low speed controller 612 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only.
- the high-speed controller 608 is coupled to memory 604, display 616 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 610, which may accept various expansion cards (not shown).
- low-speed controller 612 is coupled to storage device 606 and low- speed expansion port 614.
- the low-speed expansion port which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 600 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 620, or multiple times in a group of such servers. It may also be implemented as part of a rack server system 624. In addition, it may be implemented in a personal computer such as a laptop computer 622. Alternatively, components from computing device 600 may be combined with other components in a mobile device, such as device 650. Each of such devices may contain one or more of computing device 600, 650, and an entire system may be made up of multiple computing devices 600, 650 communicating with each other.
- Computing device 650 includes a processor 652, memory 664, and an input/output device such as a display 654, a communication interface 667, and a transceiver 668, among other components.
- the device 650 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage.
- a storage device such as a microdrive or other device, to provide additional storage.
- the processor 652 can execute instructions within the computing device
- the processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors.
- the processor may provide, for example, for coordination of the other components of the device 650, such as control of user interfaces, applications run by device 650, and wireless communication by device 650.
- Processor 652 may communicate with a user through control interface
- the display 654 may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 656 may comprise appropriate circuitry for driving the display 654 to present graphical and other information to a user.
- the control interface 658 may receive commands from a user and convert them for submission to the processor 652.
- an external interface 662 may be provide in communication with processor 652, so as to enable near area communication of device 650 with other devices. External interface 662 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 664 stores information within the computing device 650.
- the memory 664 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- Expansion memory 674 may also be provided and connected to device 650 through expansion interface 672, which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- expansion memory 674 may provide extra storage space for device 650, or may also store applications or other information for device 650.
- expansion memory 674 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- expansion memory 674 may be provide as a security module for device 650, and may be programmed with instructions that permit secure use of device 650.
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory, as discussed below.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 664, expansion memory 674, memory on processor 652, or a propagated signal that may be received, for example, over transceiver 668 or external interface 662.
- Device 650 may communicate wirelessly through communication interface 667, which may include digital signal processing circuitry where necessary. Communication interface 667 may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver 668. In addition, short- range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module 670 may provide additional navigation- and location-related wireless data to device 650, which may be used as appropriate by applications running on device 650.
- GPS Global Positioning System
- Device 650 may also communicate audibly using audio codec 660, which may receive spoken information from a user and convert it to usable digital information. Audio codec 660 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 650. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 650.
- Audio codec 660 may receive spoken information from a user and convert it to usable digital information. Audio codec 660 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device 650. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 650.
- the computing device 650 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 680. It may also be implemented as part of a smartphone 682, personal digital assistant, or other similar mobile device.
- Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- machine-readable medium refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal.
- machine-readable signal refers to any signal used to provide machine instructions and/or data to a programmable processor.
- the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
- the systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN”), a wide area network (“WAN”), and the Internet.
- LAN local area network
- WAN wide area network
- the Internet the global information network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
Claims
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP19168323.4A EP3534364A1 (en) | 2012-06-26 | 2013-06-26 | Distributed speech recognition |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261664324P | 2012-06-26 | 2012-06-26 | |
US13/838,379 US10354650B2 (en) | 2012-06-26 | 2013-03-15 | Recognizing speech with mixed speech recognition models to generate transcriptions |
PCT/US2013/047780 WO2014004612A1 (en) | 2012-06-26 | 2013-06-26 | Mixed model speech recognition |
Related Child Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP19168323.4A Division EP3534364A1 (en) | 2012-06-26 | 2013-06-26 | Distributed speech recognition |
EP19168323.4A Division-Into EP3534364A1 (en) | 2012-06-26 | 2013-06-26 | Distributed speech recognition |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2864980A1 true EP2864980A1 (en) | 2015-04-29 |
EP2864980B1 EP2864980B1 (en) | 2019-10-23 |
Family
ID=49775156
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP13734936.1A Active EP2864980B1 (en) | 2012-06-26 | 2013-06-26 | Mixed model speech recognition |
EP19168323.4A Withdrawn EP3534364A1 (en) | 2012-06-26 | 2013-06-26 | Distributed speech recognition |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP19168323.4A Withdrawn EP3534364A1 (en) | 2012-06-26 | 2013-06-26 | Distributed speech recognition |
Country Status (4)
Country | Link |
---|---|
US (4) | US10354650B2 (en) |
EP (2) | EP2864980B1 (en) |
CN (2) | CN108648750B (en) |
WO (1) | WO2014004612A1 (en) |
Families Citing this family (120)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10354650B2 (en) * | 2012-06-26 | 2019-07-16 | Google Llc | Recognizing speech with mixed speech recognition models to generate transcriptions |
KR101961139B1 (en) * | 2012-06-28 | 2019-03-25 | 엘지전자 주식회사 | Mobile terminal and method for recognizing voice thereof |
US9583100B2 (en) * | 2012-09-05 | 2017-02-28 | GM Global Technology Operations LLC | Centralized speech logger analysis |
CN103065630B (en) * | 2012-12-28 | 2015-01-07 | 科大讯飞股份有限公司 | User personalized information voice recognition method and user personalized information voice recognition system |
JP2015011170A (en) * | 2013-06-28 | 2015-01-19 | 株式会社ＡＴＲ−Ｔｒｅｋ | Voice recognition client device performing local voice recognition |
US10885918B2 (en) | 2013-09-19 | 2021-01-05 | Microsoft Technology Licensing, Llc | Speech recognition using phoneme matching |
US20150149169A1 (en) * | 2013-11-27 | 2015-05-28 | At&T Intellectual Property I, L.P. | Method and apparatus for providing mobile multimodal speech hearing aid |
US9601108B2 (en) | 2014-01-17 | 2017-03-21 | Microsoft Technology Licensing, Llc | Incorporating an exogenous large-vocabulary model into rule-based speech recognition |
US10749989B2 (en) * | 2014-04-01 | 2020-08-18 | Microsoft Technology Licensing Llc | Hybrid client/server architecture for parallel processing |
US9462112B2 (en) | 2014-06-19 | 2016-10-04 | Microsoft Technology Licensing, Llc | Use of a digital assistant in communications |
US9502032B2 (en) | 2014-10-08 | 2016-11-22 | Google Inc. | Dynamically biasing language models |
EP3690879A3 (en) * | 2014-11-07 | 2020-08-26 | Samsung Electronics Co., Ltd. | Speech signal processing method and speech signal processing apparatus |
EP4350558A2 (en) | 2014-11-07 | 2024-04-10 | Samsung Electronics Co., Ltd. | Speech signal processing method and speech signal processing apparatus |
KR102536944B1 (en) * | 2014-11-07 | 2023-05-26 | 삼성전자주식회사 | Method and apparatus for speech signal processing |
KR102346302B1 (en) * | 2015-02-16 | 2022-01-03 | 삼성전자 주식회사 | Electronic apparatus and Method of operating voice recognition in the electronic apparatus |
US10013981B2 (en) * | 2015-06-06 | 2018-07-03 | Apple Inc. | Multi-microphone speech recognition systems and related techniques |
US9865265B2 (en) | 2015-06-06 | 2018-01-09 | Apple Inc. | Multi-microphone speech recognition systems and related techniques |
US9691380B2 (en) | 2015-06-15 | 2017-06-27 | Google Inc. | Negative n-gram biasing |
DE102015212650B4 (en) * | 2015-07-07 | 2020-02-13 | Volkswagen Aktiengesellschaft | Method and system for computer-assisted processing of a speech input |
US10332509B2 (en) * | 2015-11-25 | 2019-06-25 | Baidu USA, LLC | End-to-end speech recognition |
US10133821B2 (en) * | 2016-01-06 | 2018-11-20 | Google Llc | Search result prefetching of voice queries |
US10743101B2 (en) | 2016-02-22 | 2020-08-11 | Sonos, Inc. | Content mixing |
US9947316B2 (en) | 2016-02-22 | 2018-04-17 | Sonos, Inc. | Voice control of a media playback system |
US10264030B2 (en) | 2016-02-22 | 2019-04-16 | Sonos, Inc. | Networked microphone device control |
US10509626B2 (en) | 2016-02-22 | 2019-12-17 | Sonos, Inc | Handling of loss of pairing between networked devices |
US10095470B2 (en) | 2016-02-22 | 2018-10-09 | Sonos, Inc. | Audio response playback |
US9965247B2 (en) | 2016-02-22 | 2018-05-08 | Sonos, Inc. | Voice controlled media playback system based on user profile |
US9978367B2 (en) | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
KR102151682B1 (en) * | 2016-03-23 | 2020-09-04 | 구글 엘엘씨 | Adaptive audio enhancement for multi-channel speech recognition |
US10192555B2 (en) * | 2016-04-28 | 2019-01-29 | Microsoft Technology Licensing, Llc | Dynamic speech recognition data evaluation |
EP3469585B1 (en) * | 2016-06-08 | 2020-08-26 | Google LLC | Scalable dynamic class language modeling |
US9978390B2 (en) | 2016-06-09 | 2018-05-22 | Sonos, Inc. | Dynamic player selection for audio signal processing |
US10891959B1 (en) * | 2016-07-01 | 2021-01-12 | Google Llc | Voice message capturing system |
US10134399B2 (en) | 2016-07-15 | 2018-11-20 | Sonos, Inc. | Contextualization of voice inputs |
US10152969B2 (en) | 2016-07-15 | 2018-12-11 | Sonos, Inc. | Voice detection by multiple devices |
DE102016009196B4 (en) * | 2016-07-27 | 2021-02-11 | Audi Ag | Method for operating multiple speech recognizers |
US10019986B2 (en) * | 2016-07-29 | 2018-07-10 | Google Llc | Acoustic model training using corrected terms |
DE102016114265A1 (en) * | 2016-08-02 | 2018-02-08 | Claas Selbstfahrende Erntemaschinen Gmbh | Method for at least partially machine transferring a word sequence written in a source language into a word sequence of a target language |
US10115400B2 (en) | 2016-08-05 | 2018-10-30 | Sonos, Inc. | Multiple voice services |
US9972320B2 (en) * | 2016-08-24 | 2018-05-15 | Google Llc | Hotword detection on multiple devices |
US9942678B1 (en) | 2016-09-27 | 2018-04-10 | Sonos, Inc. | Audio playback settings for voice interaction |
US9959861B2 (en) * | 2016-09-30 | 2018-05-01 | Robert Bosch Gmbh | System and method for speech recognition |
US9743204B1 (en) | 2016-09-30 | 2017-08-22 | Sonos, Inc. | Multi-orientation playback device microphones |
US10181323B2 (en) | 2016-10-19 | 2019-01-15 | Sonos, Inc. | Arbitration-based voice recognition |
US9959864B1 (en) * | 2016-10-27 | 2018-05-01 | Google Llc | Location-based voice query recognition |
CN108121735B (en) * | 2016-11-29 | 2022-03-11 | 百度在线网络技术（北京）有限公司 | Voice search method and device |
CN108231073B (en) * | 2016-12-16 | 2021-02-05 | 深圳富泰宏精密工业有限公司 | Voice control device, system and control method |
KR20180075009A (en) * | 2016-12-26 | 2018-07-04 | 현대자동차주식회사 | Speech processing apparatus, vehicle having the same and speech processing method |
US10831366B2 (en) * | 2016-12-29 | 2020-11-10 | Google Llc | Modality learning on mobile devices |
US10311860B2 (en) | 2017-02-14 | 2019-06-04 | Google Llc | Language model biasing system |
US11183181B2 (en) | 2017-03-27 | 2021-11-23 | Sonos, Inc. | Systems and methods of multiple voice services |
US10438584B2 (en) * | 2017-04-07 | 2019-10-08 | Google Llc | Multi-user virtual assistant for verbal device control |
US10607606B2 (en) * | 2017-06-19 | 2020-03-31 | Lenovo (Singapore) Pte. Ltd. | Systems and methods for execution of digital assistant |
US11263399B2 (en) * | 2017-07-31 | 2022-03-01 | Apple Inc. | Correcting input based on user context |
US10475449B2 (en) | 2017-08-07 | 2019-11-12 | Sonos, Inc. | Wake-word detection suppression |
US10048930B1 (en) | 2017-09-08 | 2018-08-14 | Sonos, Inc. | Dynamic computation of system response volume |
US10446165B2 (en) | 2017-09-27 | 2019-10-15 | Sonos, Inc. | Robust short-time fourier transform acoustic echo cancellation during audio playback |
US10051366B1 (en) | 2017-09-28 | 2018-08-14 | Sonos, Inc. | Three-dimensional beam forming with a microphone array |
US10621981B2 (en) | 2017-09-28 | 2020-04-14 | Sonos, Inc. | Tone interference cancellation |
US10482868B2 (en) | 2017-09-28 | 2019-11-19 | Sonos, Inc. | Multi-channel acoustic echo cancellation |
US10466962B2 (en) | 2017-09-29 | 2019-11-05 | Sonos, Inc. | Media playback system with voice assistance |
DE102017128651A1 (en) * | 2017-12-02 | 2019-06-06 | Tobias Rückert | Dialogue system and method for implementing a user's instructions |
US10880650B2 (en) | 2017-12-10 | 2020-12-29 | Sonos, Inc. | Network microphone devices with automatic do not disturb actuation capabilities |
US10818290B2 (en) | 2017-12-11 | 2020-10-27 | Sonos, Inc. | Home graph |
WO2019152722A1 (en) | 2018-01-31 | 2019-08-08 | Sonos, Inc. | Device designation of playback and network microphone device arrangements |
JP2019144790A (en) * | 2018-02-20 | 2019-08-29 | 富士ゼロックス株式会社 | Information processing device and program |
US11676062B2 (en) * | 2018-03-06 | 2023-06-13 | Samsung Electronics Co., Ltd. | Dynamically evolving hybrid personalized artificial intelligence system |
US10621983B2 (en) * | 2018-04-20 | 2020-04-14 | Spotify Ab | Systems and methods for enhancing responsiveness to utterances having detectable emotion |
US11175880B2 (en) | 2018-05-10 | 2021-11-16 | Sonos, Inc. | Systems and methods for voice-assisted media content selection |
US10847178B2 (en) | 2018-05-18 | 2020-11-24 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection |
US10959029B2 (en) | 2018-05-25 | 2021-03-23 | Sonos, Inc. | Determining and adapting to changes in microphone performance of playback devices |
JP2021156907A (en) * | 2018-06-15 | 2021-10-07 | ソニーグループ株式会社 | Information processor and information processing method |
US10681460B2 (en) | 2018-06-28 | 2020-06-09 | Sonos, Inc. | Systems and methods for associating playback devices with voice assistant services |
US10461710B1 (en) | 2018-08-28 | 2019-10-29 | Sonos, Inc. | Media playback system with maximum volume setting |
US11076035B2 (en) | 2018-08-28 | 2021-07-27 | Sonos, Inc. | Do not disturb feature for audio notifications |
US10878811B2 (en) | 2018-09-14 | 2020-12-29 | Sonos, Inc. | Networked devices, systems, and methods for intelligently deactivating wake-word engines |
US10587430B1 (en) | 2018-09-14 | 2020-03-10 | Sonos, Inc. | Networked devices, systems, and methods for associating playback devices based on sound codes |
KR102146524B1 (en) * | 2018-09-19 | 2020-08-20 | 주식회사 포티투마루 | Method, system and computer program for generating speech recognition learning data |
US11024331B2 (en) | 2018-09-21 | 2021-06-01 | Sonos, Inc. | Voice detection optimization using sound metadata |
US10811015B2 (en) | 2018-09-25 | 2020-10-20 | Sonos, Inc. | Voice detection optimization based on selected voice assistant service |
US11100923B2 (en) | 2018-09-28 | 2021-08-24 | Sonos, Inc. | Systems and methods for selective wake word detection using neural network models |
US10692518B2 (en) | 2018-09-29 | 2020-06-23 | Sonos, Inc. | Linear filtering for noise-suppressed speech detection via multiple network microphone devices |
GB2577879B (en) | 2018-10-08 | 2022-08-24 | B & W Group Ltd | Content playback system |
US11899519B2 (en) | 2018-10-23 | 2024-02-13 | Sonos, Inc. | Multiple stage network microphone device with reduced power consumption and processing load |
CN113016030A (en) * | 2018-11-06 | 2021-06-22 | 株式会社赛斯特安国际 | Method and device for providing voice recognition service |
EP3654249A1 (en) | 2018-11-15 | 2020-05-20 | Snips | Dilated convolutions and gating for efficient keyword spotting |
TWI698857B (en) * | 2018-11-21 | 2020-07-11 | 財團法人工業技術研究院 | Speech recognition system and method thereof, and computer program product |
GB2579554A (en) * | 2018-12-03 | 2020-07-01 | Audiogum Uk Ltd | Content playback system |
US11183183B2 (en) | 2018-12-07 | 2021-11-23 | Sonos, Inc. | Systems and methods of operating media playback systems having multiple voice assistant services |
US11132989B2 (en) | 2018-12-13 | 2021-09-28 | Sonos, Inc. | Networked microphone devices, systems, and methods of localized arbitration |
US10602268B1 (en) | 2018-12-20 | 2020-03-24 | Sonos, Inc. | Optimization of network microphone devices using noise classification |
JP7020390B2 (en) * | 2018-12-20 | 2022-02-16 | トヨタ自動車株式会社 | Control device, voice dialogue device, voice recognition server and program |
CN111524508A (en) * | 2019-02-03 | 2020-08-11 | 上海蔚来汽车有限公司 | Voice conversation system and voice conversation implementation method |
JP7241190B2 (en) * | 2019-02-06 | 2023-03-16 | グーグル エルエルシー | Voice Query Quality of Service QoS Based on Client Computed Content Metadata |
US11315556B2 (en) | 2019-02-08 | 2022-04-26 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing by transmitting sound data associated with a wake word to an appropriate device for identification |
US10867604B2 (en) | 2019-02-08 | 2020-12-15 | Sonos, Inc. | Devices, systems, and methods for distributed voice processing |
US11093720B2 (en) * | 2019-03-28 | 2021-08-17 | Lenovo (Singapore) Pte. Ltd. | Apparatus, method, and program product for converting multiple language variations |
US11462216B2 (en) | 2019-03-28 | 2022-10-04 | Cerence Operating Company | Hybrid arbitration system |
US11120794B2 (en) | 2019-05-03 | 2021-09-14 | Sonos, Inc. | Voice assistant persistence across multiple network microphone devices |
US11361756B2 (en) | 2019-06-12 | 2022-06-14 | Sonos, Inc. | Conditional wake word eventing based on environment |
US10586540B1 (en) | 2019-06-12 | 2020-03-10 | Sonos, Inc. | Network microphone device with command keyword conditioning |
US11200894B2 (en) | 2019-06-12 | 2021-12-14 | Sonos, Inc. | Network microphone device with command keyword eventing |
CN112151024B (en) * | 2019-06-28 | 2023-09-22 | 声音猎手公司 | Method and apparatus for generating edited transcription of speech audio |
CA3147589A1 (en) * | 2019-07-15 | 2021-01-21 | Axon Enterprise, Inc. | Methods and systems for transcription of audio data |
US11138969B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US11138975B2 (en) | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
US10871943B1 (en) | 2019-07-31 | 2020-12-22 | Sonos, Inc. | Noise classification for event detection |
US11189286B2 (en) | 2019-10-22 | 2021-11-30 | Sonos, Inc. | VAS toggle based on device orientation |
US11200900B2 (en) | 2019-12-20 | 2021-12-14 | Sonos, Inc. | Offline voice control |
CN111081225B (en) * | 2019-12-31 | 2022-04-01 | 思必驰科技股份有限公司 | Skill voice awakening method and device |
US11562740B2 (en) | 2020-01-07 | 2023-01-24 | Sonos, Inc. | Voice verification for media playback |
US11556307B2 (en) | 2020-01-31 | 2023-01-17 | Sonos, Inc. | Local voice data processing |
US11308958B2 (en) | 2020-02-07 | 2022-04-19 | Sonos, Inc. | Localized wakeword verification |
US11482224B2 (en) | 2020-05-20 | 2022-10-25 | Sonos, Inc. | Command keywords with input detection windowing |
US11308962B2 (en) | 2020-05-20 | 2022-04-19 | Sonos, Inc. | Input detection windowing |
US11727919B2 (en) | 2020-05-20 | 2023-08-15 | Sonos, Inc. | Memory allocation for keyword spotting engines |
US11698771B2 (en) | 2020-08-25 | 2023-07-11 | Sonos, Inc. | Vocal guidance engines for playback devices |
US11580959B2 (en) * | 2020-09-28 | 2023-02-14 | International Business Machines Corporation | Improving speech recognition transcriptions |
CN112509585A (en) * | 2020-12-22 | 2021-03-16 | 北京百度网讯科技有限公司 | Voice processing method, device and equipment of vehicle-mounted equipment and storage medium |
US11551700B2 (en) | 2021-01-25 | 2023-01-10 | Sonos, Inc. | Systems and methods for power-efficient keyword detection |
Family Cites Families (78)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
ZA948426B (en) | 1993-12-22 | 1995-06-30 | Qualcomm Inc | Distributed voice recognition system |
US7003463B1 (en) * | 1998-10-02 | 2006-02-21 | International Business Machines Corporation | System and method for providing network coordinated conversational services |
US6446076B1 (en) | 1998-11-12 | 2002-09-03 | Accenture Llp. | Voice interactive web-based agent system responsive to a user location for prioritizing and formatting information |
US7881936B2 (en) | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
US7720682B2 (en) | 1998-12-04 | 2010-05-18 | Tegic Communications, Inc. | Method and apparatus utilizing voice input to resolve ambiguous manually entered text input |
US7679534B2 (en) | 1998-12-04 | 2010-03-16 | Tegic Communications, Inc. | Contextual prediction of user words and user actions |
US20020032564A1 (en) * | 2000-04-19 | 2002-03-14 | Farzad Ehsani | Phrase-based dialogue modeling with particular application to creating a recognition grammar for a voice-controlled user interface |
JP2001100781A (en) * | 1999-09-30 | 2001-04-13 | Sony Corp | Method and device for voice processing and recording medium |
US6865528B1 (en) * | 2000-06-01 | 2005-03-08 | Microsoft Corporation | Use of a unified language model |
US7085723B2 (en) * | 2001-01-12 | 2006-08-01 | International Business Machines Corporation | System and method for determining utterance context in a multi-context speech application |
US6754626B2 (en) * | 2001-03-01 | 2004-06-22 | International Business Machines Corporation | Creating a hierarchical tree of language models for a dialog system based on prompt and dialog context |
US7277853B1 (en) * | 2001-03-02 | 2007-10-02 | Mindspeed Technologies, Inc. | System and method for a endpoint detection of speech for improved speech recognition in noisy environments |
DE60113644T2 (en) | 2001-03-27 | 2006-07-06 | Nokia Corp. | Method and system for managing a database in a communications network |
US7225130B2 (en) * | 2001-09-05 | 2007-05-29 | Voice Signal Technologies, Inc. | Methods, systems, and programming for performing speech recognition |
US7308404B2 (en) * | 2001-09-28 | 2007-12-11 | Sri International | Method and apparatus for speech recognition using a dynamic vocabulary |
US20030120493A1 (en) * | 2001-12-21 | 2003-06-26 | Gupta Sunil K. | Method and system for updating and customizing recognition vocabulary |
US7013275B2 (en) * | 2001-12-28 | 2006-03-14 | Sri International | Method and apparatus for providing a dynamic speech-driven control and remote service access system |
US7016849B2 (en) * | 2002-03-25 | 2006-03-21 | Sri International | Method and apparatus for providing speech-driven routing between spoken language applications |
US6999930B1 (en) * | 2002-03-27 | 2006-02-14 | Extended Systems, Inc. | Voice dialog server method and system |
US7236931B2 (en) * | 2002-05-01 | 2007-06-26 | Usb Ag, Stamford Branch | Systems and methods for automatic acoustic speaker adaptation in computer-assisted transcription systems |
US20040019488A1 (en) | 2002-07-23 | 2004-01-29 | Netbytel, Inc. | Email address recognition using personal information |
US7570943B2 (en) | 2002-08-29 | 2009-08-04 | Nokia Corporation | System and method for providing context sensitive recommendations to digital services |
US7302383B2 (en) * | 2002-09-12 | 2007-11-27 | Luis Calixto Valles | Apparatus and methods for developing conversational applications |
US7328155B2 (en) * | 2002-09-25 | 2008-02-05 | Toyota Infotechnology Center Co., Ltd. | Method and system for speech recognition using grammar weighted based upon location information |
CA2516941A1 (en) * | 2003-02-19 | 2004-09-02 | Custom Speech Usa, Inc. | A method for form completion using speech recognition and text comparison |
US7346493B2 (en) * | 2003-03-25 | 2008-03-18 | Microsoft Corporation | Linguistically informed statistical models of constituent structure for ordering in sentence realization for a natural language generation system |
US8311835B2 (en) * | 2003-08-29 | 2012-11-13 | Microsoft Corporation | Assisted multi-modal dialogue |
JP2005202014A (en) * | 2004-01-14 | 2005-07-28 | Sony Corp | Audio signal processor, audio signal processing method, and audio signal processing program |
US7624018B2 (en) * | 2004-03-12 | 2009-11-24 | Microsoft Corporation | Speech recognition using categories and speech prefixing |
US20050246325A1 (en) | 2004-04-30 | 2005-11-03 | Microsoft Corporation | Method and system for recording and accessing usage of an item in a computer system |
US8589156B2 (en) | 2004-07-12 | 2013-11-19 | Hewlett-Packard Development Company, L.P. | Allocation of speech recognition tasks and combination of results thereof |
US20060069564A1 (en) | 2004-09-10 | 2006-03-30 | Rightnow Technologies, Inc. | Method of weighting speech recognition grammar responses using knowledge base usage data |
US9224394B2 (en) * | 2009-03-24 | 2015-12-29 | Sirius Xm Connected Vehicle Services Inc | Service oriented speech recognition for in-vehicle automated interaction and in-vehicle user interfaces requiring minimal cognitive driver processing for same |
US7769142B2 (en) | 2005-07-14 | 2010-08-03 | Microsoft Corporation | Asynchronous discrete manageable instant voice messages |
US7542904B2 (en) * | 2005-08-19 | 2009-06-02 | Cisco Technology, Inc. | System and method for maintaining a speech-recognition grammar |
US20110143731A1 (en) | 2005-09-14 | 2011-06-16 | Jorey Ramer | Mobile Communication Facility Usage Pattern Geographic Based Advertising |
US8131548B2 (en) | 2006-03-06 | 2012-03-06 | Nuance Communications, Inc. | Dynamically adjusting speech grammar weights based on usage |
US7752152B2 (en) * | 2006-03-17 | 2010-07-06 | Microsoft Corporation | Using predictive user models for language modeling on a personal device with user behavior models based on statistical modeling |
US8332218B2 (en) * | 2006-06-13 | 2012-12-11 | Nuance Communications, Inc. | Context-based grammars for automated speech recognition |
US20080288252A1 (en) * | 2007-03-07 | 2008-11-20 | Cerra Joseph P | Speech recognition of speech recorded by a mobile communication facility |
US8886540B2 (en) * | 2007-03-07 | 2014-11-11 | Vlingo Corporation | Using speech recognition results based on an unstructured language model in a mobile communication facility application |
US20110054896A1 (en) | 2007-03-07 | 2011-03-03 | Phillips Michael S | Sending a communications header with voice recording to send metadata for use in speech recognition and formatting in mobile dictation application |
US20080221884A1 (en) * | 2007-03-07 | 2008-09-11 | Cerra Joseph P | Mobile environment speech processing facility |
US20110054894A1 (en) | 2007-03-07 | 2011-03-03 | Phillips Michael S | Speech recognition through the collection of contact information in mobile dictation application |
US20090030697A1 (en) | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Using contextual information for delivering results generated from a speech recognition facility using an unstructured language model |
US8949266B2 (en) * | 2007-03-07 | 2015-02-03 | Vlingo Corporation | Multiple web-based content category searching in mobile search application |
US8635243B2 (en) * | 2007-03-07 | 2014-01-21 | Research In Motion Limited | Sending a communications header with voice recording to send metadata for use in speech recognition, formatting, and search mobile search application |
US20110060587A1 (en) * | 2007-03-07 | 2011-03-10 | Phillips Michael S | Command and control utilizing ancillary information in a mobile voice-to-speech application |
US20110054900A1 (en) | 2007-03-07 | 2011-03-03 | Phillips Michael S | Hybrid command and control between resident and remote speech recognition facilities in a mobile voice-to-speech application |
US20090030687A1 (en) * | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Adapting an unstructured language model speech recognition system based on usage |
US8838457B2 (en) * | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
US8515757B2 (en) * | 2007-03-20 | 2013-08-20 | Nuance Communications, Inc. | Indexing digitized speech with words represented in the digitized speech |
US8204746B2 (en) | 2007-03-29 | 2012-06-19 | Intellisist, Inc. | System and method for providing an automated call center inline architecture |
US8396713B2 (en) | 2007-04-30 | 2013-03-12 | Nuance Communications, Inc. | Method and system for using a statistical language model and an action classifier in parallel with grammar for better handling of out-of-grammar utterances |
US8538757B2 (en) * | 2007-05-17 | 2013-09-17 | Redstart Systems, Inc. | System and method of a list commands utility for a speech recognition command system |
US7881930B2 (en) * | 2007-06-25 | 2011-02-01 | Nuance Communications, Inc. | ASR-aided transcription with segmented feedback training |
US8140335B2 (en) * | 2007-12-11 | 2012-03-20 | Voicebox Technologies, Inc. | System and method for providing a natural language voice user interface in an integrated voice navigation services environment |
US7953598B2 (en) * | 2007-12-17 | 2011-05-31 | Verizon Patent And Licensing Inc. | Grammar weighting voice recognition information |
US8099289B2 (en) * | 2008-02-13 | 2012-01-17 | Sensory, Inc. | Voice interface and search for electronic devices including bluetooth headsets and remote systems |
US20090326937A1 (en) | 2008-04-21 | 2009-12-31 | Microsoft Corporation | Using personalized health information to improve speech recognition |
US8108423B2 (en) * | 2008-10-03 | 2012-01-31 | Disney Enterprises, Inc. | System and method for ontology and rules based segmentation engine for networked content delivery |
US8126715B2 (en) * | 2008-11-26 | 2012-02-28 | Microsoft Corporation | Facilitating multimodal interaction with grammar-based speech applications |
CA2690174C (en) * | 2009-01-13 | 2014-10-14 | Crim (Centre De Recherche Informatique De Montreal) | Identifying keyword occurrences in audio data |
EP2211336B1 (en) | 2009-01-23 | 2014-10-08 | Harman Becker Automotive Systems GmbH | Improved speech input using navigation information |
US20100268534A1 (en) * | 2009-04-17 | 2010-10-21 | Microsoft Corporation | Transcription, archiving and threading of voice communications |
US8892439B2 (en) | 2009-07-15 | 2014-11-18 | Microsoft Corporation | Combination and federation of local and remote speech recognition |
US8346549B2 (en) | 2009-12-04 | 2013-01-01 | At&T Intellectual Property I, L.P. | System and method for supplemental speech recognition by identified idle resources |
US8914289B2 (en) * | 2009-12-16 | 2014-12-16 | Symbol Technologies, Inc. | Analyzing and processing a verbal expression containing multiple goals |
US20110184740A1 (en) * | 2010-01-26 | 2011-07-28 | Google Inc. | Integration of Embedded and Network Speech Recognizers |
US8359865B2 (en) | 2010-02-04 | 2013-01-29 | United Technologies Corporation | Combustor liner segment seal member |
US9305553B2 (en) * | 2010-04-28 | 2016-04-05 | William S. Meisel | Speech recognition accuracy improvement through speaker categories |
US9626429B2 (en) * | 2010-11-10 | 2017-04-18 | Nuance Communications, Inc. | Text entry with word prediction, completion, or correction supplemented by search of shared corpus |
US8898065B2 (en) | 2011-01-07 | 2014-11-25 | Nuance Communications, Inc. | Configurable speech recognition system using multiple recognizers |
US9674328B2 (en) | 2011-02-22 | 2017-06-06 | Speak With Me, Inc. | Hybridized client-server speech recognition |
US9009041B2 (en) * | 2011-07-26 | 2015-04-14 | Nuance Communications, Inc. | Systems and methods for improving the accuracy of a transcription using auxiliary data such as personal data |
US10354650B2 (en) * | 2012-06-26 | 2019-07-16 | Google Llc | Recognizing speech with mixed speech recognition models to generate transcriptions |
US9058805B2 (en) * | 2013-05-13 | 2015-06-16 | Google Inc. | Multiple recognizer speech recognition |
KR102167719B1 (en) * | 2014-12-08 | 2020-10-19 | 삼성전자주식회사 | Method and apparatus for training language model, method and apparatus for recognizing speech |
-
2013
- 2013-03-15 US US13/838,379 patent/US10354650B2/en active Active
- 2013-06-26 CN CN201810238968.6A patent/CN108648750B/en active Active
- 2013-06-26 EP EP13734936.1A patent/EP2864980B1/en active Active
- 2013-06-26 EP EP19168323.4A patent/EP3534364A1/en not_active Withdrawn
- 2013-06-26 CN CN201380041641.5A patent/CN104541325A/en active Pending
- 2013-06-26 WO PCT/US2013/047780 patent/WO2014004612A1/en active Application Filing
-
2018
- 2018-03-06 US US15/912,780 patent/US10847160B2/en active Active
-
2020
- 2020-10-22 US US17/078,030 patent/US11341972B2/en active Active
-
2022
- 2022-05-03 US US17/661,837 patent/US20220262365A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20180197543A1 (en) | 2018-07-12 |
US20130346078A1 (en) | 2013-12-26 |
CN108648750A (en) | 2018-10-12 |
US11341972B2 (en) | 2022-05-24 |
WO2014004612A1 (en) | 2014-01-03 |
CN108648750B (en) | 2023-02-28 |
US20210043212A1 (en) | 2021-02-11 |
US10847160B2 (en) | 2020-11-24 |
EP3534364A1 (en) | 2019-09-04 |
CN104541325A (en) | 2015-04-22 |
EP2864980B1 (en) | 2019-10-23 |
US10354650B2 (en) | 2019-07-16 |
US20220262365A1 (en) | 2022-08-18 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11341972B2 (en) | Speech recognition using two language models | |
US11741970B2 (en) | Determining hotword suitability | |
US11727219B2 (en) | System and method for inferring user intent from speech inputs | |
US9502032B2 (en) | Dynamically biasing language models | |
CN106796788B (en) | Improving automatic speech recognition based on user feedback | |
US8626511B2 (en) | Multi-dimensional disambiguation of voice commands | |
US9293136B2 (en) | Multiple recognizer speech recognition | |
CN111710333B (en) | Method and system for generating speech transcription | |
TWI566107B (en) | Method for processing a multi-part voice command, non-transitory computer readable storage medium and electronic device | |
EP2994908B1 (en) | Incremental speech input interface with real time feedback | |
US8862467B1 (en) | Contextual speech recognition | |
KR102364401B1 (en) | Contextual voice-driven deep bookmarking | |
US11632345B1 (en) | Message management for communal account |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20150114 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
RIN1 | Information on inventor provided before grant (corrected) |
Inventor name: ALEKSIC, PETARInventor name: GRUENSTEIN, ALEXANDER, H. |
|
DAX | Request for extension of the european patent (deleted) | ||
17Q | First examination report despatched |
Effective date: 20160824 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20181121 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAJ | Information related to disapproval of communication of intention to grant by the applicant or resumption of examination proceedings by the epo deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR1 |
|
GRAL | Information related to payment of fee for publishing/printing deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR3 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
INTC | Intention to grant announced (deleted) | ||
GRAR | Information related to intention to grant a patent recorded |
Free format text: ORIGINAL CODE: EPIDOSNIGR71 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
GRAJ | Information related to disapproval of communication of intention to grant by the applicant or resumption of examination proceedings by the epo deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR1 |
|
GRAL | Information related to payment of fee for publishing/printing deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR3 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
GRAR | Information related to intention to grant a patent recorded |
Free format text: ORIGINAL CODE: EPIDOSNIGR71 |
|
INTG | Intention to grant announced |
Effective date: 20190806 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
INTC | Intention to grant announced (deleted) | ||
INTG | Intention to grant announced |
Effective date: 20190911 |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602013061979Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1194556Country of ref document: ATKind code of ref document: TEffective date: 20191115 |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20191023 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200123Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200124Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200123Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200224Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200224Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602013061979Country of ref document: DE |
|
PG2D | Information on lapse in contracting state deleted |
Ref country code: IS |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20200223 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1194556Country of ref document: ATKind code of ref document: TEffective date: 20191023 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
26N | No opposition filed |
Effective date: 20200724 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200626 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20200630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200630Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200626Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20200630 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: MTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20191023 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230508 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230626Year of fee payment: 11Ref country code: DEPayment date: 20230626Year of fee payment: 11 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230627Year of fee payment: 11 |