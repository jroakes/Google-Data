WO2023027701A1 - Structural encoding and attention paradigms for sequence modeling - Google Patents
Structural encoding and attention paradigms for sequence modeling Download PDFInfo
- Publication number
- WO2023027701A1 WO2023027701A1 PCT/US2021/047461 US2021047461W WO2023027701A1 WO 2023027701 A1 WO2023027701 A1 WO 2023027701A1 US 2021047461 W US2021047461 W US 2021047461W WO 2023027701 A1 WO2023027701 A1 WO 2023027701A1
- Authority
- WO
- WIPO (PCT)
- Prior art keywords
- given
- token
- generating
- document
- processing system
- Prior art date
Links
- 238000012545 processing Methods 0.000 claims description 76
- 239000013598 vector Substances 0.000 claims description 75
- 230000015654 memory Effects 0.000 claims description 17
- 238000013528 artificial neural network Methods 0.000 claims description 7
- 238000010276 construction Methods 0.000 claims description 6
- 238000012015 optical character recognition Methods 0.000 claims description 6
- 238000000034 method Methods 0.000 abstract description 18
- 238000005516 engineering process Methods 0.000 description 40
- 230000006870 function Effects 0.000 description 20
- 238000013459 approach Methods 0.000 description 17
- 238000010586 diagram Methods 0.000 description 13
- 238000012549 training Methods 0.000 description 7
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 description 5
- 239000011159 matrix material Substances 0.000 description 5
- 230000002093 peripheral effect Effects 0.000 description 5
- 230000008569 process Effects 0.000 description 5
- 230000009466 transformation Effects 0.000 description 4
- 238000004422 calculation algorithm Methods 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 230000002829 reductive effect Effects 0.000 description 3
- 230000026676 system process Effects 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- YSGQGNQWBLYHPE-CFUSNLFHSA-N (7r,8r,9s,10r,13s,14s,17s)-17-hydroxy-7,13-dimethyl-2,6,7,8,9,10,11,12,14,15,16,17-dodecahydro-1h-cyclopenta[a]phenanthren-3-one Chemical compound C1C[C@]2(C)[C@@H](O)CC[C@H]2[C@@H]2[C@H](C)CC3=CC(=O)CC[C@@H]3[C@H]21 YSGQGNQWBLYHPE-CFUSNLFHSA-N 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 239000000470 constituent Substances 0.000 description 1
- 230000001627 detrimental effect Effects 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 238000010348 incorporation Methods 0.000 description 1
- 230000000670 limiting effect Effects 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000036961 partial effect Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/0895—Weakly supervised learning, e.g. semi-supervised or self-supervised learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
Definitions
- the present technology concerns systems and methods for providing a structure-aware sequence model that can interpret a document’s text without first inferring the proper reading order of the document.
- the model uses a graph convolutional network (“GCN”) to generate contextualized “supertoken” embeddings for each token, and feeds them to a transformer that employs a sparse attention paradigm in which attention weights for at least some supertokens are modified based on differences between predicted and actual values of the order and distance between the attender and attendee supertokens.
- GCN graph convolutional network
- the transformer may use an extended transformer construction (“ETC”) with a sparse global-local attention mechanism, or another model architecture adapted to long-sequences which employs a similar sparse attention paradigm (e.g., BigBird).
- ETC extended transformer construction
- BigBird another model architecture adapted to long-sequences which employs a similar sparse attention paradigm
- GCN-generated supertokens the structure-aware sequence models of the present technology can explicitly preserve local syntactic information that may otherwise be missed in the local attention calculations (e.g., for “long-long” pairings in ETC and BigBird) for a sequence that has not been properly serialized.
- the present technology may reduce both the size of the models needed, and the amount of training required, to obtain (or exceed) state-of-the-art performance.
- the systems and methods disclosed herein may thus be used to enhance the extraction and classification of text from images containing text in non-standard layouts, such as forms, marketing documents, menus, photographs or the like.
- the disclosure describes a processing system comprising: a memory storing a neural network comprising a graph convolutional network and a transformer; and one or more processors coupled to the memory and configured to classify text from a given document, comprising: (a) generating a beta-skeleton graph based on a plurality of tokens, each given token of the plurality of tokens corresponding to a given string of text in the given document, and wherein the beta-skeleton graph comprises, for each given token: (i) a node corresponding to the given token and comprising a vector based on content and location of the given string of text within the given document; and (ii) one or more edges, each edge of the one or more edges linking the node corresponding to the given token to a neighboring node corresponding to another token of the plurality of tokens; (b) generating, using the graph convolutional network, a plurality of supertokens based on the beta-skeleton graph, each given supertoken of the plurality
- generating the plurality of predictions based on the plurality of supertokens using the transformer comprises, for a given attender supertoken and a given attendee supertoken: generating a first prediction regarding how the given attender supertoken and given attendee supertoken should be ordered if the given attender supertoken and given attendee supertoken are related to one another; generating a second prediction regarding how far the given attender supertoken should be from the given attendee supertoken if the given attender supertoken and given attendee supertoken are related to one another; generating a first error value based on the first prediction and a value based on how text corresponding to the given attender supertoken and given attendee supertoken is actually ordered in the given document;
- [0004] generating a second error value based on the second prediction and a value based on how far text corresponding to the given attender supertoken actually is from text corresponding to the given attendee supertoken in the given document; generating a query vector based on the given attender supertoken; generating a key vector based on the given attendee supertoken; generating a first attention score based on the query vector and the key vector; and generating a second attention score based on the first attention score, the first error value, and the second error value.
- the beta-skeleton graph further comprises, for each given token: a given edge embedding corresponding to each given edge of the one or more edges, the given edge embedding being based on a spatial relationship in the given document between the given token and a token corresponding to the neighboring node to which the given edge is linked.
- the transformer is configured to use a sparse global-local attention paradigm.
- the transformer is based on an Extended Transformer Construction architecture.
- the given document comprises an image of a document, and the one or more processors are further configured to identify, for each given token of the plurality of tokens, the content and location of the given string of text in the given document to which the given token corresponds.
- identifying the content and location of the given string of text in the given document comprises using optical character recognition.
- generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions.
- the set of classifications based on the plurality of predictions are BIOES classifications.
- generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions to determine a Viterbi path representing an optimal combination of BIOES types and entity classes that generates the highest overall probability based on the plurality of predictions.
- the disclosure describes a processing system comprising: a memory storing a neural network comprising a transformer; and one or more processors coupled to the memory and configured to classify text from a given document, comprising: (a) generating, using the transformer, a plurality of predictions based on a plurality of tokens, each given token of the plurality of tokens corresponding to a given string of text in the given document, and wherein generating a given prediction of the plurality of predictions for a given attender token and a given attendee token of a plurality of tokens comprises: (i) generating a first prediction regarding how the given attender token and given attendee token should be ordered if the given atender token and given atendee token are related to one another; (ii) generating a second prediction regarding how far the given atender token should be from the given atendee token if the given atender token and given atendee token are related to one another; (iii) generating a first error value based on the
- the neural network further comprises a graph convolutional network
- the one or more processors are further configured to: (c) generate a betaskeleton graph based on the plurality of tokens, wherein the beta-skeleton graph comprises, for each given token of the plurality of tokens: (i) a node corresponding to the given token and comprising a vector based on content and location of the given string of text within the given document; and (ii) one or more edges, each edge of the one or more edges linking the node corresponding to the given token to a neighboring node corresponding to another token of the plurality of tokens; and
- (d) generate, using the graph convolutional network, a plurality of supertokens based on the betaskeleton graph, each given supertoken of the plurality of supertokens being based at least in part on the vector of a given node and the vector of each neighboring node to which the given node is linked via one of its one or more edges; and, for each given prediction of the plurality of predictions that is generated by the transformer, the given attender token and the given attendee token are each a supertoken of the plurality of supertokens.
- the beta-skeleton graph further comprises, for each given token: a given edge embedding corresponding to each given edge of the one or more edges, the given edge embedding being based on a spatial relationship in the given document between the given token and a token corresponding to the neighboring node to which the given edge is linked.
- the transformer is configured to use a sparse global-local attention paradigm.
- the transformer is based on an Extended Transformer Construction architecture.
- the given document comprises an image of a document, and the one or more processors are further configured to identify, for each given token of the plurality of tokens, content and location of the given string of text in the given document to which the given token corresponds.
- identifying the content and location of the given string of text in the given document comprises using optical character recognition.
- generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions.
- the set of classifications based on the plurality of predictions are BIOES classifications.
- generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions to determine a Viterbi path representing an optimal combination of BIOES types and entity classes that generates the highest overall probability based on the plurality of predictions.
- FIG. 1 is a functional diagram of an example system in accordance with aspects of the disclosure.
- FIG. 2 is a functional diagram of an example system in accordance with aspects of the disclosure.
- FIG. 3 is a flow diagram illustrating how a structure-aware sequence model may process an exemplary document, in accordance with aspects of the disclosure.
- FIG. 4 is a flow diagram illustrating how a set of bounding boxes may be processed in order to generate an exemplary beta-skeleton graph, in accordance with aspects of the disclosure.
- FIG. 5 depicts an exemplary method for generating a beta-skeleton graph from a set of bounding boxes, in accordance with aspects of the disclosure.
- FIG. 6 depicts an exemplary portion of a document, a corresponding exemplary beta-skeleton graph, an exemplary serialized version of the text of the document, and an illustration of how local attention scores may be adjusted based on differences between predicted and actual values of the order and distance between the attender and attendee tokens, in accordance with aspects of the disclosure.
- FIG. 7 is a flow diagram illustrating how an attention head of a transformer may employ a “Rich Attention” paradigm, in accordance with aspects of the disclosure.
- FIG. 1 shows a high-level system diagram 100 of an exemplary processing system 102 for performing the methods described herein.
- the processing system 102 may include one or more processors 104 and memory 106 storing instructions 108 and data 110.
- the instructions 108 and data 110 may include any of the structure-aware sequence models described herein.
- the data 110 may store documents and/or training examples to be used in training a sequence model, and/or documents to be used during inference.
- Processing system 102 may be resident on a single computing device.
- processing system 102 may be a server, personal computer, or mobile device, and a sequence model may thus be local to that single computing device.
- processing system 102 may be resident on a cloud computing system or other distributed system.
- a sequence model may be distributed across two or more different physical computing devices.
- the processing system may comprise a first computing device storing the GCN and a second computing device storing the transformer.
- FIG. 2 shows a high-level system diagram 200 in which the exemplary processing system 102 just described is shown in communication with various websites and/or remote storage systems over one or more networks 208, including websites 210 and 218 and remote storage system 226.
- websites 210 and 218 each include one or more servers 212a-212n and 220a-220n, respectively.
- Each of the servers 212a-212n and 220a-220n may have one or more processors (e.g., 214 and 222), and associated memory (e.g., 216 and 224) storing instructions and data, including the content of one or more webpages.
- remote storage system 226 may also include one or more processors and memory storing instructions and data.
- the processing system 102 may be configured to retrieve documents and/or training data from one or more of website 210, website 218, and/or remote storage system 226 to be provided to a sequence model for training or inference.
- the processing systems described herein may be implemented on any type of computing device(s), such as any type of general computing device, server, or set thereof, and may further include other components typically present in general purpose computing devices or servers.
- the memory of such processing systems may be of any non-transitory type capable of storing information accessible by the processor(s) of the processing systems.
- the memory may include a non- transitory medium such as a hard-drive, memory card, optical disk, solid-state, tape memory, or the like.
- Computing devices suitable for the roles described herein may include different combinations of the foregoing, whereby different portions of the instructions and data are stored on different types of media.
- the computing devices described herein may further include any other components normally used in connection with a computing device such as a user interface subsystem.
- the user interface subsystem may include one or more user inputs (e.g., a mouse, keyboard, touch screen and/or microphone) and one or more electronic displays (e.g., a monitor having a screen or any other electrical device that is operable to display information).
- Output devices besides an electronic display, such as speakers, lights, and vibrating, pulsing, or haptic elements, may also be included in the computing devices described herein.
- the one or more processors included in each computing device may be any conventional processors, such as commercially available central processing units (“CPUs”), graphics processing units (“GPUs”), tensor processing units (“TPUs”), etc.
- the one or more processors may be a dedicated device such as an ASIC or other hardware-based processor.
- Each processor may have multiple cores that are able to operate in parallel.
- the processors), memory, and other elements of a single computing device may be stored within a single physical housing, or may be distributed between two or more housings.
- the memory of a computing device may include a hard drive or other storage media located in a housing different from that of the processor(s), such as in an external database or networked storage device. Accordingly, references to a processor or computing device will be understood to include references to a collection of processors or computing devices or memories that may or may not operate in parallel, as well as one or more servers of a load-balanced server farm or cloud-based system.
- the computing devices described herein may store instructions capable of being executed directly (such as machine code) or indirectly (such as scripts) by the processor(s).
- the computing devices may also store data, which may be retrieved, stored, or modified by one or more processors in accordance with the instructions.
- Instructions may be stored as computing device code on a computing device-readable medium.
- the terms “instructions” and “programs” may be used interchangeably herein.
- Instructions may also be stored in object code format for direct processing by the processor(s), or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance.
- the programming language may be C#, C++, JAVA or another computer programming language.
- any components of the instructions or programs may be implemented in a computer scripting language, such as JavaScript, PHP, ASP, or any other computer scripting language.
- any one of these components may be implemented using a combination of computer programming languages and computer scripting languages.
- FIG. 3 shows a flow diagram 300 illustrating how a structure-aware sequence model may process an exemplary document 302, in accordance with aspects of the disclosure.
- the processing system e.g., processing system 102
- the words “Cameras,” “16MP,” and “8MP” are listed on or near the top left comer of the smartphone; the word “OLED” is listed within the smartphone’s screen; the word “IP68” is shown in a box overlapping the right side of the smartphone’s case; the word “5G” is shown in a star overlapping the left side of the smartphone’s case; and the words “USB” and “3.5mm” are listed around the bottom right comer of the smartphone’s case.
- the processing system processes document 302 (e.g., an image of a document comprising pixel data) to obtain a listing of the text on the page, and the location of each word, as shown in layout 304.
- the information represented in layout 304 may be harvested from document 302 in any suitable way.
- the processing system may perform optical character recognition (“OCR”) on document 302 to identify each word, and its respective position on the page.
- OCR optical character recognition
- exemplary layout 304 the words of document 302 have each been replaced with numbered tokens (to through t?).
- the size and position of each word has been represented with corresponding bounding boxes.
- the word “Cameras” is represented in layout 304 as token to and bounding box 306a; “16MP” is represented with token ti and bounding box 306b; “8MP” is represented with token tz and bounding box 306c; “OLED” is represented with token t3 and bounding box 306d; “IP68” is represented with token ft and bounding box 306e; “5G” is represented with token ts and bounding box 306f; “USB” is represented with token to and bounding box 306g; and “3.5mm” is represented with token t?
- the processing system may be configured to use a wordpiece tokenization paradigm (e.g., the multilingual wordpiece tokenization approach employed by BERT-type transformers), in which certain words may be tokenized into one or more constituent wordpieces.
- a wordpiece tokenization paradigm e.g., the multilingual wordpiece tokenization approach employed by BERT-type transformers
- one or more words of document 302 may be broken into multiple wordpiece tokens (e.g., “Cameras” may be tokenized into “Camera” and “##s”, “16MP” may be tokenized into “16” etc.), each of which would have its own smaller bounding box in layout 304.
- each token’s size and position on the page may be represented using the coordinates of a single point (e.g., top left comer of the token on the page) combined with the token’s height and width.
- the size and position of the token may be an estimate.
- the processing system may be configured to take the overall width of the word “16MP” and simply divide that overall width evenly for each character such that the width of the individual bounding boxes for “16” and “MP” are both half of the overall width of “16MP.” This estimation approach may be used even in cases where the lettering in document 302 is not proportional, and thus the bounding box for “16” would in fact overlap the letter “M” on the page.
- the processing system processes layout 304 to generate a beta-skeleton graph 308 in which each token (to-t?) of the layout 304 is represented as a node (310a-310h), and each node is connected to one or more nearby nodes with one or more edges (e.g., 312).
- An exemplary method for generating the nodes and edges of a beta-skeleton graph is set forth below with respect to FIGS. 4 and 5, but any other suitable method for generating a beta-skeleton graph may also be employed.
- Each node 310a-310h of beta-skeleton graph 308 contains a vector (vo-v?) corresponding to the information listed for its corresponding token in layout 304.
- These beta-skeleton vectors vo-v? may be based on any suitable combination of the information contained in layout 304 for their respective tokens.
- the beta-skeleton vector for each token may generated by concatenating a text embedding based on the text of the token, with a spatial embedding based on where the token is located in document 302 (e.g., as represented by the bounding box of layout 304).
- the beta-skeleton vector vo corresponding token to may be generated by concatenating a text embedding based on the word “Cameras” and a spatial embedding based on the bounding box for token to in layout 304.
- the spatial embedding may be based on any suitable representation of the token’s location, including the coordinates of two comers of the bounding box (e.g., top-left and bottom-right corners), the coordinates of one comer combined with the height and width of the bounding box, etc.
- any suitable learned or static embedding function may be used to generate the text embeddings and spatial embeddings on which the beta-skeleton vectors are based.
- the beta-skeleton graph 308 may further include an edge embedding for each of the edges extending from the given node in the beta-skeleton graph.
- edge embeddings may be based on any suitable representation of the spatial relationship between the given node and each neighboring node to which it is linked.
- an edge embedding may comprise the distance between one or more common points in the bounding boxes of the tokens to which nodes A and B correspond (e.g., the distance between the centers of each bounding box, between the top-right comers of each bounding box, between the top-left comers of each bounding box, between the bottom right-comers of each bounding box, between the bottom-left comers of each bounding box, etc.).
- an edge embedding for an edge between node A and B may comprise the shortest total distance, shortest vertical distance, and/or the shortest horizontal distance between the bounding boxes of the tokens to which nodes A and B correspond.
- an edge embedding for an edge between node A and B may comprise the coordinates (or coordinates of a given point and associated height and width) of a larger bounding box that encloses the individual bounding boxes of the tokens to which nodes A and B correspond.
- an edge embedding for an edge between node A and B may comprise the aspect ratio of a larger bounding box that would enclose the individual bounding boxes of the tokens to which nodes A and B correspond.
- an edge embedding for an edge between node A and B may comprise any combination or subcombination of any of the options just described.
- the processing system will feed the betaskeleton graph 308 to a graph convolutional network, GCN 314, to generate a set of supertokens 316 corresponding to each node.
- GCN 314 a graph convolutional network
- the processing system may be configured to provide GCN 314 with both the beta-skeleton graph 308 and layout 304 (or information based thereon representing the tokens in layout 304 and/or their corresponding position on the page), and GCN 314 may be configured to generate the set of supertokens 316 based thereon.
- the supertoken corresponding to a given node will be based on the given node’s beta-skeleton vector, as well as the beta-skeleton vectors of each node to which it is connected in beta-skeleton graph 308.
- the supertoken corresponding to a given node may further be based on the edge embeddings between the given node and each of these neighboring nodes.
- the supertoken STi corresponding to node 310a will be based on beta-skeleton vector vo, as well as the beta-skeleton vectors vi and V2 of its neighboring nodes.
- the supertoken ST i corresponding to node 310a may further be based on the edge embeddings for the edges between nodes 310a and 310c, and between nodes 310a and 310b.
- GCN 314 may be configured to generate the supertoken for a given node based on any suitable way of combining the beta-skeleton vectors for the given node and its neighboring nodes.
- the supertoken for a given node may be generated by concatenating the beta-skeleton vectors for the given node and its neighboring nodes, and then by further processing the resulting concatenated vector using a multilayer perceptron (“MLP”).
- MLP multilayer perceptron
- the GCN 314 may be configured to further process the concatenated vector using a learned embedding function or another type of feed-forward neural network.
- the set of supertokens 316 will be serialized prior to being supplied to transformer 318, and the processing system may be configured to do this using any suitable serialization approach. For example, as shown in FIG. 3, the set of supertokens 316 has been serialized using a left-to-right, top-to-bottom approach based on where each corresponding token is found on layout 304. Likewise, in some aspects of the technology, the set of supertokens may be serialized using a reading order approach.
- the GCN may be configured to serialize the set of supertokens 316 using a right-to-left, top-to-bottom approach based on where each corresponding token is found on layout 304.
- the GCN may be configured to serialize the set of supertokens 316 using a top- to-bottom, right-to-left approach based on where each corresponding token is found on layout 304.
- transformer 318 is configured to employ a sparse attention paradigm, although aspects of the present technology may also be applied to models with transformers that employ traditional (non-sparse) attention paradigms.
- transformer 318 may use an extended transformer construction (“ETC”) with a sparse global- local attention mechanism, or another model architecture adapted to long-sequences which similarly employs a sparse attention paradigm (e.g., BigBird).
- ETC extended transformer construction
- BigBird another model architecture adapted to long-sequences which similarly employs a sparse attention paradigm
- transformer 318 is configured to base local attention scores (e.g., for “long-long” pairings in ETC and BigBird) for a given pair of attender and attendee supertokens at least in part on the difference between predicted and actual values of the order and distance between the attender and attendee supertokens (e.g., using a “Rich Attention” paradigm, as described below with respect to FIG. 7).
- An exemplary method for modifying local attention scores in this way is explained in more detail below with respect to FIG. 7.
- any other suitable manner of using the order and distance between attender and attendee supertokens to influence local attention scores may be employed by transformer 318.
- supertokens may be used to improve transformers that are not otherwise configured to modify local attention scores based on the order and distance between attender and attendee supertokens.
- transformer 318 is configured to output a set of entity BIOES logits 320.
- Each of these entity BIOES logits L0-L7 represents the model’s predictions regarding how likely the corresponding token of layout 304 is to be: (1) the beginning token of a text segment in a recognized entity class; (2) an inside token of a text segment in a recognized entity class; (3) an end token of a text segment in a recognized entity class; (4) a singleton token in a recognized entity class; or (5) an “outside” token that it is not within any recognized entity class.
- the processing system processes the set of entity BIOES logits 320 to determine a final prediction of the most likely BIOES type and entity class for the entire set of tokens harvested from layout 304.
- the processing system is configured to make this determination based on a Viterbi algorithm 322 configured to find the “Viterbi path” representing the optimal combination of BIOES types and entity classes that has the highest overall probability.
- a Viterbi algorithm 322 configured to find the “Viterbi path” representing the optimal combination of BIOES types and entity classes that has the highest overall probability.
- any other suitable dynamic processing algorithm may also be used in this regard.
- the Viterbi algorithm 322’s final predictions are represented as 324a-324h.
- prediction 324a indicates that token to corresponding to the text “Cameras” has been predicted to be a beginning token in Entity Class 2
- prediction 324b indicates that token ti corresponding to the text “16MP” has been predicted to be an inside token in Entity Class 2
- prediction 324c indicates that token t2 corresponding to the text “8MP” has been predicted to be an end token in Entity Class 2
- prediction 324d indicates that token tj corresponding to the text “OLED” has been predicted to be an outside token not belonging to any recognized entity class
- prediction 324e indicates that token t4 corresponding to the text “IP68” has been predicted to be an outside token not belonging to any recognized entity class
- prediction 324f indicates that token ts corresponding to the text “5G” has been predicted to be a singleton token in Entity Class 5
- prediction 324g indicates that token t&
- FIGS. 4 and 5 set forth a flow diagram 400 and exemplary method 500 showing how a set of bounding boxes A-D may be processed in order to generate an exemplary beta-skeleton graph.
- the processing system e.g., processing system 102 identifies a set of peripheral points at a preset spacing along the periphery of each bounding box, and a set of internal points along a longitudinal midline of each bounding box.
- FIG. 4 provides an illustrative example of how this might appear.
- the periphery of bounding box A is identified with reference numeral 404
- one of the identified peripheral points of bounding box A is identified with reference numeral 406, and one of the internal points of bounding box A is identified with reference numeral 408.
- step 504 of FIG. 5 the processing system generates a Delaunay triangulation graph based on the set of the points identified in step 502.
- FIG. 4 does not show this intermediate step, as will be understood, a Delaunay triangulation graph based on block 402 of FIG. 4 would result in the points of bounding boxes A-D being interconnected such that no point will fall within the circumcircle of any triangle of the triangulation graph.
- the processing system may use any such approach.
- step 506 of FIG. 5 the processing system identifies all “inside points” that are inside the periphery of at least one of the bounding boxes. This will include not only the internal points that were originally identified in step 502, but also any peripheral points of one bounding box that may be inside the periphery of another bounding box by virtue of those two bounding boxes overlapping. For example, looking at block 402 of FIG. 4, point 410 will initially be identified in step 502 as being a peripheral point of bounding box B. However, point 410 will later be identified in step 506 as being an “inside point” by virtue of the fact that it falls within the periphery of bounding box A. All such inside points are shown in a light gray in block 402 (as well as in blocks 412 and 416).
- the processing system may be configured to identify these “inside points” in any suitable way. For example, in some aspects of the technology, the processing system may be configured to traverse each “edge” (link) of the Delaunay triangulation graph that happens to start with a peripheral point of a given bounding box and extends inside of the given bounding box, as the end points of any such edges will by definition be points within one of the bounding boxes.
- edge link
- step 508 of FIG. 5 the processing system identifies a set of every edge of the Delaunay triangulation graph for which: (1) the edge’s vertices vl and v2 are not “inside points”; and (2) the circle with vlv2 as its diameter does not cover any other point.
- Block 412 of FIG. 4 provides an illustrative example of how this might appear, with edges 414 and 416 representing two of the 16 edges that would be identified according to step 508. As can be seen, there are no edges in block 412 between bounding boxes B and C, or between bounding boxes A and D, as every possible connection between the points of those bounding boxes would end up creating circles which overlap at least one other point.
- the processing system may be configured to identify these edges in any suitable way. For example, based on the fact that the point closest to vlv2 will always be a neighbor of either vl or v2 (based on the properties of Delaunay triangulation graphs), the processing system may be configured to determine whether a circle with diameter vlv2 will cover any points by simply checking whether the neighbors of vl and v2 will fall within the circle with vlv2 as its diameter.
- step 510 of FIG. 5 for every pair of intersecting bounding boxes, the processing system adds an artificial edge of length 0 to the set of edges identified in step 508. In the example of FIG. 4, such an edge would be identified between intersecting bounding boxes A and B. However, since the edge is artificial (in that it doesn’t actually correspond to an edge of the Delaunay triangulation graph), it has not been visually represented in block 412.
- step 510 follows step 508 in exemplary method 500, it will be understood that the order of these processes may also be reversed.
- the processing system may first initialize the set of edges by adding 0-length edges for all intersecting bounding boxes, and then augment that set of edges with those identified from the beta-skeleton graph as discussed above with respect to step 508.
- step 512 of FIG. 5 the processing system identifies the shortest edge between each pair of bounding boxes from within the set of edges identified in steps 508 and 510.
- Block 418 of FIG. 4 provides an illustrative example of how this might appear, with edge 420a being the shortest edge between bounding boxes A and C, edge 420b being the shortest edge between bounding boxes B and D, and edge 420c being the shortest edge between bounding boxes C and D.
- block 412 shows an edge 416 between bounding boxes A and B, the edge with the shortest distance between these boxes will be the artificial edge of length 0 that was identified in step 510. Although this artificial edge is not visually represented in block 418, it is represented in the resulting beta-skeleton graph shown in block 422.
- step 514 of FIG. 5 the processing system generates a beta-skeleton graph comprising a node for each bounding box, and each of the edges identified in step 512.
- Block 422 of FIG. 4 provides an illustrative example of how this might appear, showing an exemplary beta-skeleton graph in which bounding box A is represented with node 424a, bounding box B is represented with node 424b, bounding box C is represented with node 424c, and bounding box D is represented with node 424d.
- nodes 424a and 424c are connected with edge 420a (as also shown in block 418)
- nodes 424c and 424d are connected with edge 420c (as also shown in block 418)
- nodes 424d and 424b are connected with edge 420b (as also shown in block 418)
- nodes 424a and 424b are connected with edge 426.
- a length for each edge of the beta-skeleton graph is shown next to each edge.
- edge 420a has a length of 1
- edge 420b has a length of 2.5
- edge 420c has a length of 1.1
- edge 426 has a length of 0 (since it is an artificial edge identified based on the overlapping of bounding boxes A and B).
- edge lengths are shown pictorially for purposes of illustration, in practice they may simply be encoded into the data structure representing the beta-skeleton graph, for example as part of edge embeddings.
- FIG. 6 depicts an exemplary portion of a document, a corresponding exemplary beta-skeleton graph, an exemplary serialized version of the text of the document, and an illustration of how local attention scores may be adjusted based on differences between predicted and actual values of the order and distance between the attender and attendee tokens, in accordance with aspects of the disclosure.
- box 602 depicts a portion of original text as it would appear in a given exemplary document
- box 604 depicts an exemplary beta-skeleton graph overlaying that original text.
- This betaskeleton graph may be generated as described above with respect to FIGS. 3-5.
- Box 616 depicts a serialized version of the text of box 602.
- the original text has been serialized using a simple left-to-right, top-to-bottom approach.
- box 616 helps to illustrate some of the issues that can arise from imperfect serialization.
- the serialization approach shown in box 616 results in the column headings “TAR,” “NIC,” “MOIST,” and “MENT” immediately preceding the first word of the second line (“KOOL”), despite the column headings being very far from “KOOL” in the original text shown in box 602.
- the column headings are all at least five words removed from the values “9.1,” “.88,” “14.0,” and “.474,” despite those values being listed directly beneath each of the column headings in the original text shown in box 602.
- this serialization approach results in “tip-” being immediately followed by the values “9.1,” “.88,” “14.0,” and “.474,” despite there being a substantial gap between them in the original text shown in box 602.
- Box 618 illustrates how these types of serializing issues can present challenges for a transformer that employs a sparse global- local attention paradigm.
- box 618 also illustrates how these issues may be mitigated by basing local attention scores (e.g., for “long-long” pairings in ETC and BigBird) at least in part on the difference between predicted and actual values of the order and distance between the atender and attendee tokens (e.g., using a “Rich Attention” paradigm, as described below with respect to FIG. 7).
- window 620a represents an exemplary three-word local attention radius around an attender token for the word “white.”
- this local attention radius will result in the transformer generating local attention weights between the token for “white” and the tokens for “KOOL,” “Lights,” KS,” “tip-,” “9.1,” and “.88.”
- this three-word local attention radius will prevent the transformer from assessing attention between “white” and the tokens for “ping” and “masked,” even though those words are within three words of “white” according to the proper reading order of the original document (as shown in box 602).
- Windows 620b-620d illustrate how attention scores for the tokens within window 620a may be modified using “Rich Attention” or a similar order- and distance-based adjustment paradigm.
- the transformer may be configured to generate predictions for each pair of attender and attendee tokens which represent the “ideal” order that those tokens would be in relative to one another, and the “ideal” distance those tokens would be from one another, if it is assumed that the attender and attendee tokens do indeed relate to each other in some way. In FIG. 7, these predictions are made by learned parametric functions 708 and 712, but the transformer may generate “ideal” order and distance predictions in any suitable way (e.g., blocks 720 and 724 of FIG. 7).
- the transformer may then compare the actual order of the attender and attendee token (e.g., block 718 of FIG. 7) to the predicted “ideal” order to generate a first error value (e.g., block 730 of FIG. 7), and may compare the actual distance between the attender and attendee token (e.g., block 722 of FIG. 7) to the predicted “ideal” distance to generate a second error value (e.g., block 732 of FIG. 7). These error values may then be used to modify an initial pre-SoftMax attention score that the transformer generates between the attender and attendee token based on a typical query-key approach.
- the transformer may use the error values to penalize the attention scores of those token pairs that have an actual order and/or distance that is different than their “ideal” order and/or distance, thus tending to decrease unwarranted attention between tokens that are in fact out of order and/or spatially distant from one another in the original document.
- the transformer may instead (or in addition) be configured to amplify the attention scores of those token pairs that have an actual order and/or distance that is close to their “ideal” order and/or distance, thus tending to increase warranted atention between tokens that are in fact close and/or logically ordered in the original document.
- the transformers of the present technology may be configured to accept supertokens as their initial input, spatial and semantic information regarding a given token’s neighbors will automatically be weighed in the first layer of the transformer’s atention mechanism even where imperfect serializing and/or the size of the local atention radius would prevent atention from being directly assessed between a given token and one or more of its neighboring tokens from the beta-skeleton graph.
- the transformer will end up implicitly assessing attention between “white” and “ping” because the betaskeleton vector for “ping” will propagate into the supertoken for “KOOL” through edge 606.
- the transformer will end up implicitly assessing attention between “white” and “masked” because the beta-skeleton vector for “masked” will propagate into the supertokens for “KOOL,” “Lights,” and “KS” through edges 608, 610, and 612, respectively.
- FIGS. 3 and 7 both involve a transformer that is configured to both take supertokens as input, and to adjust local attention scores based on the order and distance between attender and attendee supertokens (as discussed above and below), these features may also be used separately.
- a combined approach as shown in FIGS. 3 and 7 may provide the biggest improvements over sequence models based on transformers with standard global-local sparse attention paradigms, significant benefits may be possible from using only supertokens, or only the attention-adjustment paradigms discussed herein.
- FIG. 7 shows a flow diagram 700 illustrating how an attention head of a transformer may employ a “Rich Attention” paradigm, in accordance with aspects of the disclosure.
- the term “Rich Attention” is not meant to identify a paradigm that is an alternative to sparse attention, nor are “Rich Attention” and sparse attention mutually exclusive. Rather, the term “rich” in “Rich Attention” is simply meant to indicate that the attention score between a given attender and attendee token will be influenced or “enriched” based on how closely the actual order and distance between those supertokens matches a predicted “ideal” order and distance for those supertokens.
- Ratich Attention can be used to enrich any type of attention score, including local attention scores calculated according to a sparse global-local attention paradigm (e.g., as used in ETC and BigBird transformers), as well as attention scores calculated according to standard attention paradigms (e.g., as used in BERT transformers).
- a sparse global-local attention paradigm e.g., as used in ETC and BigBird transformers
- attention scores calculated according to standard attention paradigms e.g., as used in BERT transformers.
- the input to the attention head are two vectors h ; (box 702) and h 7 (box 704).
- vectors h, and h 7 will be the supertokens for two nodes i and j of a beta-skeleton graph generated from an input document, as discussed above with respect to FIGS. 3-5.
- vectors h ; and h 7 will be based on outputs of the previous layer.
- the elements of flow diagram 700 will each be described below using the assumption that processing is taking place in the first layer of the transformer, and thus that vectors h, and h 7 are the supertokens for nodes i and j.
- vector h is the attender and vector h 7 is the attendee.
- the transformer is configured to adjust attention scores based on how closely the actual order and distance between those supertokens matches a predicted “ideal” order and distance for those supertokens.
- the transformer will generate four values: (1) an actual order value oy representing an actual order between tokens i and j (box 718); (2) an “ideal” order value p v representing the model’s prediction of what order tokens i and j should be in if that they are related to one another (box 720); (3) an actual distance value dy representing an actual distance between tokens i and j (box 722); and (4) an “ideal” distance value representing the model’s prediction of what the distance should be between tokens i and j if they are related to one another (box 724).
- each of these values will be calculated based at least in part on h, and h 7 .
- the individual details of functions 706, 708, 710, and 712 will be discussed further below.
- the actual order value o (box 718) is based on the indices i and j of vectors h ; and h 7 Specifically, as shown in box 706, the transformer will compare i to j, and will assign a value of 1 if i is greater than j, and will otherwise assign a value of 0. Any suitable manner of indexing h, and h 7 may be used in this regard.
- indices i and j may be the coordinates of bounding boxes surrounding tokens i and j, coordinates of the center points of the bounding boxes surrounding tokens i and j, coordinates of a common corner of the bounding boxes surrounding tokens i and j, etc.
- the actual order value o may represent the order of tokens i and j along a single axis or path (e.g., along the x-axis of the document, along the y-axis of the document, along a straight or curved line corresponding to the text direction in the relevant portion of the document, etc.).
- the actual order value oy may represent the order of tokens i and j along two axes (e.g., oy may be a vector with values corresponding to how i and j are ordered in the x-axis, and in the y-axis).
- the ideal order value p (box 720) is generated based on vectors h ; and h 7 using a learned parametric function 708.
- Any suitable parametric function may be used in this regard.
- the transformer may use a sigmoid classifier trained to predict the probability of the ordering of tokens, such as shown in Equation 1 below:
- affine (0 x) represents W ⁇ + where is a weight matrix of free parameters, and b (o) is a bias vector composed of free parameters.
- the free parameters of W (o) and b (o) may be randomly initialized and then updated as the sequence model is trained.
- the sigmoid classifier may be trained to minimize cross-entropy loss, or may be trained using any other suitable loss function.
- the actual distance value dy (box 722) is also based on the indices i and j of vectors h ; and h 7 Specifically, as shown in box 710, the transformer will find the absolute value of i minus j. As mentioned above, any suitable manner of indexing and comparing h, and h 7 may be used in this regard. Thus, in some aspects of the technology, the actual distance value d may be based on distance measured along a single axis or path (e.g., along the x-axis of the document, along the y-axis of the document, along a straight or curved line corresponding to the text direction in the relevant portion of the document, etc.).
- the actual distance value dy may be based on distances measured along two axes (e.g., d may be a vector with values corresponding to how far token i is from token j along both the x-axis and the y-axis, or may be a value representing the absolute straight-line distance between token i and token j).
- the ideal distance value (box 724) is generated based on vectors h, and h 7 using another learned parametric function 712.
- any suitable parametric function may be used in this regard.
- the transformer may use an affine transformation, such as shown in Equation 2 below:
- affine ⁇ ( ) represents W ⁇ + b ® where is a weight matrix of free parameters, and b (d) is a bias vector composed of free parameters.
- the free parameters of W (d) and b (d) may also be randomly initialized and then updated as the sequence model is trained.
- the affine transformation may be trained to minimize an L2 loss, or may be trained using any other suitable loss function.
- negative error value s (o) may be calculated using a negative sigmoid cross entropy loss, such as shown in Equation 3 below.
- negative error value s (o) may be calculated using any negative log-likelihood function or other suitable equation that is likewise based on a comparison of, or difference between, the actual order value oy and the ideal order value py.
- negative error value s (d) may be calculated using a scaled negative L2 loss, such as shown in Equation 4 below.
- Equation 4 the variable t represents a free parameter that will be randomly initialized and updated as the sequence model is trained.
- negative error value s (d) may be calculated using any negative log-likelihood function or other suitable equation which is based on a comparison of, or difference between, the actual distance value dy and ideal distance value p .
- t may be a variable value.
- t may be the output of another affine transformation that takes vectors h, and h 7 as input.
- vectors h, and h 7 would be initially provided to parametric functions 714 and 716 to generate the query and key vectors q, and k,, and then q, and k, would be fed to functions 706, 708, 710, and 712 in order to generate the actual order value o , ideal order value p , actual distance value dy, and ideal distance value p .
- the transformer will calculate a dot product of the transpose of query vector q ; and the key vector k,.
- the resulting initial pre-SoftMax attention score will be added to the negative error values 730 and 732 to generate an adjusted pre-SoftMax attention score, as shown in the circle labeled with reference numeral 736. That adjusted pre-SoftMax attention score will then be provided to SoftMax Function 738.
- vector h 7 is provided to a learned parametric function 740 to generate a value vector V 7 (box 742).
- the post-SoftMax attention score produced by SoftMax Function 738 is then multiplied by the value vector V 7 as shown in box 744, resulting in an updated vector h ; as shown in box 746.
- this updated vector h may then be used in successive layers of the transformer consistent with standard transformer architecture.
- the outputs of the last layer of the transformer will be used to generate the BIOES logits described above with respect to FIG. 3.
- the structure-aware sequence models of the present technology may be trained in any suitable way.
- a structure-aware sequence model may be pretrained using one more sets of masked-language modeling tasks, and/or next-sentence prediction tasks, and may be fine-tuned using one or more annotated sets of training examples (e.g., the public FUNSD and/or CORD datasets).
- pretraining may not be deemed necessary, and the structure-aware sequence model may be trained from scratch using only annotated sets of training examples.
Abstract
Systems and methods for providing a structure-aware sequence model that can interpret a document's text without first inferring the proper reading order of the document. In some examples, the model may use a graph convolutional network to generate contextualized "supertoken" embeddings for each token, which are then fed to a transformer that employs a sparse attention paradigm in which attention weights for at least some supertokens are modified based on differences between predicted and actual values of the order and distance between the attender and attendee supertokens.
Description
STRUCTURAL ENCODING AND ATTENTION PARADIGMS FOR SEQUENCE MODELING
BACKGROUND
[0001] Advances in natural language processing and sequence modeling continue to improve the ability of language models to parse and understand information gathered from different types of documents. However, to meaningfully learn from any document, the model must either know or be able to accurately infer the order of (or “serialize”) the words in the document. For a simple document consisting of only a single block of text, properly serializing the text may only require a “left-to-right, top-to bottom” approach in which each word is collected by moving from left to right across the first line, and then moving down to the next line. However, for documents with more complicated forms (e.g., marketing documents; advertisements; menus; photographs of signs; documents where text is organized into columns and/or tables; documents where text is broken up and/or wrapped around pictures), properly serializing the text can be more challenging, and may thus adversely impact the language model’s ability to draw conclusions and derive meaningful information from that text.
BRIEF SUMMARY
[0002] The present technology concerns systems and methods for providing a structure-aware sequence model that can interpret a document’s text without first inferring the proper reading order of the document. In some aspects of the technology, the model uses a graph convolutional network (“GCN”) to generate contextualized “supertoken” embeddings for each token, and feeds them to a transformer that employs a sparse attention paradigm in which attention weights for at least some supertokens are modified based on differences between predicted and actual values of the order and distance between the attender and attendee supertokens. In some aspects of the technology, the transformer may use an extended transformer construction (“ETC”) with a sparse global-local attention mechanism, or another model architecture adapted to long-sequences which employs a similar sparse attention paradigm (e.g., BigBird). Through the incorporation of GCN-generated supertokens, the structure-aware sequence models of the present technology can explicitly preserve local syntactic information that may otherwise be missed in the local attention calculations (e.g., for “long-long” pairings in ETC and BigBird) for a sequence that has not been properly serialized. In addition, by removing the need for the sequence model to correctly infer the reading layout of the input document, the present technology may reduce both the size of the models needed, and the amount of training required, to obtain (or exceed) state-of-the-art performance. The systems and methods disclosed herein may thus be used to enhance the extraction and classification of text from images containing text in non-standard layouts, such as forms, marketing documents, menus, photographs or the like.
[0003] In one aspect, the disclosure describes a processing system comprising: a memory storing a neural network comprising a graph convolutional network and a transformer; and one or more processors coupled to the memory and configured to classify text from a given document, comprising: (a) generating a beta-skeleton graph based on a plurality of tokens, each given token of the plurality of tokens corresponding to a given string of text in the given document, and wherein the beta-skeleton graph comprises, for each given token: (i) a node corresponding to the given token and comprising a vector based on content and location of the given string of text within the given document; and (ii) one or more edges, each edge of the one or more edges linking the node corresponding to the given token to a neighboring node corresponding to another token of the plurality of tokens; (b) generating, using the graph convolutional network, a plurality of supertokens based on the beta-skeleton graph, each given supertoken of the plurality of supertokens being based at least in part on the vector of a given node and the vector of each neighboring node to which the given node is linked via one of its one or more edges;
(c) generating, using the transformer, a plurality of predictions based on the plurality of supertokens; and
(d) generating a set of classifications based on the plurality of predictions, the set of classifications identifying at least one entity class corresponding to at least one token of the plurality of tokens. In some aspects, generating the plurality of predictions based on the plurality of supertokens using the transformer comprises, for a given attender supertoken and a given attendee supertoken: generating a first prediction regarding how the given attender supertoken and given attendee supertoken should be ordered if the given attender supertoken and given attendee supertoken are related to one another; generating a second prediction regarding how far the given attender supertoken should be from the given attendee supertoken if the given attender supertoken and given attendee supertoken are related to one another; generating a first error value based on the first prediction and a value based on how text corresponding to the given attender supertoken and given attendee supertoken is actually ordered in the given document;
[0004] generating a second error value based on the second prediction and a value based on how far text corresponding to the given attender supertoken actually is from text corresponding to the given attendee supertoken in the given document; generating a query vector based on the given attender supertoken; generating a key vector based on the given attendee supertoken; generating a first attention score based on the query vector and the key vector; and generating a second attention score based on the first attention score, the first error value, and the second error value. In some aspects, the beta-skeleton graph further comprises, for each given token: a given edge embedding corresponding to each given edge of the one or more edges, the given edge embedding being based on a spatial relationship in the given document between the given token and a token corresponding to the neighboring node to which the given edge is linked. In some aspects, the transformer is configured to use a sparse global-local attention paradigm. In
some aspects, the transformer is based on an Extended Transformer Construction architecture. In some aspects, the given document comprises an image of a document, and the one or more processors are further configured to identify, for each given token of the plurality of tokens, the content and location of the given string of text in the given document to which the given token corresponds. In some aspects, identifying the content and location of the given string of text in the given document comprises using optical character recognition. In some aspects, generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions. In some aspects, the set of classifications based on the plurality of predictions are BIOES classifications. In some aspects, generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions to determine a Viterbi path representing an optimal combination of BIOES types and entity classes that generates the highest overall probability based on the plurality of predictions.
[0005] In another aspect, the disclosure describes a processing system comprising: a memory storing a neural network comprising a transformer; and one or more processors coupled to the memory and configured to classify text from a given document, comprising: (a) generating, using the transformer, a plurality of predictions based on a plurality of tokens, each given token of the plurality of tokens corresponding to a given string of text in the given document, and wherein generating a given prediction of the plurality of predictions for a given attender token and a given attendee token of a plurality of tokens comprises: (i) generating a first prediction regarding how the given attender token and given attendee token should be ordered if the given atender token and given atendee token are related to one another; (ii) generating a second prediction regarding how far the given atender token should be from the given atendee token if the given atender token and given atendee token are related to one another; (iii) generating a first error value based on the first prediction and a value based on how the text corresponding to the given atender token and given attendee token is actually ordered in the given document; (iv) generating a second error value based on the second prediction and a value based on how far the text corresponding to the given atender token actually is from the text corresponding to the given atendee token in the given document; (v) generating a query vector based on the given atender token; (vi) generating a key vector based on the given atendee token; (vii) generating a first atention score based on the query vector and the key vector; (viii) generating a second atention score based on the first atention score, the first error value, and the second error value; and (ix) generating the given prediction based at least in part on the second atention score; and (b) generating a set of classifications based on the plurality of predictions, the set of classifications identifying at least one entity class corresponding to at least one token of the plurality of tokens. In some aspects, the neural network further comprises a graph
convolutional network, and the one or more processors are further configured to: (c) generate a betaskeleton graph based on the plurality of tokens, wherein the beta-skeleton graph comprises, for each given token of the plurality of tokens: (i) a node corresponding to the given token and comprising a vector based on content and location of the given string of text within the given document; and (ii) one or more edges, each edge of the one or more edges linking the node corresponding to the given token to a neighboring node corresponding to another token of the plurality of tokens; and
[0006] (d) generate, using the graph convolutional network, a plurality of supertokens based on the betaskeleton graph, each given supertoken of the plurality of supertokens being based at least in part on the vector of a given node and the vector of each neighboring node to which the given node is linked via one of its one or more edges; and, for each given prediction of the plurality of predictions that is generated by the transformer, the given attender token and the given attendee token are each a supertoken of the plurality of supertokens. In some aspects, the beta-skeleton graph further comprises, for each given token: a given edge embedding corresponding to each given edge of the one or more edges, the given edge embedding being based on a spatial relationship in the given document between the given token and a token corresponding to the neighboring node to which the given edge is linked. In some aspects, the transformer is configured to use a sparse global-local attention paradigm. In some aspects, the transformer is based on an Extended Transformer Construction architecture. In some aspects, the given document comprises an image of a document, and the one or more processors are further configured to identify, for each given token of the plurality of tokens, content and location of the given string of text in the given document to which the given token corresponds. In some aspects, identifying the content and location of the given string of text in the given document comprises using optical character recognition. In some aspects, generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions. In some aspects, the set of classifications based on the plurality of predictions are BIOES classifications. In some aspects, generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions to determine a Viterbi path representing an optimal combination of BIOES types and entity classes that generates the highest overall probability based on the plurality of predictions.
BRIEF DESCRIPTION OF THE DRAWINGS
[0007] FIG. 1 is a functional diagram of an example system in accordance with aspects of the disclosure. [0008] FIG. 2 is a functional diagram of an example system in accordance with aspects of the disclosure. [0009] FIG. 3 is a flow diagram illustrating how a structure-aware sequence model may process an exemplary document, in accordance with aspects of the disclosure.
[0010] FIG. 4 is a flow diagram illustrating how a set of bounding boxes may be processed in order to generate an exemplary beta-skeleton graph, in accordance with aspects of the disclosure.
[0011] FIG. 5 depicts an exemplary method for generating a beta-skeleton graph from a set of bounding boxes, in accordance with aspects of the disclosure.
[0012] FIG. 6 depicts an exemplary portion of a document, a corresponding exemplary beta-skeleton graph, an exemplary serialized version of the text of the document, and an illustration of how local attention scores may be adjusted based on differences between predicted and actual values of the order and distance between the attender and attendee tokens, in accordance with aspects of the disclosure.
[0013] FIG. 7 is a flow diagram illustrating how an attention head of a transformer may employ a “Rich Attention” paradigm, in accordance with aspects of the disclosure.
DETAILED DESCRIPTION
[0014] The present technology will now be described with respect to the following exemplary systems and methods.
Example Systems
[0015] FIG. 1 shows a high-level system diagram 100 of an exemplary processing system 102 for performing the methods described herein. The processing system 102 may include one or more processors 104 and memory 106 storing instructions 108 and data 110. The instructions 108 and data 110 may include any of the structure-aware sequence models described herein. In addition, the data 110 may store documents and/or training examples to be used in training a sequence model, and/or documents to be used during inference. Processing system 102 may be resident on a single computing device. For example, processing system 102 may be a server, personal computer, or mobile device, and a sequence model may thus be local to that single computing device. Similarly, processing system 102 may be resident on a cloud computing system or other distributed system. In such a case, a sequence model may be distributed across two or more different physical computing devices. For example, in some aspects of the technology, where a sequence model includes both a GCN and a transformer (as discussed further below), the processing system may comprise a first computing device storing the GCN and a second computing device storing the transformer.
[0016] Further in this regard, FIG. 2 shows a high-level system diagram 200 in which the exemplary processing system 102 just described is shown in communication with various websites and/or remote storage systems over one or more networks 208, including websites 210 and 218 and remote storage system 226. In this example, websites 210 and 218 each include one or more servers 212a-212n and 220a-220n, respectively. Each of the servers 212a-212n and 220a-220n may have one or more processors (e.g., 214 and 222), and associated memory (e.g., 216 and 224) storing instructions and data, including
the content of one or more webpages. Likewise, although not shown, remote storage system 226 may also include one or more processors and memory storing instructions and data. In some aspects of the technology, the processing system 102 may be configured to retrieve documents and/or training data from one or more of website 210, website 218, and/or remote storage system 226 to be provided to a sequence model for training or inference.
[0017] The processing systems described herein may be implemented on any type of computing device(s), such as any type of general computing device, server, or set thereof, and may further include other components typically present in general purpose computing devices or servers. Likewise, the memory of such processing systems may be of any non-transitory type capable of storing information accessible by the processor(s) of the processing systems. For instance, the memory may include a non- transitory medium such as a hard-drive, memory card, optical disk, solid-state, tape memory, or the like. Computing devices suitable for the roles described herein may include different combinations of the foregoing, whereby different portions of the instructions and data are stored on different types of media.
[0018] In all cases, the computing devices described herein may further include any other components normally used in connection with a computing device such as a user interface subsystem. The user interface subsystem may include one or more user inputs (e.g., a mouse, keyboard, touch screen and/or microphone) and one or more electronic displays (e.g., a monitor having a screen or any other electrical device that is operable to display information). Output devices besides an electronic display, such as speakers, lights, and vibrating, pulsing, or haptic elements, may also be included in the computing devices described herein.
[0019] The one or more processors included in each computing device may be any conventional processors, such as commercially available central processing units (“CPUs”), graphics processing units (“GPUs”), tensor processing units (“TPUs”), etc. Alternatively, the one or more processors may be a dedicated device such as an ASIC or other hardware-based processor. Each processor may have multiple cores that are able to operate in parallel. The processors), memory, and other elements of a single computing device may be stored within a single physical housing, or may be distributed between two or more housings. Similarly, the memory of a computing device may include a hard drive or other storage media located in a housing different from that of the processor(s), such as in an external database or networked storage device. Accordingly, references to a processor or computing device will be understood to include references to a collection of processors or computing devices or memories that may or may not operate in parallel, as well as one or more servers of a load-balanced server farm or cloud-based system.
[0020] The computing devices described herein may store instructions capable of being executed directly (such as machine code) or indirectly (such as scripts) by the processor(s). The computing devices may
also store data, which may be retrieved, stored, or modified by one or more processors in accordance with the instructions. Instructions may be stored as computing device code on a computing device-readable medium. In that regard, the terms “instructions” and “programs” may be used interchangeably herein. Instructions may also be stored in object code format for direct processing by the processor(s), or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. By way of example, the programming language may be C#, C++, JAVA or another computer programming language. Similarly, any components of the instructions or programs may be implemented in a computer scripting language, such as JavaScript, PHP, ASP, or any other computer scripting language. Furthermore, any one of these components may be implemented using a combination of computer programming languages and computer scripting languages.
Example Methods
[0021] FIG. 3 shows a flow diagram 300 illustrating how a structure-aware sequence model may process an exemplary document 302, in accordance with aspects of the disclosure. In that regard, in the example of FIG. 3, it is assumed that the processing system (e.g., processing system 102) receives a document 302 depicting a smart phone with various features highlighted in text. Specifically, the words “Cameras,” “16MP,” and “8MP” are listed on or near the top left comer of the smartphone; the word “OLED” is listed within the smartphone’s screen; the word “IP68” is shown in a box overlapping the right side of the smartphone’s case; the word “5G” is shown in a star overlapping the left side of the smartphone’s case; and the words “USB” and “3.5mm” are listed around the bottom right comer of the smartphone’s case.
[0022] In this example, the processing system processes document 302 (e.g., an image of a document comprising pixel data) to obtain a listing of the text on the page, and the location of each word, as shown in layout 304. The information represented in layout 304 may be harvested from document 302 in any suitable way. For example, in some aspects of the technology, the processing system may perform optical character recognition (“OCR”) on document 302 to identify each word, and its respective position on the page.
[0023] In exemplary layout 304, the words of document 302 have each been replaced with numbered tokens (to through t?). In addition, the size and position of each word has been represented with corresponding bounding boxes. Thus, the word “Cameras” is represented in layout 304 as token to and bounding box 306a; “16MP” is represented with token ti and bounding box 306b; “8MP” is represented with token tz and bounding box 306c; “OLED” is represented with token t3 and bounding box 306d; “IP68” is represented with token ft and bounding box 306e; “5G” is represented with token ts and bounding box 306f; “USB” is represented with token to and bounding box 306g; and “3.5mm” is
represented with token t? and bounding box 306h. In this example, it has been assumed for the sake of illustration that each of the words of document 302 will translate to a single token. However, in some aspects of the technology, the processing system may be configured to use a wordpiece tokenization paradigm (e.g., the multilingual wordpiece tokenization approach employed by BERT-type transformers), in which certain words may be tokenized into one or more constituent wordpieces. In such a case, one or more words of document 302 may be broken into multiple wordpiece tokens (e.g., “Cameras” may be tokenized into “Camera” and “##s”, “16MP” may be tokenized into “16”
[0024] Likewise, while the exemplary layout 304 shows each token having a bounding box, any other suitable approach may be used to record the size and position of each token. For example, in some aspects of the technology, each token’s size and position on the page may be represented using the coordinates of a single point (e.g., top left comer of the token on the page) combined with the token’s height and width. Likewise, in some aspects of the technology, the size and position of the token may be an estimate. For example, if “16MP” is tokenized into “16” and “##MP” (“##” being a suffix token identifier), the processing system may be configured to take the overall width of the word “16MP” and simply divide that overall width evenly for each character such that the width of the individual bounding boxes for “16” and “MP” are both half of the overall width of “16MP.” This estimation approach may be used even in cases where the lettering in document 302 is not proportional, and thus the bounding box for “16” would in fact overlap the letter “M” on the page.
[0025] The processing system processes layout 304 to generate a beta-skeleton graph 308 in which each token (to-t?) of the layout 304 is represented as a node (310a-310h), and each node is connected to one or more nearby nodes with one or more edges (e.g., 312). An exemplary method for generating the nodes and edges of a beta-skeleton graph is set forth below with respect to FIGS. 4 and 5, but any other suitable method for generating a beta-skeleton graph may also be employed.
[0026] Each node 310a-310h of beta-skeleton graph 308 contains a vector (vo-v?) corresponding to the information listed for its corresponding token in layout 304. These beta-skeleton vectors vo-v? may be based on any suitable combination of the information contained in layout 304 for their respective tokens. For example, the beta-skeleton vector for each token may generated by concatenating a text embedding based on the text of the token, with a spatial embedding based on where the token is located in document 302 (e.g., as represented by the bounding box of layout 304). Thus, assuming that a full-word tokenization paradigm is used such that the text of token to is “Cameras,” the beta-skeleton vector vo corresponding token to may be generated by concatenating a text embedding based on the word “Cameras” and a spatial embedding based on the bounding box for token to in layout 304. Here as well,
the spatial embedding may be based on any suitable representation of the token’s location, including the coordinates of two comers of the bounding box (e.g., top-left and bottom-right corners), the coordinates of one comer combined with the height and width of the bounding box, etc. Likewise, any suitable learned or static embedding function may be used to generate the text embeddings and spatial embeddings on which the beta-skeleton vectors are based.
[0027] In addition, in some aspects of the technology, the beta-skeleton graph 308 may further include an edge embedding for each of the edges extending from the given node in the beta-skeleton graph. In such a case, edge embeddings may be based on any suitable representation of the spatial relationship between the given node and each neighboring node to which it is linked.
[0028] Thus, in some aspects of the technology, for a given node A and a neighboring node B, an edge embedding may comprise the distance between one or more common points in the bounding boxes of the tokens to which nodes A and B correspond (e.g., the distance between the centers of each bounding box, between the top-right comers of each bounding box, between the top-left comers of each bounding box, between the bottom right-comers of each bounding box, between the bottom-left comers of each bounding box, etc.). Likewise, in some aspects, an edge embedding for an edge between node A and B may comprise the shortest total distance, shortest vertical distance, and/or the shortest horizontal distance between the bounding boxes of the tokens to which nodes A and B correspond. Likewise, in some aspects, an edge embedding for an edge between node A and B may comprise the coordinates (or coordinates of a given point and associated height and width) of a larger bounding box that encloses the individual bounding boxes of the tokens to which nodes A and B correspond. Likewise, in some aspects, an edge embedding for an edge between node A and B may comprise the aspect ratio of a larger bounding box that would enclose the individual bounding boxes of the tokens to which nodes A and B correspond. Further, in some aspects of the technology, an edge embedding for an edge between node A and B may comprise any combination or subcombination of any of the options just described.
[0029] Once beta-skeleton graph 308 has been generated, the processing system will feed the betaskeleton graph 308 to a graph convolutional network, GCN 314, to generate a set of supertokens 316 corresponding to each node. In some aspects of the technology, the processing system may be configured to provide GCN 314 with both the beta-skeleton graph 308 and layout 304 (or information based thereon representing the tokens in layout 304 and/or their corresponding position on the page), and GCN 314 may be configured to generate the set of supertokens 316 based thereon.
[0030] The supertoken corresponding to a given node will be based on the given node’s beta-skeleton vector, as well as the beta-skeleton vectors of each node to which it is connected in beta-skeleton graph 308. In addition, in some aspects of the technology, the supertoken corresponding to a given node may
further be based on the edge embeddings between the given node and each of these neighboring nodes. Thus, for example, since node 310a is connected to nodes 310b and 310c, the supertoken STi corresponding to node 310a will be based on beta-skeleton vector vo, as well as the beta-skeleton vectors vi and V2 of its neighboring nodes. In addition, in some aspects of the technology, the supertoken ST i corresponding to node 310a may further be based on the edge embeddings for the edges between nodes 310a and 310c, and between nodes 310a and 310b.
[0031] GCN 314 may be configured to generate the supertoken for a given node based on any suitable way of combining the beta-skeleton vectors for the given node and its neighboring nodes. For example, in some aspects of the technology, the supertoken for a given node may be generated by concatenating the beta-skeleton vectors for the given node and its neighboring nodes, and then by further processing the resulting concatenated vector using a multilayer perceptron (“MLP”). Likewise, in some aspects of the technology, the GCN 314 may be configured to further process the concatenated vector using a learned embedding function or another type of feed-forward neural network.
[0032] The set of supertokens 316 will be serialized prior to being supplied to transformer 318, and the processing system may be configured to do this using any suitable serialization approach. For example, as shown in FIG. 3, the set of supertokens 316 has been serialized using a left-to-right, top-to-bottom approach based on where each corresponding token is found on layout 304. Likewise, in some aspects of the technology, the set of supertokens may be serialized using a reading order approach. Thus, if the text of document 302 were written in a language where the reading order is right-to-left, top-to-bottom (e.g., as may be true for documents written in Hebrew, Arabic, etc.), the GCN may be configured to serialize the set of supertokens 316 using a right-to-left, top-to-bottom approach based on where each corresponding token is found on layout 304. Likewise, if the text of document 302 were written in a language where reading order is top-to-bottom, right-to-left (e.g., as may be true for documents written in Chinese, Japanese, etc.), the GCN may be configured to serialize the set of supertokens 316 using a top- to-bottom, right-to-left approach based on where each corresponding token is found on layout 304.
[0033] The processing system will next process the serialized set of supertokens 316 using a transformer 318. In this example, it is assumed that transformer 318 is configured to employ a sparse attention paradigm, although aspects of the present technology may also be applied to models with transformers that employ traditional (non-sparse) attention paradigms. Thus, for example, in some aspects of the technology, transformer 318 may use an extended transformer construction (“ETC”) with a sparse global- local attention mechanism, or another model architecture adapted to long-sequences which similarly employs a sparse attention paradigm (e.g., BigBird).
[0034] In addition, it is also assumed in this example that transformer 318 is configured to base local attention scores (e.g., for “long-long” pairings in ETC and BigBird) for a given pair of attender and attendee supertokens at least in part on the difference between predicted and actual values of the order and distance between the attender and attendee supertokens (e.g., using a “Rich Attention” paradigm, as described below with respect to FIG. 7). An exemplary method for modifying local attention scores in this way is explained in more detail below with respect to FIG. 7. However, any other suitable manner of using the order and distance between attender and attendee supertokens to influence local attention scores may be employed by transformer 318. Likewise, in some aspects of the technology, supertokens may be used to improve transformers that are not otherwise configured to modify local attention scores based on the order and distance between attender and attendee supertokens.
[0035] In the example of FIG. 3, transformer 318 is configured to output a set of entity BIOES logits 320. Each of these entity BIOES logits L0-L7 represents the model’s predictions regarding how likely the corresponding token of layout 304 is to be: (1) the beginning token of a text segment in a recognized entity class; (2) an inside token of a text segment in a recognized entity class; (3) an end token of a text segment in a recognized entity class; (4) a singleton token in a recognized entity class; or (5) an “outside” token that it is not within any recognized entity class.
[0036] The processing system processes the set of entity BIOES logits 320 to determine a final prediction of the most likely BIOES type and entity class for the entire set of tokens harvested from layout 304. In the example of FIG. 3, the processing system is configured to make this determination based on a Viterbi algorithm 322 configured to find the “Viterbi path” representing the optimal combination of BIOES types and entity classes that has the highest overall probability. However, any other suitable dynamic processing algorithm may also be used in this regard.
[0037] In FIG. 3, the Viterbi algorithm 322’s final predictions are represented as 324a-324h. Specifically, in this example: prediction 324a indicates that token to corresponding to the text “Cameras” has been predicted to be a beginning token in Entity Class 2; prediction 324b indicates that token ti corresponding to the text “16MP” has been predicted to be an inside token in Entity Class 2; prediction 324c indicates that token t2 corresponding to the text “8MP” has been predicted to be an end token in Entity Class 2; prediction 324d indicates that token tj corresponding to the text “OLED” has been predicted to be an outside token not belonging to any recognized entity class; prediction 324e indicates that token t4 corresponding to the text “IP68” has been predicted to be an outside token not belonging to any recognized entity class; prediction 324f indicates that token ts corresponding to the text “5G” has been predicted to be a singleton token in Entity Class 5; prediction 324g indicates that token t& corresponding to the text “USB” has been predicted to be a singleton token in Entity Class 9; and
prediction 324h indicates that token t? corresponding to the text “3.5mm” has been predicted to be a singleton token in Entity Class 9.
[0038] FIGS. 4 and 5 set forth a flow diagram 400 and exemplary method 500 showing how a set of bounding boxes A-D may be processed in order to generate an exemplary beta-skeleton graph.
[0039] In that regard, in step 502 of FIG. 5, the processing system (e.g., processing system 102) identifies a set of peripheral points at a preset spacing along the periphery of each bounding box, and a set of internal points along a longitudinal midline of each bounding box. FIG. 4 provides an illustrative example of how this might appear. In that regard, in block 402 of FIG. 4, the periphery of bounding box A is identified with reference numeral 404, one of the identified peripheral points of bounding box A is identified with reference numeral 406, and one of the internal points of bounding box A is identified with reference numeral 408.
[0040] In step 504 of FIG. 5, the processing system generates a Delaunay triangulation graph based on the set of the points identified in step 502. Although FIG. 4 does not show this intermediate step, as will be understood, a Delaunay triangulation graph based on block 402 of FIG. 4 would result in the points of bounding boxes A-D being interconnected such that no point will fall within the circumcircle of any triangle of the triangulation graph. There are many known approaches for generating Delaunay triangulations from a set of points, and the processing system may use any such approach.
[0041] In step 506 of FIG. 5, the processing system identifies all “inside points” that are inside the periphery of at least one of the bounding boxes. This will include not only the internal points that were originally identified in step 502, but also any peripheral points of one bounding box that may be inside the periphery of another bounding box by virtue of those two bounding boxes overlapping. For example, looking at block 402 of FIG. 4, point 410 will initially be identified in step 502 as being a peripheral point of bounding box B. However, point 410 will later be identified in step 506 as being an “inside point” by virtue of the fact that it falls within the periphery of bounding box A. All such inside points are shown in a light gray in block 402 (as well as in blocks 412 and 416).
[0042] The processing system may be configured to identify these “inside points” in any suitable way. For example, in some aspects of the technology, the processing system may be configured to traverse each “edge” (link) of the Delaunay triangulation graph that happens to start with a peripheral point of a given bounding box and extends inside of the given bounding box, as the end points of any such edges will by definition be points within one of the bounding boxes.
[0043] In step 508 of FIG. 5, the processing system identifies a set of every edge of the Delaunay triangulation graph for which: (1) the edge’s vertices vl and v2 are not “inside points”; and (2) the circle with vlv2 as its diameter does not cover any other point. Block 412 of FIG. 4 provides an illustrative
example of how this might appear, with edges 414 and 416 representing two of the 16 edges that would be identified according to step 508. As can be seen, there are no edges in block 412 between bounding boxes B and C, or between bounding boxes A and D, as every possible connection between the points of those bounding boxes would end up creating circles which overlap at least one other point.
[0044] Here as well, the processing system may be configured to identify these edges in any suitable way. For example, based on the fact that the point closest to vlv2 will always be a neighbor of either vl or v2 (based on the properties of Delaunay triangulation graphs), the processing system may be configured to determine whether a circle with diameter vlv2 will cover any points by simply checking whether the neighbors of vl and v2 will fall within the circle with vlv2 as its diameter.
[0045] In step 510 of FIG. 5, for every pair of intersecting bounding boxes, the processing system adds an artificial edge of length 0 to the set of edges identified in step 508. In the example of FIG. 4, such an edge would be identified between intersecting bounding boxes A and B. However, since the edge is artificial (in that it doesn’t actually correspond to an edge of the Delaunay triangulation graph), it has not been visually represented in block 412.
[0046] Although step 510 follows step 508 in exemplary method 500, it will be understood that the order of these processes may also be reversed. Thus, in some aspects of the technology, the processing system may first initialize the set of edges by adding 0-length edges for all intersecting bounding boxes, and then augment that set of edges with those identified from the beta-skeleton graph as discussed above with respect to step 508.
[0047] In step 512 of FIG. 5, the processing system identifies the shortest edge between each pair of bounding boxes from within the set of edges identified in steps 508 and 510. Block 418 of FIG. 4 provides an illustrative example of how this might appear, with edge 420a being the shortest edge between bounding boxes A and C, edge 420b being the shortest edge between bounding boxes B and D, and edge 420c being the shortest edge between bounding boxes C and D. In addition, although block 412 shows an edge 416 between bounding boxes A and B, the edge with the shortest distance between these boxes will be the artificial edge of length 0 that was identified in step 510. Although this artificial edge is not visually represented in block 418, it is represented in the resulting beta-skeleton graph shown in block 422.
[0048] In step 514 of FIG. 5, the processing system generates a beta-skeleton graph comprising a node for each bounding box, and each of the edges identified in step 512. Block 422 of FIG. 4 provides an illustrative example of how this might appear, showing an exemplary beta-skeleton graph in which bounding box A is represented with node 424a, bounding box B is represented with node 424b, bounding box C is represented with node 424c, and bounding box D is represented with node 424d. In addition,
nodes 424a and 424c are connected with edge 420a (as also shown in block 418), nodes 424c and 424d are connected with edge 420c (as also shown in block 418), nodes 424d and 424b are connected with edge 420b (as also shown in block 418), and nodes 424a and 424b are connected with edge 426. Further, in the exemplary beta-skeleton graph of block 422, a length for each edge of the beta-skeleton graph is shown next to each edge. Thus, as can be seen, edge 420a has a length of 1 , edge 420b has a length of 2.5, edge 420c has a length of 1.1, and edge 426 has a length of 0 (since it is an artificial edge identified based on the overlapping of bounding boxes A and B). Although these edge lengths are shown pictorially for purposes of illustration, in practice they may simply be encoded into the data structure representing the beta-skeleton graph, for example as part of edge embeddings.
[0049] FIG. 6 depicts an exemplary portion of a document, a corresponding exemplary beta-skeleton graph, an exemplary serialized version of the text of the document, and an illustration of how local attention scores may be adjusted based on differences between predicted and actual values of the order and distance between the attender and attendee tokens, in accordance with aspects of the disclosure.
[0050] In that regard, box 602 depicts a portion of original text as it would appear in a given exemplary document, and box 604 depicts an exemplary beta-skeleton graph overlaying that original text. This betaskeleton graph may be generated as described above with respect to FIGS. 3-5.
[0051] Box 616 depicts a serialized version of the text of box 602. In this example, the original text has been serialized using a simple left-to-right, top-to-bottom approach. In this way, box 616 helps to illustrate some of the issues that can arise from imperfect serialization. For example, the serialization approach shown in box 616 results in the column headings “TAR,” “NIC,” “MOIST,” and “MENT” immediately preceding the first word of the second line (“KOOL”), despite the column headings being very far from “KOOL” in the original text shown in box 602. Likewise, the column headings are all at least five words removed from the values “9.1,” “.88,” “14.0,” and “.474,” despite those values being listed directly beneath each of the column headings in the original text shown in box 602. Similarly, this serialization approach results in “tip-” being immediately followed by the values “9.1,” “.88,” “14.0,” and “.474,” despite there being a substantial gap between them in the original text shown in box 602. This further results in the hyphenated partial word “tip-“ being five words removed from its intended suffix (“ping”), despite “ping” being closer to “tip-” than are values “9.1,” “.88,” “14.0,” and “.474” in the original text shown in box 602.
[0052] Box 618 illustrates how these types of serializing issues can present challenges for a transformer that employs a sparse global- local attention paradigm. In addition, box 618 also illustrates how these issues may be mitigated by basing local attention scores (e.g., for “long-long” pairings in ETC and BigBird) at least in part on the difference between predicted and actual values of the order and distance
between the atender and attendee tokens (e.g., using a “Rich Attention” paradigm, as described below with respect to FIG. 7).
[0053] In that regard, window 620a represents an exemplary three-word local attention radius around an attender token for the word “white.” As can be seen, this local attention radius will result in the transformer generating local attention weights between the token for “white” and the tokens for “KOOL,” “Lights,” KS,” “tip-,” “9.1,” and “.88.” Thus, this three-word local attention radius will prevent the transformer from assessing attention between “white” and the tokens for “ping” and “masked,” even though those words are within three words of “white” according to the proper reading order of the original document (as shown in box 602). Likewise, this local attention paradigm will result in the transformer assessing attention between “white” and the tokens for “9.1” and “.88” even though the word “white” is farther away from “9.1” and “8.8” in the original document than it is from “ping” and “masked” in the original document.
[0054] Windows 620b-620d illustrate how attention scores for the tokens within window 620a may be modified using “Rich Attention” or a similar order- and distance-based adjustment paradigm. In that regard, the transformer may be configured to generate predictions for each pair of attender and attendee tokens which represent the “ideal” order that those tokens would be in relative to one another, and the “ideal” distance those tokens would be from one another, if it is assumed that the attender and attendee tokens do indeed relate to each other in some way. In FIG. 7, these predictions are made by learned parametric functions 708 and 712, but the transformer may generate “ideal” order and distance predictions in any suitable way (e.g., blocks 720 and 724 of FIG. 7). The transformer may then compare the actual order of the attender and attendee token (e.g., block 718 of FIG. 7) to the predicted “ideal” order to generate a first error value (e.g., block 730 of FIG. 7), and may compare the actual distance between the attender and attendee token (e.g., block 722 of FIG. 7) to the predicted “ideal” distance to generate a second error value (e.g., block 732 of FIG. 7). These error values may then be used to modify an initial pre-SoftMax attention score that the transformer generates between the attender and attendee token based on a typical query-key approach.
[0055] In some aspects of the technology, and as shown and described below with respect to FIG. 7, the transformer may use the error values to penalize the attention scores of those token pairs that have an actual order and/or distance that is different than their “ideal” order and/or distance, thus tending to decrease unwarranted attention between tokens that are in fact out of order and/or spatially distant from one another in the original document. However, in some aspects of the technology, the transformer may instead (or in addition) be configured to amplify the attention scores of those token pairs that have an actual order and/or distance that is close to their “ideal” order and/or distance, thus tending to increase
warranted atention between tokens that are in fact close and/or logically ordered in the original document.
[0056] Returning now to the example shown in box 618 of FIG. 6, it has been assumed that the ideal order and/or distance between the token for “white” and the token for “9.1” will be different than the actual order and distance between “white” and “9.1” in the original document, and thus that the atention score between “white” and “9.1” will be reduced based on these differences. For example, this may result from the transformer predicting that the “ideal” distance between “white” and “9.1” (if they were related) would be much closer than how those words are actually spaced in the original document, and thus that the atention score should be reduced to some degree based on this difference. Likewise, this may result from the transformer predicting that the “ideal” order between “white” and “9.1” would be for “9.1” to precede “white” rather than follow it (as is the case in the text of the original document), and thus that the atention score should be reduced to some other degree based on this difference.
[0057] Similarly, it has been assumed that the model’s predictions will result in reductions of the atention scores between “white” and “.88,” and between “white” and “KOOL,” but by different amounts. These varying amounts of atention score reductions are visually represented in FIG. 6 by boxes 620b, 620c, and 620d having different levels of shading.
[0058] In contrast, because the token for the word “KS” is both related to and spatially close to the token for “white” in the original text, it has been assumed that there will be litle or no difference between how these tokens are actually ordered and spaced in the original document and the model’s learned prediction of the “ideal” order and spacing. As such, in this example, it is assumed that there will be litle or no change made to the atention score that the transformer would otherwise generate between “white” and “KS.” For the same reason, it has been assumed that there will be litle or no changes to the atention scores that the transformer would otherwise generate between “white” and “Lights” and between “white and “tip-.”
[0059] Thus, although the detrimental impact of imperfect serializing may be amplified where a sparse global-local atention paradigm is employed, the potential for the model to reach false conclusions based on improperly serialized tokens may be effectively and efficiently managed by adjusting local attention scores based on differences between predicted and actual values of the order and distance between the atender and atendee tokens, as illustrated in box 618.
[0060] In addition, because the transformers of the present technology may be configured to accept supertokens as their initial input, spatial and semantic information regarding a given token’s neighbors will automatically be weighed in the first layer of the transformer’s atention mechanism even where imperfect serializing and/or the size of the local atention radius would prevent atention from being
directly assessed between a given token and one or more of its neighboring tokens from the beta-skeleton graph. Thus, for example, although the local attention radius illustrated with box 620a will prevent the transformer from directly assessing attention between the token for “white” and the token for “ping,” the transformer will end up implicitly assessing attention between “white” and “ping” because the betaskeleton vector for “ping” will propagate into the supertoken for “KOOL” through edge 606. Likewise, although attention will not be directly assessed between the token for “white” and the token for “masked,” the transformer will end up implicitly assessing attention between “white” and “masked” because the beta-skeleton vector for “masked” will propagate into the supertokens for “KOOL,” “Lights,” and “KS” through edges 608, 610, and 612, respectively.
[0061] Although the examples of FIGS. 3 and 7 both involve a transformer that is configured to both take supertokens as input, and to adjust local attention scores based on the order and distance between attender and attendee supertokens (as discussed above and below), these features may also be used separately. In that regard, although a combined approach as shown in FIGS. 3 and 7 may provide the biggest improvements over sequence models based on transformers with standard global-local sparse attention paradigms, significant benefits may be possible from using only supertokens, or only the attention-adjustment paradigms discussed herein.
[0062] FIG. 7 shows a flow diagram 700 illustrating how an attention head of a transformer may employ a “Rich Attention” paradigm, in accordance with aspects of the disclosure. As used herein, the term “Rich Attention” is not meant to identify a paradigm that is an alternative to sparse attention, nor are “Rich Attention” and sparse attention mutually exclusive. Rather, the term “rich” in “Rich Attention” is simply meant to indicate that the attention score between a given attender and attendee token will be influenced or “enriched” based on how closely the actual order and distance between those supertokens matches a predicted “ideal” order and distance for those supertokens. In this way, “Rich Attention” can be used to enrich any type of attention score, including local attention scores calculated according to a sparse global-local attention paradigm (e.g., as used in ETC and BigBird transformers), as well as attention scores calculated according to standard attention paradigms (e.g., as used in BERT transformers).
[0063] In the example of FIG. 7, the input to the attention head are two vectors h; (box 702) and h7 (box 704). In the first layer of the transformer, vectors h, and h7 will be the supertokens for two nodes i and j of a beta-skeleton graph generated from an input document, as discussed above with respect to FIGS. 3-5. For each successive layer, vectors h; and h7 will be based on outputs of the previous layer. For clarity, the elements of flow diagram 700 will each be described below using the assumption that processing is taking
place in the first layer of the transformer, and thus that vectors h, and h7 are the supertokens for nodes i and j. Further, in all cases, it will be assumed that vector h, is the attender and vector h7 is the attendee. [0064] In the example of FIG. 7, the transformer is configured to adjust attention scores based on how closely the actual order and distance between those supertokens matches a predicted “ideal” order and distance for those supertokens. To do this, the transformer will generate four values: (1) an actual order value oy representing an actual order between tokens i and j (box 718); (2) an “ideal” order value pv representing the model’s prediction of what order tokens i and j should be in if that they are related to one another (box 720); (3) an actual distance value dy representing an actual distance between tokens i and j (box 722); and (4) an “ideal” distance value representing the model’s prediction of what the distance should be between tokens i and j if they are related to one another (box 724). As can be seen from the arrows of extending from h; and h, in flow diagram 700, each of these values will be calculated based at least in part on h, and h7. The individual details of functions 706, 708, 710, and 712 will be discussed further below.
[0065] In that regard, in the example of FIG. 7, the actual order value o (box 718) is based on the indices i and j of vectors h; and h7 Specifically, as shown in box 706, the transformer will compare i to j, and will assign a value of 1 if i is greater than j, and will otherwise assign a value of 0. Any suitable manner of indexing h, and h7 may be used in this regard. For example, indices i and j may be the coordinates of bounding boxes surrounding tokens i and j, coordinates of the center points of the bounding boxes surrounding tokens i and j, coordinates of a common corner of the bounding boxes surrounding tokens i and j, etc. Likewise, any suitable manner of ordering h; and h, may be used. Thus, in some aspects of the technology, the actual order value o may represent the order of tokens i and j along a single axis or path (e.g., along the x-axis of the document, along the y-axis of the document, along a straight or curved line corresponding to the text direction in the relevant portion of the document, etc.). Likewise, in some aspects of the technology, the actual order value oy may represent the order of tokens i and j along two axes (e.g., oy may be a vector with values corresponding to how i and j are ordered in the x-axis, and in the y-axis).
[0066] In the example of FIG. 7, the ideal order value p (box 720) is generated based on vectors h; and h7 using a learned parametric function 708. Any suitable parametric function may be used in this regard. For example, in some aspects of the technology, the transformer may use a sigmoid classifier trained to predict the probability of the ordering of tokens, such as shown in Equation 1 below:
Ptj = Sigmoid (affine^ h,; hy]) (1)
[0067] In Equation 1, affine(0 x) represents W^ + where is a weight matrix of free parameters, and b(o) is a bias vector composed of free parameters. The free parameters of W(o) and b(o) may be
randomly initialized and then updated as the sequence model is trained. In that regard, the sigmoid classifier may be trained to minimize cross-entropy loss, or may be trained using any other suitable loss function.
[0068] The actual distance value dy (box 722) is also based on the indices i and j of vectors h; and h7 Specifically, as shown in box 710, the transformer will find the absolute value of i minus j. As mentioned above, any suitable manner of indexing and comparing h, and h7 may be used in this regard. Thus, in some aspects of the technology, the actual distance value d may be based on distance measured along a single axis or path (e.g., along the x-axis of the document, along the y-axis of the document, along a straight or curved line corresponding to the text direction in the relevant portion of the document, etc.). Likewise, in some aspects of the technology, the actual distance value dy may be based on distances measured along two axes (e.g., d may be a vector with values corresponding to how far token i is from token j along both the x-axis and the y-axis, or may be a value representing the absolute straight-line distance between token i and token j).
[0069] In the example of FIG. 7, the ideal distance value (box 724) is generated based on vectors h, and h7 using another learned parametric function 712. Here as well, any suitable parametric function may be used in this regard. For example, in some aspects of the technology, the transformer may use an affine transformation, such as shown in Equation 2 below:
[0070] In Equation 2, affine^( ) represents W^ + b ® where
[0071] As shown in box 730, the actual order value oy (box 718) and ideal order value py (box 720) will be used to generate a negative error value s(o). In some aspects of the technology, negative error value s(o) may be calculated using a negative sigmoid cross entropy loss, such as shown in Equation 3 below.
[0072] However, negative error value s(o) may be calculated using any negative log-likelihood function or other suitable equation that is likewise based on a comparison of, or difference between, the actual order value oy and the ideal order value py.
[0073] Likewise, as shown in box 732, the actual distance value d (box 722) and ideal distance value py (box 724) will be used to generate a negative error value s(d). In some aspects of the technology, negative error value s(d) may be calculated using a scaled negative L2 loss, such as shown in Equation 4 below.
[0074] In Equation 4, the variable t represents a free parameter that will be randomly initialized and updated as the sequence model is trained. However, negative error value s(d) may be calculated using any negative log-likelihood function or other suitable equation which is based on a comparison of, or difference between, the actual distance value dy and ideal distance value p . Likewise, in some aspects of the technology, instead of being a constant, t may be a variable value. For example, in some aspects, t may be the output of another affine transformation that takes vectors h, and h7 as input.
[0075] Although the example of FIG. 7 assumes that the actual order value o (box 718), ideal order value py (box 720), actual distance value d (box 722), and ideal distance value py (box 724) will all be generated based directly on vectors h; and h7, they may also be indirectly based on vectors h; and h7. For example, each of these values may instead be generated using the reduced-rank query and key vectors q, (box 726) and k (box 728). In such an arrangement, vectors h, and h7 would be initially provided to parametric functions 714 and 716 to generate the query and key vectors q, and k,, and then q, and k, would be fed to functions 706, 708, 710, and 712 in order to generate the actual order value o , ideal order value p , actual distance value dy, and ideal distance value p .
[0076] Regardless of the approach used for calculating the actual order value o , ideal order value py, actual distance value dy, and ideal distance value p , vectors h, and h7 will be provided to the learned parametric functions 714 and 716 to generate query and key vectors q, (box 726) and k (box 728). These learned parametric functions 714 and 716 will both be separate affine transformations, each with then- own weight matrix and bias vector. As discussed above, each such weight matrix and bias vector may be randomly initialized and then updated as the sequence model is trained. The resulting the query and key vectors q, and k,, will then be subjected to matrix multiplication (function 734) to generate an initial pre- SoftMax attention score. Specifically, as shown in FIG. 7, the transformer will calculate a dot product of the transpose of query vector q; and the key vector k,. The resulting initial pre-SoftMax attention score will be added to the negative error values 730 and 732 to generate an adjusted pre-SoftMax attention score, as shown in the circle labeled with reference numeral 736. That adjusted pre-SoftMax attention score will then be provided to SoftMax Function 738.
[0077] The remaining steps of flow diagram 700 are consistent with traditional attention processing. In that regard, vector h7 is provided to a learned parametric function 740 to generate a value vector V7 (box 742). The post-SoftMax attention score produced by SoftMax Function 738 is then multiplied by the value vector V7 as shown in box 744, resulting in an updated vector h; as shown in box 746. As discussed above, this updated vector h; may then be used in successive layers of the transformer consistent with
standard transformer architecture. Finally, the outputs of the last layer of the transformer will be used to generate the BIOES logits described above with respect to FIG. 3.
[0078] The structure-aware sequence models of the present technology may be trained in any suitable way. In that regard, in some aspects of the technology, a structure-aware sequence model may be pretrained using one more sets of masked-language modeling tasks, and/or next-sentence prediction tasks, and may be fine-tuned using one or more annotated sets of training examples (e.g., the public FUNSD and/or CORD datasets). Likewise, in some cases, pretraining may not be deemed necessary, and the structure-aware sequence model may be trained from scratch using only annotated sets of training examples.
[0079] Unless otherwise stated, the foregoing alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of exemplary systems and methods should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. In addition, the provision of the examples described herein, as well as clauses phrased as “such as,” “including,” “comprising,” and the like, should not be interpreted as limiting the subject matter of the claims to the specific examples; rather, the examples are intended to illustrate only some of the many possible embodiments. Further, the same reference numbers in different drawings can identify the same or similar elements.
Claims
1. A processing system comprising: a memory storing a neural network comprising a graph convolutional network and a transformer; and one or more processors coupled to the memory and configured to classify text from a given document, comprising: generating a beta-skeleton graph based on a plurality of tokens, each given token of the plurality of tokens corresponding to a given string of text in the given document, and wherein the betaskeleton graph comprises, for each given token: a node corresponding to the given token and comprising a vector based on content and location of the given string of text within the given document; and one or more edges, each edge of the one or more edges linking the node corresponding to the given token to a neighboring node corresponding to another token of the plurality of tokens; generating, using the graph convolutional network, a plurality of supertokens based on the beta-skeleton graph, each given supertoken of the plurality of supertokens being based at least in part on the vector of a given node and the vector of each neighboring node to which the given node is linked via one of its one or more edges; generating, using the transformer, a plurality of predictions based on the plurality of supertokens; and generating a set of classifications based on the plurality of predictions, the set of classifications identifying at least one entity class corresponding to at least one token of the plurality of tokens.
2. The processing system of claim 1, wherein generating the plurality of predictions based on the plurality of supertokens using the transformer comprises, for a given attender supertoken and a given attendee supertoken: generating a first prediction regarding how the given attender supertoken and given attendee supertoken should be ordered if the given attender supertoken and given attendee supertoken are related to one another; generating a second prediction regarding how far the given attender supertoken should be from the given attendee supertoken if the given attender supertoken and given attendee supertoken are related to one another;
-22-
generating a first error value based on the first prediction and a value based on how text corresponding to the given attender supertoken and given attendee supertoken is actually ordered in the given document; generating a second error value based on the second prediction and a value based on how far text corresponding to the given attender supertoken actually is from text corresponding to the given attendee supertoken in the given document; generating a query vector based on the given attender supertoken; generating a key vector based on the given attendee supertoken; generating a first attention score based on the query vector and the key vector; and generating a second attention score based on the first attention score, the first error value, and the second error value.
3. The processing system of any of claims 1 or 2, wherein the beta-skeleton graph further comprises, for each given token: a given edge embedding corresponding to each given edge of the one or more edges, the given edge embedding being based on a spatial relationship in the given document between the given token and a token corresponding to the neighboring node to which the given edge is linked.
4. The processing system of any preceding claim, wherein the transformer is configured to use a sparse global-local attention paradigm.
5. The processing system of claim 4, wherein the transformer is based on an Extended Transformer Construction architecture.
6. The processing system of any preceding claim, wherein the given document comprises an image of a document, and wherein the one or more processors are further configured to identify, for each given token of the plurality of tokens, the content and location of the given string of text in the given document to which the given token corresponds.
7. The processing system of claim 6, wherein identifying the content and location of the given string of text in the given document comprises using optical character recognition.
8. The processing system of any of claims 1 to 7, wherein generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions.
9. The processing system of any of claims 1 to 7, wherein the set of classifications based on the plurality of predictions are BIOES classifications.
10. The processing system of claim 9, wherein generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions to determine a Viterbi path representing an optimal combination of BIOES types and entity classes that generates the highest overall probability based on the plurality of predictions.
11. A processing system comprising: a memory storing a neural network comprising a transformer; and one or more processors coupled to the memory and configured to classify text from a given document, comprising: generating, using the transformer, a plurality of predictions based on a plurality of tokens, each given token of the plurality of tokens corresponding to a given string of text in the given document, and wherein generating a given prediction of the plurality of predictions for a given attender token and a given attendee token of a plurality of tokens comprises: generating a first prediction regarding how the given attender token and given attendee token should be ordered if the given attender token and given attendee token are related to one another; generating a second prediction regarding how far the given attender token should be from the given attendee token if the given attender token and given attendee token are related to one another; generating a first error value based on the first prediction and a value based on how the text corresponding to the given attender token and given attendee token is actually ordered in the given document; generating a second error value based on the second prediction and a value based on how far the text corresponding to the given attender token actually is from the text corresponding to the given attendee token in the given document; generating a query vector based on the given attender token;
generating a key vector based on the given attendee token; generating a first attention score based on the query vector and the key vector; generating a second attention score based on the first attention score, the first error value, and the second error value; and generating the given prediction based at least in part on the second attention score; and generating a set of classifications based on the plurality of predictions, the set of classifications identifying at least one entity class corresponding to at least one token of the plurality of tokens.
12. The processing system of claim 11, wherein the neural network further comprises a graph convolutional network, and the one or more processors are further configured to: generate a beta-skeleton graph based on the plurality of tokens, wherein the beta-skeleton graph comprises, for each given token of the plurality of tokens: a node corresponding to the given token and comprising a vector based on content and location of the given string of text within the given document; and one or more edges, each edge of the one or more edges linking the node corresponding to the given token to a neighboring node corresponding to another token of the plurality of tokens; and generate, using the graph convolutional network, a plurality of supertokens based on the betaskeleton graph, each given supertoken of the plurality of supertokens being based at least in part on the vector of a given node and the vector of each neighboring node to which the given node is linked via one of its one or more edges; and wherein, for each given prediction of the plurality of predictions that is generated by the transformer, the given attender token and the given attendee token are each a supertoken of the plurality of supertokens.
13. The processing system of claim 12, wherein the beta-skeleton graph further comprises, for each given token: a given edge embedding corresponding to each given edge of the one or more edges, the given edge embedding being based on a spatial relationship in the given document between the given token and a token corresponding to the neighboring node to which the given edge is linked.
-25-
14. The processing system of any of claims 11 to 13, wherein the transformer is configured to use a sparse global-local attention paradigm.
15. The processing system of claim 14, wherein the transformer is based on an Extended Transformer Construction architecture.
16. The processing system of any preceding claim, wherein the given document comprises an image of a document, and wherein the one or more processors are further configured to identify, for each given token of the plurality of tokens, content and location of the given string of text in the given document to which the given token corresponds.
17. The processing system of claim 16, wherein identifying the content and location of the given string of text in the given document comprises using optical character recognition.
18. The processing system of any of claims 11 to 17, wherein generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions.
19. The processing system of any of claims 11 to 17, wherein the set of classifications based on the plurality of predictions are BIOES classifications.
20. The processing system of claim 19, wherein generating the set of classifications based on the plurality of predictions comprises performing dynamic processing based on the plurality of predictions to determine a Viterbi path representing an optimal combination of BIOES types and entity classes that generates the highest overall probability based on the plurality of predictions.
-26-
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/047461 WO2023027701A1 (en) | 2021-08-25 | 2021-08-25 | Structural encoding and attention paradigms for sequence modeling |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/047461 WO2023027701A1 (en) | 2021-08-25 | 2021-08-25 | Structural encoding and attention paradigms for sequence modeling |
Publications (1)
Publication Number | Publication Date |
---|---|
WO2023027701A1 true WO2023027701A1 (en) | 2023-03-02 |
Family
ID=77838911
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2021/047461 WO2023027701A1 (en) | 2021-08-25 | 2021-08-25 | Structural encoding and attention paradigms for sequence modeling |
Country Status (1)
Country | Link |
---|---|
WO (1) | WO2023027701A1 (en) |
-
2021
- 2021-08-25 WO PCT/US2021/047461 patent/WO2023027701A1/en unknown
Non-Patent Citations (1)
Title |
---|
ZUO ENGUANG ET AL: "Context-Specific Heterogeneous Graph Convolutional Network for Implicit Sentiment Analysis", IEEE ACCESS, IEEE, USA, vol. 8, 20 February 2020 (2020-02-20), pages 37967 - 37975, XP011775983, DOI: 10.1109/ACCESS.2020.2975244 * |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Zhao et al. | EmotionGAN: Unsupervised domain adaptation for learning discrete probability distributions of image emotions | |
US10467508B2 (en) | Font recognition using text localization | |
US11244205B2 (en) | Generating multi modal image representation for an image | |
US10963632B2 (en) | Method, apparatus, device for table extraction based on a richly formatted document and medium | |
She et al. | Hierarchical layout-aware graph convolutional network for unified aesthetics assessment | |
US20180114097A1 (en) | Font Attributes for Font Recognition and Similarity | |
US11508173B2 (en) | Machine learning prediction and document rendering improvement based on content order | |
Zhang et al. | Weakly supervised emotion intensity prediction for recognition of emotions in images | |
CN113378710B (en) | Layout analysis method and device for image file, computer equipment and storage medium | |
US20230222285A1 (en) | Layout-Aware Multimodal Pretraining for Multimodal Document Understanding | |
JP2007128195A (en) | Image processing system | |
US20230244849A1 (en) | Content aware font recommendation | |
US11017016B2 (en) | Clustering product media files | |
US11741734B2 (en) | Identification of blocks of associated words in documents with complex structures | |
CN111488732B (en) | Method, system and related equipment for detecting deformed keywords | |
CN109643396A (en) | Construct convolutional neural networks | |
Hu et al. | Multimodal learning via exploring deep semantic similarity | |
CN117058554A (en) | Power equipment target detection method, model training method and device | |
WO2023027701A1 (en) | Structural encoding and attention paradigms for sequence modeling | |
US11599856B1 (en) | Apparatuses and methods for parsing and comparing video resume duplications | |
US8488183B2 (en) | Moving labels in graphical output to avoid overprinting | |
CN117011737A (en) | Video classification method and device, electronic equipment and storage medium | |
JP7251609B2 (en) | Processing device, processing method and program | |
Wang et al. | Text reading order in uncontrolled conditions by sparse graph segmentation | |
KR20220039075A (en) | Electronic device, contents searching system and searching method thereof |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
121 | Ep: the epo has been informed by wipo that ep was designated in this application |
Ref document number: 21773429Country of ref document: EPKind code of ref document: A1 |
|
NENP | Non-entry into the national phase |
Ref country code: DE |