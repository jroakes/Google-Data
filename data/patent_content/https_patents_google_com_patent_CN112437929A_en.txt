CN112437929A - Temporal coding in spiking neural networks with leakage - Google Patents
Temporal coding in spiking neural networks with leakage Download PDFInfo
- Publication number
- CN112437929A CN112437929A CN201980047993.9A CN201980047993A CN112437929A CN 112437929 A CN112437929 A CN 112437929A CN 201980047993 A CN201980047993 A CN 201980047993A CN 112437929 A CN112437929 A CN 112437929A
- Authority
- CN
- China
- Prior art keywords
- spiking
- neurons
- neural network
- neuron
- spike
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/048—Activation functions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/049—Temporal neural networks, e.g. delay elements, oscillating neurons or pulsed inputs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/088—Non-supervised learning, e.g. competitive learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/004—Artificial life, i.e. computing arrangements simulating life
- G06N3/006—Artificial life, i.e. computing arrangements simulating life based on simulated virtual individual or collective life forms, e.g. social simulations or particle swarm optimisation [PSO]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
Abstract
A spiking neural network performing temporal coding for phase-coherent neural computation is provided. In particular, according to an aspect of the present disclosure, a spiking neural network may include one or more spiking neurons having an activation layer that models a leaky-in input using a dual-exponential function, an afferent neuron spike providing the leaky-in input to a membrane potential of the spiking neuron. The use of a bi-exponential function in the temporal transfer function of the neuron creates a more definite maximum in time. This allows to generate very clearly defined state transitions between "present" and "future step" without loss of phase coherence.
Description
RELATED APPLICATIONS
This application claims priority and benefit of U.S. provisional patent application No. 62/744,150 filed on 11/10/2018. United states provisional patent application No. 62/744,150 is incorporated herein by reference in its entirety.
Technical Field
The present disclosure relates generally to neural networks. More particularly, the present disclosure relates to a leaky spiking neural network (leaky spiking neural network) that performs temporal encoding (temporal encoding).
Background
Traditionally, artificial neural networks have been constructed primarily from idealized neurons that generate continuous activation values based on a set of weighted inputs using a non-linear activation layer. Some neural networks have multiple sequential layers of such neurons, in which case they may be referred to as "deep" neural networks.
Non-spiking neural networks (non-spiking neural networks) typically use a non-linear active layer that produces a continuous value output to transmit information through the network. These non-linear activation layers are differentiable, which makes it possible to determine the gradient of the loss function with respect to the network weight. In the case of multiple layers, the presence of the gradient of the penalty function makes it possible to use a gradient-based optimization method in combination with a back-propagation algorithm to learn specific weight values that enable the network to accurately perform a certain task.
Gradient-based optimization techniques (e.g., gradient descent) have been very successful in training neural networks of continuous values. However, gradient-based techniques are not easily converted to spiking neural networks due to the strict non-linearity of spike generation and the discretization of spike communication.
Furthermore, spiking neural networks are dynamic systems in which the respective times at which individual neurons spike play an important role. This is in contrast to conventional feed-forward neural networks, which ignore time. In particular, state transitions in classical neural networks occur globally and synchronously.
The possibility that the synchronization system must distribute the clock and discard some of the phases to be used in order to extend the information transfer bandwidth between the neurons. From a bandwidth perspective, ideally, the neurons would be self-synchronizing. This would eliminate the clock distribution requirement and would increase the information transfer bandwidth in both the hardware and software implementations of the recurrent neural network.
More specifically, unlike non-spiking neurons that output analog values, spiking neurons typically communicate using discrete spikes that are binary in nature (e.g., output or non-output spikes). Typically, a spike triggers tracking of synaptic current in a receiving neuron or otherwise affects the membrane potential of the receiving neuron. In some exemplary concepts, the receiving neuron integrates the received synaptic current over time until a firing threshold (firing of the neuron) is reached, at which point the neuron spikes or discharges itself. Because of the strict non-linearity of the neuron spiking rate, it is generally undifferentiated, which prevents the wide application of gradient-based techniques to spiking neural networks.
Thus, while back propagation is an established general technique for training conventional non-spiking neural networks, a general technique for training spiking neural networks has not been established. Some previous approaches to training spiking neural networks to generate specific spiking patterns rely on the absence of any hidden layers (e.g., the input layer is directly connected to the output layer). Therefore, these methods cannot be used to train a multi-layer network.
Training spiking networks, especially with multi-tiered learning (e.g., deep spiking neural networks), remains a challenge. Implementing learning in a multi-layer spiking neural network is an area that is being developed and may greatly improve the performance of spiking neural networks on different tasks.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
An exemplary aspect of the present disclosure is directed to a computer system that includes one or more processors and one or more non-transitory computer-readable media. The one or more non-transitory computer-readable media collectively store a machine-learned spiking neural network that includes one or more spiking neurons having an activation layer that models a leaky-band input using a bi-exponential function, an afferent neuron spike providing the leaky-band input to a membrane potential of the spiking neuron. The one or more non-transitory computer-readable media collectively store instructions that, when executed by the one or more processors, cause the computer system to perform operations. The operations include obtaining a network input. The operations include implementing the machine-learned spiking neural network to process the network input. The operations include receiving a network output generated by the machine-learned spiking neural network as a result of processing the network input.
In some implementations, the machine-learned spiking neural network encodes information in respective spike times associated with the one or more spiking neurons.
In some embodiments, the bi-exponential function models the band leakage input as a bi-exponential pulse. In some embodiments, the dual-exponential function has e-t(t-1+ c) wherein c is a hyperparameter. In some embodiments, the dual-exponential function has te-tIn the form of (1).
In some embodiments, the membrane potential of each of the one or more spiking neurons, if not already spiked, has
In some embodiments, implementing the spiking neural network for machine learning comprises: for each of the one or more neurons, determining a spiking time corresponding to an earliest time that the membrane potential of the spiking neuron is equal to a firing threshold.
In some implementations, determining the spike time for each of the one or more spiking neurons includes applying a Lambert W-function to determine the spike time.
In some embodiments, the operations further comprise: training the machine-learned spiking neural network on training data via a gradient descent technique prior to obtaining the network input. In some embodiments, training the machine-learned spiking neural network via gradient descent comprises: for each of the one or more spiking neurons, determining one or both of: the spike time of the spiking neuron with respect to the time point tiA derivative of (a); and the spike time of the spiking neuron with respect to the weight wiWherein the spiking time corresponds to an earliest time that the membrane potential of the spiking neuron equals a firing threshold. In some embodiments, for each of the one or more spiking neurons, the time point t is based at least in part on the spiking time of the spiking neuron with respect to the time point tiAnd the spike time of the spiking neuron with respect to the weight wiOne or both of the derivatives of one or more of them to modify the weight wiAt least one of (a).
In some embodiments, the machine-learned spiking neural network comprises a plurality of layers, at least two layers of the plurality of layers comprising at least one of the one or more spiking neurons, and wherein the machine-learned spiking neural network has been trained on training data using a back propagation technique.
In some embodiments, the operations further comprise: training the machine-learned spiking neural network on training data via a gradient descent technique to simultaneously learn both parameters of the machine-learned spiking neural network and a topology of the machine-learned spiking neural network.
Another exemplary aspect of the present disclosure is directed to a computer-implemented method for training a spiking neural network that encodes information with respective spike times associated with a plurality of spiking neurons contained in the spiking neural network. The method includes obtaining, by one or more computing devices, a data description of the spiking neural network including the plurality of spiking neurons. Each of the plurality of spiking neurons is respectively connected to one or more pre-synaptic neurons via one or more artificial synapses having one or more weights associated therewith. Each of the plurality of spiking neurons has an activation layer that controls a respective spiking time of the spiking neuron based on a membrane potential of the spiking neuron. The activation layer for each of the plurality of spiking neurons comprises a bi-exponential function that models an incoming spike received from the one or more pre-synaptic neurons as a band-leakage input to the membrane potential. The method includes training, by the one or more computing devices, the spiking neural network based on a training dataset. Training, by the one or more computing devices, the spiking neural network comprises: determining, by the one or more computing devices, a gradient of a loss function that evaluates performance of the spiking neural network over the training dataset; and modifying, by the one or more computing devices, for at least one of the plurality of spiking neurons, at least one of the one or more weights based at least in part on a gradient of the loss function.
In some embodiments, each of the plurality of spiking neurons receives the afferent spike from the one or more pre-synaptic neurons at a respective incoming spike time. In some implementations, determining, by the one or more computing devices, the gradient of the loss function includes: determining, by the one or more computing devices, a derivative of a spike time of the spiking neuron with respect to the entry spike time for at least one of the plurality of spiking neurons.
In some implementations, determining, by the one or more computing devices, the gradient of the loss function includes: determining, by the one or more computing devices, for at least one of the spiking neurons, a derivative of a spiking time of the spiking neuron with respect to one or more of the weights associated with the spiking neuron.
In some implementations, training, by the one or more computing devices, the spiking neural network further comprises: modifying, by the one or more computing devices, for at least one of the plurality of spiking neurons, at least one synaptic delay parameter based at least in part on a gradient of the loss function.
In some embodiments, the plurality of spiking neurons are arranged in a plurality of layers. In some implementations, training, by the one or more computing devices, the spiking neural network includes back-propagating, by the one or more computing devices, the loss function through the plurality of layers.
In some embodiments, for each of the plurality of spiking neurons, the membrane potential has a magnitude that is greater than the magnitude of the spike potential if the spiking neuron does not spike
Another exemplary aspect of the present disclosure is directed to an electronic device. The electronic device includes a machine-learned spiking neural network including one or more spiking neurons. Each of the one or more spiking neurons has an activation layer that models a leaky-in input using a bi-exponential function, an afferent neuron spike providing the leaky-in input to a membrane potential of the spiking neuron. The machine-learned spiking neural network is configured to receive a network input and process the network input to generate a network output.
In some implementations, the machine-learned spiking neural network includes computer-readable instructions stored on a non-transitory computer-readable medium.
In some embodiments, the machine-learned spiking neural network comprises one or more electronic circuits comprising electronic components arranged to perform the machine-learned spiking neural network using electrical current.
In some implementations, for each of the one or more spiking neurons, the respective electronic component that models the dual-exponential function includes two capacitors, two resistors, and one or more transistors.
Other aspects of the disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles involved.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is presented in the specification, with reference to the accompanying drawings, in which:
fig. 1 depicts a graphical schematic of an exemplary spiking neuron, according to an exemplary embodiment of the present disclosure.
2A-2C depict exemplary graphs showing a neuron model with a dual-exponential synaptic function, according to exemplary embodiments of the present disclosure.
Fig. 3A depicts a block schematic diagram of an exemplary computing system, according to an exemplary embodiment of the present disclosure.
Fig. 3B depicts a block schematic diagram of an exemplary computing device, according to an exemplary embodiment of the present disclosure.
Fig. 3C depicts a block schematic diagram of an exemplary computing device, according to an exemplary embodiment of the present disclosure.
Reference numerals repeated among the various figures are intended to identify identical features in different embodiments.
Detailed Description
SUMMARY
In general, the present disclosure is directed to spiking neural networks that perform temporal coding for phase-coherent neural computations. In particular, according to an aspect of the present disclosure, a spiking neural network may include one or more spiking neurons with an activation layer that uses a double exponential function (which may also be referred to as an "alpha function") to model the leaky-in input of an afferent (communicating) neuron spike to a membrane potential of the spiking neuron. The use of bi-exponential functions in the temporal transfer function of the neuron creates a more definite maximum in time. This allows to generate very clearly defined state transitions between "present" and "future step" without loss of phase coherence.
More specifically, the present disclosure provides a synaptic transfer function of biological reality (biological-realtistic), e.g., at te-tBy integrating the exponentially decaying kernel. In contrast to the single exponential function, the double exponential function gradually rises before slowly decaying (see, e.g., fig. 2A-2C), which allows for more complex interactions between presynaptic inputs. The double-exponential function provides a model of biological-plausibility (biological-plausible) for exploring the problem-solving capabilities of a spiking network employing a temporal coding scheme. In particular, an accurate gradient with respect to the spike time can be derived using this model.
Accordingly, aspects of the present disclosure are directed to a spike network model that uses a dual-exponential function for synaptic transmission and encodes information in relative spike times. The network can be fully trained in the time domain using a precise gradient over the domain where the relative spike order is preserved. Exemplary experimental results with this type of model have been shown that can learn standard fiducial (benchmark) problems such as boolean logic gates and MNIST (hand-written digit recognition database) encoded in individual peak times. To facilitate the transition of class boundaries, synchronization pulses, which are neurons that spike at learning times independent of input, may be used.
The proposed model can easily solve the boolean logic and other reference problems of temporal coding. Analysis of the behavior of the spiking network during training shows that it spontaneously exhibits two operating states that reflect a trade-off between speed and accuracy: the slow state is very accurate, although slow, while the fast state is slightly less accurate but makes decisions much faster.
Accordingly, the present disclosure develops the concept of temporal coding in leaky-neurons (e.g., leaky-integration-and-fire neurons). One main aspect described herein is to encode information with the spike time of the spiking neuron instead of the spike rate. In particular, the output of a neuron may be its spike time, which may depend on the timing and weight of the pre-synaptic neuron that caused the neuron to fire. The expression of the spike time of a neuron in the continuous time domain makes it differentiable so that the spike timing in the network can be learned using back propagation and gradient-based techniques. This also optionally allows for the addition of synaptic delays, which may also be trained using back propagation techniques.
Thus, according to another aspect, the present disclosure provides a system capable of applying a gradient-based learning algorithm to learn a bi-exponential temporal transfer function. Further, the systems described herein may implement a gradient-based learning algorithm to learn to build internal states in a rotating network, allowing the network to learn states and state transitions faster.
The present disclosure provides a number of technical effects and benefits. As an exemplary technical effect and benefit, by encoding information in spike time, the use of spike count or spike rate may be eliminated. Furthermore, as described herein, neuron spike times can be expressed as a continuous representation that is differentiable and thus suitable for gradient-based training techniques. Using gradient-based techniques allows for accurate learning within the network (e.g., at the level of a single spike time) and naturally extends to multi-layer scenarios, which is not possible in rate-coding based training methods. In addition, training the network using gradient-based techniques may be more efficient than various other prior art techniques that are more computationally expensive.
Implementing efficient training of spiking neural networks using gradient-based techniques provides further technical effects and benefits. By the techniques described herein that utilize gradient-based techniques to implement training of spiking neural networks, spiking neural networks may be trained to perform many supervised and reinforcement learning tasks that were previously impossible or at least infeasible to train to perform. In many cases, implementation of spiking neural networks on neuromorphic hardware requires significantly less energy resources than alternatives capable of performing these tasks (e.g., perceptron-based networks).
The trained spiking neural network described above may be adapted to perform a series of machine learning tasks. In particular, the inherent temporal characteristics of trained spiking neural networks make them particularly suitable for machine learning tasks that operate on temporal data such as audio, video, and/or sensor data. Examples of such machine learning tasks include speech recognition, event detection, and pattern recognition.
As another exemplary technical effect and benefit, the network may be enabled to operate asynchronously by encoding information in a succession of spatial spikes in time. This allows better modelling of the human brain and differential equations can be used. Further, in some embodiments, the use of an asynchronous network may allow multiple information rhythms or streams to propagate simultaneously through the network, which may allow for parallel, sequential, and/or cyclical processing of the input.
As another exemplary technical effect and benefit, by encoding information in spike times, neuron firing can be very sparse, as the time of each spike can encode a large amount of information. Thus, the networks described herein may be implemented much more efficiently than networks that encode information using spiking rates, which may themselves be more efficient than conventional non-spiking networks. More specifically, because time-coded neurons typically fire fewer times than rate-based coded neurons, fewer computing resources (e.g., energy resources, processing resources, memory resources, etc.) need to be expended to run the network. Thus, by encoding in spike time (e.g., high information content in time-sparse spikes) rather than spike rate, the number of neuron spikes can be greatly reduced (e.g., each neuron spike can consume resources).
The use of a dual-exponential function in the activation layer of a neuron also provides technical effects and benefits. As an example, the double-exponential function better mimics the actual biological neuron behavior and provides a natural intrinsic rhythm/velocity for information propagation within the network.
As another example, a bi-exponential function creates a more definite maximum over time (e.g., as opposed to a square-wave representation, a single-exponential representation, or other monotonic representation). This allows to generate very clearly defined state transitions between "present" and "future step" without loss of phase coherence.
In addition, since the impact of the incoming spike moves from the exact instant of time of receipt to a point slightly delayed in the future, the summation or integration of the incoming spike can be done more efficiently. This slight delay allows more information to be collected before the neuron spikes.
The use of a bi-exponential function also enables differentiation to occur in double differentiation rather than single differentiation. Double-differential optimized surfaces are generally smoother than single-differential optimized surfaces, which often exhibit waviness. Such a smoother optimized surface may result in faster training times and better convergence, since the gradient descent technique may be able to locate the best point on the surface faster and more easily. Faster training and better convergence may result in saving various resources as less computing resources (e.g., energy resources, processing resources, memory resources, etc.) are spent training the network.
Although the use of a bi-exponential function is particularly emphasized in this disclosure, other functions may be used in addition to or instead of the bi-exponential function. As an example, a gaussian or poisson distribution may be used as or in the time-active layer. As other examples, other non-monotonic and/or unimodal functions may be used in addition to or instead of the bi-exponential function. In general, aspects of the present disclosure may be applied to and/or use any smooth, always positive, function that has a single maximum in the near future and becomes zero in the future.
Exemplary description of temporal coding
In an exemplary embodiment of the proposed model, the information may be encoded in the relative time of the individual spikes. The input features may be encoded in the time domain as the spike times of individual input neurons, one for each different feature. More significant information about the feature may be encoded as an earlier spike in the corresponding neuron. Information may propagate through the network in a temporal manner. Each hidden and output neuron may spike when its membrane potential rises above a fixed threshold. Similar to the input layer, the output layer of the network may encode the results with the relative timing of the output spikes. In other words, the calculation process may include generating a time series of spikes across the entire network in a particular order, the results of which are encoded in the order of the spikes in the output layer.
The model can be used to solve standard classification problems. Given a classification problem with m inputs and n possible classes, the inputs may be encoded as the spike times of individual neurons in the input layer, while the results may be encoded as the index of the first spiking neuron among multiple neurons in the output layer. Examples derived from class k can be correctly classified if and only if the kth output neuron is the first spiking neuron. The earlier output spike may reflect a higher confidence that the network classified the particular instance because it implies a higher synaptic efficiency or a smaller number of pre-synaptic spikes. In a biological setting, a winning neuron may suppress the activity of neighboring neurons through lateral suppression, while in a machine learning environment, the spike time of a non-winning neuron may be used to indicate an alternative prediction of the network. The learning process aims to change the synaptic weights and thus the spike timing, resulting in a target sequence of spikes.
Exemplary spiking neuron architecture
Fig. 1 provides a graphical schematic of an exemplary spiking neuron 10. The spiking neuron 10 may be connected to one or more presynaptic neurons 12, 14, 16 (e.g., they may themselves be spiking neurons). The spiking neuron 10 may be connected to the presynaptic neurons 12, 14, 16 by artificial synapses 18, 20, 22. The presynaptic neurons 12, 14, 16 may transmit spikes to the synaptic neuron 10 through the artificial synapses 18, 20, 22.
Each synapse 18, 20, 22 may have an adjustable weight 24, 26, 28 (e.g., a scalar weight) associated therewith. The weights 24, 26, 28 may change as a result of learning. As described above, since the non-differentiability of spike trains limits the application of back propagation algorithms, the technique of performing this learning rule in the context of spiking neural networks has become one of the most challenging parts of developing multi-layer spiking neural networks.
Referring again to fig. 1, each artificial synapse 18, 20, 22 may be excitatory (e.g., with a positive weight) that increases the membrane potential of the receiving neuron 10 when received, or inhibitory (e.g., with a negative weight) that decreases the membrane potential of the receiving neuron 10 when received.
More specifically, the spiking neuron 10 may have a membrane potential 30. The membrane potential 30 may be a continuous value function with respect to time. In particular, activity (e.g., a transmitted spike) of the presynaptic neuron 12, 14, 16 may modulate or otherwise affect the membrane potential 30 of the spiking neuron 10. The spiking neuron 10 may also have an activation layer 32 that controls the spiking of the neuron (e.g., the spike time of the neuron 10) based on the membrane potential 30.
As an example, the activation layer 32 may generate an action potential or spike when the membrane potential 30 exceeds a dispensing threshold. Thus, in an example, implementing a spiking neuron 10 may include determining a spiking time corresponding to the earliest time that the membrane potential 30 of the spiking neuron 10 is equal to a firing threshold.
When a spiking neuron 10 fires or spikes, the spikes may be sent to one or more downstream neurons along one or more downstream synapses 34. Alternatively, the spike may be the output of the network, depending on the location of the neuron 10 in the model structure. Although one downstream synapse 34 is shown, the spiking output of the neuron 10 may be sent down to any number of downstream synapses 34.
Although not explicitly shown in fig. 1, various other parameters (e.g., synaptic delay parameters, bias parameters, and/or the like) may affect the behavior of spiking neuron 10.
According to an aspect of the disclosure, the activation layer 32 of the spiking neuron 10 may model the leaky-to-input using a bi-exponential function, the afferent neuron spike (e.g., an afferent spike from one of the pre-synaptic neurons 12, 14, 16) providing the leaky-to-input to the membrane potential 30 of the spiking neuron 10. In particular, this is achieved by temporally pairing the form ∈ (t) ═ τ-1e-τtIs obtained by integrating the input exponential synaptic current kernel, wherein tau is the decay constant. The neuronal membrane potential in response to a single afferent spike is then u (t) te-τtIn the form of (1). This function has a gentle rise and a slow decay, at tmax＝τ-1Reaching a peak value. Each synaptic connection has an efficiency or weight. The decay rate has the effect of scaling the sense potential in amplitude and time, while the weight of the synapse has the effect of scaling only the amplitude.
The use of bi-exponential functions in the activation layer 32 of neurons can create more definite maxima in time. This allows for the generation of very clearly defined state transitions between "present" and "future step" without loss of phase coherence.
More specifically, in some embodiments, the bi-exponential function may model the band leakage input as a bi-exponential pulse. The bi-exponential function may be any function that conforms to: e.g. of the type-At-e-BtAnd A is<B, defined as a positive time t. For example, in some embodiments, the dual-exponential function may employ e-t(t-1+ c) wherein c is a hyperparameter. In the example where c is set equal to 1, the double exponential function may take te-tIn the form of (1). Fig. 2A provides an exemplary plot of a band leakage input modeled using a double exponential function of this form. In some embodiments, the dual-exponential function may be mathematically modeled using the alpha function of Rall (see Rall, theoretical synaptic potentials calculated to distinguish different soma-dendrite distributions of synaptic inputs 1967). In some embodiments, the dual-exponential function may be referred to as a "dual-exponential" function.
Referring again to FIG. 1, using the dual exponential function shown in FIG. 2A, a set of weights w is giveni(e.g., 24, 26, 28) and at respective points in time tiAt the spiked presynaptic neuron I (e.g., 12, 14, 16), the membrane potential 30 at time t (if spiking has not occurred) may be represented as
On the other hand, if a neuron has spiked, there are several ways to "reset" it. One example is to restore the membrane potential to its default value and/or to place the neuron in a refractory period that is unable to respond to the incoming stimulus.
Thus, when the membrane potential 30 exceeds the firing threshold, the neuron 10 spikes (see fig. 2A-C). To calculate the spike time t of a neuronout: determining the time t which causes the membrane potential to reach the threshold value theta when risingi≤toutAll presynaptic inputs in case
this may be done by sorting the inputs and adding them one by one to
For a set of inputs I, representingequation 2 using a Lambert (Lambert) W function:
as long as the lambertian W function has a valid argument and the resulting value toutAbove all input spikes, spikes will occur. Since the earlier solution of the equation is valued, the main branch of the lambertian W function can be used. When its independent variable is greater than or equal to-e-1The lambertian W function is real-valued. It can be shown that when equation 2 is extended by Vmem(tmax) This is always the case with solutions ≧ θ, where
Fig. 2B depicts an exemplary graph of a bi-exponential function of different sets of weights w and decay constants τ. The weights scale the function in amplitude, while the decay constants scale the function in both amplitude and time.
Fig. 2C depicts an exemplary graph of membrane potential dynamics in response to excitatory and inhibitory inputs followed by spikes. In this example τ is 1, w is {0.3, -0.4,0.5,0.7,0.5,0.8}, t is {1,8,12,15,17,18}, and a spike occurs at toutAt 18.64.
Exemplary neural network architecture
An exemplary spiking neural network architecture according to the present disclosure may include one or more (e.g., a number of) spiking neurons and/or non-spiking neurons. Some or all of the spiking neurons may have the structure and function shown in fig. 1 and described with respect to fig. 1.
In some embodiments, the neurons of the spiking neural network may be arranged in a plurality of sequential (sequential) layers, including, for example, a plurality of sequential layers each layer comprising spiking neurons (e.g., a "deep" spiking neural network). In a particular example, one or more layers including spiking neurons may be followed by one or more layers containing non-spiking neurons.
The spike network may be a feed-forward network, a cyclic network, a convolutional network, or a combination thereof. Connections between neurons in adjacent layers may be constructed in a full-to-full (all-to-all) configuration and/or a sparse configuration.
In some implementations, the spiking neural network may encode information in the spike time of a spike output by a spiking neuron of the network. Thus, the information output of a neuron may be encoded in its spike time, which depends on the timing and weight of the presynaptic neuron causing its firing. This may enable the network to operate asynchronously. This better models the human brain and enables the use of differential equations and back propagation to adjust for the spike times in the network.
In some embodiments, the input class may be determined by which neuron in the output layer first spikes, for example in a classification problem. In some embodiments, each spiking neuron in the network is allowed to spike only once per cycle.
Further, in some embodiments, the use of an asynchronous network may enable the rhythm or stream (also referred to as a "wavefront") of multiple messages to propagate through the network simultaneously, which may allow the inputs to be processed in parallel, sequentially, and/or cyclically. For example, multiple wavefronts may propagate through the network with different phases (e.g., different but coherent phases). Propagating wavefronts in this manner does not rely on a synchronous clock. Instead, the wavefront itself is the clock. In some embodiments, an explicit clocking policy may be applied for data input and/or output at or near the interface.
In a particular example, the spiking neural network may be ring-shaped in structure. In such embodiments, the wavefront may propagate periodically around the ring network with or without additional inputs, outputs, and/or other modifications (e.g., sequential inputs may be input over time at each cycle).
In some embodiments, the spiking neural network may be implemented in the form of computer-readable instructions stored in a computer-readable medium that are accessed and executed by one or more processors. Alternatively or additionally, the spiking neural network may be implemented in the form of one or more electronic circuits comprising electronic components of the spiking neural network arranged to perform machine learning using electrical current. For example, the respective electronic components that model the dual exponential function may include two capacitors, two resistors, and one or more transistors.
Exemplary training techniques
As an example training technique, a back propagation technique may be used in conjunction with a gradient-based technique to back propagate the loss through multiple layers of the network. For example, the loss may be a supervised loss of a loss function that evaluates network performance on the labeled training data set. Thus, in some embodiments, training the spiking neural network may include determining a gradient of a loss function that evaluates spiking neural network performance over a training data set; and modifying, for at least one of the plurality of spiking neurons, at least one of the one or more weights based at least in part on a gradient of the loss function.
As an example, a spike network may learn to solve a problem, where the input and solution to the problem are encoded in the time of each input spike and output spike. Thus, one possible goal is to adjust the output spike times so that their relative order is correct. Given a classification problem with n classes, neurons corresponding to the correct label should spike earliest. Thus, an exemplary loss function that may be used attempts to minimize the spike time for target neurons and maximize the spike time for non-target neurons. Note that this is in contrast to conventional classification settings that involve probabilities, in which the value corresponding to the correct category is maximized and the value corresponding to the wrong category is minimized. As an exemplary technique to achieve this effect, the spike time o may be in the output layeriThe flexible maximum (softmax) function is used on negative values (always positive):
the cross-entropy loss can be in the conventional form
In some implementations, determining a gradient of a loss function (e.g., the loss or other loss function described above) may include determining, for at least one of a plurality of spiking neurons, a derivative of a spiking time of such spiking neuron with respect to a weight associated with such spiking neuron.
As an example, to minimize the cross-entropy loss described above, the training system may change the weight values of the entire network. This has the effect of delaying or advancing the spike time of the entire network. For with weight wjAt time tjAny presynaptic spike, denoted by E I, reached
since the post-synaptic spike time moves earlier or later in time, the time when the spike is detected is less than the time when the spike is detected
In some embodiments, the gradient may be used to train one or more synaptic delay parameters associated with the neuron. Thus, in some embodiments, determining the gradient of the loss function may include determining, for at least one of a plurality of spiking neurons, a derivative of a spike time of such spiking neuron with respect to a weight associated with such spiking neuron and an incoming spike time associated with an incoming spike received by such neuron.
Additional exemplary details regarding the derivation of the gradient expressions described above are contained in U.S. provisional patent application No. 62/744,150.
Exemplary synchronization pulse
In some embodiments, to adjust the class boundaries in the time domain, a bias in the form of time may be used to adjust the spike time (i.e., delay or advance the spike time in time). In this model, the synchronization pulse may be used as an additional input on some or all layers of the network to provide a time offset on the network. These can be considered similar to the rhythmic activity generated inside biological networks, such as alpha waves in the visual cortex or theta and gamma waves in the hippocampus.
A set of pulses may be connected to all neurons in the network, to neurons within various layers, or to individual neurons. Biasing per neuron is biologically impossible and computationally more demanding, and therefore some proposed models either use a single set of pulses per network to solve simpler problems or a set of pulses per layer to solve more difficult problems. All pulses may be fully connected to all non-input neurons in the network or may be fully connected to all neurons of the non-input layer to which they are assigned.
Each pulse may spike at a predefined and trainable time, providing a reference spike delay. Each set of pulses may be initialized to spikes evenly distributed in time in interval (0, 1). Equation 4 may then be used to learn the spike time for each pulse, while equation 5 is used to train the weights between the pulses and the neurons in the same way as all other weights in the network.
Exemplary hyper-parameters
An exemplary experiment was performed on a fully connected feed forward network with topology n _ hidden (a vector of hidden layer size). Adam optimization with a small batch size of batch size was used to minimize cross-entropy loss. The performance of the Adam optimizer outperforms the random gradient descent. Different learning rates are used for the pulse spike times (learning _ rates _ pulses) and the weights of both the spiking neurons and the non-spiking neurons (learning _ rates). A fixed firing threshold (fire _ threshold) and decay constant (decay _ constant) are used.
Network weight initialization is critical for subsequent network training. In spiking networks, it is important that the initial weights be large enough to cause at least some of the neurons to spike. In the absence of a spike event, no gradient will be used for learning. Thus, in some embodiments, a modified form of Glorot initialization may be used, where the weights are from having a standard deviation
The following table shows some exemplary possible hyper-parameters of the model. The first column shows the default parameters selected to solve the boolean logic problem. The second column shows the search range used in the hyper-parametric search. Asterisks (#) mark ranges probed according to a logarithmic scale; all others were detected linearly. The last column shows the values selected from these ranges to address an exemplary MNIST-based experiment.
Despite careful initialization, in some instances, the network may still become stationary during training. This problem can be prevented by adding a fixed small penalty (penalty _ no _ spike) to the derivatives of all the pre-synaptic weights of the non-firing neurons. Indeed, after the training phase, some neurons will spike too late to contribute to the classification, so these neurons do need to spike.
Another problem is that the gradient can become very large as the spikes become close but not enough for the post-synaptic neuron to reach the firing threshold. In this case, in equations 4 and 5, as its argument approaches-e-1The value of the Lambert W function will be near its minimum (-1), the denominator of the derivative will be near zero and the derivative will be near infinity. To solve this problem, the derivative may be truncated to a fixed value clip _ derivative. Note that this behavior will occur in any activation function that has a maximum (and thus a biologically possible morphology), is differentiable, and has a continuous derivative.
In addition to these hyper-parameters, several other heuristic algorithms for spiking networks may alternatively be used. These include weight attenuation, adding random noise to the spike time of input or all non-output neurons in the network during training, averaging the luminance values in a convolution-like manner, and adding other additional input neurons that respond to the flipped version of the image, similar to the on/off bipolar cells in the retina. Furthermore, in some embodiments, presynaptic neurons may be removed from the presynaptic set once their respective contributions to the potential decay below a decay threshold. This can be accomplished by solving an equation similar to equation 2 to reach the attenuation threshold in the attenuation portion of the function using the-1 branch of the Lambert W function.
Exemplary devices and systems
Fig. 3A depicts a block schematic diagram of an exemplary computing system 100, according to an exemplary embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled through a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop computer), a mobile computing device (e.g., a smartphone or tablet computer), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC (application specific integrated circuit), an FPGA (field programmable gate array), a controller, a microcontroller, etc.), and may be one processor or a plurality of processors operatively connected. Memory 114 may include one or more non-transitory computer-readable storage media such as RAM (random access memory), ROM (read only memory), EEPROM (electrically erasable programmable read only memory), EPROM (electrically programmable read only memory), flash memory devices, disks, and the like, as well as combinations thereof. The memory 114 may store data 116 and instructions 118 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 may store or include one or more spiking neural networks 120. For example, spiking neural network 120 may be or may otherwise include spiking neurons as described herein. The neural network may include a feed-forward neural network, a recurrent neural network (e.g., a long-short term memory recurrent neural network), a convolutional neural network, or other form of neural network. An exemplary spiking neural network 120 is discussed with reference to fig. 1 and 2.
In some implementations, one or more spiking neural networks 120 may be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of a single spiking neural network 120.
Additionally or alternatively, one or more spiking neural networks 140 may be included in or otherwise stored and implemented by the server computing system 130, the server computing system 130 communicating with the user computing device 102 according to a client-server relationship. For example, the spiking neural network 140 may be implemented by the server computing system 140 as part of a network service. Accordingly, one or more networks 120 may be stored and implemented at the user computing device 102, and/or one or more networks 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other exemplary user input components include a microphone, a conventional keyboard, or other means by which a user may provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor and operatively connected processors. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, a disk, etc., and combinations thereof. The memory 134 may store data 136 and instructions 138 that are executed by the processor 132 to cause the server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In the case where the server computing system 130 includes multiple server computing devices, such server computing devices may operate according to a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine-learned spiking neural networks 140. For example, the network 140 may be or may otherwise include various machine-learned models. Exemplary machine-learned models include neural networks or other multi-layered nonlinear models. Exemplary neural networks include feed-forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. An exemplary network 140 is discussed with reference to fig. 1 and 2.
The user computing device 102 and/or the server computing system 130 may train the networks 120 and/or 140 via interaction with a training computing system 150 communicatively coupled through the network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
In particular, the model trainer 160 may train the spiking neural networks 120 and/or 140 based on a set of training data 162. In some implementations, model trainer 160 may perform supervised learning techniques to train the network based on training data 162. Model trainer 160 may perform any of the techniques or operations described in the "exemplary training techniques" section above.
In some implementations, the training examples may be provided by the user computing device 102 if the user has provided consent. Thus, in such an implementation, the network 120 provided to the user computing device 102 may be trained by the training computing system 150 on user-specific data received from the user computing device 102. In some cases, this process may be referred to as a personalization model.
The model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some embodiments, model trainer 160 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other embodiments, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium (such as a RAM hard disk or an optical or magnetic medium).
FIG. 3A illustrates an exemplary computing system that can be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such an implementation, the network 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement the model trainer 160 to personalize the network 120 based on user-specific data.
Fig. 3B depicts a block schematic diagram of an exemplary computing device 190, according to an exemplary embodiment of the present disclosure. Computing device 190 may be a user computing device or a server computing device.
Computing device 190 includes a plurality of applications (e.g., applications 1 through N). Each application contains its own machine learning library and model for machine learning. For example, each application may contain a machine-learned model. Exemplary applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As shown in fig. 3B, each application may communicate with many other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or other components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some embodiments, the APIs used by each application are specific to that application.
Fig. 3C depicts a block schematic diagram of an exemplary computing device 195, according to an exemplary embodiment of the present disclosure. Computing device 1 may be a user computing device or a server computing device.
The central smart inlay includes a number of machine-learned models. For example, as shown in FIG. 3C, a respective machine-learned model (e.g., model) may be provided for each application and managed by a central smart tier. In other embodiments, two or more applications may share a single machine-learned model. For example, in some embodiments, the central smart inlay may provide a single model (e.g., a single model) for all applications. In some embodiments, the central smart inlay is included within or otherwise implemented by the operating system of the computing device 195.
The central smart inlay may communicate with a central device data plane. The central device data layer may be a centralized repository of data for the computing devices 195. As shown in fig. 3C, the central device data layer may communicate with many other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).
Additional disclosure
The techniques discussed herein make reference to servers, databases, software applications, and other computer-based systems, and the actions taken by such systems and the information sent to and from such systems. The inherent flexibility of computer-based systems allows for a wide variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components operating in combination. The database and applications may be implemented on a single system or may be distributed across multiple systems. The distributed components may run sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific exemplary embodiments thereof, each example is provided by way of illustration, and not limitation, of the present disclosure. Alterations, modifications and equivalents of such embodiments may be readily obtained by those skilled in the art, after understanding the foregoing description. Accordingly, the present disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (22)
1. A computer system, comprising:
one or more processors; and
one or more non-transitory computer-readable media collectively storing:
a machine-learned spiking neural network comprising one or more spiking neurons having an activation layer that models a leaky-in input using a bi-exponential function, an afferent neuron spike providing the leaky-in input to a membrane potential of the spiking neuron; and
instructions that, when executed by the one or more processors, cause the computer system to perform operations comprising:
obtaining a network input;
implementing the machine-learned spiking neural network to process the network input; and
receiving a network output generated by the machine-learned spiking neural network as a result of processing the network input.
2. The computer system of claim 1, wherein the machine-learned spiking neural network encodes information in respective spike times associated with the one or more spiking neurons.
3. The computer system of claim 1 or claim 2, wherein the bi-exponential function models the band leakage input as a bi-exponential pulse.
4. The computer system of claim 1, 2 or 3, wherein the bi-exponential function has e-t(t-1+ c) wherein c is a hyperparameter.
5. The computer system of claim 1, 2 or 3, wherein the bi-exponential function has te-tIn the form of (1).
7. The computer system of claim 6, wherein implementing the machine-learned spiking neural network comprises: for each of the one or more spiking neurons, determining a spiking time corresponding to an earliest time that a membrane potential of the spiking neuron is equal to a firing threshold.
8. The computer system of claim 7, wherein determining the spike time for each of the one or more spiking neurons comprises applying a Lambert W function to determine the spike time.
9. The computer system of claim 8, wherein the operations further comprise:
prior to obtaining the network input, training the machine-learned spiking neural network on training data via a gradient descent technique, wherein training the machine-learned spiking neural network via gradient descent comprises:
for each of the one or more spiking neurons, determining one or both of:
the spike time of the spiking neuron with respect to the time point tiA derivative of (a); and
the spike time of the spiking neuron with respect to the weight wiWherein the spiking time corresponds to an earliest time that a membrane potential of the spiking neuron equals a firing threshold; and
for the one or moreEach of the spiking neurons being based at least in part on a spiking time of the spiking neuron with respect to the point in time tiAnd the spike time of the spiking neuron with respect to the weight wiOne or both of the derivatives of one or more of them to modify the weight wiAt least one of (a).
10. The computer system of any preceding claim, wherein the machine-learned spiking neural network comprises a plurality of layers, at least two layers of the plurality of layers comprising at least one of the one or more spiking neurons, and wherein the machine-learned spiking neural network has been trained on training data using a back propagation technique.
11. The computer system of any preceding claim, wherein the operations further comprise:
training the machine-learned spiking neural network on training data via a gradient descent technique to simultaneously learn both parameters of the machine-learned spiking neural network and a topology of the machine-learned spiking neural network.
12. A computer-implemented method for training a spiking neural network that encodes information with respective spike times associated with a plurality of spiking neurons included in the spiking neural network, the method comprising:
obtaining, by one or more computing devices, a data description of the spiking neural network comprising the plurality of spiking neurons, wherein each of the plurality of spiking neurons is respectively connected to one or more pre-synaptic neurons via one or more artificial synapses having one or more weights associated therewith, wherein each of the plurality of spiking neurons has an activation layer that controls a respective spiking time of the spiking neuron based on a membrane potential of the spiking neuron, and wherein the activation layer of each of the plurality of spiking neurons comprises a bi-exponential function that models incoming spikes received from the one or more pre-synaptic neurons as a band-leakage input to the membrane potential; and
training, by the one or more computing devices, the spiking neural network based on a training data set, wherein training, by the one or more computing devices, the spiking neural network comprises:
determining, by the one or more computing devices, a gradient of a loss function that evaluates performance of the spiking neural network over the training dataset; and
modifying, by the one or more computing devices, for at least one of the plurality of spiking neurons, at least one of the one or more weights based at least in part on a gradient of the loss function.
13. The computer-implemented method of claim 12, wherein:
each of the plurality of spiking neurons receiving the afferent spike from the one or more pre-synaptic neurons at a respective incoming spike time; and
determining, by the one or more computing devices, a gradient of the loss function comprises: determining, by the one or more computing devices, a derivative of a spike time of the spiking neuron with respect to the entry spike time for at least one of the plurality of spiking neurons.
14. The computer-implemented method of claim 12 or 13, wherein determining, by the one or more computing devices, the gradient of the loss function comprises: determining, by the one or more computing devices, for at least one of the spiking neurons, a derivative of a spiking time of the spiking neuron with respect to one or more of the weights associated with the spiking neuron.
15. The computer-implemented method of claim 12, 13, or 14, wherein training, by the one or more computing devices, the spiking neural network further comprises: modifying, by the one or more computing devices, for at least one of the plurality of spiking neurons, at least one synaptic delay parameter based at least in part on a gradient of the loss function.
16. The computer-implemented method of any of claims 12-15, wherein:
the plurality of spiking neurons arranged in a plurality of layers; and is
Training, by the one or more computing devices, the spiking neural network comprises: back propagating, by the one or more computing devices, the loss function through the plurality of layers.
17. The computer-implemented method of any one of claims 12-16, wherein, for each of the plurality of spiking neurons, the membrane potential has a value if the neuron has not spiked yet
18. An electronic device, comprising:
a machine-learned spiking neural network comprising one or more spiking neurons;
wherein each of the one or more spiking neurons has an activation layer that models a leaky-in input using a bi-exponential function, an afferent neuron spike providing the leaky-in input to a membrane potential of the spiking neuron;
and wherein the machine-learned spiking neural network is configured to receive a network input and process the network input to generate a network output.
19. The electronic device of claim 18, wherein the machine-learned spiking neural network comprises computer-readable instructions stored on a non-transitory computer-readable medium.
20. The electronic device of claim 18, wherein the machine-learned spiking neural network comprises one or more electronic circuits comprising electronic components arranged to perform the machine-learned spiking neural network using electrical current.
21. The electronic device of claim 20, wherein, for each of the one or more spiking neurons, the corresponding electronic component that models the double exponential function comprises two capacitors, two resistors, and one or more transistors.
22. A computer program comprising machine-readable instructions which, when executed by a computing device, cause it to perform the method of any of claims 12 to 17.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862744150P | 2018-10-11 | 2018-10-11 | |
US62/744,150 | 2018-10-11 | ||
PCT/US2019/055848 WO2020077215A1 (en) | 2018-10-11 | 2019-10-11 | Temporal coding in leaky spiking neural networks |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112437929A true CN112437929A (en) | 2021-03-02 |
Family
ID=68470609
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980047993.9A Pending CN112437929A (en) | 2018-10-11 | 2019-10-11 | Temporal coding in spiking neural networks with leakage |
Country Status (4)
Country | Link |
---|---|
US (1) | US20210232930A1 (en) |
EP (1) | EP3815000A1 (en) |
CN (1) | CN112437929A (en) |
WO (1) | WO2020077215A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210048806A1 (en) * | 2019-08-16 | 2021-02-18 | Arizona Board Of Regents On Behalf Of Arizona State University | System and methods for gray-box adversarial testing for control systems with machine learning components |
WO2022118340A1 (en) * | 2020-12-03 | 2022-06-09 | Indian Institute Of Technology Delhi | Novel activation function with hardware realization for recurrent neuromorphic networks |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170228646A1 (en) * | 2016-02-04 | 2017-08-10 | Qualcomm Incorporated | Spiking multi-layer perceptron |
-
2019
- 2019-10-11 US US16/972,427 patent/US20210232930A1/en active Pending
- 2019-10-11 EP EP19798767.0A patent/EP3815000A1/en active Pending
- 2019-10-11 CN CN201980047993.9A patent/CN112437929A/en active Pending
- 2019-10-11 WO PCT/US2019/055848 patent/WO2020077215A1/en unknown
Also Published As
Publication number | Publication date |
---|---|
US20210232930A1 (en) | 2021-07-29 |
EP3815000A1 (en) | 2021-05-05 |
WO2020077215A1 (en) | 2020-04-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11853875B2 (en) | Neural network apparatus and method | |
KR102492318B1 (en) | Model training method and apparatus, and data recognizing method | |
Rumelhart et al. | The basic ideas in neural networks | |
US10032498B2 (en) | Memory cell unit and recurrent neural network including multiple memory cell units | |
US10970626B2 (en) | Multi-memristive synapse with clock-arbitrated weight update | |
Neftci et al. | Event-driven contrastive divergence for spiking neuromorphic systems | |
KR101700140B1 (en) | Methods and apparatus for spiking neural computation | |
EP3136304A1 (en) | Methods and systems for performing reinforcement learning in hierarchical and temporally extended environments | |
US9330355B2 (en) | Computed synapses for neuromorphic systems | |
US20150212861A1 (en) | Value synchronization across neural processors | |
US9959499B2 (en) | Methods and apparatus for implementation of group tags for neural models | |
US20150262054A1 (en) | Analog signal reconstruction and recognition via sub-threshold modulation | |
CN112437929A (en) | Temporal coding in spiking neural networks with leakage | |
US11100396B2 (en) | Self-adjusting threshold for synaptic activity in neural networks | |
US9418332B2 (en) | Post ghost plasticity | |
Hu et al. | An STDP-based supervised learning algorithm for spiking neural networks | |
US9342782B2 (en) | Stochastic delay plasticity | |
JP7149503B2 (en) | A Computational Method for Feedback in Hierarchical Neural Networks | |
Zhou et al. | Evolutionary optimization of liquid state machines for robust learning | |
Kubo et al. | Biologically-inspired neuronal adaptation improves learning in neural networks | |
Karimi et al. | Digital implementation of biologically inspired Wilson model, population behavior, and learning | |
US20150213356A1 (en) | Method for converting values into spikes | |
Jin et al. | Calcium-modulated supervised spike-timing-dependent plasticity for readout training and sparsification of the liquid state machine | |
Kolay et al. | Classification with some artificial neural network classifiers trained a modified particle swarm optimization | |
Végh et al. | On the Role of Information Transfer’s Speed in Technological and Biological Computations |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |