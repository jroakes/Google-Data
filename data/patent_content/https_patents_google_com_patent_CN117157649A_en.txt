CN117157649A - Machine learning rank distillation - Google Patents
Machine learning rank distillation Download PDFInfo
- Publication number
- CN117157649A CN117157649A CN202280026681.1A CN202280026681A CN117157649A CN 117157649 A CN117157649 A CN 117157649A CN 202280026681 A CN202280026681 A CN 202280026681A CN 117157649 A CN117157649 A CN 117157649A
- Authority
- CN
- China
- Prior art keywords
- item
- training
- score
- machine learning
- model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000010801 machine learning Methods 0.000 title claims abstract description 126
- 238000004821 distillation Methods 0.000 title claims abstract description 91
- 238000012549 training Methods 0.000 claims abstract description 202
- 238000000034 method Methods 0.000 claims abstract description 90
- 238000003860 storage Methods 0.000 claims abstract description 20
- 238000004590 computer program Methods 0.000 claims abstract description 14
- 230000006870 function Effects 0.000 claims description 26
- 230000009471 action Effects 0.000 claims description 13
- 230000004044 response Effects 0.000 claims description 13
- 230000008569 process Effects 0.000 description 27
- 238000012545 processing Methods 0.000 description 21
- 238000013528 artificial neural network Methods 0.000 description 11
- 238000013459 approach Methods 0.000 description 8
- 230000003993 interaction Effects 0.000 description 8
- 238000013140 knowledge distillation Methods 0.000 description 8
- 238000004891 communication Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 5
- 238000004364 calculation method Methods 0.000 description 4
- 238000011156 evaluation Methods 0.000 description 4
- 238000004519 manufacturing process Methods 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 230000008859 change Effects 0.000 description 3
- 238000005553 drilling Methods 0.000 description 3
- 230000006872 improvement Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000012546 transfer Methods 0.000 description 3
- 230000003247 decreasing effect Effects 0.000 description 2
- 230000007547 defect Effects 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 230000002349 favourable effect Effects 0.000 description 2
- 230000002452 interceptive effect Effects 0.000 description 2
- 238000007477 logistic regression Methods 0.000 description 2
- 239000000463 material Substances 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- 238000011084 recovery Methods 0.000 description 2
- 230000008439 repair process Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 239000013589 supplement Substances 0.000 description 2
- CYJRNFFLTBEQSQ-UHFFFAOYSA-N 8-(3-methyl-1-benzothiophen-5-yl)-N-(4-methylsulfonylpyridin-3-yl)quinoxalin-6-amine Chemical compound CS(=O)(=O)C1=C(C=NC=C1)NC=1C=C2N=CC=NC2=C(C=1)C=1C=CC2=C(C(=CS2)C)C=1 CYJRNFFLTBEQSQ-UHFFFAOYSA-N 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 239000012141 concentrate Substances 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000007423 decrease Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000003754 machining Methods 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 235000015927 pasta Nutrition 0.000 description 1
- 238000003825 pressing Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/096—Transfer learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training and using machine learning models of distillation. In one aspect, a method includes obtaining a first input comprising training example sets, each training example set comprising one or more feature values, and for each item, a result tag that indicates whether the item has a positive result. The first machine learning model is trained using the first input, and is configured to generate a set of scores that represent whether the item will have a positive result in the context of the training example set and with each other item in the example set. The set of component scores for each example set is used to train a machine learning model of distillation. The machine learning model of distillation is configured to generate a score of distillation.
Description
Technical Field
The present description relates to machine learning knowledge distillation for recommendation systems.
Background
Training and evaluating complex machine learning models, such as Deep Neural Networks (DNNs), can be computationally complex, requiring powerful computers to do so. While these models can produce very accurate results, the use of these models is computationally infeasible in computationally constrained environments such as mobile phones or personal computers.
Disclosure of Invention
In general, this specification describes machine learning knowledge distillation techniques that improve rank accuracy while maintaining prediction accuracy.
Improving the accuracy of ranking is important in recommendation systems (i.e., systems that predict a rating or other score for each item in a set of items based on one or more metrics related to the item, its intended use, past performance in that use, its user, etc., and utilize those ratings to provide recommendations). For example, given appropriate data about defects in components produced by a manufacturing process, a properly trained recommendation system may provide a ranked set of automated process sets that may be used to correct the defects, and may first attempt the highest ranked alternative. Note that in this case, the ranking of alternatives may be more important than the actual score, as the highest ranked alternative will typically be tried first.
The knowledge distillation techniques described in this document may also be used to improve the display of interactive content. For example, when determining, for example, which digital components to display in conjunction with search results or other digital content and where, the ranking of candidate digital components may be more important (or equally important) than the actual score generated by the machine learning model used to predict the performance (e.g., interaction rate) of the digital components. The highest ranked number component may be displayed in the most prominent position, while the sequentially lower ranked number components may be displayed in less prominent positions.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The techniques described in this document may be used in a recommendation system to accurately rank and score items using a distilled machine learning model (also referred to as a student model) that requires less computing resources than the corresponding teacher model, allowing recommendations to be provided by computers having less computing power than would be required to process data using the teacher model. Furthermore, the following techniques may be used in a recommendation system to calculate scores and rankings in less time than using a teacher model, allowing for quick and large-scale response. This also enables distilled machine learning models to be used in situations where quick provision of output is required, such as on assembly lines and in the context of delivering digital content (e.g., content including images and/or video) to a client device, where excessive latency can cause the user device to make mistakes while waiting for the content. Training a distilled machine learning model using a robust teacher model may provide these performance improvements with no or minimal loss in accuracy in ranking the items. Furthermore, the techniques described below, particularly the list-wise penalty calculation technique, result in a more efficient implementation, i.e., an implementation that requires fewer processor instructions to complete. Furthermore, using the techniques described below, a teacher model that does not meet various system constraints may be trained to provide even stronger signals. The deployed model, which must meet these constraints, can then still utilize the teacher model's signals to improve its performance, despite these constraints. In addition, due to complexity and/or system requirements, student models may not be able to train on many data examples, such as a distributed system in which the student model is a node, but the teacher is a centralized model (e.g., joint learning). Using the techniques described below, a teacher may train on a data set that a student is unable to train while still providing the student with the ability to benefit from such examples, including accurate ranking.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include obtaining a first input that includes a training example set, each training example set including, for a set of items, one or more feature values that represent features in which a context of each item in the set of items is recommended, and, for each item, a result tag that represents whether the item has a positive result. The first machine learning model is trained using the first input. The first machine learning model is configured to generate a set of scores for each training example set, wherein the set of scores for each training example set may include a training score for each item in the training example set that represents whether the item will have a positive result when presented in the context of the training example set and with each other item in the example set. The set of component scores for each example set is used to train a machine learning model of distillation. The distilled machine learning model is configured to generate a distilled score for each item in a set of actual items, the distilled score representing: (i) When in a given context and presented with each other item in the set of actual items, whether the item will have a positive result, and (ii) the ranking of the item in the set of actual items. A positive result may indicate that a particular action occurred with respect to the item when the item was selected for deployment to the device. Other embodiments of this aspect include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods and encoded on computer storage devices.
These and other implementations can each optionally include one or more of the following features. Each item may be a digital component. The training system may provide a model of the distillation to a recommendation system that distributes the digital components. The recommendation system may determine digital components to provide to the client device in response to a request received from the client device and provide the selected digital components to the client device.
For each item of the training example set, the distilled machine learning model may be trained using: (i) a real label corresponding to the result label of the item; (ii) A comparison between the distilled model score for the item and the teacher model score for the item; and (iii) a comparison between the ranking of the item in the items of the plurality of training example sets and the true tags of each item; and project-wise score differences between training examples within the same training example set. The loss may be an L2 loss.
Training the distilled machine learning model may include determining project-wise score differences between training examples within the same training example set; and minimizing losses corresponding to the per-item score differences.
Training the distilled machine learning model may include, for each pair of items in the same training example set, determining each per-item score difference by: (i) Determining a first difference between a first teacher model score for a first one of the pair of items and a second teacher model score for a second one of the pair of items; (ii) Determining a second difference between the model score of the first retum of the first item and the model score of the second retum of the second item in the pair of items; and (iii) determining a difference between the first difference and the second difference as a per-item score difference for the pair of items.
Training the distilled machine learning model may include reducing aggregate values per project score differences for each training example set.
Training the machine learning model of distillation may include determining project-wise score differences by: for a first item in the list of items in the same training example set and each second item in the list of items in the same training example set, wherein the first item is different from the second item: (i) determining a first difference between a first teacher model score of the first item and a second teacher model score of the second item, (ii) determining a second difference between a model score of the first distill of the first item and a model score of the second distill of the second item, and (iii) determining an individual loss value based on the first and second differences; and determining a per-list loss value based on the individual loss values.
Training the machine learning model of distillation may include reducing aggregate values of tabulated loss values for each training example set. Each per-item score difference may be a per-score difference or a per-list score difference.
Training the machine learning model of distillation may include computing a sum of squares of differences between the losses computed for the items over all of the items of the plurality of training example sets as a loss function.
Training the distilled machine learning model may include, for each item in the plurality of training example sets, determining a loss function based on a comparison between a result of the item predicted by the distilled model and an actual result of the item represented by a result tag of the item.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1A is a diagram of an example data flow for generating and using a machine learning model of distillation to provide recommendations.
FIG. 1B is a diagram of an example environment in which a distill recommendation system provides recommendations using a machine learning model of distillation.
FIG. 2 is a flow chart of an example process for training and providing a machine learning model of distillation.
FIG. 3 is a flow chart of an example process for providing results to a query using a distilled machine learning model.
FIG. 4 is a block diagram of an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
Knowledge distillation may be applied to machine learning models, including Deep Neural Networks (DNNs), to extract information captured by training a complex (or more knowledgeable) teacher model and transmit that knowledge to a simpler student model and thus require less computational resources to evaluate. While training a teacher model may require a significant investment in computational resources, using knowledge distillation, student models may be deployed to perform reasoning on devices with more limited resources and still produce results that are nearly as accurate as the teacher model would produce.
However, due to less knowledge available, the predictions of the student model may not be as accurate as the teacher model. In some cases, inaccuracy may result in inaccurate result ranking. For example, consider that the teacher model produces a score of 0.669 for one example, 0.667 for the second example, and the student model produces scores of 0.667 and 0.668. In this example, although the scores generated by the student model are very similar to the scores generated by the teacher model, the ranking of the scores is incorrect: in the teacher model, the first set of feature data is ranked before the second set of feature data, while in the student model, the second set of feature data is ranked before the first set of feature data.
In the context of a recommendation system, a goal that may be equally important as or more important than accuracy is to rank the examples appropriately. However, optimizing for this purpose should not reduce the prediction accuracy. The systems and techniques described in this document for ranking knowledge distillation from a powerful or more knowledgeable teacher model to a simpler or less knowledgeable teacher model may be applied with optimization and distillation of prediction accuracy.
In a recommendation system, such as distributing digital components or content/document recommendations with interactive rate prediction, ranking items according to possible positive results may be as important as or more important than predicting the probability of such results. For example, such a system can only show the top k items to the user, and if more items with more favorable results are shown in a better position, the position of the shown items also affects the results. If the ranking is incorrect, the result may therefore not be the best. Without the techniques described in this document, the system may train on separate examples (or items) without the ability to consider interactions or effects between items that are provided (e.g., displayed) together in response to a user request. For some applications, accurate prediction of labels and accurate ranking of results are important. For the interaction rate of digital components provided to the user, the ranking may determine which digital component is shown to the user, while accurate tag probability predictions are important to determine the amount of digital components that need to be provided to the publisher to display.
The recommendation system may train on the tagged data. For example, cross entropy loss on a label may be used to train the probability of a label for a certain example. In many cases, training for tag probabilities does not take into account the ranking of one example relative to another. It also does not consider interactions between multiple examples that are displayed in conjunction in response to a query. Good accuracy of the predicted tag probability may also extend to ranking if the model is correctly specified. However, the real model may in some sense erroneously specify that some features affecting the label result may not be available to the designer. This may result in disregarding predictions of unusable features. This may lead to the situation that the correct ranking may not always be consistent with the label prediction leading to the best average accuracy. For example, item a may have a higher expected positive label on all of its occurrences than item B. However, when they occur together (in a small fraction of examples), B always has a more positive result.
Thus, while the positive marginal rate of a is higher, B should be ranked higher than a when they are shown together in the same set.
FIG. 1A is a diagram of an example data flow 100 for generating and using a machine learning model of distillation to provide recommendations. FIG. 1B is a diagram of an example environment 101 in which a distill recommendation system provides recommendations using a machine learning model of distillation.
Knowledge distillation may include distilling (transmitting) predictions from expensive or data-rich teacher models to less computationally complex student models. The rank distillation technique described in this document is a form of knowledge distillation that may involve distilling (transmitting) rank predictions from predictions (or scores) of expensive or data-rich teacher models to computationally less complex student models, which may be referred to as distilled machine learning models.
The ranking between two items depends on the relationship between scores reflecting the results of the two items. In some implementations, the first item ranks higher if the score of the first item is higher than the score of the second item, and vice versa. However, when two items are provided together as a recommendation, the score does not always reflect which of the multiple items will receive a positive result.
In a recommendation system, the results are typically binary: one item is selected (positive result) or one item is not selected (negative result). When comparing the ranks in this case, if the first item is selected and the second item is not selected, the comparison result tag of the first item is 1; if the second item is selected but the first item is not selected, the comparison result label of the first item is 0; and if both items are selected or neither are selected, the comparison result tag of both items is 0.5. In some cases, the latter examples may be omitted from the calculation. Note that from the perspective of the second item, a similar tag map may be derived.
This document describes a framework that distills the rank from the teacher model to the student model and is agnostic to the actual score of a given example. Training of the actual score may be addressed by loss of accuracy of the model, and in some embodiments, by conventional distillation loss minimization for additional levels of accuracy.
The example environment 100 for rank distillation includes a machine learning model training system 150 (also referred to as a "training system" 150 for brevity) and a recommendation system 170. Training system 150 includes training example acquisition engine 152, machine learning model training engine 155, distilled machine learning model training engine 160, and model provisioning engine 165.
The recommendation system 170, which may be implemented using one or more computers at one or more locations, may provide recommendations, for example, in response to the request 132. The recommendation may include a set of recommended items, which may be ordered based on the ranking. In some implementations, the recommendation system 170 can provide a single recommended item based on the ranking.
The recommendation system 170 may be used in a variety of environments or contexts and recommend different items based on the environments or contexts. For example, in the context of a manufacturing facility, the recommendation system 170 may recommend tasks or processes to repair or improve a component. In another example, the recommendation system 170 may recommend digital content, such as electronic resources (e.g., web pages), in response to a query and provide a set of search results referencing the electronic resources. Another example of digital content that the recommendation system 170 can recommend is a digital component. For example, the recommendation system 170 may provide digital components for display with other digital content.
As used throughout this document, the phrase "digital component" refers to a discrete unit of digital content or digital information (e.g., a video clip, an audio clip, a multimedia clip, an image, text, or another unit of content). The digital components may be electronically stored in the physical memory device as a single file or collection of files, and the digital components may take the form of video files, audio files, multimedia files, image files, or text files, and may include advertising information, video recommendations, and the like. For example, the digital component may be content intended to supplement web page content, application content (e.g., application pages), or other resources displayed by the application. More specifically, the digital components may include digital content related to the resource content, e.g., the digital components may be related to the same topic as the web page content, or related topics. Thus, the provision of digital components may supplement and generally enhance web pages or application content.
The search results and/or the numerical components may be ordered based on a ranking generated by the recommendation system 170 using a machine learning model, as described in more detail below. If multiple search results or numeric components are provided, the search results or numeric components may be ranked based on scores output by the machine learning model, as described in more detail below. If only a single search result or numeric component is provided, the recommendation system 170 may provide the search result or numeric component with the highest score output by the machine learning model.
The training example acquisition engine 152 may acquire training data including the training examples 110 from the training example data store 115. Training example data store 115 may be any suitable data storage system such as a relational database, unstructured database, file system, cloud-based storage, and the like.
Training examples 110 may be grouped into training example set 112. Each training example 110 may correspond to a particular item, such as a particular digital component. For a set of items (i.e., training examples 110), each training example set 112 may include one or more feature values representing features that recommend the context of each item in the set of items, and for each item, a result tab that indicates whether the item has a positive result.
For example, if the recommendation system 170 recommends digital content, the item may be a digital component and the features may include features related to the context in which the digital component is displayed. For example, the characteristics may include a resource locator (e.g., a Uniform Resource Locator (URL) of an electronic resource for displaying the digital component, a number of digital component slots (slots) on the electronic resource, a geographic location of a device on which the digital component is displayed, a date and time at which the digital component is displayed, a keyword for a query for which search results are provided, a hyperlink of the electronic resource, the presence of an image on the electronic resource, a text size of the electronic resource, a layout of the electronic resource, etc.).
Each training example set 112 may include training examples 110 recommended in the same context. For example, each training example set 112 may include training examples 110 of items that are recommended together (e.g., displayed together in the same context). In another example, each training example set 112 may include training examples of items recommended in the same context but at different times (e.g., not together). For example, training example set 112 may include training examples of multiple digital components displayed on a given web page but at different times and/or to different users.
In some implementations, the result tag of the training example has a value of 0 for negative results and a value of 1 for positive results. Other suitable values may be used to represent positive and negative results. In the context of the recommendation system 170, a negative result may indicate that the item fails to meet the target, while a positive result may indicate that the item meets the target.
If the recommendation system 170 recommends digital content, the results tab may indicate whether the item corresponding to the training example received user interaction, such as whether it was selected by a user. A positive result may indicate that digital content is selected, while a negative result may indicate that digital content is not selected.
If the recommendation system 170 recommends an action or process to repair a component of a manufacturing facility, the result tag of the item (e.g., the action or process) may indicate whether the action or process successfully repaired the component.
In another example, the characteristic values may be associated with properties of components used during a machining operation (such as a drill bit), including drill bit length, drill bit diameter, drill bit geometry, material to be drilled, angle of drilling operation, and the like. A recommended drill bit may be considered successful if and only if the recommended drill bit meets the target, for example, drills to the correct depth at the correct location within a specified time.
Table 1 shows an example of a training data set including Training Data (TD) for N items in the set, and M features and results (O) for each item.
TABLE 1
TD 11 | TD 12 | ... | TD 1M | O 1 |
TD 21 | TD 22 | ... | TD 2M | O 2 |
TD 31 | TD 32 | ... | TD 3M | O 3 |
... | ... | ... | ... | ... |
TD N1 | TD N2 | ... | TD NM | O N |
Training examples may be stored in a structured text format such as extensible markup language (XML) or Javascript object notation (JSON), encoded as binary data, or stored in another suitable format.
TABLE 2
TD 111 | TD 112 | ... | TD 11M | O 11 |
TD 121 | TD 22 | ... | TD 2M | O 12 |
... | ... | ... | ... | ... |
TD 1N1 | TD 1N2 | ... | TD 1NM | O 1N |
TD 211 | TD 212 | ... | TD 21M | O 21 |
TD 221 | TD 222 | ... | TD 22M | O 22 |
... | ... | ... | ... | ... |
TD 2N1 | TD 2N2 | ... | TD 2NM | O 2N |
... | ... | ... | ... | ... |
TD P11 | TD P12 | ... | TD P1M | O P1 |
TD P21 | TD P22 | ... | TD P2M | O P2 |
... | ... | ... | ... | ... |
TD PN1 | TD PN2 | ... | TD PNM | O PN |
Training examples may be grouped into training example sets 112, or "sets" for brevity, as shown in Table 2, containing P sets of N items each, with M features for each item. (note that the collection need not have the same number of items). Each set may contain related training examples. For example, in the context of the recommendation system 170, results related to one query may be grouped into one set, while results related to a second query may be grouped into a different set. The system may store training examples set 112 in a data structure such as a hash map that uses the set identifier as a key and training examples in the set as values associated with the corresponding key.
Training example acquisition engine 152 may provide training examples 110 to machine learning model training engine 155.
The machine learning model training engine 155 may be configured to train the machine learning model 120 (i.e., a teacher model), such as a deep neural network, using the training example set 112. The machine learning model training engine 155 may train the machine learning model 120 using supervised machine learning model training techniques as described below. Note that while this specification describes a deep neural network, other machine learning models, such as linear models, may also be used.
Once the teacher machine learning model 120 is trained, the distilled machine learning model training engine 160 may execute the trained teacher machine learning model 120 using the training examples 110 assigned to the training set 112 (which may be training examples for training the trained machine learning model 120 or other training examples) to produce results 125 assigned to the result set 126. The system may store the result sets 126 (where each result set is associated with a training set used to generate it) in a data structure such as a hash map that uses the result set identifiers as keys and the results associated with training examples in that set as values associated with the corresponding keys. The system may also use the result set 126 for analysis.
Distilled machine learning model training engine 160 may then use training examples 110 and results 125 to train distilled (student) machine learning model 130. Distilled machine learning model 130 may be any suitable type of machine learning model, such as a neural network. Preferably, distilled machine learning model 130 will require less computing resources to execute and will execute faster than machine learning model 120. When performed using the training examples 110, the distilled machine learning model 130 is configured to produce results 135 that approximate, in both score and ranking, the results 135 to the results 125 produced by the machine learning model 120 on the training examples 110.
Once distilled machine learning model 130 is trained, model provider engine 165 may provide the trained distilled machine learning model for use by recommendation system 170 or other systems utilizing such machine learning model.
Recommendation system 170 may include distilled machine learning model acquisition engine 172, distilled machine learning model evaluation engine 175, and device interaction engine 180.
Distilled machine learning model acquisition engine 172 may acquire distilled machine learning model 130 from machine learning model training system 150 or from a repository configured to store distilled machine learning models. For example, distilled machine learning model acquisition engine 172 may obtain distilled machine learning model 170 (i.e., a student model) from machine learning model training system 150 over a network. In another example, the machine learning model training system 150 may push the distilled machine learning model 170 to the distilled machine learning model acquisition engine 172 after the model is trained.
The recommender system 170 may then accept the query requests 132 from the one or more devices 185a-185 n. Each query request may be a request for one or more recommended items. If the recommender system 170 is located in a domain corresponding to URL example. Com, the query request may be of the form: https: /(www.example.com/? q= < query_request >. The query request may include components such as keywords (e.g., "football," "pasta," "road," etc.), constraints (e.g., "search on website example. Com," "search for results after 1 month 1 day 2020," etc.), and so forth. The query request may also include data describing the context of the requested recommended item as part of the query. For example, if the query request is for search results, the query request may include keywords for the query, the geographic location of the device submitting the query request, the time of day, the type of device submitting the query request, and so forth. If the query request is for a digital component, the query request may include a resource locator for the electronic resource with which the digital component(s) will be displayed, the number of digital component slots on the resource, and so forth.
The components of the request may be used as characteristic values, or to generate corresponding characteristic values, which are used by the recommendation system 170 to select one or more recommended items to be provided in response to the query request. Returning to the drill bit example, the request may include features such as the type of material being drilled, the desired drilling depth, the drilling angle, and the like.
The device interaction engine 180 within the recommender system 170 may then accept the query requests 132 from the devices 185a-185 n. Devices 185a-185n may be any type of network-enabled computing device including desktop computers, laptop computers, mobile phones, server computers, robotic environments, and the like.
The device interaction engine 180 may receive the query request 132 via any suitable network protocol, such as hypertext transfer protocol (HTTP) or HTTP-Secure (HTTPs).
In response to receiving the query request 132, the recommendation system 170 may invoke the distilled machine learning model evaluation engine 175 to execute the distilled machine learning model 130 for the query request 132, for example, by providing feature values corresponding to the query request as input to the distilled machine learning model 130. Distilled machine learning model evaluation engine 175 may generate results 135 associated with query request 132 and return results 135 to device 185.
FIG. 2 is a flow chart of an example process 200 for training and providing a machine learning model of distillation. For convenience, process 200 will be described as being performed by a machine learning model training system (e.g., machine learning model training system 150 of fig. 1A and 1B) that is suitably programmed to perform the process. The operations of process 200 may also be implemented as instructions stored on one or more computer-readable media, which may be non-transitory, and execution of the instructions by one or more data processing apparatus may cause the one or more data processing apparatus to perform the operations of process 200. One or more other components described herein may perform the operations of process 200.
In operation 205, the system obtains a training example. Training examples may be obtained from the training example data store using any suitable data retrieval method, such as retrieving data from a relational database using a Structured Query Language (SQL) query, retrieving information from a file system using a file system request, retrieving data from a web server using an HTTP request, and so forth.
In operation 210, the system trains a machine learning model, and more specifically, trains a "teacher" machine learning model, or simply "teacher model. The teacher model may be trained using supervised machine learning model training techniques for training machine learning models. The teacher model may train on different types of direct label losses (such as cross entropy, square losses, or other losses with respect to the actual labels). In addition to or instead of direct label loss, it may also be trained with a ranking loss, such as cross entropy on labels, that describes the difference of real labels between example pairs, sets or lists. When training on more than a single penalty, the teacher trains on the sum of the penalties, and the gradient appears as a sum of gradients of different penalties. The losses may be weighted with different weights that emphasize that one loss exceeds another. During training, the weights may be increased or decreased stepwise. The weights may be adjusted for optimal or improved empirical performance on a particular application. Once trained, the teacher machine learning model is configured to generate a set of scores for a training example set. For each item in the training example set, the set of scores for the training example set includes a score that indicates a likelihood that the item will have a positive result when presented in the content of the training example set and with each other item in the training example set. A positive result for an item (e.g., a digital component) may be a result that indicates that a particular action has occurred with respect to the item when the item is selected for deployment to a device.
For example, for a neural network, the loss function may be calculated by comparing the results predicted by executing a machine learning model for the training example with actual results (such as the Oi values shown in table 1). In some embodiments, a calculation of the loss squared is used. The loss function may then be used to train the neural network, for example, using random gradient descent or small batch gradient descent.
In some implementations, the system may optionally continue to train the teacher model on the currently arrived data instance. The system may provide predictions from the teacher model to the student model in real-time (i.e., when the predictions are generated by the teacher model), and the student follows training according to the arriving examples with the newly generated teacher predictions. In some embodiments, after stopping using the signal from the teacher, the student may continue training on the newly arrived data.
In operation 215, the system trains a machine learning model of distillation, or simply "model of distillation". The training process is similar to that of the teacher model, except that the distilled model can be trained on four loss functions: (i) a genuine label; (ii) model score of distillation and teacher model score; (iii) ranking of the real tags; and (iv) score differences by pair and/or by list between examples within the same group. Any subset of the four loss functions may be used, and other objectives may also be used. The system may optimize or attempt to optimize the superposition (i.e., weighted sum) of the selected losses, i.e., all four losses or a selected subset. The system may include other losses, in combination with optimizing all selected losses. The system scales (scales) each loss with configurable weights and optimizes the student for a linear combination of these losses. The direct loss may be cross entropy, and the distillation loss may be cross entropy, square loss, or other loss. The system may also calculate cross entropy distillation.
The loss function of a real tag can be calculated by summing the square of the difference between the actual result score (e.g., the value Si, which is the logical score of the tag prediction Oi shown in table 1) and the model generated score (Di) of distillation:
other losses, such as cross entropy losses, may also be used.
In this equation, N is the number of training examples, and for each training example i, si is the actual result score for that training example, di is the log-division (log) value produced by the model of distillation when evaluating the training example i. However, the actual real result label is either a 1 or a 0. Thus, in practice, cross entropy can be used for the following losses:
L ＝ -label * log(p) - (1 - label) * log(1-p) (2)
where p is the probability predicted by the model, label is the true label (e.g., actual result) of this example, which is 0 or 1.
When logistic regression is used, if s is a fraction in the logic space
-log(p)＝log(1+e -s ) And-log (1-p) =log (1+e s ) (3)
If the distillation is based on a least squares error (L2) logic, a loss function of the model score of the distillation and the teacher model score is calculated by summing squares of differences between the scores calculated by the teacher model for the training examples and the scores generated by the model of the distillation for the training examples over all training examples:
In equation (4), N is the number of training examples, and for each training example i, ti is the teacher score output by the teacher model for training example i, and Di is the score generated by the model of distillation for training example i.
In some implementations, the loss may be calculated using probabilistic cross entropy, where the loss may be calculated as:
L＝-qlog(p)(5)
and
L＝-(1-q)log(1-p)(6)
in equations 5-7, q is the probability that the teacher predicts for the example, and p is the probability that the student predicts,
q＝Sigmoid(T)＝1/(1+e -T )(7a)
and
p＝Sigmoid(D)＝1/(1+e -D )(7b)
the loss function of the ranking of the real tags may be calculated by comparing the relationship of the results predicted by the model of distillation to the real ranking relationship. If the true tag of the first item is 1 and the true tag of the second item is 0, then the direct rank penalty tag is 1. If the true tag of the second item is 1 and the true tag of the first item is 0, then the direct rank penalty tag is 0. If the two items are equal, the result tag is 0.5. In some implementations, this situation will be ignored for training ranking losses, and the ranking losses will only be trained on different pairs of tags. The penalty function may then be calculated using a logic penalty that compares the value of the calculated ranking relationship (which would be a real number representing a prediction of the real tag) to the real ranking relationship:
Where s_i and s_j are the scores predicted by the model for items i and j, respectively, and y_ { ij } is the tag score in {0,0.5,1} calculated as described above.
The loss function in pairs and/or list of fractional differences between examples within the same group may be calculated as explained below. As explained below, the tabulated method may be calculated more quickly.
When ranking an example of a list, the paired framework may be reduced to a per list approach. As described above, if example i has a positive result, then the label of i is positive, while all other j have negative labels, otherwise 0. If the labels of all N examples are equal, then for all i, y i ＝1/N。
In the case where there are more than a single positive label and there are still some negative labels, the approximation can be determined by assigning yi=1/P to all P examples with positive labels and 0 to all examples with negative labels. The loss then becomes a cross entropy loss on the softmax prediction based on the learning score given by:
where yi in {0,1/P,1/N,1} takes on the value as described. When n=2, the above-described per-pair loss for a single pair is a special case of such loss. For larger N the losses are not equal, but they can still optimize the fractional difference, similar to the function of the tag, converging to the same optimal value. Also, if there are no multiple positives in the example set, the tags {0,1} may be used, ignoring all sets where all tags are equal, to obtain a conditional ranking solution that is conditional on a single example (only) event with a positive tag. In some applications, such as online learning, this approach can still affect accuracy, as discussed for the per pair penalty.
In some cases, the penalty in equation (9) is not common to an example set of multiple positive and negative labels. This result occurs because, for logistic regression, for an optimal solution, the empirical ratio of positive labels on a slice (slice) must match the average predictions on that slice. If 1 is used for any positive label, then the example set is counted multiple times for each positive label. This approach may deviate (skew) the optimal predictions for examples with negative labels. To correct this, a 1/P tag may be used for each of the P positively tagged examples in the example set. However, this will deviate from the empirical distribution of these positive examples, since each positive is counted as 1/P, not 1.
To address these deviations, an additional alternative extension of the method is to consider the example tags as disjoint events for cases where more than a single positive tag is expected in the example set, where the union of these events is the example set with more than a single positive tag. Such a notion gives a 0 penalty if all examples in the set have the same label. Using the same tag of yi in {0,1/P,1/N,1}, the softmax probability in the penalty is the sum of all the examples in the set that have the same tag as the example currently processed. The ranking penalty is given by
In the case of continuous tags, this loss is equal to the loss calculated using equation (9). While this adjustment accounts for the deviation of the penalty in equation (9), it still has its own deviation for events in which all examples in the example set have the same label, with the penalty being 0.
Distillation loss by rank L2
In the pairwise ranking distillation method, the component of knowledge transmitted from the teacher to the students is the score difference between training example i and training example j, so the score difference is distilled instead of the actual score. This difference is computed as a pair across all training examples within a set of examples, which concentrates the ranking differences on features that differ between elements within the set, rather than the same features, such as query-only features in a search system. In some implementations, the rank differences by pairs may be included on examples within a complete training small lot of a complete lot, rather than on examples within the same set only.
Such distillation may use regression that matches the differences learned by students with the differences learned by teachers. Thus, the distillation loss can be calculated as the square loss between fractional differences. Thus, the penalty to be optimized by the student is given by:
Wherein, in the set of N items, t i Is the teacher's logic score for item i, and s j Is the student's logic score for item j. The N items may be related items (e.g., from a query) selected from a larger set of items and considered together.
The advantage of applying distillation to the squared difference of the scores instead of to the probabilities is that distilling the squared difference of the scores provides a gradient that depends only on the magnitude (magnitude) of the difference of the scores in the log space where this property is lost for cross entropy and the convergence rate will slow. In other words, the L2 loss provides a strong convex loss, while the cross entropy does not, so convergence to the optimal value is faster for a strong convex loss.
This factor allows the conventional stochastic gradient optimizer to adjust the distance uniformly and independently of the actual probability.
When applied to ranking problems, this definition may be considered as a limit case for a general distillation framework. The loss in equation (1) can also be expressed as:
from this equation, it is apparent that if all teacher scores translate the same amount from student scores, then the student ranking is still correct and no update is needed.
This translational invariance provides a degree of freedom for accuracy loss to improve tag loss accuracy independent of rank distillation loss. This is true because there are only N-1 independent equations when there are N terms and the model only constrains the difference between pairs. Any solution where all values are parallel to the same amount will satisfy all constraints. Thus, the actual value has a degree of freedom, which can be learned by direct loss. In other words, if all examples in the set have two features (e.g., one query and one digital component), then the examples will all have the same first feature (e.g., query feature) but a different second feature (e.g., digital component feature). Ranking will subtract one set of features from another and is based only on the second (e.g., digital component) feature. Thus, the direct penalty may be used to determine the value of a first (e.g., query) feature, which is the same for all second (e.g., digital component) features.
Direct squaring loss between student model scores and teacher model scores on the same project may not provide this freedom, thus forcing the student model scores toward matching the teacher model scores. When the goal is simply to distill the rank from the teacher to the student, such a penalty imposes a greater constraint-i.e., the student score of the student will be forced to match the score of the teacher model.
Furthermore, if the student score is considered to beThe combination of feature weights, some scores specific to an example, and other scores specific to the set of examples that contains the example, then the direct distillation squaring loss will typically include an update to the features that are common to all examples in the set (e.g., in a query system, only the features are queried) and the gradients of the features specific to the examples in the set. On the other hand, the squared loss of distillation differences will have a non-zero total set gradient only for features that distinguish between different examples in a set, e.g. the case where all examples in a set consist of two features may be considered: a is that i (digital component features i) and Q, where Q is equal for all items. Thus:
S i -S j ＝(A i +Q)-(A j +Q)＝A i -A j (13)
thus, the difference between item i and item j is only a function of the digital component characteristics, not the query characteristics that are common to all digital components. Because only the rank in the query is of interest, there is no need to update the Q that has no impact on it, as it is considered by the bad penalty. The loss of L2 in the score will not ensure this result, especially if the learning rate and gradient for each coordinate are different. Thus, a loss that does not directly tend to be poor in score will also update feature Q, but feature Q gives no information about the ranking within the example set.
This advantage is lost when distilling directly between the teacher model and the student model, because if the actual score is distilled instead of bad, then it is not the case that feature Q does not learn when it does not. For ranking examples within a set, the method of distillation difference loss results in favorable results: improvements to the ranking (or query) within a set are achieved by learning the weights of each example feature that differs between different examples and avoiding updating features that are common to all examples within the set (e.g., only query features in a query system). This property is also produced by the method described below of replacing the squared L2 loss by probabilistic distillation, but lacks other desirable properties of gradients that depend only on the distance of the fractional differences between the student model and the teacher model, which gives consistent behavior over different prediction probabilities.
List implementation of L2 rank distillation loss
Implementing the per pair ranking penalty as previously described above may involve O (N) 2 And (3) operating. However, equation (12) can be rewritten to open the square term, resulting in a more efficient implementation, i.e., requiring fewer processor instructions to complete, and can complete faster. Note that the loss by list for distillation is the same as that used in the paired method, but is expressed in a different form. For the square distillation loss, make
Is the sum of the logic scores of both the teacher and the student, for all N items in the example training set, respectively. Applying the above simplification gives the following ranking penalty:
thus, to determine the L2 rank distillation by ground or on a list, the loss may be calculated as in equation (15) above, and the gradient may be derived accordingly. For the set, the score sum is calculated for both the teacher and the student, and equation (15) above is applied. Loss relative to fraction s k Can use etc
Calculation of 16
In this example, relative to score s k The gradient of (k in {1,2,., N }) can be calculated together using only O (N) operations.
The nature of the gradient is that if s k At t k Will then gradient s k Push to t k . If s is j (j. Noteq.k) and t k S of (2) k At t j Then the jth example pair s k The influence of the gradient of (c) pushes in the opposite direction. This result is expected because losses typically enhance the differences of the different examples. In both the pairwise and tabulated cases, the method relies on the fact that the teacher has been optimized for the ranking loss of distillation. Thus, if the teacher is optimized for the loss of pairs, then the L2 rank distillation will produce more accurate loss of pairs. The distillation form relies on matching pair differences, which are the optimization statistics in the direct list-wise losses, so it is also useful for the list-wise case. As described above, the direct per-pair loss and the per-list loss have different values, but both optimize the score difference.
Four calculated loss components are calculated-i) a true label; (ii) model score of distillation and teacher model score; (iii) ranking of the real tags; and (iv) at least some of the pairwise and/or list-wise score differences between examples within the same group, the system may train the student model. The system may perform training by using, for example, random gradient descent or small batches of random gradient descent. Note that training may occur with all four losses, any subset of these four losses, or other types of losses. Importantly, in this specification, distillation ranking losses are included and the model is trained with a weighted sum of losses (including ranking distillation losses, optionally including all or some other loss types). Ranking distillation loss emphasizes ranking scores and may provide improvement in ranking if the teacher's ranking is accurate.
Ranking distillation may be combined with techniques such as LambdaRank where the ranking of the centered example of the applied penalty is used to scale (or reduce) the penalty of some of the rankings. For example, if r i Representing the absolute ranking of example i in the set, then the element (i, j) lost by pair may be reduced by applying a multiplier:
Where D (x) is an inverse reduction function that may be equal to the rank, log (rank +1), or another increasing function of rank. More generally, rank distillation may be combined with other methods of increasing or decreasing the loss component based on relative or absolute ranks.
Alternatively, for example, an inverse function in the relative ranking may be used,
1/(r i -r j ) α (18)
wherein alpha >0 is an index. In a distillation setting, this technique may be applied according to teacher's rank, student's rank, or a combination of both. Under the setting that certain scores (such as document relevance scores or revenue-based scores) give more weight to higher ranked items, it may be reasonable to use this approach. An alternative may include adding weights by a function of the score other than ranking, including a Softmax score for the weighted logic score.
In some implementations, the system may optionally train the student model with the currently arrived data examples. In these cases, the system trains the student model without additional information from the teacher. (such an approach may be beneficial due to resource or system design constraints.)
In some cases, various alternatives may also be used and are suitable. For example, if a teacher has superior ranking knowledge than a student, and the objective goal is to make the student's difference as close as possible to the teacher's difference, it may be reasonable to use the distillation square loss between score differences. As shown below, this method may be the limit of fractional differences at high temperatures for temperature-based distillation.
Before demonstrating the method, the present specification shows that probability distillation is possible, where as discussed above, probability is defined as the probability that item i has a higher value of label than item j. As previously described, a disadvantage of probabilistic distillation is that it forces a non-uniform gradient as a function of fractional differences. If the score difference for a positive tag is a large negative number (e.g., -3 or greater negative), the upper gradient limit is 1 and recovery from incorrect ranking may be slower. Conversely, it is sometimes more appropriate to keep the model change smaller for each update (e.g., to ensure model training stability).
Ranking distillation by pair probability
Direct (unconditional) pressing pair
Ranking the distillation by probability may be performed by defining the distillation loss as that of equation (19), where the label is given as a logical (Sigmoid) function of the teacher score difference.
Students only learn towards the teacher's labels.
Conditional paired distillation
Considering only the loss of conditional distributions, for each pair, a method may be taken that is conditional on events for which the labels of the pairs are not equal (one positive, one negative), the probability of the teacher score being an example is its Sigmoid. Thus, equation 20 can be used to calculate the teacher label (y i )。
For a probability q i >q j The expected number of examples of tags with i tags ranked higher than j for pairs (i, j) of (i) is q i -q j ＝y i -y j . Thus, a loss that depends only on rank differences can distill students to the difference labels using equation 21.
Similar to the direct ranking penalty, while this penalty encourages student rankings to match teacher rankings, this may not be ideal when tag probability predictions also need to be accurate, as it ignores tag-equivalent events in anticipation, thus making predictions dependent on fewer examples, and typically in online AdaGrad settings, applying fewer but larger updates.
Ranking distillation by list probability
Similarly, list-ranked distillation of probability loss may use equation 22.
Where yi is defined as the Softmax probability of the teacher score using equation 23.
Also, the student learns to the teacher's label in the event of the loss of equation (9).
Rank distillation based on general temperature
A temperature distillation method may also be used to rank by adding temperature gamma to zoom in or out on the score. To compensate for the loss to the same scale as before, the loss must also be scaled by temperature (otherwise, with a random gradient approach optimizer, the learning rate is changed). This gives the loss in pairs shown in equation 24.
This also gives the per-list penalty shown in equation 25.
The effect of the temperature parameter is to stretch (or shrink) the x-axis inverse Sigmoid gradient (in the case of pairwise, and similarly in the case of tabulated), given as a scoreGradient gamma of function of difference>1. This allows to obtain a difference in the score difference (s i -s j ) More nearly uniform update movement in a larger area of (c). (the slope of the gradient change decreases). If gamma is<1, this will shrink the linear change in the gradient region, giving a larger gradient earlier, allowing a faster recovery from a largely incorrect ranking, which will slow down rapidly as the ranking approaches neutrality. Using high gamma, the technique approaches square (L2) distillation (scaled by 0.5, where the scaling can be compensated when the learning rate is adjusted).
Distillation-rank relevance for ranking only
Consider a case where a complex teacher model rich in training data is trained only for ranking (and its ranking location is not a score that is trusted), or alternatively, only ranking is important rather than an appropriate score. Ranking u at teacher i And rank of student (by r i Representation) distillation can occur where both ranks are the arrangement of {1,2, …, N } depending on the ranking of the logic scores of the two models. This can be done using the square ranking penalty shown in equation 26.
Minimizing this loss is equivalent to maximizing Spearman (Spearman) rank relevance between the teacher and the students. Such minimization may be achieved by obtaining some mapping between rank and score over which gradients may be calculated. Alternatively, the student score may take the student rank r relative to the example i A step size proportional to the loss gradient of the rank or the sign gradient of the rank.
In operation 218, the system provides a trained, distilled machine learning model, for example, to a recommendation system that distributes items (e.g., digital components). The system may provide the distilled machine learning model by transmitting the distilled machine learning model over a network, for example using HTTP or TCP/IP, or by storing the trained distilled machine learning model in a repository, such as a database or file system, accessible, for example, through a recommendation system.
FIG. 3 is a flow chart of an example process 300 for providing results to a query using a machine learning model of distillation. For convenience, the process 300 will be described as being performed by a recommendation system (e.g., the recommendation system 170 of fig. 1A and 1B) that is suitably programmed to perform the process. The operations of process 300 may also be implemented as instructions stored on one or more computer-readable media, which may be non-transitory, and execution of the instructions by one or more data processing apparatus may cause the one or more data processing apparatus to perform the operations of process 300. One or more other components described herein may perform the operations of process 300.
In operation 305, the system obtains a machine learning model of distillation. In some implementations, the system can obtain the model by accepting transmissions from a machine learning model training system. Such transmission may use a suitable network protocol, such as HTTP or File Transfer Protocol (FTP). In some implementations, the system can retrieve the model by retrieving the model from a repository, for example, by retrieving the model from a database using SQL queries, retrieving the model from a file store using file system operations, or retrieving the model from a web server using HTTP.
In operation 310, the system accepts a query request that includes data describing the context of the recommended item, which is then used as a feature value for a machine learning model input to the distillation. The system may accept the query request via any suitable network protocol, such as HTTP or HTTPs. As described above, if the recommender system is located at URL xyz.com, the query request may be of the form: https:// www.example.com/? q= < query_request >. The system may parse the query using any conventional HTTP request parser, which in this example would locate the query request after the equal sign.
In operation 320, the system then processes the input based at least in part on the eigenvalues using the trained, distilled machine learning model. Processing the input may use conventional model evaluation techniques that depend on the type of machine learning model used. For deep neural networks, the system encodes the query request as a series of numbers (each number corresponding to a word) that are used as inputs to the neural network. The neural network then evaluates each node using the weights and bias learned during the training process of operation 215. The result of evaluating the neural network is a series of scores generated by evaluating the input using a distilled machine learning model, each score corresponding to a recommended item.
In operation 330, the system uses the score generated in operation 320 to determine recommended items to provide to the client device. The system may select a configured number of recommended items, particularly the item with the highest score generated in operation 320. Alternatively, the system may select all recommended items having a score that exceeds the configuration threshold.
In operation 340, the system provides the results to a requestor, such as a requestor associated with the client device. The results may be encoded into HTTP responses using conventional techniques.
FIG. 4 is a block diagram of an example computer system 400 that may be used to perform the operations described above. System 400 includes a processor 410, a memory 420, a storage device 430, and an input/output device 440. Each of the components 410, 420, 430, and 440 may be interconnected, for example, using a system bus 450. Processor 410 is capable of processing instructions for execution within system 400. In some implementations, the processor 410 is a single-threaded processor. In another embodiment, the processor 410 is a multi-threaded processor. The processor 410 is capable of processing instructions stored in the memory 420 or on the storage device 430.
Memory 420 stores information within system 400. In one implementation, the memory 420 is a computer-readable medium. In some implementations, the memory 420 is a volatile memory unit. In another implementation, the memory 420 is a non-volatile memory unit.
Storage device 430 is capable of providing mass storage for system 400. In some implementations, the storage device 430 is a computer-readable medium. In various different implementations, storage device 430 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices over a network (e.g., a cloud storage device), or some other mass storage device.
The input/output device 440 provides input/output operations of the system 400. In some implementations, the input/output device 440 may include one or more of a network interface device, such as an ethernet card, a serial communication device, such as an RS-232 port, and/or a wireless interface device, such as an 802.11 card. In another embodiment, the input/output devices may include a drive device configured to receive input data and transmit output data to external devices 460 (e.g., keyboard, printer, and display device). However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and the like.
Although an example processing system is depicted in fig. 4, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
The term "configured" is used in this specification in connection with systems and computer program components. For a system of one or more computers configured to perform particular operations or actions, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operations or actions. For one or more computer programs configured to perform particular operations or actions, it is meant that the one or more programs comprise instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software application, app, module, software module, script, or code, may be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or combination of, special purpose logic circuitry (e.g., an FPGA or ASIC) and one or more programmed computers.
A computer adapted to execute a computer program may be based on a general purpose or a special purpose microprocessor or both, or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for carrying out or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. Furthermore, a computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD-ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on a user device in response to a request received from the web browser. Further, the computer may interact with the user by sending text messages or other forms of messages to a personal device (e.g., a smart phone running a messaging application), and receiving response messages from the user accordingly.
The data processing means for implementing the machine learning model may also comprise, for example, a dedicated hardware accelerator unit for handling the general and computationally intensive parts of machine learning training or production, i.e. reasoning, workload.
The machine learning model can be implemented and deployed using a machine learning framework (e.g., a TensorFlow framework, microsoft cognitive toolkit framework, apache Single framework, or Apache MXNet framework).
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an application through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include Local Area Networks (LANs) and Wide Area Networks (WANs), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data, such as HTML pages, to the user device, for example for the purpose of displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, such as the results of a user interaction, may be received at the server from the device.
In addition to the embodiments described above, the following embodiments are also innovative:
embodiment 1 is a method comprising:
obtaining a first input comprising a plurality of training example sets, each training example set comprising, for a set of items, one or more feature values representing features of a context in which each item in the set of items is recommended, and, for each item, each training example set comprising a result tag representing whether the item has a positive result;
training a first machine learning model using a first input, the first machine learning model configured to generate a set of scores for each training example set, wherein the set of scores for each training example set includes a training score for each item in the training example set, the training score representing whether the item will have a positive result when presented in the context of the training example set and with each other item in the example set; and
training a distilled machine learning model using the set of component scores for each example set, the distilled machine learning model configured to generate a distilled score for each item in a set of actual items, the distilled score representing: (i) When in a given context and presented with each other item in the set of actual items, whether the item will have a positive result, and (ii) the ranking of the item in the set of actual items,
Wherein a positive result for an item indicates that a particular action occurred with respect to the item when the item was provided as a recommendation to the device.
Embodiment 2 is the method of embodiment 1, wherein each item includes a digital component.
Embodiment 3 is the method of embodiment 2, further comprising:
providing, by the training system, the distilled model to a recommendation system that distributes the digital components;
determining, by the recommender system, a digital component to be provided to the client device in response to a request received from the client device; and
the selected digital components are provided to the client device by the recommendation system.
Embodiment 4 is the method of embodiment 1, wherein the machine learning model of distillation is trained using:
for each item of the plurality of training example sets: (i) a real label corresponding to the result label of the item; (ii) A comparison between the distilled model score for the item and the teacher model score for the item; and (iii) a comparison between the ranking of the item in the items of the plurality of training example sets and the true tags of each item; and
the per-item score differences between training examples within the same training example set.
Embodiment 5 is the method of embodiments 1-4, wherein training a machine learning model of distillation comprises:
Determining per-item score differences between training examples within the same training example set; and
losses corresponding to differences in project scores are minimized.
Embodiment 6 is the method of embodiments 1-5, wherein training a machine learning model of distillation includes determining each per-item score difference, the determining including:
for each pair of items in the same training example set:
determining a first difference between a first teacher model score for a first one of the pair of items and a second teacher model score for a second one of the pair of items;
determining a second difference between the model score of the first retum of the first item and the model score of the second retum of the second item in the pair of items; and
a difference between the first difference and the second difference is determined as a per-item score difference for the pair of items.
Embodiment 7 is the method of embodiment 5, wherein the loss is an L2 loss.
Embodiment 8 is the method of embodiment 6, wherein training the machine learning model of distillation includes reducing aggregate values per project score differences for each training example set.
Embodiment 9 is the method of embodiment 5, wherein training the machine learning model of distillation includes determining a per-item score difference, the determining including:
For a first item in a list of items in the same training example set:
for each second item in the list of items in the same training example set, wherein the first item is different from the second item:
determining a first difference between a first teacher model score of a first item and a second teacher model score of a second item;
determining a second difference between the model score of the first retorting of the first item and the model score of the second retorting of the second item; and
determining an individual loss value based on the first difference and the second difference; and
the per-list loss values are determined based on the individual loss values.
Embodiment 10 is the method of embodiment 9, wherein training the machine learning model of distillation includes reducing aggregate values of the per-list loss values for each training example set.
Embodiment 11 is the method of any one of embodiments 5-10, wherein each per item score difference is a per pair score difference or a per list score difference.
Embodiment 12 is the method of embodiment 4, wherein training the machine learning model of distillation includes computing a sum of squares of differences between losses computed for the items over all the items of the plurality of training example sets as a loss function.
Embodiment 13 is the method of embodiment 4, wherein training a machine learning model of distillation comprises: for each item of the plurality of training example sets, a penalty function is determined based on a comparison between a result of the item predicted by the distilled model and an actual result of the item represented by a result tag of the item.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, and in the claims, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (16)
1. A computer-implemented method, comprising:
obtaining a first input comprising a plurality of training example sets, each training example set comprising, for a set of items, one or more feature values representing features of a context in which each item in the set of items is recommended, and, for each item, each training example set comprising a result tag representing whether the item has a positive result;
training a first machine learning model using a first input, the first machine learning model configured to generate a set of scores for each training example set, wherein the set of scores for each training example set includes a training score for each item in the training example set, the training score representing whether the item will have a positive result when presented in the context of the training example set and with each other item in the example set; and
Training a distilled machine learning model using the set of component scores for each example set, the distilled machine learning model configured to generate a distilled score for each item in a set of actual items, the distilled score representing: (i) When in a given context and presented with each other item in the set of actual items, whether the item will have a positive result, and (ii) the ranking of the item in the set of actual items,
wherein a positive result for an item indicates that a particular action occurred with respect to the item when the item was provided as a recommendation to the device.
2. The computer-implemented method of claim 1, wherein each item comprises a digital component.
3. The computer-implemented method of claim 2, further comprising:
providing, by the training system, the distilled model to a recommendation system that distributes the digital components;
determining, by the recommender system, a digital component to be provided to the client device in response to a request received from the client device; and
the selected digital components are provided to the client device by the recommendation system.
4. The computer-implemented method of claim 1, wherein the machine learning model of distillation is trained using:
For each item of the plurality of training example sets: (i) a real label corresponding to the result label of the item; (ii) A comparison between the distilled model score for the item and the teacher model score for the item; and (iii) a comparison between the ranking of the item among the items of the plurality of training example sets and the true tags of each item; and
the per-item score differences between training examples within the same training example set.
5. The computer-implemented method of claims 1-4, wherein training a machine learning model of distillation comprises:
determining per-item score differences between training examples within the same training example set; and
losses corresponding to differences in project scores are minimized.
6. The computer-implemented method of claims 1 to 5, wherein training a machine learning model of distillation includes determining each per-item score difference, the determining including:
for each pair of items in the same training example set:
determining a first difference between a first teacher model score for a first one of the pair of items and a second teacher model score for a second one of the pair of items;
determining a second difference between the model score of the first retum of the first item and the model score of the second retum of the second item in the pair of items; and
A difference between the first difference and the second difference is determined as a per-item score difference for the pair of items.
7. The computer-implemented method of claim 5, wherein the penalty is an L2 penalty.
8. The computer-implemented method of claim 6, wherein training the machine learning model of distillation includes reducing aggregate values per project score differences for each training example set.
9. The computer-implemented method of claim 5, wherein training a machine learning model of distillation includes determining per-item score differences, the determining including:
for a first item in a list of items in the same training example set:
for each second item in the list of items in the same training example set, wherein the first item is different from the second item:
determining a first difference between a first teacher model score of a first item and a second teacher model score of a second item;
determining a second difference between the model score of the first retorting of the first item and the model score of the second retorting of the second item; and
determining an individual loss value based on the first difference and the second difference; and
the per-list loss values are determined based on the individual loss values.
10. The computer-implemented method of claim 9, wherein training a machine learning model of distillation includes reducing aggregate values of tabulated loss values for each training example set.
11. The computer-implemented method of any of claims 5 to 10, wherein each per-item score difference is a per-pair score difference or a per-list score difference.
12. The computer-implemented method of claim 4, wherein training a machine learning model of distillation comprises computing a sum of squares of differences between losses computed for an item over all items of the plurality of training example sets as a loss function.
13. The computer-implemented method of claim 4, wherein training a machine learning model of distillation comprises: for each item in the plurality of training example sets, a loss function is determined based on a comparison between a result of the item predicted by the distilled model and an actual result of the item represented by a result tag of the item.
14. A system, comprising:
one or more processors; and
one or more storage devices storing instructions that, when executed by the one or more processors, cause the one or more processors to perform the method of any preceding claim.
15. A computer-readable storage medium carrying instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any one of claims 1 to 13.
16. A computer program product comprising instructions which, when executed by a computer, cause the computer to perform the steps of the method of any one of claims 1 to 13.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
IL287233A IL287233A (en) | 2021-10-13 | 2021-10-13 | Machine learning ranking distillation |
IL287233 | 2021-10-13 | ||
PCT/US2022/044479 WO2023064083A1 (en) | 2021-10-13 | 2022-09-23 | Machine learning ranking distillation |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117157649A true CN117157649A (en) | 2023-12-01 |
Family
ID=83900268
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280026681.1A Pending CN117157649A (en) | 2021-10-13 | 2022-09-23 | Machine learning rank distillation |
Country Status (4)
Country | Link |
---|---|
EP (1) | EP4298566A1 (en) |
CN (1) | CN117157649A (en) |
IL (1) | IL287233A (en) |
WO (1) | WO2023064083A1 (en) |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112132146A (en) * | 2020-08-14 | 2020-12-25 | 北京三快在线科技有限公司 | Training method and device of image cropping model and image cropping method and device |
-
2021
- 2021-10-13 IL IL287233A patent/IL287233A/en unknown
-
2022
- 2022-09-23 WO PCT/US2022/044479 patent/WO2023064083A1/en active Application Filing
- 2022-09-23 CN CN202280026681.1A patent/CN117157649A/en active Pending
- 2022-09-23 EP EP22792982.5A patent/EP4298566A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4298566A1 (en) | 2024-01-03 |
IL287233A (en) | 2023-05-01 |
WO2023064083A1 (en) | 2023-04-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN1702654B (en) | Method and system for calculating importance of a block within a display page | |
US10019518B2 (en) | Methods and systems relating to ranking functions for multiple domains | |
US20070174320A1 (en) | Method and system for generating a concept-based keyword function, search engine applying the same, and method for calculating keyword correlation values | |
CN101390096B (en) | Training a ranking function using propagated document relevance | |
US10102482B2 (en) | Factorized models | |
US20210125108A1 (en) | Training a ranking model | |
US20190095788A1 (en) | Supervised explicit semantic analysis | |
US8832096B1 (en) | Query-dependent image similarity | |
US10102503B2 (en) | Scalable response prediction using personalized recommendation models | |
US20120011112A1 (en) | Ranking specialization for a search | |
US10229190B2 (en) | Latent semantic indexing in application classification | |
US11194848B2 (en) | Method of and system for building search index using machine learning algorithm | |
US20140214711A1 (en) | Intelligent job recruitment system and method | |
US9110923B2 (en) | Ranking over hashes | |
Patel et al. | CaPaR: a career path recommendation framework | |
Tang et al. | On optimization of expertise matching with various constraints | |
US20200004886A1 (en) | Generating supervised embedding representations for search | |
US11361028B2 (en) | Generating a graph data structure that identifies relationships among topics expressed in web documents | |
US10997264B2 (en) | Delivery of contextual interest from interaction information | |
US20240119047A1 (en) | Answer facts from structured content | |
US11194878B2 (en) | Method of and system for generating feature for ranking document | |
CN115577185B (en) | Muting course recommendation method and device based on mixed reasoning and mesopic group decision | |
US20200005134A1 (en) | Generating supervised embeddings using unsupervised embeddings | |
CN110737824A (en) | Content query method and device | |
Yuen et al. | An online-updating algorithm on probabilistic matrix factorization with active learning for task recommendation in crowdsourcing systems |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |