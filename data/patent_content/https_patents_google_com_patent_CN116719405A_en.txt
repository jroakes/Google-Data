CN116719405A - Object screening and information display in augmented reality experience - Google Patents
Object screening and information display in augmented reality experience Download PDFInfo
- Publication number
- CN116719405A CN116719405A CN202310521305.6A CN202310521305A CN116719405A CN 116719405 A CN116719405 A CN 116719405A CN 202310521305 A CN202310521305 A CN 202310521305A CN 116719405 A CN116719405 A CN 116719405A
- Authority
- CN
- China
- Prior art keywords
- objects
- data
- image
- user interface
- scene
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000003190 augmentative effect Effects 0.000 title claims abstract description 32
- 238000012216 screening Methods 0.000 title abstract description 50
- 238000000034 method Methods 0.000 claims abstract description 191
- 238000010801 machine learning Methods 0.000 claims description 91
- 238000012545 processing Methods 0.000 claims description 51
- 230000008569 process Effects 0.000 description 69
- 238000010586 diagram Methods 0.000 description 20
- 230000003993 interaction Effects 0.000 description 18
- 238000012549 training Methods 0.000 description 18
- 238000013528 artificial neural network Methods 0.000 description 12
- 239000000796 flavoring agent Substances 0.000 description 12
- 235000019634 flavors Nutrition 0.000 description 12
- 230000015654 memory Effects 0.000 description 12
- 241000207199 Citrus Species 0.000 description 10
- 235000020971 citrus fruits Nutrition 0.000 description 10
- 230000004044 response Effects 0.000 description 9
- 230000000694 effects Effects 0.000 description 8
- 230000006870 function Effects 0.000 description 8
- 230000000007 visual effect Effects 0.000 description 8
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 238000001514 detection method Methods 0.000 description 7
- 238000001914 filtration Methods 0.000 description 7
- 230000007704 transition Effects 0.000 description 7
- 230000008901 benefit Effects 0.000 description 6
- 239000004615 ingredient Substances 0.000 description 6
- 230000033001 locomotion Effects 0.000 description 6
- 230000011218 segmentation Effects 0.000 description 6
- 238000013519 translation Methods 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 238000003709 image segmentation Methods 0.000 description 5
- 235000014443 Pyrus communis Nutrition 0.000 description 4
- 230000009471 action Effects 0.000 description 4
- 238000004458 analytical method Methods 0.000 description 4
- 239000000203 mixture Substances 0.000 description 4
- 230000004048 modification Effects 0.000 description 4
- 238000012986 modification Methods 0.000 description 4
- 244000299461 Theobroma cacao Species 0.000 description 3
- 235000019219 chocolate Nutrition 0.000 description 3
- 238000013145 classification model Methods 0.000 description 3
- 235000013305 food Nutrition 0.000 description 3
- 230000006872 improvement Effects 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 230000008859 change Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 239000012634 fragment Substances 0.000 description 2
- 238000012423 maintenance Methods 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 238000012015 optical character recognition Methods 0.000 description 2
- -1 purpose Substances 0.000 description 2
- 238000009877 rendering Methods 0.000 description 2
- 239000004984 smart glass Substances 0.000 description 2
- 206010020751 Hypersensitivity Diseases 0.000 description 1
- CDBYLPFSWZWCQE-UHFFFAOYSA-L Sodium Carbonate Chemical compound [Na+].[Na+].[O-]C([O-])=O CDBYLPFSWZWCQE-UHFFFAOYSA-L 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000000172 allergic effect Effects 0.000 description 1
- 230000007815 allergy Effects 0.000 description 1
- 230000000386 athletic effect Effects 0.000 description 1
- 208000010668 atopic eczema Diseases 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 235000009508 confectionery Nutrition 0.000 description 1
- 238000007405 data analysis Methods 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 239000003599 detergent Substances 0.000 description 1
- 229940079593 drug Drugs 0.000 description 1
- 239000003814 drug Substances 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 238000009313 farming Methods 0.000 description 1
- 235000012020 french fries Nutrition 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000007787 long-term memory Effects 0.000 description 1
- 230000036651 mood Effects 0.000 description 1
- 235000016709 nutrition Nutrition 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000010408 sweeping Methods 0.000 description 1
- 208000024891 symptom Diseases 0.000 description 1
- 229940034610 toothpaste Drugs 0.000 description 1
- 239000000606 toothpaste Substances 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/20—Editing of 3D images, e.g. changing shapes or colours, aligning objects or positioning parts
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2219/00—Indexing scheme for manipulating 3D models or images for computer graphics
- G06T2219/004—Annotating, labelling
Abstract
A system and method for providing scene understanding may include: obtaining a plurality of images, stitching the images associated with the scene, detecting objects in the scene, and providing information associated with the objects in the scene. The system and method may include: a screening tag or query tag is determined that can be selected to screen multiple objects and then provided as information to a user to provide further insight into the scene. This information may be provided in an augmented reality experience via text or other user interface elements anchored to objects in the image.
Description
Cross Reference to Related Applications
The present application claims priority and benefit from U.S. provisional patent application No. 63/340078 filed on 5/10 of 2022. U.S. provisional patent application No. 63/340078 is incorporated herein by reference in its entirety.
Technical Field
The present disclosure relates generally to providing a user interface that provides information associated with a scene. More specifically, the present disclosure relates to: identifying objects in the scene, generating tags associated with the objects, screening the objects based on selection of a particular tag, and providing object information for the screened objects.
Background
Understanding a scene and objects within a scene can be difficult. In particular, understanding a scene may require repeated and tedious searching of objects within the scene, and sometimes it is difficult to determine what to search. In addition, during each visit to a particular location, the user may ask the same question at the particular location. The user may be forced to search for the same query inefficiently during each visit.
For example, the user may go to a local grocery store to make a purchase. During a shopping trip, the user may wish to select a new coffee type or brand to try, which they may do for each visit. The user may eventually pick up each bag, determine the name, and search for each coffee type and brand to see which coffee meets the user's preferences. Searching can be tedious and time consuming. In addition, it may be difficult for the user to track which coffee meets the preference and which does not. The result may be inefficiency during each shopping visit.
Disclosure of Invention
Aspects and advantages of embodiments of the disclosure will be set forth in part in the description which follows, or may be learned from the description, or may be learned by practice of the embodiments.
One example aspect of the present disclosure is directed to a computing system. The system may include one or more processors and one or more non-transitory computer-readable media collectively storing instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations may include obtaining image data generated by a mobile image capture device. The image data may depict a scene. Operations may include processing image data to determine a plurality of objects in a scene. In some implementations, the plurality of objects may include one or more consumer goods. The operations may include obtaining object specific information for one or more objects of the plurality of objects. The object specific information may include one or more details associated with each of the one or more objects. The operations may include providing one or more user interface elements overlaid on the image data. In some implementations, one or more user interface elements may describe object-specific information.
Another example aspect of the present disclosure is directed to a computer-implemented method. The method may include obtaining, by a computing system including one or more processors, video stream data generated by a mobile image capture device. In some implementations, the video stream data may include a plurality of image frames. The method may include determining, by a computing system, that a first image frame and a second image frame are associated with a scene. The method may include generating, by a computing system, scene data including a first image frame and a second image frame of a plurality of image frames. In some implementations, a method may include processing, by a computing system, scene data to determine a plurality of objects in a scene. The plurality of objects may include one or more consumer goods. The method may include obtaining, by a computing system, object-specific information for one or more objects of a plurality of objects. The object specific information may include one or more details associated with each of the one or more objects. The method may include providing, by the computing system, one or more user interface elements overlaid on one or more objects. In some implementations, one or more user interface elements may describe object-specific information.
Another example aspect of the disclosure is directed to one or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more computing devices, cause the one or more computing devices to perform operations. The operations may include obtaining image data. The image data may depict a scene. Operations may include processing the image data to determine a plurality of filters. Multiple filters may be associated with multiple objects in a scene. In some implementations, the operations may include providing one or more particular filters of the plurality of filters for display in a user interface. The operations may include obtaining input data. The input data may be associated with a selection of a particular filter of the plurality of filters. The operations may include providing one or more indicators overlaid on the image data. The one or more indicators may describe one or more particular objects associated with a particular filter.
Other aspects of the disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification in reference to the accompanying drawings, in which:
FIG. 1A depicts a block diagram of an example computing system that performs object recognition and screening, according to an example embodiment of the present disclosure.
FIG. 1B depicts a block diagram of an example computing device performing object recognition and screening, according to an example embodiment of the present disclosure.
FIG. 1C depicts a block diagram of an example computing device performing object recognition and screening, according to an example embodiment of the present disclosure.
Fig. 2A depicts a diagram of an example object screening and information display system, according to an example embodiment of the present disclosure.
Fig. 2B depicts a diagram of an example object screening and information display system, according to an example embodiment of the present disclosure.
Fig. 3A depicts a diagram of an example object screening and information display system, according to an example embodiment of the present disclosure.
Fig. 3B depicts a diagram of an example object screening and information display system, according to an example embodiment of the present disclosure.
Fig. 4A depicts a diagram of an example object screening and information display system, according to an example embodiment of the present disclosure.
Fig. 4B depicts a diagram of an example object screening and information display system, according to an example embodiment of the present disclosure.
Fig. 5A depicts a pictorial representation of an example question-answer interaction, according to an example embodiment of the present disclosure.
Fig. 5B depicts a pictorial representation of an example question-answer interaction, according to an example embodiment of the present disclosure.
Fig. 6 depicts a flowchart of an example method for performing object recognition and information display, according to an example embodiment of the present disclosure.
Fig. 7 depicts a flowchart of an example method for performing object recognition and information display, according to an example embodiment of the present disclosure.
Fig. 8 depicts a flowchart of an example method for performing object screening, according to an example embodiment of the present disclosure.
FIG. 9 depicts an illustration of an example zoom interaction, according to an example embodiment of the present disclosure.
Fig. 10A depicts an illustration of an example mobile map application use, according to an example embodiment of the present disclosure.
Fig. 10B depicts an illustration of an example mobile map application use, according to an example embodiment of the present disclosure.
Fig. 11 depicts a graphical representation of example book screening based on scoring in accordance with an example embodiment of the present disclosure.
FIG. 12 depicts a pictorial representation of an example object specific information display, according to an example embodiment of the present disclosure.
Fig. 13 depicts a pictorial representation of an example object specific information display, according to an example embodiment of the present disclosure.
Fig. 14 depicts a graphical representation of example book screening based on scoring in accordance with an example embodiment of the present disclosure.
FIG. 15 depicts an illustration of an example object-specific search user interface, according to an example embodiment of the present disclosure.
FIG. 16 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure.
FIG. 17 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure.
FIG. 18 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure.
Fig. 19 depicts a graphical representation of an example user interface transition, according to an example embodiment of the present disclosure.
Fig. 20 depicts an illustration of an example focus interaction, according to an example embodiment of the present disclosure.
FIG. 21 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure.
FIG. 22 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure.
Fig. 23 depicts a diagram of an example switching element for turning on and off object markers, according to an example embodiment of the present disclosure.
Fig. 24 depicts a diagram of an example score screening element for screening based on scores, according to an example embodiment of the present disclosure.
Fig. 25 depicts a diagram of an example score filter slider element for filtering based on scores, according to an example embodiment of the present disclosure.
FIG. 26 depicts an illustration of an example search interface, according to an example embodiment of the present disclosure.
FIG. 27 depicts a block diagram of an example tag generation model, according to an example embodiment of the present disclosure.
Repeated reference characters in the drawings are intended to identify identical features in the various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure is directed to systems and methods for providing object-specific information via augmented reality coverage. In particular, the systems and methods disclosed herein may utilize image processing techniques (e.g., object detection, optical character recognition, reverse image search, image segmentation, video segmentation, etc.) and augmented reality rendering to provide a user interface that overlays object specific details on an object depicted in image data. For example, the systems and methods disclosed herein may be used to obtain image data, process the image data to understand a scene, and provide details about the scene via an augmented reality experience. In some implementations, the systems and methods disclosed herein may provide suggested filters or candidate queries that may be used to provide more information about identified objects. Additionally and/or alternatively, object specific information (e.g., scores or components of specific objects) may be obtained and overlaid on the image of the object. In some implementations, the systems and methods disclosed herein may provide suggested filters or candidate queries that may be used to provide more information about identified objects. Additionally and/or alternatively, object specific information (e.g., scores or components of specific objects) may be obtained and overlaid on the image of the object. For example, the systems and methods may include obtaining image data generated by a mobile image capture device. The image data may depict a scene. The image data may be processed (e.g., with one or more machine learning models stored locally on the device) to determine a plurality of objects in the scene. In some implementations, the plurality of objects may include one or more consumer products (e.g., products sold in a grocery store (e.g., coffee, chocolate, soda, books, toothpaste, etc.)). Systems and methods may include obtaining object-specific information for one or more objects of a plurality of objects. The object specific information may include one or more details associated with each of the one or more objects. Systems and methods may include providing one or more user interface elements overlaid on image data. In some implementations, one or more user interface elements may describe object-specific information.
Specifically, the user may open the mobile application. The user may capture one or more images with an image sensor on the mobile device. The images may be processed with a machine learning model stored on the mobile device to determine one or more tags (e.g., one or more queries and/or one or more filters). The tag may be provided to the user via a user interface. The user may select a particular label that may cause the user interface to provide an augmented reality experience that includes object-specific information overlaid on a particular object in the captured image.
Systems and methods may obtain image data (e.g., one or more images and/or a plurality of image frames including a first image frame and a second image frame associated with a scene). In some implementations, the image data can include video streaming data (e.g., live streaming video of a scene). The video stream data may include a plurality of image frames. Image data (e.g., video streaming data) may be generated by a mobile image capture device (e.g., a mobile computing device with an image sensor). In some implementations, the image data may depict a scene.
In some implementations, the image data may include a plurality of frames. The plurality of frames may be processed to determine that the first image frame and the second image frame are associated with a scene. The first image frame may include a first set of objects and the second image frame may include a second set of objects. Determining that the first image frame and the second image frame are associated with the scene may include determining that the first set of objects and the second set of objects are associated with a particular object class.
Alternatively and/or additionally, determining that the first image frame and the second image frame are associated with the scene may include determining that the first image frame and the second image frame are captured at a particular location. The particular location may be determined based on the time between image frames being below a threshold time. Alternatively and/or additionally, the location may be determined based on one or more location sensors (e.g., a global positioning system on a mobile computing device). In some implementations, determining that image frames are associated with each other can include processing the plurality of image frames with one or more machine learning models (e.g., an image classification model, an image segmentation model, an object classification model, an object recognition model, etc.). One or more machine learning models may be trained to determine semantic understanding of an image frame based on detected contexts and/or features in a scene.
In some implementations, the plurality of image frames may be associated with one another based on determining overlapping portions of the image frame capture scene and/or determining portions of the image frame capture scene that are proximate to one another. Systems and methods may utilize various techniques to determine that an image frame depicts different portions of the same scene. Various techniques may include image analysis (e.g., pixel-by-pixel analysis), timestamp analysis (e.g., comparing metadata associated with image frames), and/or motion data analysis (e.g., obtaining and processing motion sensor data (e.g., inertial data from inertial motion sensors)).
The acquisition and/or generation of image frames may occur in response to input data received from a user. The input data may include text data, user interface selections, audio data (e.g., audio data describing a voice command), or another form of input. Additionally and/or alternatively, image frame association may be prompted based in part on received input (e.g., user interface selections, touch screen interactions, text inputs, voice commands, and/or gestures).
In some implementations, the systems and methods may include generating scene data based on a first image frame and a second image frame of a plurality of image frames. The scene data may include and/or describe a first image frame and a second image frame. In some implementations, generating the scene data can include stitching together image frames. Alternatively and/or additionally, image frames may be concatenated. The stitched image frames may then be cropped to remove data that may not be relevant to the semantic understanding of the scene. The stitched frames may be provided for display. Alternatively and/or additionally, the stitched frames may be used only for scene understanding at the back end.
The image data may be processed to determine a plurality of objects in the scene. In some implementations, the plurality of objects may include one or more consumer goods. Alternatively and/or additionally, the scene data may be processed to determine a plurality of objects in the scene, and the plurality of objects may include a plurality of consumer goods (e.g., food, appliances, detergents, tools, etc.). The image data and/or scene data may be processed to understand the scene. Processing the image data and/or scene data may include optical character recognition, object detection and recognition, pixel-by-pixel analysis, feature extraction and then processing, image classification, object class determination, image segmentation, and/or environmental or scene classification. In some implementations, processing may occur on a device (e.g., a mobile computing device using a machine learning model stored on a limited computing resource device). Processing on a device may limit the cost of resources to send large amounts of data over a network to a server computing system for processing.
In some implementations, the systems and methods may determine objects in a scene. Additionally and/or alternatively, the systems and methods may determine object categories or another form of relationship between objects. The systems and methods may then ignore objects that are not included in the relationship (e.g., the systems and methods may only process data associated with objects of a particular object class). In some implementations, multiple object categories may be determined. The systems and methods may determine more general object categories and/or may focus on use cases of higher utility. Alternatively and/or additionally, the systems and methods may focus on objects associated with object categories having more previous searches. In some implementations, the systems and methods may include biasing based on user preferences or past user interactions.
In some implementations, the systems and methods may determine tags associated with multiple object categories and refine to a particular object category based on selection. The systems and methods may focus on one or more objects in a reticle (reticle) of an image capture interface or one or more objects in a focus of a scene. Alternatively and/or additionally, the systems and methods may focus on the determined user preferences and/or the determined regional or global preferences. The favorites and preferences can be learned using a machine learning model. The machine learning model may be trained to generate probability scores associated with the processed image data and the processed context data. One or more tags may then be selected based on the probability scores (e.g., a highest probability score and/or a probability score above a given threshold may be selected).
Multiple tags (e.g., candidate queries, filters, and/or annotations) may be generated based on the determined scene understanding. The tags may include candidate queries, and the candidate queries may include questions posed by other users when having similar contexts, questions associated with a particular object category (e.g., a genre of a book and ingredients of food), questions associated with a particular detected object, questions associated with a particular location (e.g., a grocery store and a museum), and/or questions associated with past user interactions (e.g., what the user queried during a previous trip to the location, what the user's common questions were, and/or user browsing history related to the location or object category). In some implementations, the tags (e.g., filters, candidate queries, and/or annotations) can include data associated with a user profile (profile) that includes user preferences. The user profile may include allergies, which may be used as context data when the object is a food product. Additionally and/or alternatively, the user preferences may include genre preferences (e.g., book genres such as teenagers or moods), taste preferences (e.g., sweet and salty and/or citrus and earthy), and/or ingredient preferences (e.g., a certain percentage of an ingredient and/or a limit on the number of ingredients).
A tag or chip tag (chip) may be determined and/or selected such that each tag may be applied to at least one object in the scene. Additionally and/or alternatively, tags applied to all objects may not be selected. Tags may be generated and/or determined based on determined distinguishing features between objects in a scene (e.g., tags may include components or flavor notes that differ between objects in a scene).
The systems and methods may determine one or more tags of a plurality of tags (e.g., one or more candidate queries of a plurality of candidate queries) based on the image data and/or the scene data. In some implementations, one or more tags can be determined based at least in part on the obtained context data. Tags may be ranked and/or selected based on context of the scene, location, data associated with a particular user, and/or tag popularity among multiple users. The popularity may include popularity for all times, or may include popularity for a given period of time (e.g., trend labels). The determination of one or more tags may include user-specific refinement. In some implementations, the determination may cause the system and method to display only annotations or tags of the high value item.
Additionally and/or alternatively, the systems and methods may obtain object-specific information for one or more objects of the plurality of objects. The object specific information may include one or more details associated with each of the one or more objects. In some implementations, the object-specific information can include one or more consumer product details associated with each of the plurality of objects.
In some implementations, systems and methods may include obtaining context data. The context data may be associated with a user. The query may then be determined based on the image data and the context data. Object specific information may be obtained based at least in part on the query. In some implementations, the context data can describe at least one of a user location, a user preference, past user queries, and/or a user shopping history.
In some implementations, the context data can describe a user location. For example, the systems and methods may obtain one or more popular queries associated with a user location. The query may then be determined based at least in part on the one or more popular queries.
Alternatively and/or additionally, object categories associated with a plurality of objects may be determined. Object specific information may then be obtained based at least in part on the object category.
Systems and methods may include providing one or more user interface elements overlaid on image data. One or more user interface elements may describe object specific information. In some implementations, one or more user interface elements may be provided to overlay one or more objects. One or more user interface elements may describe object-specific information associated with the object over which the element is overlaid.
In some implementations, the plurality of user interface elements may describe object-specific information associated with one or more objects, and the plurality of user interface elements may be associated with a plurality of consumer products.
The one or more user interface elements may include and/or may describe a plurality of product attributes associated with a particular object in the scene. The plurality of product attributes may include a plurality of different product types. For example, the systems and methods may obtain input data associated with a selection of a particular user interface element associated with a particular product attribute (e.g., the particular product attribute may include a threshold product score) and may provide one or more indicators overlaid on the image data. The one or more indicators may describe one or more particular objects associated with one or more particular product attributes. In some implementations, a particular user interface element may include a slider associated with a range of consumer product scores.
In some implementations, providing one or more user interface elements overlaid on one or more objects may include adjusting a plurality of pixels associated with an outer region surrounding the one or more objects. The pixel adjustment may be utilized to provide a spotlight effect, which may be indicative of an object meeting the criteria associated with the selected tag.
Systems and methods may provide one or more user interface elements as part of an augmented reality experience. For example, one or more labels may be provided as user interface elements at the bottom of the display overlaid on one or more image frames. Additionally and/or alternatively, the one or more user interface elements may include text or icons overlaid on the particular object. For example, product attributes associated with a particular object may be anchored to the object in the augmented reality experience. The user interface element may include bubbles at the bottom of the user interface and/or text anchored to the object.
Alternatively and/or additionally, the systems and methods may obtain image data. The image data may depict a scene. The image data may be processed to determine a plurality of filters. Multiple filters may be associated with multiple objects in a scene. One or more particular filter of the plurality of filters may then be provided for display in the user interface. The system and method may then obtain the input data. In some implementations, the input data can be associated with a selection of a particular filter of the plurality of filters. The systems and methods may then provide one or more indicators overlaid on the image data. The one or more indicators may describe one or more particular objects associated with a particular filter.
In some implementations, processing the image data to determine a plurality of filters may include processing the image data to identify a plurality of objects in a scene, determining a plurality of distinguishing attributes associated with a distinguisher (distinguisher) between the plurality of objects, and determining the plurality of filters based at least in part on the plurality of distinguishing attributes.
Additionally and/or alternatively, processing the image data to identify a plurality of objects in the scene may include processing the image data with a machine learning model.
The system and method may obtain second input data. The second input data may be associated with a zoom input. In some implementations, the zoom input may be associated with one or more particular objects. The systems and methods may then obtain second information associated with the one or more particular objects. An enhanced image may be generated based at least in part on the image data and the second information. The enhanced image may include a magnified portion of the scene associated with the region including the one or more particular objects. In some implementations, the one or more indicators and the second information may be overlaid on one or more particular objects.
Additionally and/or alternatively, the one or more indicators may include object-specific information associated with the one or more specific objects. In some implementations, providing one or more indicators overlaid on the image data may include an augmented reality experience.
For example, the systems and methods may determine a plurality of filters associated with a plurality of objects. Each filter may include criteria associated with a subset of the plurality of objects. Multiple filters may be provided for display in a user interface. The systems and methods may then obtain a filter selection associated with a particular filter of the plurality of filters. An augmented reality overlay on one or more image frames may then be provided. The augmented reality overlay may include one or more user interface elements provided on respective objects that meet respective criteria of a particular filter.
In some implementations, systems and methods may include receiving audio data. The audio data may describe voice commands. Systems and methods may include determining a particular object associated with a voice command and providing an enhanced image frame indicating the particular object associated with the voice command. Additionally and/or alternatively, the obtained audio data may describe voice commands that may be processed with one or more images to generate an output. For example, a multimodal query may be obtained that includes one or more captured images and audio data describing the voice command (e.g., one or more images of a scene and the voice command "which grains are organic sources. The multimodal query can be processed to generate a response to the voice command determined based at least in part on the one or more images. In some implementations, the response may include one or more user interface elements overlaid on the captured image and/or the live image stream in the viewfinder. The voice input along with the camera input may provide a conversation assistant that visually perceives the environment, which may enable the user to be informed of the information of the environment as the user navigates through the environment. In some implementations, processing of the image data can be adjusted based on the voice command. For example, the image(s) may be cropped based on the voice command to segment one or more points of interest that may be subsequently processed. Additionally and/or alternatively, voice inputs and image inputs may be entered and processed in tandem.
In some implementations, a user may capture an image of an object and may provide voice commands to request information about the particular object. The requested information may include querying the status of a particular object. For example, the user may capture an image of a pear, and may provide a voice command "do this done? The systems and methods disclosed herein may process image and voice commands to determine a maturity classification to provide. The systems and methods may then process the image of the pear to output a maturity classification, which may then be provided to the user. In some embodiments, data describing the determination of the maturity of the pear and/or data describing the nutritional or farming information of the pear may be further provided.
The voice input may be processed to generate text data describing the voice command, which may be processed with the image data for search result determination. Text inserts may be generated based on the transcribed voice commands, image inserts may be generated based on the captured images, and the text inserts and image inserts may be processed to determine one or more search results.
The systems and methods disclosed herein may involve obtaining one or more inputs from a user. The user input may include selection of a particular tag associated with a particular candidate query, text input (e.g., which may be used to generate a new query and/or a new filter), voice input, and/or adjustment of a filter slider (e.g., for a price or score).
In some implementations, the systems and methods disclosed herein may be used to screen objects in a scene to determine one or more particular objects that answer a question and/or meet one or more criteria. For example, the systems and methods disclosed herein may obtain image data, may determine a plurality of objects depicted in the image data, and may determine one or more objects in a scene that are related or associated with a candidate query (e.g., have given product attributes and/or meet input criteria). The determination may involve searching the network. The search may include extracting data from knowledge graphs, local databases, regional databases, global databases, web pages, and/or data stored on the processing device. The systems and methods may also obtain object details associated with the object associated with the selected candidate query.
Additionally and/or alternatively, a user interface may be provided that indicates which objects are associated with or related to the selected candidate query. The user interface may highlight particular objects associated with the selected tags (e.g., candidate queries and/or filters). In some implementations, the systems and methods may darken pixels that are not associated with a particular object. Additionally and/or alternatively, the systems and methods may provide an indicator overlaid on a particular object. The indicators may include object specific details (e.g., ingredients, flavor notes, scores, genre, etc.).
The user interface may include an augmented reality experience. A user interface including an augmented reality experience may be provided as part of a mobile application, a web application, and/or as part of an integrated system of smart wearable devices. The systems and methods disclosed herein may be implemented in an augmented reality application including augmented reality translation, object recognition, and/or various other features. Alternatively and/or additionally, the systems and methods disclosed herein may be implemented as stand-alone applications. Additionally and/or alternatively, the systems and methods disclosed herein may be used by smart wearable devices (such as smart glasses) to learn different scenarios and objects while traversing daily transactions.
In some embodiments, the systems and methods disclosed herein may be always on and/or may be turned on and off. The systems and methods may be provided in an application having multiple tabs associated with multiple different functions. The tab that is currently open during processing may be used as a context for determining one or more tags.
The systems and methods disclosed herein may utilize a variety of different user interface/user experience features and elements. The elements may include two-dimensional shapes, three-dimensional shapes, text, popups, dynamic elements, input boxes, graphical keyboards, extension elements, transition effects, graticules, shading effects, and/or processing indicators. The labels may be at the bottom of the user interface, at the top of the user interface, and/or at the sides. The annotation may be superimposed on the object, may be placed above or below the object, and/or may be indicated via a symbol, icon, or indicator. In some implementations, the systems and methods may include an off-screen indicator that indicates that an object in a scene meets a given criteria or has specific details but is not currently displayed in a user interface. Additionally and/or alternatively, the user interface may include a manual spotlight feature for indicating objects that meet a given criteria associated with the selected filter or query.
The systems and methods disclosed herein may be used for a variety of different purposes. For example, the systems and methods may be used to narrow down and select objects in a scene that meet various criteria. In some embodiments, the refinement may be used to select consumer products based on scores, ingredients, and/or attributes.
Additionally and/or alternatively, the systems and methods disclosed herein may be used to determine and provide object distinguisher for different objects in a scene.
The systems and methods may be used to provide instructions on how to interact with a scene (e.g., car maintenance and/or use a particular device such as a blender).
In some embodiments, the systems and methods may be used for shopping (e.g., to avoid allergic components and/or for screening for symptoms based on the purchase of a drug).
Additionally and/or alternatively, the systems and methods may determine and provide information about related objects based on scene analysis.
In some implementations, the systems and methods disclosed herein may generate and/or determine tags such that tags may be automatically generated to provide tags of what a user should query based on what is in a scene and/or based on context. The systems and methods disclosed herein may process scene data to determine what search queries or filters will provide the most insight into the scene and/or what search queries or filters will provide the most insight into the content that can separate different objects from each other. For example, an image of a coffee pad in a shopping channel may cause the system to automatically generate a label of a flavor profile, score, local source, fair transaction, etc., and an image of a book may cause the system to automatically generate a label of a genre, score, length, time period, etc. Additionally and/or alternatively, the image of a row of merchants may cause the system to automatically generate tags for restaurants, clothing, chain merchants, local owners, current businesses, and the like.
Additionally and/or alternatively, the tag (e.g., the filter and/or the candidate query) may include determining a plurality of candidate tags associated with the image data and/or the context data. The plurality of candidate tags may then be processed to limit the displayed tags to (1) tags associated with (e.g., applied to) at least one object in the scene and (2) tags not associated with all objects in the scene. Limiting the candidate queries based on one or two factors may ensure that the selection of tags provides the user with actual information, rather than leaving the user with the same options they were originally provided when capturing the image.
A selection of a particular object and/or a tag associated with the particular object may be received and additional information about the particular object may be obtained and displayed. For example, a selection of a particular product may be received and additional product details may be obtained and displayed. The additional information may be based in part on one or more past user interactions (e.g., purchase history, search history, and/or previously selected filter tags). The additional information may be obtained by utilizing the image data and/or the identification data as a search query to determine one or more search results that may be displayed and/or processed to determine the additional information. The search query may additionally include text input, voice input, and/or contextual data (e.g., location, other objects in the scene, time, user profile data, and/or image classification).
In some implementations, the systems and methods disclosed herein may be used to capture (generate or obtain) and process video. The video may be captured and then processed to detect and identify one or more objects in the video, which may then be annotated at playback. Additionally and/or alternatively, actions performed in the video may be determined and annotated at playback. In some implementations, one or more objects in the video may be segmented and then searched. Additionally and/or alternatively, annotations may be determined and provided in real-time, which may then be provided as augmented reality annotations.
The systems and methods of the present disclosure provide a number of technical effects and benefits. As one example, the systems and methods may provide a real-time augmented reality experience that may provide a user with a scene understanding. In particular, the systems and methods disclosed herein may obtain image data, may process the image data, may identify objects depicted in the image data, and may provide object-specific information for the objects. Additionally and/or alternatively, the systems and methods may process the image data and provide tags (e.g., a filter tag for filtering objects in a scene and/or a query tag for obtaining specific information associated with the objects). The tag may then be selected, and the systems and methods disclosed herein may provide an indicator of the particular object anchored to the image data. The indicator may include an augmented reality rendering that includes object specific information about the object to which it is anchored.
Another technical benefit of the systems and methods of the present disclosure is the ability to utilize multimodal searches to help users narrow down choices or learn how to interact with the environment. For example, the systems and methods disclosed herein may be used to extract data from an image, and additionally receive voice commands, text input, and/or user selections, which may then be used to generate a query based on features identified in the image and the input data. Multimodal searches can provide a more comprehensive search, which can then be used to understand the scenario. For example, a user may capture an image and select one or more tags associated with user preferences in order to determine what objects the user wants. Additionally and/or alternatively, one or more of the tags may be tags entered via a graphical keyboard. Alternatively and/or additionally, the user may capture an image and ask how to accomplish a certain task. The system and method may then process the image and the entered questions to provide step-by-step guidance using indicators overlaid on a portion of the image to provide more accurate indications.
Another example of technical effects and benefits relates to improvements in computing efficiency and improvements in computing system functionality. For example, the systems and methods disclosed herein may utilize machine learning models and functions on a device to perform local processing on the device. Local processing on the device may limit the data transmitted over the network to the server computing system for processing, which may be more friendly to users with limited network access.
Referring now to the drawings, example embodiments of the present disclosure will be discussed in more detail.
Example devices and systems
FIG. 1A depicts a block diagram of an example computing system 100 that performs object recognition and screening in accordance with an example embodiment of the present disclosure. The system 100 includes a user computing device 102, a server computing system 130, and a training computing system 150 communicatively coupled by a network 180.
The user computing device 102 may be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop), a mobile computing device (e.g., a smart phone or tablet), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 102 includes one or more processors 112 and memory 114. The one or more processors 112 may be any suitable processing device (e.g., a processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. Memory 114 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, and combinations thereof. The memory 114 may store data 116 and instructions 118 that are executed by the processor 112 to cause the user computing device 102 to perform operations.
In some implementations, the user computing device 102 can store or include one or more machine learning models 120 (e.g., one or more machine-learned tag generation models). For example, the machine learning model 120 may be or may otherwise include various machine learning models, such as a neural network (e.g., deep neural network) or other types of machine learning models, including nonlinear models and/or linear models. The neural network may include a feed forward neural network, a recurrent neural network (e.g., a long and short term memory recurrent neural network), a convolutional neural network, or other form of neural network. An example machine learning model 120 is discussed with reference to fig. 2A-5B and fig. 9-26.
In some implementations, one or more machine learning models 120 may be received from the server computing system 130 over the network 180, stored in the user computing device memory 114, and then used or otherwise implemented by the one or more processors 112. In some implementations, the user computing device 102 can implement multiple parallel instances of the single machine learning model 120 (e.g., to perform parallel object recognition and tag generation across multiple instances of object recognition and filtering).
More specifically, a machine learning model (e.g., a label generation model) may process image data to identify a plurality of objects in a scene depicted in the image data. A machine learning model (e.g., a tag generation model) may determine tags based at least in part on the plurality of objects and the context data. The tag may be generated based on the determined generic object category, based on previous interactions, based on location, and/or based on comparing details between multiple objects to determine distinguishing features. The tag may include a query or filter. Tags may then be selected to filter objects that will be indicated as meeting specific criteria.
Additionally or alternatively, one or more machine learning models 140 (e.g., one or more tag generation models) may be included in the server computing system 130 in communication with the user computing device 102 according to a client-server relationship, or otherwise stored and implemented by the server computing system 130. For example, the machine learning model 140 may be implemented by the server computing system 130 as part of a web service (e.g., an object lookup and screening service). Accordingly, one or more models 120 may be stored and implemented at the user computing device 102 and/or one or more models 140 may be stored and implemented at the server computing system 130.
The user computing device 102 may also include one or more user input components 122 that receive user input. For example, the user input component 122 may be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component may be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other components that a user may use to provide user input.
The server computing system 130 includes one or more processors 132 and memory 134. The one or more processors 132 may be any suitable processing device (e.g., a processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. Memory 134 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, and combinations thereof. Memory 134 may store data 136 and instructions 138 that are executed by processor 132 to cause server computing system 130 to perform operations.
In some implementations, the server computing system 130 includes or is otherwise implemented by one or more server computing devices. Where the server computing system 130 includes multiple server computing devices, such server computing devices may operate in accordance with a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 130 may store or otherwise include one or more machine learning models 140 (e.g., one or more machine learning tag generation models). For example, model 140 may be or may otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer nonlinear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. An example model 140 is discussed with reference to fig. 2A-5B and fig. 9-26.
The user computing device 102 and/or the server computing system 130 may train the models 120 and/or 140 via interactions with a training computing system 150 communicatively coupled via a network 180. The training computing system 150 may be separate from the server computing system 130 or may be part of the server computing system 130.
The training computing system 150 includes one or more processors 152 and memory 154. The one or more processors 152 may be any suitable processing device (e.g., a processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.), and may be one processor or multiple processors operatively connected. The memory 154 may include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, and combinations thereof. Memory 154 may store data 156 and instructions 158 that are executed by processor 152 to cause training computing system 150 to perform operations. In some implementations, the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
Training computing system 150 may include a model trainer 160, model trainer 160 using various training or learning techniques (such as back propagation of errors) to train machine learning models 120 and/or 140 stored at user computing device 102 and/or server computing system 130. For example, the loss function may be counter-propagated through the model(s) to update one or more parameters of the model(s) (e.g., based on gradients of the loss function). Various loss functions may be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques may be used to iteratively update parameters over multiple training iterations.
In some embodiments, performing back-propagation of the error may include performing truncated back-propagation over time. Model trainer 160 may perform a variety of generalization (e.g., weight decay, discard (dropout), etc.) techniques to enhance the generalization ability of the trained model.
In particular, model trainer 160 may train label generation models 120 and/or 140 based on a set of training data 162. Training data 162 may include, for example, training images, training tags (e.g., ground truth object tags and/or ground truth tags), training context data, and/or training athletic data.
In some implementations, the training examples can be provided by the user computing device 102 if the user has provided consent. Thus, in such embodiments, the model 120 provided to the user computing device 102 may be trained by the training computing system 150 based on user-specific data received from the user computing device 102. In some cases, this process may be referred to as personalizing the model.
Model trainer 160 includes computer logic for providing the desired functionality. Model trainer 160 may be implemented in hardware, firmware, and/or software controlling a general purpose processor. For example, in some embodiments, model trainer 160 includes program files stored on a storage device, to be loaded into memory, and executed by one or more processors. In other implementations, model trainer 160 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium (such as a RAM hard disk or an optical or magnetic medium).
The network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and may include any number of wired or wireless links. In general, communications over network 180 may be carried via any type of wired and/or wireless connection using a variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), coding or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
The machine learning model described in this specification may be used in a variety of tasks, applications, and/or use cases.
In some implementations, the input of the machine learning model(s) of the present disclosure can be image data. The machine learning model(s) may process the image data to generate an output. As an example, the machine learning model(s) may process the image data to generate an image recognition output (e.g., recognition of the image data, potential embedding of the image data, encoded representation of the image data, hashing of the image data, etc.). As another example, the machine learning model(s) may process the image data to generate an image segmentation output. As another example, the machine learning model(s) may process the image data to generate an image classification output. As another example, the machine learning model(s) may process the image data to generate an image data modification output (e.g., a change in the image data, etc.). As another example, the machine learning model(s) may process the image data to generate an encoded image data output (e.g., an encoded representation and/or a compressed representation of the image data, etc.). As another example, the machine learning model(s) may process the image data to generate an expanded image data output. As another example, the machine learning model(s) may process the image data to generate a prediction output.
In some implementations, the input of the machine learning model(s) of the present disclosure can be text or natural language data. The machine learning model(s) may process text or natural language data to generate an output. For example, the machine learning model(s) may process natural language data to generate a linguistic coded output. As another example, the machine learning model(s) may process text or natural language data to generate a potential text-embedded output. As another example, the machine learning model(s) may process text or natural language data to generate a translation output. As another example, the machine learning model(s) may process text or natural language data to generate a classification output. As another example, the machine learning model(s) may process text or natural language data to generate text segmentation output. As another example, the machine learning model(s) may process text or natural language data to generate semantic intent output. As another example, the machine learning model(s) may process text or natural language data to generate a predictive output.
In some implementations, the input of the machine learning model(s) of the present disclosure can be speech data. The machine learning model(s) may process the speech data to generate an output. For example, the machine learning model(s) may process the speech data to generate speech recognition output. As another example, the machine learning model(s) may process the speech data to generate a speech translation output. As another example, the machine learning model(s) may process the speech data to generate potentially embedded output. As another example, the machine learning model(s) may process the speech data to generate an encoded speech output (e.g., an encoded representation and/or a compressed representation of the speech data, etc.). As another example, the machine learning model(s) may process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, the machine learning model(s) may process the speech data to generate a prediction output.
In some implementations, the input of the machine learning model(s) of the present disclosure can be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model(s) may process the potentially encoded data to generate an output. For example, the machine learning model(s) may process the potentially encoded data to generate the recognition output. As another example, the machine learning model(s) may process the potentially encoded data to generate a reconstructed output. As another example, the machine learning model(s) may process the potentially encoded data to generate a search output. As another example, the machine learning model(s) may process the potentially encoded data to generate a reclustering output. As another example, the machine learning model(s) may process the potentially encoded data to generate a prediction output.
In some implementations, the input to the machine learning model(s) of the present disclosure can be statistical data. The machine learning model(s) may process the statistical data to generate an output. For example, the machine learning model(s) may process the statistical data to generate an identification output. As another example, the machine learning model(s) may process the statistical data to generate a prediction output. As another example, the machine learning model(s) may process the statistical data to generate a classification output. As another example, the machine learning model(s) may process the statistical data to generate a segmentation output. As another example, the machine learning model(s) may process the statistical data to generate a translation output. As another example, the machine learning model(s) may process the statistical data to generate a visual output. As another example, the machine learning model(s) may process the statistical data to generate a diagnostic output.
In some implementations, the input to the machine learning model(s) of the present disclosure can be sensor data. The machine learning model(s) may process the sensor data to generate an output. For example, the machine learning model(s) may process the sensor data to generate an identification output. As another example, the machine learning model(s) may process the sensor data to generate a prediction output. As another example, the machine learning model(s) may process the sensor data to generate classification output. As another example, the machine learning model(s) may process the sensor data to generate a segmented output. As another example, the machine learning model(s) may process the sensor data to generate a translation output. As another example, the machine learning model(s) may process the sensor data to generate a visual output. As another example, the machine learning model(s) may process the sensor data to generate diagnostic output. As another example, the machine learning model(s) may process the sensor data to generate a detection output.
In some cases, the machine learning model(s) may be configured to perform tasks that include encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). In another example, the input includes visual data (e.g., one or more images or videos), the output includes compressed visual data, and the task is a visual data compression task. In another example, a task may include generating an embedding for input data (e.g., input audio or video data).
In some cases, the input includes visual data and the task is a computer visual task. In some cases, pixel data including one or more images is input, and the task is an image processing task. For example, the image processing task may be an image classification, wherein the output is a set of scores, each score corresponding to a different object class and representing a likelihood that one or more images depict an object belonging to that object class. The image processing task may be object detection, wherein the image processing output identifies one or more regions in the one or more images, and for each region, identifies a likelihood that the region depicts an object of interest. As another example, the image processing task may be image segmentation, wherein the image processing output defines a respective likelihood of each of a set of predetermined classes for each pixel in the one or more images. For example, the collection of categories may be foreground and background. As another example, the category set may be an object category. As another example, the image processing task may be depth estimation, where the image processing output defines a respective depth value for each pixel in one or more images. As another example, the image processing task may be motion estimation, wherein the network input includes a plurality of images, and the image processing output defines for each pixel of one of the input images a motion of a scene depicted at the pixel between the images in the network input.
In some cases, the input includes audio data representing a spoken utterance, and the task is a speech recognition task. The output may include a text output mapped to the spoken utterance. In some cases, the task includes encrypting or decrypting the input data. In some cases, tasks include microprocessor performance tasks such as branch prediction or memory address translation.
FIG. 1A illustrates one example computing system that may be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the user computing device 102 may include a model trainer 160 and a training data set 162. In such an embodiment, the model 120 may be trained and used locally at the user computing device 102. In some such implementations, the user computing device 102 may implement the model trainer 160 to personalize the model 120 based on user-specific data.
FIG. 1B depicts a block diagram of an example computing device 10, performed in accordance with an example embodiment of the present disclosure. Computing device 10 may be a user computing device or a server computing device.
Computing device 10 includes a plurality of applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine learning model(s). For example, each application may include a machine learning model. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As shown in fig. 1B, each application may communicate with a number of other components of the computing device, such as one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., public API). In some implementations, the API used by each application is specific to that application.
Fig. 1C depicts a block diagram of an example computing device 50, performed in accordance with an example embodiment of the present disclosure. Computing device 50 may be a user computing device or a server computing device.
Computing device 50 includes a plurality of applications (e.g., applications 1 through N). Each application communicates with a central intelligent layer. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like. In some implementations, each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications).
The central intelligence layer includes a plurality of machine learning models. For example, as shown in fig. 1C, a corresponding machine learning model (e.g., model) may be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications may share a single machine learning model. For example, in some embodiments, the central intelligence layer may provide a single model (e.g., a single model) for all applications. In some implementations, the central intelligence layer is included within the operating system of the computing device 50 or is otherwise implemented by the operating system of the computing device 50.
The central intelligence layer may communicate with the central device data layer. The central device data layer may be a centralized data repository for computing devices 50. As shown in fig. 1C, the central device data layer may be in communication with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or an additional component. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Example model arrangement
Fig. 2A and 2B depict illustrations of an example object screening and information display system 200, according to example embodiments of the present disclosure. In some implementations, the object screening and information display system 200 may include one or more machine learning models trained to identify objects in the captured image 210 depicting a scene and, as a result of the object identification, provide an enhanced image 250 including one or more user interface elements superimposed on the object meeting the screening criteria. Thus, in some implementations, the object screening and information display system 200 may include an intermediate enhanced image 230 operable to indicate objects in a scene that meet a first criterion.
As shown in fig. 2A and 2B, the systems and methods disclosed herein may be provided as a native application, mobile application, and/or web application running on the mobile computing device 212. The mobile computing device 212 may include one or more stored and/or downloaded machine learning models for image processing to determine the plurality of tags 222. The mobile computing device 212 may include one or more processors and may be configured to provide the user interfaces disclosed herein. For example, the mobile computing device 212 may include a display screen configured to display a user interface, which may include displaying one or more images captured by an image sensor (e.g., an image capture device of the mobile computing device). Alternatively and/or additionally, the systems and methods disclosed herein may be implemented in a smart wearable device (e.g., smart glasses).
In particular, the user may open a mobile device application that may be used to capture one or more images 210 of the scene (e.g., a grocery store aisle that includes a variety of coffee options for selection). The image may be processed to determine that there are a number of different coffee in the scene, and that the scene is primarily the object of the coffee category. Based on the identification of the plurality of coffee and/or based on the determined coffee categories, the object screening and information display system 200 may generate a plurality of tags 222 associated with flavor profiles of different coffee and may provide the tags 222 (e.g., citrus, earthy, and fruity) for display 220. The user may then select a particular label (e.g., citrus taste). The object screening and information display system 200 may obtain object specific information for each coffee in the scene to determine which coffee has a flavor profile associated with the particular label selected. Then, an object (i.e., coffee) having a specific flavor profile (i.e., citrus taste) may be indicated 230 inside the user interface. The indication may include one or more user interface elements overlaid on the particular object, and/or may include highlighting the particular object and darkening surrounding areas.
The object screening and information display system 200 may then determine one or more new tags (e.g., local and LGBTQ owned) while continuing to provide the selected tags for display 240. The user may then select a second tag (e.g., local). The object screening and information display system 200 may determine which objects satisfy the first criteria of the first tag and the second criteria of the second tag. One or more objects that both criteria meet may then be indicated with one or more user interface elements and may be highlighted 250.
In some implementations, the indicator and highlighting may occur on live stream image data that may be different from the originally processed image data. For example, annotations, tags, and user interface elements may be provided as part of an augmented reality experience that anchors user interface elements and effects to objects in a scene, such that when a camera moves, the user interface elements may remain at the associated objects.
Fig. 3A and 3B depict illustrations of an example object screening and information display system 300, according to example embodiments of the present disclosure. The object screening and information display system 300 is similar to the object screening and information display system 200 of fig. 2A and 2B, and also includes text input.
In particular, the object screening and information display system 300 may capture one or more images 310 of a scene (e.g., a grocery store aisle that includes a plurality of different objects (e.g., different chocolates)). One or more images may be processed to identify a plurality of objects. Object specific information (e.g., a score for a particular chocolate) for each of the plurality of objects may then be obtained. Text associated with the object-specific information may then be superimposed 320 on the corresponding object. Additionally and/or alternatively, a plurality of tags (e.g., fair transactions, organic, and local) may be determined based on the identified objects, object categories of the objects, and/or context data (e.g., location, user profile, etc.). Multiple tags may be provided for display and a particular tag (e.g., fair transaction) may be selected 330. Object screening and information display system 300 may determine an object associated with a particular tag and may indicate an object associated with the tag or not (e.g., whether the object is produced and sold in a fair transaction). Check marks may then be provided adjacent to the text of the selected label. The user may then select a second tab, such as a text input tab, to open the text input interface to generate a new tab 340. The text input interface may include a graphical keyboard and the user may enter a new filter or candidate query (e.g., 72% black) 350. The input text, as well as the identified objects, may then be searched to determine which objects are associated with a particular text input. The object that meets the criteria of the first tag and is associated with the text input may then be indicated 360 in the user interface via the spotlight feature.
Fig. 4A and 4B depict illustrations of an example object screening and information display system according to example embodiments of the present disclosure. The object screening and information display system 400 is similar to the object screening and information display system 200 of fig. 2A and 2B and the object screening and information display system 300 of fig. 3A and 3B.
For example, one or more images may be obtained and processed. In some implementations, a processing interface effect 410 may be provided when processing one or more images. Multiple objects in the scene may be identified and a score for each object may be obtained. Additionally and/or alternatively, a plurality of tags may be determined based on the image and/or the context data. The user interface may then provide a score superimposed over the corresponding object, with a label provided at the bottom of the interface for selection 420. Tags may be selected and objects may be screened to determine specific objects that meet certain criteria. The specific object 430 may then be indicated by removing the score from the objects that do not meet the criteria. A second tag may be selected and a second screening may be performed. The user interface may be updated to remove the score 440 from objects that do not meet the first criteria and the second criteria. A third tag may be selected and a third screen may be performed. The user interface may be updated to remove the score 440 from objects that do not meet the first criteria, the second criteria, and the third criteria.
In some implementations, determining that an object meets a certain criteria can involve obtaining object-specific information for the particular object, parsing the information into one or more segments, processing the segments to determine a particular segment classification (e.g., the segments relate to flavor, composition, origin, location, etc.). The systems and methods disclosed herein may then process the fragments and the given criteria to determine if there is an association. The processing may involve natural language processing, and may involve determining whether one or more segments are associated with a given criterion based on one or more knowledge graphs (e.g., whether the segment includes a language that matches or describes the given criterion (e.g., the segment states "citrus" or a synonym of citrus, and the criterion is an item having a citrus flavor)).
Alternatively and/or additionally, the object-specific information may include index data pre-structured into one or more categories of information (e.g., score, calories, flavor, purpose, composition, emissions, etc.). Then, when the keywords or information associated with the selected tag is checked, the object specific information may be crawled.
In some implementations, an object can be associated with a particular tag before the tag is provided for display. For example, multiple objects may be recognized, and multiple corresponding sets of object-specific information may be obtained. The object-specific information sets may be parsed and processed to generate a profile set for each object. The profile sets may be compared to each other to determine distinguishing attributes between objects. The distinguishing attributes may be used to generate tags that narrow the scope of the object list. Objects having particular distinguishing attributes may be pre-associated with a tag such that once the tag is presented and selected, the systems and methods may automatically highlight or indicate the particular object associated with the particular tag.
Additionally and/or alternatively, the object specific information may include one or more predetermined tags indexed in the database and/or knowledge graph. In response to obtaining object-specific information, the systems and methods may determine what tags are generic to all objects in the scene and prune those tags. The remaining predetermined labels may be provided for display and selection. Once a tag is selected, the system and method may indicate each object that includes an index reference to a particular predetermined tag.
In some implementations, one or more tags may be selected so as not to obscure the user experience. One or more tags may be based on search queries of other users when searching for a given object class or particular object. In some implementations, the systems and methods may store and retrieve data related to initial and final searches associated with a particular object and a particular object category. Additionally and/or alternatively, search query data for a particular user or users may be indexed with the user's location given the query or filter. This data may then be used to determine the tag of a particular user or other user. One or more tags may be generated to predict what a user may wish to know about a scene, environment, and/or object. The systems and methods may generate tags based on what the user should search to reach a final action (e.g., purchase selection, self-service (do-it-yourself) step, etc.).
Fig. 5A and 5B depict illustrations of example question-answer interactions, according to example embodiments of the present disclosure. Specifically, image data can be obtained. The image data may be processed to determine that the image data describes an engine compartment of the automobile. Different components of the automobile may be identified and annotated in the augmented reality interface 500. For example, the oil gauge 502, the engine 504, and the battery 508 may be identified. Additionally and/or alternatively, the positive port (+) 506 and the negative port (-) 510 of the battery may be annotated. An input describing a question 554 may be received. Questions 554 may be determined and provided for display in the augmented reality interface 500. A response to the question 554 may then be determined. The response may include an annotation 552 of the object in the scene associated with the answer to the question.
In some implementations, the question-answer interactions may be used for "self-help" items (e.g., automobile maintenance, home improvement, and/or daily activities). Alternatively and/or additionally, the question-answer interaction may be used to answer questions about the environment in which the user is currently located.
FIG. 9 depicts an illustration of an example zoom interaction 900, according to an example embodiment of the disclosure. In particular, in some implementations, the systems and methods disclosed herein may provide more and more information as an object becomes a larger portion of an image (e.g., via scaling or via movement toward the object). In fig. 9, a first example 910 depicts that a single book is fully displayed, and detailed information about an object is superimposed on the single book. The second instance 920 may depict that both books are fully displayed and detailed information about the object may be superimposed on the corresponding books. A third example 930 may depict four books being displayed entirely; only the score may then be superimposed on the corresponding book. A fourth example 940 may depict nine books being displayed entirely and only the score may be superimposed on the corresponding book. The fifth example 950 may include a large number of books that are fully displayed. In response to a large number of books per book utilizing a relatively small portion of the image, the user interface may remove details until a zoom input is received or until a selection input is received. Scaling the interactive interface may enable a user to receive more and more information about objects in the environment by zooming in on the image.
Fig. 10A and 10B depict illustrations of example mobile map application usage in accordance with example embodiments of the present disclosure. In particular, the systems and methods disclosed herein may be implemented into a map application to allow a user to be informed of information associated with different locations. For example, the user may open the map application 1010 and may select an augmented reality experience user interface element (e.g., what is in the vicinity) to open the augmented reality experience. Image data may then be continuously obtained from the image sensor. The image data may be processed to determine what stores, restaurants, landmarks, and/or monuments are depicted in the image data. One or more annotation user interface elements may be generated to mark the identified location. The identified location data may be processed with a machine learning model to determine one or more suggested tags to provide as user interface elements (e.g., a distinguisher tag) that may narrow the scope of the identified location. The augmented reality experience may include an initial interface 1020 with an image stream, a location indicator, and a plurality of tags (e.g., restaurants, coffee, shopping, etc.) for selection. The tags may be determined based on the processed image data, predetermined, determined based on location, determined based on a plurality of user interface elements (e.g., annotations (e.g., text and/or icons) of the depicted building and monument), and/or determined based on various other data. A selection of a particular tag (e.g., a restaurant tag) may be received and a first screening interface 1030 may be provided that includes an image stream, a location indicator, and screened annotations for buildings and monuments associated with the selected tag. New tags may be provided to further screen identified buildings and monuments. The new tag may be determined by determining one or more discriminators between the remaining identified locations.
A second tag (e.g., a american restaurant tag) may be selected, as well as a location (e.g., a building or monument) associated with the second tag. The second screening interface 1040 may include image streams, location indicators, selected second tags, annotations of determined locations, and detailed information user interface elements (e.g., bubbles that may provide details regarding the name, score, distance, and/or business hours of the location). The location user interface element(s) may be selected and a direction prompt interface 1050 may be provided. The direction hint interface 1050 can be interacted with to reopen the route and direction portion of the map application having route information to the location.
Fig. 11 depicts a graphical representation of example book screening based on scoring in accordance with an example embodiment of the present disclosure. As shown, the image capture interface 1110 may be opened and used to capture an image. The image may be processed to identify objects in the image. Object specific information of an object may be obtained and utilized to generate a plurality of corresponding user interface elements for a plurality of objects. Annotating interface 1120 can be provided with objects in the image annotated with a plurality of corresponding user interface elements. A particular object may be selected and a detail bar interface 1130 may be provided. The detail column interface 1130 may include a particular object indicated by darkening surrounding portions of the image. Additionally and/or alternatively, other user interface elements may be moved to the boundary of the interface, and a detail bar may be provided at the bottom of the interface. The details bar may include more detailed information about a particular object, may include selectable elements for transitioning to a search application, and may be configured such that sweeping up may expand the details bar.
FIG. 12 depicts a pictorial representation of an example object specific information display, according to an example embodiment of the present disclosure. In particular, a plurality of images associated with a plurality of different respective objects may be obtained. Multiple images may be generated by segmenting different portions of one or more original images to segment different objects into different images. Alternatively and/or additionally, the plurality of images may be generated separately with one or more image sensors.
In some embodiments, the plurality of images may be selected from a set of images. The user may select multiple images for processing via a selection interface 1210 that displays thumbnails of a collection of images. The selected image may be processed to identify objects in the image, and object-specific information associated with the objects may be obtained for each object. An object specific detail interface may then be provided that may display a first detail panel 1220 associated with the object of the first image. In some implementations, the object-specific detail interface can include a carousel of thumbnails with scoring indicators associated with multiple objects in multiple images. The thumbnail may be selected, which may then cause the associated image to be displayed with information about the objects in the associated image. For example, a second thumbnail may be selected and a second detail panel 1230 may be provided while the carousel and the second image are displayed. Alternatively and/or additionally, the image may be navigated via a swipe gesture and/or various other inputs. In some implementations, the interface can include an automatic navigation that displays each image and detail panel for a given period of time.
Fig. 13 depicts a pictorial representation of an example object specific information display, according to an example embodiment of the present disclosure. Fig. 13 may utilize a user interface similar to fig. 12. In some implementations, a user may capture panoramic images and/or videos depicting multiple objects. Panoramic images and/or video may be processed to detect objects. The object may then be segmented from the input data to generate a plurality of image frames associated with the plurality of objects. For example, the panoramic image may begin with a first object 1310 and end with a fourth object 1320. The panoramic image may be segmented into four image frames associated with four objects. Objects may be identified and then object specific information for each object may be obtained. The object specific information and the image frames may then be used to provide detailed information about the object via the object specific detail interface. The object specific detail interface may include a first detail panel 1330 of a first object, a second detail panel 1340 of a second object, a third detail panel of a third object, and a fourth detail panel of a fourth object.
Fig. 14 depicts a graphical representation of example book screening based on scoring in accordance with an example embodiment of the present disclosure. Specifically, an image may be obtained via image capture interface 1410. The image may be used as an image query and may identify a plurality of objects (e.g., books). Object specific information (e.g., scores) for a plurality of objects may be obtained. A suggestion interface 1420 may be provided, the suggestion interface 1420 providing at least a portion of object-specific information superimposed on a respective object. A shutter user interface element (e.g., a shutter button) may be selected. The systems and methods may determine the focus of the image and provide more detailed information about objects in the focus area of the image via answer interface 1430. In some implementations, the focus object can be indicated via a refined reticle. The focus area may be a central area, an area in the reticle, an area selected via user input, a determined area of user gaze, and/or an area determined to be the focus of the scene. In some implementations, objects in focus may be annotated, and objects outside of focus may not be annotated (however, un-annotated objects may be detected, processed, and identified, where object details are determined to be displayed once the object comes into focus).
FIG. 15 depicts an illustration of an example object-specific search user interface, according to an example embodiment of the present disclosure. The systems and methods disclosed herein may include various user interface display alternatives for an object specific detail interface, which may include an object specific detail panel based on user selection. For example, for each respective identified object in the image, the selected object may be indicated with a user interface element. The first interface 1510 may include a bubble user interface element having text information over a selected object, and text information user interface elements superimposed over corresponding other objects. The second interface 1520 may include a bubble user interface element having text information over the selected object, and text information user interface elements of corresponding other objects at the periphery of the user interface. The third interface 1530 may include a bubble user interface element having text information over the selected object, as well as undepicted user interface elements superimposed over corresponding other objects.
FIG. 16 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure. The systems and methods may utilize a variety of different user interface elements. In particular, the user interface elements may include user interface elements having only icons, user interface elements having only text (e.g., 1602, 1608, and 1614), user interface elements having text and icons (e.g., 1616, 1606, and 1612), and user interface elements having text of different styles and sizes (e.g., 1618). The user interface elements may have different sizes and shapes. Additionally and/or alternatively, the user interface element may have a pointer, backbone, or another indicator of a particular associated object.
FIG. 17 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure. In fig. 17, a first user interface 1710 includes a plurality of identified objects annotated with scores. The user may select a tag request icon to obtain a screening interface 1720, and may interact with the screening interface 1720 to screen annotations as those having only objects meeting a given criteria (e.g., objects scored above a particular threshold). A second set of tags may then be determined and provided for selection (e.g., tags associated with the genre of the particular object in the scene). One or more tags may be selected to provide a third interface 1730 that may describe annotations that are superimposed only on objects that satisfy both criteria.
FIG. 18 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure. In some implementations, the systems and methods may include a selectable user interface element (e.g., a button) for hiding the annotation user interface element. Hidden buttons may be provided at the bottom 1810 of the user interface, at the corners 1820 of the user interface, or at the top 1830 of the user interface.
Fig. 19 depicts a graphical representation of an example user interface transition, according to an example embodiment of the present disclosure. The user interface transition may include a thought phase 1910 that may indicate that an image is being processed. Next, the user interface transition may include an annotation stage 1920 that overlays the annotation user interface element overlaid on the identified object. The filter may then be selected and the filtering stage 1930 may be provided with annotation user interface elements that are limited to the objects associated with the selected filter. An annotation user interface element may be selected and a search stage 1940 may be provided for display. In search stage 1940, the region with the selected object may be highlighted with one or more visual effects. In some implementations, a detail panel (e.g., a knowledge panel) can be provided for display and can describe information associated with the selected object.
Fig. 20 depicts an illustration of an example focus interaction, according to an example embodiment of the present disclosure. In some implementations, the annotation user interface element may change in appearance based on whether an object associated with the annotation is in focus of the camera interface. For example, the first stage 2010 may include all annotation user interface elements being semi-transparent. In the second stage 2020, the camera interface may have a single object 2002 in the reticle. The annotation user interface element associated with the single object 2002 may then be displayed as completely opaque.
FIG. 21 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure. The annotation user interface elements associated with the identified object may include one or more icons 2110, text in a bubble and icons 2120, and/or bubbles 2130 of multiple text sizes with more detailed information (e.g., scoring of the object and where the score comes from). The different levels of information provided may be determined based on user preferences, one or more user selections, the number of objects annotated, the amount of information available, the distance to the objects, and/or the screen size.
In some implementations, the location and/or size of the user interface element overlay can be determined and/or adjusted based on interface display availability. For example, user interface elements may be displayed at a higher location on the object than adjacent user interface elements to avoid overcrowding and/or element overlapping. Alternatively and/or additionally, the amount of information and/or the text size may be adjusted.
FIG. 22 depicts a pictorial representation of example user interface elements, according to an example embodiment of the present disclosure. The user interface elements may include three-dimensional dynamic elements 2210 that may be rotated based on where the reticle is located. Alternatively and/or additionally, the dimensions, content and/or size of the user interface element may vary based on the location of the reticle. For example, at 2220, a point may be displayed on an object in the scene, and when a reticle hovers over the point, the point may expand to include a text bubble. At 2230 and 2240, the annotation user interface element may be provided over the object in the augmented reality experience, rather than superimposed over the object.
Fig. 23 depicts a diagram of an example switching element 2302 for turning on and off object markers, according to an example embodiment of the disclosure. In particular, the first interface 2310 may include a plurality of annotation user interface elements that indicate information of objects in a scene. The systems and methods may then receive a selection of the handover element 2302 and may provide an annotation user interface element to the second interface 2320. Additionally and/or alternatively, the switching element 2302 may be used to interchangeably switch between the first interface 2310 and the second interface 2320.
Fig. 24 depicts a diagram of an example score screening element for screening based on scores, according to an example embodiment of the present disclosure. At 2410, in response to identifying the object in the scene, a plurality of annotation user interface elements 2412 may be provided. The systems and methods may then receive selection 2414 of screening tags (e.g., only the highest scoring tags) and transition to 2420. At 2420, annotation user interface element 2422 may include only user interface elements associated with objects that meet the screening criteria. Based on whether the filter tab 2414 has been selected, unselected, or deselected, the filter tab 2414 may be provided in a different color and/or with a different icon.
Fig. 25 depicts a diagram of an example score filter slider element for filtering based on scores, according to an example embodiment of the present disclosure. In some implementations, the filtering can be based on interactions with the filtering slider 2522. For example, at 2510, a plurality of annotation user interface elements may be provided for display with the filter tag 2512. The filter tag 2512 may be selected to open the filter slide 2522. At 2520, the filter slider 2522 has been interacted with to filter the annotation user interface elements, displaying only the end user interface elements 2524 associated with the object having a score of about 90%.
FIG. 26 depicts an illustration of an example search interface, according to an example embodiment of the present disclosure. In particular, the systems and methods disclosed herein may switch between the first interface 2610 and the second interface 2620 based on search element selection. The first interface 2610 may include one or more out-of-line user interface elements 2612 provided for display as semi-transparent, and one or more in-focus user interface elements 2614 provided completely opaque to indicate that the associated object is within the reticle. The search element may then be selected to transition to a second interface 2620, the second interface 2620 providing a detail panel associated with the in-focus object (e.g., the object associated with the in-focus user interface element 2614).
Fig. 27 depicts a block diagram of an example tag generation model 2700, according to an example embodiment of the disclosure. The example tag generation model 2700 may include a plurality of machine learning models and may include one or more deterministic functions. The tag generation model 2700 may be trained to receive image data 2702 (e.g., a plurality of image frames associated with a scene) and output one or more tags 2724 (e.g., screening tags and/or candidate query tags).
Stitching model 2704 may process image data 2702 to determine whether two or more image frames describe the same scene. If the image frames are determined to be associated with the same scene, the stitching model may generate scene data 2706 describing the image frames stitched together. The recognition model may process scene data 2706 and/or image data 2702 to identify and/or classify object(s) in the scene and/or image(s). The recognition models may include a detection model 2708, a segmentation model 2710, and a recognition model 2712. Detection model 2708 may process image data 2702 and/or scene data 2706 to generate a bounding box around one or more objects detected in the scene. Segmentation model 2710 may process bounding box(s) and image data 2702 (and/or scene data 2706) to segment image portions associated with the bounding box(s). The recognition model 2712 may process segmented portions of the image to recognize each detected object to generate object data 2714. The object data 2714 may then be used to search 2716 one or more databases for object specific information 2718 for each recognized object.
The tag determination model 2722 may then process the object specific information 2718 and/or the context data 2720 to generate one or more tags. One or more tags may then be used to receive input from the user to provide more customized data to the user.
Example method
Fig. 6 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the order or arrangement specifically shown. The various steps of method 600 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 602, a computing system may obtain image data generated by a mobile image capture device. The image data may depict a scene.
At 604, the computing system may process the image data to determine a plurality of objects in the scene. The plurality of objects may include one or more consumer goods.
At 606, the computing system may obtain object-specific information for one or more objects of the plurality of objects. The object specific information may include one or more details associated with each of the one or more objects.
In some implementations, a computing system may obtain context data associated with a user and determine a query based on the image data and the context data. Object specific information may be obtained based at least in part on the query. The context data may describe a user location, user preferences, past user queries, and/or user shopping history. For example, the context data may describe a user location. The computing system may obtain one or more popular queries associated with the user location. The query may be determined based at least in part on one or more popular queries.
Alternatively and/or additionally, the computing system may determine an object class associated with the plurality of objects and may obtain the object-specific information based at least in part on the object class.
At 608, the computing system may provide one or more user interface elements overlaid on the image data. One or more user interface elements may describe object specific information. In some implementations, the one or more user interface elements can include a plurality of product attributes associated with a particular object in the scene.
In some implementations, a computing system may obtain input data associated with a selection of a particular user interface element associated with a particular product attribute and provide one or more indicators overlaid on the image data. The one or more indicators may describe one or more particular objects associated with one or more particular product attributes. In some implementations, the particular product attribute can include a threshold product score, and the particular user interface element can include a slider associated with a range of consumer product scores. Additionally and/or alternatively, the plurality of product attributes may include a plurality of different product types.
Alternatively and/or additionally, the computing system may determine a plurality of filters associated with the plurality of objects. Each filter may include criteria associated with a subset of the plurality of objects. The computing system may provide a plurality of filters for display in the user interface. In some implementations, the computing system may obtain a filter selection associated with a particular filter of the plurality of filters and provide augmented reality coverage on one or more image frames. The augmented reality overlay may include one or more user interface elements provided on respective objects that meet respective criteria of a particular filter.
In some implementations, a computing system may receive audio data. The audio data may describe voice commands. The computing system may determine a particular object associated with the voice command and provide an enhanced image frame indicating the particular object associated with the voice command.
Fig. 7 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 7 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the order or arrangement specifically shown. The various steps of method 700 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 702, a computing system may obtain video stream data generated by a mobile image capture device. The video stream data may include a plurality of image frames.
At 704, the computing system may determine that the first image frame and the second image frame are associated with a scene. The first image frame may include a first set of objects and the second image frame may include a second set of objects. In some implementations, determining that the first image frame and the second image frame are associated with the scene may include determining that the first set of objects and the second set of objects are associated with a particular object class. Alternatively and/or additionally, determining that the first image frame and the second image frame are associated with the scene may include determining that the first image frame and the second image frame are captured at a particular location.
At 706, the computing system may generate scene data including a first image frame and a second image frame of the plurality of image frames.
At 708, the computing system may process the scene data to determine a plurality of objects in the scene. The plurality of objects may include one or more consumer goods. In some implementations, the plurality of objects may include a plurality of consumer goods.
At 710, the computing system may obtain object-specific information for one or more objects of the plurality of objects. The object specific information may include one or more details associated with each of the one or more objects. In some implementations, the object-specific information can include one or more consumer product details associated with each of the plurality of objects.
At 712, the computing system may provide one or more user interface elements overlaid on the one or more objects. One or more user interface elements may describe object specific information. In some implementations, multiple user interface elements may be provided overlaid on multiple objects. The plurality of user interface elements may describe object specific information. The plurality of user interface elements may be associated with a plurality of consumer goods. In some implementations, providing one or more user interface elements overlaid on one or more objects may include adjusting a plurality of pixels associated with an outer region surrounding the one or more objects.
Fig. 8 depicts a flowchart of an example method performed in accordance with an example embodiment of the present disclosure. Although fig. 8 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the order or arrangement specifically shown. The various steps of method 800 may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At 802, a computing system may obtain image data. The image data may depict a scene.
At 804, the computing system may process the image data to determine a plurality of filters. Multiple filters may be associated with multiple objects in a scene. In some implementations, processing the image data to determine a plurality of filters may include processing the image data to identify a plurality of objects in the scene, determining a plurality of distinguishing attributes associated with a distinguisher between the plurality of objects, and determining the plurality of filters based at least in part on the plurality of distinguishing attributes. The image data may be processed with one or more machine learning models (e.g., detection models, segmentation models, classification models, and/or recognition models).
In some implementations, the computing system can determine a plurality of filters based at least in part on the obtained context data. The context data may include a current location of the user, a specific user profile, a global trend, a time of day, a time of year, and/or a recent interaction of the user with one or more applications (e.g., a recent search in a search application). For example, a user's recent query may be used as a filter in the case where the query is applied to at least one object in a scene. Additionally and/or alternatively, other users previously at one location may have used a certain tag at a higher rate than another tag. The user may be provided with a particular tag based on previous interactions of other users at a given location.
At 806, the computing system may provide one or more particular filters of the plurality of filters for display in a user interface. One or more particular filters may be provided via user interface french fries tags provided as selectable user interface elements.
At 808, the computing system may obtain input data. The input data may be associated with a selection of a particular filter of the plurality of filters.
At 810, the computing system may provide one or more indicators overlaid on the image data. The one or more indicators may describe one or more particular objects associated with a particular filter. In some implementations, the one or more indicators can include object-specific information associated with one or more particular objects. Providing one or more indicators overlaid on the image data may include an augmented reality experience.
In some implementations, the computing system may obtain the second input data. The second input data may be associated with a zoom input. The zoom input may be associated with one or more particular objects. The computing system may obtain second information associated with one or more particular objects. An enhanced image may be generated based at least in part on the image data and the second information. The enhanced image may include an enlarged portion of the scene associated with an area including one or more particular objects. In some implementations, the one or more indicators and the second information may be overlaid on one or more particular objects.
In some implementations, determining that an object meets a certain criteria can involve obtaining object-specific information for the particular object, parsing the information into one or more segments, processing the segments to determine a particular segment classification (e.g., the segments relate to flavor, composition, origin, location, etc.). The computing system may then process the fragments and the given criteria to determine if an association exists. The processing may involve natural language processing, and may involve determining whether one or more segments are associated with a given criterion based on one or more knowledge graphs (e.g., whether the segment includes a language that matches or describes the given criterion (e.g., the segment states "citrus" or a synonym of citrus, and the criterion is an item having a citrus flavor)).
Alternatively and/or additionally, the object-specific information may include index data pre-structured into one or more categories of information (e.g., score, calories, flavor, purpose, composition, emissions, etc.). Then, when the keywords or information associated with the selected tag is checked, the object specific information may be crawled.
In some implementations, an object can be associated with a particular tag before the tag is provided for display. For example, multiple objects may be recognized, and multiple corresponding sets of object-specific information may be obtained. The object-specific information sets may be parsed and processed to generate a profile set for each object. The profile sets may be compared to each other to determine distinguishing attributes between objects. The distinguishing attributes may be used to generate tags that narrow the scope of the object list. Objects having particular distinguishing attributes may be pre-associated with a tag such that once the tag is presented and selected, the computing system may automatically highlight or indicate the particular object associated with the particular tag.
Additionally and/or alternatively, the object specific information may include one or more predetermined tags indexed in the database and/or knowledge graph. In response to obtaining object-specific information, the computing system may determine what tags are generic to all objects in the scene and prune those tags. The remaining predetermined labels may be provided for display and selection. Once the tags are selected, the computing system may indicate each object that includes an index reference to a particular predetermined tag.
In some implementations, one or more tags may be selected so as not to obscure the user experience. One or more tags may be based on search queries of other users when searching a given object class or a particular object. In some implementations, the computing system may store and retrieve data related to initial searches and final searches associated with particular objects and particular object categories. Additionally and/or alternatively, search query data for a particular user or users may be indexed with the user's location given the query or filter. This data may then be utilized to determine a tag for a particular user or other user. One or more tags may be generated to predict what a user may wish to know about a scene, environment, and/or object. The computing system may generate the tags based on what the user should search for to reach the final action (e.g., purchase selection, self-help step, etc.).
Additional disclosure
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from these systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components working in combination. The database and applications may be implemented on a single system or may be distributed across multiple systems. Distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Modifications, variations and equivalents of these embodiments will occur to those skilled in the art upon review of the foregoing description. Accordingly, this subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Claims (20)
1. A computing system, the system comprising:
one or more processors; and
one or more non-transitory computer-readable media collectively storing instructions that, when executed by the one or more processors, cause the computing system to perform operations comprising:
obtaining image data generated by a mobile image capture device, wherein the image data depicts a scene;
processing the image data to determine a plurality of objects in the scene, wherein the plurality of objects comprises one or more consumer goods;
obtaining object-specific information for one or more of the plurality of objects, wherein the object-specific information includes one or more details associated with each of the one or more objects; and
one or more user interface elements are provided overlaid on the image data, wherein the one or more user interface elements describe the object-specific information.
2. The system of claim 1, wherein the one or more user interface elements comprise a plurality of product attributes associated with a particular object in the scene; and is also provided with
Wherein the operations further comprise:
Obtaining input data associated with a selection of a particular user interface element associated with a particular product attribute; and
one or more indicators overlaid on the image data are provided, wherein the one or more indicators describe one or more particular objects associated with the one or more particular product attributes.
3. The system of claim 2, wherein the particular product attribute comprises a threshold product score; and is also provided with
Wherein the particular user interface element includes a slider associated with a consumer product score range.
4. The system of claim 2, wherein the plurality of product attributes comprises a plurality of different product types.
5. The system of claim 1, wherein the operations further comprise:
obtaining context data associated with a user;
determining a query based on the image data and the context data; and is also provided with
Wherein the object specific information is obtained based at least in part on the query.
6. The system of claim 5, wherein the context data describes a user location;
wherein the operations further comprise:
Obtaining one or more popular queries associated with the user location; and is also provided with
Wherein the query is determined based at least in part on the one or more popular queries.
7. The system of claim 5, wherein the context data describes at least one of a user location, a user preference, a past user query, or a user shopping history.
8. The system of claim 1, wherein the operations further comprise:
determining a plurality of filters associated with the plurality of objects, wherein each filter includes criteria associated with a subset of the plurality of objects;
providing the plurality of filters for display in a user interface;
obtaining a filter selection associated with a particular filter of the plurality of filters; and
an augmented reality overlay is provided on one or more image frames, wherein the augmented reality overlay includes the one or more user interface elements provided on respective objects that satisfy respective criteria of the particular filter.
9. The system of claim 1, wherein the operations further comprise:
determining an object class associated with the plurality of objects; and is also provided with
Wherein the object specific information is obtained based at least in part on the object category.
10. The system of claim 1, wherein the operations further comprise:
receiving audio data, wherein the audio data describes a voice command;
determining a particular object associated with the voice command; and
an enhanced image frame is provided that indicates a particular object associated with the voice command.
11. A computer-implemented method for providing a user interface, the method comprising:
obtaining, by a computing system comprising one or more processors, video stream data generated by a mobile image capture device, wherein the video stream data comprises a plurality of image frames;
determining, by the computing system, that the first image frame and the second image frame are associated with a scene;
generating, by the computing system, scene data comprising the first image frame and the second image frame of the plurality of image frames;
processing, by the computing system, the scene data to determine a plurality of objects in the scene, wherein the plurality of objects includes one or more consumer goods;
obtaining, by the computing system, object-specific information for one or more objects of the plurality of objects, wherein the object-specific information includes one or more details associated with each object of the one or more objects; and
One or more user interface elements overlaid on the one or more objects are provided by the computing system, wherein the one or more user interface elements describe the object specific information.
12. The method according to claim 11, wherein:
the plurality of objects includes a plurality of consumer goods;
the object specific information includes one or more consumer product details associated with each of a plurality of objects; and is also provided with
Providing, by the computing system, one or more user interface elements overlaid on the one or more objects includes:
a plurality of user interface elements overlaid on the plurality of objects is provided by the computing system, wherein the plurality of user interface elements describe the object-specific information, and wherein the plurality of user interface elements are associated with the plurality of consumer goods.
13. The method of claim 11, wherein the first image frame comprises a first set of objects, and wherein the second image frame comprises a second set of objects; and is also provided with
Wherein determining, by the computing system, that the first image frame and the second image frame are associated with the scene comprises:
determining, by the computing system, that the first set of objects and the second set of objects are associated with a particular object class.
14. The method of claim 11, wherein determining, by the computing system, that the first image frame and the second image frame are associated with a scene comprises:
the first image frame and the second image frame are determined by the computing system to be captured at a particular location.
15. The method of claim 11, wherein providing, by the computing system, one or more user interface elements overlaid on the one or more objects comprises adjusting a plurality of pixels associated with an outer region surrounding the one or more objects.
16. One or more non-transitory computer-readable media collectively storing instructions that, when executed by one or more computing devices, cause the one or more computing devices to perform operations comprising:
obtaining image data, wherein the image data depicts a scene;
processing the image data to determine a plurality of filters, wherein the plurality of filters are associated with a plurality of objects in the scene;
providing one or more particular filter of the plurality of filters for display in a user interface;
obtaining input data, wherein the input data is associated with a selection of a particular filter of the plurality of filters; and
One or more indicators overlaid on the image data are provided, wherein the one or more indicators describe one or more particular objects associated with the particular filter.
17. The one or more non-transitory computer-readable media of claim 16, wherein processing the image data to determine the plurality of filters comprises:
processing the image data to identify a plurality of objects in the scene;
determining a plurality of distinguishing attributes associated with distinguishing characters between the plurality of objects; and
the plurality of filters is determined based at least in part on the plurality of distinguishing attributes.
18. The one or more non-transitory computer-readable media of claim 17, wherein processing the image data to identify a plurality of objects in the scene comprises: the image data is processed with a machine learning model.
19. The one or more non-transitory computer-readable media of claim 16, wherein the operations further comprise:
obtaining second input data, wherein the second input data is associated with a zoom input, wherein the zoom input is associated with the one or more particular objects;
Obtaining second information associated with the one or more particular objects; and
an enhanced image is generated, wherein the enhanced image includes a magnified portion of the scene associated with an area including the one or more particular objects, and wherein the one or more indicators and the second information are overlaid on the one or more particular objects.
20. The one or more non-transitory computer-readable media of claim 16, wherein the one or more indicators comprise object-specific information associated with the one or more specific objects, and wherein providing one or more indicators overlaid on the image data comprises an augmented reality experience.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US63/340,078 | 2022-05-10 | ||
US18/084,710 | 2022-12-20 | ||
US18/084,710 US20230368527A1 (en) | 2022-05-10 | 2022-12-20 | Object Filtering and Information Display in an Augmented-Reality Experience |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116719405A true CN116719405A (en) | 2023-09-08 |
Family
ID=87870465
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310521305.6A Pending CN116719405A (en) | 2022-05-10 | 2023-05-10 | Object screening and information display in augmented reality experience |
Country Status (1)
Country | Link |
---|---|
CN (1) | CN116719405A (en) |
-
2023
- 2023-05-10 CN CN202310521305.6A patent/CN116719405A/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10031649B2 (en) | Automated content detection, analysis, visual synthesis and repurposing | |
AU2017225018B2 (en) | A system for creating virtual reality experiences from pdf | |
JP6328761B2 (en) | Image-based search | |
CN106471458B (en) | Intelligent sliding and wiping strip in electronic book navigation interface | |
KR101780034B1 (en) | Generating augmented reality exemplars | |
CN114375435A (en) | Enhancing tangible content on a physical activity surface | |
WO2017066543A1 (en) | Systems and methods for automatically analyzing images | |
CN105320428A (en) | Image provided method and device | |
US20170220591A1 (en) | Modular search object framework | |
CN101783886A (en) | Information processing apparatus, information processing method, and program | |
CN107430479A (en) | Information processor, information processing method and program | |
KR20160062565A (en) | Device and method for providing handwritten content | |
CN107209775A (en) | Method and apparatus for searching for image | |
CN104217008A (en) | Interactive type labeling method and system for Internet figure video | |
KR20200122362A (en) | Browser for mixed reality systems | |
CN107229741A (en) | Information search method, device, equipment and storage medium | |
US20220155940A1 (en) | Dynamic collection-based content presentation | |
US9619519B1 (en) | Determining user interest from non-explicit cues | |
CN113330455A (en) | Finding complementary digital images using conditional generative countermeasure networks | |
Sluÿters et al. | Consistent, continuous, and customizable mid-air gesture interaction for browsing multimedia objects on large displays | |
JP2023162232A (en) | Intelligent systems and methods for visual search queries | |
CN113168354B (en) | System and method for selecting and providing available actions to a user from one or more computer applications | |
KR101794137B1 (en) | Data visualization method and system using reference semantic map | |
US20230368527A1 (en) | Object Filtering and Information Display in an Augmented-Reality Experience | |
CN116719405A (en) | Object screening and information display in augmented reality experience |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |