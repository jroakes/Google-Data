US20150363640A1 - Automatically organizing images - Google Patents
Automatically organizing images Download PDFInfo
- Publication number
- US20150363640A1 US20150363640A1 US14/303,997 US201414303997A US2015363640A1 US 20150363640 A1 US20150363640 A1 US 20150363640A1 US 201414303997 A US201414303997 A US 201414303997A US 2015363640 A1 US2015363640 A1 US 2015363640A1
- Authority
- US
- United States
- Prior art keywords
- images
- image
- user
- information
- group
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/40—Document-oriented image-based pattern recognition
- G06V30/41—Analysis of document content
- G06V30/413—Classification of content, e.g. text, photographs or tables
-
- G06K9/00456—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q10/00—Administration; Management
- G06Q10/10—Office automation; Time management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G06F17/30247—
Definitions
- Digital photographs are commonly stored on a computer, portable computing device, camera, or other computing device in a manner that allows them to be reviewed by a user at a later time. It is not common, however, for digital photographs to be associated with rich identifying information or contextual information. Instead, digital photographs often have very little useful information in their name or in associated metadata to identify them, unless a user takes affirmative steps to rename the photographs and/or add metadata to the photographs (e.g. “tagging”).
- Date and time information is useful, but may not be sufficient to allow users to find a particular photograph among a large number of photographs or to find a photograph taken long ago.
- the disclosure relates generally to automatically organizing images.
- One aspect of the disclosed embodiments is a method that includes obtaining a plurality of images, obtaining geolocation information for each image, and obtaining time information for each image.
- the method also includes identifying, by one or more computing devices, a group of related images from the plurality of images from a common geographical area and a common time period based on the geolocation information and the time information for each of the plurality of images, and identifying, by the one or more computing devices, an event that occurred within the common time period and within the common geographical area by searching a repository of event information.
- the method also includes storing, by the one or more computing devices, the group of related images in an image collection, and assigning, by the one or more computing devices, a title to the image collection, wherein the title is based at least in part on a title of the event.
- Another aspect of the disclosed embodiments is a method that includes obtaining a plurality of images, and identifying a group of related images from the plurality of images.
- the method also includes extracting visible features from at least some images from the plurality of images and identifying, by one or more computing devices, one or more text-based feature labels for at least some of the extracted visible features by comparing the extracted visible features to a repository of images that include indexed features.
- the method also includes storing, by the one or more computing devices, the group of related images in an image collection and assigning, by the one or more computing devices, a title to the image collection, wherein the title is based at least in part on the one or more text-based feature labels.
- Another aspect of the disclosed embodiments is a method that includes obtaining a plurality of images that are associated with a user and obtaining geolocation information for each of the images.
- the method also includes defining, by one or more computing devices and based on the geolocation information, a plurality of image groups that each include a plurality of images that correspond a distinct geographical area.
- the method also includes identifying, by the one or more computing devices, a home location for the user and identifying, by the one or more computing devices, a first image group from the plurality of image groups that does not correspond to the home location for the user.
- the method also includes storing, by the one or more computing devices, the images from the first image group in an image collection and assigning, by the one or more computing devices, a title to the image collection, wherein the title is based at least in part on the distinct geographical area for the first image group.
- FIG. 1 is a block diagram showing an example of an environment in which a system for automatically organizing images can be implemented
- FIG. 2 is a block diagram showing an example of a hardware configuration for a server computer
- FIG. 3 is a block diagram showing operation of an image organizing system
- FIG. 4 is a flowchart showing a first example of a process for automatically organizing images
- FIG. 5 is a flowchart showing a second example of a process for automatically organizing images.
- FIG. 6 is a flowchart showing a third example of a process for automatically organizing images.
- digital images such as digital photographs can be automatically organized by grouping the digital images and placing the digital images into albums that are titled based on information associated with the digital images.
- a plurality of images can be
- FIG. 1 shows an example of an environment 100 in which a system for automatically organizing images can be implemented.
- the environment 100 can include a user system 110 , one or more additional user systems 120 , and an application hosting service 130 .
- the user system 110 and the additional user systems 120 are each representative of a large number (e.g. millions) of systems that can be included in the environment 100 , with each system being able to utilize one or more applications that are provided by the application hosting service 130 .
- the user system 110 and the additional user systems 120 can each be any manner of computer or computing device, such as a desktop computer, a laptop computer, a tablet computer, or a smart-phone (a computationally-enabled mobile telephone).
- the application hosting service 130 can be implemented using one or more server computers 132 .
- the user system 110 , the additional user systems 120 , and the application hosting service 130 can each be implemented as a single system, multiple systems, distributed systems, or in any other form.
- the systems, services, servers, and other computing devices described here are in communication via a network 150 .
- the network 150 can be one or more communications networks of any suitable type in any combination, including wireless networks, wired networks, local area networks, wide area networks, cellular data networks, and the internet.
- the application hosting service 130 can provide access to one or more hosted applications to a defined group of users including operators associated with the user system 110 and the additional user systems 120 .
- One or more of the hosted applications can be a storage system that is operable to implement storage and retrieval functions and output, for display to a user, a user interface that allows the user to store, browse, organize, retrieve, view, delete, and/or perform other operations with respect to digital images that are stored as data files at the application hosting service 130 .
- the digital images can be arranged by a hierarchical manner, such as a folder structure.
- the application hosting service can allow access to store, edit, delete, and/or view the digital images by a single user, by a group of designated users, or by all users.
- the user interface for the storage system can be output by the application hosting service 130 for display at a device associated with the user, such as the user system 110 , by transmission of signals and/or data from the application hosting service to the user system 110 that, when interpreted by the user system 110 , cause display of the interface at the user system 110 .
- FIG. 2 is a block diagram of an example of a hardware configuration for the one or more server computers 132 of FIG. 1 .
- the same hardware configuration or a similar hardware configuration can be used to implement the user system 110 and the additional user systems 120 .
- Each server computer 132 can include a CPU 210 .
- the CPU 210 can be a conventional central processing unit. Alternatively, the CPU 210 can be any other type of device, or multiple devices, capable of manipulating or processing information now-existing or hereafter developed. Although the disclosed examples can be practiced with a single processor as shown, e.g. CPU 210 , advantages in speed and efficiency can be achieved using more than one processor.
- Each server computer 132 can include memory 220 , such as a random access memory device (RAM). Any other suitable type of storage device can also be used as the memory 220 .
- the memory 220 can include code and data 222 that can be accessed by the CPU 210 using a bus 230 .
- the memory 220 can further include one or more application programs 224 and an operating system 226 .
- the application programs 224 can include software components in the form of computer executable program instructions that cause the CPU 210 to perform the operations and methods described here.
- a storage device 240 can be optionally provided in the form of any suitable computer readable medium, such as a hard disc drive, a memory device, a flash drive, or an optical drive.
- One or more input devices 250 such as a keyboard, a mouse, or a gesture sensitive input device, receive user inputs and can output signals or data indicative of the user inputs to the CPU 210 .
- One or more output devices can be provided, such as a display device 260 .
- the display device 260 such as a liquid crystal display (LCD) or a cathode-ray tube (CRT), allows output to be presented to a user, for example, in response to receiving a video signal.
- LCD liquid crystal display
- CRT cathode-ray tube
- FIG. 2 depicts the CPU 210 and the memory 220 of each server computer 132 as being integrated into a single unit, other configurations can be utilized.
- the operations of the CPU 210 can be distributed across multiple machines (each machine having one or more of processors) which can be coupled directly or across a local area or other network.
- the memory 220 can be distributed across multiple machines such as network-based memory or memory in multiple machines.
- the bus 230 of each of each server computer 132 can be composed of multiple buses.
- the storage device 240 can be directly coupled to the other components of the respective server computer 132 or can be accessed via a network and can comprise a single integrated unit such as a memory card or multiple units such as multiple memory cards.
- the one or more server computers can thus be implemented in a wide variety of configurations.
- FIG. 3 is a block diagram showing an image organizing system 300 .
- the image organizing system 300 includes a data extraction component 310 , a grouping component 320 , and a titling component 330 .
- the image organizing system 300 receives a plurality of images 340 as an input and generates an album 350 (i.e. a digital photo album) as an output.
- the images 340 can be any group or collection of images, and can be unorganized images that are not associated with user-defined tags, labels, categories, or collections.
- the images 340 are images stored at a server-based storage system and are associated with a user-account at the server-based storage system.
- the images 340 are received from a single image capture device, such as a digital camera or a smart phone with an integrated digital camera.
- the images 340 can be in any suitable digital image format, with the JPEG format being a typical example.
- the images 340 can each be associated with metadata that describes some characteristic associated with the image.
- Metadata that can be associated with any or all of the images 340 is time information that describes when the image was recorded (e.g. by a digital camera). Time information can be in the form of a time stamp including a value or values representing the date and/or time at which the image was recorded.
- geolocation information e.g. coordinates
- the image organizing system 300 can be implemented using any computing device that is operable to receive the images 340 , execute instructions operable to perform the functions associated with the image organizing system 300 , and output the album 350 .
- the image organizing system 300 can be implemented as a hosted application at the application hosting service 130
- the images 340 can be received from the storage system that is implemented by the application hosting service 130
- the album 350 generated in this example will be accessible using the storage system of the application hosting service 130 via an interface that is displayed by a remote computing device through a web browser application or a dedicated application.
- the image organizing system 300 can be implemented as a local application at the user system 110 , the images 340 can be received from a local file system of the user system 110 , and the album 350 can be generated as, for example, a folder containing a subset of the images 340 .
- the data extraction component 310 is operable to obtain information for each of the images 340 .
- Obtaining information for an image can include accessing information stored within the image itself, accessing information stored in association with the image, or using the information stored within or in association with the image to access external information that is relevant to the image but is not associated with the image.
- Information contained in the image itself includes information describing one or more visible features of the image as represented by the pixel values of the image.
- the images 340 can be processed to extract visible features from them. Visible features can be extracted from the images 340 in any suitable manner. The visible features that are extracted from the images 340 can then be compared with known features, as will be discussed further herein. Methods of extracting visible features and identifying them by comparing them to known features are well known in the art. For example, methods of object recognition in images are described in David G. Lowe, “Object recognition from local scale-invariant features,” International Conference on Computer Vision, Corfu, Greece (September 1999), pp. 1150-1157.
- Information associated with the image includes metadata that is written by the image capture device (e.g. digital camera) that recorded the image and metadata that is added to the image after it is recorded, such as by manual entry of the metadata by a user.
- Metadata can include many types of information, including geolocation information and time information, as previously noted.
- the information extracted from the images 340 by the data extraction component 310 is passed to the grouping component 320 .
- the grouping component 320 is operable to identify a group of related images from the plurality of images 340 .
- the grouping component 320 can group images based on a single characteristic or based on a combination of multiple characteristics.
- images can be grouped by the grouping component 320 based on geolocation information.
- a first image is selected, and other images that were recorded at locations within a determined distance are grouped with the first image.
- the locations at which images were recorded are determined, and a clustering algorithm can be utilized to define clusters of images, with each cluster including images that were recorded within a certain geographic proximity of one another, as determined by the clustering algorithm.
- Clustering algorithms are well-known, with a k-means clustering algorithm being an example of a suitable clustering algorithm.
- a relatedness metric that represents a probability that the images in the group are actually related (i.e. grouped correctly) can be generated for each group by the clustering algorithm.
- images can be grouped by the grouping component 320 based on time information.
- images within a predetermined time window can be considered related.
- a clustering algorithm can be used to generate time based clustering algorithms, and a relatedness metric can be generated as previously described.
- images can be grouped by grouping component 320 based on visible features in the images. Visible features are extracted as previously described, and images are grouped based on the presence of common or similar visible features. For example, if a particular object appears in multiple images, a group can be defined that includes those images.
- images can be grouped by grouping component 320 based on visible features in the images. Visible features are extracted as previously described, and images are grouped based on the presence of common or similar visible features.
- images can be grouped by grouping component 320 based on inclusion of related subject matter.
- visible features are extracted from each of the images by the data extraction component 310 , and those features are identified using external information such as a repository of feature information, with visible features in the repository being associated with annotations in the form of text-based labels that are associated with individual features.
- Those text-based labels are associated with the images, and the grouping component then groups the images based on semantic relatedness. For example, among pictures taken at a birthday party, the images might include features such as presents, cake, and decorations, these features can be identified as semantically related based on their common association with the concept of a party, and this can be used by the grouping component as a basis for defining a group of related images.
- multiple characteristics can be combined by the grouping component to identify a group of related images.
- groups of possibly related images can be defined, and the relatedness metric is calculated for each of multiple characteristics.
- geographic clustering could be biased using time information, such that larger and/or less-well defined geographic clusters would be identified as related images if those images were captured within a related time period.
- the result may be a plurality of images that form a loose geographical cluster, as the user travels within a region and visits multiple areas over the course of several days.
- this cluster would be distinct from those clusters around the user's home, and since the images are clustered time-wise over the span of a few days, these two circumstances can, in combination, be sufficient for the grouping component 320 to define a group that includes those images.
- the grouping component 320 is a basis for identifying a group of related images. Any other characteristic obtained from information in the image, information associated with the image, or identified based on information in or associated with the image can be utilized as a basis for identifying groups of related images. In addition, any combination of these characteristics can be utilized as a basis for identifying groups of related images.
- Information identifying the group of related images that was identified by the grouping component 320 is passed to the titling component 330 .
- This information can include, for example, a list of the images.
- the titling component 330 is operable to assign a title to the group of related images, and this title is utilized as the title of the album 350 that is generated by the image organizing system 300 .
- the titling component 330 is operable to determine the title for the album based at least in part on the information extracted from images by the data extraction component 310 . Any of the previously discussed characteristics can be utilized as a basis for determining the title, as well as any other characteristic based on the extracted information.
- the titling component 330 is operable to access the external data 360 , and to utilize the external data 360 to generate the title for the album 350 .
- any type of relevant information can be utilized as the external data 360 .
- Information utilized by the titling component may be associated with the user or may not be associated with the user.
- Types of publicly available information that can be utilized as the external data 360 include, but are not limited to, information obtained from a search engines, web pages, or other publicly accessible databases.
- One example of publicly available information is a repository of geographical feature information.
- Another such example of publicly available information is a repository of information describing events, including the place of the event and the time at which it was held, which can be accessed from or compiled from a publicly available data source. Specific examples of information describing events include list of concerts held at a specific venue, a list of sporting events at a stadium, or a list of events at a convention center.
- An example of an information source that may be publicly available or private to the user is a repository of images that include indexed features, where the indexed features can be objects, landmarks, persons, or other features.
- Examples of information that are private to the user and accessed with user permission and, in some cases, login credentials include electronic mail messages associated with the user, calendar data associated with the user.
- the information from the data extraction component 310 is utilized to identify an external data source to provide the external data 360 . This is done based on the data type or characteristic type of the information provided by the data extraction component 310 .
- the information from the image is then utilized in conjunction with the external data 360 .
- a repository of geographic features is selected as the source of the external data 360 , and the geolocation information from the images is utilized to identify a geographic feature such as a city, state, country province, or landmark that is relevant to the geolocation information.
- the titling component could generate a title such as “Eiffel Tower” or “42 photos near the Eiffel Tower.”
- Titling groups of images using geographic features names is of particular relevance when the user is away from their home location (i.e. where the user lives or spends a significant amount of time).
- the titling component 330 can be configured such that it is more likely to select a location-based title when the user is away from their home location.
- the titling component 330 determines that a group of related images does not correspond to the user's home location, the titling component 330 can generate a title for the group of images that corresponds to a distinct geographical area that corresponds to the images in the group.
- the home location can be obtained from the user or can be obtained from the images 340 .
- the home location is a user-specified value that is provided by the user as an input to the image organizing system 300 .
- the home location is obtained from a data source associated with the user that indicates where the user is normally located, such as a social networking profile associated with the user or an email signature that contains an address.
- the plurality of images 340 can be clustered geographically and the image organizing system 300 can select the distinct geographic area of the largest image groups or the geographic area containing the largest number or image groups or the geographic area containing the most images as the user's home location.
- the image organizing system can identify the home location based on patterns over time, such as by selecting the user's home location as the distinct geographical area having the largest number of time-based clusters.
- the titling component 330 can be utilized as the basis for a single title.
- the extracted information also includes information describing faces recognized in the photographs via facial recognition technology
- photographs of the user's social networking contacts or other images associated with persons can be utilized as the external data 360
- text-based labels i.e. names of persons
- the resulting title generated by the titling component 330 might then become “at the Eiffel Tower with Chris and Samantha.”
- the titling component 330 can base the title in part on the common time period.
- the resulting title generated by the titling component 330 might then become “at the Eiffel Tower with Chris and Samantha in May 2014.”
- the titling component 330 can identify multiple types of information that each correspond to a descriptor that can be incorporated in a title. In some implementations, the titling component is able to determine which of these descriptors to incorporate in the title and how to incorporate them. As one example, the titling component can utilize a decision tree to determine titles. The decisions made in the decision tree can be based in part on absence or presence of particular types of descriptors, and the result of the decision tree is selection of a template that is populated with the descriptors to generate the title. In another example, machine learning can be utilized to select titles by generating a title selection model. In this example, when a title is generated, it is suggested to the user, and acceptance or rejection of the title by the user can be utilized as an additional training input for the title selection model.
- the image organizing system stores the group of related images in an image collection, such as the album 350 and assigns the title to the image collection.
- the image collection is stored in association with the title in a form that allows the user to identify the image collection and gain insight into its contents based on the title. For example, where images are stored in folder-based album, the title can be assigned as the name of the folder.
- FIG. 4 is a flowchart showing a first example of a process 400 for organizing images.
- the operations described in connection with the process 400 can be performed at one or more computers, such as at the one or more server computers 132 of the application hosting service 130 .
- an operation is described as being performed by one or more computers, it is completed when it is performed by one computer working alone, or by multiple computers working together.
- the operations described in connection with the process 400 can be embodied as a non-transitory computer readable storage medium including program instructions executable by one or more processors that, when executed, cause the one or more processors to perform the operations.
- the operations described in connection with the process 400 could be stored at the memory 220 of one of the server computers 132 and be executable by the CPU 210 thereof.
- a plurality of images is obtained.
- Obtaining images means that they become available for processing by a computing device such as the one or more server computers in any manner. Examples of obtaining images include accessing the images from memory, accessing the images from a storage device, receiving the images via network transmission, and accessing the images from a user account.
- the images obtained at operation 410 are related in some manner, such as by association with a single user, location in a common folder, or having been recorded by the same image capture device.
- Operation 420 includes obtaining geolocation information and time information for each of the images obtained at operation 410 . This can be performed as described with respect to the data extraction component 310 .
- Operation 430 includes identify a group of related images from the plurality of images obtained at operation 410 .
- the group of images identified at operation 430 can be a subset of the images obtained at operation 410 .
- the group of images that is identified at operation 430 is identified based on a common geographical area and a common time period based for the images, based on the geolocation information and the time information for each of the images. This can be performed in the manner described with respect to the grouping component 320 .
- the common time period and the common geographical area are utilized to identify an event that occurred within the common time period and within the common geographical area. This can be done by searching a repository of event information using the common time period and the common geographical area as inputs, as explained with respect to the titling component 330 .
- the repository of event information includes information obtained from a publicly available source.
- the repository of event information includes non-public information that is associated with a user.
- each of the plurality of images is associated with a user and the repository of event information includes calendar data associated with the user.
- each of the plurality of images is associated with a user and the repository of event information includes information obtained from electronic mail messages that are associated with the user.
- the images are stored in an image collection such as an album, and a title is assigned to the image collection.
- the title is based at least in part on a name of the event that was identified at operation 440 . This can be done in the manner described with respect to the titling component 330 .
- FIG. 5 is a flowchart showing a second example of a process 500 for organizing images.
- the operations described in connection with the process 500 can be performed at one or more computers, such as at the one or more server computers 132 of the application hosting service 130 . When an operation is described as being performed by one or more computers, it is completed when it is performed by one computer working alone, or by multiple computers working together.
- the operations described in connection with the process 500 can be embodied as a non-transitory computer readable storage medium including program instructions executable by one or more processors that, when executed, cause the one or more processors to perform the operations.
- the operations described in connection with the process 500 could be stored at the memory 220 of one of the server computers 132 and be executable by the CPU 210 thereof.
- a plurality of images is obtained, as described previously.
- a group of related images is identified in the manner described with respect to the grouping component 320 , where the group can be a subset of the images obtained at operation 510 .
- the group can be defined in part based on geolocation information and/or time information that are associated with the images.
- Operation 530 includes extracting visible features from at least some images from the plurality of images. This can be performed, for example, as described with respect to the data extraction component 310 .
- one or more text-based feature labels are identified for at least some of the extracted visible features by comparing the extracted visible features to a repository of images that include indexed features.
- Indexed features can be visible portions of an image that are associated with information, such as feature vectors, that make the features searchable.
- the one or more text-based feature labels can be obtained from the repository of images, by identifying an image in the repository that has both a matching indexed feature and annotation information that describes the matching feature.
- the annotation data is then used as the text-based feature label for the subject feature in the image being analyzed.
- the images are stored in an image collection such as an album, and a title is assigned to the image collection.
- the title is based in part of on the text-based feature labels for the images in the group. Where multiple feature labels are present, they can be ranked and the top-ranked label can be selected for inclusion in the title.
- semantic analysis can be utilized to identify a common subject matter descriptor based on the text based feature labels, and this descriptor can be incorporated in the title. This can be done in the manner described with respect to the titling component 330 .
- the title is also based in part on additional information, such as a name representing a common geographical area for the images or a name representing a common time period for the images.
- FIG. 6 is a flowchart showing a second example of a process 600 for organizing images.
- the operations described in connection with the process 600 can be performed at one or more computers, such as at the one or more server computers 132 of the application hosting service 130 . When an operation is described as being performed by one or more computers, it is completed when it is performed by one computer working alone, or by multiple computers working together.
- the operations described in connection with the process 600 can be embodied as a non-transitory computer readable storage medium including program instructions executable by one or more processors that, when executed, cause the one or more processors to perform the operations.
- the operations described in connection with the process 600 could be stored at the memory 220 of one of the server computers 132 and be executable by the CPU 210 thereof.
- Operation 610 a plurality of images is obtained, as described previously.
- Operation 620 includes obtaining geolocation information for each of the images obtained at operation 610 .
- a home location is identified for the user.
- the home location can be identified in the manners previously described.
- the home located is identified by receiving information from the user that specifies the home location.
- the home location is identified based on geolocation information by selecting the distinct geographical area of a largest image group from the plurality of image groups as the home location for the user.
- time information is obtained from the images and the home location is determined by identifying one or more time-based image clusters, and selecting the distinct geographical area of the image group having a largest number of time-based clusters as the home location for the user.
- a group of images that does not correspond to the home location for the user is selected. This can be a group of images that were all recorded at locations outside of the home location. Operation 640 can be performed by defining a plurality of image groups that each include a plurality of images that correspond a distinct geographical area, determining whether each of these groups is inside or outside the home location, and selecting one of the groups that falls outside of the home location.
- the images are stored in an image collection such as an album, and a title is assigned to the image collection.
- the title is based at least in part on a name of the distinct geographical area for the group of images that was identified at operation 640 . This can be done in the manner described with respect to the titling component 330 . Other descriptors can also be included in the title, as previously described.
- example or “exemplary” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “exemplary” is intended to present concepts in a concrete fashion.
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or”. That is, unless specified otherwise, or clear from context, “X includes A or B” is intended to mean any of the natural inclusive permutations.
- the implementations of the computer devices can be realized in hardware, software, or any combination thereof.
- the hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors or any other suitable circuit.
- IP intellectual property
- ASICs application-specific integrated circuits
- programmable logic arrays optical processors
- programmable logic controllers microcode, microcontrollers, servers, microprocessors, digital signal processors or any other suitable circuit.
- signal processor should be understood as encompassing any of the foregoing hardware, either singly or in combination.
- signals and “data” are used interchangeably. Further, portions of each of the clients and each of the servers described herein do not necessarily have to be implemented in the same manner.
- Operations that are described as being performed by a single processor, computer, or device can be distributed across a number of different processors, computers or devices. Similarly, operations that are described as being performed by different processors, computers, or devices can, in some cases, be performed by a single processor, computer or device.
- the systems described herein can be implemented using general purpose computers/processors with a computer program that, when executed, carries out any of the respective methods, algorithms and/or instructions described herein.
- special purpose computers/processors can be utilized which can contain specialized hardware for carrying out any of the methods, algorithms, or instructions described herein.
- At least one implementation of this disclosure relates to an apparatus for performing the operations herein.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable storage medium that can be accessed by the computer.
- All or a portion of the embodiments of the disclosure can take the form of a computer program product accessible from, for example, a non-transitory computer-usable or computer-readable medium.
- the computer program when executed, can carry out any of the respective techniques, algorithms and/or instructions described herein.
- a non-transitory computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor.
- the non-transitory medium can be, for example, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for tangibly containing, storing, communicating, or transporting electronic instructions.
- any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for tangibly containing, storing, communicating, or transporting electronic instructions.
Abstract
Description
- Digital photographs are commonly stored on a computer, portable computing device, camera, or other computing device in a manner that allows them to be reviewed by a user at a later time. It is not common, however, for digital photographs to be associated with rich identifying information or contextual information. Instead, digital photographs often have very little useful information in their name or in associated metadata to identify them, unless a user takes affirmative steps to rename the photographs and/or add metadata to the photographs (e.g. “tagging”).
- One type of information that is usually available is the date and time on which a digital photograph was taken. Date and time information is useful, but may not be sufficient to allow users to find a particular photograph among a large number of photographs or to find a photograph taken long ago.
- Users sometimes manually organize photographs into albums. Commonly, albums are named after events, places, people, dates or a combination of the above. Manually organizing photographs and naming albums can be time consuming.
- The disclosure relates generally to automatically organizing images.
- One aspect of the disclosed embodiments is a method that includes obtaining a plurality of images, obtaining geolocation information for each image, and obtaining time information for each image. The method also includes identifying, by one or more computing devices, a group of related images from the plurality of images from a common geographical area and a common time period based on the geolocation information and the time information for each of the plurality of images, and identifying, by the one or more computing devices, an event that occurred within the common time period and within the common geographical area by searching a repository of event information. The method also includes storing, by the one or more computing devices, the group of related images in an image collection, and assigning, by the one or more computing devices, a title to the image collection, wherein the title is based at least in part on a title of the event.
- Another aspect of the disclosed embodiments is a method that includes obtaining a plurality of images, and identifying a group of related images from the plurality of images. The method also includes extracting visible features from at least some images from the plurality of images and identifying, by one or more computing devices, one or more text-based feature labels for at least some of the extracted visible features by comparing the extracted visible features to a repository of images that include indexed features. The method also includes storing, by the one or more computing devices, the group of related images in an image collection and assigning, by the one or more computing devices, a title to the image collection, wherein the title is based at least in part on the one or more text-based feature labels.
- Another aspect of the disclosed embodiments is a method that includes obtaining a plurality of images that are associated with a user and obtaining geolocation information for each of the images. The method also includes defining, by one or more computing devices and based on the geolocation information, a plurality of image groups that each include a plurality of images that correspond a distinct geographical area. The method also includes identifying, by the one or more computing devices, a home location for the user and identifying, by the one or more computing devices, a first image group from the plurality of image groups that does not correspond to the home location for the user. The method also includes storing, by the one or more computing devices, the images from the first image group in an image collection and assigning, by the one or more computing devices, a title to the image collection, wherein the title is based at least in part on the distinct geographical area for the first image group.
- The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views, and wherein:
-
FIG. 1 is a block diagram showing an example of an environment in which a system for automatically organizing images can be implemented; -
FIG. 2 is a block diagram showing an example of a hardware configuration for a server computer; -
FIG. 3 is a block diagram showing operation of an image organizing system; -
FIG. 4 is a flowchart showing a first example of a process for automatically organizing images; -
FIG. 5 is a flowchart showing a second example of a process for automatically organizing images; and -
FIG. 6 is a flowchart showing a third example of a process for automatically organizing images. - According to the methods, systems, apparatuses, and computer programs that are discussed herein digital images such as digital photographs can be automatically organized by grouping the digital images and placing the digital images into albums that are titled based on information associated with the digital images. Thus, a plurality of images can be
-
FIG. 1 shows an example of anenvironment 100 in which a system for automatically organizing images can be implemented. Theenvironment 100 can include auser system 110, one or moreadditional user systems 120, and anapplication hosting service 130. Theuser system 110 and theadditional user systems 120 are each representative of a large number (e.g. millions) of systems that can be included in theenvironment 100, with each system being able to utilize one or more applications that are provided by theapplication hosting service 130. Theuser system 110 and theadditional user systems 120 can each be any manner of computer or computing device, such as a desktop computer, a laptop computer, a tablet computer, or a smart-phone (a computationally-enabled mobile telephone). Theapplication hosting service 130 can be implemented using one ormore server computers 132. Theuser system 110, theadditional user systems 120, and theapplication hosting service 130 can each be implemented as a single system, multiple systems, distributed systems, or in any other form. - The systems, services, servers, and other computing devices described here are in communication via a
network 150. Thenetwork 150 can be one or more communications networks of any suitable type in any combination, including wireless networks, wired networks, local area networks, wide area networks, cellular data networks, and the internet. - The
application hosting service 130 can provide access to one or more hosted applications to a defined group of users including operators associated with theuser system 110 and theadditional user systems 120. One or more of the hosted applications can be a storage system that is operable to implement storage and retrieval functions and output, for display to a user, a user interface that allows the user to store, browse, organize, retrieve, view, delete, and/or perform other operations with respect to digital images that are stored as data files at theapplication hosting service 130. The digital images can be arranged by a hierarchical manner, such as a folder structure. The application hosting service can allow access to store, edit, delete, and/or view the digital images by a single user, by a group of designated users, or by all users. The user interface for the storage system can be output by theapplication hosting service 130 for display at a device associated with the user, such as theuser system 110, by transmission of signals and/or data from the application hosting service to theuser system 110 that, when interpreted by theuser system 110, cause display of the interface at theuser system 110. -
FIG. 2 is a block diagram of an example of a hardware configuration for the one ormore server computers 132 ofFIG. 1 . The same hardware configuration or a similar hardware configuration can be used to implement theuser system 110 and theadditional user systems 120. Eachserver computer 132 can include aCPU 210. TheCPU 210 can be a conventional central processing unit. Alternatively, theCPU 210 can be any other type of device, or multiple devices, capable of manipulating or processing information now-existing or hereafter developed. Although the disclosed examples can be practiced with a single processor as shown, e.g. CPU 210, advantages in speed and efficiency can be achieved using more than one processor. - Each
server computer 132 can includememory 220, such as a random access memory device (RAM). Any other suitable type of storage device can also be used as thememory 220. Thememory 220 can include code anddata 222 that can be accessed by theCPU 210 using abus 230. Thememory 220 can further include one ormore application programs 224 and anoperating system 226. Theapplication programs 224 can include software components in the form of computer executable program instructions that cause theCPU 210 to perform the operations and methods described here. - A
storage device 240 can be optionally provided in the form of any suitable computer readable medium, such as a hard disc drive, a memory device, a flash drive, or an optical drive. One ormore input devices 250, such as a keyboard, a mouse, or a gesture sensitive input device, receive user inputs and can output signals or data indicative of the user inputs to theCPU 210. One or more output devices can be provided, such as adisplay device 260. Thedisplay device 260, such as a liquid crystal display (LCD) or a cathode-ray tube (CRT), allows output to be presented to a user, for example, in response to receiving a video signal. - Although
FIG. 2 depicts theCPU 210 and thememory 220 of eachserver computer 132 as being integrated into a single unit, other configurations can be utilized. The operations of theCPU 210 can be distributed across multiple machines (each machine having one or more of processors) which can be coupled directly or across a local area or other network. Thememory 220 can be distributed across multiple machines such as network-based memory or memory in multiple machines. Although depicted here as a single bus, thebus 230 of each of eachserver computer 132 can be composed of multiple buses. Further, thestorage device 240 can be directly coupled to the other components of therespective server computer 132 or can be accessed via a network and can comprise a single integrated unit such as a memory card or multiple units such as multiple memory cards. The one or more server computers can thus be implemented in a wide variety of configurations. -
FIG. 3 is a block diagram showing animage organizing system 300. Theimage organizing system 300 includes adata extraction component 310, agrouping component 320, and atitling component 330. - The
image organizing system 300 receives a plurality of images 340 as an input and generates an album 350 (i.e. a digital photo album) as an output. The images 340 can be any group or collection of images, and can be unorganized images that are not associated with user-defined tags, labels, categories, or collections. In some implementations, the images 340 are images stored at a server-based storage system and are associated with a user-account at the server-based storage system. In some implementations, the images 340 are received from a single image capture device, such as a digital camera or a smart phone with an integrated digital camera. The images 340 can be in any suitable digital image format, with the JPEG format being a typical example. - The images 340 can each be associated with metadata that describes some characteristic associated with the image. One example of metadata that can be associated with any or all of the images 340 is time information that describes when the image was recorded (e.g. by a digital camera). Time information can be in the form of a time stamp including a value or values representing the date and/or time at which the image was recorded. Another example of metadata that can be associated with any or all of the images 340 is geolocation information (e.g. coordinates) that describes the location at which the device that recorded the image was situated at the time when the image was recorded.
- The
image organizing system 300 can be implemented using any computing device that is operable to receive the images 340, execute instructions operable to perform the functions associated with theimage organizing system 300, and output thealbum 350. As one example, theimage organizing system 300 can be implemented as a hosted application at theapplication hosting service 130, the images 340 can be received from the storage system that is implemented by theapplication hosting service 130, and thealbum 350 generated in this example will be accessible using the storage system of theapplication hosting service 130 via an interface that is displayed by a remote computing device through a web browser application or a dedicated application. As another example, theimage organizing system 300 can be implemented as a local application at theuser system 110, the images 340 can be received from a local file system of theuser system 110, and thealbum 350 can be generated as, for example, a folder containing a subset of the images 340. - The
data extraction component 310 is operable to obtain information for each of the images 340. Obtaining information for an image can include accessing information stored within the image itself, accessing information stored in association with the image, or using the information stored within or in association with the image to access external information that is relevant to the image but is not associated with the image. - Information contained in the image itself includes information describing one or more visible features of the image as represented by the pixel values of the image. For example, the images 340 can be processed to extract visible features from them. Visible features can be extracted from the images 340 in any suitable manner. The visible features that are extracted from the images 340 can then be compared with known features, as will be discussed further herein. Methods of extracting visible features and identifying them by comparing them to known features are well known in the art. For example, methods of object recognition in images are described in David G. Lowe, “Object recognition from local scale-invariant features,” International Conference on Computer Vision, Corfu, Greece (September 1999), pp. 1150-1157.
- Information associated with the image includes metadata that is written by the image capture device (e.g. digital camera) that recorded the image and metadata that is added to the image after it is recorded, such as by manual entry of the metadata by a user. Metadata can include many types of information, including geolocation information and time information, as previously noted.
- The information extracted from the images 340 by the
data extraction component 310 is passed to thegrouping component 320. Thegrouping component 320 is operable to identify a group of related images from the plurality of images 340. Thegrouping component 320 can group images based on a single characteristic or based on a combination of multiple characteristics. - As one example, images can be grouped by the
grouping component 320 based on geolocation information. In one example, a first image is selected, and other images that were recorded at locations within a determined distance are grouped with the first image. In another example, the locations at which images were recorded are determined, and a clustering algorithm can be utilized to define clusters of images, with each cluster including images that were recorded within a certain geographic proximity of one another, as determined by the clustering algorithm. Clustering algorithms are well-known, with a k-means clustering algorithm being an example of a suitable clustering algorithm. A relatedness metric that represents a probability that the images in the group are actually related (i.e. grouped correctly) can be generated for each group by the clustering algorithm. - As another example, images can be grouped by the
grouping component 320 based on time information. In one example, images within a predetermined time window can be considered related. In another example, a clustering algorithm can be used to generate time based clustering algorithms, and a relatedness metric can be generated as previously described. - As another example, images can be grouped by grouping
component 320 based on visible features in the images. Visible features are extracted as previously described, and images are grouped based on the presence of common or similar visible features. For example, if a particular object appears in multiple images, a group can be defined that includes those images. - As another example, images can be grouped by grouping
component 320 based on visible features in the images. Visible features are extracted as previously described, and images are grouped based on the presence of common or similar visible features. - As another example, images can be grouped by grouping
component 320 based on inclusion of related subject matter. In this example, visible features are extracted from each of the images by thedata extraction component 310, and those features are identified using external information such as a repository of feature information, with visible features in the repository being associated with annotations in the form of text-based labels that are associated with individual features. Those text-based labels are associated with the images, and the grouping component then groups the images based on semantic relatedness. For example, among pictures taken at a birthday party, the images might include features such as presents, cake, and decorations, these features can be identified as semantically related based on their common association with the concept of a party, and this can be used by the grouping component as a basis for defining a group of related images. - As previously mentioned, multiple characteristics can be combined by the grouping component to identify a group of related images. In one implementation, groups of possibly related images can be defined, and the relatedness metric is calculated for each of multiple characteristics. As an example, geographic clustering could be biased using time information, such that larger and/or less-well defined geographic clusters would be identified as related images if those images were captured within a related time period. In a scenario where a user is on a vacation at a location that is far from their home, the result may be a plurality of images that form a loose geographical cluster, as the user travels within a region and visits multiple areas over the course of several days. Although loosely defined, this cluster would be distinct from those clusters around the user's home, and since the images are clustered time-wise over the span of a few days, these two circumstances can, in combination, be sufficient for the
grouping component 320 to define a group that includes those images. - The foregoing are examples of characteristics and combinations of that can be utilized by the
grouping component 320 as a basis for identifying a group of related images. Any other characteristic obtained from information in the image, information associated with the image, or identified based on information in or associated with the image can be utilized as a basis for identifying groups of related images. In addition, any combination of these characteristics can be utilized as a basis for identifying groups of related images. - Information identifying the group of related images that was identified by the
grouping component 320 is passed to thetitling component 330. This information can include, for example, a list of the images. Thetitling component 330 is operable to assign a title to the group of related images, and this title is utilized as the title of thealbum 350 that is generated by theimage organizing system 300. - The
titling component 330 is operable to determine the title for the album based at least in part on the information extracted from images by thedata extraction component 310. Any of the previously discussed characteristics can be utilized as a basis for determining the title, as well as any other characteristic based on the extracted information. Thetitling component 330 is operable to access theexternal data 360, and to utilize theexternal data 360 to generate the title for thealbum 350. - Any type of relevant information can be utilized as the
external data 360. Information utilized by the titling component may be associated with the user or may not be associated with the user. Types of publicly available information that can be utilized as theexternal data 360 include, but are not limited to, information obtained from a search engines, web pages, or other publicly accessible databases. One example of publicly available information is a repository of geographical feature information. Another such example of publicly available information is a repository of information describing events, including the place of the event and the time at which it was held, which can be accessed from or compiled from a publicly available data source. Specific examples of information describing events include list of concerts held at a specific venue, a list of sporting events at a stadium, or a list of events at a convention center. An example of an information source that may be publicly available or private to the user (e.g. accessed by theimage organizing system 300 via permission from the user) is a repository of images that include indexed features, where the indexed features can be objects, landmarks, persons, or other features. Examples of information that are private to the user and accessed with user permission and, in some cases, login credentials, include electronic mail messages associated with the user, calendar data associated with the user. - In particular, the information from the
data extraction component 310 is utilized to identify an external data source to provide theexternal data 360. This is done based on the data type or characteristic type of the information provided by thedata extraction component 310. The information from the image is then utilized in conjunction with theexternal data 360. In a simple example, where a group of related images is defined based on geolocation information, a repository of geographic features is selected as the source of theexternal data 360, and the geolocation information from the images is utilized to identify a geographic feature such as a city, state, country province, or landmark that is relevant to the geolocation information. For example, based on the geolocation information and theexternal data 360 it is determined that the images in the group are taken in the vicinity of the Eiffel Tower, the titling component could generate a title such as “Eiffel Tower” or “42 photos near the Eiffel Tower.” - Titling groups of images using geographic features names is of particular relevance when the user is away from their home location (i.e. where the user lives or spends a significant amount of time). Thus, the
titling component 330 can be configured such that it is more likely to select a location-based title when the user is away from their home location. Thus, when thetitling component 330 determines that a group of related images does not correspond to the user's home location, thetitling component 330 can generate a title for the group of images that corresponds to a distinct geographical area that corresponds to the images in the group. - The home location can be obtained from the user or can be obtained from the images 340. In one example, the home location is a user-specified value that is provided by the user as an input to the
image organizing system 300. In another example, the home location is obtained from a data source associated with the user that indicates where the user is normally located, such as a social networking profile associated with the user or an email signature that contains an address. In another example, the plurality of images 340 can be clustered geographically and theimage organizing system 300 can select the distinct geographic area of the largest image groups or the geographic area containing the largest number or image groups or the geographic area containing the most images as the user's home location. As another example, using time information for each of the images 340, the image organizing system can identify the home location based on patterns over time, such as by selecting the user's home location as the distinct geographical area having the largest number of time-based clusters. - Multiple characteristics can be utilized by the
titling component 330 as the basis for a single title. Continuing the example above where a group of photographs are taken near the Eiffel tower, if the extracted information also includes information describing faces recognized in the photographs via facial recognition technology, photographs of the user's social networking contacts or other images associated with persons can be utilized as theexternal data 360, and text-based labels (i.e. names of persons) can be assigned to the recognized faces by matching them to theexternal data 360. The resulting title generated by thetitling component 330 might then become “at the Eiffel Tower with Chris and Samantha.” As a further example, when the images in the group are associated with a common time period, thetitling component 330 can base the title in part on the common time period. The resulting title generated by thetitling component 330 might then become “at the Eiffel Tower with Chris and Samantha in May 2014.” - The
titling component 330 can identify multiple types of information that each correspond to a descriptor that can be incorporated in a title. In some implementations, the titling component is able to determine which of these descriptors to incorporate in the title and how to incorporate them. As one example, the titling component can utilize a decision tree to determine titles. The decisions made in the decision tree can be based in part on absence or presence of particular types of descriptors, and the result of the decision tree is selection of a template that is populated with the descriptors to generate the title. In another example, machine learning can be utilized to select titles by generating a title selection model. In this example, when a title is generated, it is suggested to the user, and acceptance or rejection of the title by the user can be utilized as an additional training input for the title selection model. - After the title is determined by the titling component, the image organizing system stores the group of related images in an image collection, such as the
album 350 and assigns the title to the image collection. The image collection is stored in association with the title in a form that allows the user to identify the image collection and gain insight into its contents based on the title. For example, where images are stored in folder-based album, the title can be assigned as the name of the folder. -
FIG. 4 is a flowchart showing a first example of aprocess 400 for organizing images. The operations described in connection with theprocess 400 can be performed at one or more computers, such as at the one ormore server computers 132 of theapplication hosting service 130. When an operation is described as being performed by one or more computers, it is completed when it is performed by one computer working alone, or by multiple computers working together. The operations described in connection with theprocess 400 can be embodied as a non-transitory computer readable storage medium including program instructions executable by one or more processors that, when executed, cause the one or more processors to perform the operations. For example, the operations described in connection with theprocess 400 could be stored at thememory 220 of one of theserver computers 132 and be executable by theCPU 210 thereof. - In
operation 410, a plurality of images is obtained. Obtaining images means that they become available for processing by a computing device such as the one or more server computers in any manner. Examples of obtaining images include accessing the images from memory, accessing the images from a storage device, receiving the images via network transmission, and accessing the images from a user account. In some implementations, the images obtained atoperation 410 are related in some manner, such as by association with a single user, location in a common folder, or having been recorded by the same image capture device. -
Operation 420 includes obtaining geolocation information and time information for each of the images obtained atoperation 410. This can be performed as described with respect to thedata extraction component 310. -
Operation 430 includes identify a group of related images from the plurality of images obtained atoperation 410. The group of images identified atoperation 430 can be a subset of the images obtained atoperation 410. In one implementation, the group of images that is identified atoperation 430 is identified based on a common geographical area and a common time period based for the images, based on the geolocation information and the time information for each of the images. This can be performed in the manner described with respect to thegrouping component 320. - In
operation 440, the common time period and the common geographical area are utilized to identify an event that occurred within the common time period and within the common geographical area. This can be done by searching a repository of event information using the common time period and the common geographical area as inputs, as explained with respect to thetitling component 330. In some implementations, the repository of event information includes information obtained from a publicly available source. In some implementations, the repository of event information includes non-public information that is associated with a user. In some implementations, each of the plurality of images is associated with a user and the repository of event information includes calendar data associated with the user. In some implementations, each of the plurality of images is associated with a user and the repository of event information includes information obtained from electronic mail messages that are associated with the user. - In
operation 450, the images are stored in an image collection such as an album, and a title is assigned to the image collection. The title is based at least in part on a name of the event that was identified atoperation 440. This can be done in the manner described with respect to thetitling component 330. -
FIG. 5 is a flowchart showing a second example of aprocess 500 for organizing images. The operations described in connection with theprocess 500 can be performed at one or more computers, such as at the one ormore server computers 132 of theapplication hosting service 130. When an operation is described as being performed by one or more computers, it is completed when it is performed by one computer working alone, or by multiple computers working together. The operations described in connection with theprocess 500 can be embodied as a non-transitory computer readable storage medium including program instructions executable by one or more processors that, when executed, cause the one or more processors to perform the operations. For example, the operations described in connection with theprocess 500 could be stored at thememory 220 of one of theserver computers 132 and be executable by theCPU 210 thereof. - In
operation 510, a plurality of images is obtained, as described previously. In operation 520 a group of related images is identified in the manner described with respect to thegrouping component 320, where the group can be a subset of the images obtained atoperation 510. The group can be defined in part based on geolocation information and/or time information that are associated with the images. -
Operation 530 includes extracting visible features from at least some images from the plurality of images. This can be performed, for example, as described with respect to thedata extraction component 310. - In
operation 540 one or more text-based feature labels are identified for at least some of the extracted visible features by comparing the extracted visible features to a repository of images that include indexed features. 9. Indexed features can be visible portions of an image that are associated with information, such as feature vectors, that make the features searchable. The one or more text-based feature labels can be obtained from the repository of images, by identifying an image in the repository that has both a matching indexed feature and annotation information that describes the matching feature. The annotation data is then used as the text-based feature label for the subject feature in the image being analyzed. - In
operation 550, the images are stored in an image collection such as an album, and a title is assigned to the image collection. The title is based in part of on the text-based feature labels for the images in the group. Where multiple feature labels are present, they can be ranked and the top-ranked label can be selected for inclusion in the title. Alternatively, semantic analysis can be utilized to identify a common subject matter descriptor based on the text based feature labels, and this descriptor can be incorporated in the title. This can be done in the manner described with respect to thetitling component 330. In some implementations the title is also based in part on additional information, such as a name representing a common geographical area for the images or a name representing a common time period for the images. -
FIG. 6 is a flowchart showing a second example of aprocess 600 for organizing images. The operations described in connection with theprocess 600 can be performed at one or more computers, such as at the one ormore server computers 132 of theapplication hosting service 130. When an operation is described as being performed by one or more computers, it is completed when it is performed by one computer working alone, or by multiple computers working together. The operations described in connection with theprocess 600 can be embodied as a non-transitory computer readable storage medium including program instructions executable by one or more processors that, when executed, cause the one or more processors to perform the operations. For example, the operations described in connection with theprocess 600 could be stored at thememory 220 of one of theserver computers 132 and be executable by theCPU 210 thereof. - In
operation 610, a plurality of images is obtained, as described previously.Operation 620 includes obtaining geolocation information for each of the images obtained atoperation 610. - In
operation 630, a home location is identified for the user. The home location can be identified in the manners previously described. In one implementation, the home located is identified by receiving information from the user that specifies the home location. In another implementation the home location is identified based on geolocation information by selecting the distinct geographical area of a largest image group from the plurality of image groups as the home location for the user. In another implementation, time information is obtained from the images and the home location is determined by identifying one or more time-based image clusters, and selecting the distinct geographical area of the image group having a largest number of time-based clusters as the home location for the user. - In
operation 640, a group of images that does not correspond to the home location for the user is selected. This can be a group of images that were all recorded at locations outside of the home location.Operation 640 can be performed by defining a plurality of image groups that each include a plurality of images that correspond a distinct geographical area, determining whether each of these groups is inside or outside the home location, and selecting one of the groups that falls outside of the home location. - In
operation 650, the images are stored in an image collection such as an album, and a title is assigned to the image collection. The title is based at least in part on a name of the distinct geographical area for the group of images that was identified atoperation 640. This can be done in the manner described with respect to thetitling component 330. Other descriptors can also be included in the title, as previously described. - The foregoing description describes only some exemplary implementations of the described techniques. Other implementations are available. For example, the particular naming of the components, capitalization of terms, the attributes, data structures, or any other programming or structural aspect is not mandatory or significant, and the mechanisms that implement the invention or its features may have different names, formats, or protocols. Further, the system may be implemented via a combination of hardware and software, as described, or entirely in hardware elements. Also, the particular division of functionality between the various system components described herein is merely exemplary, and not mandatory; functions performed by a single system component may instead be performed by multiple components, and functions performed by multiple components may instead performed by a single component.
- The words “example” or “exemplary” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “exemplary” is intended to present concepts in a concrete fashion. As used in this application, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or”. That is, unless specified otherwise, or clear from context, “X includes A or B” is intended to mean any of the natural inclusive permutations. That is, if X includes A; X includes B; or X includes both A and B, then “X includes A or B” is satisfied under any of the foregoing instances. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more” unless specified otherwise or clear from context to be directed to a singular form. Moreover, use of the term “an embodiment” or “one embodiment” or “an implementation” or “one implementation” throughout is not intended to mean the same embodiment or implementation unless described as such.
- The implementations of the computer devices (e.g., clients and servers) described herein can be realized in hardware, software, or any combination thereof. The hardware can include, for example, computers, intellectual property (IP) cores, application-specific integrated circuits (ASICs), programmable logic arrays, optical processors, programmable logic controllers, microcode, microcontrollers, servers, microprocessors, digital signal processors or any other suitable circuit. In the claims, the term “processor” should be understood as encompassing any of the foregoing hardware, either singly or in combination. The terms “signal” and “data” are used interchangeably. Further, portions of each of the clients and each of the servers described herein do not necessarily have to be implemented in the same manner.
- Operations that are described as being performed by a single processor, computer, or device can be distributed across a number of different processors, computers or devices. Similarly, operations that are described as being performed by different processors, computers, or devices can, in some cases, be performed by a single processor, computer or device.
- Although features may be described above or claimed as acting in certain combinations, one or more features of a combination can in some cases be excised from the combination, and the combination may be directed to a sub-combination or variation of a sub-combination.
- The systems described herein, such as client computers and server computers, can be implemented using general purpose computers/processors with a computer program that, when executed, carries out any of the respective methods, algorithms and/or instructions described herein. In addition or alternatively, for example, special purpose computers/processors can be utilized which can contain specialized hardware for carrying out any of the methods, algorithms, or instructions described herein.
- Some portions of above description include disclosure presented in terms of algorithms and symbolic representations of operations on information. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. These operations, while described functionally or logically, are understood to be implemented by computer programs. Furthermore, it has also proven convenient at times, to refer to these arrangements of operations as modules or by functional names, without loss of generality. It should be noted that the process steps and instructions of implementations of this disclosure could be embodied in software, firmware or hardware, and when embodied in software, could be downloaded to reside on and be operated from different platforms used by real time network operating systems.
- Unless specifically stated otherwise as apparent from the above discussion, it is appreciated that throughout the description, discussions utilizing terms such as “processing” or “computing” or “calculating” or “determining” or “displaying” or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system memories or registers or other such information storage, transmission or display devices.
- At least one implementation of this disclosure relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable storage medium that can be accessed by the computer.
- All or a portion of the embodiments of the disclosure can take the form of a computer program product accessible from, for example, a non-transitory computer-usable or computer-readable medium. The computer program, when executed, can carry out any of the respective techniques, algorithms and/or instructions described herein. A non-transitory computer-usable or computer-readable medium can be any device that can, for example, tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The non-transitory medium can be, for example, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for tangibly containing, storing, communicating, or transporting electronic instructions.
- It is to be understood that the disclosure is not to be limited to the disclosed embodiments but, on the contrary, is intended to cover various modifications and equivalent arrangements included within the spirit and scope of the appended claims.
Claims (20)
Priority Applications (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/303,997 US10078781B2 (en) | 2014-06-13 | 2014-06-13 | Automatically organizing images |
DE202015009255.1U DE202015009255U1 (en) | 2014-06-13 | 2015-03-13 | Automatic image organization |
PCT/US2015/020378 WO2015191132A1 (en) | 2014-06-13 | 2015-03-13 | Automatically organizing images |
US16/114,310 US20180365489A1 (en) | 2014-06-13 | 2018-08-28 | Automatically organizing images |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/303,997 US10078781B2 (en) | 2014-06-13 | 2014-06-13 | Automatically organizing images |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/114,310 Continuation US20180365489A1 (en) | 2014-06-13 | 2018-08-28 | Automatically organizing images |
Publications (2)
Publication Number | Publication Date |
---|---|
US20150363640A1 true US20150363640A1 (en) | 2015-12-17 |
US10078781B2 US10078781B2 (en) | 2018-09-18 |
Family
ID=52774594
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/303,997 Active US10078781B2 (en) | 2014-06-13 | 2014-06-13 | Automatically organizing images |
US16/114,310 Abandoned US20180365489A1 (en) | 2014-06-13 | 2018-08-28 | Automatically organizing images |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/114,310 Abandoned US20180365489A1 (en) | 2014-06-13 | 2018-08-28 | Automatically organizing images |
Country Status (3)
Country | Link |
---|---|
US (2) | US10078781B2 (en) |
DE (1) | DE202015009255U1 (en) |
WO (1) | WO2015191132A1 (en) |
Cited By (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9509772B1 (en) | 2014-02-13 | 2016-11-29 | Google Inc. | Visualization and control of ongoing ingress actions |
US9507791B2 (en) | 2014-06-12 | 2016-11-29 | Google Inc. | Storage system user interface with floating file collection |
US9531722B1 (en) | 2013-10-31 | 2016-12-27 | Google Inc. | Methods for generating an activity stream |
US9536199B1 (en) | 2014-06-09 | 2017-01-03 | Google Inc. | Recommendations based on device usage |
US9614880B1 (en) | 2013-11-12 | 2017-04-04 | Google Inc. | Methods for real-time notifications in an activity stream |
US20170098124A1 (en) * | 2015-10-06 | 2017-04-06 | Yahoo!, Inc | User classification based upon images |
US20170351703A1 (en) * | 2016-06-01 | 2017-12-07 | Facebook, Inc. | Grouping Content Based on Geographic Data |
US9870420B2 (en) | 2015-01-19 | 2018-01-16 | Google Llc | Classification and storage of documents |
WO2018187131A1 (en) * | 2017-04-07 | 2018-10-11 | Microsoft Technology Licensing, Llc | Automatic narrative creation for captured content |
US20190095067A1 (en) * | 2016-07-13 | 2019-03-28 | Tencent Technology (Shenzhen) Company Limited | Method and apparatus for uploading photographed file |
US20190171170A1 (en) * | 2017-12-06 | 2019-06-06 | Arris Enterprises Llc | System and method of iot device control using augmented reality |
US10331705B2 (en) * | 2015-06-07 | 2019-06-25 | Apple Inc. | Moments naming based on location accuracy |
US20200151453A1 (en) * | 2018-11-08 | 2020-05-14 | International Business Machines Corporation | Reducing overlap among a collection of photographs |
CN112069342A (en) * | 2020-09-03 | 2020-12-11 | Oppo广东移动通信有限公司 | Image classification method and device, electronic equipment and storage medium |
CN112535408A (en) * | 2019-09-23 | 2021-03-23 | 佛山市顺德区美的电热电器制造有限公司 | Auxiliary cooking method, device and computer storage medium |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11797599B2 (en) * | 2013-10-10 | 2023-10-24 | Aura Home, Inc. | Trend detection in digital photo collections for digital picture frames |
WO2018151760A1 (en) | 2017-02-20 | 2018-08-23 | 3M Innovative Properties Company | Optical articles and systems interacting with the same |
EP3688662A1 (en) | 2017-09-27 | 2020-08-05 | 3M Innovative Properties Company | Personal protective equipment management system using optical patterns for equipment and safety monitoring |
WO2022240443A1 (en) * | 2021-05-11 | 2022-11-17 | Google Llc | Automatic generation of events using a machine-learning model |
US11748561B1 (en) * | 2022-03-15 | 2023-09-05 | My Job Matcher, Inc. | Apparatus and methods for employment application assessment |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040085578A1 (en) * | 2002-11-03 | 2004-05-06 | Quek Su Mien | Producing personalized photo calendar |
US20040201740A1 (en) * | 2002-03-15 | 2004-10-14 | Canon Kabushiki Kaisha | Automatic determination of image storage location |
US20130156275A1 (en) * | 2011-12-20 | 2013-06-20 | Matthew W. Amacker | Techniques for grouping images |
US20130339180A1 (en) * | 2012-03-16 | 2013-12-19 | Ronald Aaron LaPierre | Collection creator and organizer for social media |
Family Cites Families (151)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CA1327407C (en) | 1988-11-29 | 1994-03-01 | Chander Kasiraj | Method for providing an object activity history |
US5579471A (en) * | 1992-11-09 | 1996-11-26 | International Business Machines Corporation | Image query system and method |
US5507845A (en) | 1994-10-11 | 1996-04-16 | Charles J. Molnar | Plant sod mats |
US6006239A (en) | 1996-03-15 | 1999-12-21 | Microsoft Corporation | Method and system for allowing multiple users to simultaneously edit a spreadsheet |
DE69719269T2 (en) | 1996-08-01 | 2003-10-30 | Ibm | Ensuring indivisibility for a collection of transactional work steps in a workflow management system |
US6453352B1 (en) | 1997-07-14 | 2002-09-17 | Electronic Data Systems Corporation | Integrated electronic commerce system and method |
US6119165A (en) | 1997-11-17 | 2000-09-12 | Trend Micro, Inc. | Controlled distribution of application programs in a computer network |
US6415373B1 (en) | 1997-12-24 | 2002-07-02 | Avid Technology, Inc. | Computer system and process for transferring multiple high bandwidth streams of data between multiple storage units and multiple applications in a scalable and reliable manner |
US8479122B2 (en) | 2004-07-30 | 2013-07-02 | Apple Inc. | Gestures for touch sensitive input devices |
US6314105B1 (en) | 1998-05-19 | 2001-11-06 | Cisco Technology, Inc. | Method and apparatus for creating and dismantling a transit path in a subnetwork |
FR2781582B1 (en) | 1998-07-21 | 2001-01-12 | Technical Maintenance Corp | SYSTEM FOR DOWNLOADING OBJECTS OR FILES FOR SOFTWARE UPDATE |
US20050131992A1 (en) | 2003-12-11 | 2005-06-16 | Eric Goldstein | System, method and apparatus for selecting, displaying, managing, tracking and transferring access to content of web pages and other sources |
US6317722B1 (en) | 1998-09-18 | 2001-11-13 | Amazon.Com, Inc. | Use of electronic shopping carts to generate personal recommendations |
AU2935800A (en) | 1999-02-11 | 2000-09-14 | Signifile B.V. | Electronic document storage and retrieval system and method |
US6614804B1 (en) | 1999-03-22 | 2003-09-02 | Webtv Networks, Inc. | Method and apparatus for remote update of clients by a server via broadcast satellite |
FI109319B (en) | 1999-12-03 | 2002-06-28 | Nokia Corp | Filtering of electronic information to be transmitted to a terminal |
ATE531162T1 (en) | 1999-12-23 | 2011-11-15 | Accenture Global Services Ltd | METHOD FOR CONTROLLING DATA COLLECTION, DATA MANIPULATION AND STORAGE IN A NETWORK WITH ASSURED SERVICE QUALITY |
US8701027B2 (en) | 2000-03-16 | 2014-04-15 | Microsoft Corporation | Scope user interface for displaying the priorities and properties of multiple informational items |
US6687735B1 (en) | 2000-05-30 | 2004-02-03 | Tranceive Technologies, Inc. | Method and apparatus for balancing distributed applications |
US7219302B1 (en) | 2000-07-19 | 2007-05-15 | Everez Systems Limited | System and method for organizing, managing, and manipulating desktop objects with an activity-oriented user interface |
US6711557B1 (en) | 2000-08-14 | 2004-03-23 | Adobe Systems Incorporated | Client-based background update monitoring |
US7047309B2 (en) | 2000-08-23 | 2006-05-16 | International Business Machines Corporation | Load balancing and dynamic control of multiple data streams in a network |
WO2003107219A1 (en) | 2000-09-11 | 2003-12-24 | Zambeel, Inc. | Storage system having partitioned migratable metadata |
US20020112116A1 (en) * | 2000-11-17 | 2002-08-15 | Nelson Mark Edward | Methods, systems, and computer program products for storing data in collections of tagged data pieces |
US6590568B1 (en) | 2000-11-20 | 2003-07-08 | Nokia Corporation | Touch screen drag and drop input technique |
US7660902B2 (en) | 2000-11-20 | 2010-02-09 | Rsa Security, Inc. | Dynamic file access control and management |
US7783972B2 (en) | 2001-01-08 | 2010-08-24 | Enfocus NV | Ensured workflow system and method for editing a consolidated file |
US7030861B1 (en) | 2001-02-10 | 2006-04-18 | Wayne Carl Westerman | System and method for packing multi-touch gestures onto a hand |
US20020167538A1 (en) | 2001-05-11 | 2002-11-14 | Bhetanabhotla Murthy N. | Flexible organization of information using multiple hierarchical categories |
US7945600B1 (en) | 2001-05-18 | 2011-05-17 | Stratify, Inc. | Techniques for organizing data to support efficient review and analysis |
US7711771B2 (en) | 2001-05-25 | 2010-05-04 | Oracle International Corporation | Management and synchronization application for network file system |
US20030208490A1 (en) | 2001-06-15 | 2003-11-06 | Jean-Jacques Larrea | System and method for data storage, control and access |
US8990712B2 (en) | 2011-08-24 | 2015-03-24 | Z124 | Unified desktop triad control user interface for file manager |
JP2003298991A (en) | 2002-03-29 | 2003-10-17 | Fuji Photo Film Co Ltd | Image arranging method and apparatus, and program |
WO2003090096A1 (en) | 2002-04-19 | 2003-10-30 | Toshiba Tec Kabushiki Kaisha | Document management system |
JP2004220569A (en) * | 2002-12-25 | 2004-08-05 | Casio Comput Co Ltd | Data file storage device, program and method |
JP2004213129A (en) | 2002-12-27 | 2004-07-29 | Fuji Photo Film Co Ltd | Method, device and program for classifying picture |
US7627552B2 (en) | 2003-03-27 | 2009-12-01 | Microsoft Corporation | System and method for filtering and organizing items based on common elements |
US7536386B2 (en) | 2003-03-27 | 2009-05-19 | Microsoft Corporation | System and method for sharing items in a computer system |
US20050033777A1 (en) | 2003-08-04 | 2005-02-10 | Moraes Mark A. | Tracking, recording and organizing changes to data in computer systems |
US7636733B1 (en) * | 2003-10-03 | 2009-12-22 | Adobe Systems Incorporated | Time-based image management |
US20080126476A1 (en) | 2004-08-04 | 2008-05-29 | Nicholas Frank C | Method and System for the Creating, Managing, and Delivery of Enhanced Feed Formatted Content |
US8504565B2 (en) | 2004-09-09 | 2013-08-06 | William M. Pitts | Full text search capabilities integrated into distributed file systems— incrementally indexing files |
US20060059174A1 (en) | 2004-09-16 | 2006-03-16 | International Business Machines Corporation | Apparatus, system, and method for locating an application compatible with a file |
US7630400B2 (en) | 2005-01-20 | 2009-12-08 | International Business Machines Corporation | Controlling software distribution or file transfer |
US7323999B2 (en) | 2005-03-11 | 2008-01-29 | International Business Machines Corporation | Automatic subscriptions to shared repositories with notifications reflecting accesses by important other users and keywords stored in a personal interest profile |
US20060229932A1 (en) | 2005-04-06 | 2006-10-12 | Johnson & Johnson Services, Inc. | Intelligent sales and marketing recommendation system |
US20140236722A1 (en) * | 2005-04-08 | 2014-08-21 | Marshall Feature Recognition Llc | System And Method For Accessing Electronic Data Via An Image Search Engine |
US7620902B2 (en) | 2005-04-20 | 2009-11-17 | Microsoft Corporation | Collaboration spaces |
US20070008321A1 (en) * | 2005-07-11 | 2007-01-11 | Eastman Kodak Company | Identifying collection images with special events |
EP1977348A4 (en) | 2006-01-27 | 2010-08-04 | El Fresko Technologies Ltd | Event structured file system (esfs) |
EP1847952A1 (en) | 2006-04-21 | 2007-10-24 | Yahoo!, Inc. | Searching method and apparatus |
US8270490B2 (en) | 2006-07-06 | 2012-09-18 | Canon Kabushiki Kaisha | Motion vector detection apparatus, motion vector detection method, image encoding apparatus, image encoding method, and computer program |
US7756821B2 (en) | 2006-11-02 | 2010-07-13 | Microsoft Corporation | Virtual deletion in merged file system directories |
US9665597B2 (en) * | 2006-12-05 | 2017-05-30 | Qualcomm Incorporated | Method and system for processing images using time and location filters |
US7956847B2 (en) | 2007-01-05 | 2011-06-07 | Apple Inc. | Gestures for controlling, manipulating, and editing of media files using touch sensitive devices |
US20080177623A1 (en) | 2007-01-24 | 2008-07-24 | Juergen Fritsch | Monitoring User Interactions With A Document Editing System |
WO2008091695A1 (en) | 2007-01-25 | 2008-07-31 | Samuel Pierce Baron | Virtual social interactions |
WO2008097810A2 (en) | 2007-02-02 | 2008-08-14 | Veoh Networks, Inc. | Indicator-based recommendation system |
US20080270398A1 (en) | 2007-04-30 | 2008-10-30 | Landau Matthew J | Product affinity engine and method |
US8522258B1 (en) * | 2007-06-15 | 2013-08-27 | At&T Mobility Ii Llc | Event handling system |
US7904303B2 (en) | 2007-08-24 | 2011-03-08 | Yahoo! Inc. | Engagement-oriented recommendation principle |
WO2009032712A2 (en) | 2007-08-29 | 2009-03-12 | Nirvanix, Inc. | Method and system for moving requested files from one storage location to another |
US8285700B2 (en) | 2007-09-07 | 2012-10-09 | Brand Affinity Technologies, Inc. | Apparatus, system and method for a brand affinity engine using positive and negative mentions and indexing |
US7827299B2 (en) | 2007-09-11 | 2010-11-02 | International Business Machines Corporation | Transitioning between historical and real time data streams in the processing of data change messages |
US9020913B2 (en) | 2007-10-25 | 2015-04-28 | International Business Machines Corporation | Real-time interactive authorization for enterprise search |
US8467955B2 (en) * | 2007-10-31 | 2013-06-18 | Microsoft Corporation | Map-centric service for social events |
US8645827B2 (en) | 2008-03-04 | 2014-02-04 | Apple Inc. | Touch event model |
US8676001B2 (en) | 2008-05-12 | 2014-03-18 | Google Inc. | Automatic discovery of popular landmarks |
US8418084B1 (en) | 2008-05-30 | 2013-04-09 | At&T Intellectual Property I, L.P. | Single-touch media selection |
US20090327975A1 (en) | 2008-06-27 | 2009-12-31 | Stedman Roy W | Multi-Touch Sorting Gesture |
KR100972104B1 (en) | 2008-06-27 | 2010-07-23 | 주식회사 켐트로닉스 | Method, apparatus for sensing moved touch and computer readable record-medium on which program for executing method thereof |
US8169414B2 (en) | 2008-07-12 | 2012-05-01 | Lim Seung E | Control of electronic games via finger angle using a high dimensional touchpad (HDTP) touch user interface |
US9251286B2 (en) | 2008-07-15 | 2016-02-02 | International Business Machines Corporation | Form attachment metadata generation |
US8520979B2 (en) * | 2008-08-19 | 2013-08-27 | Digimarc Corporation | Methods and systems for content processing |
US8644688B2 (en) | 2008-08-26 | 2014-02-04 | Opentv, Inc. | Community-based recommendation engine |
JP5329884B2 (en) | 2008-09-18 | 2013-10-30 | 株式会社東芝 | Portable electronic device and data processing method in portable electronic device |
JP5199003B2 (en) | 2008-09-25 | 2013-05-15 | 株式会社日立製作所 | Management device and computer system |
AU2009303824A1 (en) | 2008-10-14 | 2010-04-22 | Brand Affinity Technologies, Inc. | Apparatus, system and method for a brand affinity engine using positive and negative mentions and indexing |
US8624836B1 (en) | 2008-10-24 | 2014-01-07 | Google Inc. | Gesture-based small device input |
KR20100052676A (en) | 2008-11-11 | 2010-05-20 | 삼성전자주식회사 | Apparatus for albuming contents and method thereof |
US20100161441A1 (en) | 2008-12-24 | 2010-06-24 | Comcast Interactive Media, Llc | Method and apparatus for advertising at the sub-asset level |
US8298087B1 (en) | 2009-01-02 | 2012-10-30 | Nintendo Of America Inc. | Recommendation engine for electronic game shopping channel |
US8196047B2 (en) | 2009-01-20 | 2012-06-05 | Microsoft Corporation | Flexible visualization for services |
US9405752B2 (en) | 2009-02-13 | 2016-08-02 | T-Mobile Usa, Inc. | System and method for automatically presenting a media file on a mobile device based on relevance to a user |
US20100241971A1 (en) | 2009-03-20 | 2010-09-23 | Thomas Zuber | System and method for interactively collaborating within a secure online social networking community |
US9215423B2 (en) | 2009-03-30 | 2015-12-15 | Time Warner Cable Enterprises Llc | Recommendation engine apparatus and methods |
US10984397B2 (en) | 2009-03-31 | 2021-04-20 | Ebay Inc. | Application recommendation engine |
US20110044512A1 (en) | 2009-03-31 | 2011-02-24 | Myspace Inc. | Automatic Image Tagging |
WO2010114620A1 (en) * | 2009-04-03 | 2010-10-07 | Certusview Technologies, Llc | Methods, apparatus, and systems for documenting and reporting events via geo-referenced electronic drawings |
US8396287B2 (en) | 2009-05-15 | 2013-03-12 | Google Inc. | Landmarks from digital photo collections |
US8555173B2 (en) | 2009-05-31 | 2013-10-08 | Linkedin Corporation | Recommendation engine |
US8838530B2 (en) | 2009-06-03 | 2014-09-16 | The Information Company Private Limited | Method and system for directory management |
US8433993B2 (en) * | 2009-06-24 | 2013-04-30 | Yahoo! Inc. | Context aware image representation |
JP2012531674A (en) | 2009-06-26 | 2012-12-10 | シンプリヴィティ・コーポレーション | Scalable indexing in non-uniform access memory |
US20100332401A1 (en) | 2009-06-30 | 2010-12-30 | Anand Prahlad | Performing data storage operations with a cloud storage environment, including automatically selecting among multiple cloud storage sites |
US8407613B2 (en) | 2009-07-13 | 2013-03-26 | Apple Inc. | Directory management on a portable multifunction device |
US8670597B2 (en) | 2009-08-07 | 2014-03-11 | Google Inc. | Facial recognition with social network aiding |
WO2011031492A1 (en) | 2009-08-25 | 2011-03-17 | Google Inc. | Direct manipulation gestures |
US8571331B2 (en) | 2009-11-30 | 2013-10-29 | Xerox Corporation | Content based image selection for automatic photo album generation |
US8238671B1 (en) | 2009-12-07 | 2012-08-07 | Google Inc. | Scene classification for place recognition |
US8189964B2 (en) | 2009-12-07 | 2012-05-29 | Google Inc. | Matching an approximately located query image against a reference image set |
US8423046B2 (en) | 2010-02-22 | 2013-04-16 | Google Inc. | Network performance server |
KR20110128567A (en) | 2010-05-24 | 2011-11-30 | 삼성전자주식회사 | Method for controlling objects of user interface and apparatus of enabling the method |
US8323048B2 (en) | 2010-08-03 | 2012-12-04 | Ideal Industries, Inc. | Conduit connector with two housings for connection to flexible conduit |
US20120072449A1 (en) | 2010-09-17 | 2012-03-22 | Microsoft Corporation | Object instance versioning |
US9323442B2 (en) | 2010-09-30 | 2016-04-26 | Apple Inc. | Managing items in a user interface |
US9031957B2 (en) | 2010-10-08 | 2015-05-12 | Salesforce.Com, Inc. | Structured data in a business networking feed |
US8903770B2 (en) | 2010-10-15 | 2014-12-02 | Salesforce.Com, Inc. | Methods and apparatus for discontinuing the following of records in an on-demand database service environment |
EP2458512A1 (en) | 2010-11-29 | 2012-05-30 | Deutsche Telekom AG | Mobile data storage |
EP2458548A1 (en) | 2010-11-30 | 2012-05-30 | France Telecom | System and method for implementing dynamic access control rules to personal cloud information |
US8464184B1 (en) | 2010-11-30 | 2013-06-11 | Symantec Corporation | Systems and methods for gesture-based distribution of files |
US20120233227A1 (en) | 2010-12-09 | 2012-09-13 | International Business Machines Corporation | File attachment retrieval |
US8412731B2 (en) | 2010-12-09 | 2013-04-02 | International Business Machines Corporation | File management method and system |
WO2012092025A2 (en) | 2010-12-27 | 2012-07-05 | Google Inc. | Social network collaboration space |
US10554426B2 (en) | 2011-01-20 | 2020-02-04 | Box, Inc. | Real time notification of activities that occur in a web-based collaboration environment |
US9123080B2 (en) | 2011-01-27 | 2015-09-01 | Google Inc. | Content access control in social network |
US20120213404A1 (en) * | 2011-02-18 | 2012-08-23 | Google Inc. | Automatic event recognition and cross-user photo clustering |
US8468164B1 (en) | 2011-03-09 | 2013-06-18 | Amazon Technologies, Inc. | Personalized recommendations based on related users |
WO2012131429A1 (en) | 2011-03-29 | 2012-10-04 | Yogesh Chunilal Rathod | A method and system for dynamically publishing, sharing, communication and subscribing |
US20120254332A1 (en) | 2011-03-31 | 2012-10-04 | William Irvin | Systems and methods for importing media file email attachments |
US20120290947A1 (en) | 2011-04-12 | 2012-11-15 | Arcode Corporation | Methods and systems to filter and display electronic messages |
US9646291B2 (en) | 2011-05-11 | 2017-05-09 | Visa International Service Association | Electronic receipt manager apparatuses, methods and systems |
US20120290926A1 (en) | 2011-05-12 | 2012-11-15 | Infinote Corporation | Efficient document management and search |
US8332424B2 (en) | 2011-05-13 | 2012-12-11 | Google Inc. | Method and apparatus for enabling virtual tags |
US9710765B2 (en) * | 2011-05-26 | 2017-07-18 | Facebook, Inc. | Browser with integrated privacy controls and dashboard for social network data |
WO2012164648A1 (en) | 2011-05-27 | 2012-12-06 | 株式会社日立製作所 | File history recording system, file history management device, and file history recording method |
US9058612B2 (en) | 2011-05-27 | 2015-06-16 | AVG Netherlands B.V. | Systems and methods for recommending software applications |
US8788947B2 (en) | 2011-06-14 | 2014-07-22 | LogMeln, Inc. | Object transfer method using gesture-based computing device |
US9978040B2 (en) | 2011-07-08 | 2018-05-22 | Box, Inc. | Collaboration sessions in a workspace on a cloud-based content management system |
US9189551B2 (en) | 2011-07-20 | 2015-11-17 | Opentable. Inc. | Method and apparatus for category based navigation |
US20130036369A1 (en) * | 2011-08-02 | 2013-02-07 | SquaredOut, Inc. | Systems and methods for managing event-related information |
US8458174B1 (en) | 2011-09-02 | 2013-06-04 | Google Inc. | Semantic image label synthesis |
US8887035B2 (en) | 2011-09-21 | 2014-11-11 | Facebook, Inc. | Capturing structured data about previous events from users of a social networking system |
US9286641B2 (en) | 2011-10-19 | 2016-03-15 | Facebook, Inc. | Automatic photo capture based on social components and identity recognition |
US9143601B2 (en) * | 2011-11-09 | 2015-09-22 | Microsoft Technology Licensing, Llc | Event-based media grouping, playback, and sharing |
KR101812657B1 (en) | 2011-11-22 | 2018-01-31 | 삼성전자주식회사 | A method and apparatus for recommending applications based on context information |
KR20130065802A (en) | 2011-11-30 | 2013-06-20 | 삼성전자주식회사 | System and method for recommending application by using keword |
US8891907B2 (en) | 2011-12-06 | 2014-11-18 | Google Inc. | System and method of identifying visual objects |
US8417000B1 (en) | 2011-12-12 | 2013-04-09 | Google Inc. | Determining the location at which a photograph was captured |
KR101895536B1 (en) | 2011-12-29 | 2018-10-25 | 삼성전자주식회사 | Server and terminal for recommending application according to use of application, and recommending application method |
US8950009B2 (en) | 2012-03-30 | 2015-02-03 | Commvault Systems, Inc. | Information management of data associated with multiple cloud services |
US20140019317A1 (en) | 2012-04-05 | 2014-01-16 | Lemon, Inc. | Transaction managing system |
US9626381B2 (en) | 2012-06-19 | 2017-04-18 | International Business Machines Corporation | Photo album creation based on social media content |
KR101984154B1 (en) | 2012-07-16 | 2019-05-30 | 삼성전자 주식회사 | Control method for terminal using touch and gesture input and terminal thereof |
US20140068443A1 (en) * | 2012-08-28 | 2014-03-06 | Private Group Networks, Inc. | Method and system for creating mnemonics for locations-of-interests |
US9002962B2 (en) | 2012-12-10 | 2015-04-07 | Dropbox, Inc. | Saving message attachments to an online content management system |
US8612470B1 (en) | 2012-12-28 | 2013-12-17 | Dropbox, Inc. | Application recommendation using stored files |
US20140317109A1 (en) | 2013-04-23 | 2014-10-23 | Lexmark International Technology Sa | Metadata Templates for Electronic Healthcare Documents |
US20150193521A1 (en) | 2014-01-09 | 2015-07-09 | Google Inc. | Methods for Generating an Activity Stream |
US9507791B2 (en) | 2014-06-12 | 2016-11-29 | Google Inc. | Storage system user interface with floating file collection |
-
2014
- 2014-06-13 US US14/303,997 patent/US10078781B2/en active Active
-
2015
- 2015-03-13 WO PCT/US2015/020378 patent/WO2015191132A1/en active Application Filing
- 2015-03-13 DE DE202015009255.1U patent/DE202015009255U1/en active Active
-
2018
- 2018-08-28 US US16/114,310 patent/US20180365489A1/en not_active Abandoned
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040201740A1 (en) * | 2002-03-15 | 2004-10-14 | Canon Kabushiki Kaisha | Automatic determination of image storage location |
US20040085578A1 (en) * | 2002-11-03 | 2004-05-06 | Quek Su Mien | Producing personalized photo calendar |
US20130156275A1 (en) * | 2011-12-20 | 2013-06-20 | Matthew W. Amacker | Techniques for grouping images |
US20130339180A1 (en) * | 2012-03-16 | 2013-12-19 | Ronald Aaron LaPierre | Collection creator and organizer for social media |
Cited By (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9531722B1 (en) | 2013-10-31 | 2016-12-27 | Google Inc. | Methods for generating an activity stream |
US9614880B1 (en) | 2013-11-12 | 2017-04-04 | Google Inc. | Methods for real-time notifications in an activity stream |
US9509772B1 (en) | 2014-02-13 | 2016-11-29 | Google Inc. | Visualization and control of ongoing ingress actions |
US9536199B1 (en) | 2014-06-09 | 2017-01-03 | Google Inc. | Recommendations based on device usage |
US9507791B2 (en) | 2014-06-12 | 2016-11-29 | Google Inc. | Storage system user interface with floating file collection |
US9870420B2 (en) | 2015-01-19 | 2018-01-16 | Google Llc | Classification and storage of documents |
US10331705B2 (en) * | 2015-06-07 | 2019-06-25 | Apple Inc. | Moments naming based on location accuracy |
US9704045B2 (en) * | 2015-10-06 | 2017-07-11 | Yahoo! Inc. | User classification based upon images |
US20170098124A1 (en) * | 2015-10-06 | 2017-04-06 | Yahoo!, Inc | User classification based upon images |
US10552682B2 (en) * | 2015-10-06 | 2020-02-04 | Oath Inc. | User classification based upon images |
US20170351703A1 (en) * | 2016-06-01 | 2017-12-07 | Facebook, Inc. | Grouping Content Based on Geographic Data |
US11544338B2 (en) | 2016-06-01 | 2023-01-03 | Meta Platforms, Inc. | Grouping content based on geographic data |
US10715580B2 (en) * | 2016-06-01 | 2020-07-14 | Facebook, Inc. | Grouping content based on geographic data |
US20190095067A1 (en) * | 2016-07-13 | 2019-03-28 | Tencent Technology (Shenzhen) Company Limited | Method and apparatus for uploading photographed file |
US10809891B2 (en) * | 2016-07-13 | 2020-10-20 | Tencent Technology (Shenzhen) Company Limited | Method and apparatus for uploading photographed file |
US11275489B2 (en) * | 2016-07-13 | 2022-03-15 | Tencent Technology (Shenzhen) Company Limited | Method and apparatus for uploading photographed file |
CN110462608A (en) * | 2017-04-07 | 2019-11-15 | 微软技术许可有限责任公司 | For the automatic narration creation of capture content |
US10360455B2 (en) | 2017-04-07 | 2019-07-23 | Microsoft Technology Licensing, Llc | Grouping captured images based on features of the images |
WO2018187131A1 (en) * | 2017-04-07 | 2018-10-11 | Microsoft Technology Licensing, Llc | Automatic narrative creation for captured content |
US20190171170A1 (en) * | 2017-12-06 | 2019-06-06 | Arris Enterprises Llc | System and method of iot device control using augmented reality |
US11609541B2 (en) | 2017-12-06 | 2023-03-21 | Arris Enterprises Llc | System and method of IOT device control using augmented reality |
US11131973B2 (en) * | 2017-12-06 | 2021-09-28 | Arris Enterprises Llc | System and method of IOT device control using augmented reality |
US20200151453A1 (en) * | 2018-11-08 | 2020-05-14 | International Business Machines Corporation | Reducing overlap among a collection of photographs |
CN112535408A (en) * | 2019-09-23 | 2021-03-23 | 佛山市顺德区美的电热电器制造有限公司 | Auxiliary cooking method, device and computer storage medium |
CN112069342A (en) * | 2020-09-03 | 2020-12-11 | Oppo广东移动通信有限公司 | Image classification method and device, electronic equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US10078781B2 (en) | 2018-09-18 |
US20180365489A1 (en) | 2018-12-20 |
DE202015009255U1 (en) | 2017-01-19 |
WO2015191132A1 (en) | 2015-12-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20180365489A1 (en) | Automatically organizing images | |
US11243912B2 (en) | Context-based file selection | |
US8270684B2 (en) | Automatic media sharing via shutter click | |
EP2402867B1 (en) | A computer-implemented method, a computer program product and a computer system for image processing | |
US20100169326A1 (en) | Method, apparatus and computer program product for providing analysis and visualization of content items association | |
JP6396897B2 (en) | Search for events by attendees | |
EP3405903A1 (en) | Masking restrictive access control system | |
US20130042177A1 (en) | Systems and methods for incorporating a control connected media frame | |
US11768871B2 (en) | Systems and methods for contextualizing computer vision generated tags using natural language processing | |
US20130346405A1 (en) | Systems and methods for managing data items using structured tags | |
US10885619B2 (en) | Context-based imagery selection | |
US10885095B2 (en) | Personalized criteria-based media organization | |
Zhong et al. | i-Memory: An intelligence Android-based photo management system | |
US20130290372A1 (en) | Systems and methods for associating tags with files in a computer system | |
Premchaiswadi et al. | Flower information retrieval using color feature and location-based system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MEYER, CAYDEN;REEL/FRAME:033889/0316Effective date: 20140610 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044129/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |