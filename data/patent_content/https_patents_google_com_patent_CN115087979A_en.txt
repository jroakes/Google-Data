CN115087979A - Privacy preserving machine learning tags - Google Patents
Privacy preserving machine learning tags Download PDFInfo
- Publication number
- CN115087979A CN115087979A CN202180008300.2A CN202180008300A CN115087979A CN 115087979 A CN115087979 A CN 115087979A CN 202180008300 A CN202180008300 A CN 202180008300A CN 115087979 A CN115087979 A CN 115087979A
- Authority
- CN
- China
- Prior art keywords
- computing system
- mpc
- share
- cluster
- shares
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L9/00—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols
- H04L9/08—Key distribution or management, e.g. generation, sharing or updating, of cryptographic keys or passwords
- H04L9/088—Usage controlling of secret information, e.g. techniques for restricting cryptographic keys to pre-authorized uses, different access levels, validity of crypto-period, different key- or password length, or different strong and weak cryptographic algorithms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6245—Protecting personal data, e.g. for financial or medical purposes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/602—Providing cryptographic facilities or services
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6227—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database where protection concerns the structure of data, e.g. records, types, queries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for identifying tags for a data set without disclosing the data set to any individual computing system. The method can include receiving, by a first computing system in a multi-party computing (MPC) system, a query comprising a first share and a second share of a given user profile. The second share is encrypted with a key that prevents the first computing system from accessing the second share. The second share is transmitted to a second computing system in the MPC system. The first and second computing systems generate machine learning models and identify respective first and second labels. The first computing system receives a second tag in response from the second computing system. The first computing system responds to the query with a response that includes the first tag and the second tag.
Description
Background
This specification relates to data processing and machine learning models.
A client device can use an application (e.g., a web browser, a native application) to access a content platform (e.g., a search platform, a social media platform, or another platform that hosts or aggregates content). The content platform is capable of displaying, within an application launched on the client device, a digital component (discrete units of digital content or digital information, such as, for example, a video clip, an audio clip, a multimedia clip, an image, text, or another unit of content) that may be provided by one or more content sources different from the content platform.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of: receiving, by a first computing system in a multi-party computing (MPC) system, a query comprising a first share of a given user profile and a second share of the given user profile, wherein the second share is encrypted with a key that prevents the first computing system from accessing the second share; transmitting, by the first computing system, the second share to a second computing system in the MPC; determining, by the first computing system, a first label of a first cluster having a centroid closest to the first share, wherein the first cluster is one of a plurality of clusters generated by a machine learning model trained by the first computing system and the second computing system; receiving, by the first computing system, a response from a second computing system in the MPCs comprising a second tag of the second cluster; the query is responded to with a response that includes the first tag and the second tag.
Other embodiments of this aspect include corresponding apparatuses, systems, and computer programs configured to perform aspects of the methods encoded on computer storage devices. These and other embodiments can each optionally include one or more of the following features.
The method can further comprise: receiving, by a first computing system, a first plurality of partial shares of a user profile from a digital component distribution system different from the MPC system; receiving, by the second computing system from the digital component distribution system, a second plurality of partial shares of the user profile, wherein neither the first plurality of portions nor the second plurality of portions are shared for the individual user, wherein the first plurality of shares and the second plurality of shares are secret shares of all dimensions of the user profile that include the individual user; training, by the first computing system and the second computing system, the machine learning model using the first plurality of partial shares and the second plurality of partial shares.
The method can include training a clustering model to create a plurality of clusters of the user profile based on the first plurality of partial shares and the second plurality of partial shares.
The method can include: generating, by the MPC system, a centroid feature vector for each cluster from the plurality of clusters; modeling, by the MPC system, each cluster using the probability distribution of the user profile in the cluster; generating, by the MPC system for each cluster, a new centroid feature vector based on the probability distribution and the centroid feature vector of the corresponding cluster; the new centroid feature vector is shared by the MPC computing system to the digital component distribution system.
The method can include: splitting, by a client device, a given user profile into a first share and a second share; generating and transmitting a query to a first computing system as a request for a label for a cluster corresponding to a given user profile; receiving, by the client device, a response comprising the first tag and the second tag; storing, by the client device, a device final label generated based on the first label and the second label.
The method can include generating a final label, the generating the final label further comprising: modeling, by the first computing system and the second computing system, the user profiles of the first cluster and the second cluster as a normal distribution; determining, by the first computing system and the second computing system, parameters of a normal distribution including a centroid and a covariance matrix; generating, by both the first computing system and the second computing system, a first share and a second share of the final label; transmitting, by the MPC system, the first share and the second share of the final label to the client device; the first and second shares of the final label are used by the client device to reconstruct the final label.
The method can include determining, by the first computing system and the second computing system, a covariance matrix, the determining, by the first computing system and the second computing system, the covariance matrix including determining, by the first computing system and the second computing system, an integer matrix such that the matrix, when multiplied by its transpose, generates the covariance matrix.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The techniques described in this document enable the creation of user groups with similar interests and the extension of user group membership while preserving the privacy of the user, e.g., without sharing the user's online activities outside the browser. This limits access to sensitive user information, protects user privacy with respect to such platforms, and protects the security of data generated by violations during transmission to or from the platform. Encryption techniques such as secure multi-party computing (MPC) enable user group expansion based on similarities in user profiles without using third party cookies, which protects user privacy without negatively impacting the ability to expand user groups, and in some cases provides better user group expansion based on a more complete profile than can be achieved using third party cookies (i.e., cookies from a domain different from the domain of resources accessed by the client device (e.g., eTLD + 1)). In addition, where a browser (or other application) prevents the use of third-party cookies, although third-party cookies cannot be used, the techniques discussed herein still enable the creation of groups of users, solving the technical problem of how to group data for access to multiple different websites into data sets where third-party cookies cannot be used. MPC technology can ensure that user data is protected from disclosure in the clear as long as one computing system in the MPC system is not in collusion with other computing systems. Thus, the techniques discussed herein also address the technical issue of how to enable different systems to use a particular data set while preventing any individual system from accessing the particular data set in the clear (e.g., in unencrypted form). These techniques also allow user data to be identified, grouped, and transmitted in a secure manner without the need to use third-party cookies to determine any relationship between accessing user data corresponding to multiple different sites located at different eutld +1 (the valid top-level domain plus the portion of the domain immediately preceding it). This is a different approach, and an improvement over, existing approaches that require third party cookies to determine relationships between data collected from different sites (e.g., the eTLD + 1). By grouping the user data in this way, the efficiency of delivering data content to the user device is increased, as irrelevant data content need not be delivered. Particularly, a third-party cookie is not needed, so that the storage of the third-party cookie is avoided, and the memory utilization rate is improved. Exponential decay techniques can be used to build a user profile at a client device to reduce the data size of the raw data required to build the user profile, thereby reducing data storage requirements.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment in which digital components are distributed.
FIG. 2 is a swim lane diagram of an example process for training a machine learning model and using the machine learning model to determine a user group of a user.
FIG. 3 is a flow diagram of an example process of adding differential privacy to a cluster centroid.
FIG. 4 is a flow diagram illustrating an example process for generating a k-means model.
FIG. 5 is a flow diagram illustrating an example process for processing a query by a computing multi-party computing system.
FIG. 6 is a block diagram of an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This document discloses methods, systems, apparatuses, and computer-readable media that implement techniques including using a machine learning model to identify tags for a data set without disclosing the data set to any individual computing system. For example, the techniques discussed herein provide different computing systems with access to portions of a data set while preventing each computing system from accessing other portions of the data set. In some implementations, the multiple portions of the data set are created by splitting the data set into the multiple portions such that each individual portion represents an incomplete portion of the data set and does not reveal anything about the data set.
The computing system will execute a cryptographic protocol to identify the tags for the entire data set while restricting any computer system to only accessing discrete portions of the data set to which it is granted access, and return the tags in a secure manner so that only devices requesting the tags for the complete data set will have access to the tags. For example, one computing system provided with a first discrete portion of a data set can identify a first share of information (share) of a result tag, encrypt first information identifying a first secret share of the tag with a key known only to the device requesting the tag for the data set, and pass the encrypted version of the first information to a second computing system that has been provided with a second discrete portion of the data set. The second computing system can similarly identify the second tag, encrypt second information identifying the second tag with a key known only to the device requesting the tag for the complete data set, and if there are other computing systems that process other discrete portions of the complete data set, pass the encrypted version of the second secret share of the tag along with the encrypted version of the first secret share of the tag to yet another computing system, or pass the encrypted information to the device requesting the tag for the complete data set.
The device requesting the tag for the complete data set can then decrypt the received information, combining all secret shares of the tag to obtain the final tag in clear text for the data set. As mentioned above, this technique, which is described in more detail throughout this document, solves the technical problem of how to generate tags for complete data sets without providing access to the complete data sets, which is an improvement in data access techniques and data security.
The techniques discussed in this document can be used in many data processing environments. One environment that can benefit from the use of these techniques is one in which user data is organized into data sets (or included in data sets), as these techniques prevent access to a complete user data set while still enabling the user data to be marked in an aggregate. For example, as described in more detail below, these techniques enable a complete set of user data to remain stored in a single trusted location (e.g., at a user's device), while enabling the user data to be processed and/or tagged by remote systems that are capable of running more complex algorithms (e.g., machine learning algorithms) that can be executed at the user's device (e.g., a mobile phone, tablet device, wearable device, voice assistant device, gaming device, or laptop device). As described in detail below, the machine learning model used to determine labels for discrete portions of data can also be trained using user data that is also protected in a similar manner as discrete portions of data labeled by the machine learning model.
FIG. 1 is a block diagram of an example environment 100 in which digital components are distributed for presentation (e.g., with an electronic document). The example environment 100 includes a network 102, such as a Local Area Network (LAN), a Wide Area Network (WAN), the Internet, or a combination thereof. Network 102 connects content server 104, client devices 106, digital component providers 108, and digital component distribution system 110 (also referred to as a Component Distribution System (CDS)).
The client device 106 is an electronic device capable of requesting and receiving resources over the network 102. Example client devices 106 include personal computers, mobile communication devices, wearable devices, personal digital assistants, tablet devices, gaming devices, media streaming devices, IoT devices (e.g., thermostats, home control units, appliances, and various sensors), and other devices capable of sending and receiving data over the network 102. The client device 106 typically includes a user application 107, such as a web browser, to facilitate sending and receiving data over the network 102, although a native application executed by the client device 106 can also facilitate sending and receiving data over the network 102. The client device 106, and in particular the personal digital assistant, can include hardware and/or software that enables voice interaction with the client device 106. For example, the client device 106 can include a microphone through which a user can submit audio (e.g., voice) input, such as commands, search queries, browsing instructions, smart-home instructions, and/or other information. Additionally, the client device 106 can include a speaker through which audio (e.g., voice) output can be provided to the user. The personal digital assistant can be implemented in any client device 106, examples including wearable devices, smart speakers, home appliances, automobiles, tablet devices, or other client devices 106.
An electronic document is data that presents a set of content at a client device 106. Examples of electronic documents include web pages, word processing documents, Portable Document Format (PDF) documents, audio, images, video, search result pages, and feeds. Native applications (e.g., "apps"), such as applications installed on mobile devices, tablet devices, or desktop computing devices, are also examples of electronic documents. The electronic document can be provided by the content server 104 to the client device 106. For example, the content server 104 can include a server hosting a publisher's website. In this example, the client device 106 can initiate a request for a given publisher web page, and the content server 104, which includes a web server hosting the given publisher web page, can respond to the request by sending machine-executable instructions that initiate rendering the given web page at the client device 106.
In another example, the content server 104 can include an app server from which the client device 106 can download apps. In this example, the client device 106 is able to download files needed to install the app at the client device 106, and then execute the downloaded app locally. The downloaded app can be configured to present a combination of native content that is part of the app itself and one or more digital components (e.g., content created/distributed by third parties) that are obtained from the digital component server 108 and inserted into the app as the app is being executed at the client device 106.
The electronic document can include various contents. For example, the electronic document can include static content (e.g., text or other specified content) that is within the electronic document itself and/or that does not change over time. The electronic document can also include dynamic content that can change over time or on a per-request basis. For example, a publisher of a given electronic document can maintain a data source for populating portions of the electronic document. In this example, the given electronic document can include a tag or script that causes the client device 106 to request content from a data source when the given electronic document is processed (e.g., rendered or executed) by the client device 106. The client device 106 integrates the content obtained from the data source into a given electronic document to create a composite electronic document that includes the content obtained from the data source.
In some cases, a given electronic document can include a digital component tag or digital component script that references the digital component distribution system 110. In these cases, the digital component tag or digital component script is executed by the client device 106 when a given electronic document is processed by the client device 106. Execution of the digital component tag or digital component script configures the client device 106 to generate a request (referred to as a "component request") for a digital component 112 that is transmitted over the network 102 to the digital component distribution system 110. For example, a digital component tag or digital component script can enable the client device 106 to generate a packetized data request including a header and payload data. The digital component request 112 can include event data specifying characteristics such as the name (or network location) of the server from which the media was requested, the name (or network location) of the requesting device (e.g., client device 106), and/or information that the digital component distribution system 110 can use to select one or more digital components to provide in response to the request. The component request 112 is transmitted by the client device 106 over the network 102 (e.g., a telecommunications network) to a server of the digital component distribution system 110.
The digital component request 112 can include event data specifying other event characteristics, such as characteristics of the location of the requested electronic document and the electronic document that can render the digital component. For example, event data specifying a reference (e.g., a Uniform Resource Locator (URL)) to an electronic document (e.g., a web page or application) in which the digital component is to be rendered, available locations of the electronic document available for rendering the digital component, sizes of the available locations, and/or media types eligible for rendering at these locations can be provided to the digital component distribution system 110. Similarly, event data specifying keywords associated with the electronic document ("document keywords") or entities (e.g., people, places, or things) referenced by the electronic document can also be included in the component request 112 (e.g., as payload data) and provided to the digital component distribution system 110 in order to identify digital components that are eligible for presentation with the electronic document. Event data can also include search queries submitted from the client device 106 to obtain a search results page and/or data specifying search results and/or text, audible, or other visual content included in the search results.
The component request 112 can also include event data related to other information, such as information that a user of the client device has provided, geographic information indicating a status or area in which the component request was submitted, or other information that provides context of the environment in which the digital component is to be displayed (e.g., time of day of the component request, day of week of the component request, type of device that is to display the digital component, such as a mobile device or tablet device). For example, component request 112 can be transmitted over a packetized network, and component request 112 can itself be formatted as packetized data with a header and payload data. The header can specify the destination of the packet, and the payload data can include any of the information discussed above.
A digital component distribution system 110, which includes one or more digital component distribution servers, selects a digital component to be presented with a given electronic document in response to receiving a component request 112 and/or using information included in the component request 112. In some embodiments, the digital components are selected in less than one second to avoid errors that can be caused by delayed selection of the digital components. For example, delays in providing digital components in response to the component request 112 can result in page loading errors at the client device 106, or result in portions of the electronic document remaining unfilled even after other portions of the electronic document are presented at the client device 106. Moreover, as the delay in providing digital components to the client device 106 increases, it is more likely that the electronic document will no longer be presented at the client device 106 when the digital components are delivered to the client device 106, thereby negatively impacting the user's experience with the electronic document and wasting system bandwidth and other resources. Moreover, delays in providing the digital component can result in delivery failures of the digital component, for example, if the electronic document is no longer presented at the client device 106 at the time the digital component is provided.
To facilitate searching for electronic documents, the environment 100 can include a search system 150 that identifies electronic documents by crawling and indexing the electronic documents (e.g., indexed based on the content of the crawled electronic documents). Data about the electronic document can be indexed based on the electronic document associated with the data. The indexed and optionally cached copies of the electronic documents are stored in a search index 152 (e.g., a hardware memory device). The data associated with the electronic document is data representing content included in the electronic document and/or metadata of the electronic document.
The client device 106 can submit a search query to the search system 150 over the network 102. In response, the search system 150 accesses the search index 152 to identify electronic documents that are relevant to the search query. The search system 150 identifies the electronic document in the form of search results and returns the search results to the client device 106 in a search results page. The search results are data generated by the search system 150 that identifies electronic documents that are responsive to (e.g., relevant to) a particular search query, and that includes active links (e.g., hypertext links) that cause the client device to request the data from a specified location in response to user interaction with the search results. Example search results can include a web page title, a snippet of text or a portion of an image extracted from the web page, and a URL of the web page. Another example search result can include a title of the downloadable application, an excerpt that describes text of the downloadable application, an image depicting a user interface of the downloadable application, and/or a URL of a location from which the application can be downloaded to client device 106. Another example search result can include a title of the streaming media, an excerpt describing text of the streaming media, an image depicting content of the streaming media, and/or a URL to a location from which the streaming media can be downloaded to the client device 106. As with other electronic documents, the search results page can include one or more slots in which digital components (e.g., advertisements, video clips, audio clips, images, or other digital components) can be presented.
In some implementations, the digital component distribution system 110 is implemented in a distributed computing system that includes, for example, a server and a plurality of sets of computing devices 114 that are interconnected and that identify and distribute digital components in response to component requests 112. Multiple sets of computing devices 114 operate together to identify a set of digital components that qualify for presentation in an electronic document from among a corpus of millions of available digital components.
In some implementations, the digital component distribution system 110 implements different techniques for selecting and distributing digital components. For example, the digital components can include corresponding distribution parameters that facilitate (e.g., adjust or limit) selection/distribution/transmission of the corresponding digital components. For example, the distribution parameters can facilitate transmission of the digital component by requiring the component request to include at least one criterion that matches (e.g., either exactly or at some pre-specified level of similarity) one of the distribution parameters of the digital component.
In another example, the distribution parameters for a particular digital component can include distribution keywords that must be matched (e.g., by an electronic document, document keyword, or item specified in the component request 112) in order for the digital component to qualify for presentation. The distribution parameters can also require that the component request 112 include information specifying a particular geographic region (e.g., country or state) and/or information specifying that the component request 112 originates from a particular type of client device 106 (e.g., mobile device or tablet device) in order to qualify the component item for presentation. The distribution parameters can also specify a qualification value (e.g., a ranking, score, or some other specified value) for evaluating the eligibility of the component item for selection/distribution/transmission (e.g., among other available digital components), as discussed in more detail below. In some cases, the eligibility value can be based on an amount to be submitted when a particular event is attributed to a digital component item (e.g., a presentation of a digital component).
The identification of qualified digital components can be partitioned into a plurality of tasks 117a-117c, which are then assigned among the computing devices within the plurality of sets of computing devices 114. For example, different computing devices in the set 114 can each analyze different digital components to identify various digital components having distribution parameters that match the information included in the component request 112. In some implementations, each given computing device in the set 114 can analyze a different data dimension (or set of dimensions) and communicate (e.g., transmit) the results of the analysis (results 1-3) 118a-118c back to the digital component distribution system 110. For example, the results 118a-118c provided by each of the computing devices in the set 114 may identify a subset of digital component items that are eligible for distribution in response to a component request and/or a subset of digital components having certain distribution parameters. The identification of the subset of digital components can include, for example, comparing the event data to distribution parameters and identifying the subset of digital components having distribution parameters that match at least some characteristics of the event data.
The digital component distribution system 110 aggregates the results 118a-118c received from the multiple sets of computing devices 114 and uses information associated with the aggregated results to select one or more digital components to be provided in response to the component request 112. For example, the digital component distribution system 110 can select a winning set of digital components (one or more digital components) based on the results of one or more digital component evaluation processes. In turn, the digital component distribution system 110 can generate and transmit back over the network 102 complex data 120 (e.g., digital data representing replies) that enables the client device 106 to integrate the winning set of digital components into a given electronic document such that the winning set of digital components is presented at a display of the client device 106 along with the content of the electronic document.
In some implementations, the client device 106 executes instructions included in the reply data 120 that configure and enable the client device 106 to obtain a winning set of digital components from the one or more digital component servers 108. For example, the instructions in the reply data 120 can include a network location (e.g., a URL) and a script that causes the client device 106 to transmit a Server Request (SR)121 to the digital component server 108 to obtain a given winning digital component from the digital component server 108. In response to the server request 121, the digital component server 108 will identify a given winning digital component specified in the server request 121 and transmit digital component data 122(DC data) to the client device 106 that presents the given winning digital component in the electronic document at the client device 106.
In some cases, it may be beneficial for a user to receive digital components related to web pages, application pages, or other electronic resources previously visited by and/or interacted with by the user. To distribute such digital components to users, users can be assigned to groups of users based on the digital content accessed by the users, e.g., groups of user interests, groups of similar users, or other group types involving similar user data. For example, when a user visits a particular website and interacts with a particular item presented on the website or adds an item to a virtual shopping cart, the user can be assigned to a group of users who have visited the same website or other websites that are contextually similar or are interested in the same item. To illustrate, if a user of a client device 106 searches for shoes and accesses multiple web pages of different shoe manufacturers, the user can be assigned to a user group of "shoes," which can include identifiers of all users who have accessed a website related to the shoes.
In some implementations, the user's group membership can be maintained at the user's client device 106, e.g., through a browser-based application, rather than through a digital component provider or through the content platform 104 or through another party. The user groups can be specified by respective user group tags. The tags of the user group can describe the group (e.g., a gardening group) or a code (e.g., not a descriptive alphanumeric sequence) that represents the group. The tags for the user group can be stored in a secure store at the client device 106 and/or can be encrypted at the time of storage to prevent others from accessing the list.
The digital component server can use the user's group membership to select digital components or other content that may be of interest to the user or that may be beneficial to the user/user device in another manner (e.g., to help the user complete a task). For example, such digital components or other content may include data that improves the user experience, improves the operation of the user device, or benefits the user or user device in some other manner. However, the user group tags can be provided to the user in a manner that prevents the content server from associating the user group identifier with a particular user, thereby protecting user privacy when using the user group membership data to select the digital components. This document mentions user group membership data and user data as examples of data that should be protected from access by unauthorized parties (or computing systems), but the techniques discussed in this document are not limited to such applications and can be used with respect to any set of data to be protected from unauthorized access.
The application 107 can provide the user group tags to a trusted computing system that interacts with the digital component server to select digital components for presentation at the client device 107 based on user group membership in a manner that prevents the content platform or any other entity that is not the user itself from knowing the full user group membership of the user.
In some embodiments, users are assigned to only one user group at a time, and the assignment of users to user groups is a temporary assignment, as the group membership of a user can change with respect to the user's browsing activity. For example, when a user starts a web browsing session and visits a particular website and interacts with a particular item presented on the website or adds an item to a virtual shopping cart, the user can be assigned to a group of users who have visited the same website or other websites that are similar in context or are interested in the same item.
However, if the user visits another website and interacts with another type of item presented on another website, the user is assigned to another group of users who have visited the other website or other websites with similar context or interest in the other item. For example, if a user begins a browsing session by searching for shoes and visiting multiple web pages of different shoe manufacturers, the user can be assigned to a group of users "shoes," which includes all users who have visited a website related to the shoes.
Assume that 100 users have previously visited a website associated with a shoe. When a user is assigned to the user group "shoes", the total number of users included in the user group is increased to 101. However, after a user searches for hotels and accesses multiple web pages of different hotels or travel agencies, the user can be removed from the previously assigned user group "shoes" and reassigned to a different user group "hotels" or "travel". In this case, the number of users in the user group "shoes" is reduced back to 100, assuming no other users are added to or removed from the particular user group.
The number and type of user groups is managed and/or controlled by the system (or administrator). For example, the system may implement algorithmic and/or machine learning methods to supervise the management of user groups. Typically, the number of user groups and the number of users in each user group change over time as the traffic of users participating in an active browser session changes over time, and as each individual user is responsible for their respective browsing activity.
In some implementations, the component distribution system 110 includes a multi-party computing (MPC) system 130 that implements a machine learning model to oversee the management of user groups. The MPC system 130 can train a machine learning model that suggests user groups to the user (or their application 107) based on the user's profile, or can be used to generate suggestions of user groups to the user (or their application 107) based on the user's profile. The MPC system 130 includes two computing systems MPC 1132 and MPC 2134 that implement security privacy preserving techniques to train machine learning models. Although the example MPC system 130 includes two computing systems, more computing systems can be used as long as the MPC system 130 includes more than one computing system.
Computing systems MPC 1132 and MPC 2134 can be operated by different entities, which can prevent each entity from accessing a complete user profile in the clear when implementing the techniques described in this document. Plaintext is text that is computationally untagged, specially formatted, or written in code or data (including binary files) in a form that can be viewed or used without the need for a key or other decryption device or other decryption process. For example, one of computing systems MPC 1132 or MPC 2134 can be operated by a trusted party other than the user's client device 106, content platform 104, and digital component server 108. For example, an industry group, government group, or browser developer may maintain and operate one of computing systems MPC 1132 and MPC 2134. Other computing systems may be operated by different ones of these groups, such that different trusted parties operate each computing system MPC 1132 and MPC 2134. Preferably, the different parties operating different computing systems MPC 1132 and MPC 2134 have no motivation to collude to compromise user privacy. In some embodiments, computing systems MPC 1132 and MPC 2134 are architecturally separate and are monitored so as not to communicate with each other outside of performing the secure MPC processes described in this document.
In some implementations, the user profile of the user can be in the form of a feature vector. For example, the user profile can be an n-dimensional feature vector. Each of the n dimensions can correspond to a particular feature, and the value of each dimension can be a feature value of the user. For example, one dimension may be for whether a particular digital component is presented to (or interacts with) the user. In this example, the value of the feature can be "1" if the digital component is presented to (or interacts with) the user, or "0" if the digital component has not been presented to (or interacts with) the user.
The user profile of the user can include data related to user-initiated events and/or events that the user can have initiated with respect to an electronic resource (e.g., web page or application content). The event can include a view of the electronic resource, a view of the digital component, a user interaction with the electronic resource or the digital component (e.g., a selection of the electronic resource or the digital component), or a lack of user interaction with the electronic resource or the digital component (e.g., a selection of the electronic resource or the digital component), a transition that occurs after the user interacts with the electronic resource, and/or other suitable events related to the user and the electronic resource.
In some implementations, the application 107 can generate different user profiles for different machine learning models owned by the content server at the request of the content server. Different machine learning models may require different training data based on design goals. For example, the first model may be a k-NN model used to determine whether to add a user to a group of users.
When an event occurs, the content server can provide event data related to the event to an application 107 executing on the client device in order to generate a user profile for the user. In some implementations, to protect the event data during transmission, the content server encrypts the event data before transmitting the event data to the application 107. For example, the content server can encrypt the event data using a public encryption key (e.g., PubKeyEnc (event _ data, application _ public _ key)) of the application 107.
In some embodiments, the event data can include the following items as shown in table 1 below.
TABLE 1
Referring to Table 1, the model identifiers identify machine learning models, such as k-NN models, that the user profiles are to be used to train and predict user group memberships, and generate corresponding labels for the predicted user groups. A profile record is an n-dimensional feature vector that includes event-specific data, such as the type of event, electronic resources or digital components, the context of the electronic resources or digital components time when the event occurred, and/or other suitable event data that the content server wants to use in training the machine learning model and making user group inferences.
The application 107, after receiving the event data, is able to decrypt the event data using its private key, which corresponds to the public encryption key used to encrypt the event data. The application 107 can verify the event data by (i) verifying the digital signature using a public verification key of the content server corresponding to a private key of the content server used to generate the digital signature, and (ii) ensuring that the event data creation timestamp is not stale, e.g., the time indicated by the timestamp is within a threshold amount of time of the current time at which verification is occurring. If the event data is valid, the application 107 can store the event data, for example, by storing an n-dimensional profile record. If any of the verifications fail, the application 107 may ignore the event data, for example, by not storing an n-dimensional profile record.
In some implementations, the application 107 can compute the user profile by aggregating n-dimensional feature vectors (i.e., profile records). For example, the user profile may be an average of n-dimensional feature vectors of a plurality of events associated with the user. The result is an n-dimensional feature vector representing the user in the profile space. Alternatively, the application 112 may normalize the n-dimensional feature vector to a unit length, for example, using L2 normalization.
In some implementations, the application 107 can calculate the user profile (P) using the following equation:
wherein the parameter F i Comprising k feature vectors, and each vector havingAn n-dimensional feature, record _ age _ in _ seconds, that characterizes an event (e.g., a user's interaction with content or another event attributable to the user) i Is the amount of time in seconds that the profile record has been stored at the client device, and the parameter decay _ rate _ in _ seconds is the decay rate of the profile record in seconds.
In some implementations, the application 107 can update the user profile (P) as and when the event occurs. In this case, the application can update the user profile using the following equation:
where P' is the updated user profile and F is the n-dimensional feature vector of the new event, and P is the n-dimensional feature vector of the existing user profile generated at user _ profile _ time.
FIG. 2 is a swim lane diagram of an example process 200 for training a k-means machine learning model to predict a user group of users. The operations of process 200 can be implemented, for example, by client device 110, computing systems MPC 1132 and MPC 2134 of MPC system 130, and by content providers. The operations of process 200 can also be implemented as instructions stored on one or more computer-readable media, which may be non-transitory, and execution of the instructions by one or more data processing apparatus can cause the one or more data processing apparatus to perform the operations of process 200.
The content server can initiate training and/or updating of one of its machine learning models by requesting the applications 107 running on the client devices 106 to generate user profiles for their respective users and upload secret shares and/or encrypted versions of the user profiles to the MPC system 130. For the purposes of this document, a secret share of a user profile can be considered an encrypted version of the user profile because the secret share is not in the clear. In general, each application 107 is capable of storing data of a user profile and generating an updated user profile in response to receiving a request from a content platform.
The application 107 running on the client device 106 builds a user profile for the user of the client device 106 (202). The user profile of the user can include data related to user-initiated events and/or events that the user can have initiated with respect to an electronic resource (e.g., web page or application content). The event can include a context of the electronic resource, a view of the digital component, a user interaction with the electronic resource or the digital component (e.g., a selection of the electronic resource or the digital component), or a lack of user interaction with the electronic resource or the digital component (e.g., a selection of the electronic resource or the digital component), a transition that occurs after the user interacts with the electronic resource, and/or other suitable events related to the user and the electronic resource.
The user profile of the user can be in the form of a feature vector. For example, the user profile can be an n-dimensional feature vector. Each of the n dimensions can correspond to a particular feature, and the value of each dimension can be a feature value of the user. For example, one dimension may be for whether a particular digital component is presented to (or interacts with) the user. In this example, the value of the feature can be "1" if the digital component is presented to (or interacts with) the user, or "0" if the digital component has not been presented to (or interacts with) the user.
The application 107 generates shares of the user profile for the user (204). In this example, the application 107 generates two shares of the user profile, one for each computing system of the MPC system 130. Note that each share can itself be a pseudo-random variable that does not itself reveal anything about the user profile. Both shares will need to be combined to get the user profile. If the MPC system 130 includes more computing systems that participate in training the machine learning model, the application 107 will generate more shares, one for each computing system. In some implementations, to protect user privacy, the application 107 can use a pseudo-random function to split the user profile into shares. That is, the application 107 can use a pseudo-random function to generate two shares { [ P ] i,1 ]，[P i,2 ]}. The exact split can depend on the secret used by the application 107Sharing algorithms and codebooks.
In some embodiments, the application 107 must upload multiple shares of the user profile to the respective MPC system simultaneously so that the computing system can correctly match all shares of the same user profile. In some implementations, the application 107 can explicitly assign the same pseudo-random or sequentially generated identifier to multiple shares of the same user profile to facilitate matching. While some MPC techniques can rely on random scrambling of inputs or intermediate results, the MPC techniques described in this document may not include such random scrambling and may instead rely on an upload order to match.
In some implementations, operations 208 and 210 can be replaced by an alternative process in which the application 107 can upload multiple shares of a user profile to the content server and the content server uploads the multiple shares to the MPC system. This alternative process can increase the infrastructure cost of the content server to support operations 208 and 210. It can also increase the latency to begin training or updating machine learning models in an MPC system. However, this alternative process can allow the content server to store and manage user data without disclosing any user details to the content server, thereby maintaining user privacy.
In some implementations, the content server can collect shares of multiple different user profiles (or other data sets), and each share can be individually encrypted as described above (e.g., in a manner such that secret shares intended for a particular MPC server can only be accessed by that particular MPC server). Using a content server as an aggregator of shares of multiple different user profiles can enable the collection and upload of many different encrypted user profiles that can be used to train one or more machine learning models. Although training of the machine learning model typically occurs prior to a request for a tag, the machine learning model can continue to be updated using newly collected data even after the machine learning model has been used to generate the tag. The following paragraphs discuss the training of the model for generating labels using the encrypted shares of the user profile described above.
Computing systems MPC 1132 and MPC 2134 generate machine learning models (212). In some embodiments, the machine learning models implemented by the MPC 1132 and MPC 2134 systems within the MPC system 130 are k-means models. In general, the k-means algorithm is an algorithm that attempts to divide a data set into k distinct non-overlapping groups (clusters), where each data point belongs to only one group (cluster). Computing systems MPC 1132 and MPC 2134 are capable of training the k-means model using MPC techniques based on the encrypted portion of the user profile received from application 107.
To minimize or at least reduce cryptographic computations, and thus protect user privacy and data during model training and inference, is placed on computing system MThe computational burden on the PC 1132 and MPC 2134, the MPC system 130 can use a stochastic projection technique, such as SimHash, to quantify the two user profiles P quickly, securely, and probabilistically i And P j The similarity between them. Two user profiles P i And P j The similarity between the two user profiles can be determined by determining the profile representing the two user profiles P i And P j Is determined, which hamming distance is proportional with high probability to the cosine similarity between the two user profiles.
Conceptually, for each training session, m random projection hyperplanes U ═ U can be generated 1 ，U 2 …U m }. The random projection hyperplane can also be referred to as a random projection plane. One purpose of the multi-step computation between computing systems MPC 1132 and MPC 2134 is to target each user profile P used in training the k-means model i Creating a bit vector B of length m i . At the bit vector B i In for all of the bits B in which [ ] indicates the dot product of two vectors of equal length i,j Representing projection plane U j One of which is associated with a user profile P i Sign of dot product of, i.e. B i,j ＝sign(U j ⊙P i ). That is, each bit represents a user profile P i In the plane U j Which side of the frame. A bit value of one indicates a positive sign and a bit value of zero indicates a negative sign.
At the end of the multi-step calculation, each of the two computing systems MPC 1132 and MPC 2134 generates an intermediate result that includes the bit vector of each user profile and the share of each user profile in clear text. For example, the intermediate results for computing system MPC 1132 can be the data shown in table 2 below. Computing system MPC 2134 would have similar intermediate results but with different shares per user profile. To add additional privacy protection, each of the two servers in MPC system 130 can only get half of the m-dimensional bit vector in plaintext form, e.g., computing system MPC 1132 gets the first m/2 dimensions of all m-dimensional bit vectors and computing system MPC 2134 gets the second m/2 dimensions of all m-dimensional bit vectors.
Bit vector in plaintext form | For P i MPC 1132 fraction of |
… | … |
B i | … |
B i+1 | … |
… | … |
TABLE 2
Two arbitrary user profile vectors P of given unit length i ≠ j i And P j It has been shown that two user profile vectors P, provided that the number m of random projections is sufficiently large i And P j Bit vector B of i And B j Hamming distance between with high probability and user profile vector P i And P j The cosine similarity between them is proportional.
Based on the above intermediate results, and because of the bit vector B i In clear text, each computing system MPC 1132 and MPC 2134 can be created independently, for example, by training the corresponding k-means model using a k-means algorithm.
In some embodiments, the number of aggregates, k, in the k-means model is selected according to the following equation:
k＝z*2 -x
where z is the number of applications 107 and x is the bit of entropy. For example, assume that a total of 256 applications must be grouped (or clustered) together such that each group includes the same number of applications and the number of entropy bits (x) is 5. In such a scenario, the k-means model generates k 8 clusters, where each cluster includes 32 applications. An example process for training a k-means model is illustrated with reference to FIG. 4.
In some implementations, after the clusters of the user profile are generated by the k-means model, each cluster is assigned a unique identifier (referred to as a label). For example, if there are 10 clusters, the clusters can be labeled with the numbers 1 to 10. In another embodiment, the labels can be assigned to the clusters generated by the k-means model based on previous labels of most user profiles in the respective clusters. For example, assume that a cluster includes 32 user profiles. It is also assumed that 20 of the 32 user profiles have the same previous tag "id _ x". In such a scenario, the cluster is assigned the label "id _ x". It should be noted, however, that to implement such tagging techniques, the application 107 must upload the corresponding previous tags to the respective MPC system 130 along with the user profile shares. In this case, each encrypted share of the user profile includes a share of the user profile and a previous tag.
The application 107 transmits a query for the user group tags to the MPC system 130 (214). In this example, application 107 transmits a query to computing system MPC 1132 for a user group tag, the query including a first encrypted share and a second encrypted share of the user profile. In other examples, application 107 can communicate a query for user group tags to computing system MPC 2. The application 107 may submit a query for the user group tags in response to a request from the content server to provide the tags for the user group to which the application 107 is assigned. For example, the content server can request that the application 107 query the k-means model to determine the user group tags of the application 107 of the client device 110.
To initiate a query for a user group tag, the content server can send a token M for the query for the user group tag to the application 107 infer . Token M infer Enabling servers in the MPC system 130 to verify that the application 107 is authorized to query the k-means model of the content server implemented by the MPC system 130. If model access control is optional, token M infer Is optional.
In some embodiments, token M infer Can include a digital signature of the token-based content and a token creation time using a private key of the content server.
In order to query the user group tags for a specific user, the content server can generate a token M for the query of the user group tags infer And sends the token to the application 107 running on the user's client device 106. In some implementations, the content server encrypts the token M using the public encryption key of the application 107 infer So that only the application 107 can decrypt the token M using its secret private key corresponding to the public encryption key infer . That is, the content platform is able to send PubKeyEnc (M) to the application 107 infer ,application_public_key)。
The application 107 is able to decrypt and verify the token M infer . The application 107 is able to decrypt the encrypted token M using its private key infer . The application 107 can verify the token M by (i) verifying the digital signature using the public encryption key of the content server corresponding to the private key of the content server used to generate the digital signature, and (ii) ensuring that the token creation timestamp is not stale, e.g., the time indicated by the timestamp is within a threshold amount of time of the current time at which verification is occurring infer . If the token M infer Valid, the application 107 can query the MPC system 130.
Conceptually, a query for a user group tag can include a model identifier for identifying a particular machine learning model from among multiple machine learning models that can be implemented as a predictive user group and corresponding tags. The query may also include the current user profile P i . However, to prevent the user profile P to be in clear text form i Exposed to the computing system MPC 1132 or MPC 2134 and thereby protecting user privacy, the application 107 can reduce the user toGear P i Split into two shares [ P ] for MPC 1132 and MPC2, respectively i,1 ]And [ P i,2 ]. The application 107 can then select one of the two computing systems MPC 1132 or MPC 2134 for use in the query, e.g., randomly or pseudo-randomly. If the application 107 selects computing system MPC1, the application 107 can send a first share [ P ] to the computing system MPC 1132 i,1 ]And an encrypted version of the second share (e.g., PubKeyEncrypt ([ P ]) i,2 ]MPC 2)). In this example, application 107 encrypts the second share [ P ] using the public encryption key of computing system MPC 2134 i,2 ]To prevent a computing system MPC 1132 from accessing [ P ] i,2 ]This would enable computing system MPC 1132 to operate according to [ P ] i,1 ]And [ P i,2 ]To reconstruct the user profile P i 。
Computing system MPC 1132 uses parameterized as x-N n (μ 1 ，∑ 1 ) Is modeled on users of the first cluster, wherein mu is 1 Is the n-dimensional centroid of the first cluster and Σ is the covariance matrix of dimension n × n. In this example, MPC 1132 calculates μ 1 ＝∑ i∈ID [P i，1 ]. Similarly, MPC 2134 calculates μ 2 ＝∑ i∈ID [P i，2 ]Wherein [ mu ] is 1 ]And [ mu ] and 2 ]is a secret share of k x mu and k is a known number in clear text.
To calculate the covariance matrix sigma, MPC 1132 calculates sigma 1 ]＝∑ i∈ID (k*[P i，1 -μ 1 ) T *(k*[P i，1 ]-μ 1 ) Wherein k x [ P [ ] i，1 ]-μ 1 Is a matrix of 1 x n secret shares, and (k P i，1 ]-μ 1 ) T Is a matrix k P with dimension n x 1 i，1 ]-μ 1 Transposed version of (1). This results in the covariance matrix having dimensions n x n. Similarly, MPC 2134 is capable of calculating [ ∑ s 2 ]＝∑ i∈ID (k*[P i，2 ]-μ 2 ) T *(k*[P i，2 ]-μ 2 )。
MPC 1132 and MPC 2134 can use two secret shares [ ∑ s 1 ]And [ ∑ 2 ]And follows the equation
where the function recornstruct () generates the secret in plaintext from two secret shares. Either MPC 1132 or MPC 2134 can compute the integer matrix a via Cholesky decomposition, such that a is a T Sigma. After calculating matrix A, matrix A is shared with other computing systems of MPC system 130.
In this example, the usage is parameterized as x to N n (μ，∑) After modeling the users of the first cluster, MPC 1132 can use a Box-Muller transform to generate a random vector, z ═ randomly extracted from the standard normal distribution (z-normal distribution) 1 ，...，z n ) T . MPC 1132 then splits z into two shares [ z [ [ z ] 1 ]And [ z ] 2 ]. MPC 1132 then shares [ z ] with MPC2 2 ]. Similarly, MPC 2134 can use a Box-Muller transform to generate a random vector z '═ z' (z ') randomly drawn from a standard normal distribution' 1 ，...，z′ n ) T . MPC 2134 then splits z into two fractions [ z' 1 ]And [ z' 2 ]. MPC 2134 then shares [ z ] with MPC1 1 ]. MPC 1132 then computes the first label
The MPC system communicates the user group tags to the application 107 (218). Computing system MPC 2134 can provide a second label (i.e., [ result ") to computing system MPC1 2 ]) Wherein the second ticket is encrypted using the public encryption key of the application 107. Computing system MPC 1132 is capable of providing the first label of the result cluster, namely [ result ], to application 107 1 ]And an encrypted version of a second label of a second cluster determined by computing system MPC 2. The application 107 is able to decrypt the second label of the result cluster determined by the computing system MPC2 in conjunction with MPC 1. In some embodiments, to prevent computing system MPC 1132 from forging the results of computing system MPC2, computing system MPC 2134 digitally signs its results before or after encrypting its results using the public encryption key of application 107. Application 107 uses the public encryption key of MPC2 to verify the digital signature of computing system MPC 2.
The application 107 updates and stores the user group tags (220). After receiving the first tag and the second tag from the MPC system 130, the application can compute the final tag asclient device 106. In this embodiment, the FLoC ID is an n-dimensional vector that is randomly generated for the user group of which the user is a member.
As mentioned previously, the user group tag is simply an identifier of the user group to which the user belongs, and there is no contextual meaning that a content platform, such as a digital component provider, can utilize to select the digital components of the application 107. As a solution, the MPC system 130 is able to share certain information such as the centroids of clusters of k-means machine learning models to digital component providers. The centroid of the cluster of k-means machine learning models implemented by the MPC system 130 has the same relationship as the user profile (P) i ) The same dimension. Sharing the centroid to the content platform would allow the content platform to provide digital components based on previous events that occurred due to user activity.
For example, assume that the application 107 updates an n-dimensional user profile based on events that occur as a result of user actions. The MPC130 system determines the cluster to which the application 107 belongs based on the distance between the n-dimensional feature vector user profile provided by the application 107 and the n-dimensional feature vector of the centroid of the cluster of the k-means model. The application 107, after receiving the user's tag, stores the tag in the client device 106. Assume that the MPC system 130 has shared the n-dimensional centroid feature vector to the digital component provider. When an application loads a resource that includes one or more digital component slots, the client device 106 or content server providing the resource generates a request for a digital component that includes a tag for the application 107. Upon receiving a request for a digital component, the digital component provider can provide the digital component based on the n-dimensional centroid of the cluster to which the application belongs.
However, sharing the centroid of the cluster with the content platform creates privacy issues. To overcome this problem, the MPC system 130 utilizes differential privacy techniques and generates a new centroid for the cluster by adding random noise to the centroid. Details are further explained with reference to fig. 3.
Fig. 3 is a flow diagram of an example process 300 for generating a new centroid for a cluster of user profiles using differential privacy techniques. The operations of process 300 can be implemented, for example, by computing systems MPC 1132 and MPC 2134 of MPC system 130. The operations of process 300 can also be implemented as instructions stored on one or more computer-readable media, which may be non-transitory, and execution of the instructions by one or more data processing apparatus can cause the one or more data processing apparatus to perform the operations of process 300.
The MPC system 130 generates a centroid feature vector for each cluster (302). For example, the computing system of the MPC system 130 uses a k-means clustering algorithm to cluster the user profiles into k clusters. In this particular example, MPC 1132 and MPC 2134 train a k-means machine learning model, as described in step 212 of process 200. Training the k-means model requires computing the centroid of the cluster. Since the user profile is an n-dimensional feature vector, the k-means clustering algorithm forms clusters in an n-dimensional feature space and generates an n-dimensional centroid for each cluster.
The MPC system 130 models each cluster using the probability distribution of the user profiles in the cluster (304). For example, the computing system of the MPC system 130 uses step 218 of the process 200 to model the users of each cluster of the k-means machine learning model as a normal distribution.
FIG. 4 is a flow diagram illustrating an example process 400 for generating a k-means machine learning model. The operations of process 400 can be implemented, for example, by MPC system 130 of fig. 1. The operations of process 400 can also be implemented as instructions stored on one or more computer-readable media, which may be non-transitory, and the instructions of the instructions by one or more data processing apparatus can cause the one or more data processing apparatus to perform the operations of process 400.
For example, the application 107 can communicate to the computing system MPC1 a user profile P for it i Is encrypted (e.g., PubKeyEncrypt ([ P ]) i,1 ]MPC 1)). Similarly, application 107 can communicate to computing system MPC2 a user profile P for it i An encrypted second share of the user profile (e.g., PubKeyEncrypt ([ P ]) i,2 ]，MPC2))。
Computing systems MPC 1132 and MPC 2134 create a random projection plane (404). Computing systems MPC 1132 and MPC 2134 are capable of cooperatively creating m random projection planes U ═ U, U 2 …U m }. These random projection planes should be kept as secret shares between the two computing systems MPC 1132 and MPC 2. In some implementations, computing systems MPC 1132 and MPC 2134 create random projection planes and maintain their secrets using Diffie-Hellman key exchange techniques.
As described in more detail below, computing systems MPC 1132 and MPC 2134 project their share of each user profile onto each random projection plane and, for each random projection plane, determine whether the share of the user profile is on one side of the random projection plane. Each computing system MPC 1132 and MPC 2134 can then construct a bit vector in the secret shares from the secret shares of the user profile based on the results of each random projection. Ratio of usersPartial knowledge of eigenvectors, e.g. user profile P i Whether or not in the projection plane U k On one side of (1), allowing computing system MPC 1132 or MPC 2134 to obtain information about P i Some knowledge of the distribution of (a), relative to the user profile P i The prior knowledge of having a unit length is incremental. To prevent computing systems MPC 1132 and MPC 2134 from gaining access to this information (e.g., in embodiments where this is necessary or preferred for user privacy and/or data security), in some embodiments, the random projection plane is in secret shares, so neither computing system MPC 1132 nor MPC 2134 can access the random projection plane in the clear. In other embodiments, a random bit flipping pattern can be applied on the random projection results using a secret sharing algorithm, as described in optional operation 406-408.
To illustrate how the bits are flipped via the secret shares, assume that there are two secrets x and y whose values are zero or one with equal probability. If y is 0, the equality operation [ x ] ═ y ] will flip the bits of x, and if y is 1, the equality operation [ x ] ═ y ] will hold the bits of x. This operation can require a Remote Procedure Call (RPC) between the two computing systems MPC 1132 and MPC 2134, and the number of rounds depends on the data size and the chosen secret sharing algorithm.
Each computing system MPC 1132 and MPC 2134 creates a secret m-dimensional vector (406). Computing system MPC 1132 is capable of creating a secret m-dimensional vector { S } 1 ，S 2 …S m In which each element S i With equal probability having a value of zero or one. Computing system MPC 1132 splits its m-dimensional vector into two shares, the first share { [ S ] 1,1 ]，[S 2,1 ]，…[S m,1 ]A } and a second share { [ S ] 1,2 ]，[S 2,2 ]，…[S m,2 ]}. Computing system MPC 1132 is capable of keeping the first share secret and providing the second share to computing system MPC 2. Computing system MPC 1132 can then discard the m-dimensional vector { S } 1 ，S 2 …S m }。
Computing system MPC 2134 is capable of creating a secret m-dimensional vector { T } 1 ,T 2 …T m Therein, each one ofThe element Ti has a value of zero or one. Computing system MPC 2134 splits its m-dimensional vector into two shares, the first share { [ T { [ 1,1 ],[T 2,1 ],…[T m,1 ]And a second share { [ T ] 1,2 ],[T 2,2 ],…[T m,2 ]}. Computing system MPC2 can keep the first share secret and provide the second share to computing system MPC 1. Computing system MPC 2134 can then discard the m-dimensional vector { T } 1 ,T 2 …T m }。
Two computing systems MPC 1132 and MPC 2134 use secure MPC techniques to compute the share of the bit flipping pattern (408). Computing systems MPC 1132 and MPC 2134 are capable of using a secure share MPC equality test with multiple round trips between computing systems MPC 1132 and MPC 2134 to compute a share of the bit flipping pattern. The bit flipping pattern can be based on the above operation [ x ]]＝＝[y]. That is, the bit flipping pattern can be S 1 ＝＝T 1 ,S 2 ＝＝T 2 …S m ＝＝T m }. Let each ST i ＝(S i ＝＝T i ). Each ST i Have a value of zero or one. After the MPC operation is complete, the computing system MPC 1132 has a first share of the bit flipping pattern { [ ST ] 1,1 ],[ST 2,1 ],…[ST m,1 ]And the computing system MPC 2134 has a second share of the bit flipping pattern { [ ST ] 1,2 ],[ST 2,2 ],…[ST m,2 ]}. Each ST i The fraction of (c) enables the two computing systems MPC 1132 and MPC 2134 to flip bits in the bit vector in a manner that is opaque to either of the two computing systems MPC 1132 and MPC 2134.
Each computing system MPC 1132 and MPC 2134 projects its share of each user profile onto each random projection plane (410). That is, for each user profile for which computing system MPC 1132 receives a share, computing system MPC 1132 can convert the share [ P i,1 ]Projected onto each projection plane U j The above. For each share of the user profile and for each random projection plane U j Performing this operation results in a matrix R of dimension z × m, where z is the number of available user profiles and m is the random projection planeThe number of the cells. Each element R in the matrix R i,j Capable of calculating a projection plane U j And fraction [ P i,1 ]By dot product of, e.g. R i,j ＝sign(U j P i,1 ). The operation |, indicates the dot product of two vectors of equal length.
If bit flipping is used, computing system MPC 1132 is able to modify one or more elements R in the matrix using a bit flipping pattern that is shared in secret between computing system MPC 1132 and MPC 2134 i,j The value of (c). For each element R in the matrix R i,j Computing system MPC 1132 is capable of computing [ ST [ ] j,1 ]＝＝sign(R i,j ) As the element R i,j The value of (c). Thus, the element R i,j Will be in its bit in the bit flip pattern ST j,1 ]Is flipped if the corresponding bit in (b) has a value of zero. This calculation can require multiple RPCs to the computing system MPC 2134.
Similarly, for each user profile for which computing system MPC 2134 receives a share, computing system MPC 2134 can assign a share P i,2 ]Projected onto each projection plane U j The above. For each share of the user profile and for each random projection plane U j Performing this operation produces a matrix R' in dimensions z × m, where z is the number of available user profiles and m is the number of random projection planes. Each element R in the matrix R i,j ' capable of calculating the projection plane U j And fraction [ P i,2 ]By dot product of, e.g. R i，j ’＝U j ⊙[P i，2 ]. The operation |, indicates the dot product of two vectors of equal length.
If bit flipping is used, computing system MPC 2134 can modify one or more elements R in the matrix using a bit flipping pattern that is shared in secret between computing systems MPC 1132 and MPC 2134 i,j The value of (c). For each element R in the matrix R i,j ', the computing system MPC 2134 is capable of computing [ ST j,2 ]＝＝sign(R i,j ') as element R i,j The value of. Thus, the element R i,j ' will be at its bit in the bit flip pattern[ST j,2 ]Is flipped if the corresponding bit in (b) has a value of zero. This calculation can require multiple RPCs to the computing system MPC 1.
Computing systems MPC 1132 and MPC 2134 reconstruct the bit vector (412). Computing systems MPC 1132 and MPC 2134 are capable of reconstructing the bit vector of the user profile based on matrices R and R' having exactly the same size. For example, computing system MPC 1132 can send a portion of the column of matrix R, and computing system MPC 2134 can send the remaining portion of the column of matrix R' to MPC 1. In a particular example, computing system MPC 1132 can send the first half of the matrix R to computing system MPC 2134, and computing system MPC 2134 can send the second half of the matrix R' to MPC 1. Although columns are used for horizontal partitioning in this example and are preferred to protect user privacy, rows can be used for vertical reconstruction in other examples.
In this example, computing system MPC 2134 is capable of combining the first half column of matrix R' with the first half column of matrix R received from computing system MPC 1132 to reconstruct the first half (i.e., m/2 dimensions) of the bit vector in plaintext form. Similarly, computing system MPC 1132 is capable of combining the second half of matrix R with the second half of matrix R' received from computing system MPC 2134 to reconstruct the second half of the bit vector in plaintext (i.e., m/2 dimensions). Conceptually, computing systems MPC 1132 and MPC 2134 have now combined the corresponding shares in the two matrices R and R' to reconstruct the bit matrix B in clear text. This bit matrix B will comprise the bit vectors of the projection results (projected onto each projection plane) of each user profile whose share is received from the application 107 of the machine learning model. Each of the two servers in the MPC system 130 has half of the bit matrix B in plaintext form.
However, if bit flipping is used, computing systems MPC 1132 and MPC 2134 have flipped the bits of the elements in matrices R and R' in a random pattern fixed for the machine learning model. This random bit flipping pattern is opaque to either of the two computing systems MPC 1132 and MPC 2134, so that neither computing system MPC 1132 nor MPC 2134 can infer the original user profile from the bit vector of the projected results. The cryptographic design further prevents MPC 1132 or MPC 2134 from inferring the original user profile by dividing the bit vector horizontally, i.e., computing system MPC 1132 keeps the second half of the bit vector of the projection result in plaintext, while computing system MPC 2134 keeps the first half of the bit vector of the projection result in plaintext.
The MPC system 130 generates a machine learning model (414). Computing systems MPC 1132 and MPC 2134 within MPC system 130 are capable of generating machine learning models using bit vectors corresponding to previously generated user profiles. In some embodiments, if the machine learning model is a k-nn model, each of the two MPC computing systems 132 and 134 are capable of generating a separate k-nn model using the corresponding half of the bit vector. For example, the MPC system 132 can use the second half of the bit vector to generate the k-nn model. In addition, the computing system MPC 2134 is capable of generating a k-nn model using the first half of the bit vector. The generation of a model using bit flipping and horizontal partitioning of a matrix applies the principles of depth defense to protect the privacy of the user profile used to generate the model.
In some implementations, if the machine learning model is a k-means model, the computing system MPC 1132 or MPC 2134 can generate a single k-means model using two halves of the bit vector. For example, computing system MPC 1132 generates a k-means model using two halves of a bit vector. However, in some embodiments, each of computing systems MPC 1132 and MPC 2134 are capable of generating separate k-means models. In general, the k-means model represents cosine similarity (or distance) between user profiles of a set of users. The k-means model generated by either of the computing systems MPC 1132 or MPC 2134 represents the similarity between the bit vectors.
The k-means model generated by the computing system MPC 1132 or MPC 2134 can be referred to as a k-means model, with a unique model identifier as described above. Computing system MPC130 can store the model and the share of the label for each user profile used to generate the model. The application 107 can then query the model to infer the tags of the user group to which the application 107 belongs.
FIG. 5 is a flow diagram illustrating an example process 500 for training and querying a computing system of the MPC system 130. The operations of process 500 can be implemented, for example, by MPC system 130 of FIG. 1. The operations of process 500 can also be implemented as instructions stored on one or more computer-readable media, which can be non-transitory, and execution of the instructions by one or more data processing apparatus can cause the one or more data processing apparatus to perform the operations of process 500.
A first computing system in a multi-party computing (MPC) system 130 receives a query that includes a first share of a given user profile and a second share of the given user profile (502). For example, the application 107 generates two shares of the user profile (e.g., [ P ] i,1 ]、[P i,2 ]) Each share is for one computing system of the MPC system 130. Application 107 encrypts first share [ P ] using a public encryption key of computing system MPC 1132 i,1 ]. Similarly, application 107 encrypts a second share of the user profile [ P ] using the public encryption key of computing system MPC 2134 i,1 ]. The application 107 executing on the client device 106 encrypts a first encrypted share (e.g., PubKeyEncrypt ([ P ]) i,1 ]MPC1)) and a second encrypted share (e.g., PubKeyEncrypt ([ P) i,2 ]MPC2)) to computing system MPC 1132.
The first computing system in the MPC system 130 transmits the second share to the second computing system in the MPC system 130 (504). For example, an application 107 executing on the client device 106 uploads an encrypted share of the user profile to the computing system MPC 1. The application 107 shares the first share of the user profile (e.g., PubKeyEncrypt ([ P ]) i,1 ]MPC1)) and a second share (e.g., PubKeyEncrypt ([ P) i,2 ]MPC2)) to MPC 1. The computing system MPC1 decrypts the first share of the user profile using the private key of the MPC1 and transmits the second share of the user profile to the MPC2 (210). MPC2 decrypts the second share of the user profile using the private key of MPC 2.
The first computing system of the MPC system 130 determines a first label of the first cluster having a centroid closest to the first share (506). For example, after training the k-means machine learning model by MPC1 and MPC2, application 107 transmits a query to the computing system MPC 1132 for the user group tags, the query including the first and second encrypted shares of the user profile. In other examples, application 107 can communicate a query for user group tags to computing system MPC 2134. The application 107 can submit a query for the user group tags in response to a request from the content server to provide the tags for the user group to which the application 107 is assigned. For example, the content server can request that the application 107 query the k-means model to determine the user group tags of the application 107 of the client device 110.
MPC 1132 and MPC 2134 perform the cryptographic protocol described in step 212 of process 200 to base the first part P secret held by MPC1 on i,1 ]And a second part P kept secret by MPC2 i,2 ]A first bit vector is generated. In addition, MPC 1132 and MPC 2134 execute a cryptographic protocol to determine the first cluster and the first label.
The first computing system receives a response including the second label of the selected cluster (508). For example, computing system MPC2 performs the operation noted in step 212 of process 200 and determines the cluster and second label. After determining the second label, computing system MPC 2134 can provide an encrypted version of the second label to computing system MPC1, where the second label is encrypted using the public encryption key of application 107.
FIG. 6 is a block diagram of an example computer system 600 that can be used to perform the operations described above. The system 600 includes a processor 610, a memory 620, a storage device 630, and an input/output device 640. Each of the components 610, 620, 630, and 640 can be interconnected, for example, using a system bus 650. The processor 610 is capable of processing instructions for execution within the system 600. In some implementations, the processor 610 is a single-threaded processor. In another implementation, the processor 610 is a multi-threaded processor. The processor 610 is capable of processing instructions stored in the memory 620 or the storage device 630.
The storage device 630 is capable of providing mass storage for the system 600. In some implementations, the storage device 630 is a computer-readable medium. In various different implementations, the storage device 630 can include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices over a network (e.g., a cloud storage device), or some other mass storage device.
Input/output device 640 provides input/output operations for system 600. In some implementations, the input/output devices 640 can include one or more of the following: a network interface device, such as an ethernet card, a serial communication device, such as an RS-232 port, and/or a wireless interface device, such as an 802.11 card. In another embodiment, the input/output devices can include driver devices configured to receive input data and transmit output data to external devices 660 (e.g., keyboard, printer, and display devices). However, other implementations can also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 6, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium (or media) for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage medium can be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium can also be or be included in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification can be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or a plurality or combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment are capable of implementing a variety of different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Further, the computer can be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices can also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. Further, the computer is able to interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data (e.g., an HTML page) to the client device (e.g., for displaying data to a user interacting with the client device and receiving user input from the user). Data generated at the client device (e.g., a result of the user interaction) can be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (20)
1. A computer-implemented method, comprising:
receiving, by a first computing system of a multi-party computing MPC system, a query comprising a first share of a given user profile and a second share of the given user profile, wherein the second share is encrypted with a key that prevents the first computing system from accessing the second share;
transmitting, by the first computing system, the second share to a second computing system of the MPC;
determining, by the first computing system, a first label of a first cluster having a centroid closest to the first share, wherein the first cluster is one of a plurality of clusters generated by a machine learning model trained by the first computing system and the second computing system;
receiving, by the first computing system from the second computing system of the MPC, a response comprising a second tag of a second cluster;
responding to the query with a response comprising the first tag and the second tag.
2. The computer-implemented method of claim 1, further comprising:
receiving, by the first computing system, a first plurality of partial shares of a user profile from a digital component distribution system different from the MPC system;
receiving, by the second computing system from the digital component distribution system, a second plurality of partial shares of a user profile, wherein neither the first plurality of portions nor the second plurality of portions are shared for an individual user, wherein the first plurality of shares and the second plurality of shares are secret shares of all dimensions of the user profile that include the individual user;
training, by the first computing system and the second computing system, the machine learning model using the first plurality of partial shares and the second plurality of partial shares.
3. The computer-implemented method of claim 2, wherein training the machine learning model comprises training a clustering model to create a plurality of clusters of user profiles based on the first plurality of partial shares and the second plurality of partial shares.
4. The computer-implemented method of claim 3, further comprising:
generating, by the MPC system, a centroid feature vector for each cluster from the plurality of clusters;
modeling, by the MPC system, the clusters using the probability distributions of the user profiles in each cluster;
generating, by the MPC system, for each cluster, a new centroid feature vector based on the probability distribution and the centroid feature vector of the corresponding cluster;
sharing, by the MPC computing system, the new centroid feature vector to the digital component distribution system.
5. The computer-implemented method of claim 1, further comprising:
splitting, by a client device, the given user profile into the first share and the second share;
generating the query as a request for a label of a cluster corresponding to the given user profile and transmitting the query to the first computing system;
receiving, by the client device, the response including the first tag and the second tag;
storing, by the client device, a device final label generated based on the first label and the second label.
6. The computer-implemented method of claim 5, wherein generating, by the client device, the final label comprises:
modeling, by the first computing system and the second computing system, the user profiles of the first cluster and the second cluster as a normal distribution;
determining, by the first computing system and the second computing system, parameters of the normal distribution including the centroid and covariance matrices;
generating, by both the first computing system and the second computing system, a first share and a second share of the final label;
transmitting, by the MPC system, the first and second shares of the final label to the client device;
reconstructing, by the client device, the final label using the first and second shares of the final label.
7. The computer-implemented method of claim 5, wherein determining, by the first and second computing systems, the covariance matrix comprises determining, by the first and second computing systems, an integer matrix such that the matrix, when multiplied by its transpose, generates the covariance matrix.
8. A system, comprising:
receiving, by a first computing system of a multi-party computing MPC system, a query comprising a first share of a given user profile and a second share of the given user profile, wherein the second share is encrypted with a key that prevents the first computing system from accessing the second share;
transmitting, by the first computing system, the second share to a second computing system of the MPC;
determining, by the first computing system, a first label of a first cluster having a centroid closest to the first share, wherein the first cluster is one of a plurality of clusters generated by a machine learning model trained by the first computing system and the second computing system;
receiving, by the first computing system from the second computing system of the MPC, a response comprising a second tag of a second cluster;
responding to the query with a response comprising the first tag and the second tag.
9. The system of claim 8, further comprising:
receiving, by the first computing system, a first plurality of partial shares of a user profile from a digital component distribution system different from the MPC system;
receiving, by the second computing system from the digital component distribution system, a second plurality of partial shares of a user profile, wherein neither the first plurality of portions nor the second plurality of portions are shared for an individual user, wherein the first plurality of shares and the second plurality of shares are secret shares of all dimensions of the user profile that include the individual user;
training, by the first computing system and the second computing system, the machine learning model using the first plurality of partial shares and the second plurality of partial shares.
10. The system of claim 9, wherein training the machine learning model comprises training a clustering model to create a plurality of clusters of user profiles based on the first plurality of partial shares and the second plurality of partial shares.
11. The system of claim 10, further comprising:
generating, by the MPC system, a centroid feature vector for each cluster from the plurality of clusters;
modeling, by the MPC system, the clusters using the probability distributions of the user profiles in each cluster;
generating, by the MPC system, for each cluster, a new centroid feature vector based on the probability distribution and the centroid feature vector of the corresponding cluster;
sharing, by the MPC computing system, the new centroid feature vector to the digital component distribution system.
12. The system of claim 8, further comprising:
splitting, by a client device, the given user profile into the first share and the second share;
generating the query as a request for a label of a cluster corresponding to the given user profile and transmitting the query to the first computing system;
receiving, by the client device, the response including the first tag and the second tag;
storing, by the client device, a device final label generated based on the first label and the second label.
13. The system of claim 12, wherein generating, by the client device, the final label comprises:
modeling, by the first computing system and the second computing system, the user profiles of the first cluster and the second cluster as a normal distribution;
determining, by the first computing system and the second computing system, parameters of the normal distribution including the centroid and covariance matrices;
generating, by both the first computing system and the second computing system, a first share and a second share of the final label;
transmitting, by the MPC system, the first and second shares of the final label to the client device;
reconstructing, by the client device, the final label using the first and second shares of the final label.
14. The system of claim 12, wherein determining, by the first and second computing systems, the covariance matrix comprises determining, by the first and second computing systems, an integer matrix such that the matrix, when multiplied by its transpose, generates the covariance matrix.
15. A non-transitory computer-readable medium storing instructions that, when executed by one or more data processing apparatus, cause the one or more data processing apparatus to perform operations comprising:
receiving, by a first computing system of a multi-party computing MPC system, a query comprising a first share of a given user profile and a second share of the given user profile, wherein the second share is encrypted with a key that prevents the first computing system from accessing the second share;
transmitting, by the first computing system, the second share to a second computing system of the MPC;
determining, by the first computing system, a first label of a first cluster having a centroid closest to the first share, wherein the first cluster is one of a plurality of clusters generated by a machine learning model trained by the first computing system and the second computing system;
receiving, by the first computing system from the second computing system of the MPC, a response comprising a second tag of a second cluster;
responding to the query with a response comprising the first tag and the second tag.
16. The non-transitory computer-readable medium of claim 15, further comprising:
receiving, by the first computing system, a first plurality of partial shares of a user profile from a digital component distribution system different from the MPC system;
receiving, by the second computing system from the digital component distribution system, a second plurality of partial shares of a user profile, wherein neither the first plurality of portions nor the second plurality of portions are shared for an individual user, wherein the first plurality of shares and the second plurality of shares are secret shares of all dimensions of the user profile that include the individual user;
training, by the first computing system and the second computing system, the machine learning model using the first plurality of partial shares and the second plurality of partial shares.
17. The non-transitory computer-readable medium of claim 16, wherein training the machine learning model comprises training a clustering model to create a plurality of clusters of user profiles based on the first plurality of partial shares and the second plurality of partial shares.
18. The non-transitory computer-readable medium of claim 17, further comprising:
generating, by the MPC system, a centroid feature vector for each cluster from the plurality of clusters;
modeling, by the MPC system, the clusters using the probability distributions of the user profiles in each cluster;
generating, by the MPC system, for each cluster, a new centroid feature vector based on the probability distribution and the centroid feature vector of the corresponding cluster;
sharing, by the MPC computing system, the new centroid feature vector to the digital component distribution system.
19. The non-transitory computer-readable medium of claim 15, further comprising:
splitting, by a client device, the given user profile into the first share and the second share;
generating the query as a request for a label of a cluster corresponding to the given user profile and transmitting the query to the first computing system;
receiving, by the client device, the response including the first tag and the second tag;
storing, by the client device, a device final label generated based on the first label and the second label.
20. The non-transitory computer-readable medium of claim 19, wherein generating, by the client device, the final label comprises:
modeling, by the first computing system and the second computing system, the user profiles of the first cluster and the second cluster as a normal distribution;
determining, by the first computing system and the second computing system, parameters of the normal distribution including the centroid and covariance matrices;
generating, by both the first computing system and the second computing system, a first share and a second share of the final label;
transmitting, by the MPC system, the first and second shares of the final label to the client device;
reconstructing, by the client device, the final label using the first and second shares of the final label.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
IL280057 | 2021-01-10 | ||
IL280057A IL280057A (en) | 2021-01-10 | 2021-01-10 | Privacy preserving machine learning labelling |
PCT/US2021/063964 WO2022150171A1 (en) | 2021-01-10 | 2021-12-17 | Privacy preserving machine learning labelling |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115087979A true CN115087979A (en) | 2022-09-20 |
Family
ID=80113448
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180008300.2A Pending CN115087979A (en) | 2021-01-10 | 2021-12-17 | Privacy preserving machine learning tags |
Country Status (7)
Country | Link |
---|---|
US (1) | US20230078704A1 (en) |
EP (1) | EP4066141A1 (en) |
JP (1) | JP7457131B2 (en) |
KR (1) | KR20220108170A (en) |
CN (1) | CN115087979A (en) |
IL (1) | IL280057A (en) |
WO (1) | WO2022150171A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN115348541A (en) * | 2022-10-18 | 2022-11-15 | 北京融数联智科技有限公司 | Method and system for determining space distance between terminals |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP4328779A1 (en) * | 2022-08-26 | 2024-02-28 | Siemens Healthineers AG | Structuring data for privacy risks assessments |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6034927B1 (en) | 2015-07-27 | 2016-11-30 | 日本電信電話株式会社 | Secret calculation system, secret calculation device, and program |
EP4220464A1 (en) | 2017-03-22 | 2023-08-02 | Visa International Service Association | Privacy-preserving machine learning |
CN111563265A (en) | 2020-04-27 | 2020-08-21 | 电子科技大学 | Distributed deep learning method based on privacy protection |
CN111563261A (en) | 2020-05-15 | 2020-08-21 | 支付宝(杭州)信息技术有限公司 | Privacy protection multi-party computing method and system based on trusted execution environment |
-
2021
- 2021-01-10 IL IL280057A patent/IL280057A/en unknown
- 2021-12-17 US US17/795,131 patent/US20230078704A1/en active Pending
- 2021-12-17 WO PCT/US2021/063964 patent/WO2022150171A1/en active Application Filing
- 2021-12-17 JP JP2022542251A patent/JP7457131B2/en active Active
- 2021-12-17 EP EP21847826.1A patent/EP4066141A1/en active Pending
- 2021-12-17 KR KR1020227023477A patent/KR20220108170A/en unknown
- 2021-12-17 CN CN202180008300.2A patent/CN115087979A/en active Pending
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN115348541A (en) * | 2022-10-18 | 2022-11-15 | 北京融数联智科技有限公司 | Method and system for determining space distance between terminals |
Also Published As
Publication number | Publication date |
---|---|
JP2023514039A (en) | 2023-04-05 |
EP4066141A1 (en) | 2022-10-05 |
IL280057A (en) | 2022-08-01 |
JP7457131B2 (en) | 2024-03-27 |
WO2022150171A1 (en) | 2022-07-14 |
KR20220108170A (en) | 2022-08-02 |
US20230078704A1 (en) | 2023-03-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20230214684A1 (en) | Privacy preserving machine learning using secure multi-party computation | |
JP7361928B2 (en) | Privacy-preserving machine learning via gradient boosting | |
US20230078704A1 (en) | Privacy preserving machine learning labelling | |
US20220311754A1 (en) | Generating bridge match identifiers for linking identifers from server logs | |
US11843672B2 (en) | Privacy preserving centroid models using secure multi-party computation | |
JP7155437B2 (en) | Aggregation of Encrypted Network Values | |
JP7422892B2 (en) | Processing machine learning modeling data to improve classification accuracy | |
JP7471445B2 (en) | Privacy-preserving machine learning for content delivery and analytics | |
JP2023089216A (en) | Secured management of data distribution restriction | |
US20220405407A1 (en) | Privacy preserving cross-domain machine learning | |
JP7354427B2 (en) | Online privacy protection techniques | |
US20240163341A1 (en) | Privacy preserving centroid models using secure multi-party computation | |
KR20230062474A (en) | Localized encryption technology for privacy protection |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |