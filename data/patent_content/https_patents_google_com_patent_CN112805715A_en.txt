CN112805715A - Identifying entity attribute relationships - Google Patents
Identifying entity attribute relationships Download PDFInfo
- Publication number
- CN112805715A CN112805715A CN202080005433.XA CN202080005433A CN112805715A CN 112805715 A CN112805715 A CN 112805715A CN 202080005433 A CN202080005433 A CN 202080005433A CN 112805715 A CN112805715 A CN 112805715A
- Authority
- CN
- China
- Prior art keywords
- attribute
- entity
- embedding
- generating
- vector representation
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 28
- 239000013598 vector Substances 0.000 claims description 111
- 238000012545 processing Methods 0.000 claims description 25
- 238000004590 computer program Methods 0.000 abstract description 14
- 238000013145 classification model Methods 0.000 description 14
- 230000008569 process Effects 0.000 description 13
- 238000004891 communication Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 238000010801 machine learning Methods 0.000 description 5
- 230000008901 benefit Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 238000003058 natural language processing Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 238000011156 evaluation Methods 0.000 description 2
- 230000008520 organization Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000012549 training Methods 0.000 description 2
- 241000270322 Lepidosauria Species 0.000 description 1
- 241000270295 Serpentes Species 0.000 description 1
- 230000036531 allelopathy Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/36—Creation of semantic tools, e.g. ontology or thesauri
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
- G06F40/295—Named entity recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, facilitate identifying entity-attribute relationships in a corpus of text. The method includes determining whether an attribute in the candidate entity-attribute pair is an actual attribute of an entity in the entity-attribute candidate pair. This may include generating embeddings and generations for words in a sentence that includes entities and attributes. This may also include generating attribute distribution inlays for the entities based on other attributes associated with the entities, and generating attribute distribution inlays for the attributes based on known attributes associated with known entities of the attributes. Based on these embeddings, the feed-forward network may determine whether an attribute in an entity-attribute candidate pair is an actual attribute of an entity in the entity-attribute candidate pair.
Description
Cross Reference to Related Applications
This application is an international application and claims the benefit of U.S. application No. 16/504,068 filed on 5.7.2019. The disclosures of the aforementioned applications are incorporated herein by reference in their entirety.
Background
This specification relates to identifying entity-attribute relationships in a corpus of text.
Search-based applications (e.g., search engines, knowledge bases) are intended to identify resources (e.g., web pages, images, text documents, and multimedia content) that are relevant to a user's informational needs and present information about the resources in a manner that is most useful to the user. One way in which a search-based application can present information about identified resources is in the form of structured search results. Structured search results typically present a list of attributes with answers to entities specified in a user request (e.g., a query). For example, in response to a query for "Kevin Durand", the structured search results may include attributes about "Kevin Durand" (such as salary, team, year of birth, family, etc.), and answers that provide information about these attributes.
Building such structured search results typically requires identifying entity-attribute relationships. Entity-attribute relationships are a special case of textual relationships between pairs of terms. The first term in the pair of terms is an entity, which may be a person, a place, an organization, a concept, etc. The second term in the term pair is an attribute, which is a string of characters that describes some aspect or characteristic of an entity. Examples of attributes may include a person's "date of birth," a country's "population," a player's "salary," or an organization's "CEO.
However, often a large amount of data must be processed in order to identify suitable search results. This may use a significant amount of processing power. In addition, the need to process such large amounts of data may result in slow results or compromise of the processing such that the results are of poor quality.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in a method that includes the actions of: obtaining entity-attribute candidate pairs defining entities and attributes, wherein the attributes are candidate attributes of the entities; determining whether the attribute is an actual attribute of an entity in the entity-attribute candidate pair based on a sentence set comprising the entity and the attribute, the determining comprising: generating an embedding for a word in a sentence set comprising an entity and an attribute; generating an attribute distribution embedding for the entity using the known entity-attribute pair, wherein the attribute distribution embedding for the entity specifies the embedding for the entity based on other attributes associated with the entity from the known entity-attribute pair; generating an attribute distribution embedding for the attribute using the known entity-attribute pair, wherein the attribute distribution embedding for the attribute specifies an embedding for the attribute based on the known attribute associated with the known entity of the attribute in the known entity-attribute pair; based on the embedding for the words in the sentence set, the attribute distribution embedding for the entities, and the attribute distribution embedding for the attributes, it is determined whether the attributes in the entity-attribute candidate pairs are actual attributes of the entities in the entity-attribute candidate pairs. Other embodiments of this aspect include corresponding systems, apparatus, devices, and computer programs configured to perform the actions of the methods. A computer program (e.g., instructions) may be encoded on a computer storage device. These and other embodiments may each optionally include one or more of the following features.
In some implementations, generating the embedding for the words in the sentence set that includes the entity and the attribute includes: generating a first embedded vector representation specifying words between entities and attributes in the sentence set; generating a second vector representation specifying a second embedding for the entity based on the set of sentences; and generating a third vector representation specifying a third embedding for the attribute based on the set of sentences.
In some implementations, generating the attribute distribution embedding for the entity using the known entity-attribute pairs includes: a fourth vector representation is generated that specifies attribute distribution embedding for the entity using known entity-attribute pairs.
In some implementations, generating the attribute distribution embedding for the attribute using the known entity-attribute pair includes: a fifth vector representation is generated that specifies attribute distribution embedding for the attributes using known entity-attribute pairs.
In some implementations, determining whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair based on the embedding for the word in the sentence set, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute includes: determining whether an attribute in the entity-attribute candidate pair is an actual attribute of an entity in the entity-attribute candidate pair based on the first vector representation, the second vector representation, the third vector representation, the fourth vector representation, and the fifth vector representation.
In some embodiments, the following is performed using a feed-forward network: determining whether an attribute in the entity-attribute candidate pair is an actual attribute of an entity in the entity-attribute candidate pair based on the first vector representation, the second vector representation, the third vector representation, the fourth vector representation, and the fifth vector representation.
In some embodiments, determining whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair based on the first vector representation, the second vector representation, the third vector representation, the fourth vector representation, and the fifth vector representation comprises: generating a single vector representation by concatenating the first, second, third, fourth and fifth vector representations; inputting the single vector representation into a feed-forward network; and determining whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair using the single vector representation over the feed-forward network.
In some implementations, generating the fourth vector representation that specifies attribute distribution embedding for the entity using known entity-attribute pairs includes: identifying a set of attributes associated with an entity in a known entity-attribute pair, wherein the set of attributes does not include the attribute; and generating an attribute distribution embedding for the entity by computing a weighted sum of the attributes in the set of attributes.
In some implementations, generating the fifth vector representation specifying attribute distribution embedding for the attributes using known entity-attribute pairs includes: identifying a set of entities from the known entity-attribute pairs using the attribute; for each entity in the set of entities, identifying a set of attributes associated with the entity, wherein the set of attributes does not include the attribute; and generating an attribute distribution embedding for the attributes by computing a weighted sum of the attributes in the attribute set.
Particular embodiments of the subject matter described in this specification can be implemented to achieve the advantage of identifying more accurate entity-attribute relationships as compared to prior art model-based entity-attribute identification techniques. Prior art entity-attribute identification techniques use various model-based methods (e.g., Natural Language Processing (NLP) features, distance surveillance, and traditional machine learning models) that identify entity-attribute relationships by representing entities and attributes based on data (e.g., sentences) in which these terms occur. In contrast, the innovations described in this specification identify entity-attribute relationships in datasets by not only using information about how entities and attributes are described in the data in which these terms appear, but also by representing entities and attributes using other attributes known to be associated with these terms. This enables entities and attributes to be represented with attributes shared by similar entities, thereby improving the accuracy of identifying entity-attribute relationships that would otherwise not be discernable by simply considering the sentences in which these terms appear. Therefore, when the user performs a search, more useful information can be obtained. In particular, a broader range of search terms will result in more relevant information being identified. Therefore, the search does not necessarily need to be repeated. Thus, less processing may be required to obtain the desired search results.
For example, consider a scenario in which a data set includes: a sentence having two entities "Ronaldo (raldol)" and "Messi (meisi)" described using a "record" attribute; and wherein the entity "Messi (meisi)" is a sentence described using a "goals" attribute. In this scenario, the prior art can identify the following entity attribute pairs: (Ronaldo, record), (Messi, record), and (Messi, goals (meisi, goal). The innovations described in this specification go beyond these prior art approaches by identifying entity-attribute relationships that may not be readily discernable by how these terms are used in the data set. Using the example above, the innovation described in this specification determines that "Ronaldo" and "Messi" are similar entities in that they share a "record" attribute, and then represent the "record" attribute using a "goals" attribute. In this way, innovations such as those described in this specification may enable identification of entity-attribute relationships, such as (Cristiano, Goals, number of Goals), even though such relationships may not be readily discernable from a data set.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example environment for extracting entity-attribute relationships.
FIG. 2 is a flow diagram of an example process for identifying entity-attribute relationships.
FIG. 3 is a block diagram of an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification relates to identifying entity-attribute relationships in a corpus of text.
As further described in this specification, candidate entity-attribute pairs (where an attribute is a candidate attribute for an entity) are input to a classification model. A classification model including a path embedding engine, a distribution representation engine, an attribute distribution engine, and a feed-forward network determines whether an attribute in a candidate entity-attribute pair is an actual attribute of an entity in the candidate entity-attribute pair.
The path embedding engine generates a vector that represents the embedding of a path or word that connects the joint occurrences of entities and attributes in a set of sentences (e.g., 30 or more sentences) of the dataset. The distribution representation engine generates vectors representing the embedding for the entity terms and the attribute terms based on the context in which these terms appear in the sentence set. The attribute distribution engine generates a vector representing the embedding for the entity and another vector representing the embedding for the attribute. The embedding of entities by the attribute distribution engine is based on other attributes (i.e., attributes other than the candidate attributes) known to be associated with the entities in the dataset. The embedding of the attribute by the attribute distribution engine is based on other attributes of the candidate attributes that are associated with the known entity.
The classification model concatenates vector representations from the path embedding engine, the distribution representation engine, and the attribute distribution engine into a single vector representation. Since a single vector representation is used for these concatenated vectors, less data needs to be stored. The classification model then inputs the single vector representation into a feed-forward network that uses the single vector representation to determine whether an attribute in the candidate entity-attribute pair is an actual attribute of an entity in the candidate entity-attribute pair. If the feed-forward network determines that an attribute in a candidate entity-attribute pair is an actual attribute of an entity in the candidate entity-attribute pair, the candidate entity-attribute pair is stored in the knowledge base along with other known/actual entity-attribute pairs.
These and additional features are described in more detail below with reference to fig. 1-3.
FIG. 1 is a block diagram of an example environment for extracting entity attribute relationships. The environment 100 includes a classification model 114 that determines, for candidate entity-attribute pairs in the knowledge base 104, whether an attribute in the candidate entity-attribute pair is an actual attribute of an entity in the candidate pair. In some embodiments, the classification model 114 is a neural network model, the components/engines of which (and the corresponding operations of which) are described below. It should be appreciated that other types of supervised and/or unsupervised machine learning models may also be used to implement the classification model 114.
The repository 104, which may include one or more databases (or other suitable data storage structures) stored in one or more non-transitory data storage media (e.g., hard drives, flash memory, etc.), stores a set of candidate entity-attribute pairs. Candidate entity-attribute pairs may be obtained using a collection of content in the form of text documents (e.g., web pages, news articles, etc.) obtained from a data source 102, which data source 102 may include any content source, such as a news website, a data aggregator platform, a social media platform, and so forth. In some implementations, the data source 102 obtains news articles from the data aggregator platform. In some implementations, the data source 102 can use a model (e.g., a supervised or unsupervised machine learning model, a natural language processing model) to generate the set of candidate entity-attribute pairs by: sentences are extracted from the articles, and the extracted sentences are tokenized and labeled (e.g., as entities and attributes) using part-of-speech tags and dependency parse tree tags. In some implementations, the data source 102 can input the extracted sentences into a machine learning model that can be trained, for example, using a set of training sentences and their associated entity-attribute pairs. Such a machine learning model may then output candidate entity-attribute pairs for the input extracted sentence.
The data source 102 stores the candidate entity-attribute pairs in the knowledge base 104 along with sentences extracted by the data source 102 that include words of the candidate entity-attribute pairs. In some implementations, candidate entity-attribute pairs are stored in the knowledge base 104 only when the number of sentences in which the entities and attributes are present meets (e.g., meets or exceeds) a threshold number of sentences (e.g., 30 sentences). Therefore, the amount of stored data is reduced by setting a threshold value for determining whether to store data.
The classification model 114 determines whether an attribute in a candidate entity-attribute pair (stored in the knowledge base 104) is an actual attribute of an entity in the candidate entity-attribute pair. The classification model 114 includes a path embedding engine 106, a distribution representation engine 108, an attribute distribution engine 110, and a feed-forward network 112. As used herein, the term engine refers to a data processing device that performs a set of tasks. The operation of each of these engines of the classification model 114 in determining whether an attribute in a candidate entity-attribute pair is an actual attribute of an entity is described with reference to fig. 2.
FIG. 2 is a flow diagram of an example process 200 for identifying entity attribute relationships. The operations of process 200 are described below as being performed by the components of the system described and depicted in fig. 1. The operation of process 200 is described below for illustrative purposes only. The operations of process 200 may be performed by any suitable device or system (e.g., any suitable data processing apparatus). The operations of process 200 may also be implemented as instructions stored on a non-transitory computer-readable medium. Execution of the instructions causes one or more data processing devices to perform the operations of process 200.
The repository 104 obtains entity-attribute candidate pairs from the data source 104, as described with reference to fig. 1 (at 202).
The knowledge base 104 obtains (at 204) from the data source 102 a set of sentences that include words of entities and attributes in candidate entity-attribute pairs, as described above with reference to fig. 1.
The classification model 114 determines whether a candidate attribute is an actual attribute of a candidate entity based on the set of sentences and the candidate entity-attribute pairs. In some implementations, the set of sentences can be a large number of sentences, e.g., 30 or more sentences. The classification model makes this determination by performing the following operations: (1) generating an embedding for a word in a sentence set comprising entities and attributes, which is described in more detail below with reference to operations 206, 208, and 210; (2) generating attribute distribution inlays for the entities using known entity-attribute pairs, which are described in more detail below with reference to operation 212; (3) generating attribute distribution embedding for the attributes using known entity-attribute pairs, which is described in more detail below with reference to operation 214; and (4) determining whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair based on the embedding for the word in the sentence set, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute, which is described in more detail below with reference to operation 216. Operations 206 through 216 are described below.
The path embedding engine 106 generates a first vector representation specifying a first embedding of words between entities and attributes in the sentence set (at 206). The path embedding engine 106 detects relationships between candidate entity-attribute terms by embedding paths or words that connect the joint occurrences of these terms in the sentence collection. For example, for the phrase "snake is a reptile," the path embedding engine 106 generates an embedding for the path "is a," which can then be used to detect, for example, an allelopathy, which can then be used to identify other entity-attribute pairs. By generating such paths, reduced processing is required to perform the analysis and detection of such terms.
The path embedding engine 106 performs the following operations to generate an embedding of words between entities and attributes in the sentence set. For each sentence in the sentence set, the path embedding engine 106 first extracts a dependency path (which specifies a set of words) between the entity and the attribute. The path embedding engine 106 converts the sentence from a string to a list, where the first term is an entity and the last term is an attribute (or alternatively, the first term is an attribute and the last term is an entity). Each term in the dependent path (which is also referred to as an edge) is represented using the following features: lemma of terms, part-of-speech tag, dependency label, and direction of dependency path (left, right, or root). Each of these features is embedded and concatenated to produce a vector representation (V) of the term or edgee) Comprising a sequence of vectors (V)1、Vpos、Vdep、Vdir) As shown by the following equation:
the path embedding engine 106 then inputs the vector sequence of terms or edges in each path into a long-short term memory (LSTM) network, which produces a single vector representation (V) of the sentence in questions) As shown by the following equation:
this single vector representation reduces the amount of data that needs to be stored, processed or transmitted. This is because a single vector uses much less data than is required for all vectors represented by a single vector.
Finally, the path embedding engine 106 inputs a single vector representation for all sentences in the set of sentences into the attention mechanism, which determines the sentence representation (V)sents(e、a)) As shown by the following equation:
the distributed representation engine 108 generates a second vector representation for the entity and a third vector representation for the attribute based on the set of sentences (at 208 and 210). The distribution representation engine 108 detects relationships between candidate entity-attribute terms based on attributes of the candidate entity-attribute pairs and the context in which the entity occurs in the sentence set. The vector-based approach to this processing reduces the amount of processing and thus increases the speed at which results are obtained. For example, the distribution representation engine 108 may determine that the entity "New York (New York)" is used in the sentence collection in a manner that indicates that the entity refers to a city or state in the United states. As another example, the distribution representation engine 108 may determine that the attribute "capital" is used in the sentence collection in a manner that indicates that the attribute refers to a significant city of a state or country. Thus, the distributed representation engine 108 uses the context (i.e., the set of sentences) in which the entity appears) Generating a vector representation (V) specifying embedding for an entitye). Similarly, the distributed representation engine 108 uses the set of sentences in which the attribute occurs to generate an embedded vector representation (V) that specifies the attribute fora)。
The attribute distribution engine 110 generates a fourth vector representation (at 212) that specifies attribute distribution embedding for the entity using the known entity-attribute pairs. Known entity-attribute pairs stored in the repository 104 are entity-attribute pairs for which each attribute of the entity-attribute pair has been confirmed (e.g., using prior processing or based on manual evaluation by the classification model 114) as an actual attribute of an entity in the entity-attribute pair.
In some implementations, the attribute distribution engine 110 performs the following operations to determine an attribute distribution embedding that specifies an embedding for an entity using some (e.g., most common) or all of the other known attributes among known entity-attribute pairs associated with the entity. For entities in entity-attribute candidate pairs, the attribute distribution engine 110 identifies other attributes (i.e., attributes other than those included in entity-attribute candidate pairs) associated with entities in known entity-attribute pairs. For example, for entity "Michael Jordan" in a candidate entity-attribute pair (Michael Jordan, famous), the attribute distribution engine 110 may use known entity-attribute pairs related to Michael Jordan (Michael Jordan), such as (Michael Jordan, rich)) and (Michael Jordan, record), to identify attributes such as welithy (rich) and record.
The attribute distribution engine 110 then generates embeddings for the entities by computing a weighted sum of the identified known attributes (as described in the previous paragraph), where the weights are learned using/through an attention mechanism, as shown in the following equation:
the attribute distribution engine 110 generates a fifth vector representation (at 214) that specifies attribute distribution embedding for the attributes using the known entity-attribute pairs. In some implementations, the attribute distribution engine 110 performs the following operations to determine a representation for an attribute based on some (e.g., the most common) or all of the known attributes associated with known entities of a candidate attribute. For an attribute in an entity-attribute candidate pair, the attribute distribution engine 110 identifies a known entity among the known entity-attribute pairs having the attribute. For each identified known entity, the attribute distribution engine 110 identifies other attributes associated with the entity in the known entity-attribute pair (i.e., attributes other than those included in the entity-attribute candidate pair). In some implementations, the attribute distribution engine 110 can identify a subset of the attributes from the identified attributes by: (1) ranking the attributes based on the number of known entities associated with each attribute (e.g., assigning a higher ranking to attributes associated with a higher number of entities than to attributes associated with fewer entities); and (2) then select a threshold number of attributes based on the ranking (e.g., select the top five attributes).
The attribute distribution engine 110 then generates embeddings for the attributes by computing a weighted sum of (all or a subset of) the identified known attributes (as described in the previous paragraph), where the weights are learned using/through an attention mechanism, as shown in the following equation:
it should be appreciated that the embeddings generated by operations 206-214 are typically generated in parallel by the respective engines (106, 108, 110).
The feed-forward network 112 determines whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair based on the vector representation (at 216). In some implementations, the feed-forward network 112 concatenates each of the vector representations output by the path embedding engine 106, the distribution representation engine 108, and the attribute distribution engine 110 into a single vector representation (V(e、a)) As shown by the following equation:
using the single vector representation of the input, the feed-forward network 112 outputs whether the attribute in the candidate entity-attribute pair is the actual attribute of the entity in the candidate entity-attribute pair. In some embodiments, the output of the feedforward network 112 may be binary. For example, the feed-forward network 112 may output "yes" when the attribute in the candidate entity-attribute pair is the actual attribute of the entity in the candidate entity-attribute pair, and the feed-forward network 112 may output "no" when the attribute in the candidate entity-attribute pair is not the actual attribute of the entity in the candidate entity-attribute pair. To produce such a binary output, the amount of processing required to obtain such a result is reduced due to the various simplifications made by means of the above-described process. In some implementations, the output of the feed-forward network 112 can be a confidence value (e.g., a value ranging from 0 to 1), where 0 specifies that the attribute in the candidate entity-attribute pair is not an actual attribute of an entity in the candidate entity-attribute pair, and 1 specifies that the attribute in the candidate entity-attribute pair is an actual attribute of an entity in the candidate entity-attribute pair.
In some embodiments, remote supervision is used to train the feed-forward network 112. Training uses the classification model 114 to perform the above-described process on candidate entity-attribute pairs that have been identified as true pairs, i.e., whose attributes in the candidate entity-attribute pair have been identified (e.g., based on human evaluation or previous processing by the feed-forward network 112) as actual attributes of entities in the candidate entity-attribute pair.
If the output of the feed-forward network 112 specifies that an attribute in the entity-attribute candidate pair is an actual attribute (e.g., if the feed-forward network 112 outputs a "yes" indicator as described above) or has a high likelihood (e.g., the feed-forward network 112 outputs a confidence value that meets or exceeds a certain threshold, such as 0.8, as described above), the feed-forward network 112 stores the entity-attribute candidate pair as an actual entity-attribute pair in the knowledge base 104. This provides certainty of the stored data. In addition, it ensures that only relevant data is stored, which in turn improves the quality of the stored data and reduces the total amount of data stored.
FIG. 3 is a block diagram of an example computer system 300 that may be used to perform the operations described above. The system 300 includes a processor 310, a memory 320, a storage device 330, and an input/output device 340. Each of the components 310, 320, 330, and 340 may be interconnected, for example, using a system bus 350. The processor 310 is capable of processing instructions for execution within the system 300. In one implementation, the processor 310 is a single-threaded processor. In another implementation, the processor 310 is a multi-threaded processor. The processor 310 is capable of processing instructions stored in the memory 320 or on the storage device 330.
The storage device 330 is capable of providing mass storage for the system 300. In some implementations, the storage device 330 is a computer-readable medium. In various different embodiments, storage device 330 may comprise, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices (e.g., cloud storage devices) over a network, or some other mass storage device.
The input/output device 340 provides input/output operations for the system 300. In some implementations, the input/output devices 340 can include one or more network interface devices (e.g., an ethernet card), serial communication devices (e.g., an RS-232 port), and/or wireless interface devices (e.g., an 802.11 card). In another embodiment, the input/output devices may include driver devices configured to receive input data and to send output data to other input/output devices, such as a keyboard, a printer, and a display device 360. However, other implementations may also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and so forth.
Although an example processing system has been described in fig. 3, implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium (or multiple media) for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Further, although the computer storage medium is not a propagated signal, the computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium may also be or be included in one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
The operations described in this specification may be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple programmable processors, computers, systems on a chip, or a combination of the foregoing. An apparatus may comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with the instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Further, the computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending documents to and receiving documents from the device used by the user; for example, by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), an internetwork (e.g., the internet), and a peer-to-peer network (e.g., an ad hoc peer-to-peer network).
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data (e.g., an HTML page) to the client device (e.g., for purposes of displaying data to a user interacting with the client device and receiving user input from the user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (24)
1. A computer-implemented method, comprising:
obtaining an entity-attribute candidate pair defining an entity and an attribute, wherein the attribute is a candidate attribute of the entity;
determining, based on a set of sentences that includes the entity and the attribute, whether the attribute is an actual attribute of the entity in the entity-attribute candidate pair, the determining comprising:
generating an embedding for a word in the set of sentences that includes the entity and the attribute;
generating an attribute distribution embedding for the entity using a known entity-attribute pair, wherein the attribute distribution embedding for the entity specifies an embedding for the entity based on other attributes associated with the entity from the known entity-attribute pair;
generating an attribute distribution embedding for the attribute using the known entity-attribute pair, wherein the attribute distribution embedding for the attribute specifies an embedding for the attribute based on a known attribute associated with a known entity of the attribute in the known entity-attribute pair;
determining whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair based on the embedding for words in the sentence set, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute.
2. The computer-implemented method of claim 1, wherein generating an embedding for a word in the set of sentences that includes the entity and the attribute comprises:
generating a first vector representation specifying a first embedding of words between the entity and the attribute in the set of sentences;
generating, based on the set of sentences, a second vector representation specifying a second embedding for the entity; and
generating, based on the set of sentences, a third vector representation specifying a third embedding for the attribute.
3. The computer-implemented method of claim 2, wherein:
generating the attribute distribution embedding for the entity using known entity-attribute pairs comprises: generating a fourth vector representation using known entity-attribute pairs, the fourth vector representation specifying the attribute distribution embedding for the entity; and
generating the attribute distribution embedding for the attribute using the known entity-attribute pair comprises: generating a fifth vector representation using known entity-attribute pairs, the fifth vector representation specifying the attribute distribution embedding for the attribute.
4. The computer-implemented method of claim 3, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the embedding for words in the set of sentences, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute comprises:
determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations.
5. The computer-implemented method of claim 4, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations is performed using a feed-forward network.
6. The computer-implemented method of claim 5, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations comprises:
generating a single vector representation by concatenating the first, second, third, fourth, and fifth vector representations;
inputting the single vector representation into the feed-forward network; and
determining, over the feed-forward network and using the single vector representation, whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair.
7. The computer-implemented method of any of claims 3-6, wherein generating a fourth vector representation using known entity-attribute pairs, the fourth vector representation specifying the attribute distribution embedding for the entity comprises:
identifying a set of attributes associated with the entity in the known entity-attribute pair, wherein the set of attributes does not include the attribute; and
generating the attribute distribution embedding for the entity by computing a weighted sum of attributes in the set of attributes.
8. The computer-implemented method of any of claims 3-7, wherein generating a fifth vector representation using known entity-attribute pairs, the fifth vector representation specifying the attribute distribution embedding for the attribute comprises:
identifying a set of entities from the known entity-attribute pairs using the attributes;
for each entity in the set of entities, identifying a set of attributes associated with the entity, wherein the set of attributes does not include the attribute; and
generating the attribute distribution embedding for the attributes by computing a weighted sum of the attributes in the attribute set.
9. A system, comprising:
one or more memory devices to store instructions; and
one or more data processing apparatus configured to interact with the one or more memory devices and, when the instructions are executed, perform operations comprising:
obtaining an entity attribute candidate pair defining an entity and an attribute, wherein the attribute is a candidate attribute of the entity;
determining, based on a set of sentences that includes the entity and the attribute, whether the attribute is an actual attribute of the entity in the entity-attribute candidate pair, the determining comprising:
generating an embedding for a word in the set of sentences that includes the entity and the attribute;
generating an attribute distribution embedding for the entity using a known entity-attribute pair, wherein the attribute distribution embedding for the entity specifies an embedding for the entity based on other attributes associated with the entity from the known entity-attribute pair;
generating an attribute distribution embedding for the attribute using the known entity-attribute pair, wherein the attribute distribution embedding for the attribute specifies an embedding for the attribute based on a known attribute associated with a known entity of the attribute in the known entity-attribute pair;
determining whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair based on the embedding for words in the sentence set, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute.
10. The system of claim 9, wherein generating the embedding for the words in the set of sentences that includes the entity and the attribute comprises:
generating a first vector representation specifying a first embedding of words between the entity and the attribute in the set of sentences;
generating, based on the set of sentences, a second vector representation specifying a second embedding for the entity; and
generating, based on the set of sentences, a third vector representation specifying a third embedding for the attribute.
11. The system of claim 10, wherein:
generating the attribute distribution embedding for the entity using known entity-attribute pairs comprises: generating a fourth vector representation using known entity-attribute pairs, the fourth vector representation specifying the attribute distribution embedding for the entity; and
generating the attribute distribution embedding for the attribute using the known entity-attribute pair comprises: generating a fifth vector representation using known entity-attribute pairs, the fifth vector representation specifying the attribute distribution embedding for the attribute.
12. The system of claim 11, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the embedding for words in the set of sentences, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute comprises:
determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations.
13. The system of claim 12, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations is performed using a feed-forward network.
14. The system of claim 13, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations comprises:
generating a single vector representation by concatenating the first, second, third, fourth, and fifth vector representations;
inputting the single vector representation into the feed-forward network; and
determining, over the feed-forward network and using the single vector representation, whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair.
15. The system of any of claims 11-14, wherein generating a fourth vector representation using known entity-attribute pairs, the fourth vector representation specifying the attribute distribution embedding for the entity comprises:
identifying a set of attributes associated with the entity in the known entity-attribute pair, wherein the set of attributes does not include the attribute; and
generating the attribute distribution embedding for the entity by computing a weighted sum of attributes in the set of attributes.
16. The system of any of claims 11-15, wherein generating a fifth vector representation using known entity-attribute pairs, the fifth vector representation specifying the attribute distribution embedding for the attribute comprises:
identifying a set of entities from the known entity-attribute pairs using the attributes;
for each entity in the set of entities, identifying a set of attributes associated with the entity, wherein the set of attributes does not include the attribute; and
generating the attribute distribution embedding for the attributes by computing a weighted sum of the attributes in the attribute set.
17. A non-transitory computer-readable medium storing instructions that, when executed by one or more data processing apparatus, cause the one or more data processing apparatus to perform operations comprising:
obtaining an entity-attribute candidate pair defining an entity and an attribute, wherein the attribute is a candidate attribute of the entity;
determining, based on a set of sentences that includes the entity and the attribute, whether the attribute is an actual attribute of the entity in the entity-attribute candidate pair, the determining comprising:
generating an embedding for a word in the set of sentences that includes the entity and the attribute;
generating an attribute distribution embedding for the entity using a known entity-attribute pair, wherein the attribute distribution embedding for the entity specifies an embedding for the entity based on other attributes associated with the entity from the known entity-attribute pair;
generating an attribute distribution embedding for the attribute using the known entity-attribute pair, wherein the attribute distribution embedding for the attribute specifies an embedding for the attribute based on a known attribute associated with a known entity of the attribute in the known entity-attribute pair;
determining whether the attribute in the entity-attribute candidate pair is an actual attribute of the entity in the entity-attribute candidate pair based on the embedding for words in the sentence set, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute.
18. The non-transitory computer-readable storage medium of claim 17, wherein generating an embedding for a word in the set of sentences that includes the entity and the attribute comprises:
generating a first vector representation specifying a first embedding of words between the entity and the attribute in the set of sentences;
generating, based on the set of sentences, a second vector representation specifying a second embedding for the entity; and
generating, based on the set of sentences, a third vector representation specifying a third embedding for the attribute.
19. The non-transitory computer-readable medium of claim 18, wherein:
generating the attribute distribution embedding for the entity using known entity-attribute pairs comprises: generating a fourth vector representation using known entity-attribute pairs, the fourth vector representation specifying the attribute distribution embedding for the entity; and
generating the attribute distribution embedding for the attribute using the known entity-attribute pair comprises: generating a fifth vector representation using known entity-attribute pairs, the fifth vector representation specifying the attribute distribution embedding for the attribute.
20. The non-transitory computer-readable medium of claim 19, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the embedding for words in the set of sentences, the attribute distribution embedding for the entity, and the attribute distribution embedding for the attribute comprises:
determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations.
21. The non-transitory computer-readable medium of claim 20, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations is performed using a feed-forward network.
22. The non-transitory computer-readable medium of claim 21, wherein determining whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair based on the first, second, third, fourth, and fifth vector representations comprises:
generating a single vector representation by concatenating the first, second, third, fourth, and fifth vector representations;
inputting the single vector representation into the feed-forward network; and
determining, over the feed-forward network and using the single vector representation, whether the attribute of the entity-attribute candidate pair is an actual attribute of the entity-attribute candidate pair.
23. The non-transitory computer-readable medium of any one of claims 19-22, wherein generating a fourth vector representation using known entity-attribute pairs, the fourth vector representation specifying the attribute distribution embedding for the entity comprises:
identifying a set of attributes associated with the entity in the known entity-attribute pair, wherein the set of attributes does not include the attribute; and
generating the attribute distribution embedding for the entity by computing a weighted sum of attributes in the set of attributes.
24. The non-transitory computer-readable medium of any one of claims 19-23, wherein generating a fifth vector representation using known entity-attribute pairs, the fifth vector representation specifying the attribute distribution embedding for the attribute comprises:
identifying a set of entities from the known entity-attribute pairs using the attributes;
for each entity in the set of entities, identifying a set of attributes associated with the entity, wherein the set of attributes does not include the attribute; and
generating the attribute distribution embedding for the attributes by computing a weighted sum of the attributes in the attribute set.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/504,068 | 2019-07-05 | ||
US16/504,068 US11263400B2 (en) | 2019-07-05 | 2019-07-05 | Identifying entity attribute relations |
PCT/US2020/040890 WO2021007159A1 (en) | 2019-07-05 | 2020-07-06 | Identifying entity attribute relations |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112805715A true CN112805715A (en) | 2021-05-14 |
Family
ID=71895199
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080005433.XA Pending CN112805715A (en) | 2019-07-05 | 2020-07-06 | Identifying entity attribute relationships |
Country Status (4)
Country | Link |
---|---|
US (1) | US11263400B2 (en) |
KR (1) | KR20210034679A (en) |
CN (1) | CN112805715A (en) |
WO (1) | WO2021007159A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113128196A (en) * | 2021-05-19 | 2021-07-16 | 腾讯科技（深圳）有限公司 | Text information processing method and device, storage medium |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220050967A1 (en) * | 2020-08-11 | 2022-02-17 | Adobe Inc. | Extracting definitions from documents utilizing definition-labeling-dependent machine learning background |
US11929062B2 (en) * | 2020-09-15 | 2024-03-12 | International Business Machines Corporation | End-to-end spoken language understanding without full transcripts |
CN112926329B (en) * | 2021-03-10 | 2024-02-20 | 招商银行股份有限公司 | Text generation method, device, equipment and computer readable storage medium |
US11893352B2 (en) * | 2021-04-22 | 2024-02-06 | Adobe Inc. | Dependency path reasoning for measurement extraction |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120072387A1 (en) * | 2009-05-18 | 2012-03-22 | Takatoshi Yanase | Knowledge base system, logic operation method, program, and recording medium |
CN103207913A (en) * | 2013-04-15 | 2013-07-17 | 武汉理工大学 | Method and system for acquiring commodity fine-grained semantic relation |
CN104081385A (en) * | 2011-04-29 | 2014-10-01 | 汤姆森路透社全球资源公司 | Representing information from documents |
US20150310096A1 (en) * | 2014-04-29 | 2015-10-29 | International Business Machines Corporation | Comparing document contents using a constructed topic model |
CN107688616A (en) * | 2016-08-05 | 2018-02-13 | 谷歌有限责任公司 | Show unique fact of entity |
CN108052577A (en) * | 2017-12-08 | 2018-05-18 | 北京百度网讯科技有限公司 | A kind of generic text content mining method, apparatus, server and storage medium |
CN108257608A (en) * | 2016-12-29 | 2018-07-06 | 谷歌有限责任公司 | Automatic speech pronunciation ownership |
CN109783651A (en) * | 2019-01-29 | 2019-05-21 | 北京百度网讯科技有限公司 | Extract method, apparatus, electronic equipment and the storage medium of entity relevant information |
CN109885697A (en) * | 2019-02-01 | 2019-06-14 | 北京百度网讯科技有限公司 | Construct method, apparatus, equipment and the medium of data model |
CN109933785A (en) * | 2019-02-03 | 2019-06-25 | 北京百度网讯科技有限公司 | Method, apparatus, equipment and medium for entity associated |
Family Cites Families (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9110852B1 (en) * | 2012-07-20 | 2015-08-18 | Google Inc. | Methods and systems for extracting information from text |
US9501503B2 (en) * | 2013-05-09 | 2016-11-22 | Microsoft Technology Licensing, Llc | Inferring entity attribute values |
US9864795B1 (en) * | 2013-10-28 | 2018-01-09 | Google Inc. | Identifying entity attributes |
US9898458B2 (en) | 2015-05-08 | 2018-02-20 | International Business Machines Corporation | Generating distributed word embeddings using structured information |
CN107977368B (en) | 2016-10-21 | 2021-12-10 | 京东方科技集团股份有限公司 | Information extraction method and system |
US11361242B2 (en) | 2016-10-28 | 2022-06-14 | Meta Platforms, Inc. | Generating recommendations using a deep-learning model |
CN107783960B (en) | 2017-10-23 | 2021-07-23 | 百度在线网络技术（北京）有限公司 | Method, device and equipment for extracting information |
CN108073711B (en) | 2017-12-21 | 2022-01-11 | 北京大学深圳研究生院 | Relation extraction method and system based on knowledge graph |
US10803248B1 (en) * | 2018-01-04 | 2020-10-13 | Facebook, Inc. | Consumer insights analysis using word embeddings |
US20190286978A1 (en) * | 2018-03-14 | 2019-09-19 | Adobe Inc. | Using natural language processing and deep learning for mapping any schema data to a hierarchical standard data model (xdm) |
US10782986B2 (en) * | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
US11487791B2 (en) * | 2019-03-29 | 2022-11-01 | Microsoft Technology Licensing, Llc | Latent feature extraction from a network graph |
US11875253B2 (en) * | 2019-06-17 | 2024-01-16 | International Business Machines Corporation | Low-resource entity resolution with transfer learning |
-
2019
- 2019-07-05 US US16/504,068 patent/US11263400B2/en active Active
-
2020
- 2020-07-06 KR KR1020217008154A patent/KR20210034679A/en not_active Application Discontinuation
- 2020-07-06 WO PCT/US2020/040890 patent/WO2021007159A1/en active Application Filing
- 2020-07-06 CN CN202080005433.XA patent/CN112805715A/en active Pending
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120072387A1 (en) * | 2009-05-18 | 2012-03-22 | Takatoshi Yanase | Knowledge base system, logic operation method, program, and recording medium |
CN104081385A (en) * | 2011-04-29 | 2014-10-01 | 汤姆森路透社全球资源公司 | Representing information from documents |
CN103207913A (en) * | 2013-04-15 | 2013-07-17 | 武汉理工大学 | Method and system for acquiring commodity fine-grained semantic relation |
US20150310096A1 (en) * | 2014-04-29 | 2015-10-29 | International Business Machines Corporation | Comparing document contents using a constructed topic model |
CN107688616A (en) * | 2016-08-05 | 2018-02-13 | 谷歌有限责任公司 | Show unique fact of entity |
CN108257608A (en) * | 2016-12-29 | 2018-07-06 | 谷歌有限责任公司 | Automatic speech pronunciation ownership |
CN108052577A (en) * | 2017-12-08 | 2018-05-18 | 北京百度网讯科技有限公司 | A kind of generic text content mining method, apparatus, server and storage medium |
CN109783651A (en) * | 2019-01-29 | 2019-05-21 | 北京百度网讯科技有限公司 | Extract method, apparatus, electronic equipment and the storage medium of entity relevant information |
CN109885697A (en) * | 2019-02-01 | 2019-06-14 | 北京百度网讯科技有限公司 | Construct method, apparatus, equipment and the medium of data model |
CN109933785A (en) * | 2019-02-03 | 2019-06-25 | 北京百度网讯科技有限公司 | Method, apparatus, equipment and medium for entity associated |
Non-Patent Citations (5)
Title |
---|
KATRIN ERK等: "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method", PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, pages 2389 * |
刘丽佳;郭剑毅;周兰江;余正涛;邵发;张金鹏;: "基于LM算法的领域概念实体属性关系抽取", 中文信息学报, vol. 28, no. 06, pages 216 * |
刘建伟等: "生成对抗网络在各领域应用研究进展", 自动化学报, vol. 46, no. 12, pages 2500 * |
胡静等: "机器学习及其神经网络分类器优化设计", 中国博士学位论文全文数据库信息科技辑, no. 4, pages 140 - 5 * |
车海燕;孙吉贵;荆涛;白曦;: "一个基于本体主题的中文知识获取方法", 计算机科学与探索, vol. 1, no. 02, pages 206 * |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113128196A (en) * | 2021-05-19 | 2021-07-16 | 腾讯科技（深圳）有限公司 | Text information processing method and device, storage medium |
Also Published As
Publication number | Publication date |
---|---|
WO2021007159A1 (en) | 2021-01-14 |
KR20210034679A (en) | 2021-03-30 |
US20210004438A1 (en) | 2021-01-07 |
US11263400B2 (en) | 2022-03-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
AU2020321751B2 (en) | Neural network system for text classification | |
US9373075B2 (en) | Applying a genetic algorithm to compositional semantics sentiment analysis to improve performance and accelerate domain adaptation | |
US10303767B2 (en) | System and method for supplementing a question answering system with mixed-language source documents | |
US20160283468A1 (en) | Context Based Synonym Filtering for Natural Language Processing Systems | |
US11263400B2 (en) | Identifying entity attribute relations | |
CN110457708B (en) | Vocabulary mining method and device based on artificial intelligence, server and storage medium | |
US10783179B2 (en) | Automated article summarization, visualization and analysis using cognitive services | |
US9996525B2 (en) | System and method for supplementing a question answering system with mixed-language source documents | |
KR20170004154A (en) | Method and system for automatically summarizing documents to images and providing the image-based contents | |
US8463591B1 (en) | Efficient polynomial mapping of data for use with linear support vector machines | |
US20170371955A1 (en) | System and method for precise domain question and answer generation for use as ground truth | |
US20170371956A1 (en) | System and method for precise domain question and answer generation for use as ground truth | |
Biswas et al. | Scope of sentiment analysis on news articles regarding stock market and GDP in struggling economic condition | |
Kim et al. | Applying a convolutional neural network to legal question answering | |
Nasim et al. | ABSA toolkit: An open source tool for aspect based sentiment analysis | |
Hourrane et al. | Using deep learning word embeddings for citations similarity in academic papers | |
Brum et al. | Semi-supervised sentiment annotation of large corpora | |
Barik et al. | Analysis of customer reviews with an improved VADER lexicon classifier | |
US11880664B2 (en) | Identifying and transforming text difficult to understand by user | |
Chakma et al. | 5W1H-Based semantic segmentation of tweets for event detection using BERT | |
Kim et al. | Question answering of bar exams by paraphrasing and legal text analysis | |
Bhalerao et al. | Social Media Mining Using Machine Learning Techniques as a Survey | |
Zhou et al. | Nested causality extraction on traffic accident texts as question answering | |
Hernandez Barrera et al. | Automated Creation of a Repository for Learning Words in the Area of Computer Science by Keyword Extraction Methods and Text Classification | |
Mansur et al. | Text Analytics and Machine Learning (TML) CS5604 Fall 2019 |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |