CN207895435U - Neural computing module - Google Patents
Neural computing module Download PDFInfo
- Publication number
- CN207895435U CN207895435U CN201721254418.0U CN201721254418U CN207895435U CN 207895435 U CN207895435 U CN 207895435U CN 201721254418 U CN201721254418 U CN 201721254418U CN 207895435 U CN207895435 U CN 207895435U
- Authority
- CN
- China
- Prior art keywords
- module
- data
- memory
- memory group
- instruction
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000001537 neural effect Effects 0.000 title abstract description 13
- 230000004913 activation Effects 0.000 claims abstract description 103
- 238000004891 communication Methods 0.000 claims description 8
- 238000004364 calculation method Methods 0.000 claims description 5
- 210000004027 cell Anatomy 0.000 description 54
- 239000010410 layer Substances 0.000 description 42
- 238000000034 method Methods 0.000 description 30
- 238000013528 artificial neural network Methods 0.000 description 22
- 238000003491 array Methods 0.000 description 20
- 239000000872 buffer Substances 0.000 description 20
- 230000006870 function Effects 0.000 description 17
- 230000008569 process Effects 0.000 description 10
- 239000000047 product Substances 0.000 description 8
- 238000004590 computer program Methods 0.000 description 7
- 238000013500 data storage Methods 0.000 description 6
- 239000011159 matrix material Substances 0.000 description 6
- 230000001360 synchronised effect Effects 0.000 description 6
- 230000009471 action Effects 0.000 description 5
- 239000000306 component Substances 0.000 description 5
- 230000008878 coupling Effects 0.000 description 5
- 238000010168 coupling process Methods 0.000 description 5
- 238000005859 coupling reaction Methods 0.000 description 5
- 210000004205 output neuron Anatomy 0.000 description 5
- 230000004044 response Effects 0.000 description 5
- 230000005540 biological transmission Effects 0.000 description 4
- 238000007726 management method Methods 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 230000001186 cumulative effect Effects 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 210000004218 nerve net Anatomy 0.000 description 3
- 230000003213 activating effect Effects 0.000 description 2
- 230000001419 dependent effect Effects 0.000 description 2
- 238000000151 deposition Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 241000208340 Araliaceae Species 0.000 description 1
- 241001269238 Data Species 0.000 description 1
- 235000005035 Panax pseudoginseng ssp. pseudoginseng Nutrition 0.000 description 1
- 235000003140 Panax quinquefolius Nutrition 0.000 description 1
- XGRYDJSRYGHYOO-UHFFFAOYSA-N Thesine Natural products C1=CC(O)=CC=C1C1C(C(=O)OCC2C3CCCN3CC2)C(C=2C=CC(O)=CC=2)C1C(=O)OCC1C2CCCN2CC1 XGRYDJSRYGHYOO-UHFFFAOYSA-N 0.000 description 1
- 230000001133 acceleration Effects 0.000 description 1
- 230000003466 anti-cipated effect Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 239000000571 coke Substances 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 239000008358 core component Substances 0.000 description 1
- 125000004122 cyclic group Chemical group 0.000 description 1
- 238000006073 displacement reaction Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 235000013399 edible fruits Nutrition 0.000 description 1
- 230000005611 electricity Effects 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 235000008434 ginseng Nutrition 0.000 description 1
- 238000007689 inspection Methods 0.000 description 1
- 239000011229 interlayer Substances 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 210000005036 nerve Anatomy 0.000 description 1
- 210000002569 neuron Anatomy 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000012536 storage buffer Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F17/00—Digital computing or data processing equipment or methods, specially adapted for specific functions
- G06F17/10—Complex mathematical operations
- G06F17/16—Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication, matrix factorization
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/20—Handling requests for interconnection or transfer for access to input/output bus
- G06F13/28—Handling requests for interconnection or transfer for access to input/output bus using burst mode transfer, e.g. direct memory access DMA, cycle steal
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/30007—Arrangements for executing specific machine instructions to perform operations on data operands
- G06F9/3001—Arithmetic instructions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F7/00—Methods or arrangements for processing data by operating upon the order or content of the data handled
- G06F7/38—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation
- G06F7/48—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices
- G06F7/544—Methods or arrangements for performing computations using exclusively denominational number representation, e.g. using binary, ternary, decimal representation using non-contact-making devices, e.g. tube, solid state device; using unspecified devices for evaluating functions by calculation
- G06F7/5443—Sum of products
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/30007—Arrangements for executing specific machine instructions to perform operations on data operands
- G06F9/30036—Instructions to perform operations on packed data, e.g. vector, tile or matrix operations
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/30003—Arrangements for executing specific machine instructions
- G06F9/3005—Arrangements for executing specific machine instructions to perform operations for flow control
- G06F9/30065—Loop control instructions; iterative instructions, e.g. LOOP, REPEAT
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/30—Arrangements for executing machine instructions, e.g. instruction decode
- G06F9/38—Concurrent instruction execution, e.g. pipeline, look ahead
- G06F9/3824—Operand accessing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
Abstract
This application involves neural computing modules.A kind of computing unit is disclosed, the computing unit includes the first memory group for storing input activation and the second memory group for being stored in the parameter used in execution calculating.The computing unit includes at least one cell, at least one cell includes receiving parameter from the second memory group and executing computing unit described at least one multiply-accumulate (" MAC ") arithmetic unit of calculating to further comprise that the first Traversal Unit, first Traversal Unit provide control signal so that input activation is provided to the data/address bus that can be accessed by the MAC operation device to the first memory group.The computing unit executes one or more calculating associated at least one of data array element, one or more of to calculate the multiplying for executing and include partly the input activation and the parameter received from the second memory group that are received from the data/address bus by the MAC operation device.
Description
Technical field
This application involves neural computing modules.
Background technology
This specification relates generally to a kind of neural computing module for calculating deep neural network (" DNN ") layer
(tile), which allows the instruction bandwidth reduced and command memory.
Utility model content
Generally, a novel aspects described in this specification can be presented as a kind of by accelerating based on tensor computation
Calculate unit.The computing unit includes first memory group with the first data width and with bigger than the first data width by the
The second memory group of two data widths, first memory group are used to store at least one for inputting and activating or export in activation
A, second memory group is used to store one or more parameter for executing and using when calculating.Computing unit can be wrapped further
At least one cell is included, which includes receiving parameter from second memory group and executing at least one of calculating to multiply
Method adds up (" MAC ") arithmetic unit.Computing unit may further include：With at least first memory group into the of row data communication
One Traversal Unit, which is configured as providing control signal to first memory group so that input activation is provided
The data/address bus that can be extremely accessed by MAC operation device.Computing unit executes associated at least one of data array element
One or more calculating, one or more calculate executed by MAC operation device and include partly being connect from data/address bus
The multiplying of the input activation and the parameter received from second memory group of receipts.
Another novel aspects of theme described in this specification can be presented as a kind of for accelerating tensor computation
Computer implemented method.The computer implemented method includes：It is received from the first traversal in response to first memory group
The control signal of unit sends the first input activation, first memory by the first memory group with the first data width
Group is arranged in computing unit, and wherein, and the first input activation is by that can pass through at least cell of computing unit access
Data/address bus provides.This method further comprises：By at least one cell, receive from than the first data width
One or more parameter of the second memory group of the second big data width, and wherein, which includes
At least one multiply-accumulate (" MAC ") arithmetic unit.This method can further include：By MAC operation device, executes and count
According to one or more the associated calculating of at least one of array element, wherein one or more calculating section
It include multiplying for at least first input activation accessed from data/address bus and at least one parameter for being received from second memory group
Method operation.
Another novel aspects of theme described in this specification can be presented as that a kind of non-transitory is computer-readable
Storage medium.The non-transitory computer-readable storage media includes the instruction that can be executed by one or more processor,
The instruction makes one or more processor execute operation in such execution, which includes：In response to first memory
Group receives the control signal from the first Traversal Unit, by the first memory group with the first data width, sends the
One input activation, first memory group are arranged in computing unit, and wherein, and the first input activation is by that can pass through calculating
Data/address bus that at least cell of unit is accessed provides.Performed operation can also include：By at least one cell,
One or more parameter from the second memory group with second data width bigger than the first data width is received, and
And wherein, which includes at least one multiply-accumulate (" MAC ") arithmetic unit.Performed operation can also wrap
It includes：By MAC operation device, one or more calculating associated at least one of data array element is executed, wherein
It include one or more calculating section at least first input activation accessed from data/address bus and from second memory group
The multiplying of at least one parameter received.
Theme described in this specification can be realized in a particular embodiment, to realize one in following advantage
Or it is multiple.Keep tracking memory address value that one instruction iteration deep layer nesting of program is allowed to recycle using register.
Based on the memory address value retrieved from register, traverse from the single narrow memory cell and wide memory calculated in module
The tensor of unit access.Memory address value is corresponding with the element of tensor.Execution of the tensor computation based on deep-cycle nesting
It is happened at and individually calculates in module.Calculating can be distributed across multiple modules.Based on by the tensor computation of multilayer neural network across
Multiple calculating module distributions, to enhance and accelerate computational efficiency.Tensor can be traversed, and the finger of quantity reduction can be utilized
It enables to execute tensor computation.
The theme described in the present specification can also be realized in a particular embodiment, to realize further advantage.For example,
It is wide with the narrow low bandwidth memory and high band that traverse various dimensions array in any sequence by using that will allow flexibly to address
Memory coupling memory hierarchy, can be directed to far different dimension DNN layers realization MAC operation device high usage and
It can farthest utilize and calculate locality.
The other embodiment of this aspect and other aspects includes corresponding system, device and is configured as execution method
Action and encode the computer program on computer memory device.It can be by system, in operation
Software, firmware, hardware or a combination thereof that system execution acts is set to carry out one or more configured in this way system for computer.
Can be by having instruction come one or more configured in this way computer program, the instruction by data processing equipment when being executed
The device is set to execute action.
Theme described in this specification further relates to a kind of image recognition and/or sorting technique/system.Institute can be used public
It the technology opened and described with hardware computational unit or calculate the hardware computing system of module and realizes system.It calculates single
Member handles the tensor operation for computational reasoning (inference) using the neural network with multiple neural net layers.
It set forth one or more embodiment of the theme of this specification description in following attached drawing and description
Details.Other potential features, aspect and advantage of this theme will become aobvious and easy by specification, attached drawing and claim
See.
Description of the drawings
Fig. 1 is the block diagram of exemplary computing system.
Fig. 2 shows exemplary neural network and calculates module.
Fig. 3 shows example tensor Traversal Unit (TTU) structure.
Fig. 4 shows the narrow storage including providing input activation to one or more multiply-accumulate (MAC) arithmetic unit
The exemplary architecture of device unit.
Fig. 5 shows the example frame of the output bus including providing output activation to the narrow memory cell of Fig. 2 and Fig. 4
Structure.
Fig. 6 is to execute the example flow diagram of the process of tensor computation using the neural computing module in Fig. 2.
In various figures, similar reference numeral indicates similar element with title.
Specific implementation mode
Theme described in this specification is related to a kind of hardware computing system including multiple computing units, the multiple meter
Calculate the machine learning reasoning workload that unit is configured as accelerans network layer.Each computing unit of hardware computing system is
It is self-contained, and can independently execute multilayer neural network to the calculating needed for given layer.
It can carry out computational reasoning using the neural network with multiple layers.For example, in the case where input is given, nerve net
Network can calculate the reasoning of the input.Neural network handles the input to calculate the reasoning by each layer of neural network.Tool
Body, the layer of neural network is respectively provided with corresponding weight sets.Each layer, which receives, to be inputted and is located according to the weight sets of this layer
The input is managed to generate output.
Therefore, it inputs in order to which from the input computational reasoning received, neural network receives and passes through each neural network
Layer is processed for generating reasoning to the input, wherein the output of a neural net layer is provided as next nerve
The input of network layer.The data of neural net layer input, for example, the input of neural network or in sequence below this layer
Layer arrives the output of neural net layer, is referred to alternatively as the activation input of this layer.
In some embodiments, by the layer arrangement of neural network at sequence.In other embodiments, by layer arrangement at
Digraph.That is, any specific layer can receive multiple input, multiple outputs or the two.It can also be by nerve net
The layer arrangement of network is at allowing to the output of a layer to postbacking the input sent as a upper layer.
Hardware computing system described in this specification can be executed by being distributed tensor computation across multiple calculating modules
The calculating of neural net layer.The calculating process executed in neural net layer may include：Include the input tensor of input activation
With the multiplication of the parameter tensor including weight.The calculating includes that will input activation and the weight phase in one or more cycle
Multiply, and executes the cumulative of product in multiple cycles.
Tensor is hypergeometry object, and example hypergeometry object includes matrix and data array.In general, by calculating
Module executes software algorithm, to execute tensor computation by being handled nesting cycle to traverse N-dimensional tensor.Show at one
In example calculating process, each cycle can be responsible for traversing the specific dimension of N-dimensional tensor.Given tensor is constructed, mould is calculated
Group can require to access the element of specific tensor to execute multiple dot product calculating associated with tensor.Calculating is happened at is deposited by narrow
When the input activation that reservoir structures provide is with the parameter or multiplied by weight provided by wide memory structure.Because tensor is stored in
In memory, so tensor indexed set can require to be converted into storage address collection.In general, calculating the tensor Traversal Unit of module
Executive control operation, control operation provides the index of each dimension associated with tensor and traversal indexes element in terms of executing
The sequence of calculation.When output bus is written in multiplication result and is stored in memory, tensor computation terminates.
Fig. 1 shows the exemplary computing system 100 for accelerating tensor computation associated with deep neural network (DNN)
Block diagram.System 100 generally includes controller 102, host interface 108, input/output (I/O) link 110 including the first mould
Multiple modules, grader part 116 and the data identified in bus legend 118 of group collection 112 and the second module collection 114
Bus (is shown, but not including that within system 100) for purposes of clarity.Controller 102 generally includes data storage
104, it command memory 106 and is configured as executing the one or more instructions of coding in a computer-readable storage medium
At least one processor.Command memory 106 can store one or more machine readable instructions, this one or more
Machine readable instructions can be executed by one or more processor of controller 102.Data storage 104 can be for depositing
Storage and then access and any one in the various data storage mediums for the relevant various data of calculating being happened in system 100
Kind.
Controller 102 is configured as executing and be instructed with relevant one or more of the tensor operation in system 100, this one
A or multiple instruction includes the instruction being stored in command memory 106.In some embodiments, data storage 104
It is volatile memory-elements with command memory 106.In some other embodiments, data storage 104 and instruction are deposited
Reservoir 106 is Nonvolatile memery unit.Data storage 104 and command memory 106 can also be another form of
Computer-readable medium, such as：Floppy device, hard disc apparatus, compact disk equipment or tape unit, flash memory or its
Its similar solid-state memory device or equipment array, including：Equipment or other configurations in storage area network
Equipment.In various embodiments, controller 102 can also be cited as or referred to as core supervisor 102.
As depicted, host interface 108 is coupled to I/O links 110, controller 102 and grader part 116.Host
Interface 108 receives instruction and data parameter from I/O links 110 and provides instruction and parameter to controller 102.In general, instruction
One or more equipment in system 100 can be provided to by instruction bus 124 (hereafter to be described), and parameter can be with
It is provided to one or more equipment in system 100 by ring bus 128 (hereafter to be described).In some embodiments
In, controller 102 at the beginning between from host interface 108 receive instruct and store it in command memory 106, for
Controller 102 is executed in the time later.
Grader part 116 is again coupled to the module 7 of controller 102 and the second module collection 114.In some embodiment party
In formula, grader part 116 is implemented as the individual module in system 100.In an alternative embodiment, grader part
116 settings are either located at sub-circuit or sub- equipment in controller 102 as controller 102.Grader part 116 is usual
It is configured as executing one or more function to cumulative preactivate value, these preactivate values are as the defeated of the layer being fully connected
Go out to be received.The layer being fully connected can be divided into the module in module collection 112 and 114.Therefore, each module by with
It is set to the subset for generating and being stored in the preactivate value (that is, linear convergent rate) in the memory cell of module.Classification results are total
Line 120 is provided from grader part 116 to the data path of controller 102.To include function via classification results bus 120
The data of value (that is, result) are provided to controller 102 from grader part 116 afterwards.
Bus legend 118 shows and is provided between the module and the module of the second module collection 114 of the first module collection 112
The data/address bus of one or more data communication path being connected with each other.Bus legend 118 provides classification results for identification
The legend of bus 120, CSR/ main bus 122, instruction bus 124, netted bus 126 and ring bus 128, as depicted in fig. 1.
In general, module is the core component in the accelerator architecture of system 100, and it is the coke of the tensor computation occurred in systems
Point.Each module is individual computing unit, and multiple modules can be interacted with other modules in system to accelerate
The calculating (such as tensor computation) of one or more interlayer of multilayer neural network.For example, multiple moulds can be distributed in by calculating
Between group.Between the tensor computation of multilayer neural network is distributed in multiple calculating modules, it can enhance and accelerate to calculate
Efficiency.Although the module in module collection 112,114 can share the execution of tensor computation associated with given instruction, single
A computing unit is self-contained computation module, and this computation module is configured as relative to other in module collection 112,114
Corresponding module independently executes the subset of tensor computation.
Control and status register (CSR) bus 122 are single main mostly from bus, and list master mostly makes controller 102 from bus
Can be transmitted one or more instruction, this one or more instruction setting program configure and read and one or more
The associated status register of module.CSR buses 122 can with a main bus segment and multiple connected into from bus segment
Single daisy chain configuration.As shown in Figure 1, CSR buses 122 are by by the module and controller in the module collection 112,114 in ring
The 102 bus data paths for being connected to host interface 110 provide communicative couplings.In some embodiments, host interface 110 is
The individual host of CSR bus rings, and entire CSR bus address spaces are memory mapped into the storage in host interface 110
Device space.
Host interface 110 can execute one or more operation using CSR buses 122, and the operation includes：Example
Such as, the storage buffer pointer in controller 102 is programmed so that controller 102 can start from command memory
The various modules remained unchanged in one or more computing interval are arranged (for example, polynomial approximation meter in 106 prefetched instructions
The coefficient table of calculation) it is updated/program, and/or by firmware loads/and is re-loaded to classified part 116.In one example,
Firmware reload may include linear convergent rate to be applied to new function (that is, preactivate value).Therefore, have access CSR total
Each of 122 permission of line will be with being bound to this from equipment and identify the different node identifiers from equipment from equipment
(node ID).The node ID will refer to enable the part of address, and will by CSR from equipment (that is, controller 102, module 112,
114 and grader 116) using, examine or check with determines the CSR be grouped whether be addressed to from equipment.
In some embodiments, host interface 102 can transmit one or more instruction by controller 102.Example
Such as, the width of described instruction can be 32, wherein first 7 include the IA/mesh for indicating to receive and execute instruction
Ground header information.First 7 of header can include the data parameters for indicating specific node ID.Therefore, on CSR bus rings
Slave equipment (for example, each module) can with the header of check command with determine main equipment (host interface 110) propose request
Whether the module that checks header is addressed to.If being the non-indicative purpose of the node ID of header to examine module, the inspection module
The CSR instruction packets of input can be copied to and be connected to the CSR buses input of next module for being examined by next module.
Instruction bus 124 originates in controller 102, also, similar to CSR buses 122, also by by the module collection in ring
112, the module in 114 connects back to the bus data path of controller 102 and provides communicative couplings.In one embodiment,
Controller 102 broadcasts one or more instruction via instruction bus 124.The instruction broadcasted by controller 102 can with via
The instruction that CSR buses 122 provide is different.However, the finger that module receives and/or consumption or execution are received via bus 124
The mode of order can be similar to the process for executing the instruction received via CSR buses 122.
In one example, the header (for example, bitmap) of instruction to receiving module indicate the receiving module need based on
Associated bitmap is instructed to consume specific instruction.The bitmap can have the specific width defined according to position.It is typically based on
The parameter of instruction will instruct from a module and be forwarded to next module.In one embodiment, the width of instruction bus 124
It can be configured as size/width less than instruction.Therefore, in this configuration, the transmission of instruction will need multiple periods, and
And the bus station of instruction bus 124 will be associated with the module being placed on the instruction received from module with decoder
Target instruction target word buffer appropriate in.
As described further below, the module in module collection 112,114 is typically configured as supporting the other finger of two major classes
It enables.This two major classes can also not be referred to as instruction type.Instruction type includes tensor operation (TensorOp) instruction and directly deposits
Reservoir accesses (DMAOp) instruction.In some embodiments, DMAOp instructions have be allowed to be carried out at the same time one or more
A particularization (specialization).One or more particularization can be referred to as DMAOp instruction subtypes or behaviour
Make code.In some cases, each unique and/or effective DMAOp instruction types/subtype tuple is in specific module
There to be individual instruction buffer.
At specific module in module 112,114, bus station associated with instruction bus 124 will check header bitmap
With determine instruction type/subtype.Instruction can be received by module and then be written to module before module executes instruction
Instruction buffer.The finger for the module for being written with instruction can be determined by type and subtype indicator/field of instruction
Enable buffer.Instruction buffer may include the advanced elder generation that priority ranking is carried out to the consumption of one or more dependent instruction
Go out (FIFO) control program.Therefore, it under the FIFO control programs, is held the sequence of instruction bus is reached according to instruction always
The instruction of row same type/subtype.
Different instruction buffer device in module is TensorOp instruction buffers and DMAOp instruction buffers.Such as institute above
Description, instruction type includes TensorOp instructions and DMAOp instructions.It is instructed about DMAOp, instruction subtype (" write by instruction
Enter " buffer positions) include following：1) grid is passed to instruction buffer；2) grid spreads out of instruction buffer；3) narrow-wide DMA
Instruction buffer；4) wide-narrow DMA command buffer；With 5) ring bus DMA instruction buffers.It is more detailed below with reference to Fig. 2
These buffer positions carefully are described.Wide and narrow label is used throughout the specification, and these labels are commonly referred to as one
The approximate size of the width (position/byte) of a or multiple memory cells.Have respectively as it is used herein, " narrow " can refer to
There is one or more memory cell of the size or width less than 16, and " width " can refer to being less than 64
One or more of size or width memory cell.
Netted bus 126 provide from CSR buses 122, instruction bus 124 and ring bus 128 (hereafter to be described) it is different
Data communication path.As depicted in fig. 1, netted bus 126 provide in both X and Y dimensions each module is coupled or
It is connected to the communication path of its corresponding adjacent module.In various embodiments, netted bus 126 can be used in adjacent mould
Transmission input activation amount between one or more narrow memory cell in group.As shown, netted bus 126 do not allow by
Input activation data are forwarded directly to non-adjacent module.
In various embodiments, netted bus 126 and via netted bus 126 connect various modules can have with
Lower configuration.There are two outgoing port and two incoming ports for four turning module tools of grid.Four edge module tools of grid
There are three incoming port and three outgoing ports.There are four incoming ports and four outflow ends for all non-turning module tools of non-edge
Mouthful.In general, given example N × N module layout, edge module is the module only having there are three adjacent module, and turning
Module is that there are two the modules of adjacent module for tool.About the data flow method via netted bus 126, it is generally necessary to will be via
Grid bus 126 reaches one or more narrow memory cell that the input activation of each of specific module is submitted to module.This
Outside, the module having less than four incoming ports is configured, DMAOp instructions zero can be written in the narrow memory of module
Position, rather than wait for the data on the input port that is not present.Equally, for the module having less than four outgoing ports
Configuration, DMAOp instructions will not execute and the relevant narrow memory reading of transmission for being used for any port being not present and port write
Enter.
In some embodiments, based on the incoming/outflow DMAOp provided via netted bus 126, tensor traversal is single
First (hereinafter referred to as " TTU ") will generate the narrow memory cell that will be written into or be read from specific input activation position or
Address.Incoming DMAOp and outflow DMAOp are may be performed simultaneously, and will be controlled by the synchronous mark managed by controller 102
Scheme manages any desired synchronization.TTU is described in further detail referring to Fig. 2 and Fig. 3.
Ring bus 128 originates in controller 102, and similar to CSR buses 122 and instruction bus 124, also by general
Module 112,114 in ring connects back to the bus data path of controller 102 to provide communicative couplings.In various embodiments
In, ring bus 128 usually connects or be coupling in all wide memory units in all modules 112,114 (below will ginseng
It is more fully described according to Fig. 2).Therefore, the payload width of ring bus 128 be arranged it is every in module collection 112,114
The width of wide memory unit in a module is corresponding.As discussed above, ring bus 128 further includes that instruction needs to disappear
Consumption includes the bitmap head of the module of the payload data of the instruction or parameter that are communicated via ring bus 128.
About the data (that is, payload) received at specific module via ring bus 128, in response to receiving
The information, each module will reset the position data that (that is, removing) indicates in bitmap head, which is forwarding the data to
It is unique for receiving module before another module.Therefore, the specific of payload is received when head bitmap does not have instruction
When the remaining bit setting data of module, it will stop payload being forwarded to another module.Payload data typically refers to,
In the execution recycled based on deep nesting come the activation during executing tensor computation, used by one or more module and weight.
In some embodiments, controller 102 can be described as the part of ring bus 128.In one example,
For the DMAOp instructions executed in specific module, controller 102 can be used for popping up data from ring bus station/have
Load is imitated, and payload is forwarded to the ring bus station in the next module in ring.If the instruction in bitmap head needs
Such action is wanted, then one or more width that controller 102 can also be such that payload data is submitted in module is deposited
Storage unit.Need the address that one or more wide memory unit of data is written can be by the DMAOp in specific module
Instruction generates.
In various embodiments, each module of module collection 112,114 can be payload data the producer or
The consumer of person's payload data.When module is the producer of payload data, module from one in the module or
The multiple wide memory unit reads datas of person, and by ring bus 128 come multi-case data so that one or more is other
Module consumes.When module is the consumer of payload data, module receives data and writes data into module
One or more wide memory unit, and payload data is forwarded to be consumed for one or more other module.It closes
In the movement of the payload data via ring bus 128, locate at any given time, on ring bus 128 usually all only
There are a data producer/main equipments.DMAOp instruction execution sequences (for example, FIFO control programs) in all modules
It will ensure that there is only a data producer/main equipments on ring bus 128 at given time.
In some embodiments, controller 102 controls framework to ensure at given time in ring using synchronous mark
There is only a payload data producer/main equipments in shape bus 128.In one example, it is exported to annular by module
Each write-in can all trigger the increment that corresponding synchronous mark counts.Controller 102 can check payload data with true
Surely include the data block of payload or the quantity of section.Then controller 102 monitors the execution of module, to ensure in another mould
Before group is executed with holotype, which forwards and/or consumes the data segment of anticipated number.
When exist connected via ring bus 128, there is no the local multicast group of overlapping region on ring bus when, really
There is only a data producer/main equipments to be made an exception on ring bus 128 at given time by guarantor.For example, module 0
(master) can be with the module in multicast (that is, creation data) to 3 poly group of module 0- modules, and module 4 (master) can be equally multicasted to
Module in 4-module of module, 7 poly group.One important requirement of this double main multicasting methods is that different multicast groups is not allowed to see
To mutual packet, it is overlapped because grouping may occur and one or more data is caused to calculate mistake.
As shown in Figure 1, controller 102 provides communication data path, which will be in module collection 112,114
Module couples or is connected to I/O 110 and includes multiple cores function.The core function of controller 102 generally includes：To
Module in module collection 112,114 feeds one or more I/O inputs activation, feeds one received from I/O 110 to module
The activation of a or multiple input and parameter, is received to module feeding from I/O 110 one or more instruct, to host interface
108 send I/O outputs activation and the annular station as CSR buses 122 and ring bus 128.As described in more detail below
, the first module collection 112 and the second module collection 114 include multiple modules for executing one or more tensor computation,
One or more of tensor computations are based on nested with the deep-cycle that outer loop forms being performed by inside cycle.
System 100 is generally as follows operation.Host interface 108 will provide one or more instruction to controller 102, described
The direct memory access operation (DMAOp) that one or more instruction definition occurs for given calculating.It is controlled with being fed to
The associated descriptor of instruction of device 102 processed (is opened including the information required by controller with promotion and multidimensional data array
Amount) associated large-scale dot product calculates.In general, controller 102 from host interface 108 receive input activation, module instruction, with
And for the model parameter (that is, weight) for executing tensor computation to given layer for neural network.Then controller 102 can lead to
Cross makes instruction be multicasted to module 112,114 by the mode of the data flow of instruction definition.As discussed above, instruction is consumed
The bitmap data that module may then based in instruction header is initiated new/subsequent command broadcast to another module.
About data flow, input activation and parameter are sent to the mould in module collection 112,114 via ring bus 128
Group.Each module in module 112,114 calculates storage defeated needed for the subset for the output activation for being assigned to the specific module
Enter the subset of activation.DMAOp instructions for module will make input activation be moved to narrow memory from wide memory.When requiring
Input activation, parameter/weight and computations (TTU operations, storage address etc.) when can be used in module, the meter in module
Start.When the MAC operation device (hereafter to be described) in module completes all dot-product operations defined by instruction set and pre- swashs
When function living is applied to result (that is, output activation) of multiplying, the calculating being happened in module terminates.
The result of one or more tensor computation includes that the output activation of computation layer is written to the mould for executing the calculating
The narrow memory cell of group.For certain tensor computations, via grid bus 126 will export edge activation be transmitted to it is adjacent
Module.When calculating across multiple layers, edge activation will be exported by, which needing, is transmitted to output of the adjacent module with calculated for subsequent layer
Activation.When all layers of calculating are completed, final activation will be moved to grader module by DMAOp by ring bus 128
116.Then controller 102 will be read from grader module 116 finally activates and executes DMAOp to move the final activation
To host interface 108.In some embodiments, grader part 116 executes the meter of the output layer (that is, last layer) of NN
It calculates.In other embodiments, the output layer of NN be it is following in one kind：Grader layer returns layer or usually and nerve net
The associated another channel type of network.
Fig. 2 shows exemplary neural network (NN) and calculates module 200.In general, example module 200 can with above with reference to
Any of the module in the first module collection 112 and the second module collection 114 that Fig. 1 is discussed is corresponding.In various embodiment party
In formula, the reference of module 200 or referred to as computing unit 200 can also will be calculated.Each module 200 that calculates is self-contained meter
Unit is calculated, which is configured as independently executing instruction relative to other corresponding modules in module collection 112,114.Such as
It is briefly discussed above, each module 200 that calculates executes two kinds of instruction：TensorOp is instructed and DMAOp instructions.It is logical
Often, each instruction type will include calculating operation associated with deep-cycle nesting, and therefore, each instruction type is usual
It will be executed within multiple time points, to ensure the completion of all loop iterations.
It discussing in greater detail below, different instruction types is executed by the independent control unit in calculating module 200,
The independent control unit passes through synchronous in data in the synchronous mark control for calculating management in module 200.The synchronous mark control
While property between the execution of different instruction type in part management calculating module 200.By strictly according to issue sequence (that is,
First in first out) execute each calculating operation associated with each instruction type.About the two instruction types, TensorOP
And DMAOp, there is no sequences to ensure between these different instruction types, and each type is considered as independence by calculating module 200
Control thread.
It is constructed about data flow, calculates module 200 and generally include data path 202 and data path 205, the data road
Diameter 202 and data path 205 are respectively to pass in and out the data flow offer communication path for calculating module 200.As described above, it is
System 100 includes three different data bus structures for being arranged to loop configurations：CSR buses 122, instruction bus 124 and ring
Shape bus 128.With reference to Fig. 2, data path 205 is corresponding with instruction bus 124, and data path 202 usually with CSR buses
122 and ring bus 128 in one it is corresponding.As shown, data path 202 includes to leave the number for calculating module 200
According to the annular output 203 for providing outgoing route and to enter the annular input that the data of calculating module 200 provide input path
204。
It calculates module 200 and further comprises TensorOp controls 206 and DMAOp controls 208, the TensorOp controls
206 include TensorOp tensors Traversal Unit (TTU) 226, and the DMAOp controls 208 include DMAOp TTU 228.
The usually management of TensorOp controls 206 is to the write-ins of TensorOp TTU registers 232 and from TensorOp TTU registers 232
Reading, and supervise traversing operation for DMAOp TTU 226 execute.Equally, the usually management pair of DMAOp controls 208
The write-in of TensorOp TTU registers 234 and reading from TensorOp TTU registers 234, and supervise traversing operation with
It is executed for DMAOp TTU 228.TTU registers 232 include instruction buffer, which is included in for storing
TensorOp controls 206 wait for one or more instruction by the operations executed of TensorOp TTU 226 when executing instruction.Together
Sample, TTU registers 234 include instruction buffer, which is included in DMAOp controls 208 for storage and executes instruction
When wait for by the operations executed of TTU 228 one or more instruction.
It is as described further below, TTU 226 and/or 228 be used for traversing by calculating module 200 generally reside on it is narrow
The array element of memory 210 and one or more tensor in wide memory 212.In some embodiments, 226 TTU
It is used for providing the tensor of the dimension for traversing multidimensional tensor based on the nested execution of deep-cycle by TensorOp controls 206
Operation.
In some embodiments, the certain instructions executed by calculating module 200 are via data path 205 (that is, instruction is total
A part for line 124) reach module.Calculate module 200 will check head bitmap with determine instruction type (TensorOp or
DMAOp) and instruction subtype (read operation or write operation).Then received according to instruction type by module 200 is calculated
Instruction be written specific instruction buffer.In general, before the component for calculating module 200 executes instruction, receives and store and refer to
It enables (that is, write buffer).As shown in Fig. 2, instruction buffer (i.e. TensorOp TTU registers 232 and DMAOp TTU deposits
Device 234) first in first out (FIFO) control program, consumption of the program to one or more dependent instruction can be respectively included
(execution) carries out priority ranking.
As discussed briefly above, tensor is hypergeometry object, and example hypergeometry object includes matrix sum number
According to array.A kind of algorithm (including deep layer nesting cycle) can be executed so that one or more is embedding by iteration by calculating module 200
Searching loop N-dimensional tensor is covered to execute tensor operation.During an example calculations, each of loop nesting cycle can be with
It is responsible for the specific dimension of traversal N-dimensional tensor.As described herein, TensorOp controls 206 usually supervise one or more
Tensor operation, tensor operation driving traversal and the sequence for the dimension element for accessing specific tensor construction are nested by deep layer to complete
The calculating of circular in definition.
It calculates module 200 and further comprises narrow memory 210 and wide memory 212.Narrow and wide label is often referred to narrow deposit
The width size (position/byte) of the memory cell of reservoir 210 and wide memory 212.In some embodiments, narrow storage
Device 210 includes the memory cell for being respectively provided with size or width less than 16, and width memory 212 includes having respectively
There is the memory cell of the size or width less than 32.It is inputted in general, calculating module 200 and being received via data path 205
It activates and DMA controls 208 executes the operation that input activation is written to narrow memory 210.Equally, calculate module 200 via
Data path 202 receives parameter (weight) and DMA controls 208 are executed the operation of the parameter read-in wide memory 212.One
In a little embodiments, narrow memory 210 may include the memory arbitrator usually used in shared memory systems, to be directed to
Each memory cycle determines to allow which control device (for example, TensorOp controls 206 or DMAOp controls 208) access
The shared memory unit of narrow memory 210.
Module 200 is calculated to further comprise input activation bus 216 and include the MAC arrays of multiple cell (cell)
Each cell in 214, multiple cell includes MAC operation device 215 and and register 220.In general, the use of MAC arrays 214 is more
MAC operation device 215 on a cell and tensor computation is executed with register 220, which includes and dot product calculates phase
The arithmetical operation of pass.Input activation bus 216 provides data path and is directed to by narrow memory 210 in the data path
The corresponding access of each MAC operation device 215 in MAC arrays 214 provides input activation one by one.Therefore, it is based on activating input
Broadcast one by one, the single MAC operation device 215 of specific cell will receive input activation respectively.It is transported by the MAC of MAC arrays 214
Calculate the arithmetic operator performed by device generally include by input activation that narrow memory 210 provides with access from wide memory 212
Parameter is multiplied to generate single output activation value.
During arithmetic operator, can by part and it is cumulative and be stored in it is corresponding, for example, in register 220, or
Person is written wide memory 212 and is accessed again by the specific cell in MAC arrays 214, to complete subsequent multiplying.
Amount, which calculates, can be described as having first part and second part.When multiplying generates output activation, for example, by complete
It is multiplied with parameter to generate output activation at input activation, then first part completes.Second part includes answering nonlinear function
Output activation is used, also, after using the function, when output activation is written into narrow memory 210, second part is complete
At.
It calculates module 200 and further comprises that output activation bus 218 includes the non-linear list of output activation assembly line 224
Member (NLU) 222, NLU controls 238 and reference legend 230, this calculates the component in module 200 with reference to the instruction of legend 230
Core attribute.It for the sake of clarity shows with reference to legend 230, but is not included in reference to legend 230 and calculates in module 200.
Core attribute includes that specific components are unit, storage device, arithmetic unit, control device or data path.In general, in extensometer
After the completion of the first part of calculation, output activation is provided to NLU 222 from MAC arrays 214 via output activation bus 218.
After reaching NLU 22, simultaneously in output activation by the data application of the specified activation primitive received via activation assembly line 224
And narrow memory 210 then is written into output activation.In some embodiments, output activation bus 218 includes at least one
A pipeline system shift register 236, also, the second part for completing tensor computation includes the displacement using activation bus 218
Register 236 shifts output activation to narrow memory 210.
Dot product about such as two multidimensional data arrays calculates, and for individually calculating module 200, MAC arrays 214 carry
For steady single-instruction multiple-data (SIMD) function.SIMD generally means that all Parallel Units (multiple MAC operation devices 215)
Shared identical instruction (nested based on deep-cycle), but each MAC operation device 215 executes on different data elements and refers to
It enables.In a basic example, array [1,2,3,4] and [5,6,7,8] Element-Level is added in a cycle to obtain array
[6,8,10,12] four arithmetic units is usually required to execute operation to each element.By using SIMD, this four units can
To share identical instruction (for example, " addition ") and execute calculating parallel.Because instruction is shared, reduce wide to instruction
The requirement of degree and command memory, therefore improve efficiency.Therefore, compared to existing method, system 100 and calculating module 200
The acceleration of enhancing and parallel is provided in tensor computation.
In one example, and described in more detail below, single instruction can be provided to by controller 102
Multiple calculating modules 200 (see the module collection 112,114 in Fig. 1) consume for multiple MAC arrays 214.In general, neural network
Layer may include multiple output neurons, and the output neuron can be divided so as to by with output neuron
The associated tensor computation of subset is assigned to the specific module in module collection 112,114.Then, the spy in module collection 112,114
Cover half group can execute relevant tensor computation to given layer to the group of different neurons.Therefore, calculating module 200 can be with
The parallel of at least two forms is provided：1) a kind of form is included in multiple modules in module collection 112,114 to divide to export and swash
(corresponding with the subset of output neuron) living；And 2) another form includes based on the module in module collection 112,114
The division of middle progress, while calculating and (utilizing single instruction) multiple subsets of output neuron.
Fig. 3 shows example tensor Traversal Unit (TTU) structure 300, example tensor Traversal Unit (TTU) structure
300 include four tensors to be tracked, and the depth of each tensor is 8.TTU 300 generally includes counter tensor 302, step
Width tensor 304, initialization tensor 306 and limitation tensor 308.TTU 300 is with further comprising adder group 310 and tensor
Location index 312.As described above, tensor is hypergeometry object, also, in order to access the element of tensor, it is necessary to be provided
The index of each dimension.Because tensor is stored in narrow memory 210 and wide memory 212, tensor must be indexed
Collection is converted to storage address collection.In some embodiments, by making storage address become rope via tensor allocation index 312
The linear combination for the address drawn and reflected, to complete to index the conversion of storage address.
It is each to control thread there are a TTU in calculating module 200, and each instruction type (TensorOP with
DMAOp) there are a control threads.Therefore, as discussed above, there are two TTU collection in calculating module 200：1)
TensorOp TTU 226；And 2) DMAOp TTU 228.In various embodiments, TensorOp controls 206 will make TTU
300 load TensorOp TTU counters 302, limitation 308 and stride value 304 when specific tensor operation starts, and are referring to
The value of register will not be changed by enabling before retracting.In described two TTU each will need in calculating module 200 generation with
The address of lower memory address port：1) 212 address port of wide memory and 2) tool is deposited there are four the narrow of group of independent arbitration
The group of reservoir 210, this four independent arbitrations is represented as four address ports.
As discussed above, in some embodiments, narrow memory 210 may include memory arbitrator, this is deposited
Reservoir moderator is generally used in shared memory system determining which control device (example allowed to be directed to each memory cycle
Such as, TensorOp controls 206 or DMAOp controls 208) access the shared memory resource of narrow memory 210.Show at one
In example, different instruction type (TensorOp and DMAOp) is independent control thread, and control thread request needs are arbitrated
Memory access.When specific control thread submits tensor element to memory, control thread is opened to submitting to memory
The counter 302 of amount reference carries out increment.
In one example, when TensorOp controls 206 execute the instruction of the element-specific for accessing tensor, TTU
300 can determine the address of the element-specific of the tensor, to allow control 206 to access storage, for example, making narrow memory
210 read the data for the activation value for indicating element-specific.In some embodiments, program may include nested cycle, and
Control 206 can execute the two-dimentional battle array accessed in nested cycle according to current index variate-value associated with nested cycle
The instruction of the element of row variable.
TTU 300 can keep the ergodic state of up to X TTU row of given tensor simultaneously.Reside in TTU 300 simultaneously
In each tensor occupy specialized hardware tensor control descriptor.Hardware controls descriptor can be counted by the X TTU that every line position is set
Number device 302, stride 304 and the limit register 308 for supporting tensor, often there is row dimension the tensor up to X TTU to count
Device.In some embodiments, capable quantity and the quantity of every linage-counter can be different.
For given location register, by including the add operation for being added together location register, to calculate most
Whole storage address.Base address is covered in counter 302.The tensor in the same memory is resided in reference to one shared
Or multiple adders.It in one embodiment, because may only there are one individually on any given port in the circulating cycle
Load/store, so loop nesting control function be to ensure that the multiple tensors resided in identical narrow or wide memory
Its counter-increments is not made with reference in any given cycle.Include to calculate memory reference address value using register
The determination of deviant, entitled " the Matrix Processing Apparatus " submitted on 2 3rd, 2016》Patent application
Serial No. 15/014,265 describes in more detail this, and the entire disclosure is incorporated herein by reference.
For example, when software algorithm handles N-dimensional tensor, nested cycle, each cycle can be used to be responsible for traversing the N-dimensional
Each dimension in amount.Multidimensional tensor can be matrix or multi-dimensional matrix.Each dimension in N-dimensional tensor may include one
Or multiple elements, wherein each element can store corresponding data value.For example, tensor can be the variable in program,
In, which can have there are three dimension.First dimension can have one with the length of 300 elements, the second dimension
The length of thousand elements, and third dimension can be with the length of 20 elements.
Tensor in the nested cycle of traversal can require the memory address value of calculating elements to load or store this yuan
The corresponding data value of element.For example, for cycles are nested cycles, wherein can by three cycles of three loop index variation-trackings
To be nested for traversal three-dimensional tensor.In some cases, processor may need to execute cyclic boundary condition, such as, with outer
The loop boundary recycled in the setting of loop index variable.For example, when determining whether to exit the penetralia cycle of nested cycle, journey
The loop index variable that the current value for the loop index variable that sequence can recycle penetralia is recycled with the most external of nested cycle
Current value be compared.
In general, when the processing unit for calculating module executes the instruction for the element-specific for accessing tensor, tensor Traversal Unit
The address for determining the element-specific of the tensor, to make processing unit that can indicate special with access storage media (memory) to read
Determine the data of the value of element.For example, program may include nested cycle, and processing unit can be according to related to nested cycle
The current of connection indexes variate-value to execute the instruction for the element for accessing the two-dimensional array variable in nested cycle.Based on being followed with nested
The associated current index variate-value of ring, tensor Traversal Unit can determine the inclined of expression and the first element of two-dimensional array variable
The deviant of shifting amount.Then processing unit can use element-specific of the deviant from memory access two-dimensional array variable.
The template parameter that can be used for the special TTU 300 of example is provided below：1) X TTU row；2) every X TTU of row
Counter；3) X TTU adder unit；4) adder reference is shared in each TTU rows instruction；And 5) each counter instruction
X counter size [TTU] [row] [depth].All TTU registers are all visible from framework.Calculating needs to access
Specific tensor element address (that is, tensor address 312) be counter addition results.When from control thread one to TTU
When row sends out increment signal, TTU 300 executes single loop operation and is increased by the stride of the dimension 304 inner most
Dimension, and by all depth (rollover) is called to propagate.
In general, TTU 300 determines state associated with one or more tensor.The state may include loop boundary
Value, previous cycle index variables value, the dimension multiplier for calculating memory address value, and/or for handle branch recycle side
The program counter value on boundary.TTU 300 may include one or more tensor state elements and arithmetic logic unit.Tensor
Each in state elements can be storage element, for example, register or any other suitable storage circuit.At some
In embodiment, tensor state elements can be arranged in different groups physically or in logic, such as in Patent Application Serial
It is more fully described in number 15/014,265.
Fig. 4 shows exemplary architecture, which includes narrow memory 210 will activate 404 via input bus 216
It broadcasts to one or more multiply-accumulate (MAC) arithmetic unit.Shift register 404 provides shift function and will activate whereby
404 are sent to input bus 216, one are sent every time, for one or more MAC operation device 215 in MACcell410
It receives.In general, MACcell410 (including MAC operation device 215) can be defined as to calculate cell, calculating cell calculating sections
With, and in some embodiments, be configured as part and data output bus 218 is written.As shown, cell410
It can be made of one or more MAC operation device.In one embodiment, MAC operation device 215 in MACcell410
Quantity is referred to as the transmitting width of cell.As an example, double transmitting cell refer to the cell of the MAC operation device with there are two, this
Two MAC operation devices can calculate band, and there are two two activation values of parameter (coming from wide memory 212) (to come from narrow memory
210) multiplication and execute the two multipliers product and current portions and result between addition.
As described above, input bus 216 is broadcast bus, which is provided to linear list by input activation
MAC operation device 215 in first (that is, MAC arrays 214).In some embodiments, it is shared between all MAC operation devices 215
Identical input.The width of input bus 216 must sufficiently wide pair with by broadcast input supplied to given MAC arrays 214
Answer the cell of quantity.Following example is considered to illustrate the structure of input bus 216.The quantity of cell in linear unit is equal to
Four and when width being activated to be equal to 8, input bus 216 can be configured as that provide up to four inputs in each cycle sharp
It is living.In this example, the activation that each cell in MAC arrays 214 will only be accessed outside this four activation broadcasted.
TensorOp fields setting based on the instruction received by calculating module 200, the cell of MAC arrays 214 may
It needs to activate using identical input to execute calculating.This can be referred to as the divisions of the Zout in the cell of MAC arrays 214.Together
Sample, when the cell of MAC arrays 214 needs different activation to execute calculating, then the Zin occurred in cell is divided.Previous
In the case of kind, replicates four single inputs and activate and broadcast four activation read from narrow memory 210 within four periods.
In the latter case, each period is required to the reading of narrow memory 210.For example mentioned above, TensorOp
Execution of the control 206 based on the instruction received from controller 102 is come the layout broadcasting method.
Fig. 5 shows exemplary architecture, the exemplary architecture include for will export activation be provided to Fig. 2 with it is narrow in Fig. 4
The output bus 218 of memory cell 210.In general, each MACcell 215 for calculating the MAC arrays 214 in module 200 is counted
Different output activation.However, about output character array, in output depths of features is less than and calculates module 200
In the case of the quantity of MACcell 215, then poly group can be carried out to cell to form one or more cell group.Cell groups
In all MACcell 215 calculate identical output (that is, for output characteristic pattern), however each cell is only calculated and Zin
The subset of the corresponding output of subset of dimension.As a result, the output of MACcell 215 be now part and, rather than final line
Property output.In some embodiments, NLU 222 is incited somebody to action based on the control signal for being provided to NLU 222 by NLU controls 238
It these parts and is integrated into final linear convergent rate.
As discussed above, output bus 218 is continuous-flow type shift register.In various embodiments, work as tensor
The first part of calculating terminate and TensorOp controls 206 indicate (by executing instruction) need write out part and when, will deposit
In the parallel load for the part sum for being provided to output bus 218.The quantity of parallel load by with calculate module 200 in
The quantity of MACcell is corresponding.Then TensorOp controls 206 can be such that the amount of the part sum is moved out of and pass through non-linear flow
Waterline is sent.In some embodiments, it is understood that there may be actually do not execute calculating using all MAC units in module
Situation.In this case, and the not all part being displaced on output bus and be all effective.In this example,
TensorOp controls 206 can provide control signal to indicate the quantity for the effective cell that be moved out of to MAC arrays 214.
The amount for being loaded into the parallel load on output bus 218 will be still corresponding with the quantity of MACcell calculated in module, so
And only virtual value can just be moved out of and be submitted to narrow memory 210.
Fig. 6 is the example flow that the process 600 of tensor computation is executed using the neural computing module 200 in Fig. 2
Figure.Process 600 starts from frame 602 and calculates the narrow memory 210 of module 200 sending out one by one to input activation data/address bus 216
Send (that is, broadcast) activation.Activation value is stored in narrow memory 210.Narrow memory 210 can be static random access memory
The set of device (SRAM) group, the static RAM group allow to be addressed to specific memory location to access input
Amount.The activation read from narrow memory 210 is broadcast to the linear cell of MAC arrays 214 via input activation bus 216
(that is, linear unit), the linear cell include multiple MAC operation devices 215 and and register 220.Frame 604 in process 600
In, the MAC operation device 215 calculated in module 200 receives two inputs respectively：From input activation bus 216 receive one it is defeated
Enter (activation)；And receive another input (parameter) from wide memory 212.Therefore, the activation is to each MAC operation device
An input in the 215 feeding inputs, and each MAC operation device 215 in the cell of MAC arrays 214 leniently stores
Device 212 obtains the input of its second multiplier.
In frame 606 in process 600, the MAC arrays 214 of module 200 are calculated based on the data matrix from memory access
The member of array structure usually execute include dot product calculate tensor computation.Wide memory 212 has the width as unit of position, the width
Width (32) of the degree equal to linear unit.Therefore, linear unit (LU) is connect from vector memory (that is, wide memory 212)
Receive the SIMD Vector Arithmetic Logic Units (ALU) of data.In some embodiments, MAC operation device 215 can also be stored leniently
Device 212 obtains accumulator input (partly and).In some embodiments, relative to 212 port of wide memory, exist for
The time of the relevant reading of two different operation numbers (parameter and part and) and/or write-in is shared.In general, in order to be carried out to area
It optimizes, wide memory 212 can be with the port of limited quantity.As a result, when needing from 212 read operands of wide memory
(for example, parameter) can block and specific operation and at the same time when by operand (for example, part and) write-in wide memory 212
The associated assembly line of number.
In block 608, calculate module 200 calculating cell (have MAC operation device 215 with and register 220) be based on by
MAC/ calculates the multiplying that cell is executed and generates at least one output activation.The result of MACcell operations includes (in part
With during arithmetical operation) write back wide memory part and be sent to output bus 218 output activation.In block 610,
Nonlinear activation function is applied to output activation and narrow storage then is written in the activation by the NLU 222 for calculating module 200
Device 210.In some embodiments, output bus 218 is shift register and can add up from MAC operation device 215
As a result the parallel load of/output activation, but they are removed, per next, so that nonlinear function and write operation are answered
Narrow memory 210 for same module.
Fundamental Digital Circuit, the computer software or firmware visibly realized can be utilized computer hardware, to be included in
This theory is implemented in structure and its structural equivalents disclosed in this specification or one of which or multiple combinations
The embodiment of theme and feature operation described in bright book.The embodiment of theme described in this specification can be embodied as
One or more computer program, that is, coding is on the program carrier of tangible non-transitory to be held by data processing equipment
It goes or to control one or more module of the computer program instructions of the operation of the data processing equipment.Alternatively or
Person is in addition, program instruction can be encoded in manually generated transmitting signal (for example, the electrical of machine generation, optics or electromagnetism
Signal, generate the signal be in order to be used for transmission the information executed to suitable acceptor device for data processing equipment into
Row coding) on.Computer storage media can be machine readable storage device, machine readable storage substrate, random or serial
Access memory devices or one of which or multiple combinations.
The process and logic flow described in this specification can be executed by one or more programmable calculator
Journey, one or more programmable calculator execute one or more computer program with by operation input data and
Output is generated to execute function.Can also by special purpose logic circuitry (for example, FPGA (field programmable gate array),
ASIC (application-specific integrated circuit) or GPGPU (universal graphics processing unit)) come implementation procedure and logic flow, and also may be used
Device is embodied as special purpose logic circuitry (for example, FPGA (field programmable gate array), ASIC (special integrated electricity
Road) or GPGPU (universal graphics processing unit)).
The computer for being suitably executed computer program includes, for example, can be based on general either special microprocessor or
The central processing unit of the two or any other type.In general, central processing unit will receive from read-only memory or
The instruction and data of random access memory or both.The necessary component of computer be for carrying out or executing instruction in
Central Processing Unit and for storing instruction with one or more memory devices of data.In general, computer will also include using
It can in one or more mass memory unit (for example, disk, magneto-optic disk or CD) or computer of storage data
Be operatively coupled with receive the data from the mass memory unit either send data to the mass memory unit or
Both carry out.However, this equipment of computer need not have.
It is suitable for storing computer program instructions and the computer-readable medium of data including the non-volatile of form of ownership
Memory, medium and memory devices, including：For example, semiconductor memory devices are (for example, EPROM, EEPROM and flash are deposited
Storage device), disk (for example, internal hard drive or removable disk).Processor and memory can be by dedicated logic circuit systems
System supplement can be incorporated in the special purpose logic circuitry.
Although this specification contains many specific implementation details, these details should not be considered as to any practicality
The limitation of the range of content that is novel or may being claimed, but as the specific embodiment for specific utility model
Feature description.The certain features described under the context of separate embodiments in the present specification can also be implemented in combination
In single embodiment.On the contrary, the various features described in the context of single embodiment can also be individually or according to any
Suitable sub-portfolio is implemented in various embodiments.In addition, although may describe feature as working in certain combinations
And it is even initially so claimed, but the one or more features in some cases from combination claimed
It can be left out from combination, and combination claimed can be related to the modification of sub-portfolio or sub-portfolio.
Similarly, it is operated although describing in the accompanying drawings according to particular order, this is understood not to require according to institute
The particular order shown either by sequential order execute this operation or execute it is all shown in operation to realize desired knot
Fruit.In some cases, multitasking and parallel processing can be advantageous.In addition, should not will in the above-described embodiments
The separation of various system modules and component be interpreted as needing this separation in all embodiments, and it should be understood that retouched
The program assembly and system stated usually can together be integrated in single software product or be encapsulated into multiple software product.
Further embodiment is summarized in the following example：
Example 1：A kind of computing unit for accelerating tensor computation, the computing unit include：With the first data width
First memory group, the first memory group for store input activation or output activation at least one of；Have
The second memory group of the second big data width than first data width, the second memory group hold for being stored in
One or more parameter that row uses when calculating；At least one cell, at least one cell include being deposited from described second
Reservoir group receives parameter and executes at least one multiply-accumulate (" MAC ") arithmetic unit of calculating；With at least described first storage
Device group into row data communication the first Traversal Unit, first Traversal Unit be configured as to the first memory group provide
Control signal is so that input activation is provided to the data/address bus that can be accessed by MAC operation device；And wherein, the calculating is single
Member executes one or more calculating associated at least one of data array element, one or more of calculating by
The MAC operation device executes and includes partly that the input activation received from the data/address bus is deposited with from described second
The multiplying for the parameter that reservoir group receives.
Example 2：According to the computing unit of example 1, wherein computing unit passes through the loop nesting that execution includes multiple cycles
One or more calculating in the calculating is executed, wherein the structure of the loop nesting includes single by first traversal
Member uses the respective cycle of one or more dimension to traverse the data array.
Example 3：According to the computing unit of example 2, wherein one or more of calculating sections based on by described the
The tensor operation that one Traversal Unit provides executes, which includes for accessing one or more in data array
The nested loop structure of element.
Example 4：According to the computing unit of example 1 to 3, further comprise the second Traversal Unit, the second Traversal Unit quilt
It is configured to deposit to access described at least one of first memory group from the instruction that the source outside the computing unit receives
At least one processor position of memory location and the second memory group.
Example 5：According to the computing unit of example 4, wherein first Traversal Unit is tensor operation Traversal Unit, and
And second Traversal Unit is direct memory access Traversal Unit, and wherein, data array with including multiple elements
Tensor is corresponding.
Example 6：According to the computing unit of example 1 to 5, wherein the computing unit includes non-linear unit, and described
The first part of calculating includes generating one or more output activation, and the second part of the calculating based on multiplying
Including：Nonlinear function one or more output is applied to by the non-linear unit to activate.
Example 7：According to the computing unit of example 6, wherein wrapped by one or more calculating that the computing unit executes
It includes and output activation is displaced to the first memory group using shift register.
Example 8：According to the computing unit of example 1 to 8, the annular for further comprising extending to outside the computing unit is total
The part of line, wherein the ring bus between the first memory group and the memory group of another neighboring computational unit with
And provide data path between the second memory group and the memory group of another neighboring computational unit.
Example 9:According to an exemplary computing unit in example 1 to 8, wherein second memory group is configured as depositing
At least one of storage part and/or one or more pond layer input.
Example 10：A kind of computer implemented method for accelerating tensor computation, this method include：It is deposited in response to first
Reservoir group receives the control signal from the first Traversal Unit, passes through the first memory group with the first data width, hair
Send the first input to activate, the first memory group is arranged in computing unit, and wherein, the first input activation by
It can be provided by the data/address bus that at least cell of the computing unit is accessed；By at least one cell, reception comes from
One or more parameter of second memory group with second data width bigger than first data width, and its
In, which includes at least one multiply-accumulate (" MAC ") arithmetic unit；By the MAC operation device, executes and count
According to one or more the associated calculating of at least one of array element, wherein one or more of calculating sections
Ground include accessed from the data/address bus it is at least described first input activation and received from the second memory group to
The multiplying of a few parameter.
Example 11：According to the computer implemented method of example 10, wherein one or more of calculating section grounds
It is executed in computing unit execution includes the loop nesting of multiple cycles, wherein described in the structure instruction of the loop nesting
First Traversal Unit traverses the mode of one or more dimension in the data array.
Example 12：According to the computer implemented method of example 11, further comprise：By first Traversal Unit,
Offer includes the tensor operation of the nested loop structure of one or more element for accessing the data array.
Example 13：According to an exemplary computer implemented method in example 10 to 12, wherein first traversal
Unit is tensor operation Traversal Unit, and second Traversal Unit is direct memory access Traversal Unit, and wherein,
Data array is corresponding with including the tensor of multiple elements.
Example 14：According to an exemplary computer implemented method in example 10 to 13, further comprise：Pass through base
At least one output activation of generation is calculated in the multiplication to execute the first part of one or more of calculating.
Example 15：According to the computer implemented method of example 14, further comprise：By the way that nonlinear function is applied to
One or more of outputs activate to execute the second part of one or more of calculating.
Example 16：A kind of non-transitory computer-readable storage media, the non-transitory computer-readable storage media packet
The instruction that can be executed by one or more processor is included, which makes one or more of places in such execution
Reason device executes operation, which includes：The letter of the control from the first Traversal Unit is received in response to the first memory group
Number, by the first memory group with the first data width, the first input activation is sent, the first memory group is set
In computing unit, and wherein, the first input activation is by that can pass through the number of at least cell of computing unit access
It is provided according to bus；By at least one cell, receive from wide with second data bigger than first data width
One or more parameter of the second memory group of degree, and wherein, which includes that at least one multiplication is tired
Add (" MAC ") arithmetic unit；And it by the MAC operation device, executes associated at least one of data array element
One or more is calculated, wherein includes one or more of calculating sections being accessed at least from the data/address bus
The multiplying of first the input activation and at least one parameter received from the second memory group.
Example 17：According to the non-transitory computer-readable medium of example 16, wherein one or more of calculating parts
Divide ground based on computing unit execution includes the loop nesting of multiple cycles to execute, wherein the structure of the loop nesting refers to
Show that first Traversal Unit traverses the mode of one or more dimension in the data array.
Example 18：According to the non-transitory computer-readable medium of example 17, further comprise：It is traversed by described first
Unit provides the tensor operation of the nested loop structure including one or more element for accessing the data array.
Example 19：According to an exemplary non-transitory computer-readable medium in example 16 to 18, further comprise：
The first part of one or more of calculating is executed by calculating at least one output activation of generation based on the multiplication.
Example 20：According to the non-transitory computer-readable medium of example 19, further comprise：By by nonlinear function
One or more of output activation are applied to execute the second part of one or more of calculating.
The specific embodiment of this theme has been described.Other embodiments are in the scope of the following claims.For example,
The action described in detail in the claims can execute and desired result still may be implemented in a different order.Make
For an example, the process described in the accompanying drawings be not necessarily required to shown in particular order or sequential order, with realize it is expected
Result.In some embodiments, multitasking and parallel processing can be advantageous.
Claims (5)
1. a kind of computing unit for accelerating tensor computation, the computing unit include：
First memory group with the first data width, the first memory group is for storing input activation and output activation
At least one of；
Second memory group with second data width bigger than first data width, the second memory group are used for
It stores and is executing one or more parameter used when calculating；
At least one cell, at least one cell include receiving parameter from the second memory group and executing calculating
At least one multiply-accumulate (" MAC ") arithmetic unit；
With at least described first memory group into the first Traversal Unit of row data communication, first Traversal Unit is configured as
Control signal, which is provided, to the first memory group is provided to and can be visited by the multiply-accumulate arithmetic unit so as to input activation
The data/address bus asked；And
Wherein, the computing unit is executed related at least one of data array element by the multiply-accumulate arithmetic unit
One or more calculating of connection, one or more of calculate are swashed including at least the input received from the data/address bus
Multiplying living with the parameter received from the second memory group.
2. computing unit according to claim 1, further comprise the second Traversal Unit, second Traversal Unit by with
It is set to and at least one storage of the first memory group is accessed based on the instruction received from the source outside the computing unit
At least one processor position of device position and the second memory group.
3. computing unit according to claim 1, wherein the one or more of meters executed by the computing unit
Calculation includes that one or more of outputs activation is displaced to the first memory group using shift register.
4. computing unit according to claim 1 further comprises extending to the ring bus outside the computing unit
Part, wherein the ring bus between the first memory group and the memory group of another neighboring computational unit with
And provide data path between the second memory group and the memory group of another neighboring computational unit.
5. computing unit according to claim 1, wherein the second memory group be configured as storage section and and
At least one of one or more pond layer input.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/335,769 US10175980B2 (en) | 2016-10-27 | 2016-10-27 | Neural network compute tile |
US15/335,769 | 2016-10-27 |
Publications (1)
Publication Number | Publication Date |
---|---|
CN207895435U true CN207895435U (en) | 2018-09-21 |
Family
ID=59296600
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201710889996.XA Pending CN108009106A (en) | 2016-10-27 | 2017-09-27 | Neural computing module |
CN201721254418.0U Active CN207895435U (en) | 2016-10-27 | 2017-09-27 | Neural computing module |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201710889996.XA Pending CN108009106A (en) | 2016-10-27 | 2017-09-27 | Neural computing module |
Country Status (10)
Country | Link |
---|---|
US (4) | US10175980B2 (en) |
EP (1) | EP3533001A1 (en) |
JP (2) | JP6995851B2 (en) |
KR (2) | KR102317668B1 (en) |
CN (2) | CN108009106A (en) |
DE (2) | DE102017121825A1 (en) |
GB (1) | GB2555936B (en) |
HK (1) | HK1254698A1 (en) |
SG (1) | SG11201903684RA (en) |
WO (1) | WO2018080617A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108009106A (en) * | 2016-10-27 | 2018-05-08 | 谷歌公司 | Neural computing module |
Families Citing this family (106)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9875104B2 (en) | 2016-02-03 | 2018-01-23 | Google Llc | Accessing data in multi-dimensional tensors |
KR102459855B1 (en) * | 2016-06-14 | 2022-10-27 | 삼성전자주식회사 | Accelerator for deep neural networks |
US9959498B1 (en) * | 2016-10-27 | 2018-05-01 | Google Llc | Neural network instruction set architecture |
US10360163B2 (en) | 2016-10-27 | 2019-07-23 | Google Llc | Exploiting input data sparsity in neural network compute units |
US10891534B2 (en) | 2017-01-11 | 2021-01-12 | International Business Machines Corporation | Neural network reinforcement learning |
US11157801B2 (en) * | 2017-02-28 | 2021-10-26 | Microsoft Technology Licensing, Llc | Neural network processing with the neural network model pinned to on-chip memories of hardware nodes |
US10365987B2 (en) | 2017-03-29 | 2019-07-30 | Google Llc | Synchronous hardware event collection |
US9875167B1 (en) | 2017-03-29 | 2018-01-23 | Google Inc. | Distributed hardware tracing |
US10817293B2 (en) * | 2017-04-28 | 2020-10-27 | Tenstorrent Inc. | Processing core with metadata actuated conditional graph execution |
CN108875956B (en) * | 2017-05-11 | 2019-09-10 | 广州异构智能科技有限公司 | Primary tensor processor |
US11169732B2 (en) * | 2017-05-18 | 2021-11-09 | Kabushiki Kaisha Toshiba | Computing device |
US10248908B2 (en) | 2017-06-19 | 2019-04-02 | Google Llc | Alternative loop limits for accessing data in multi-dimensional tensors |
CN107729989B (en) * | 2017-07-20 | 2020-12-29 | 安徽寒武纪信息科技有限公司 | Device and method for executing artificial neural network forward operation |
US10790828B1 (en) | 2017-07-21 | 2020-09-29 | X Development Llc | Application specific integrated circuit accelerators |
US10879904B1 (en) * | 2017-07-21 | 2020-12-29 | X Development Llc | Application specific integrated circuit accelerators |
GB2568776B (en) * | 2017-08-11 | 2020-10-28 | Google Llc | Neural network accelerator with parameters resident on chip |
US11243880B1 (en) | 2017-09-15 | 2022-02-08 | Groq, Inc. | Processor architecture |
US11360934B1 (en) | 2017-09-15 | 2022-06-14 | Groq, Inc. | Tensor streaming processor architecture |
US11868804B1 (en) * | 2019-11-18 | 2024-01-09 | Groq, Inc. | Processor instruction dispatch configuration |
US11114138B2 (en) | 2017-09-15 | 2021-09-07 | Groq, Inc. | Data structures with multiple read ports |
EP3682330B1 (en) * | 2017-09-21 | 2022-08-24 | Huawei Technologies Co., Ltd. | Multi-thread systolic array |
US11170307B1 (en) | 2017-09-21 | 2021-11-09 | Groq, Inc. | Predictive model compiler for generating a statically scheduled binary with known resource constraints |
US11620490B2 (en) * | 2017-10-17 | 2023-04-04 | Xilinx, Inc. | Multi-layer neural network processing by a neural network accelerator using host communicated merged weights and a package of per-layer instructions |
US11694066B2 (en) | 2017-10-17 | 2023-07-04 | Xilinx, Inc. | Machine learning runtime library for neural network acceleration |
US11222256B2 (en) * | 2017-10-17 | 2022-01-11 | Xilinx, Inc. | Neural network processing system having multiple processors and a neural network accelerator |
GB2568086B (en) * | 2017-11-03 | 2020-05-27 | Imagination Tech Ltd | Hardware implementation of convolution layer of deep neutral network |
US10936942B2 (en) * | 2017-11-21 | 2021-03-02 | Google Llc | Apparatus and mechanism for processing neural network tasks using a single chip package with multiple identical dies |
JP6877594B2 (en) * | 2017-11-21 | 2021-05-26 | グーグル エルエルシーＧｏｏｇｌｅ ＬＬＣ | Low power ambient computing system with machine learning |
US20190179635A1 (en) * | 2017-12-11 | 2019-06-13 | Futurewei Technologies, Inc. | Method and apparatus for tensor and convolution operations |
US10747631B2 (en) * | 2018-01-19 | 2020-08-18 | DinoplusAI Holdings Limited | Mission-critical AI processor with record and replay support |
CN111742331A (en) * | 2018-02-16 | 2020-10-02 | 多伦多大学管理委员会 | Neural network accelerator |
US10908879B2 (en) | 2018-03-02 | 2021-02-02 | Neuchips Corporation | Fast vector multiplication and accumulation circuit |
US11614941B2 (en) | 2018-03-30 | 2023-03-28 | Qualcomm Incorporated | System and method for decoupling operations to accelerate processing of loop structures |
US10657442B2 (en) * | 2018-04-19 | 2020-05-19 | International Business Machines Corporation | Deep learning accelerator architecture with chunking GEMM |
US11501138B1 (en) * | 2018-04-20 | 2022-11-15 | Perceive Corporation | Control circuits for neural network inference circuit |
US11481612B1 (en) | 2018-04-20 | 2022-10-25 | Perceive Corporation | Storage of input values across multiple cores of neural network inference circuit |
US10977338B1 (en) | 2018-04-20 | 2021-04-13 | Perceive Corporation | Reduced-area circuit for dot product computation |
US11783167B1 (en) | 2018-04-20 | 2023-10-10 | Perceive Corporation | Data transfer for non-dot product computations on neural network inference circuit |
US11568227B1 (en) | 2018-04-20 | 2023-01-31 | Perceive Corporation | Neural network inference circuit read controller with multiple operational modes |
US20190325295A1 (en) * | 2018-04-20 | 2019-10-24 | International Business Machines Corporation | Time, space, and energy efficient neural inference via parallelism and on-chip memory |
US11556762B2 (en) | 2018-04-21 | 2023-01-17 | Microsoft Technology Licensing, Llc | Neural network processor based on application specific synthesis specialization parameters |
US20190332925A1 (en) * | 2018-04-30 | 2019-10-31 | International Business Machines Corporation | Neural hardware accelerator for parallel and distributed tensor computations |
US20190392287A1 (en) * | 2018-06-22 | 2019-12-26 | Samsung Electronics Co., Ltd. | Neural processor |
US11663461B2 (en) | 2018-07-05 | 2023-05-30 | International Business Machines Corporation | Instruction distribution in an array of neural network cores |
US20200019836A1 (en) * | 2018-07-12 | 2020-01-16 | International Business Machines Corporation | Hierarchical parallelism in a network of distributed neural network cores |
CN109117949A (en) * | 2018-08-01 | 2019-01-01 | 南京天数智芯科技有限公司 | Flexible data stream handle and processing method for artificial intelligence equipment |
CN110796244B (en) * | 2018-08-01 | 2022-11-08 | 上海天数智芯半导体有限公司 | Core computing unit processor for artificial intelligence device and accelerated processing method |
US10817042B2 (en) * | 2018-09-27 | 2020-10-27 | Intel Corporation | Power savings for neural network architecture with zero activations during inference |
US10834024B2 (en) * | 2018-09-28 | 2020-11-10 | International Business Machines Corporation | Selective multicast delivery on a bus-based interconnect |
US20210382691A1 (en) * | 2018-10-15 | 2021-12-09 | The Board Of Trustees Of The University Of Illinois | In-Memory Near-Data Approximate Acceleration |
KR102637733B1 (en) | 2018-10-31 | 2024-02-19 | 삼성전자주식회사 | Neural network processor and convolution operation method thereof |
US11204976B2 (en) | 2018-11-19 | 2021-12-21 | Groq, Inc. | Expanded kernel generation |
US11361050B2 (en) | 2018-11-20 | 2022-06-14 | Hewlett Packard Enterprise Development Lp | Assigning dependent matrix-vector multiplication operations to consecutive crossbars of a dot product engine |
US11263011B2 (en) | 2018-11-28 | 2022-03-01 | International Business Machines Corporation | Compound instruction set architecture for a neural inference chip |
JP7189000B2 (en) * | 2018-12-12 | 2022-12-13 | 日立Astemo株式会社 | Information processing equipment, in-vehicle control equipment, vehicle control system |
US20200202198A1 (en) * | 2018-12-21 | 2020-06-25 | Waymo Llc | Neural network processor |
US11144282B2 (en) | 2019-01-16 | 2021-10-12 | Mediatek Inc. | Mathematical accelerator for artificial intelligence applications |
US11347297B1 (en) | 2019-01-23 | 2022-05-31 | Perceive Corporation | Neural network inference circuit employing dynamic memory sleep |
US10824370B2 (en) * | 2019-02-07 | 2020-11-03 | quadric.io, Inc. | Systems and methods for implementing random access memory in a flow-based machine perception and dense algorithm integrated circuit based on computing and coalescing of indices |
US11023379B2 (en) | 2019-02-13 | 2021-06-01 | Google Llc | Low-power cached ambient computing |
US11748599B2 (en) * | 2019-02-21 | 2023-09-05 | Texas Instruments Incorporated | Super-tiling in neural network processing to enable analytics at lower memory speed |
CN111782133A (en) * | 2019-04-04 | 2020-10-16 | 安徽寒武纪信息科技有限公司 | Data processing method and device and related product |
US11176493B2 (en) * | 2019-04-29 | 2021-11-16 | Google Llc | Virtualizing external memory as local to a machine learning accelerator |
US11625585B1 (en) | 2019-05-21 | 2023-04-11 | Perceive Corporation | Compiler for optimizing filter sparsity for neural network implementation configuration |
JP7278150B2 (en) | 2019-05-23 | 2023-05-19 | キヤノン株式会社 | Image processing device, imaging device, image processing method |
US20210026686A1 (en) * | 2019-07-22 | 2021-01-28 | Advanced Micro Devices, Inc. | Chiplet-integrated machine learning accelerators |
GB2586277B (en) | 2019-08-16 | 2022-11-23 | Siemens Ind Software Inc | Broadcasting event messages in a system on chip |
GB2586279B (en) | 2019-08-16 | 2022-11-23 | Siemens Ind Software Inc | Routing messages in a integrated circuit chip device |
GB2586278B (en) * | 2019-08-16 | 2022-11-23 | Siemens Ind Software Inc | Addressing mechanism for a system on chip |
CN114270319A (en) * | 2019-10-07 | 2022-04-01 | 谷歌有限责任公司 | Reallocating tensor elements among machine learning computation units |
US11327690B2 (en) | 2019-11-13 | 2022-05-10 | Google Llc | Enhanced input of machine-learning accelerator activations |
CN112990421B (en) * | 2019-12-02 | 2023-09-05 | 杭州海康威视数字技术股份有限公司 | Method, device and storage medium for optimizing operation process of deep learning network |
KR20210071471A (en) | 2019-12-06 | 2021-06-16 | 삼성전자주식회사 | Apparatus and method for performing matrix multiplication operation of neural network |
KR20210077352A (en) * | 2019-12-17 | 2021-06-25 | 에스케이하이닉스 주식회사 | Data Processing System and accelerating DEVICE therefor |
US20220413592A1 (en) | 2019-12-17 | 2022-12-29 | Google Llc | Low-power vision sensing |
US20220335945A1 (en) | 2019-12-18 | 2022-10-20 | Google Llc | Machine learning based privacy processing |
WO2021126203A1 (en) * | 2019-12-19 | 2021-06-24 | Google Llc | Processing sequential inputs using neural network accelerators |
KR20210085461A (en) | 2019-12-30 | 2021-07-08 | 삼성전자주식회사 | Processing apparatus and method for processing floating point operation thereof |
US11537323B2 (en) * | 2020-01-07 | 2022-12-27 | SK Hynix Inc. | Processing-in-memory (PIM) device |
DE102020201182A1 (en) * | 2020-01-31 | 2021-08-05 | Robert Bosch Gesellschaft mit beschränkter Haftung | Hardware-accelerated calculation of convolutions |
US11630991B2 (en) | 2020-02-04 | 2023-04-18 | Apple Inc. | Broadcasting mode of planar engine for neural processor |
US11226816B2 (en) | 2020-02-12 | 2022-01-18 | Samsung Electronics Co., Ltd. | Systems and methods for data placement for in-memory-compute |
US11281554B2 (en) | 2020-03-17 | 2022-03-22 | Samsung Electronics Co., Ltd. | System and method for in-memory computation |
CN113537476A (en) * | 2020-04-16 | 2021-10-22 | 中科寒武纪科技股份有限公司 | Arithmetic device and related product |
US11928176B2 (en) * | 2020-07-30 | 2024-03-12 | Arm Limited | Time domain unrolling sparse matrix multiplication system and method |
US11954580B2 (en) | 2020-09-16 | 2024-04-09 | Meta Platforms, Inc. | Spatial tiling of compute arrays with shared control |
US20220114440A1 (en) * | 2020-10-14 | 2022-04-14 | Google Llc | Queue Allocation in Machine Learning Accelerators |
US11704562B1 (en) | 2020-11-04 | 2023-07-18 | Meta Platforms, Inc. | Architecture for virtual instructions |
US11709783B1 (en) | 2020-11-11 | 2023-07-25 | Meta Platforms, Inc. | Tensor data distribution using grid direct-memory access (DMA) controller |
US11972349B1 (en) | 2020-11-12 | 2024-04-30 | Meta Platforms, Inc. | Flexible compute array utilization in a tensor processor |
US20220188492A1 (en) * | 2020-12-10 | 2022-06-16 | Memryx Incorporated | Chiplet based artificial intelligence accelerators and configuration methods |
US11922306B2 (en) | 2020-12-28 | 2024-03-05 | Meta Platforms, Inc. | Tensor controller architecture |
US11790611B2 (en) | 2020-12-30 | 2023-10-17 | Meta Platforms, Inc. | Visual editor for designing augmented-reality effects that utilize voice recognition |
CN112596684B (en) | 2021-03-08 | 2021-06-22 | 成都启英泰伦科技有限公司 | Data storage method for voice deep neural network operation |
US20220405557A1 (en) * | 2021-06-10 | 2022-12-22 | Samsung Electronics Co., Ltd. | Sram-sharing for reconfigurable neural processing units |
US11675592B2 (en) | 2021-06-17 | 2023-06-13 | International Business Machines Corporation | Instruction to query for model-dependent information |
US11269632B1 (en) | 2021-06-17 | 2022-03-08 | International Business Machines Corporation | Data conversion to/from selected data type with implied rounding mode |
US11797270B2 (en) | 2021-06-17 | 2023-10-24 | International Business Machines Corporation | Single function to perform multiple operations with distinct operation parameter validation |
US11693692B2 (en) | 2021-06-17 | 2023-07-04 | International Business Machines Corporation | Program event recording storage alteration processing for a neural network accelerator instruction |
US11669331B2 (en) | 2021-06-17 | 2023-06-06 | International Business Machines Corporation | Neural network processing assist instruction |
US11734013B2 (en) | 2021-06-17 | 2023-08-22 | International Business Machines Corporation | Exception summary for invalid values detected during instruction execution |
US11694733B2 (en) | 2021-08-19 | 2023-07-04 | Apple Inc. | Acceleration of in-memory-compute arrays |
US11494321B1 (en) * | 2021-09-30 | 2022-11-08 | Amazon Technologies, Inc. | State buffer memloc reshaping |
KR102395743B1 (en) * | 2021-11-09 | 2022-05-09 | 오픈엣지테크놀로지 주식회사 | Pooling method for 1-dimensional array and a device for the same |
CN115599442B (en) * | 2022-12-14 | 2023-03-10 | 成都登临科技有限公司 | AI chip, electronic equipment and tensor processing method |
CN117093509B (en) * | 2023-10-18 | 2024-01-26 | 上海为旌科技有限公司 | On-chip memory address allocation method and system based on greedy algorithm |
Family Cites Families (74)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US3754128A (en) | 1971-08-31 | 1973-08-21 | M Corinthios | High speed signal processor for vector transformation |
JPS4874139A (en) | 1971-12-29 | 1973-10-05 | ||
JPS5364439A (en) | 1976-11-20 | 1978-06-08 | Agency Of Ind Science & Technol | Linear coversion system |
JPS58134357A (en) | 1982-02-03 | 1983-08-10 | Hitachi Ltd | Array processor |
EP0156648B1 (en) | 1984-03-29 | 1992-09-30 | Kabushiki Kaisha Toshiba | Convolution arithmetic circuit for digital signal processing |
US5267185A (en) | 1989-04-14 | 1993-11-30 | Sharp Kabushiki Kaisha | Apparatus for calculating matrices |
US5138695A (en) | 1989-10-10 | 1992-08-11 | Hnc, Inc. | Systolic array image processing system |
JPH03167664A (en) | 1989-11-28 | 1991-07-19 | Nec Corp | Matrix arithmetic circuit |
JP3210319B2 (en) * | 1990-03-01 | 2001-09-17 | 株式会社東芝 | Neurochip and neurocomputer using the chip |
WO1991019248A1 (en) | 1990-05-30 | 1991-12-12 | Adaptive Solutions, Inc. | Neural network using virtual-zero |
WO1991019267A1 (en) | 1990-06-06 | 1991-12-12 | Hughes Aircraft Company | Neural network processor |
JP3251947B2 (en) | 1991-03-27 | 2002-01-28 | 株式会社日立国際電気 | Automatic gain control circuit |
JP3318753B2 (en) | 1991-12-05 | 2002-08-26 | ソニー株式会社 | Product-sum operation device and product-sum operation method |
AU658066B2 (en) | 1992-09-10 | 1995-03-30 | Deere & Company | Neural network based control system |
JPH06139218A (en) * | 1992-10-30 | 1994-05-20 | Hitachi Ltd | Method and device for simulating neural network completely in parallel by using digital integrated circuit |
JPH076146A (en) * | 1993-06-18 | 1995-01-10 | Fujitsu Ltd | Parallel data processing system |
US6067536A (en) | 1996-05-30 | 2000-05-23 | Matsushita Electric Industrial Co., Ltd. | Neural network for voice and pattern recognition |
US5742741A (en) | 1996-07-18 | 1998-04-21 | Industrial Technology Research Institute | Reconfigurable neural network |
EP0834817B1 (en) | 1996-10-01 | 2000-08-02 | FINMECCANICA S.p.A. AZIENDA ANSALDO | Programmed neural module |
US5905757A (en) | 1996-10-04 | 1999-05-18 | Motorola, Inc. | Filter co-processor |
JPH117432A (en) * | 1997-06-16 | 1999-01-12 | Hitachi Ltd | Information processor and semiconductor device |
US6243734B1 (en) * | 1998-10-30 | 2001-06-05 | Intel Corporation | Computer product and method for sparse matrices |
JP2001117900A (en) * | 1999-10-19 | 2001-04-27 | Fuji Xerox Co Ltd | Neural network arithmetic device |
US20020044695A1 (en) | 2000-05-05 | 2002-04-18 | Bostrom Alistair K. | Method for wavelet-based compression of video images |
JP2003244190A (en) | 2002-02-19 | 2003-08-29 | Matsushita Electric Ind Co Ltd | Processor for data flow control switch and data flow control switch |
US7016529B2 (en) | 2002-03-15 | 2006-03-21 | Microsoft Corporation | System and method facilitating pattern recognition |
US7493498B1 (en) | 2002-03-27 | 2009-02-17 | Advanced Micro Devices, Inc. | Input/output permission bitmaps for compartmentalized security |
US20070124565A1 (en) * | 2003-06-18 | 2007-05-31 | Ambric, Inc. | Reconfigurable processing array having hierarchical communication network |
US7426501B2 (en) | 2003-07-18 | 2008-09-16 | Knowntech, Llc | Nanotechnology neural network methods and systems |
US7818729B1 (en) | 2003-09-15 | 2010-10-19 | Thomas Plum | Automated safe secure techniques for eliminating undefined behavior in computer software |
JP2007518199A (en) | 2004-01-13 | 2007-07-05 | ニューヨーク・ユニバーシティ | Method, system, storage medium, and data structure for image recognition using multiple linear independent element analysis |
GB2436377B (en) | 2006-03-23 | 2011-02-23 | Cambridge Display Tech Ltd | Data processing hardware |
CN101441441B (en) | 2007-11-21 | 2010-06-30 | 新乡市起重机厂有限公司 | Design method of intelligent swing-proof control system of crane |
JP4513865B2 (en) | 2008-01-25 | 2010-07-28 | セイコーエプソン株式会社 | Parallel computing device and parallel computing method |
CN102037652A (en) | 2008-05-21 | 2011-04-27 | Nxp股份有限公司 | A data handling system comprising memory banks and data rearrangement |
US8321652B2 (en) | 2008-08-01 | 2012-11-27 | Infineon Technologies Ag | Process and method for logical-to-physical address mapping using a volatile memory device in solid state disks |
JP2010039625A (en) * | 2008-08-01 | 2010-02-18 | Renesas Technology Corp | Parallel arithmetic device |
JP5376920B2 (en) * | 2008-12-04 | 2013-12-25 | キヤノン株式会社 | Convolution operation circuit, hierarchical convolution operation circuit, and object recognition device |
EP2290563B1 (en) | 2009-08-28 | 2017-12-13 | Accenture Global Services Limited | Accessing content in a network |
US8589600B2 (en) | 2009-12-14 | 2013-11-19 | Maxeler Technologies, Ltd. | Method of transferring data with offsets |
US8595467B2 (en) | 2009-12-29 | 2013-11-26 | International Business Machines Corporation | Floating point collect and operate |
US8457767B2 (en) | 2010-12-31 | 2013-06-04 | Brad Radl | System and method for real-time industrial process modeling |
US8977629B2 (en) * | 2011-05-24 | 2015-03-10 | Ebay Inc. | Image-based popularity prediction |
US8812414B2 (en) | 2011-05-31 | 2014-08-19 | International Business Machines Corporation | Low-power event-driven neural computing architecture in neural networks |
JP5911165B2 (en) * | 2011-08-05 | 2016-04-27 | 株式会社メガチップス | Image recognition device |
US8909576B2 (en) | 2011-09-16 | 2014-12-09 | International Business Machines Corporation | Neuromorphic event-driven neural computing architecture in a scalable neural network |
US9153230B2 (en) * | 2012-10-23 | 2015-10-06 | Google Inc. | Mobile speech recognition hardware accelerator |
US9201828B2 (en) * | 2012-10-23 | 2015-12-01 | Analog Devices, Inc. | Memory interconnect network architecture for vector processor |
US9921832B2 (en) * | 2012-12-28 | 2018-03-20 | Intel Corporation | Instruction to reduce elements in a vector register with strided access pattern |
US9811116B2 (en) * | 2013-05-24 | 2017-11-07 | Qualcomm Incorporated | Utilization and configuration of wireless docking environments |
US20150071020A1 (en) * | 2013-09-06 | 2015-03-12 | Sony Corporation | Memory device comprising tiles with shared read and write circuits |
US20160026912A1 (en) | 2014-07-22 | 2016-01-28 | Intel Corporation | Weight-shifting mechanism for convolutional neural networks |
CN110110843B (en) | 2014-08-29 | 2020-09-25 | 谷歌有限责任公司 | Method and system for processing images |
CN104463209B (en) | 2014-12-08 | 2017-05-24 | 福建坤华仪自动化仪器仪表有限公司 | Method for recognizing digital code on PCB based on BP neural network |
US10223635B2 (en) * | 2015-01-22 | 2019-03-05 | Qualcomm Incorporated | Model compression and fine-tuning |
US10013652B2 (en) | 2015-04-29 | 2018-07-03 | Nuance Communications, Inc. | Fast deep neural network feature transformation via optimized memory bandwidth utilization |
US10262259B2 (en) | 2015-05-08 | 2019-04-16 | Qualcomm Incorporated | Bit width selection for fixed point neural networks |
US10489703B2 (en) * | 2015-05-20 | 2019-11-26 | Nec Corporation | Memory efficiency for convolutional neural networks operating on graphics processing units |
US9747546B2 (en) * | 2015-05-21 | 2017-08-29 | Google Inc. | Neural network processor |
US10671564B2 (en) | 2015-10-08 | 2020-06-02 | Via Alliance Semiconductor Co., Ltd. | Neural network unit that performs convolutions using collective shift register among array of neural processing units |
US9875104B2 (en) * | 2016-02-03 | 2018-01-23 | Google Llc | Accessing data in multi-dimensional tensors |
US10552119B2 (en) | 2016-04-29 | 2020-02-04 | Intel Corporation | Dynamic management of numerical representation in a distributed matrix processor architecture |
CN106023065B (en) | 2016-05-13 | 2019-02-19 | 中国矿业大学 | A kind of tensor type high spectrum image spectral-spatial dimension reduction method based on depth convolutional neural networks |
CN106127297B (en) | 2016-06-02 | 2019-07-12 | 中国科学院自动化研究所 | The acceleration of depth convolutional neural networks based on tensor resolution and compression method |
US10175980B2 (en) | 2016-10-27 | 2019-01-08 | Google Llc | Neural network compute tile |
US9959498B1 (en) | 2016-10-27 | 2018-05-01 | Google Llc | Neural network instruction set architecture |
US10360163B2 (en) | 2016-10-27 | 2019-07-23 | Google Llc | Exploiting input data sparsity in neural network compute units |
US10733505B2 (en) * | 2016-11-10 | 2020-08-04 | Google Llc | Performing kernel striding in hardware |
CN106529511B (en) | 2016-12-13 | 2019-12-10 | 北京旷视科技有限公司 | image structuring method and device |
US10037490B2 (en) * | 2016-12-13 | 2018-07-31 | Google Llc | Performing average pooling in hardware |
US20180189675A1 (en) * | 2016-12-31 | 2018-07-05 | Intel Corporation | Hardware accelerator architecture and template for web-scale k-means clustering |
US11164071B2 (en) * | 2017-04-18 | 2021-11-02 | Samsung Electronics Co., Ltd. | Method and apparatus for reducing computational complexity of convolutional neural networks |
US10621489B2 (en) * | 2018-03-30 | 2020-04-14 | International Business Machines Corporation | Massively parallel neural inference computing elements |
US10572409B1 (en) * | 2018-05-10 | 2020-02-25 | Xilinx, Inc. | Sparse matrix processing circuitry |
-
2016
- 2016-10-27 US US15/335,769 patent/US10175980B2/en active Active
-
2017
- 2017-03-17 US US15/462,180 patent/US9710265B1/en active Active
- 2017-08-15 SG SG11201903684RA patent/SG11201903684RA/en unknown
- 2017-08-15 WO PCT/US2017/046963 patent/WO2018080617A1/en active Search and Examination
- 2017-08-15 JP JP2019522728A patent/JP6995851B2/en active Active
- 2017-08-15 EP EP17758698.9A patent/EP3533001A1/en active Pending
- 2017-08-15 KR KR1020197014238A patent/KR102317668B1/en active IP Right Grant
- 2017-08-15 KR KR1020217033780A patent/KR102387334B1/en active IP Right Grant
- 2017-09-14 GB GB1714815.6A patent/GB2555936B/en active Active
- 2017-09-20 DE DE102017121825.4A patent/DE102017121825A1/en active Pending
- 2017-09-20 DE DE202017105708.9U patent/DE202017105708U1/en active Active
- 2017-09-27 CN CN201710889996.XA patent/CN108009106A/en active Pending
- 2017-09-27 CN CN201721254418.0U patent/CN207895435U/en active Active
-
2018
- 2018-10-25 HK HK18113683.5A patent/HK1254698A1/en unknown
-
2019
- 2019-01-04 US US16/239,760 patent/US11422801B2/en active Active
-
2021
- 2021-12-15 JP JP2021203544A patent/JP7451483B2/en active Active
-
2022
- 2022-08-22 US US17/892,807 patent/US11816480B2/en active Active
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN108009106A (en) * | 2016-10-27 | 2018-05-08 | 谷歌公司 | Neural computing module |
Also Published As
Publication number | Publication date |
---|---|
WO2018080617A1 (en) | 2018-05-03 |
DE202017105708U1 (en) | 2018-01-03 |
US10175980B2 (en) | 2019-01-08 |
JP6995851B2 (en) | 2022-01-17 |
US11816480B2 (en) | 2023-11-14 |
US20190213005A1 (en) | 2019-07-11 |
EP3533001A1 (en) | 2019-09-04 |
US20230004386A1 (en) | 2023-01-05 |
GB201714815D0 (en) | 2017-11-01 |
CN108009106A (en) | 2018-05-08 |
JP2022046552A (en) | 2022-03-23 |
GB2555936B (en) | 2019-01-30 |
KR102317668B1 (en) | 2021-10-26 |
GB2555936A (en) | 2018-05-16 |
US11422801B2 (en) | 2022-08-23 |
KR102387334B1 (en) | 2022-04-15 |
US9710265B1 (en) | 2017-07-18 |
DE102017121825A1 (en) | 2018-05-03 |
JP2019537793A (en) | 2019-12-26 |
US20180121196A1 (en) | 2018-05-03 |
JP7451483B2 (en) | 2024-03-18 |
KR20190066058A (en) | 2019-06-12 |
SG11201903684RA (en) | 2019-05-30 |
KR20210129270A (en) | 2021-10-27 |
HK1254698A1 (en) | 2019-07-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN207895435U (en) | Neural computing module | |
CN109389214A (en) | Neural network accelerator with the parameter resided on chip | |
CN208061184U (en) | Vector processor unit | |
CN108416436A (en) | The method and its system of neural network division are carried out using multi-core processing module | |
CN108009626A (en) | It is sparse using the input data in neural computing unit | |
CN105892989A (en) | Neural network accelerator and operational method thereof | |
CN107688854A (en) | A kind of arithmetic element, method and device that can support different bit wide operational datas | |
CN108491924A (en) | A kind of serial stream treatment device of Neural Network Data calculated towards artificial intelligence | |
CN110490308A (en) | Accelerate design method, terminal device and the storage medium in library | |
JP6888074B2 (en) | Chip equipment and related products | |
WO2020226903A1 (en) | Memory processing unit architecture | |
KR102662211B1 (en) | Neural network accelerator with parameters resident on chip | |
Hattink et al. | A scalable architecture for cnn accelerators leveraging high-performance memories | |
Franzon et al. | Hardware Acceleration of Sparse Cognitive Algorithms | |
KR20240060857A (en) | Neural network accelerator with parameters resident on chip | |
Ho | Implementation of a Cellular Computing Memory Array |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
GR01 | Patent grant | ||
GR01 | Patent grant |