EP3625687B1 - Efficient oblivious cloud storage - Google Patents
Efficient oblivious cloud storage Download PDFInfo
- Publication number
- EP3625687B1 EP3625687B1 EP18702001.1A EP18702001A EP3625687B1 EP 3625687 B1 EP3625687 B1 EP 3625687B1 EP 18702001 A EP18702001 A EP 18702001A EP 3625687 B1 EP3625687 B1 EP 3625687B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- memory
- level
- shelter
- data block
- memory level
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 230000015654 memory Effects 0.000 claims description 405
- 238000012545 processing Methods 0.000 claims description 67
- 238000000034 method Methods 0.000 claims description 31
- 238000004891 communication Methods 0.000 claims description 15
- 238000013507 mapping Methods 0.000 claims description 6
- 239000000872 buffer Substances 0.000 description 16
- 238000004590 computer program Methods 0.000 description 15
- 238000013500 data storage Methods 0.000 description 7
- 238000005192 partition Methods 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 6
- 238000005516 engineering process Methods 0.000 description 5
- 238000004422 calculation algorithm Methods 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- 238000012546 transfer Methods 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000006855 networking Effects 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000007726 management method Methods 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000007423 decrease Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000036541 health Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0866—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches for peripheral storage systems, e.g. disk cache
- G06F12/0868—Data transfer between cache memory and other subsystems, e.g. storage devices or host systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6227—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database where protection concerns the structure of data, e.g. records, types, queries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F21/00—Security arrangements for protecting computers, components thereof, programs or data against unauthorised activity
- G06F21/60—Protecting data
- G06F21/62—Protecting access to data via a platform, e.g. using keys or access control rules
- G06F21/6218—Protecting access to data via a platform, e.g. using keys or access control rules to a system of files or objects, e.g. local or distributed file system or database
- G06F21/6245—Protecting personal data, e.g. for financial or medical purposes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1052—Security improvement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/15—Use in a specific computing environment
- G06F2212/154—Networked environment
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/21—Employing a record carrier using a specific recording technology
- G06F2212/214—Solid state disk
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/26—Using a specific storage system architecture
- G06F2212/261—Storage comprising a plurality of storage devices
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/26—Using a specific storage system architecture
- G06F2212/263—Network storage, e.g. SAN or NAS
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/46—Caching storage objects of specific type in disk cache
- G06F2212/465—Structured object, e.g. database record
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/46—Caching storage objects of specific type in disk cache
- G06F2212/466—Metadata, control data
Definitions
- This disclosure relates to obliviously moving data blocks stored on memory.
- Enterprises and individuals are using distributed storage systems (i.e., cloud storage services) to store data on memory overlying multiple memory locations. Many of these enterprises and individuals encrypt their data before uploading onto distributed storage system.
- cloud storage services In order to use essential functionalities offered by the cloud storage services, such as performing search queries on stored data, enterprises are required to provide plaintext access to the cloud storage services.
- many government and sensitive private sectors, such as health, finance, and legal, or reluctant to use cloud storage services despite their increased convenience and cost advantages.
- encryption alone may not suffice for ensuring data privacy, as the mere knowledge of data access patterns can provide a significant amount of information about the data without ever needing to decrypt the data.
- US 2014/0007250 A1 relates to methods and systems of concealing access patterns to data storage, such as within servers of a cloud computing environment.
- Server data storage is securely partitioned into smaller electronic data storage partitions of predetermined size. Concealment is performed with respect to accesses from the client to server using an oblivious sorting protocol. Access operation is concealed with each block being randomly assigned to any of the data storage partitions, and whenever a block is accessed, the block is logically removed from its current partition and logically assigned to a fresh random partition selected from all partitions, while the client maintains tracking of which partition each block is associated with at any point of time.
- Implementations of the disclosure may include one or more of the following optional features.
- the corresponding dummy data block ( D j ) of the respective memory level ( l j ) includes a permutation ( ⁇ j ) of a pointer ( dCnt j ) to a respective data block ( N j ) at the respective memory level ( l j ).
- the method may also include incrementing, by the data processing hardware, the pointer ( dCnt j ).
- the method includes updating, by the data processing hardware, the level map to indicate that the retrieved data block is stored in the virtual memory ( Shelter l ) of the lowest memory level ( l l ).
- the distributed system may be configured to initialize at least one data block ( N i ) of the corresponding virtual memory ( Shelter i ) of at least one memory level ( l i ) as a respective dummy data block ( D i ).
- the respective dummy data block ( D i ) may include a permutation of a size of the corresponding data block ( N i ), an index of the corresponding data block ( N i ), or a memory level number of the corresponding memory level ( l i ).
- the method includes obliviously shuffling, by the data processing hardware, the corresponding virtual memory ( Shelter i ) of each memory level ( l i ).
- the method may also include obliviously shuffling, by the data processing hardware, the virtual memory ( Shelter i ) of the lowest memory level ( l l ) with the virtual memory ( Shelter l ⁇ 1 ) of a next memory level ( l i ) greater than the lowest memory level (/ / ).
- Obliviously shuffling may further include: selecting a random permutation on the data blocks (B) from the virtual memory ( Shelter l ), ( Shelter l ⁇ 1 ); decrypting each of the data blocks (B) from the virtual memory ( Shelter l ), ( Shelter l ⁇ 1 ); re-encrypting each of the data blocks (B) from the virtual memory ( Shelter l ), ( Shelter l ⁇ 1 ); and shuffling the re-encrypted data blocks (B) using the random permutation on the re-encrypted data blocks (B).
- Another aspect of the disclosure provides a client device according to claim 8.
- the corresponding dummy data block ( D j ) of the respective memory level ( l j ) may include a permutation ( ⁇ j ) of a pointer ( dCnt j ) to a respective data block ( N j ) at the respective memory level ( l j ).
- the operations may also include incrementing the pointer ( dCnt j ).
- the operations may include updating the level map to indicate that the retrieved data block is stored in the virtual memory ( Shelter i ) of the lowest memory level ( l l ).
- the distributed system may be configured to initialize at least one data block ( N i ) of the corresponding virtual memory ( Shelter i ) of at least one memory level ( l i ) as a respective dummy data block ( D i ).
- the respective dummy block may include a permutation of a size of the corresponding data block ( N i ), an index of the corresponding data block ( N i ), or a memory level number of the corresponding memory level ( l i ).
- the operations include obliviously shuffling the corresponding virtual memory ( Shelter i ) of each memory level ( l i ).
- the operations may also include obliviously shuffling the virtual memory ( Shelter i ) of the lowest memory level ( l l ) with the virtual memory ( Shelter l ⁇ 1 ) of a next memory level ( l i ) greater than the lowest memory level (/ / ).
- Obliviously shuffling may further include: selecting a random permutation for the data blocks (B) from the virtual memory ( Shelter l ), ( Shelter l ⁇ 1 ); decrypting each of the data blocks (B) from the virtual memory ( Shelter l ), ( Shelter l ⁇ 1 ); re-encrypting each of the data blocks (B) from the virtual memory ( Shelter l ), ( Shelter l ⁇ 1 ); and shuffling the re-encrypted data blocks (B) using the random permutation on the re-encrypted data blocks (B).
- FIGS. 1A and 1B depict an example system 100 for storing N data blocks (B) owned by a client 104 on a distributed system 140 and obliviously moving the data blocks (B) around the distributed system 140 to conceal access patterns while preserving search functionalities on the data blocks by the client 104.
- a client device 120 e.g., a computer
- the client device 120 may include associated memory hardware 122 and associated data processing hardware 124.
- the storage abstraction 200 (e.g., key/value store, file system, data store, etc.) is overlain on storage resources 114 to allow scalable use of the storage resources 114 by one or more client devices 120.
- the distributed system 140 executes a computing device 112 that manages access to the storage abstraction 200.
- the client device 120 may encrypt and store the data blocks (B) on the storage abstraction 200, as well as retrieve and decrypt the data blocks (B) from the storage abstraction 200.
- the example shown depicts the system 100 having a trusted side associated with the client device 120 in communication, via the network 130, with an untrusted side associated with the distributed system 140, the system 100 may be alternatively implemented on a large intranet having a trusted computing device(s) (CPU) and untrusted data storage.
- CPU trusted computing device
- the distributed system 100 includes resources 110, 110a ⁇ z.
- the resources 110 may include hardware resources 110 and software resources 110.
- the hardware resources 110 may include computing devices 112 (also referred to as data processing devices and data processing hardware) or non-transitory memory 114 (also referred to as memory hardware and storage resources).
- the software resources 110 may include software applications, software services, application programming interfaces (APIs) or the like.
- the software resources 110 may reside in the hardware resources 110.
- the software resources 110 may be stored in the memory hardware 114 or the hardware resources 110 (e.g., the computing devices 112) may be executing the software resources 110.
- a software application may refer to computer software that causes a computing device to perform a task.
- a software application may be referred to as an "application,” an "app,” or a "program.”
- Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- the memory hardware 114, 122 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by a computing device 112 and/or a client device 120 (i.e., the data processing hardware 124 of the client device 120).
- the memory hardware 114, 122 may be volatile and/or non-volatile addressable semiconductor memory.
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), oblivious random access memory (ORAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the network 130 may include various types of networks, such as local area network (LAN), wide area network (WAN), and/or the Internet. Although the network 130 may represent a long range network (e.g., Internet or WAN), in some implementations, the network 130 includes a shorter range network, such as a local area network (LAN). In some implementations, the network 130 uses standard communications technologies and/or protocols.
- LAN local area network
- WAN wide area network
- LAN local area network
- the network 130 uses standard communications technologies and/or protocols.
- the network 130 can include links using technologies, such as Ethernet, Wireless Fidelity (WiFi) (e.g., 802.11), worldwide interoperability for microwave access (WiMAX), 3G, Long Term Evolution (LTE), digital subscriber line (DSL), asynchronous transfer mode (ATM), InfiniBand, PCI Express Advanced Switching, Bluetooth, Bluetooth Low Energy (BLE), etc.
- the networking protocols used on the network 130 can include multiprotocol label switching (MPLS), the transmission control protocol/Internet protocol (TCP/IP), the User Datagram Protocol (UDP), the hypertext transport protocol (HTTP), the simple mail transfer protocol (SMTP), the file transfer protocol (FTP), etc.
- MPLS multiprotocol label switching
- TCP/IP transmission control protocol/Internet protocol
- UDP User Datagram Protocol
- HTTP hypertext transport protocol
- SMTP simple mail transfer protocol
- FTP file transfer protocol
- the data exchanged over the network 130 can be represented using technologies and/or formats including the hypertext markup language (HTML), the extensible markup language (XML), etc.
- all or some of the links can be encrypted using conventional encryption technologies, such as secure sockets layer (SSL), transport layer security (TLS), virtual private networks (VPNs), Internet Protocol security (IPsec), etc.
- SSL secure sockets layer
- TLS transport layer security
- VPNs virtual private networks
- IPsec Internet Protocol security
- the network 130 uses custom and/or dedicated data communications technologies instead of, or in addition to, the ones described above.
- the data blocks (B) correspond to atomic units of data and each have size B bytes each.
- a typical value for B for storage on a distributed system may be 64 KB to 256B.
- a notation N denotes a total number of the data blocks (B) associated with the client 104 and stored on the storage abstraction 200 using Oblivious Random Access Memory (O-RAM).
- Each of the N data blocks is stored at a corresponding memory location 118, 118A-N ( FIG. 1B ) of the storage abstraction 200 overlain across the memory hardware 114.
- the traditional encryption schemes provide confidentiality, the traditional encryption schemes are ineffective at hiding data access patterns which may reveal very sensitive information to the untrusted distributed system 140.
- the traditional encryption schemes allow the client 104 to search for encrypted data (represented by data blocks (B, B 1 ⁇ B N ) stored on the distributed system 140 only if the client 104 provides plain text access for the data to the distributed system 140. As the client device 120 originates the data, the client device 120 is considered trusted.
- the client device 120 and the distributed system 140 execute an oblivious permutation routine 450 for obliviously moving the encrypted data blocks (B) around the storage abstraction 200 to completely hide data access patterns (which data blocks (B) were read/written) from the distributed system 140.
- the oblivious permutation routine 450 may cause the distributed system 140 to allocate new memory locations 118 of the storage abstraction 200 for storing re-permutated data blocks (B) and organize/divide/partition the storage abstraction 200 into multiple data buckets 350.
- the client device 120 may iteratively download each of the n data buckets 350 one at a time from the distributed system 140 and allocates substantially n cache slots on the memory hardware 122 while executing the oblivious permutation routine 450.
- the client device 120 applies a random permutation on the n data blocks (B) within the corresponding data bucket 350 to generate permutated data blocks and determines a corresponding buffer bucket 360 and a corresponding cache slot for each permutated data block (B).
- the cache slots may temporarily store the recently permutated data blocks (B) at the memory hardware 122 of the client device 120 until the data blocks (B) are uploaded/sent to the distributed system 140 for storage at the new memory locations 118. Additional details executing the oblivious permutation routine for obliviously moving the encrypted data blocks (B) around the storage abstraction 200 can be found in U.S. Patent Application 62/490,804, filed on April 27, 2017 .
- the data processing hardware 124 at the client device 120 executes an instruction 400 to execute a query (q) for the data block (B).
- the client device 120 is able to retrieve the data block (B) without revealing the contents of the data block (B) as well as the sequence of the query (q) executed by the client device 120 to the distributed system 140. Further, execution of the instruction 400 completely hides data access patterns (which data blocks (B) were read/written) from the distributed system 140.
- Execution of the instruction 400 only requires a single roundtrip between the client device 120 and the distributed system 140 when the client device 120 executes the corresponding query (q) for the data block (B). For instance, all operations that require writing back to the server are sent with the query. Similarly, all read operations can also be sent with the query. All data can also be sent back to the distributed system 140 with the query results.
- the distributed storage system 140 includes loosely coupled memory hosts 110, 110a ⁇ z (e.g., computers or servers), each having a computing resource 112 (e.g., one or more processors or central processing units (CPUs)) in communication with storage resources 114 (e.g., memory hardware, memory hardware, flash memory, dynamic random access memory (DRAM), phase change memory (PCM), and/or disks) that may be used for caching data.
- the storage abstraction 200 overlain on the storage resources 114 allows scalable use of the storage resources 114 by one or more client devices 120, 120a-n.
- the client devices 120 may communicate with the memory hosts 110 through the network 130.
- the distributed storage system 140 is "single-sided," eliminating the need for any server jobs for responding to real and/or fake queries 402, 404 from client devices 120 to retrieve data blocks (B) and/or dummy blocks (D) from the storage abstraction 200 when the client device executes instructions 400 to execute queries (q) for data blocks (B).
- Single-sided refers to the method by which most of the request processing on the memory hosts 110 may be done in hardware rather than by software executed on CPUs 112 of the memory hosts 110. Additional concepts and features related to a single-sided distributed caching system can be found in U.S. Patent 9,164,702 .
- the distributed system 140 may obliviously move data blocks (B) around the storage resources 114 (e.g., memory hardware) of the remote memory hosts 110 (e.g., the storage abstraction 200) and get the data blocks (B) from the remote memory hosts 110 via remote direct memory access (RDMA)-capable network interface controllers (NIC) 116.
- a network interface controller 116 also known as a network interface card, network adapter, or LAN adapter
- Both the memory hosts 110a ⁇ z and the client device 120 may each have a network interface controller 116 for network communications.
- the oblivious permutation routine 450 executing on the physical processor 112 of the hardware resource 110 registers a set of remote direct memory accessible regions/locations 118A-N of the memory 114 with the network interface controller 116. Each memory location 118 is configured to store a corresponding data block (B).
- the client device 120 when the client device 120 executes the instruction 400 to execute the query (q) for a data block (B) and determines that the data block (B) is stored locally at the memory hardware 122 of the client device 120, the client device 120 retrieves the data block (B) from the memory hardware 122 and sends one or more fake queries 404 to the NIC 116 for retrieving corresponding dummy blocks (D) to conceal the retrieval of the data block (B) from the local memory hardware 122. The client device 120 may discard each retrieved dummy block (D). On the other hand, if the client device 120 determines that the data block (B) is stored on the storage abstraction 200, the client device 120 may send a real query 402 to the NIC 116 for retrieving the corresponding data block (D) from the storage abstraction 200.
- the client device 120 stores a memory-level map 300 locally in the memory hardware 122 that maps memory levels ( l i ) of memory 118, 122, 200.
- the sizes and number of memory levels ( l i ) may be selected based on query and shuffling costs, in addition to the amount of storage capacity required at each of the client device 120 and the storage abstraction 200.
- Each memory level ( l i ) includes physical memory ( RAM i ) 210 and virtual memory ( Shelter i ) 220. As shown in FIG.
- the virtual memory ( Shelter l ) 220 of a lowest memory level ( l l ) resides on the client device 120 (i.e., within the memory hardware 122), while the remaining physical memory ( RAM i ) 210 and virtual memory ( Shelter i ) 220 resides on the storage abstraction 200 (e.g., memory hardware 114) of the distributed system 140.
- FIG. 2 provides a schematic view of example memory levels ( l i ) including two levels of memory 122, 200.
- the two levels may be extended to log N levels yielding a slowdown of O(log N ) and client storage O(N / B) for a RAM capacity of N data blocks (B) of size B.
- the physical memory ( RAM 1 ) 210 of the first level (Level 1) may correspond to virtual memory ( Shelter 0 ) that initially stores all of the N data blocks (B).
- the physical memory ( RAM 1 ) 210 and virtual memory ( Shelter 0 , Shelter 1 ) 220 each reside on the storage abstraction 200 of the distributed system 140.
- the constant c may be any value greater than one (1) so that the size/capacity of Si data blocks (B) associated with Shelter 1 decreases from the size/capacity of Ni data blocks (B) stored in the RAMi.
- the value for N 1 is equal to 16 data blocks (B), (B 1 ⁇ B N ) stored in RAMi and the constant c is equal to two (2). Accordingly, the virtual memory ( Shelter 1 ) 220 of the first level (Level 1) includes a value of Si equal to eight (8) data blocks (B).
- the second level (Level 2) corresponds to a lowest memory level (/ / ), and therefore, the physical memory ( RAM 2 ) 210 resides on the storage abstraction 200 and the virtual memory ( Shelter 2 ) 220 resides on the memory hardware 122 at the client device 120.
- the RAM 2 includes a size of N 2 data blocks (B) and the Shelter 2 includes a size of S 2 data blocks (B), whereby the value of N 2 is equal to the value of Si associated with Shelter 1 of the first level ( l - 1 ).
- the value for N 2 is equal to 8 data blocks (B) stored in RAMi and the constant c is equal to two (2).
- the virtual memory ( Shelter 1 ) 220 of the lowest level ( l l ) residing on the memory hardware 122 of the client device 120 includes a value for S 2 equal to four (4) data blocks (B).
- FIG. 3 provides a schematic view of an example memory-level map 300 residing at the client device 120 for mapping the memory levels ( l i ) of the memory 122, 200.
- the example memory-level map 300 maps the two memory levels ( l i ) of FIG. 2 .
- the memory-level map 300 maps each data block (B), (B 1 ⁇ B N ) to a corresponding query memory level ( l q ) associated with a lowest one of the memory levels ( l i ) at which the corresponding data block (B) of the executed query (q) is stored.
- data blocks (B 1 , B N ) each include a corresponding query memory level ( l q ) equal to Level 1 indicating that the data blocks (B 1 , B N ) are stored in Shelter 1 .
- the client device 120 executes a query (q) for either of the data blocks (B 1 , B N )
- the client device 120 will send a real query 402 to RAM 2 , which corresponds to Shelter 1 , residing at the storage abstraction 200 to retrieve the requested data blocks (B 1 , B N ).
- Data block (B 3 ) includes a corresponding query memory level ( l q ) equal to Level 0 indicating that the data block (B 3 ) is stored in Shelter0 corresponding to RAMi.
- the client device 120 executes a query (q) for the data blocks (B 3 )
- the client device 120 will send a real query 402 to RAMi residing at the storage abstraction 200 to retrieve the requested data blocks (B 3 ).
- the client device 120 updates the memory-level map 300 to indicate that the retrieved data block (B) is now stored at the client device 120 in the virtual memory ( Shelter l ) of the lowest memory level (/ / ).
- the client device 120 when the client device 120 retrieves a data block (B) from the storage abstraction 200 (e.g., RAM 1 or RAM 2 ) having a corresponding query memory level ( l q ) less than the lowest memory level (/ / ), the client device 120 stores the retrieved data block (B) locally in the Shelter 2 of the memory hardware 122 and updates the level map 300 to indicate that the retrieved data block (B) now includes a corresponding query memory level ( l q ) equal to Level 2, i.e., the lowest memory level (/ / ).
- the storage abstraction 200 e.g., RAM 1 or RAM 2
- the client device 120 may further initialize a shuffle buffer 330 in the local memory hardware 122 for shuffling the virtual memory ( Shelter i ) 220 of the memory levels ( l i ).
- the shuffle buffer 330 may shuffle Shelter l with Shelter l-1 .
- Shelter l-1 is the shelter of the next highest level from the lowest memory level (/ / ), and thus, resides on the storage abstraction 200.
- the client device 120 may download the data blocks (B) of shelters Shelter l and Shelter l-1 and decrypt/re-encrypt the data blocks (B) before shuffling the re-encrypted data blocks (B) according to a new randomly selected permutation. Thereafter, the client device 120 uploads the re-permutated data blocks (B) into Shelter l-1 on the storage abstraction 200.
- FIGS. 4A and 4B provide an example instruction 400 executing on the client device 120 to execute a query (q) for a data block (B q ).
- the data block (B q ) may be stored either at the client device 120 or the storage abstraction 200 using full recursive square root O-RAM.
- each of the multiple memory levels ( l l ) include physical memory ( RAM i ) 210 and virtual memory ( Shelter i ) 220, whereby the virtual memory ( Shelter l ) 220 of the lowest memory level ( l l ) resides on the client device 120 (e.g., in the memory hardware 122) and the remaining physical memory ( RAM i ) 210 and virtual memory ( Shelter i ) 220 reside on the memory hardware 114 (e.g., storage abstraction 200) of the distributed system 140.
- RAM 1 ⁇ RAM l and Shelter 0 ⁇ Shelter l-1 reside on the memory hardware 114 of the distributed system 140 and Shelter l resides on the client device 120.
- the distributed system 140 is configured to initialize at least one data block ( B i ) of the corresponding virtual memory ( Shelter i ) of at least one memory level ( l i ) as a respective dummy data block ( D i ).
- the respective dummy data block ( D i ) may include a permutation of a size of the corresponding data block ( B i ), an index ( N i ) of the corresponding data block ( B i ), or a memory level number of the corresponding memory level ( l i ).
- FIG. 4A shows the data processing hardware 124 of the client device 120 retrieving a query memory level ( l q ) corresponding to the data block (B q ) from the memory-level map 300 when the data processing hardware 124 executes the query (q) for the data block (Bq).
- the data processing hardware may retrieve the data block (B q ) to perform a get/read operation or to perform an update/write operation on the data block (Bq). Additionally, for each memory level ( l j ) greater than the lowest memory level ( l l ) and the physical memory ( RAM l ) at the lowest memory level ( l l ), the data processing hardware 124 sends a corresponding fake query 404 to each respective memory level ( l j ), ( l l ) to retrieve a corresponding dummy data block (D j ) therefrom. In the example shown, the data processing hardware 124 retrieves the corresponding dummy data block (D j ) from each of the RAM 1 ⁇ RAM l .
- the client device 120 retrieves the dummy data blocks (D j ) to obfuscate the retrieval of the data block (B q ) from the virtual memory ( Shelter l ) 220 on the memory hardware 122 at the client device 120.
- the data processing hardware 124 discards the retrieved dummy data blocks (D j ).
- each corresponding dummy data block (D j ) of the respective memory level (l j ) includes a permutation ( ⁇ j ) of a pointer ( dCnt j ) to a respective data block ( N j ) at the respective memory level ( l j ).
- the data processing hardware 124 may increment the corresponding pointer ( dCnt j ) when the corresponding dummy block (D j ) is retrieved from the respective memory level ( l j ) to prevent the data processing hardware 124 from retrieving the same dummy block (D j ) twice.
- FIG. 4B shows the data processing hardware 124 of the client device 120 retrieving a query memory level ( l q ) corresponding to the data block (B q ) from the memory-level map 300 and determining that the retrieved query memory level ( l q ) is not the lowest memory level ( l l ), ( l q ⁇ l l ).
- the memory-level map 300 indicates that the data block (B q ) is not currently stored on the virtual memory ( Shelter l ) 220 of the lowest memory level ( l l ) residing on the client device 120.
- the retrieved query memory level ( l q ) is equal to level 1 indicating that the corresponding data block (B q ) is stored on the physical memory ( RAM 2 ) 210 at the storage abstraction 200 of the distributed system 140.
- data processing hardware 124 sends a real query 402 to the physical memory ( RAM 2 ) 210 associated with the query memory level ( lq ) to retrieve the data block (Bq).
- the data processing hardware 124 stores the retrieved data block (B q ) in the virtual memory ( Shelter l ) 220 of the lowest memory level ( l l ) residing on the client device 120. Thereafter, the data processing hardware 124 may update the memory-level map 300 to indicate that the retrieved data block (B q ) is stored in the virtual memory ( Shelter l ) 220 of the lowest memory level ( l l ).
- the data processing hardware 124 sends a corresponding fake query 404 to each respective memory level ( l j ) to retrieve a corresponding dummy data block (D j ) therefrom.
- the data processing hardware 124 retrieves the corresponding dummy data block (D j ) from each of the RAM 1 and RAM 3 ⁇ RAM l .
- the data processing hardware 124 discards the retrieved dummy data blocks (D j ).
- 4B also shows the data processing hardware 124 incrementing the corresponding pointer ( dCnt j ) when the corresponding dummy block (D j ) is retrieved from the respective memory level ( l j ) to prevent the data processing hardware 124 from retrieving the same dummy block (D j ) twice.
- the data processing hardware 124 initializes the shuffle buffer 330 to obliviously shuffle a corresponding virtual memory ( Shelter i ) 220 of each memory level ( l i ).
- the shuffle buffer 330 In order to obliviously shuffle the corresponding virtual memory ( Shelter i ) 220, the shuffle buffer 330 must also shuffle each of the shelters Shelter i +1 , Shelter i +2 , ..., Shelter l . Accordingly, the shuffle buffer 330 initially shuffles Shelter l ⁇ 1 by incorporating Shelter l into Shelter l ⁇ 1 and shuffling Shelter l ⁇ 1 and Shelter l together.
- the client device 120 may download the data blocks (B) of shelters Shelter l and Shelter l-1 and decrypt/re-encrypt the data blocks (B) before shuffling the re-encrypted data blocks (B) according to a new randomly selected permutation. Thereafter, the client device 120 uploads the re-permutated data blocks (B) into Shelter l-1 on the storage abstraction 200.
- the shuffle buffer 330 incorporates Shelter l ⁇ 1 into Shelter l ⁇ 2 and shuffles Shelter l ⁇ 2 and Shelter l ⁇ 1 together. The shuffle buffer 330 repeats this process until Shelter i is obliviously shuffled.
- an oblivious shuffle must be complete after each S i queries. Additionally, the last S i queries must be available to the client in Shelter i . Since a given Shelter i consists of Shelter i + 1 , ... ,Shelter l , the S i queries may appear anywhere in Shelter i , ... , Shelter l .
- the obliviously shuffling of Shelter i occurs over a period of S i / 2 queries and the shuffle buffer 330 stores two shuffle buffers having a size of S i / 2 data blocks (B). During a collection phase, the shuffle buffer 330 may just store the queried data block (B q ).
- obliviously shuffling During a work phase, a constant number of steps of obliviously shuffling will complete with each query, e.g., oblivious shuffling will terminate before all queries S i / 2 occur.
- the obliviously shuffling will occur on data that was recently shuffled by the last instance of obliviously shuffling and the corresponding shuffle buffer 330.
- the shuffle buffer 330 may use the original data for Shelter 0 and a dummy set of data for all other shelters.
- Buffer1 i of the shuffle buffer 330 can be emptied to be used again.
- the collection phase of a second shuffle occurs and all queries are stored in Buffer2 i as the first shuffle is complete. Accordingly, the shuffled data from the first shuffle is available during the work phase of the second shuffle. This pattern may repeat as more queries arrive.
- the shuffle buffer 330 contains multiple versions of the same data block (B). For instance, the client device 120 can query the same data block (B q ) multiple times. However, the shuffle buffer 330 may require at most one updated version of each data block (B). To resolve the issue of multiple versions of the same data, older data block accesses may be denoted as dummy data blocks and may be discarded. However, no data blocks (B) are ever discarded from Shelter 0 .
- the client device 120 may execute an encryption module 342 or access the encryption module 342 to randomly select an Advanced Encryption Standard (AES) key for use in applying the random permutation on the data blocks (B) as well as encrypting, decrypting, and re-encrypting the data blocks (B).
- AES Advanced Encryption Standard
- the encryption module 342 may provide a randomly generated key (e.g., an AES key) for obliviously moving the data blocks (B) to new memory locations 118 of the storage abstraction 200 without revealing the permutation to the distributed system 140.
- the randomly generated key is temporary and new keys are randomly generated each time the data blocks (B) are re-permutated.
- FIG. 5 provides an example algorithm 500 initializing the memory levels ( l i ) of the memory hardware 114, 122.
- FIG. 6 provides an example algorithm 600 for execution of the instruction 400 at the client device 120 to execute a query (q) for a data block (B q ).
- FIGS. 7A and 7B illustrate a method 700 for obliviously executing queries for data blocks (B).
- the method 700 includes executing, by data processing hardware 124, an instruction 400 to execute a query (q) for a data block (B).
- the method 700 includes obtaining, by the data processing hardware 124, a query memory level ( l q ) corresponding to the data block ( B ) from a memory-level map 300.
- the memory-level map 300 maps memory levels ( l i ) of memory 114, 112, each memory level ( l i ) including physical memory ( RAM i ) 210 and virtual memory ( Shelter i ) 220.
- the virtual memory ( Shelter l ) 220 of a lowest memory level ( l l ) resides on a client device 120 and the remaining physical memory ( RAM i ) 210 and virtual memory ( Shelter i ) 220 reside on memory hardware 114 of a distributed system 140 in communication with the data processing hardware 124.
- the method 700 includes retrieving, by the data processing hardware 124, the data block ( B ) from the virtual memory ( Shelter l ) 220 of the lowest memory level ( l l ).
- the method 700 For each memory level ( l j ) greater than the lowest memory level ( l l ) and the physical memory ( RAM l ) 210 at the lowest memory level ( l l ), the method 700 includes, at block 710, retrieving, by the data processing hardware 124, a corresponding dummy data block ( D j ) from the respective memory level ( l j ), ( l l ) and, at block 712, discarding, by the data processing hardware 124, the retrieved dummy data block ( D j ).
- the method 700 includes, at block 714, retrieving, by the data processing hardware 124, the data block ( B ) from the query memory level ( l q ) and, at block 716, storing the retrieved data block (B) in the virtual memory ( Shelter l ) 220 of a lowest memory level ( l l ).
- the method 700 includes retrieving, by the data processing hardware 124, the corresponding dummy block ( D j ) from the respective memory level ( l j ).
- the method includes discarding, by the data processing hardware 124, the retrieved dummy data block ( D j ).
- FIG. 8 is schematic view of an example computing device 800 that may be used to implement the systems and methods described in this document.
- the computing device 800 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 800 includes a processor 810 (e.g., data processing hardware 112), memory 820, a storage device 830, a high-speed interface/controller 840 connecting to the memory 820 and high-speed expansion ports 850, and a low speed interface/controller 860 connecting to low speed bus 870 and storage device 830.
- the computing device 800 may reside at the client device 120 and/or the distributed system 140.
- Each of the components 810, 820, 830, 840, 850, and 860 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 810 can process instructions for execution within the computing device 800, including instructions stored in the memory 820 or on the storage device 830 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 880 coupled to high speed interface 840.
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 800 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 820 (e.g., memory hardware) stores information non-transitorily within the computing device 800.
- the memory 820 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 820 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 800.
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 830 is capable of providing mass storage for the computing device 800.
- the storage device 830 is a computer-readable medium.
- the storage device 830 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 820, the storage device 830, or memory on processor 810.
- the high speed controller 840 manages bandwidth-intensive operations for the computing device 800, while the low speed controller 860 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 840 is coupled to the memory 820, the display 880 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 850, which may accept various expansion cards (not shown).
- the low-speed controller 860 is coupled to the storage device 830 and low-speed expansion port 870.
- the low-speed expansion port 870 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 800 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 800a or multiple times in a group of such servers 800a, as a laptop computer 800b, or as part of a rack server system 800c.
- implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter affecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompass all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as an application, program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
- One or more aspects of the disclosure can be implemented in a computing system that includes a backend component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a frontend component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such backend, middleware, or frontend components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
Description
- This disclosure relates to obliviously moving data blocks stored on memory.
- Enterprises and individuals are using distributed storage systems (i.e., cloud storage services) to store data on memory overlying multiple memory locations. Many of these enterprises and individuals encrypt their data before uploading onto distributed storage system. In order to use essential functionalities offered by the cloud storage services, such as performing search queries on stored data, enterprises are required to provide plaintext access to the cloud storage services. As a result, many government and sensitive private sectors, such as health, finance, and legal, or reluctant to use cloud storage services, despite their increased convenience and cost advantages. Additionally, encryption alone may not suffice for ensuring data privacy, as the mere knowledge of data access patterns can provide a significant amount of information about the data without ever needing to decrypt the data.
-
US 2014/0007250 A1 relates to methods and systems of concealing access patterns to data storage, such as within servers of a cloud computing environment. Server data storage is securely partitioned into smaller electronic data storage partitions of predetermined size. Concealment is performed with respect to accesses from the client to server using an oblivious sorting protocol. Access operation is concealed with each block being randomly assigned to any of the data storage partitions, and whenever a block is accessed, the block is logically removed from its current partition and logically assigned to a fresh random partition selected from all partitions, while the client maintains tracking of which partition each block is associated with at any point of time. - The invention is defined in the appended claims. The dependent claims set out particular embodiments. One aspect of the disclosure provides a method according to
claim 1. - Implementations of the disclosure may include one or more of the following optional features. In some implementations, for each memory level (li ), the physical memory (RAMi ) has a defined first size to hold Ni data blocks (B) and the virtual memory (Shelteri ) has a defined second size to hold Si data blocks (B), wherein Si = N
i lc and c is a constant greater than one. The corresponding dummy data block (Dj ) of the respective memory level (lj ) includes a permutation (πj ) of a pointer (dCntj ) to a respective data block (Nj ) at the respective memory level (lj ). The method may also include incrementing, by the data processing hardware, the pointer (dCntj ). - In some examples, when the memory level (lq ) is not the lowest memory level (ll ), the method includes updating, by the data processing hardware, the level map to indicate that the retrieved data block is stored in the virtual memory (Shelterl ) of the lowest memory level (ll ). The distributed system may be configured to initialize at least one data block (Ni ) of the corresponding virtual memory (Shelteri ) of at least one memory level (li ) as a respective dummy data block (Di ). The respective dummy data block (Di ) may include a permutation of a size of the corresponding data block (Ni ), an index of the corresponding data block (Ni ), or a memory level number of the corresponding memory level (li ).
- In some implementations, the method includes obliviously shuffling, by the data processing hardware, the corresponding virtual memory (Shelteri ) of each memory level (li ). The method may also include obliviously shuffling, by the data processing hardware, the virtual memory (Shelteri ) of the lowest memory level (ll ) with the virtual memory (Shelter l―1) of a next memory level (li ) greater than the lowest memory level (//). Obliviously shuffling may further include: selecting a random permutation on the data blocks (B) from the virtual memory (Shelterl ), (Shelter l―1); decrypting each of the data blocks (B) from the virtual memory (Shelterl ), (Shelter l―1); re-encrypting each of the data blocks (B) from the virtual memory (Shelterl ), (Shelter l―1); and shuffling the re-encrypted data blocks (B) using the random permutation on the re-encrypted data blocks (B).
- Another aspect of the disclosure provides a client device according to
claim 8. - This aspect may include one or more of the following optional features. In some implementations, for each memory level (li ), the physical memory (RAMi ) has a defined first size to hold Ni data blocks (B) and the virtual memory (Shelteri ) has a defined second size to hold Si data blocks (B), wherein Si = Nilc and c is a constant greater than one. The corresponding dummy data block (Dj ) of the respective memory level (lj ) may include a permutation (πj ) of a pointer (dCntj ) to a respective data block (Nj ) at the respective memory level (lj ). The operations may also include incrementing the pointer (dCntj ).
- When the memory level (lq ) is not the lowest memory level (ll ), the operations may include updating the level map to indicate that the retrieved data block is stored in the virtual memory (Shelteri ) of the lowest memory level (ll ). The distributed system may be configured to initialize at least one data block (Ni ) of the corresponding virtual memory (Shelteri ) of at least one memory level (li ) as a respective dummy data block (Di ). The respective dummy block may include a permutation of a size of the corresponding data block (Ni ), an index of the corresponding data block (Ni ), or a memory level number of the corresponding memory level (li ).
- In some examples, the operations include obliviously shuffling the corresponding virtual memory (Shelteri ) of each memory level (li ). The operations may also include obliviously shuffling the virtual memory (Shelteri ) of the lowest memory level (ll ) with the virtual memory (Shelter l―1) of a next memory level (li ) greater than the lowest memory level (//). Obliviously shuffling may further include: selecting a random permutation for the data blocks (B) from the virtual memory (Shelterl ), (Shelter l―1); decrypting each of the data blocks (B) from the virtual memory (Shelterl ), (Shelter l―1); re-encrypting each of the data blocks (B) from the virtual memory (Shelterl ), (Shelter l―1); and shuffling the re-encrypted data blocks (B) using the random permutation on the re-encrypted data blocks (B).
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
-
FIG. 1 is a schematic view of an example system for obliviously executing queries for data blocks stored on non-transitory data storage of a distributed system. -
FIG. 1B is a schematic view of an example system for allowing one or more clients to obliviously execute queries for data blocks stored on non-transitory data storage of a distributed storage system. -
FIG. 2 provides a schematic view of example memory levels including two levels of non-transitory memory. -
FIG. 3 provides a schematic view of an example memory-level map for mapping memory levels of non-transitory memory. -
FIGS. 4A and4B provide an example instruction executing on a client device to execute a query for a data block. -
FIG. 5 provides an example algorithm for initializing memory levels of non-transitory memory. -
FIG. 6 provides an example algorithm for execution of an instruction at a client device to execute a query for a data block -
FIGS. 7A and7B illustrate a method for obliviously executing queries for data blocks. -
FIG. 8 is a schematic view of an example computing device executing a query for a data block. - Like reference symbols and the various drawings indicate like elements.
-
FIGS. 1A and1B depict anexample system 100 for storing N data blocks (B) owned by aclient 104 on adistributed system 140 and obliviously moving the data blocks (B) around thedistributed system 140 to conceal access patterns while preserving search functionalities on the data blocks by theclient 104. A client device 120 (e.g., a computer) associated with theclient 104 communicates, via anetwork 130, with thedistributed system 140 having a scalable/elasticnon-transitory storage abstraction 200. Theclient device 120 may include associatedmemory hardware 122 and associateddata processing hardware 124. The storage abstraction 200 (e.g., key/value store, file system, data store, etc.) is overlain onstorage resources 114 to allow scalable use of thestorage resources 114 by one ormore client devices 120. - In some implementations, the
distributed system 140 executes acomputing device 112 that manages access to thestorage abstraction 200. For instance, theclient device 120 may encrypt and store the data blocks (B) on thestorage abstraction 200, as well as retrieve and decrypt the data blocks (B) from thestorage abstraction 200. While the example shown depicts thesystem 100 having a trusted side associated with theclient device 120 in communication, via thenetwork 130, with an untrusted side associated with thedistributed system 140, thesystem 100 may be alternatively implemented on a large intranet having a trusted computing device(s) (CPU) and untrusted data storage. - In some implementations, the
distributed system 100 includesresources resources 110 may includehardware resources 110 andsoftware resources 110. Thehardware resources 110 may include computing devices 112 (also referred to as data processing devices and data processing hardware) or non-transitory memory 114 (also referred to as memory hardware and storage resources). Thesoftware resources 110 may include software applications, software services, application programming interfaces (APIs) or the like. Thesoftware resources 110 may reside in thehardware resources 110. For example, thesoftware resources 110 may be stored in thememory hardware 114 or the hardware resources 110 (e.g., the computing devices 112) may be executing thesoftware resources 110. - A software application (i.e., a software resource 110) may refer to computer software that causes a computing device to perform a task. In some examples, a software application may be referred to as an "application," an "app," or a "program." Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- The
memory hardware computing device 112 and/or a client device 120 (i.e., thedata processing hardware 124 of the client device 120). Thememory hardware - The
network 130 may include various types of networks, such as local area network (LAN), wide area network (WAN), and/or the Internet. Although thenetwork 130 may represent a long range network (e.g., Internet or WAN), in some implementations, thenetwork 130 includes a shorter range network, such as a local area network (LAN). In some implementations, thenetwork 130 uses standard communications technologies and/or protocols. Thus, thenetwork 130 can include links using technologies, such as Ethernet, Wireless Fidelity (WiFi) (e.g., 802.11), worldwide interoperability for microwave access (WiMAX), 3G, Long Term Evolution (LTE), digital subscriber line (DSL), asynchronous transfer mode (ATM), InfiniBand, PCI Express Advanced Switching, Bluetooth, Bluetooth Low Energy (BLE), etc. Similarly, the networking protocols used on thenetwork 130 can include multiprotocol label switching (MPLS), the transmission control protocol/Internet protocol (TCP/IP), the User Datagram Protocol (UDP), the hypertext transport protocol (HTTP), the simple mail transfer protocol (SMTP), the file transfer protocol (FTP), etc. The data exchanged over thenetwork 130 can be represented using technologies and/or formats including the hypertext markup language (HTML), the extensible markup language (XML), etc. In addition, all or some of the links can be encrypted using conventional encryption technologies, such as secure sockets layer (SSL), transport layer security (TLS), virtual private networks (VPNs), Internet Protocol security (IPsec), etc. In other examples, thenetwork 130 uses custom and/or dedicated data communications technologies instead of, or in addition to, the ones described above. - The data blocks (B) correspond to atomic units of data and each have size B bytes each. For example, a typical value for B for storage on a distributed system may be 64 KB to 256B. A notation N denotes a total number of the data blocks (B) associated with the
client 104 and stored on thestorage abstraction 200 using Oblivious Random Access Memory (O-RAM). Each of the N data blocks is stored at acorresponding memory location 118, 118A-N (FIG. 1B ) of thestorage abstraction 200 overlain across thememory hardware 114. - While traditional encryption schemes provide confidentiality, the traditional encryption schemes are ineffective at hiding data access patterns which may reveal very sensitive information to the untrusted distributed
system 140. Moreover, the traditional encryption schemes allow theclient 104 to search for encrypted data (represented by data blocks (B, B1―BN) stored on the distributedsystem 140 only if theclient 104 provides plain text access for the data to the distributedsystem 140. As theclient device 120 originates the data, theclient device 120 is considered trusted. - In some implementations, the
client device 120 and the distributedsystem 140 execute an oblivious permutation routine 450 for obliviously moving the encrypted data blocks (B) around thestorage abstraction 200 to completely hide data access patterns (which data blocks (B) were read/written) from the distributedsystem 140. For instance, the oblivious permutation routine 450 may cause the distributedsystem 140 to allocate new memory locations 118 of thestorage abstraction 200 for storing re-permutated data blocks (B) and organize/divide/partition thestorage abstraction 200 intomultiple data buckets 350. In some implementations, the oblivious permutation routine 450 organizes thestorage abstraction 200 inton data buckets 350 each containing n data blocks (B), whereby the value n is equal to the square root of the N data blocks (i.e.,client device 120 may iteratively download each of then data buckets 350 one at a time from the distributedsystem 140 and allocates substantially n cache slots on thememory hardware 122 while executing the oblivious permutation routine 450. For eachdata bucket 350 received, theclient device 120 applies a random permutation on the n data blocks (B) within the correspondingdata bucket 350 to generate permutated data blocks and determines acorresponding buffer bucket 360 and a corresponding cache slot for each permutated data block (B). Here, the cache slots may temporarily store the recently permutated data blocks (B) at thememory hardware 122 of theclient device 120 until the data blocks (B) are uploaded/sent to the distributedsystem 140 for storage at the new memory locations 118. Additional details executing the oblivious permutation routine for obliviously moving the encrypted data blocks (B) around thestorage abstraction 200 can be found inU.S. Patent Application 62/490,804, filed on April 27, 2017 - In some implementations, when the
client device 120 needs to access (read/write) an encrypted data block (B) stored on thestorage abstraction 200, thedata processing hardware 124 at theclient device 120 executes aninstruction 400 to execute a query (q) for the data block (B). By executing theinstruction 400, theclient device 120 is able to retrieve the data block (B) without revealing the contents of the data block (B) as well as the sequence of the query (q) executed by theclient device 120 to the distributedsystem 140. Further, execution of theinstruction 400 completely hides data access patterns (which data blocks (B) were read/written) from the distributedsystem 140. Execution of theinstruction 400 only requires a single roundtrip between theclient device 120 and the distributedsystem 140 when theclient device 120 executes the corresponding query (q) for the data block (B). For instance, all operations that require writing back to the server are sent with the query. Similarly, all read operations can also be sent with the query. All data can also be sent back to the distributedsystem 140 with the query results. - Referring to
FIG. 1B , in some implementations, the distributedstorage system 140 includes loosely coupled memory hosts 110, 110a―z (e.g., computers or servers), each having a computing resource 112 (e.g., one or more processors or central processing units (CPUs)) in communication with storage resources 114 (e.g., memory hardware, memory hardware, flash memory, dynamic random access memory (DRAM), phase change memory (PCM), and/or disks) that may be used for caching data. Thestorage abstraction 200 overlain on thestorage resources 114 allows scalable use of thestorage resources 114 by one ormore client devices client devices 120 may communicate with the memory hosts 110 through thenetwork 130. - In the invention, the distributed
storage system 140 is "single-sided," eliminating the need for any server jobs for responding to real and/orfake queries client devices 120 to retrieve data blocks (B) and/or dummy blocks (D) from thestorage abstraction 200 when the client device executesinstructions 400 to execute queries (q) for data blocks (B). "Single-sided" refers to the method by which most of the request processing on the memory hosts 110 may be done in hardware rather than by software executed onCPUs 112 of the memory hosts 110. Additional concepts and features related to a single-sided distributed caching system can be found inU.S. Patent 9,164,702 - The distributed
system 140 may obliviously move data blocks (B) around the storage resources 114 (e.g., memory hardware) of the remote memory hosts 110 (e.g., the storage abstraction 200) and get the data blocks (B) from the remote memory hosts 110 via remote direct memory access (RDMA)-capable network interface controllers (NIC) 116. A network interface controller 116 (also known as a network interface card, network adapter, or LAN adapter) may be a computer hardware component that connects a computing device/resource 112 to thenetwork 130. Both the memory hosts 110a―z and theclient device 120 may each have anetwork interface controller 116 for network communications. The oblivious permutation routine 450 executing on thephysical processor 112 of thehardware resource 110 registers a set of remote direct memory accessible regions/locations 118A-N of thememory 114 with thenetwork interface controller 116. Each memory location 118 is configured to store a corresponding data block (B). - In some implementations, when the
client device 120 executes theinstruction 400 to execute the query (q) for a data block (B) and determines that the data block (B) is stored locally at thememory hardware 122 of theclient device 120, theclient device 120 retrieves the data block (B) from thememory hardware 122 and sends one or morefake queries 404 to theNIC 116 for retrieving corresponding dummy blocks (D) to conceal the retrieval of the data block (B) from thelocal memory hardware 122. Theclient device 120 may discard each retrieved dummy block (D). On the other hand, if theclient device 120 determines that the data block (B) is stored on thestorage abstraction 200, theclient device 120 may send areal query 402 to theNIC 116 for retrieving the corresponding data block (D) from thestorage abstraction 200. - The
client device 120 stores a memory-level map 300 locally in thememory hardware 122 that maps memory levels (li ) ofmemory client device 120 and thestorage abstraction 200. Each memory level (li ) includes physical memory (RAMi ) 210 and virtual memory (Shelteri ) 220. As shown inFIG. 1A , the virtual memory (Shelterl ) 220 of a lowest memory level (ll ) resides on the client device 120 (i.e., within the memory hardware 122), while the remaining physical memory (RAMi ) 210 and virtual memory (Shelteri ) 220 resides on the storage abstraction 200 (e.g., memory hardware 114) of the distributedsystem 140. -
FIG. 2 provides a schematic view of example memory levels (li ) including two levels ofmemory storage abstraction 200 of the distributedsystem 140. The RAMi includes a size of Ni data blocks (B) and the Shelter1 includes a size of Si data blocks (B), whereby the S1 is equal to the value of Ni divided by a constant c (i.e., S1 = N1 / c). The constant c may be any value greater than one (1) so that the size/capacity of Si data blocks (B) associated with Shelter1 decreases from the size/capacity of Ni data blocks (B) stored in the RAMi. In the example shown, the value for N1 is equal to 16 data blocks (B), (B1―BN) stored in RAMi and the constant c is equal to two (2). Accordingly, the virtual memory (Shelter1 ) 220 of the first level (Level 1) includes a value of Si equal to eight (8) data blocks (B). - The second level (Level 2), (i=2) includes physical memory (RAM2 ) 210 and virtual memory (Shelter2 ) 200. As the memory levels (li ) include two levels, the second level (Level 2) corresponds to a lowest memory level (//), and therefore, the physical memory (RAM2 ) 210 resides on the
storage abstraction 200 and the virtual memory (Shelter2 ) 220 resides on thememory hardware 122 at theclient device 120. The RAM2 includes a size of N2 data blocks (B) and the Shelter2 includes a size of S2 data blocks (B), whereby the value of N2 is equal to the value of Si associated with Shelter1 of the first level (l-1). Thus, Shelter1 of the first level may correspond to new data blocks (B) stored in the RAM2 at the second level of size N2 =S1 (e.g., N2 = eight (8) data blocks (B)). Additionally, the value for S2 of the Shelter2 is equal to the value of N2 divided by the constant c (i.e., S2 = N2 / c). In the example shown, the value for N2 is equal to 8 data blocks (B) stored in RAMi and the constant c is equal to two (2). Accordingly, the virtual memory (Shelter1 ) 220 of the lowest level (ll ) residing on thememory hardware 122 of theclient device 120 includes a value for S2 equal to four (4) data blocks (B). -
FIG. 3 provides a schematic view of an example memory-level map 300 residing at theclient device 120 for mapping the memory levels (li ) of thememory level map 300 maps the two memory levels (li ) ofFIG. 2 . The memory-level map 300 maps each data block (B), (B1―BN) to a corresponding query memory level (lq ) associated with a lowest one of the memory levels (li ) at which the corresponding data block (B) of the executed query (q) is stored. For instance, data blocks (B1, BN) each include a corresponding query memory level (lq ) equal toLevel 1 indicating that the data blocks (B1, BN) are stored in Shelter1 . Thus, if theclient device 120 executes a query (q) for either of the data blocks (B1, BN), theclient device 120 will send areal query 402 to RAM2, which corresponds to Shelter1 , residing at thestorage abstraction 200 to retrieve the requested data blocks (B1, BN). Data block (B3) includes a corresponding query memory level (lq ) equal toLevel 0 indicating that the data block (B3) is stored in Shelter0 corresponding to RAMi. Thus, if theclient device 120 executes a query (q) for the data blocks (B3), theclient device 120 will send areal query 402 to RAMi residing at thestorage abstraction 200 to retrieve the requested data blocks (B3). - In some implementations, when query memory level (lq ) is not the lowest memory level (ll ) (i.e., lq ≠ ll ), the
client device 120 updates the memory-level map 300 to indicate that the retrieved data block (B) is now stored at theclient device 120 in the virtual memory (Shelterl ) of the lowest memory level (//). In the example shown, when theclient device 120 retrieves a data block (B) from the storage abstraction 200 (e.g., RAM1 or RAM2 ) having a corresponding query memory level (lq ) less than the lowest memory level (//), theclient device 120 stores the retrieved data block (B) locally in the Shelter2 of thememory hardware 122 and updates thelevel map 300 to indicate that the retrieved data block (B) now includes a corresponding query memory level (lq ) equal toLevel 2, i.e., the lowest memory level (//). - Referring back to
FIG. 1A , theclient device 120 may further initialize ashuffle buffer 330 in thelocal memory hardware 122 for shuffling the virtual memory (Shelteri ) 220 of the memory levels (li ). To avoid overflow in the virtual memory (Shelterl ) of the lowest memory level (ll ) residing at theclient device 120, theshuffle buffer 330 may shuffle Shelterl with Shelterl-1 . Shelterl-1 is the shelter of the next highest level from the lowest memory level (//), and thus, resides on thestorage abstraction 200. Here, theclient device 120 may download the data blocks (B) of shelters Shelterl and Shelterl-1 and decrypt/re-encrypt the data blocks (B) before shuffling the re-encrypted data blocks (B) according to a new randomly selected permutation. Thereafter, theclient device 120 uploads the re-permutated data blocks (B) into Shelterl-1 on thestorage abstraction 200. -
FIGS. 4A and4B provide anexample instruction 400 executing on theclient device 120 to execute a query (q) for a data block (Bq). The data block (Bq) may be stored either at theclient device 120 or thestorage abstraction 200 using full recursive square root O-RAM. In the example shown, each of the multiple memory levels (ll ) include physical memory (RAMi ) 210 and virtual memory (Shelteri ) 220, whereby the virtual memory (Shelterl ) 220 of the lowest memory level (ll ) resides on the client device 120 (e.g., in the memory hardware 122) and the remaining physical memory (RAMi ) 210 and virtual memory (Shelteri ) 220 reside on the memory hardware 114 (e.g., storage abstraction 200) of the distributedsystem 140. Thus, RAM1―RAMl and Shelter0―Shelterl-1 reside on thememory hardware 114 of the distributedsystem 140 and Shelterl resides on theclient device 120. - In some implementations, the virtual memory (Shelterl ) 220 occupies a space/size on the
client device 120 of Sl equal to O(1). Additionally, each physical memory (RAMi ) 210 occupies a space/size on thestorage abstraction 200 of Ni , whereby Ni is equal to the value of Ni divided by the constant c to the power i (i.e., Ni = N1 / ci ). Similarly, each virtual memory (Shelteri―Shelterl-1 ) 220 occupies a space/size on thestorage abstraction 200 of Si , whereby Si is equal to the value of Ni divided by the constant c (i.e., Si = Ni / c). - In some examples, the distributed
system 140 is configured to initialize at least one data block (Bi ) of the corresponding virtual memory (Shelteri ) of at least one memory level (li ) as a respective dummy data block (Di ). Here, the respective dummy data block (Di ) may include a permutation of a size of the corresponding data block (Bi ), an index (Ni ) of the corresponding data block (Bi ), or a memory level number of the corresponding memory level (li ). -
FIG. 4A shows thedata processing hardware 124 of theclient device 120 retrieving a query memory level (lq ) corresponding to the data block (Bq) from the memory-level map 300 when thedata processing hardware 124 executes the query (q) for the data block (Bq). Thedata processing hardware 124 determines that the query memory level (lq ) is the lowest memory level (ll ), (lq = ll ) and subsequently retrieves the data block (Bq) from the virtual memory (Shelterl ) 220 of the lowest memory level (ll ) residing on theclient device 120. For instance, the data processing hardware may retrieve the data block (Bq) to perform a get/read operation or to perform an update/write operation on the data block (Bq). Additionally, for each memory level (lj ) greater than the lowest memory level (ll ) and the physical memory (RAMl ) at the lowest memory level (ll ), thedata processing hardware 124 sends a correspondingfake query 404 to each respective memory level (lj ), (ll ) to retrieve a corresponding dummy data block (Dj) therefrom. In the example shown, thedata processing hardware 124 retrieves the corresponding dummy data block (Dj) from each of the RAM1―RAMl . Theclient device 120 retrieves the dummy data blocks (Dj) to obfuscate the retrieval of the data block (Bq) from the virtual memory (Shelter l) 220 on thememory hardware 122 at theclient device 120. In some examples, thedata processing hardware 124 discards the retrieved dummy data blocks (Dj). - In some examples, each corresponding dummy data block (Dj) of the respective memory level (lj) includes a permutation (πj ) of a pointer (dCntj ) to a respective data block (Nj ) at the respective memory level (lj ). The
data processing hardware 124 may increment the corresponding pointer (dCntj ) when the corresponding dummy block (Dj) is retrieved from the respective memory level (lj ) to prevent thedata processing hardware 124 from retrieving the same dummy block (Dj) twice. -
FIG. 4B shows thedata processing hardware 124 of theclient device 120 retrieving a query memory level (lq ) corresponding to the data block (Bq) from the memory-level map 300 and determining that the retrieved query memory level (lq ) is not the lowest memory level (ll ), (lq < ll ). Here, the memory-level map 300 indicates that the data block (Bq) is not currently stored on the virtual memory (Shelterl ) 220 of the lowest memory level (ll ) residing on theclient device 120. In the example shown, the retrieved query memory level (lq ) is equal tolevel 1 indicating that the corresponding data block (Bq) is stored on the physical memory (RAM2) 210 at thestorage abstraction 200 of the distributedsystem 140. Accordingly,data processing hardware 124 sends areal query 402 to the physical memory (RAM2) 210 associated with the query memory level (lq) to retrieve the data block (Bq). Thedata processing hardware 124 stores the retrieved data block (Bq) in the virtual memory (Shelterl ) 220 of the lowest memory level (ll ) residing on theclient device 120. Thereafter, thedata processing hardware 124 may update the memory-level map 300 to indicate that the retrieved data block (Bq) is stored in the virtual memory (Shelterl ) 220 of the lowest memory level (ll ). - Moreover, for each memory level (lj ) other than the query memory level (lq ) (e.g., RAM2 ), the
data processing hardware 124 sends a correspondingfake query 404 to each respective memory level (lj ) to retrieve a corresponding dummy data block (Dj) therefrom. In the example shown, thedata processing hardware 124 retrieves the corresponding dummy data block (Dj) from each of the RAM1 and RAM3―RAMl . In some examples, thedata processing hardware 124 discards the retrieved dummy data blocks (Dj).FIG. 4B also shows thedata processing hardware 124 incrementing the corresponding pointer (dCntj ) when the corresponding dummy block (Dj) is retrieved from the respective memory level (lj ) to prevent thedata processing hardware 124 from retrieving the same dummy block (Dj) twice. - Referring to
FIGS. 4A and4B , in some implementations, thedata processing hardware 124 initializes theshuffle buffer 330 to obliviously shuffle a corresponding virtual memory (Shelteri ) 220 of each memory level (li ). In order to obliviously shuffle the corresponding virtual memory (Shelteri ) 220, theshuffle buffer 330 must also shuffle each of the shelters Shelter i+1, Shelter i+2, ..., Shelterl . Accordingly, theshuffle buffer 330 initially shuffles Shelter l―1by incorporating Shelterl into Shelter l―1and shuffling Shelter l―1and Shelterl together. Here, theclient device 120 may download the data blocks (B) of shelters Shelterl and Shelterl-1 and decrypt/re-encrypt the data blocks (B) before shuffling the re-encrypted data blocks (B) according to a new randomly selected permutation. Thereafter, theclient device 120 uploads the re-permutated data blocks (B) into Shelterl-1 on thestorage abstraction 200. Next, to obliviously shuffle Shelter l―2, theshuffle buffer 330 incorporates Shelter l―1 into Shelterl―2 and shuffles Shelter l―2and Shelter l―1 together. Theshuffle buffer 330 repeats this process until Shelteri is obliviously shuffled. - Generally, for any Shelteri , an oblivious shuffle must be complete after each Si queries. Additionally, the last Si queries must be available to the client in Shelteri . Since a given Shelteri consists of Shelter i+1 , ... ,Shelterl , the Si queries may appear anywhere in Shelteri, ... , Shelterl . In some implementations, the obliviously shuffling of Shelteri occurs over a period of Si /2 queries and the
shuffle buffer 330 stores two shuffle buffers having a size of Si /2 data blocks (B). During a collection phase, theshuffle buffer 330 may just store the queried data block (Bq). During a work phase, a constant number of steps of obliviously shuffling will complete with each query, e.g., oblivious shuffling will terminate before all queries Si /2 occur. The obliviously shuffling will occur on data that was recently shuffled by the last instance of obliviously shuffling and thecorresponding shuffle buffer 330. For the very first instance of obliviously shuffling, theshuffle buffer 330 may use the original data for Shelter0 and a dummy set of data for all other shelters. After the completion of the obliviously shuffling, Buffer1i of theshuffle buffer 330 can be emptied to be used again. Simultaneously, the collection phase of a second shuffle occurs and all queries are stored in Buffer2i as the first shuffle is complete. Accordingly, the shuffled data from the first shuffle is available during the work phase of the second shuffle. This pattern may repeat as more queries arrive. - In some examples, the
shuffle buffer 330 contains multiple versions of the same data block (B). For instance, theclient device 120 can query the same data block (Bq) multiple times. However, theshuffle buffer 330 may require at most one updated version of each data block (B). To resolve the issue of multiple versions of the same data, older data block accesses may be denoted as dummy data blocks and may be discarded. However, no data blocks (B) are ever discarded from Shelter0. -
-
- As shown in
FIGS. 4A and4B , theclient device 120 may execute anencryption module 342 or access theencryption module 342 to randomly select an Advanced Encryption Standard (AES) key for use in applying the random permutation on the data blocks (B) as well as encrypting, decrypting, and re-encrypting the data blocks (B). Accordingly, theencryption module 342 may provide a randomly generated key (e.g., an AES key) for obliviously moving the data blocks (B) to new memory locations 118 of thestorage abstraction 200 without revealing the permutation to the distributedsystem 140. In some examples, the randomly generated key is temporary and new keys are randomly generated each time the data blocks (B) are re-permutated. -
FIG. 5 provides anexample algorithm 500 initializing the memory levels (li ) of thememory hardware FIG. 6 provides anexample algorithm 600 for execution of theinstruction 400 at theclient device 120 to execute a query (q) for a data block (Bq). -
FIGS. 7A and7B illustrate amethod 700 for obliviously executing queries for data blocks (B). Atblock 702, themethod 700 includes executing, bydata processing hardware 124, aninstruction 400 to execute a query (q) for a data block (B). At 704, themethod 700 includes obtaining, by thedata processing hardware 124, a query memory level (lq ) corresponding to the data block (B) from a memory-level map 300. The memory-level map 300 maps memory levels (li ) ofmemory client device 120 and the remaining physical memory (RAMi ) 210 and virtual memory (Shelteri ) 220 reside onmemory hardware 114 of a distributedsystem 140 in communication with thedata processing hardware 124. - At
block 706, themethod 700 includes determining, by thedata processing hardware 124, whether the query memory level (lq ) is the lowest memory level (ll ), (lq = ll ). Atblock 708, when the query memory level (lq ) is the lowest memory level (ll ), (lq = ll ), themethod 700 includes retrieving, by thedata processing hardware 124, the data block (B) from the virtual memory (Shelterl ) 220 of the lowest memory level (ll ). For each memory level (lj ) greater than the lowest memory level (ll ) and the physical memory (RAMl ) 210 at the lowest memory level (ll ), themethod 700 includes, atblock 710, retrieving, by thedata processing hardware 124, a corresponding dummy data block (Dj ) from the respective memory level (lj ), (ll ) and, atblock 712, discarding, by thedata processing hardware 124, the retrieved dummy data block (Dj ). - On the other hand, when the query memory level (lq ) is not lowest memory level (ll ), (lq = ll ), the
method 700 includes, atblock 714, retrieving, by thedata processing hardware 124, the data block (B) from the query memory level (lq ) and, atblock 716, storing the retrieved data block (B) in the virtual memory (Shelterl ) 220 of a lowest memory level (ll ). - At
block 718, for each memory level (li ) other than the query memory level (lq ), themethod 700 includes retrieving, by thedata processing hardware 124, the corresponding dummy block (Dj ) from the respective memory level (lj ). At block 720, the method includes discarding, by thedata processing hardware 124, the retrieved dummy data block (Dj ). -
FIG. 8 is schematic view of anexample computing device 800 that may be used to implement the systems and methods described in this document. Thecomputing device 800 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 800 includes a processor 810 (e.g., data processing hardware 112),memory 820, astorage device 830, a high-speed interface/controller 840 connecting to thememory 820 and high-speed expansion ports 850, and a low speed interface/controller 860 connecting to low speed bus 870 andstorage device 830. Thecomputing device 800 may reside at theclient device 120 and/or the distributedsystem 140. Each of thecomponents processor 810 can process instructions for execution within thecomputing device 800, including instructions stored in thememory 820 or on thestorage device 830 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 880 coupled tohigh speed interface 840. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 800 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The memory 820 (e.g., memory hardware) stores information non-transitorily within the
computing device 800. Thememory 820 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 820 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 800. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 830 is capable of providing mass storage for thecomputing device 800. In some implementations, thestorage device 830 is a computer-readable medium. In various different implementations, thestorage device 830 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 820, thestorage device 830, or memory onprocessor 810. - The
high speed controller 840 manages bandwidth-intensive operations for thecomputing device 800, while thelow speed controller 860 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 840 is coupled to thememory 820, the display 880 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 850, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 860 is coupled to thestorage device 830 and low-speed expansion port 870. The low-speed expansion port 870, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 800 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 800a or multiple times in a group ofsuch servers 800a, as alaptop computer 800b, or as part of arack server system 800c. - Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
- Implementations of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Moreover, subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter affecting a machine-readable propagated signal, or a combination of one or more of them. The terms "data processing apparatus", "computing device" and "computing processor" encompass all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- A computer program (also known as an application, program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- One or more aspects of the disclosure can be implemented in a computing system that includes a backend component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a frontend component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such backend, middleware, or frontend components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server.
- While this specification contains many specifics, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features specific to particular implementations of the disclosure. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely, various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub-combination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multi-tasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made. Accordingly, other implementations are within the scope of the following claims.
Claims (15)
- A method (700) comprising:executing, by data processing hardware (124), an instruction (400) to execute a query (q) for a data block (B) of a plurality of data blocks;obtaining, by the data processing hardware (124), a query memory level (lq ) corresponding to the data block (B) from a memory-level map (300), the memory-level map (300) mapping memory levels (li ) of a memory (122) of a client device (120) and of memory hardware (114) of memory hosts (110) of a storage abstraction (200) of a single-sided distributed system (140), the memory-level map (300) mapping data blocks (B), (B1 ― BN ) to a corresponding query memory level (lq ) associated with a lowest one of the memory levels (li ) at which the corresponding data block (B) of the executed query (q) is stored, each memory level (li ) comprising physical memory, RAMi, and virtual memory, Shelteri, the virtual memory, Shelterl, of a lowest memory level (ll ) residing on the memory (122) of the client device (120), the remaining physical memory, RAMi, and virtual memory, Shelteri, residing on memory hardware (114) of the memory hosts (110) of the single-sided distributed system (140) in communication with the data processing hardware (124), wherein each of the memory hosts (110) include a remote direct memory access , RDMA-capable network interface controller, NIC (116);determining, by the data processing hardware (124), whether the query memory level (lq ) is the lowest memory level (ll ), (lq = ll );when the query memory level (lq ) is the lowest memory level (ll ), (lq = ll ):retrieving, by the data processing hardware (124), the data block (B) from the virtual memory, Shelterl, of the lowest memory level (ll ); andfor each memory level (lj ) greater than the lowest memory level (ll ) and the physical memory, RAMl, at the lowest memory level (ll ):sending, by the data processing hardware (124), one or more fake queries (404) to a respective RDMA-capable NIC (116) corresponding to the respective memory level (lj );retrieving, by the data processing hardware (124), a corresponding dummy data block (Dj ) from the respective memory level (lj ), (ll ); anddiscarding, by the data processing hardware (124), the retrieved dummy data block (Dj ); andwhen the memory level (lq ) is not the lowest memory level (ll ), (lq < ll ):retrieving, by the data processing hardware (124), the data block (B) from the query memory level (lq );storing the retrieved data block (B) in the virtual memory , Shelterl, of the lowest memory level (ll );for each memory level (lj ) other than the query memory level (lq ),retrieving, by the data processing hardware (124), the corresponding dummy data block (Dj ) from the respective memory level (lj ) via a respective RDMA-capable NIC (116) corresponding to the respective memory level (lj ); anddiscarding, by the data processing hardware (124), the retrieved dummy data block (Dj ).
- The method (700) of claim 1, wherein, for each memory level (li ), the physical memory RAMi, has a defined first size to hold Ni data blocks (B) and the virtual memory, Shelteri, has a defined second size to hold Si data blocks (B), wherein Si = Ni /c and c is a constant greater than one.
- The method (700) of claim 1 or 2, wherein the corresponding dummy data block (Dj ) of the respective memory level (lj ) comprises a permutation (πj ) of a pointer (dCntj ) to a respective data block (Nj ) at the respective memory level (lj ); and
further comprising incrementing, by the data processing hardware (124), the pointer (dCntj ). - The method (700) of any of claims 1-3, further comprising, when the memory level (lq ) is not the lowest memory level (ll ), updating, by the data processing hardware (124), the level map (300) to indicate that the retrieved data block (B) is stored in the virtual memory, Shelterl, of the lowest memory level (ll ).
- The method (700) of any of claims 1-4, wherein the distributed system (140) is configured to initialize at least one data block (Bi ) of the corresponding virtual memory, Shelteri, of at least one memory level (li ) as a respective dummy data block (Di ); or wherein the respective dummy data block (Di ) comprises a permutation of a size of the corresponding data block (Ni ), an index of the corresponding data block (Ni ), or a memory level number of the corresponding memory level (li ).
- The method (700) of any of claims 1-5, further comprising obliviously shuffling, by the data processing hardware (124), the corresponding virtual memory, Shelteri, of each memory level (li ); or
obliviously shuffling, by the data processing hardware (124), the virtual memory, Shelterl, of the lowest memory level (ll ) with the virtual memory, Shelterl-1, of a next memory level (li ) greater than the lowest memory level (ll ). - The method (700) of claim 6, wherein obliviously shuffling comprises:selecting a random permutation on the data blocks (B) from the virtual memory, Shelterl, Shelterl-1;decrypting each of the data blocks (B) from the virtual memory, Shelterl, Shelterl-1;re-encrypting each of the data blocks (B) from the virtual memory, Shelterl, Shelterl-1; andshuffling the re-encrypted data blocks (B) using the random permutation on the re-encrypted data blocks (B).
- A client device (120) comprising:data processing hardware (124); andmemory hardware (122) in communication with the data processing hardware (124), the memory hardware (122) storing instructions that when executed on the data processing hardware (124) cause the data processing hardware (124) to perform operations comprising:executing an instruction (400) to execute a query (q) for a data block (B) of a plurality of data blocks;obtaining a query memory level (lq ) corresponding to the data block (B) from a memory-level map (300), the memory-level map (300) mapping memory levels (li ) of the memory (122) of the client device (120) and of memory hardware (114) of memory hosts (110) of a storage abstraction (200) of a single-sided distributed system (140), the memory-level map (300) mapping data blocks (B), (B 1 ― BN ) to a corresponding query memory level (lq ) associated with a lowest one of the memory levels (li ) at which the corresponding data block (B) of the executed query (q) is stored, each memory level (li ) comprising physical memory, RAMi, and virtual memory, Shelteri, the virtual memory, Shelterl, of a lowest memory level (ll ) residing on the memory hardware (122) of the client device (120), the remaining physical memory, RAMi, and virtual memory, Shelteri, residing on memory hardware (114) of the memory hosts (110) of the single-sided distributed system (140) in communication with the data processing hardware (124), wherein each of the memory hosts (110) include a remote direct memory access, RDMA-capable network interface controller, NIC (116);determining whether the query memory level (lq ) is the lowest memory level (ll ), (lq = ll );when the query memory level (lq ) is the lowest memory level (ll ), (lq = ll ): retrieving the data block (B) from the virtual memory, Shelterl, of the lowest memory level (ll ); andfor each memory level (lj ) greater than the lowest memory level (ll ) and the physical memory, RAMl, at the lowest memory level (ll ):sending, by the data processing hardware (124), one or more fake queries (404) to a respective RDMA-capable NIC (116) corresponding to the respective memory level (lj ); retrieving a corresponding dummy data block (Dj ) from the respective memory level (lj ), (ll ); anddiscarding the retrieved dummy data block (Dj ); andwhen the memory level (lq ) is not the lowest memory level (ll ), (lq< ll ):retrieving the data block (B) from the query memory level (lq );storing the retrieved data block (B) in the virtual memory , Shelterl, of the lowest memory level (ll );for each memory level (lj ) other than the query memory level (lq ), retrieving the corresponding dummy data block (Dj ) from the respective memory level (lj ) via a respective RDMA-capable NIC (116) corresponding to the respective memory level (lj ); anddiscarding the retrieved dummy data block (Dj ).
- The client device (120) of claim 8, wherein, for each memory level (li ), the physical memory, RAMi, has a defined first size to hold Ni data blocks (B) and the virtual memory (Shelteri ) has a defined second size to hold Si data blocks (B), wherein Si = Ni /c and c is a constant greater than one.
- The client device (120) of claim 8 or 9, wherein the corresponding dummy data block (Dj ) of the respective memory level (lj ) comprises a permutation (πj ) of a pointer (dCntj ) to a respective data block (Nj ) at the respective memory level (lj ); and
wherein the operations further comprise incrementing the pointer (dCntj ). - The client device (120) of any of claims 8-10, wherein the operations further comprise, when the memory level (lq ) is not the lowest memory level (ll ), updating the level map (300) to indicate that the retrieved data block is stored in the virtual memory, Shelteri, of the lowest memory level (ll ).
- The client device (120) of any of claims 8-11, wherein the distributed system (140) is configured to initialize at least one data block (Bi ) of the corresponding virtual memory, Shelteri, of at least one memory level (li ) as a respective dummy data block (Di ).
- The client device (120) of any of claims 8-12, wherein the respective dummy data block (Di ) comprises a permutation of a size of the corresponding data block (Ni ), an index of the corresponding data block (Ni ), or a memory level number of the corresponding memory level (li ).
- The client device (120) of any of claims 8-13, wherein the operations further comprise obliviously shuffling the corresponding virtual memory, Shelteri, of each memory level (li ); and obliviously shuffling the virtual memory, Shelterl, of the lowest memory level (ll ) with the virtual memory, Shelterl-1, of a next memory level (li ) greater than the lowest memory level (ll ).
- The client device (120) of claim 14, wherein obliviously shuffling comprises:selecting a random permutation for the data blocks (B) from the virtual memory, Shelterl, Shelterl-1;decrypting each of the data blocks (B) from the virtual memory, Shelterl, Shelterl-1;re-encrypting each of the data blocks (B) from the virtual memory, Shelterl, Shelterl-1; andshuffling the re-encrypted data blocks (B) using the random permutation on the re-encrypted data blocks (B).
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762508523P | 2017-05-19 | 2017-05-19 | |
PCT/US2018/013115 WO2018212797A1 (en) | 2017-05-19 | 2018-01-10 | Efficient oblivious cloud storage |
Publications (2)
Publication Number | Publication Date |
---|---|
EP3625687A1 EP3625687A1 (en) | 2020-03-25 |
EP3625687B1 true EP3625687B1 (en) | 2021-10-27 |
Family
ID=61074583
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP18702001.1A Active EP3625687B1 (en) | 2017-05-19 | 2018-01-10 | Efficient oblivious cloud storage |
Country Status (3)
Country | Link |
---|---|
EP (1) | EP3625687B1 (en) |
CN (1) | CN110622142B (en) |
WO (1) | WO2018212797A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111258785B (en) * | 2020-01-20 | 2023-09-08 | 北京百度网讯科技有限公司 | Data shuffling method and device |
CN111898157B (en) * | 2020-07-23 | 2024-03-26 | 东南大学 | Unintentional storage access method for machine learning multisource training set |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9164702B1 (en) * | 2012-09-07 | 2015-10-20 | Google Inc. | Single-sided distributed cache system |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5123045A (en) * | 1989-08-18 | 1992-06-16 | Massachusetts Institute Of Technology | Comprehensive software protection system |
US9015853B2 (en) * | 2012-06-15 | 2015-04-21 | The Regents Of The University Of California | Concealing access patterns to electronic data storage for privacy |
-
2018
- 2018-01-10 CN CN201880031664.0A patent/CN110622142B/en active Active
- 2018-01-10 EP EP18702001.1A patent/EP3625687B1/en active Active
- 2018-01-10 WO PCT/US2018/013115 patent/WO2018212797A1/en active Application Filing
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9164702B1 (en) * | 2012-09-07 | 2015-10-20 | Google Inc. | Single-sided distributed cache system |
Also Published As
Publication number | Publication date |
---|---|
WO2018212797A1 (en) | 2018-11-22 |
EP3625687A1 (en) | 2020-03-25 |
CN110622142A (en) | 2019-12-27 |
CN110622142B (en) | 2021-04-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11574073B2 (en) | Encrypted search cloud service with cryptographic sharing | |
US11727124B2 (en) | Oblivious access with differential privacy | |
US11544353B2 (en) | Oblivious RAM with logarithmic overhead | |
EP3586260B1 (en) | Encrypting data records and processing encrypted records without exposing plaintext | |
US20230185960A1 (en) | Private Information Retrieval with Sublinear Public-Key Operations | |
EP3625687B1 (en) | Efficient oblivious cloud storage | |
EP3616068B1 (en) | Efficient oblivious permutation | |
Tandon et al. | Cache-based side-channel attack on aes in cloud computing environment | |
Li et al. | Efficient ORAM Based on Binary Tree without Data Overflow and Evictions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: UNKNOWN |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE INTERNATIONAL PUBLICATION HAS BEEN MADE |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: REQUEST FOR EXAMINATION WAS MADE |
|
17P | Request for examination filed |
Effective date: 20191028 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: BA ME |
|
DAV | Request for validation of the european patent (deleted) | ||
DAX | Request for extension of the european patent (deleted) | ||
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20210519 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602018025646Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1442423Country of ref document: ATKind code of ref document: TEffective date: 20211115 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG9D |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20211027 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1442423Country of ref document: ATKind code of ref document: TEffective date: 20211027 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220127Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220227Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220228Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220127Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20220128Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602018025646Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20220131 |
|
26N | No opposition filed |
Effective date: 20220728 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220110Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220131 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220131Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220131 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20220110 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230125Year of fee payment: 6 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230505 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20211027 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20240129Year of fee payment: 7Ref country code: GBPayment date: 20240129Year of fee payment: 7 |