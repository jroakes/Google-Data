CN112189220A - Soft occlusion for computer graphics rendering - Google Patents
Soft occlusion for computer graphics rendering Download PDFInfo
- Publication number
- CN112189220A CN112189220A CN201880093698.2A CN201880093698A CN112189220A CN 112189220 A CN112189220 A CN 112189220A CN 201880093698 A CN201880093698 A CN 201880093698A CN 112189220 A CN112189220 A CN 112189220A
- Authority
- CN
- China
- Prior art keywords
- occlusion
- geometry
- physical object
- data
- soft
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/40—Hidden part removal
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/50—Lighting effects
- G06T15/503—Blending, e.g. for anti-aliasing
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G5/00—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators
- G09G5/36—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators characterised by the display of a graphic pattern, e.g. using an all-points-addressable [APA] memory
- G09G5/363—Graphics controllers
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G2340/00—Aspects of display data processing
- G09G2340/12—Overlay of images, i.e. displayed pixel being the result of switching between the corresponding input pixels
Abstract
Systems and methods for rendering computer graphics using soft occlusion are provided. A computing system may obtain display data for a virtual element to be displayed in association with imagery depicting an environment including physical objects. The computing system may generate a set of graphical occlusion parameters associated with rendering the image data and the display data based at least in part on the estimated geometry of the physical object. The set of graphical occlusion parameters may define a mix of display data and imagery of the virtual element at a soft occlusion region that includes one or more locations within the estimated geometry. The computing system may render a composite image from the display data and imagery of the virtual element based at least in part on the set of graphical occlusion parameters.
Description
Technical Field
The present disclosure relates generally to mapping applications that provide and/or display map data associated with a geographic area.
Background
Geographic Information Systems (GIS) are systems for archiving, retrieving and manipulating data that has been stored and indexed according to the geographic coordinates of its elements. The system may generally utilize various data types such as imagery (imagery), maps, and tables. GIS technology can be integrated into internet-based mapping applications.
Such a mapping application may be or may be otherwise associated with a software application that displays an interactive digital map. For example, mapping applications may run on laptop and tablet computers, mobile phones, car navigation systems, handheld Global Positioning System (GPS) units, and the like. Generally, mapping applications may display various types of geographic data, including terrain (topographic) data, street data, city bus (transit) information, and traffic data. Further, the geographic data may be schematic or photographic-based, such as satellite imagery. Still further, the mapping application may display information in a two-dimensional (2D) or three-dimensional (3D) format.
More recently, mapping applications have incorporated virtual elements into the display of geographic information. While some applications have incorporated virtual elements, integrating virtual elements in an efficient manner remains a challenge.
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or may be learned from the description, or may be learned through practice of the embodiments.
One example aspect of the present disclosure relates to a computing system for generating occlusion (occlusion) parameters for rendering (render) computer graphics. The computing system includes one or more image sensors, one or more processors, and one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the computing system to perform operations. The operations include obtaining display data for a virtual element to be displayed by a computing system in association with imagery from one or more image sensors. The imagery depicts an environment that includes physical objects. The operations include obtaining geometric data indicative of an estimated geometry of the physical object. The operations include generating a set of graphics occlusion parameters associated with rendering the image data and the display data based at least in part on the estimated geometry of the physical object. The set of graphical occlusion parameters defines a blend (blend) of display data and imagery of the virtual element at a soft occlusion region that includes one or more locations within an estimated geometry associated with the physical object.
Other example aspects of the disclosure relate to systems, apparatuses, computer program products (such as tangible, non-transitory computer-readable media, but also such as software downloadable through a communication network without necessarily being stored in a non-transitory form), user interfaces, memory devices, and electronic devices for providing map data for display in a user interface.
These and other features, aspects, and advantages of various embodiments will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments will be set forth in the specification with reference to the drawings, in which:
FIG. 1 depicts a block diagram of an example computing environment for generating and merging soft occlusions for graphics rendering, according to an example embodiment.
FIG. 2 depicts a graphical diagram of an example composite rendering of image data of an environment and display data of virtual elements using soft occlusion regions, according to an example embodiment.
FIG. 3 depicts a graphical diagram of a sequence of example composite renderings of image data of an environment and display data of virtual elements using soft occlusion regions, according to an example embodiment.
FIG. 4 depicts a block diagram of an example computing environment for rendering a composite image using soft occlusion, according to an example embodiment.
FIG. 5 is a flowchart depicting an example method of generating a composite rendering of image data of an environment and display data of virtual elements using soft occlusion regions, according to an example embodiment.
FIG. 6 is a flowchart depicting an example method of generating soft occlusion data based on an estimated geometry of a physical object, according to an example embodiment.
FIG. 7 depicts a block diagram illustrating a method of generating a soft occlusion region using a reduced geometry and an enlarged geometry of a physical object, according to an example embodiment.
FIG. 8 depicts a block diagram illustrating a method of generating an enlarged geometry of a physical object for an occlusion region according to an example embodiment.
FIG. 9 depicts a block diagram that illustrates a method of determining an orientation of an enlarged geometry based on a simulated face (simulated face) of a physical object, according to an example embodiment.
FIG. 10 depicts a graphical diagram of a sequence of example composite renderings of image data and display data of virtual elements of an environment using soft and hard occlusion regions, according to an example embodiment.
FIG. 11 depicts a graphical diagram of a sequence of example composite renderings including image data and display data of a user interface for generating soft occlusion regions, according to an example embodiment.
Detailed Description
Reference will now be made in detail to embodiments, one or more examples of which are illustrated in the drawings. Each example is provided by way of explanation of the embodiments and not limitation of the disclosure. In fact, it will be apparent to those skilled in the art that various modifications and variations can be made in the embodiments without departing from the scope or spirit of the disclosure. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, aspects of the present disclosure are intended to cover such modifications and variations.
In general, the present disclosure relates to systems and methods for generating soft occlusion data for a physical object, and using the soft occlusion data to render a virtual element relative to the physical object in an augmented or virtual reality environment. More specifically, the disclosed technology provides a technique for generating occlusion parameters that define a soft occlusion region associated with a physical object. A graphics processing system may render a composite scene that includes a joint rendering of display data for virtual elements within soft occlusion regions and image data depicting physical objects. For example, the soft occlusion parameters may form an occluder (occluder) of a graphics processing system that defines soft occlusion regions relative to edges (edges) of physical objects. The soft occlusion parameters may define soft occlusion regions and a mix of image data and display data of virtual elements.
In an example embodiment, the soft occlusion region may be generated based on modified geometric data associated with the physical object. The soft occlusion region may include one or more locations within an estimated geometry associated with the physical object and/or one or more locations within an enlarged geometry extending outward from the estimated geometry. Within the soft occlusion region, the virtual element may be rendered along with image data depicting physical objects in the environment. One or more compositing rendering techniques may be used to jointly render image data and display data of virtual elements within soft occlusion regions. For example, techniques may be used to blend image data and display data by selectively rendering a subset of pixels of display data from a virtual element and a subset of pixels from the image data. This technique may provide a screen-door effect in which the virtual element appears to be partially, but not completely, occluded by a physical object within the soft occlusion region. More specifically, the shader for the soft occlusion region may define a gradient that varies the concentration (concentration) of pixels from the display data over the soft occlusion region.
In some examples, a hard occlusion region may be defined for at least a portion of the estimated geometry of the physical object. Within the hard occlusion region, pixels associated with the display data may be discarded or otherwise occluded (mask) such that the virtual element appears to be occluded by the physical object. In some implementations, the virtual element may be displayed with a changing appearance (apearance) when in the hard occlusion region.
Aspects of the present disclosure provide techniques for generating soft occlusion regions associated with physical objects depicted in imagery of an environment. Within the soft occlusion region of the composite image, one or more composite rendering techniques may be used to blend the display data of the virtual element and the image data depicting the environment. A set of soft occlusion parameters may be generated that define a soft occlusion region relative to a physical object. The occlusion parameter may define an opacity (opacity) gradient that varies over the soft occlusion region. In an example embodiment, the opacity gradient defines a variable concentration of display data of the virtual element at the location of the soft occlusion region. For example, a higher concentration of display data of a virtual element may be included at a location adjacent to the edge associated with the physical object relative to a location away from the edge including a lower concentration of display data of the virtual element. The soft occlusion region may include a plurality of bands (bands), each band defining a different concentration of display data for the virtual element. The intensity of the display data in a given band may be greater than the intensity in any band closer to the edge of the physical object. In this way, in a composite scene, misalignment between the estimated geometry of the object and the depiction of the physical object in the imagery may be obscured or made less noticeable. This may improve the ability of a virtual or augmented reality environment to integrate virtual elements into imagery of a real-world environment.
According to some aspects of the present disclosure, a set of soft occlusion parameters of a graphics processing system may define a soft occlusion region that includes one or more locations within an estimated geometry associated with a physical object. Additionally and/or alternatively, the soft occlusion region may include one or more locations outside of an estimated geometry associated with the physical object. For example, a soft-inclusion (soft-inclusion) region may extend from at least one location within the estimated perimeter of the physical object to at least one location outside the estimated perimeter. The system may obtain geometric data indicative of an estimated perimeter of the physical object. The system may modify the geometric data to determine a reduced geometry, such as a reduced perimeter, of the physical object. The system may generate one or more soft occlusion regions based on the reduced perimeter. For example, a soft occlusion region may be defined that extends from an edge of the scaled-down geometry to the outside of the physical object. The soft occlusion parameter(s) may define an opacity gradient over the soft occlusion region to change the amount of display data for the displayed virtual element. For example, higher concentrations of display data may be included for locations in the soft containment area that are farther from the edge relative to locations closer to the edge.
In an example embodiment, the system may determine the reduced geometry of the physical object by identifying one or more vertices of an outer dimension of the physical object defined by the estimated geometry data. For each vertex, the system may determine an average normal for a set of faces adjacent to the vertex. The estimated geometry may then be reduced in size (e.g., moved inward) at each vertex in the direction of the mean normal.
At each vertex where the angle of the adjacent faces meets a threshold, a set of "hard" edges from the reduced geometry may be identified. For example, each vertex having adjacent faces separated by more than a threshold angle (e.g., 10 degrees) may be identified as a "hard" or "convex" edge. The reduced geometry of the physical object may extend over each hard edge. The enlarged geometry at the edge may be referred to as a fin (fin) forming a soft occlusion region. The fins may extend outwardly from the hard edge relative to the reduced geometry. In some examples, the fins extend outward a distance greater than a distance that the respective vertices move inward due to the reduced geometry. In this way, the fins may extend from within the estimated geometry to outside the estimated geometry. As such, the soft occlusion region may extend across the estimated perimeter of the physical object to potentially obscure or obscure any misalignment between the location of the estimated perimeter from the geometric data and the visual depiction of the physical object in the image.
Each fin may define a soft occlusion region having an opacity gradient that decreases across the fin. For example, the fins may define a full occlusion at a location adjacent the hard edge and a continuously lower occlusion at a location of the fins further spaced from the hard edge. For example, the fin may define a full occlusion at a location adjacent to the hard edge and no occlusion at a location furthest from the hard edge. The variable amount of occlusion may be defined by changing a density of display data of the virtual element according to the indicated amount of occlusion. For example, a larger number of pixels may be selected from the display data of the virtual element for lower occlusion regions in the fins, and a smaller number of pixels may be selected for higher occlusion regions. A barrier gate or other graphics processing technique may be used to specify the amount of display data for the virtual elements included in the composite image. For example, the shielded gate technique may use a threshold to determine whether to include or discard individual pixels of display data when generating the composite image.
According to some aspects of the disclosure, the system may generate the occlusion region based on incomplete geometric data of the physical object. For example, the system may generate additional geometric data representing an unknown geometric shape of an object (e.g., a building or other structure) from incomplete geometric data of the object. For example, the estimated geometry may be analyzed to identify any missing portions of the estimated geometry. For example, some geometric data may specify data for a vertical facade (facade) of a building, but not data for the top or bottom of a building. The facade data may be used to calculate geometry data for the missing portion of the estimated geometry. A normal to the missing part of the estimated geometry may be determined and used to determine an adjusted normal for calculating the direction to add additional occlusion regions. In some examples, as previously described, the normal may be decomposed (factor) into a mean normal that is calculated for determining the fin direction.
After computing the normal that factors in the normal associated with the additional surface, additional occlusion regions may be defined. In some examples, the additional occlusion regions may be referred to as skirts (skerts), which represent additional geometries outside of the estimated geometry and/or the scaled-down geometry of the physical object. The edges of the skirt and/or the fin may be generated based on the average normal calculated using the estimated geometry and the additional surface. In some examples, the skirt region may be defined as a soft occlusion region. As previously mentioned, a gradient may be defined for the skirt region. In some examples, the skirt region may be defined as a hard mask region.
The soft occlusion region may be defined using a two-dimensional or three-dimensional representation. For example, in some examples, the soft occlusion region may be defined as a two-dimensional surface or plane. In this case, a reduced geometry having an area smaller than the estimated geometry of the object may be calculated. In another example, the soft occlusion region may be defined as a three-dimensional volume. In this case, a reduced geometry having a volume smaller than the estimated geometry of the object may be calculated.
According to some examples, duplicate and/or collocated vertices present in the geometric data of the physical object may be accommodated. For example, some objects may be represented by contours derived from geometric data. In some cases, there may be repeated, co-located vertices at the cyclic closure points of the contour. If the average normal is calculated for repeated vertices, the vertices may separate (split) as the geometry shrinks. According to some examples, the collocated vertices may be identified and removed by a client device or a server computing system. In another example, co-located vertices may be reused when appropriate. Thus, by removing or reusing the collocated vertices, segmentation of the vertices and generation of misaligned masks may be avoided.
In some cases, a physical object may cross a node edge, and thus the estimated geometry of the object may be segmented at a pixel (cell) boundary. This may result in additional soft occlusion regions being generated in the middle of the wall or other surface of the object. According to some examples, geometric data may be provided in semantically grouped sections, rather than segmenting the object and geometric data on node boundaries. In this manner, the generation of additional and/or misaligned blockers due to node edges may be avoided.
The geometric data indicative of the estimated geometry of the physical object may be obtained in various ways. In some examples, the geometric data may include a set of geocentric geodetic coordinates that define a physical object (e.g., a building, a road, a geographic feature, a body of water, etc.) location as a bounding box in a geographic area associated with the map. For example, the bounding box may represent the perimeter of a building. The geometric data may be generated using various techniques and data sources, such as through analysis of aerial imagery, satellite imagery, street level imagery, and the like. The analysis may be manual or computer implemented. In some examples, one or more machine learning models may be used to automatically generate geographic data associated with a building based on such imagery or the like. Alternatively or additionally, the geometry data may be added manually (e.g., through interaction of the operator of the service or the owner of the building with the geographic information service).
In accordance with some aspects of the disclosed technology, a client device may obtain geometry data from a remote computing device (such as a server operating a geographic information system). For example, a client device may capture an image and/or location information and transmit it to a server. Based on the image (e.g., using feature recognition) and/or the location information, the server may locate the client device and provide geometric data for physical objects in a geographic area of the located client device. The client device may generate a set of occlusion parameters including one or more soft occlusion regions based on the geometric data from the server. Additionally and/or alternatively, the client device may generate a set of occlusion parameters based on locally determined geometric data, such as by using image recognition or other techniques associated with the physical object. For example, the client device may continue to locate itself using on-board sensors or the like. A client device may obtain display data for a virtual element to be displayed in association with image data representing an environment including a physical object. The client device may render a composite scene from the display data and the image data based on the set of occlusion parameters. For example, the occlusion parameter(s) may specify an amount of display data for a virtual element in the composite scene to be included at the one or more soft occlusion regions.
In accordance with some aspects of the disclosed technology, a server computing system may provide occlusion data in response to a request from a client device. For example, a client device may issue one or more requests for geographic information associated with a geographic area. The server may locate the client device based on imagery and/or location information received from the client device. In response to a user request, the server may transmit occlusion data to the client device. The occlusion data may specify one or more soft occlusion regions that extend from one or more locations within an estimated outer dimension of the physical object derived from the geometric data to one or more locations outside of the estimated outer dimension.
Additional graphics processing may be combined with the soft occlusion region to further enhance the visual depiction of the virtual element in conjunction with imagery of the environment including the physical object. For example, the visual appearance of the virtual element may be modified based on the position of the virtual element relative to the physical object. In an example, a hard occlusion region may be defined that includes at least a portion of a location within an estimated geometry of a physical object. For example, the hard occlusion region may include one or more locations associated with a reduced geometry of the physical object. One or more soft occlusion regions may also be defined that extend from one or more locations within the estimated geometry to one or more locations outside the estimated geometry. Outside of the soft occlusion and hard occlusion regions, the virtual element may have a first basic appearance and may be displayed in an unoccluded form. Within the soft occlusion region, the virtual element may have a first basic appearance and be displayed in soft occlusion. Within the hard occlusion region, the virtual element may be displayed in a second basic appearance. In this way, the virtual element may be visible when appearing in a position occluded by the physical object. This technique may be referred to as an x-ray vision technique because to the user they appear to be able to see through the physical objects in the composite image. Other techniques may be used to provide a visual depiction of the virtual element when in the hard occlusion region.
The systems and methods of the present disclosure may provide a variety of technical effects and benefits. As an example, various implementations address the technical problem of how to integrate virtual elements with real-world imagery. By generating soft occlusion regions for the physical object, alignment problems caused by differences in the geometric data and the image data associated with the physical object may be reduced. Such misalignment typically results in the virtual element appearing over the physical object in the composite image or being occluded at a location in the composite image that does not correspond to the physical object. This misalignment may be compensated for using the modified geometry of the physical object as described herein. Generating reduced and enlarged geometries for soft occlusion regions may reduce alignment issues that may otherwise arise between physical objects and virtual elements. More specifically, soft occlusion regions of the physical object may be defined based on the estimated geometric data. The soft occlusion region may extend from one or more locations within the estimated perimeter of the physical object to one or more locations outside the estimated perimeter. In this way, misaligned edges in the geometric data of the physical object may be blurred by soft occlusion areas.
Furthermore, occlusion regions may be generated based on incomplete geometric data of the physical object. Occlusion regions can be generated based on one or more simulated facets generated at an open edge with missing neighboring facets. Open edges in the geometric data may be identified and an average normal calculated from neighboring surfaces identified in the geometric data and simulated surfaces generated from incomplete geometric data. With the calculated average normal, additional occlusion regions can be generated to compensate for incomplete geometric data. In this way, in the event that the geometric data of the physical object is incomplete, the virtual element may be rendered with appropriate occlusion.
Generating occlusion data based on geometric data of a physical object depicted in an image provides further technical effects and benefits. For example, processing time may be reduced by alleviating the requirements associated with conventional frame-based rendering of occlusions. By calculating the soft occlusion region based on the geometric data, frame-by-frame rendering of the occluder may be avoided. For example, the soft occlusion region may be computed once on the client device and then used to render a plurality of frames of image data that include at least a portion of the soft occlusion region. This technique may avoid the processing overhead that is typically incurred when calculating occlusions in graphics processing. For example, in an example embodiment, no additional render pass (render-pass) or post-processing may be required.
In some implementations, to obtain the benefits of the techniques described herein, a user may be required to allow for the collection and analysis of location information associated with the user or her device. For example, in some implementations, a user may be provided with an opportunity to control whether programs or features collect such information. If the user does not allow such signals to be collected and used, the user may not receive the benefits of the techniques described herein. The user may also be provided with a tool to revoke or modify consent. In addition, certain information or data may be processed in one or more ways to remove personally identifiable information before being stored or used. By way of example, a computing system may obtain real-time location data that may indicate a location without identifying any particular user(s) or particular user computing device(s).
Referring now to the drawings, example aspects of the disclosure will be discussed in more detail.
FIG. 1 depicts a block diagram of an example computing environment 100 in which an embodiment of the present disclosure may be practiced to generate and utilize soft occlusion data to render virtual elements associated with imagery. The environment 100 may be implemented using a client-server architecture including a server computing system 130, the server computing system 130 in communication with one or more user computing devices 102, 104, 106 (e.g., client devices) over a network 180. The environment 100 may be implemented using other suitable architectures, such as a single computing device or additional computing devices.
The environment 100 includes a server computing system 130, such as a web server and/or an application server, among others. The server computing system 130 may host a geographic information system 140, such as a geographic information system associated with a mapping service. The server computing system 130 may be implemented using any suitable computing device(s). The server computing system 130 may have one or more processors 132 and one or more memory devices 134. The server computing system 130 may also include a network interface for communicating with one or more user computing devices over the network 180. The network interface may include any suitable components for interfacing with one or more networks, including, for example, a transmitter, a receiver, a port, a controller, an antenna, or other suitable components.
The one or more processors 132 may include any suitable processing device, such as a microprocessor, microcontroller, integrated circuit, logic device, or other suitable processing device. The one or more memory devices 134 may include one or more computer-readable media, including but not limited to non-transitory computer-readable media, RAM, ROM, hard disk drives, flash drives, or other memory devices. The one or more memory devices 134 may store information accessible by the one or more processors 132, including computer-readable instructions 138 that are executable by the one or more processors 132. The instructions 138 may be any set of instructions that, when executed by the one or more processors 132, cause the one or more processors 132 to perform operations. For example, the instructions 138 may be executable by the one or more processors 132 to implement the map manager 142. Thus, although shown separately from instructions 138, map manager 142 may be included as instructions 138 in memory 134.
As shown in fig. 1, the one or more memory devices 134 may also store data 136 that may be retrieved, manipulated, created, or stored by the one or more processors 132. The data 136 may include, for example, map data 146 and/or map element records 152. The data 136 may be stored in one or more databases. The one or more databases may be connected to the server computing system 130 through a high bandwidth LAN or WAN, or may also be connected to the server computing system 130 through the network 180. One or more databases may be split so that they are located in multiple regions.
The server computing system 130 may exchange data with one or more client devices, such as the user computing devices 102, 104, 106. The user computing devices 102, 104, 106 are one example of communication devices. Although three client devices are shown in fig. 1, any number of client devices may be connected to server computing system 130 via network 180. Each of the client devices may be any suitable type of computing device, such as a general purpose computer, a special purpose computer, a laptop computer, a desktop computer, a mobile device, a navigation system, a smartphone, a tablet computer, a wearable computing device, a display with one or more processors, or other suitable computing device.
Similar to the server computing system 130, the user computing devices 102, 104, 106 may include one or more processors 112 and memory 114. The one or more processors 112 may include one or more Central Processing Units (CPUs), Graphics Processing Units (GPUs) dedicated to efficiently rendering images or performing other specialized computations, and/or other processing devices. The memory 114 may include one or more computer-readable media and may store information accessible by the one or more processors 112, including instructions 118 and data 116 executable by the one or more processors 112. For example, the memory 114 may store instructions 118 for implementing a digital mapping application 120, the digital mapping application 120 for displaying map data and other data determined in accordance with example aspects of the present disclosure.
The user computing device of fig. 1 may include various input/output devices for providing and receiving information from a user, such as a touch screen, a touch pad, data input keys, a speaker, and/or a microphone suitable for speech recognition. For example, according to an example aspect of the present disclosure, a user computing device may have a display device for presenting a graphical user interface 122, the graphical user interface 122 displaying the data and other data within a map viewport 124.
The user computing device may also include a network interface for communicating with one or more remote computing devices (e.g., server computing system 130) over network 180. The network interface may include any suitable components for interfacing with one or more networks, including, for example, a transmitter, a receiver, a port, a controller, an antenna, or other suitable components.
The network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), a cellular network, or some combination thereof. The network 180 may also include direct connections between the user computing devices 102, 104, 106 and the server computing system 130. In general, communications between the server computing system 130 and the user computing devices 102, 104, 106 may be performed via a network interface using any type of wired and/or wireless connection, using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encoding or formatting (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
The server computing system 130 may host a Geographic Information System (GIS) 140. GIS140 may implement a mapping application, a virtual sphere application, or any other suitable GIS. The GIS140 may provide archiving, retrieval, and manipulation of geospatial data that has been indexed and stored according to geographic coordinates (such as latitude, longitude, and altitude coordinates) associated with the geospatial data. The GIS140 may combine satellite imagery, photographs, maps, models, and other geographic data, as well as internet search capabilities, to enable a user to view imagery of planets (e.g., map data associated with multiple geographic areas) and related geographic information (e.g., areas such as islands and cities, and points of interest such as local restaurants, hospitals, parks, hotels, and schools). The GIS140 also allows users to conduct local searches to obtain directions of travel to one or both locations, or to otherwise retrieve map data for a selected area. The user may virtually fly from space (e.g., some vantage point above the earth) to and around the entered target address or location, such as a neighborhood or other area of interest. The results may be displayed in a two-dimensional or three-dimensional representation of the region of interest. In some cases, a user may pan (pan), tilt, and rotate the view to view three-dimensional terrain and buildings.
The GIS140 may also allow users to annotate maps and/or enable data layers to display, for example, parks, schools, hospitals, airports, stores, and other points or regions of interest. The GIS140 may also allow users to tier multiple searches, save results to folders, and share search results and maps with others.
The geographic information system 140 may include a map manager 142 configured to manage map data and/or map element records 152. The map manager 142 may determine map data to display with a map associated with a geographic area, such as a user-selected area. For example, the geographic area may be a geographic area to be displayed within a viewport of the digital mapping application 120 on the user computing device 102. In some implementations, the user can select a geographic region through interaction with the digital mapping application 120. The map element record 152 may include data describing the location and physical attributes of physical objects corresponding to map elements (such as buildings, roads, geographic features, bodies of water, etc.). The map element record 152 may include geometric data 154 that describes physical attributes and/or location data associated with the map element. The geometric data may include data indicative of the outer dimensions or perimeter of a building or other structure. The geometric data may include a set of geocentric geo-stationary coordinates that define a structure as a bounding box in a geographic region associated with the map. For example, the bounding box may represent the perimeter of a building. The geometric data may be generated using various techniques and data sources, such as through analysis of aerial imagery, satellite imagery, street level imagery, and the like. The analysis may be manual or computer implemented. In some examples, one or more machine learning models may be used to automatically generate geographic data associated with a structure based on such imagery or the like. Alternatively or additionally, the geometry data may be added manually (e.g., by the operator of the service or the owner of the building, interaction with a geographic information service).
The digital mapping application 120 may be configured to display a map view of a geographic area of the selected area including the drawn area. As indicated, such a map view may provide an intuitive user experience for a user viewing the digital mapping application 120. In this manner, the user computing device 102 may communicate with the GIS140 in order to provide a map view to the user.
In some embodiments, the digital mapping application 120 may be configured to display imagery of an environment associated with a geographic area, such as a display of one or more images based on image data from one or more sensors 127 (e.g., cameras). The digital mapping application 120 may additionally display virtual elements associated with imagery of a geographic area. The virtual elements may include any type of virtual graphic, such as an indication of a map element (e.g., a building, a street, a geographic feature) in the imagery, an indication of additional data (e.g., metadata) associated with the map element, a direction signal associated with the route, any comprehensive element, and so forth. The digital mapping application may render display data for the virtual elements along with the image data to create an augmented or virtual reality environment.
According to an example embodiment, the map manager 142 may provide the geometric data 154 to the user computing devices 102, 104, 106 for occlusion processing at the client devices. The user computing device may generate occlusion data 125, the occlusion data 125 including occlusion parameters for rendering a virtual object relative to a physical object depicted in the imagery. For example, the user computing device may capture an image and transmit it to the GIS 140. Additionally and/or alternatively, the user computing device may transmit location information, such as GPS coordinates. Based on the image (e.g., using feature recognition) and/or the location information, the server may locate the client device and provide geometric data for physical objects in a geographic area of the located client device.
The user computing device may then render the virtual object in imagery of the geographic area based at least in part on the geometric data. More specifically, the user computing device may generate one or more occlusion parameters for rendering the virtual element relative to the depiction of the physical object in the digital imagery. The occlusion parameters may form a graphics blocker that is used to determine whether pixels or other portions of the virtual element should be displayed or discarded when rendering the composite image.
In some embodiments, the map manager 142 may optionally generate and store occlusion data 156 including occlusion parameters at the server computing system 130. The map manager 142 may provide occlusion data in response to a request from a user computing device. In such an example, the user computing device may transmit the location information and/or image data to the GIS 140. The map manager 142 may determine occlusion data 156 associated with one or more physical objects in the geographic region of the located client device. The map manager 142 may transmit the occlusion data 156 to the requesting user computing device. The occlusion data may include occlusion parameters that specify one or more soft occlusion regions. The user computing device may directly use the occlusion parameters when rendering the virtual elements with imagery of the geographic area corresponding to the occlusion parameters.
FIG. 2 depicts a graphical diagram of an example composite rendering of image data of an environment and display data of virtual elements using soft occlusion regions, according to an example embodiment. FIG. 2 depicts a composite image 302 that may be rendered by the user computing device 102 from image data depicting a physical object 304 and display data depicting a virtual element 308. An estimated geometric shape 306 of the physical object 304 defined by the geometric data is depicted. In an example embodiment, the estimated geometry of object 304 may be an estimated outer dimension or an estimated perimeter associated with object 304. As shown in fig. 2, the estimated geometry 306 includes a misalignment with the actual position of the object 304 depicted in the composite image 302.
The estimated geometry 306 may be used to define a hard occlusion region 312 associated with the object 304. The hard occlusion region includes a location within the estimated geometry 306 depicted in FIG. 2. The hard occlusion parameter may specify a location where the virtual element is not to be rendered if a depth value of the virtual element (such as a depth value that may be specified in a depth buffer) is greater than a depth value of a corresponding location of the hard occlusion region. In this case, the hard occlusion region contains the actual location of the object 304 and additional locations in the composite image 302 that do not correspond to the object 304. The hard occlusion region is defined based on an estimated geometry 306 that is not aligned with the actual position of the object 304. As such, the hard mask region 312 is misaligned with the object 304 it represents.
Embodiments of the disclosed technology provide soft occlusion regions defined by geometric data to mitigate or otherwise compensate for misalignment between geometric data and image data of objects in an environment representing a geographic area. One or more soft occlusion parameters may define a soft occlusion region in which display data associated with a virtual element and image data depicting an object are blended using one or more rendering techniques. The image data and the display data may be jointly rendered within the soft occlusion region in an effort to blur misalignment between the geometry data and the image data.
One or more soft occlusion parameters are used to define a soft occlusion region 310 in which the virtual element 308 is selectively rendered by blending with image data of the environment. In an example embodiment, display data for a virtual element may be rendered using an opacity gradient. The soft occlusion region 310 is located near an edge of the object 304 represented by the estimated geometry 306 of the object 304. In this example, the soft occlusion region 310 extends from a region within the estimated geometry 306 of the object 304 at the boundary 311 to a region outside the estimated geometry 306 at the boundary 313. As will be described in more detail below, the soft occlusion region 310 may extend from a first edge of the object 304 derived from the scaled-down geometric data of the physical object 304 in a direction based on a calculated normal associated with the edge. The soft occlusion region may extend outward to a boundary 313 that is derived from the enlarged geometric data of the physical object 304. In another example, the soft containment area may include one or more locations within the estimated geometry associated with the physical object that do not extend to locations outside of the estimated geometry. In yet another example, the soft containment area may include one or more locations within the enlarged geometry associated with the physical object without extending to locations within the estimated geometry.
Within soft occlusion region 310, the display data of the virtual element and the image data depicting the environment may be blended using various compositing rendering techniques. The one or more soft occlusion parameters may define a blending of the display data with the image data within the soft occlusion region. The soft occlusion parameter(s) may define a concentration or amount of display data to be included at various locations or other portions of the soft occlusion region. As shown in fig. 2, the density of the display data of the virtual element 308 varies over the soft occlusion region 310. At the inner boundary 311 of the soft occlusion region, the density of the display data is low, and at the outer boundary 313, the density of the display data is high. In some examples, the concentration of display data at the inner boundary may be 0% or completely occluded, while the concentration of display data at the outer boundary may be 100% or completely unoccluded. Between the boundary edge and the outer boundary, the concentration of the display data may vary between percentages at the boundary.
The soft occlusion region may include a plurality of bands, each band defining a different concentration of display data for the virtual element. For example, 16 bands may be defined, although any number of bands may be used. The concentration of the display data in a given band may be greater than the concentration in any band closer to the inner boundary 311 of the soft occlusion region. In this way, in a composite scene, misalignment between the estimated geometry of the object and the depiction of the physical object in the imagery may be obscured or made less noticeable. This may improve the ability of a virtual or augmented reality environment to integrate virtual elements into imagery of a real-world environment.
A hard occlusion region 312 associated with at least a portion of the scaled-down geometry is defined. More specifically, hard mask region 312 is defined to include a location within the reduced geometry defined by inner boundary 311.
FIG. 3 depicts a graphical diagram of a sequence of example composite renderings of image data of an environment and display data of virtual elements using soft occlusion regions, according to an example embodiment. Fig. 3 depicts a sequence of composite images 402, 422, 432, 442 that may be captured from a video sequence or a still image sequence, where the perspective changes throughout the sequence, resulting in a change in the position of the physical object 404 relative to the depicted environment.
An estimated geometry 406 of the physical object 404 defined by the geometry data is depicted relative to each composite image. The one or more soft occlusion parameters are used to define a soft occlusion region 410 and to define a blending of the image data and display data of the virtual element at one or more locations associated with the soft occlusion region. One or more hard occlusion parameters are used to define a hard occlusion region 412 in which the virtual element 408 is completely occluded based on a rendered depth value that is greater than a depth value of the occlusion data. Hard mask region 412 is defined to include a location within the reduced geometry defined by inner boundary 411.
In the composite image 402, the position of the virtual element 408 is entirely within the hard occlusion region 412. Thus, the composite image 402 depicts image data of a geographic area without display data of the rendered virtual element 408. However, in the composite image 422, the perspective changes such that the position of the virtual element 408 within the composite image is no longer within the hard occlusion region. More specifically, the virtual element 408 is partially within the soft occlusion region 410 and partially outside the soft occlusion region 410 and the hard occlusion region 412. At locations outside of the soft occlusion region 410 and the hard occlusion region 412, the virtual element 408 is fully rendered without occlusion. Within the soft occlusion region 410, the virtual element 408 is rendered by mixing image data of the environment and display data of the virtual element. In this example, the opacity gradient defines a variable blend of the display data of the virtual element over the soft occlusion region. The virtual element 408 is more fully rendered at a location adjacent the outer boundary 413 relative to a location adjacent the inner boundary 411. Between the boundaries, the concentration of the display data changes such that the virtual element appears to be more fully rendered at a location away from the inner boundary 411.
In the composite image 432, the perspective is further changed so that the virtual element 408 is more completely within the unobstructed area and less within the soft obstructed area 410. Likewise, the virtual element 408 is fully rendered at locations outside of the soft occlusion region and the hard occlusion region. Within soft occlusion region 410, virtual element 408 is rendered using an opacity gradient to provide a variable concentration of display data of the virtual element over the soft occlusion region. Finally, in the composite image 442, the viewing angle is again changed so that the virtual element 408 is outside the soft occlusion region 410 and the hard occlusion region 412. Thus, the virtual element 408 is completely rendered within the unobstructed area.
FIG. 4 depicts a block diagram of an example computing environment 500, the example computing environment 500 showing generation of occlusion parameters for rendering a virtual element with imagery of the environment, according to an example embodiment. Computing environment 500 includes digital mapping application 120 and rendering engine 520 as depicted in fig. 1. The rendering engine 520 may be implemented by the user computing devices 102, 104, 106.
The digital mapping application 120 obtains geometric data 510 and generates one or more occlusion parameters 502 including hard occlusion data 504 and soft occlusion data 506. The soft occlusion data 506 may define one or more soft occlusion regions based on modified geometric data associated with the physical object. The soft occlusion region may include a portion of an estimated geometry associated with the physical object and/or an enlarged geometry extending outward from the estimated geometry. One or more compositing rendering techniques may be used to jointly render image data within soft occlusion regions.
The hard occlusion data 504 may define one or more hard occlusion regions for at least a portion of the estimated geometry of the physical object. In some examples, one or more hard occlusion regions may be defined based on a scaled-down geometry determined from an estimated geometry associated with the physical object. Within the hard occlusion region, pixels associated with the display data may be discarded or otherwise occluded such that the virtual element appears obscured by the physical object. In some implementations, the hard occlusion parameter can specify that the virtual element is to be displayed with a changed appearance in the hard occlusion region.
The soft occlusion data 506 and the hard occlusion data 508 may be provided to a depth buffer 522 (e.g., a z-buffer) of a rendering engine 520. In addition, image data 512, such as may be obtained from the output of one or more image sensors of the user computing device, may be provided to a depth buffer 522. Display data 514 for one or more virtual elements may also be provided to depth buffer 522. Rendering engine 520 may generate composite image 530 based on blended image data 512 and display data 514. More specifically, the rendering engine 520 may render the composite image 530 by blending the image data and the display data based at least in part on the one or more occlusion parameters 502 indicated by the hard occlusion data 504 and/or the soft occlusion data 506. In an example embodiment, each pixel of the image data 512, the display data 514, and the hard occlusion data 504 and/or soft occlusion data 506 may be associated with a depth value. The depth values may be used to determine whether to render image data or display data for a given pixel location in the final composite image 530. For example, within a hard occlusion region, image data 512 may be rendered when a depth value associated with the image data is less than a depth value associated with display data 514 of a corresponding pixel. When the depth value of the display data is less than the depth value of the image data, the display data of the virtual element may be rendered.
Within the soft occlusion region, display data of the virtual element may be combined with image data of the environment using one or more synthetic rendering techniques. For example, a first subset of pixels of display data from a virtual element may be rendered while a second subset of pixels from image data is rendered. In an example embodiment, a shielded gate technique may be used to determine whether to discard or render pixels from the display data in the final composite image 530. Rendering engine 520 may implement a shader (shader) to selectively render a subset of pixels from the display data while discarding some pixels. In an example embodiment, the shader may index into a pixel mask and utilize the mapping to determine whether a single pixel of display data is rendered in the composite image 530. In an example embodiment, a thresholding process may be used to compare a single pixel value to a threshold value to determine whether the pixel is rendered in the composite image. In an example embodiment, the blocker of the soft occlusion region may be rendered prior to the display data of the virtual element. In some examples, a different or changed representation of the virtual element may be rendered rather than discarding pixel data to communicate that the virtual element is occluded. Shaders within the rendering engine 520 may be used to change the visual appearance of the virtual element when in a hard occlusion region, such as to provide an x-ray visual effect.
In some examples, the rendering engine 520 may implement an opacity gradient for soft occlusion regions. An opacity gradient may be defined that specifies a greater amount of display data for rendering within some locations of the soft occlusion region than other locations within the soft occlusion region. Higher concentrations of display data may be selected for locations closer to the unobstructed area, while lower concentrations of display data for the virtual element may be rendered at locations closer to the hard obstructed area. For example, edges of the reduced geometry determined for the physical object may be identified. A soft occlusion region may be generated that extends from the identified edge to outside the reduced geometry. The opacity gradient may define a variable amount of display data for locations within the soft occlusion region relative to the edge. For example, the soft occlusion region may include a first end proximate the identified edge and a second end distal from the first end. The opacity gradient may define a greater amount of display data for the virtual element at the second end and a lesser amount of display data for the virtual element at the first end. For example, the opacity gradient may define a full occlusion at a first end such that the virtual element is completely obscured, and an un-occlusion at a second end such that the virtual element is completely rendered. Between the first end and the second end, the opacity gradient may increase a concentration of the display data as the location is spaced further from the physical object, thereby increasing an appearance of the virtual element. In some examples, the rendering engine 520 may implement noise to provide non-uniformity of the opacity gradient over soft occlusion regions.
FIG. 5 is a flowchart depicting an example method 600 of rendering a composite image according to an example embodiment. One or more portions of the method 600 may be implemented by one or more computing devices, such as, for example, the server computing system 130 or the user computing devices 102, 104, 106 as depicted in fig. 1. One or more portions of the method 600 described herein may be implemented as an algorithm on a hardware component of a device described herein (e.g., as shown in fig. 1), for example, to generate occlusion parameters for a physical object and render an image using the occlusion parameters. In an example embodiment, the method 600 may be performed by the map manager 142 of the server computing system 130. Although fig. 5 depicts steps performed in a particular order for purposes of illustration and discussion, the method 600 of fig. 5 and other methods described below (e.g., the method 700) are not limited to the particular illustrated order or arrangement. Various steps of the methods disclosed herein may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
At (602), an image of an environment is obtained. In an example embodiment, the image may be obtained from the output of one or more sensors of the user computing devices 102, 104, 106. For example, a user computing device may obtain image data that includes one or more digital images (such as a single photograph or a video that includes multiple images).
At (604), display data for a virtual element to be displayed in association with an environment depicted in an imagery is obtained. The virtual elements may include any graphical representation, including any graphical element and/or graphical elements associated with map elements or physical objects depicted in the environment. For example, a virtual element may be provided in association with a physical object, such as a building or business location, to provide information related to the physical object. As another example, the virtual element may be a navigation element, such as a pin (pin) specifying a geographic location or any other identifier associated with a mapping, routing, and/or navigation process, etc.
At (606), geometric data indicative of an estimated geometry of the physical object is obtained. For example, the geometric data may include polygon data defining at least a portion of a perimeter or an exterior dimension of a building or other structure. In some examples, the polygon data is defined to be smaller than all external dimensions of the structure. The polygon data may include latitude/longitude coordinates, geocentric geodetic coordinates, world geodetic system coordinates, or other suitable coordinates describing the perimeter or outer dimensions of the structure.
At (608), one or more graphical occlusion parameters defining a soft occlusion region associated with the physical object are generated. In some examples, the occlusion parameter may be associated with at least one edge of the physical object. The one or more soft occlusion regions may define a blended rendering of image data depicting the environment and display data of virtual elements to be rendered relative to the environment. The soft occlusion region(s) may be associated with a physical object depicted in the image data. In some examples, the one or more occlusion parameters may define an amount of display data for the virtual element to be included within the soft occlusion region. In some examples, the one or more occlusion parameters may define an amount of display data for various locations within the soft occlusion region. For example, the occlusion parameters may define multiple bands for soft occlusion regions. Within each band, different concentrations of display data may be defined. In some examples, noise may be introduced to avoid the appearance of bands in the rendered composite image. For example, noise may be introduced (such as at the boundaries between bands) to provide an uneven distribution of display data. In some examples, one or more soft occlusion parameters may specify an opacity gradient.
At (610), image data and display data are rendered based on the one or more graphical occlusion parameters generated at 608. Based on the one or more occlusion parameters, various processes may be used to render image data and display data. For example, in some examples, the occlusion data may be rendered using a depth buffer prior to rendering the display data of the virtual element. The shader may index using the pixel mask to generate a barrier transparency for the virtual element based on combining the display data at a particular pixel location.
FIG. 6 is a flow diagram depicting an example method 700 of generating soft occlusion data based on data indicative of data of a modified geometry of a physical object. One or more portions of the method 700 may be implemented by one or more computing devices, such as, for example, the server computing system 130 or the user computing devices 102, 104, 106 as depicted in fig. 1. One or more portions of the method 700 described herein may be implemented as an algorithm on a hardware component of a device described herein (e.g., as shown in fig. 1), for example, to generate a set of soft occlusion parameters. In an example embodiment, the method 700 may be performed by the map manager 144 of the server computing system 130. In some examples, method 700 may be performed at 608 of method 600.
At (702), geometric data comprising an estimated geometry of a physical object is obtained. At (704), a scaled-down geometry associated with the physical object is determined from the estimated geometry obtained at 702. Various techniques may be used to generate the scaled-down geometry based on the estimated geometry. Generally, the reduced geometry is less than or less than the estimated geometry. The reduced geometry may be generated by reducing a size associated with one or more portions of the estimated geometry. At 704, a two-dimensional or three-dimensional reduction of the geometry may be utilized. In some examples, one or more edges or vertices from the estimated geometry are identified. The geometry may be reduced by reducing the size of the faces or surfaces adjacent to the identified vertices. In some examples, an average normal to the neighboring surfaces may be calculated, and the downsizing may be performed in the direction of the average normal.
At (706), an enlarged geometry associated with the physical object is determined from the reduced geometry and/or the estimated geometry. Various techniques may be used to generate the enlarged geometry. For example, a subset of edges of the scaled-down geometry may be identified. The subset of edges may be hard, convex edges identified from the edges of the reduced geometry. A hard edge may be an edge where the outer angle of the edge where two adjacent edges meet meets a threshold. For example, if the outer angle is greater than 270 °, the edge may be identified as a hard edge. It will be appreciated that any suitable threshold angle may be used. The enlarged geometry may be generated on or otherwise based on the identified edges. For example, faces may be generated that extend outward from the identified edge relative to the interior of the reduced geometry. In some examples, such a face may be referred to as a fin. In an example embodiment, the dilated geometry at the identified edge may dilate outward at the average normal computed for the respective vertex.
At (708), hard occlusion data associated with at least a portion of the scaled-down geometry may be generated. For example, the hard occlusion data may include one or more hard occlusion parameters that define a hard occlusion region based on the scaled-down geometry. In some examples, the hard occlusion region may contain all of the reduced geometry. The hard occlusion parameter may further define an attribute associated with the hard occlusion region. For example, the hard occlusion parameter may specify that full occlusion is to be utilized when rendering a virtual element relative to a hard occlusion region. In another example, the hard occlusion parameter may specify an x-ray or other blending technique for combining the image data and display data of the virtual element at the hard occlusion region. In some examples, the hard occlusion parameter may specify a visual appearance of the visual virtual element based on a location corresponding to the hard occlusion region.
At (710), soft occlusion data associated with the dilated geometry determined at 706 is generated. For example, the soft occlusion data may include one or more soft occlusion parameters that define a soft occlusion region based on the enlarged geometry. The occlusion parameters may define a soft occlusion region based on coordinates associated with the dilated geometry. The occlusion parameters may further define attributes associated with the soft occlusion region. For example, the occlusion parameter may specify that display data from the virtual element is to be combined with the image data for rendering within the soft occlusion region. The soft occlusion parameters may specify a blending or concentration of the display data at one or more locations of the soft occlusion region. In some examples, the soft occlusion parameter(s) may specify an opacity gradient associated with the soft occlusion region. Additionally and/or alternatively, the soft occlusion parameters may specify one or more occlusion bands associated with a predetermined concentration of display data.
According to an example embodiment, the expected error associated with the estimated geometry may be used to determine a reduced geometry at 704 and/or an enlarged geometry at 706. For example, the geometry data may be associated with an expected error value that indicates an expected error that may be present in the geometry data. In response to a higher level of expected error, a larger size reduction may be applied to generate a reduced geometry, and a larger size increase may be applied to generate an enlarged geometry. In response to a smaller expected error, a smaller size reduction may be applied to generate a reduced geometry, and a smaller size increase may be applied to generate an enlarged geometry.
FIG. 7 is a graphical diagram depicting an example of generating a reduced geometry and an expanded geometry based on an estimated geometry associated with a physical object. Fig. 7 depicts an estimated geometry 802 of an example physical object, such as a face or surface of a building. A computing system, such as the server computing system 130 or the user computing device 102, may access the estimated geometry 802 and generate a reduced geometry 804 and an expanded geometry 806.
Referring to the estimated geometry 802, each vertex associated with the geometry data may be identified. Six vertices are identified from the estimated geometry 802 depicted in fig. 7. For each vertex, the faces adjacent to the vertex are identified. The normal of each of the neighboring surfaces is then calculated. An average normal may then be calculated from the normals of each of the neighboring surfaces. An average normal 808 is depicted in fig. 7 for each vertex of the estimated geometry 802.
A reduced geometry 804 is then generated from the estimated geometry 802 based on the calculated average normal associated with each vertex. As shown in fig. 7, the estimated geometry is reduced by reducing one or more dimensions of the estimated geometry based on the average normal at each vertex. For example, the length or width of the faces adjacent to the vertex may decrease in magnitude, thus shifting the vertex based on the direction of the average normal, as shown in fig. 7.
After the reduced geometry 804 is calculated, an enlarged geometry 806 may be calculated. In this example, the expanded geometry 806 includes a set of faces 812, 814, 816, 818 extending outward from a subset of vertices 801, 803, 805, 807 associated with the reduced geometry. The subset of vertices may be identified as a subset comprising hard, convex edges. As mentioned earlier, a hard, convex edge may be identified as an edge whose outer angle of the edge engaging the adjacent face satisfies a threshold angle. In this example, each face 812, 814, 816, 818 extends outward from its respective vertex at the respective average normal of that vertex.
As shown in fig. 7, each face 812, 814, 816, 818 extends outward a distance greater than the distance the respective vertex moves inward to generate a reduced geometry. In this way, each facet will extend from one or more locations inside or within the original estimated geometry to one or more locations outside of the estimated geometry, as depicted in fig. 7. For example, as part of generating the reduced geometry, the distance that the faces of the expanded geometry extend outward may be twice the distance that the corresponding vertices move inward. However, it will be appreciated that other ratios may be used to narrow and enlarge the estimated geometry.
Fig. 8 is a graphical diagram depicting an example of generating an expanded geometry that includes extended faces generated at hard edges and extended faces generated at open edges of a reduced geometry. FIG. 8 depicts a scaled-down geometry 822, which may be generated from an estimated geometry as previously described. The reduced geometry 822 is a simple geometry comprising two faces 828 and 830 adjacent the edge 829. For example, edge 829 may be identified as a hard edge based on face 828 abutting face 830 at an angle that exceeds an angular threshold. At hard edge 829, extended face 832 may be generated that extends outward with an average normal associated with faces 828 and 830.
A set of open edges associated with the reduced geometry 822 may then be identified. Due to incomplete geometry data, open edges may exist. For example, the geometric data of a building may include facade data for vertical walls, but not the geometric data of the top or bottom of the building. An open edge may be identified as an edge having only one connected triangle and/or one adjacent face. In FIG. 8, edges 821, 823, 825, 827, 829, 831 are identified as open edges.
At each open edge, an additional extension surface (also referred to as a skirt) may be created that extends outward from the open edge. The adjusted normal may be calculated to determine the direction in which to add the additional extension surface at the open edge. In fig. 8, an enlarged geometric shape 826 is calculated that includes extension surfaces 834, 836, 838, 840, 842, 844. In some examples, one or more soft occlusion parameters may be generated that define soft occlusion regions at each of the extended faces at the open edges. In other examples, one or more hard occlusion parameters defining an occlusion region at each of the extended faces at the open edge may be generated.
Fig. 9 is a graphical diagram depicting an example of determining an adjusted average normal according to an example embodiment. The process depicted in FIG. 9 may be used to generate an expanded geometry that includes an extended face along the open edge of a reduced geometry. At 852, a reduced geometry is depicted, including faces 828, 830 abutting at edge 829. An average normal is calculated for each vertex of the reduced geometry. The average normal for each vertex can be calculated as the average normal for each of the neighboring faces. A set of average normals 860, 862, 864, 866, 868, and 870 are depicted at 853.
A set of simulated faces is then determined for each open edge, as shown at 854. The simulation table represents the expected neighboring polygons at the open edges. In some examples, each open side may be assumed to be a 90 ° corner. A simulated surface may then be determined for the open edge. In fig. 9, simulation surfaces 872, 874, and 876 are calculated. For each open edge, the normals of the adjacent simulated faces are computed by taking the cross product of the normals of the known faces in the direction of the edge (consistent with the polygon winding order), as shown at 855. A set of normals 880, 881, and 882 are computed for the set of simulation surfaces. An adjusted normal may then be calculated for each vertex based on the average normal calculated from the normals of the original and simulated surfaces. For example, the original normal computed from the original face may be adjusted based on the normal associated with the simulated face adjacent to the edge. In some examples, the normal form of the simulation surface may be weighted to determine an adjusted normal. A set of adjusted normals 890, 891, 892, 893, 894, and 895 are depicted at 856.
FIG. 10 depicts a graphical diagram of an example composite rendered sequence of image data and display data of virtual elements of an environment rendered using x-rays within soft and hard occlusion regions, according to an example embodiment. Fig. 10 depicts a sequence of composite images 922, 924, and 926 that may be captured from a video sequence or a still image sequence, wherein the virtual element 906 changes position relative to the environment depicted in the images.
An estimated geometry 908 of a physical object (e.g., a table) defined by the geometry data is depicted relative to each composite image. One or more soft occlusion parameters are used to define a soft occlusion region 910 in which virtual element 906 is selectively rendered using an opacity gradient. One or more hard occlusion parameters are used to define a hard occlusion region 912 in which the virtual element 906 is to be rendered with a changing visual appearance if associated with a depth value that is greater than a depth value of the occlusion data.
In the composite image 922, the virtual element 906 is located entirely outside of the soft occlusion region 910 and the hard occlusion region 912. Thus, the virtual element 906 is fully rendered within the unobstructed area. The virtual element 906 is rendered in a first appearance outside of the soft occlusion region and the hard occlusion region.
In the composite image 924, the position of the virtual element 906 has changed such that a first portion of the virtual element 906 is located within the soft occlusion region 910 and a second portion of the virtual element 906 is located within the hard occlusion region 912. Within soft occlusion region 910, virtual element 906 is rendered using an opacity gradient that defines a variable blend of the virtual element's display data over the soft occlusion region. In this example, it can be seen that the virtual element 906 is more fully rendered at a location adjacent the outer edge 913 relative to a location adjacent the inner edge 911. The concentration changes between edges so that the virtual element appears to be more fully rendered with a gradient over the soft occlusion region. Within the hard occlusion region 912, the virtual element 906 is rendered with a changing appearance. In a particular example, the outline of the virtual element 906 is depicted as appearing in a hard occlusion region. It should be understood that other changing visual appearances may be used. Generally, in hard occlusion regions, x-ray vision techniques are used to make the virtual element appear to be occluded by a physical object, but still allow the virtual element to appear.
In the composite image 926, the location of the virtual element 906 is entirely within the hard occlusion region 912. Thus, the composite image 926 is rendered with the virtual element 906 having a changed appearance to indicate that the virtual element is occluded by a physical object, but is still displayed.
FIG. 11 depicts a graphical diagram of an example sequence of composite renderings of image data, including a graphical user interface enabling a user to generate soft occlusion parameters. FIG. 11 depicts a sequence of composite images 951, 952, 953, and 954 that may be captured from a video sequence or a still image sequence, with a virtual element 966 displayed relative to a wall 962.
A graphical user interface is depicted in image 951, including User Interface (UI) elements 961 and 970 and 978. In some examples, a user may provide input to a display of a computing device, such as by tapping the display to cause a UI element to appear. For example, tapping the display of the computing device may cause UI elements 961 to appear, including a menu that enables the user to modify the soft occlusion parameters. For example, the UI elements 961 may include menus that enable different features (such as occlusion parameter definitions). The UI elements may also include input to reset and/or hide the graphical user interface. The user may also tap the display to cause the UI element 970 to appear, thereby depicting a representation of the geometry of the soft occlusion region. The UI elements 971-978 are input elements that allow the user to modify the geometry of the soft occlusion region.
As shown in image 952, the user may provide input to input elements 971 and/or 972 and modify the geometry of the soft occlusion region along a plane. In this example, input at input elements 971 and/or 972 enables a user to zoom in or out of a plane in a left/right direction relative to the page. Similarly, the user may provide input to input elements 976 and/or 975 and modify the geometry of the soft occlusion region in the left/right direction along a plane.
As shown in image 953, a user may provide input to input elements 973 and/or 974 and modify the geometry of the soft occlusion region along a plane. In this example, input at input elements 973 and/or 974 enables a user to zoom in or out of a plane in an up/down direction relative to a page. Similarly, as shown in image 954, the user may provide input to input elements 977 and/or 978, and modify the geometry of the soft occlusion region in an up/down direction along a plane.
As shown in fig. 11, a user may define and/or modify the geometric data of the soft occlusion region. This technique may facilitate testing of various concepts including x-ray vision, occlusion planes, and dynamic soft occlusion. Various inputs may be provided to edit the soft occlusion region. For example, a single-finger drag may be used to move along a plane, a two-finger drag may be used to raise and/or lower an element, a two-finger rotation may be used to rotate an element, and a pinch-and-pinch may be used to zoom an element.
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and received from these systems. One of ordinary skill in the art will recognize that the inherent flexibility of computer-based systems allows for a large number of possible configurations, combinations, and divisions of tasks and functions between components. For example, the server processes discussed herein may be implemented using a single server or multiple servers working in combination. The database and applications may be implemented on a single system or distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to specific exemplary embodiments thereof, it will be appreciated that those skilled in the art, upon attaining an understanding of the foregoing may readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the scope of the present disclosure is by way of example rather than by way of limitation, and the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art.
Claims (20)
1. A computing system, comprising:
one or more image sensors;
one or more processors; and
one or more non-transitory computer-readable media storing instructions that, when executed by the one or more processors, cause the computing system to perform operations comprising:
obtaining display data for a virtual element to be displayed by the computing system in association with imagery from the one or more image sensors, the imagery depicting an environment comprising physical objects;
obtaining geometric data indicative of an estimated geometry of the physical object;
generating a set of graphical occlusion parameters associated with rendering the imagery and the display data based at least in part on an estimated geometry of the physical object, wherein the set of graphical occlusion parameters define a blending of the display data of the virtual element and the imagery at a soft occlusion region comprising one or more locations within the estimated geometry associated with the physical object; and
rendering a composite image from the display data of the virtual element and the imagery based at least in part on the set of graphics occlusion parameters.
2. The computing system of claim 1, wherein:
the soft occlusion regions extend from one or more locations within an estimated geometry associated with the physical object to one or more locations outside of the estimated geometry associated with the physical object.
3. The computing system of claim 1 or 2, wherein:
the graphical occlusion parameter defines an opacity gradient that specifies a variable blending of display data of the virtual element and image data comprising the imagery based on a location associated with the soft occlusion region.
4. The computing system of claim 3, wherein:
the opacity gradient specifies a full occlusion of the virtual element at a first end based on a scaled-down geometry of the physical object and an absence of occlusion of display data of the virtual element at a second end of the soft occlusion region farthest from the first end of the soft occlusion region adjacent to the edge of the physical object.
5. The computing system of claim 4, wherein:
the opacity gradient includes a plurality of bands, each of the bands specifying a different blend of the display data of the virtual element and the image data; and
the concentration of display data increases over the plurality of bands such that the concentration of display data in a first band including the first end is less than the concentration in a second band including the second end.
6. The computing system of claim 5, wherein:
the opacity gradient is non-uniform within one or more of the plurality of bands.
7. The computing system of any of the preceding claims, wherein generating the set of graphical occlusion parameters comprises:
determining a scaled-down geometry of the physical object based on the estimated geometry; and
identifying at least one edge of the physical object based on the scaled-down geometry, the at least one edge being adjacent to a plurality of faces of the physical object derived from the scaled-down geometry;
wherein the soft occlusion region extends outwardly from the at least one edge.
8. The computing system of claim 7, wherein generating the set of graphical occlusion parameters comprises:
determining an average normal associated with the plurality of faces adjacent to the at least one edge;
wherein the soft occlusion region extends from the at least one edge in a direction based at least in part on the average normal.
9. The computing system of claim 8, wherein generating the set of graphical occlusion parameters comprises:
identifying at least one open edge based on the reduced geometry;
generating geometric data for a simulation surface associated with the at least one open edge;
determining an adjusted normal associated with the at least one open edge based at least in part on the simulation surface; and
generating data indicative of an additional occlusion region extending outward from the at least one open edge in a direction based at least in part on the adjusted normal.
10. The computing system of any of the preceding claims, wherein obtaining geometry data indicative of an estimated geometry of at least a portion of the physical object comprises:
the geometric data is generated based on image recognition associated with imagery of the environment.
11. The computing system of any of the preceding claims, wherein obtaining geometry data indicative of an estimated geometry of at least a portion of the physical object comprises:
the geometric data is obtained from one or more remote computing devices in response to a query for geographic information associated with an environment.
12. The computing system of any of the preceding claims, wherein rendering the composite image comprises:
selectively rendering the display data at various locations within the soft occlusion region based on the set of graphical occlusion parameters.
13. A computer-implemented method, comprising:
obtaining, by one or more computing devices, geometric data indicative of an estimated geometry of a physical object in a geographic area;
determining, by the one or more computing devices, a scaled-down geometry associated with the physical object based at least in part on the estimated geometry;
defining, by the one or more computing devices, a hard occlusion region associated with at least a portion of the scaled-down geometry of the physical object and a soft occlusion region associated with at least a portion of the estimated geometry of the physical object; and
rendering, by the one or more computing devices, a composite image based at least in part on the hard occlusion region and the soft occlusion region, the composite image comprising virtual elements and a visual depiction of the physical object in the geographic region.
14. The computer-implemented method of claim 13, further comprising:
based on the location associated with the soft occlusion region, an opacity gradient is defined that specifies a variable blend of display data of the virtual element and image data depicting the physical object.
15. The computer-implemented method of claim 13 or 14, wherein:
determining the scaled-down geometry associated with the physical object comprises determining a scaled-down outer dimension associated with the physical object;
the soft occlusion region extending outward from at least one edge adjacent to faces of the physical object derived from the reduced outer dimension; and
the soft occlusion region includes at least one location outside of the estimated geometry.
16. The computer-implemented method of claim 15, wherein defining the soft occlusion region comprises:
determining a direction based at least in part on an average normal associated with the plurality of faces adjacent to the at least one edge;
wherein the soft occlusion region extends from the at least one edge in a direction based at least in part on the average normal.
17. The computer-implemented method of claim 15 or 16, wherein defining the soft occlusion region comprises:
identifying at least one open edge based on the reduced outer dimension;
generating geometric data for a simulation surface associated with the at least one open edge;
determining an adjusted normal for the at least one open edge based at least in part on the simulated surface; and
generating data indicative of an additional occlusion region extending outward from the at least one open edge in a direction based at least in part on the adjusted normal.
18. One or more non-transitory computer-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
obtaining geometric data indicative of an estimated geometry of a physical object in a geographic area;
determining a scaled-down geometry associated with the physical object based on the estimated geometry;
generating hard occlusion data associated with the physical object, the hard occlusion data associated with at least a portion of the scaled-down geometry of the physical object;
generating soft occlusion data indicative of a soft occlusion region associated with the physical object, the soft occlusion region extending from an edge associated with the scaled-down geometry; and
providing a response comprising the hard occlusion data and the soft occlusion data associated with the physical object in response to at least one query for geometric data associated with a geographic area comprising the physical object.
19. The one or more non-transitory computer-readable media of claim 18, wherein:
the soft occlusion data defining an opacity gradient that specifies a variable blend of display data of a virtual element and image data depicting the physical object based on a location associated with the soft occlusion region; and
the hard occlusion data defines a full occlusion at a location associated with at least a portion of the scaled-down geometry of the physical object.
20. The one or more non-transitory computer-readable media of claim 18 or 19, wherein generating soft occlusion data comprises:
identifying at least one edge of the physical object based on the scaled-down geometry, the at least one edge being adjacent to a plurality of faces of the physical object derived from the scaled-down geometry; and
determining an average normal associated with the plurality of faces adjacent to the at least one edge;
wherein the soft occlusion region extends outward from the at least one edge in a direction based at least in part on the average normal.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2018/052944 WO2020068073A1 (en) | 2018-09-26 | 2018-09-26 | Soft-occlusion for computer graphics rendering |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112189220A true CN112189220A (en) | 2021-01-05 |
CN112189220B CN112189220B (en) | 2022-06-28 |
Family
ID=63794773
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880093698.2A Active CN112189220B (en) | 2018-09-26 | 2018-09-26 | Soft occlusion for computer graphics rendering |
Country Status (4)
Country | Link |
---|---|
US (1) | US10878599B2 (en) |
EP (1) | EP3655928B1 (en) |
CN (1) | CN112189220B (en) |
WO (1) | WO2020068073A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210295602A1 (en) * | 2020-03-17 | 2021-09-23 | Apple Inc. | Systems, Methods, and Graphical User Interfaces for Displaying and Manipulating Virtual Objects in Augmented Reality Environments |
US11632600B2 (en) | 2018-09-29 | 2023-04-18 | Apple Inc. | Devices, methods, and graphical user interfaces for depth-based annotation |
US11941764B2 (en) | 2021-04-18 | 2024-03-26 | Apple Inc. | Systems, methods, and graphical user interfaces for adding effects in augmented reality environments |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DK201870350A1 (en) | 2018-05-07 | 2019-12-05 | Apple Inc. | Devices and Methods for Measuring Using Augmented Reality |
US10872463B2 (en) * | 2019-04-01 | 2020-12-22 | Microsoft Technology Licensing, Llc | Depth-compressed representation for 3D virtual scene |
US11227446B2 (en) | 2019-09-27 | 2022-01-18 | Apple Inc. | Systems, methods, and graphical user interfaces for modeling, measuring, and drawing using augmented reality |
US11080879B1 (en) | 2020-02-03 | 2021-08-03 | Apple Inc. | Systems, methods, and graphical user interfaces for annotating, measuring, and modeling environments |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130271483A1 (en) * | 2006-04-11 | 2013-10-17 | Noregin Assets N.V., L.L.C. | Method and System for Transparency Adjustment and Occlusion Resolution for Urban Landscape Visualization |
US20160049013A1 (en) * | 2014-08-18 | 2016-02-18 | Martin Tosas Bautista | Systems and Methods for Managing Augmented Reality Overlay Pollution |
CN105574933A (en) * | 2015-12-03 | 2016-05-11 | 广州博进信息技术有限公司 | Accurate drawing method for omnidirectional object contour |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8698799B2 (en) * | 2009-01-20 | 2014-04-15 | Adobe Systems Incorporated | Method and apparatus for rendering graphics using soft occlusion |
US9122053B2 (en) * | 2010-10-15 | 2015-09-01 | Microsoft Technology Licensing, Llc | Realistic occlusion for a head mounted augmented reality display |
US20130286053A1 (en) * | 2012-04-25 | 2013-10-31 | Rod G. Fleck | Direct view augmented reality eyeglass-type display |
US9330501B2 (en) * | 2012-08-20 | 2016-05-03 | Autodesk, Inc. | Systems and methods for augmenting panoramic image data with performance related data for a building |
EP3295368A1 (en) * | 2015-05-13 | 2018-03-21 | Google LLC | Deepstereo: learning to predict new views from real world imagery |
US20170255258A1 (en) | 2016-03-02 | 2017-09-07 | The Trustees Of Columbia University In The City Of New York | Imperceptible Automatic Field-of-View Restrictors to Combat VR Sickness and Cybersickness |
US10354446B2 (en) | 2016-04-13 | 2019-07-16 | Google Llc | Methods and apparatus to navigate within virtual-reality environments |
US9726896B2 (en) * | 2016-04-21 | 2017-08-08 | Maximilian Ralph Peter von und zu Liechtenstein | Virtual monitor display technique for augmented reality environments |
US10496156B2 (en) | 2016-05-17 | 2019-12-03 | Google Llc | Techniques to change location of objects in a virtual/augmented reality system |
-
2018
- 2018-09-26 WO PCT/US2018/052944 patent/WO2020068073A1/en unknown
- 2018-09-26 CN CN201880093698.2A patent/CN112189220B/en active Active
- 2018-09-26 EP EP18783346.2A patent/EP3655928B1/en active Active
-
2019
- 2019-06-19 US US16/446,012 patent/US10878599B2/en active Active
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130271483A1 (en) * | 2006-04-11 | 2013-10-17 | Noregin Assets N.V., L.L.C. | Method and System for Transparency Adjustment and Occlusion Resolution for Urban Landscape Visualization |
US20160049013A1 (en) * | 2014-08-18 | 2016-02-18 | Martin Tosas Bautista | Systems and Methods for Managing Augmented Reality Overlay Pollution |
CN105574933A (en) * | 2015-12-03 | 2016-05-11 | 广州博进信息技术有限公司 | Accurate drawing method for omnidirectional object contour |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11632600B2 (en) | 2018-09-29 | 2023-04-18 | Apple Inc. | Devices, methods, and graphical user interfaces for depth-based annotation |
US20210295602A1 (en) * | 2020-03-17 | 2021-09-23 | Apple Inc. | Systems, Methods, and Graphical User Interfaces for Displaying and Manipulating Virtual Objects in Augmented Reality Environments |
US11727650B2 (en) * | 2020-03-17 | 2023-08-15 | Apple Inc. | Systems, methods, and graphical user interfaces for displaying and manipulating virtual objects in augmented reality environments |
US11941764B2 (en) | 2021-04-18 | 2024-03-26 | Apple Inc. | Systems, methods, and graphical user interfaces for adding effects in augmented reality environments |
Also Published As
Publication number | Publication date |
---|---|
US20200098140A1 (en) | 2020-03-26 |
US10878599B2 (en) | 2020-12-29 |
EP3655928B1 (en) | 2021-02-24 |
CN112189220B (en) | 2022-06-28 |
EP3655928A1 (en) | 2020-05-27 |
WO2020068073A1 (en) | 2020-04-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN112189220B (en) | Soft occlusion for computer graphics rendering | |
US10984582B2 (en) | Smooth draping layer for rendering vector data on complex three dimensional objects | |
US11783543B2 (en) | Method and system for displaying and navigating an optimal multi-dimensional building model | |
EP3170151B1 (en) | Blending between street view and earth view | |
US9105129B2 (en) | Level of detail transitions for geometric objects in a graphics application | |
US9626790B1 (en) | View-dependent textures for interactive geographic information system | |
US7561156B2 (en) | Adaptive quadtree-based scalable surface rendering | |
US9153011B2 (en) | Movement based level of detail adjustments | |
US9704282B1 (en) | Texture blending between view-dependent texture and base texture in a geographic information system | |
US9547921B1 (en) | Texture fading for smooth level of detail transitions in a graphics application | |
US10733777B2 (en) | Annotation generation for an image network | |
US20110242271A1 (en) | Synthesizing Panoramic Three-Dimensional Images | |
US10275939B2 (en) | Determining two-dimensional images using three-dimensional models | |
Trapp | Interactive rendering techniques for focus+ context visualization of 3d geovirtual environments | |
Sauerbier et al. | Multi-resolution image-based visualization of archaeological landscapes in Palpa (Peru) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |