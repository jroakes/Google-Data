US11640426B1 - Background audio identification for query disambiguation - Google Patents
Background audio identification for query disambiguation Download PDFInfo
- Publication number
- US11640426B1 US11640426B1 US17/334,378 US202117334378A US11640426B1 US 11640426 B1 US11640426 B1 US 11640426B1 US 202117334378 A US202117334378 A US 202117334378A US 11640426 B1 US11640426 B1 US 11640426B1
- Authority
- US
- United States
- Prior art keywords
- background audio
- query
- search
- terms
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/63—Querying
- G06F16/632—Query formulation
- G06F16/634—Query by example, e.g. query by humming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9035—Filtering based on additional data, e.g. user or group profiles
Definitions
- the techniques provided herein relate to query disambiguation.
- Internet search engines provide information about Internet-accessible resources, e.g., web pages, documents, and images, in response to a user's query.
- a user can submit a query using a device, such as a mobile telephone, that includes a microphone.
- a device such as a mobile telephone
- users submit queries to internet search engines that are ambiguous in that they relate to more than one concept and/or entity.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a search query, receiving background audio, identifying one or more concepts related to the background audio, generating a set of related terms related to the identified concepts, receiving search results based on the search query and on at least one of the terms related to the identified concepts, and providing the search results.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- receiving search results based on the search query and at least one at least one of the terms related to the identified concepts may include providing the query to a search engine, receiving scored results from the search engine, and altering a score for a search result containing at least one of the related terms.
- the method may further include determining an amount to alter the score by using a plurality of training queries.
- providing the search results may include providing only search results that satisfy a threshold score.
- receiving search results based on the search query and at least on at least one of the terms related to the identified concepts may include providing the query to a search engine together with at least one of the related terms and receiving results from the search engine.
- identifying concepts related to the background audio may include recognizing at least a portion of the background audio by matching it to an acoustic fingerprint and identifying concepts related to the background audio including concepts associated with the acoustic fingerprint.
- generating a set of terms related to the background audio may include generating a set of terms based on querying a conceptual expansion database based on the concepts related to the background audio.
- Disclosed techniques provide certain technical advantages. Some implementations are capable of using background audio to clarify an ambiguous search query based on background audio. Such implementations provide more accurate search results, thus achieving a technical advantage.
- FIG. 1 A is a schematic diagram of an example implementation.
- FIG. 1 B is a schematic diagram of a search system included in an example implementation.
- FIG. 1 C is a schematic diagram of a computing device included in an example implementation.
- FIG. 2 is a flowchart of a method according to some implementations.
- a search engine Users gain information about internet-accessible resources by submitting a query to a search engine using a client device, such as a mobile telephone, which may be equipped with a microphone.
- the user's query is ambiguous. For example, the user can submit the query “scorpions”. Without more information, the search engine has no way to determine whether the user wants information on scorpion arthropods or the rock band Scorpions. However, if the search engine determined that the user submitted the query while listening to the Scorpions' song “Rock You Like A Hurricane”, it could provide search results relevant to the rock band, instead of those relevant to the arthropods.
- Some implementations perform a search based on both the user's query and terms that are related to background audio present at, or around, a time of the search query's input. In this manner, implementations respond to search queries by taking into account the context of background audio. The context of background audio thus allows ambiguous queries to be matched to more relevant results.
- FIG. 1 A is a schematic diagram of an example implementation.
- user 102 provides voice input of a search query 106 to computing device 104 .
- user 102 can provide search query 106 using a keyboard or other input device in lieu of voice input.
- background audio source 108 for example, a radio or television, provides background audio 110 , which computing device 104 detects.
- Computing device 104 conveys search query 106 and background audio 110 to search system 114 through communications channel 112 .
- Search system 114 receives both search query 106 and background audio 110 through communications channel 112 from computing device 104 . If necessary, search system 114 uses speech recognition to convert search query 106 to computer-readable form. Search system 114 utilizes techniques disclosed herein to identify background audio 110 and retrieve terms that are related to the background audio. For example, if the background audio is a popular song, search system 114 can retrieve a set of terms that include other songs by the same artist, the name of the album on which the song appears, the names of the performers, etc.
- Search system 114 then executes search query 106 using at least one term related to the background audio.
- search system 114 can supplement search query 106 with at least one term related to background audio 110 and submit the supplemented search query to a search engine for processing.
- search system 114 can execute the search using augmented match scores for search results that include terms related to background audio 110 . Irrespective of the particular use of the terms related to background audio 110 , search system 114 obtains search results, which search system 114 conveys through communications channel 116 back to computing device 104 for display or audio presentation to user 102 .
- FIG. 1 B is a schematic diagram of a search system included in an example implementation.
- Search system 114 includes various components. These components and their interaction allow improved searching that uses information provided from the background audio to permit such improved searching.
- search system 114 involves with the receipt of search query 106 and background audio data 110 from computing device 104 .
- Search query 106 includes parameters for a search that specify the desired results. However, these parameters may be ambiguous, especially if search query 106 includes keywords that have multiple meanings.
- Background audio data 110 may include audio such as music or the soundtrack to a video program that, when identified, is associated with certain concepts that can help narrow the scope of search query 106 by reducing ambiguity or otherwise providing information that helps improve the quality of the results obtained by searching with search query 106 .
- Search query 106 may optionally be processed by voice recognition module 124 , in the case in which it is an audio query and it needs to be converted into a textual query, such as a natural language question or a set of keywords. However, if search query 106 is received as text to begin with, such as from a keyboard, then voice recognition module 124 may not be necessary for that implementation. Voice recognition module 124 is capable of receiving audio speech input and converting such input to computer readable data, for example, ASCII or Unicode text, using conventional techniques.
- background audio data 110 requires processing in order to lead to results that allow it to help improve results for search query 106 .
- Background audio data 110 is processed by background audio recognizer 126 .
- Background audio recognizer 126 analyzes background audio data 110 and determines that background audio data 110 includes audio that corresponds to a known segment of audio. While one example of how audio may be a known segment of audio is if the audio includes audio from an existing media entity, such as an audio component of a television or movie, or a piece of music, it will be recognize that implementations may generally use any identifications that can be made from analyzing the background audio. For example, simple examples of identified background audio might be that dialogue from an episode of “The Simpsons” is playing in the background, or the song “Penny Lane” is playing in the background. However, other implementations might take advantage of other identifications, such as recognizing voices of participants in a background conversation or recognizing noises made by a certain type of animal.
- Background audio source 108 may produce background audio 110 that user 102 may want to keep private or otherwise would prefer not to have recorded and/or analyzed.
- background audio 110 may include a private conversation, or some other type of background audio 110 that user 102 does not wish to have captured. Even background audio that may seem innocuous, such as a song playing in the background, may divulge information about user 102 that user 102 would prefer not to have made available to a third party.
- implementations should provide user 102 with a chance to affirmatively consent to the receipt of background audio 110 before receiving or analyzing audio that is received from background audio source 108 . Therefore, user 102 may be required to take action to specifically indicate that he or she is willing to allow the implementations to capture background audio 110 before the implementations are permitted to start recording background audio 110 .
- computing device 104 may prompt user 102 with a dialog box or other graphical user interface element to alert user 102 with a message that makes user aware that computing device 104 is about to monitor background audio 110 . For example, the message might state, “Please authorize use of background audio.
- background audio 110 may be shared with third parties.
- implementations should notify user 102 that gathering background audio 110 is about to begin, and furthermore that user 102 should be aware that the background audio 110 information that is accumulated may be shared in order to draw conclusions based on the background audio 110 . Only after user 102 has been alerted to these issues, and has affirmatively agreed that he or she is comfortable with recording the background audio, will background audio 110 be gathered from background audio source 108 .
- certain implementations may prompt the user 102 again to ensure that user 102 is comfortable with recording background audio 110 if the system has remained idle for a period of time, as the idle time may indicate that a new session has begun and prompting again will help ensure that user 102 is aware of privacy issues related to gathering background audio 110 and is comfortable having background audio 110 be recorded.
- the users may be provided with an opportunity to control whether programs or features collect personal information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), or to control whether and/or how to receive content from the content server that may be more relevant to the user.
- personal information e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location
- certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed.
- a user's identity may be anonymized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code, or state level
- the user may have control over how information is collected about him or her and used by a content server.
- Background audio recognizer 126 is capable of identifying an audio sample using conventional techniques. For example, background audio recognizer 126 accepts as an input data reflecting a query audio sample, uses such information to match the query audio sample to a known audio sample, and outputs an identification of the known audio sample. Background audio recognizer 126 thus includes or is coupled to a database storing data reflecting a large number of audio samples, e.g., songs, television program audio, etc.
- Example data reflecting an audio sample can include a spectrogram of the sample, or derivations of a spectrogram of the sample, e.g., a hash of part of the spectrogram.
- the spectrogram can include or be represented by, for example, identified peaks, e.g., local maxima, in a frequency domain.
- background audio recognizer 126 may be able to recognize background audio data 110 is to use an acoustic fingerprint database 128 .
- Acoustic fingerprint database 128 may communicate with background audio recognizer 126 to process background audio data 110 produce fingerprints of background audio data 110 that represent features of background audio data 110 , and match those fingerprints to other fingerprints in acoustic fingerprint database 128 .
- background audio recognizer 126 may receive background audio data 110 and code fingerprints based on background audio data 110 . By using those fingerprints as a query into acoustic fingerprint database 128 , background audio recognizer 126 may be able to draw a conclusion, such as that an audio snippet of the Eagles' “Hotel California” is playing in the background.
- background audio recognizer 126 recognizes background audio data 110 , it produces recognized background audio 132 .
- the next stage performed by search system 114 is that recognized background audio 132 must be processed through conceptual expander 134 .
- mapping engine can include or be coupled to a relational database, and can map an identification of an audio sample to terms related to the audio sample in the database.
- Conceptual expander 134 may use a variety of approaches to produce terms 138 .
- the identification of recognized background audio 132 may be used as a query to search a conceptual expander database 130 .
- Conceptual expander database 130 is an information repository that can map recognized background audio 132 to terms 138 that are related to it.
- Conceptual expander database 130 may accomplish this task in several ways, depending on its contents.
- conceptual expander database 130 may include a variety of documents, such as articles related to various topics, and these documents may be mined for keywords related to concepts based on the background audio.
- search query 106 is “Data Android” and background audio data 110 includes an audio snippet of the theme song from “Star Trek: The Next Generation.”
- Background audio recognizer 126 can identify recognized background audio 132 , and use this information with conceptual expander 134 .
- conceptual expander database 130 might include an article about “Star Trek: The Next Generation” that could be mined to produce terms 138 that are indicative of “Data Android” in that context.
- terms 138 might be “Brent Spiner” or “Lieutenant Commander”, rather than other senses of the terms “Data” and “Android.”
- An information repository that can serve in the role of conceptual expander 134 is an interconnected network of concepts, for example a comprehensive collection of real-world entities, such as people, places, things, and concepts along with the relationships and factual attributes that describe them. Examples of such networks include the Google Knowledge Graph, or Wikipedia. These networks describe entities that are related to literals in specific ways.
- recognized background audio 132 may include information about terms related to background audio 110 . If conceptual expander 134 uses such a network of concepts, it becomes possible to use the terms to identify entities and related literals, that can be considered for use in query disambiguation. For example, suppose that the recognized background audio 132 is a clip from the “Men in Black” theme song, sung by Will Smith.
- the network of concepts may serve in the role of conceptual expander 134 based on this information. Recognized background audio 132 leads the network of concepts to suggest certain entities as being relevant, based on recognized background audio 132 . For example, the entities “Will Smith” and “Men in Black” might be derived from recognized background audio 132 . Based on these entities, the network of concepts can then provide literals that have a relationship with these entities, defined by a schema. For example, the network of concepts can provide the date “Sep.
- the network of concepts may be a repository of entities that are associated with related literals, the network is well-suited to begin with entities derived from recognized background audio 132 and suggest related literals as terms 138 that expand the concepts and improve query performance.
- search query 106 can be combined to produce modified search query 136 , and then get results using a search engine 140 .
- This can be done either by concatenating one or more of terms 138 to search query 106 to produce modified search query 136 , or by filtering the results of a search on search query 106 as recognized by voice recognition module 124 .
- modified search query might take “Data Android” and add some of terms 138 to search on “Lieutenant Commander Data Android.”
- search engine 140 can filter results from searching on modified search query 136 . For example, before search engine 140 returns results 150 to computing device 104 , it may filter using terms 138 . For example, only search results that include “Brent Spiner” might be provided as results 150 .
- search engine 140 may include scoring engine 142 and scoring engine 142 may score results 150 based at least on on the relationship between results 150 and terms 138 .
- search system 114 includes search engine 140 .
- Search engine 140 can include a web-based search engine, a proprietary search engine, a document lookup engine, or a different response engine.
- Search engine 220 may include an indexing engine, which includes an index of a large portion of the internet or other network such as a local area network (LAN) or wide area network (WAN).
- Search engine 140 may further include scoring engine 142 .
- search engine 114 receives a search query, it matches it to the index in order to retrieve search results.
- Scoring engine 142 attributes a score to each such search result, and search engine 142 ranks, e.g., orders, the search results based on the scores. Search engine 142 is thus capable of returning search results in response to a search query.
- Search system 114 conveys the search query to search engine 140 . If the user supplied the search query in computer readable form, search system 114 conveys the query directly to search engine 140 . If the user supplied the search query to computing device 104 audibly, then search system 114 conveys the search query to voice recognition module 124 to obtain the search query in computer readable form, then provides the computer readable search query to search engine 140 .
- Search engine 140 processes the query taking into account at least one of the terms 138 related to the background audio. There are several ways that the system can accomplish this.
- search engine 140 supplements the search query with the term, or terms, related to the background audio. In such implementations search engine 140 obtains search results based on this supplemented search query.
- search engine 140 matches the terms of the search query to search results, and uses both the search query terms and the terms related to the background audio for scoring purposes using scoring engine 142 . In such implementations, the terms related to the background audio are not used for matching purposes.
- search engine 140 processes the query while taking into account the term, or terms, related to the background audio by augmenting scores attributed to search results matching the search query that also contain the term, or terms, related to the background audio.
- scoring engine 142 adds an amount to the scores of search results that also contain the term, or terms, related to the background audio.
- search system 114 conveys search results 150 back to computing device 104 , which presents search results 150 on display 180 to the user and/or outputs an audio rendering of search results 150 to the user using speaker 182 .
- FIG. 1 C is a schematic diagram of a computing device included in an example implementation.
- FIG. 1 C illustrates various hardware and other resources that can be used in implementations.
- Computing device 104 in some implementations can be a mobile telephone, a personal digital assistant, a laptop computer, a desktop computer, or another computer or hardware resource.
- Computing device 104 is communicatively coupled to search system 114 through communications channel 112 by way of interface 190 .
- Interface 190 includes components of computing device 104 that allows computing device 104 to interact with other entities such as search system 114 .
- Communications channel 112 can include, for example, a cellular communications channel, the internet, another network, or another wired or wireless data connection, in any combination.
- Computing device 104 further includes one or more processors 184 , which are coupled to various components.
- computing device 104 includes display 180 .
- Display 180 can be, by way of non-limiting example, a liquid crystal display.
- display 180 can be a touchscreen.
- display 180 can include a user interface, such as a virtual keyboard.
- Computing device 104 also includes input device 192 .
- Input device 192 can be, for example, a physical keyboard, e.g., a keyboard that includes physical, as opposed to virtual, keys.
- input device 192 can be combined with display 180 .
- display 18 can be implemented using a touchscreen, and in such implementations, the touchscreen can include a virtual keyboard as input device 192 .
- Computing device 104 may include microphone 188 , which may be omnidirectional and capable of picking up background audio in addition to a user's voice input.
- Computing device 104 may further include speaker 182 .
- Speaker 182 can be configured to output sound, such as that received over a telephone link. Alternately, or in addition, speaker 182 can output device-generated sound, such as tones or synthesized speech.
- a user of computing device 104 provides a query to computing device 104 using, for example, microphone 188 or input device 192 .
- Computing device 104 also receives, through microphone 188 , any background audio that is present at or around the time of the query input.
- Computing device 104 sends search query 106 and the background audio 108 to search system 114 through communications channel 112 .
- Search system 114 processes search query 106 and background audio 108 as discussed in FIG. 1 B .
- FIG. 2 is a flowchart of a method according to some implementations.
- computing device 104 receives a query from a user.
- the user can supply the query as a voice input using microphone 188 , for example. Alternately, or in addition, in some implementations, the user can supply the query using input device 192 .
- computing device 104 obtains background audio using, for example, microphone 188 .
- Computing device 104 can gather such background audio while the user is entering or submitting a search query, whether the user enters the query as a voice input or in computer readable form.
- computing device 104 gathers background audio in a time interval that commences after the user has submitted the search query. That is, in some implementations, computing device 104 detects the background audio immediately after the user requests that the search query be executed, e.g., by activating an ENTER key or otherwise providing input that query entry is complete.
- the time interval can be any period of time from 0.1 seconds to 10 seconds.
- computing device 104 determines that the user has finished entering the search query as a voice input by detecting a drop below a threshold volume level.
- computing device 104 gathers background audio both before and after the user submits the search query.
- background audio recognizer 126 of voice recognition system 114 identifies concepts associated with the background audio to produce recognized background audio 132 .
- One way in which this may occur is that background audio recognizer 126 may search an acoustic fingerprint database 128 using background audio data 110 to identify the nature of the background audio and the corresponding related concepts.
- conceptual expander 134 obtains terms related to the recognized background audio 132 .
- recognized background audio 132 may search a conceptual expander database 130 that provides terms 138 associated with the concepts from recognized background audio 132 .
- the background audio is a song
- such related terms can include, for example, the song title, the song lyrics, the performing artist, the composer, the album, the titles for the other songs on the same album, and any other related information, e.g., from a relational database.
- the background audio is a television program or movie audio segment
- such related terms can include, for example, the actors, the producers, the title of the program or movie, the network, and any portions from a transcript of the program or movie.
- conceptual expander database 130 may include other terms suggested based on the identity of recognized background audio 132 .
- search engine 140 obtains search results based on the search query and on at least one related term.
- the number of related terms that search engine 140 takes into account can vary. For example, the number of terms may vary from 1 to 15, but there is no absolute limit on the number of related terms.
- search engine 140 executes the search multiple times, each time with an additional related term, retaining only those search results whose score attributed by scoring engine 142 exceeds a threshold. There are several ways in which search engine 140 can obtain search results based on the query while taking into account the at least one related term.
- search engine 140 supplements the search query with at least one term related to the background audio and executes the supplemented query in a conventional manner to obtain search results. In some implementations, search engine 140 provides an ordered set of search results.
- search engine 140 matches the search query to search results using an indexing engine, requiring all terms from the search query to be present in the search result.
- search engine 140 allows the at least one term related to the background audio to be present in the matching search results, but does not require it. Instead, in such implementations, all terms, including those in the search query and those related to the background audio, count toward the score attributed to the matching results by scoring engine 142 .
- search engine 140 processes the search query and at least one term related to the background audio in such a way as to require the terms from the search query to be present in the search results, but uses at least one term related to the background audio as an optional search term or terms. Search engine 140 then orders the results according to score. Search engine 140 thus obtains search results 150 for the search query while taking into account the background audio.
- search engine 140 augments scores attributed to search results matching the search query that also contain at least one term related to the background audio.
- search engine 140 matches the search query to search results using an indexing engine. Scoring engine 142 attributes a score to each such matched search result, and then adds an amount to the scores of search results that also contain at least one term related to the background audio. After scoring engine 142 adjusts each score as appropriate, search engine 140 ranks the results according to score. In this manner, search engine 140 obtains search results for the search query while taking into account the background audio.
- scoring engine 142 adjusts scores for search results that include the term, or terms, related to the background audio
- the amount of the adjustment can be set by fiat, or learned. There are several learning techniques that can be used.
- the amount by which to increase scores can be learned by first obtaining a large set, e.g., 100-100,000, of ambiguous queries followed by related unambiguous queries. For each such query pair, the technique calculates the difference between the score of the highest ranked search result for the unambiguous query and the score attributed to the same search result for the ambiguous query. The technique then takes the average of these differences as the amount by which to increase search result scores.
- Another technique for learning the amount to add utilizes a large set, e.g., 100-100,000, of ambiguous/unambiguous query pairs together with background audio for each pair.
- This technique determines the amount of score increase, denoted X, that maximizes the proportion of the large set of query pairs for which the following two conditions are satisfied: (1) the highest ranked ambiguous query search result does not appear in the top N unambiguous query search results, where N is fixed at any number from 1 to 25, in an example implementation, and (2) adding X to the score of each of the ambiguous query search results that contain terms related to the background audio causes the highest ranked ambiguous query search result to change as a result of such addition(s).
- the technique selects the value of X that maximizes the proportion of the query pairs for which conditions (1) and (2) are satisfied.
- the technique can use, for example, an exhaustive search, a gradient descent, or another methodology.
- the learning process can be ongoing, for example, as the system receives more and more search queries from users.
- search system 114 sends the search results to computing device 104 using communications channel 112 .
- Computing device 104 receives the search results and presents them to the user using, for example, display 180 and/or speaker 182 .
- Each hardware component can include one or more processors coupled to random access memory operating under control of, or in conjunction with, an operating system.
- the search system can include network interfaces to connect with clients through a network. Such interfaces can include one or more servers.
- each hardware component can include persistent storage, such as a hard drive or drive array, which can store program instructions to perform the techniques disclosed herein. That is, such program instructions can serve to perform techniques as disclosed.
- Other configurations of search system 114 , computing device 104 , associated network connections, and other hardware, software, and service resources are possible.
Abstract
Implementations relate to techniques for providing context-dependent search results. The techniques can include receiving a query and background audio. The techniques can also include identifying the background audio, establishing concepts related to the background audio and obtaining terms related to the concepts related to the background audio. The techniques can also include obtaining search results based on the query and on at least one of the terms. The techniques can also include providing the search results.
Description
The techniques provided herein relate to query disambiguation.
Internet search engines provide information about Internet-accessible resources, e.g., web pages, documents, and images, in response to a user's query. A user can submit a query using a device, such as a mobile telephone, that includes a microphone. Sometimes users submit queries to internet search engines that are ambiguous in that they relate to more than one concept and/or entity.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a search query, receiving background audio, identifying one or more concepts related to the background audio, generating a set of related terms related to the identified concepts, receiving search results based on the search query and on at least one of the terms related to the identified concepts, and providing the search results.
Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. A system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
The foregoing and other implementations can each optionally include one or more of the following features, alone or in combination. In particular, one implementation may include all of the following features in combination. For example, receiving search results based on the search query and at least one at least one of the terms related to the identified concepts may include providing the query to a search engine, receiving scored results from the search engine, and altering a score for a search result containing at least one of the related terms. The method may further include determining an amount to alter the score by using a plurality of training queries. Alternatively, providing the search results may include providing only search results that satisfy a threshold score.
In another implementation, receiving search results based on the search query and at least on at least one of the terms related to the identified concepts may include providing the query to a search engine together with at least one of the related terms and receiving results from the search engine.
In yet another implementation, identifying concepts related to the background audio may include recognizing at least a portion of the background audio by matching it to an acoustic fingerprint and identifying concepts related to the background audio including concepts associated with the acoustic fingerprint.
Also, generating a set of terms related to the background audio may include generating a set of terms based on querying a conceptual expansion database based on the concepts related to the background audio.
Disclosed techniques provide certain technical advantages. Some implementations are capable of using background audio to clarify an ambiguous search query based on background audio. Such implementations provide more accurate search results, thus achieving a technical advantage.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate implementations of the described technology. In the figures:
Like reference numbers and designations in the various drawings indicate like elements.
Users gain information about internet-accessible resources by submitting a query to a search engine using a client device, such as a mobile telephone, which may be equipped with a microphone. Sometimes the user's query is ambiguous. For example, the user can submit the query “scorpions”. Without more information, the search engine has no way to determine whether the user wants information on scorpion arthropods or the rock band Scorpions. However, if the search engine determined that the user submitted the query while listening to the Scorpions' song “Rock You Like A Hurricane”, it could provide search results relevant to the rock band, instead of those relevant to the arthropods. Some implementations perform a search based on both the user's query and terms that are related to background audio present at, or around, a time of the search query's input. In this manner, implementations respond to search queries by taking into account the context of background audio. The context of background audio thus allows ambiguous queries to be matched to more relevant results.
Reference will now be made in detail to example implementations, which are illustrated in the accompanying drawings. Where possible the same reference numbers will be used throughout the drawings to refer to the same or like parts.
The operation of search system 114 involves with the receipt of search query 106 and background audio data 110 from computing device 104. Search query 106 includes parameters for a search that specify the desired results. However, these parameters may be ambiguous, especially if search query 106 includes keywords that have multiple meanings. Background audio data 110 may include audio such as music or the soundtrack to a video program that, when identified, is associated with certain concepts that can help narrow the scope of search query 106 by reducing ambiguity or otherwise providing information that helps improve the quality of the results obtained by searching with search query 106.
However, background audio data 110 requires processing in order to lead to results that allow it to help improve results for search query 106. Background audio data 110 is processed by background audio recognizer 126. Background audio recognizer 126 analyzes background audio data 110 and determines that background audio data 110 includes audio that corresponds to a known segment of audio. While one example of how audio may be a known segment of audio is if the audio includes audio from an existing media entity, such as an audio component of a television or movie, or a piece of music, it will be recognize that implementations may generally use any identifications that can be made from analyzing the background audio. For example, simple examples of identified background audio might be that dialogue from an episode of “The Simpsons” is playing in the background, or the song “Penny Lane” is playing in the background. However, other implementations might take advantage of other identifications, such as recognizing voices of participants in a background conversation or recognizing noises made by a certain type of animal.
Because of the need to ensure that the user is comfortable with having the background audio processed in case the background audio includes content that the user does not wish to have recorded and/or analyzed, implementations should provide user 102 with a chance to affirmatively consent to the receipt of background audio 110 before receiving or analyzing audio that is received from background audio source 108. Therefore, user 102 may be required to take action to specifically indicate that he or she is willing to allow the implementations to capture background audio 110 before the implementations are permitted to start recording background audio 110. For example, computing device 104 may prompt user 102 with a dialog box or other graphical user interface element to alert user 102 with a message that makes user aware that computing device 104 is about to monitor background audio 110. For example, the message might state, “Please authorize use of background audio. Please note that information about background audio may be shared with third parties.” Thus, in order to ensure that background audio 110 is gathered exclusively from consenting users, implementations should notify user 102 that gathering background audio 110 is about to begin, and furthermore that user 102 should be aware that the background audio 110 information that is accumulated may be shared in order to draw conclusions based on the background audio 110. Only after user 102 has been alerted to these issues, and has affirmatively agreed that he or she is comfortable with recording the background audio, will background audio 110 be gathered from background audio source 108. Furthermore, certain implementations may prompt the user 102 again to ensure that user 102 is comfortable with recording background audio 110 if the system has remained idle for a period of time, as the idle time may indicate that a new session has begun and prompting again will help ensure that user 102 is aware of privacy issues related to gathering background audio 110 and is comfortable having background audio 110 be recorded.
For situations in which the systems discussed here collect personal information about users, or may make use of personal information, the users may be provided with an opportunity to control whether programs or features collect personal information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current location), or to control whether and/or how to receive content from the content server that may be more relevant to the user. In addition, certain data may be anonymized in one or more ways before it is stored or used, so that personally identifiable information is removed. For example, a user's identity may be anonymized so that no personally identifiable information can be determined for the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over how information is collected about him or her and used by a content server.
One way that background audio recognizer 126 may be able to recognize background audio data 110 is to use an acoustic fingerprint database 128. Acoustic fingerprint database 128 may communicate with background audio recognizer 126 to process background audio data 110 produce fingerprints of background audio data 110 that represent features of background audio data 110, and match those fingerprints to other fingerprints in acoustic fingerprint database 128. For example, background audio recognizer 126 may receive background audio data 110 and code fingerprints based on background audio data 110. By using those fingerprints as a query into acoustic fingerprint database 128, background audio recognizer 126 may be able to draw a conclusion, such as that an audio snippet of the Eagles' “Hotel California” is playing in the background.
After background audio recognizer 126 recognizes background audio data 110, it produces recognized background audio 132. The next stage performed by search system 114 is that recognized background audio 132 must be processed through conceptual expander 134.
The role of conceptual expander 134 is to take recognized background audio 132 and use the identification information for recognized background audio 132 to produce terms 138 that can influence the query to improve the results. Conceptual expander 134 is capable of returning, in response to an identification of an audio sample, terms related to such sample. Thus, mapping engine can include or be coupled to a relational database, and can map an identification of an audio sample to terms related to the audio sample in the database. Conceptual expander 134 may use a variety of approaches to produce terms 138. As an example way of doing this, the identification of recognized background audio 132 may be used as a query to search a conceptual expander database 130. Conceptual expander database 130 is an information repository that can map recognized background audio 132 to terms 138 that are related to it. Conceptual expander database 130 may accomplish this task in several ways, depending on its contents. For example, conceptual expander database 130 may include a variety of documents, such as articles related to various topics, and these documents may be mined for keywords related to concepts based on the background audio. For example, suppose that search query 106 is “Data Android” and background audio data 110 includes an audio snippet of the theme song from “Star Trek: The Next Generation.” Background audio recognizer 126 can identify recognized background audio 132, and use this information with conceptual expander 134. For example, conceptual expander database 130 might include an article about “Star Trek: The Next Generation” that could be mined to produce terms 138 that are indicative of “Data Android” in that context. For example, terms 138 might be “Brent Spiner” or “Lieutenant Commander”, rather than other senses of the terms “Data” and “Android.”
One example of an information repository that can serve in the role of conceptual expander 134 is an interconnected network of concepts, for example a comprehensive collection of real-world entities, such as people, places, things, and concepts along with the relationships and factual attributes that describe them. Examples of such networks include the Google Knowledge Graph, or Wikipedia. These networks describe entities that are related to literals in specific ways. As discussed above, recognized background audio 132 may include information about terms related to background audio 110. If conceptual expander 134 uses such a network of concepts, it becomes possible to use the terms to identify entities and related literals, that can be considered for use in query disambiguation. For example, suppose that the recognized background audio 132 is a clip from the “Men in Black” theme song, sung by Will Smith. The network of concepts may serve in the role of conceptual expander 134 based on this information. Recognized background audio 132 leads the network of concepts to suggest certain entities as being relevant, based on recognized background audio 132. For example, the entities “Will Smith” and “Men in Black” might be derived from recognized background audio 132. Based on these entities, the network of concepts can then provide literals that have a relationship with these entities, defined by a schema. For example, the network of concepts can provide the date “Sep. 25, 1968” as having the “date of birth” relationship to “Will Smith,” or “Tommy Lee Jones” as having a “lead actor” relationship to “Men in Black.” Because the network of concepts may be a repository of entities that are associated with related literals, the network is well-suited to begin with entities derived from recognized background audio 132 and suggest related literals as terms 138 that expand the concepts and improve query performance.
Once conceptual expander 134 produces terms 138, search query 106, as recognized by voice recognition module 124, if necessary, can be combined to produce modified search query 136, and then get results using a search engine 140. This can be done either by concatenating one or more of terms 138 to search query 106 to produce modified search query 136, or by filtering the results of a search on search query 106 as recognized by voice recognition module 124. Continuing the example previously discussed, modified search query might take “Data Android” and add some of terms 138 to search on “Lieutenant Commander Data Android.” Additionally, search engine 140 can filter results from searching on modified search query 136. For example, before search engine 140 returns results 150 to computing device 104, it may filter using terms 138. For example, only search results that include “Brent Spiner” might be provided as results 150. In one implementation, search engine 140 may include scoring engine 142 and scoring engine 142 may score results 150 based at least on on the relationship between results 150 and terms 138.
Thus, search system 114 includes search engine 140. Search engine 140 can include a web-based search engine, a proprietary search engine, a document lookup engine, or a different response engine. Search engine 220 may include an indexing engine, which includes an index of a large portion of the internet or other network such as a local area network (LAN) or wide area network (WAN). Search engine 140 may further include scoring engine 142. When search engine 114 receives a search query, it matches it to the index in order to retrieve search results. Scoring engine 142 attributes a score to each such search result, and search engine 142 ranks, e.g., orders, the search results based on the scores. Search engine 142 is thus capable of returning search results in response to a search query.
In some implementations, search engine 140 supplements the search query with the term, or terms, related to the background audio. In such implementations search engine 140 obtains search results based on this supplemented search query.
In some implementations, search engine 140 matches the terms of the search query to search results, and uses both the search query terms and the terms related to the background audio for scoring purposes using scoring engine 142. In such implementations, the terms related to the background audio are not used for matching purposes.
In some implementations, search engine 140 processes the query while taking into account the term, or terms, related to the background audio by augmenting scores attributed to search results matching the search query that also contain the term, or terms, related to the background audio. In implementations that use this technique, scoring engine 142 adds an amount to the scores of search results that also contain the term, or terms, related to the background audio.
Regardless as to the particular technique that search engine 140 uses to obtain search results 150, search system 114 conveys search results 150 back to computing device 104, which presents search results 150 on display 180 to the user and/or outputs an audio rendering of search results 150 to the user using speaker 182.
Thus, a user of computing device 104 provides a query to computing device 104 using, for example, microphone 188 or input device 192. Computing device 104 also receives, through microphone 188, any background audio that is present at or around the time of the query input. Computing device 104 sends search query 106 and the background audio 108 to search system 114 through communications channel 112. Search system 114 processes search query 106 and background audio 108 as discussed in FIG. 1B .
At block 210, computing device 104 receives a query from a user. The user can supply the query as a voice input using microphone 188, for example. Alternately, or in addition, in some implementations, the user can supply the query using input device 192.
At block 220, computing device 104 obtains background audio using, for example, microphone 188. Computing device 104 can gather such background audio while the user is entering or submitting a search query, whether the user enters the query as a voice input or in computer readable form. In some implementations, computing device 104 gathers background audio in a time interval that commences after the user has submitted the search query. That is, in some implementations, computing device 104 detects the background audio immediately after the user requests that the search query be executed, e.g., by activating an ENTER key or otherwise providing input that query entry is complete. The time interval can be any period of time from 0.1 seconds to 10 seconds. In some implementations, computing device 104 determines that the user has finished entering the search query as a voice input by detecting a drop below a threshold volume level. In some implementations, computing device 104 gathers background audio both before and after the user submits the search query.
At block 230, background audio recognizer 126 of voice recognition system 114 identifies concepts associated with the background audio to produce recognized background audio 132. One way in which this may occur is that background audio recognizer 126 may search an acoustic fingerprint database 128 using background audio data 110 to identify the nature of the background audio and the corresponding related concepts.
At block 240, conceptual expander 134 obtains terms related to the recognized background audio 132. One way in which this may occur is that recognized background audio 132 may search a conceptual expander database 130 that provides terms 138 associated with the concepts from recognized background audio 132.
If the background audio is a song, such related terms can include, for example, the song title, the song lyrics, the performing artist, the composer, the album, the titles for the other songs on the same album, and any other related information, e.g., from a relational database. If the background audio is a television program or movie audio segment, such related terms can include, for example, the actors, the producers, the title of the program or movie, the network, and any portions from a transcript of the program or movie. However, these are only example terms, and conceptual expander database 130 may include other terms suggested based on the identity of recognized background audio 132.
At block 250, search engine 140 obtains search results based on the search query and on at least one related term. The number of related terms that search engine 140 takes into account can vary. For example, the number of terms may vary from 1 to 15, but there is no absolute limit on the number of related terms. In some implementations, search engine 140 executes the search multiple times, each time with an additional related term, retaining only those search results whose score attributed by scoring engine 142 exceeds a threshold. There are several ways in which search engine 140 can obtain search results based on the query while taking into account the at least one related term.
In some implementations, search engine 140 supplements the search query with at least one term related to the background audio and executes the supplemented query in a conventional manner to obtain search results. In some implementations, search engine 140 provides an ordered set of search results.
In some implementations, search engine 140 matches the search query to search results using an indexing engine, requiring all terms from the search query to be present in the search result. In such implementations, search engine 140 allows the at least one term related to the background audio to be present in the matching search results, but does not require it. Instead, in such implementations, all terms, including those in the search query and those related to the background audio, count toward the score attributed to the matching results by scoring engine 142. Thus, in such implementations, search engine 140 processes the search query and at least one term related to the background audio in such a way as to require the terms from the search query to be present in the search results, but uses at least one term related to the background audio as an optional search term or terms. Search engine 140 then orders the results according to score. Search engine 140 thus obtains search results 150 for the search query while taking into account the background audio.
In some implementations, search engine 140 augments scores attributed to search results matching the search query that also contain at least one term related to the background audio. In implementations that use this technique for obtaining search results 150 for the search query while taking into account the background audio, search engine 140 matches the search query to search results using an indexing engine. Scoring engine 142 attributes a score to each such matched search result, and then adds an amount to the scores of search results that also contain at least one term related to the background audio. After scoring engine 142 adjusts each score as appropriate, search engine 140 ranks the results according to score. In this manner, search engine 140 obtains search results for the search query while taking into account the background audio.
For implementations in which scoring engine 142 adjusts scores for search results that include the term, or terms, related to the background audio, the amount of the adjustment can be set by fiat, or learned. There are several learning techniques that can be used.
In some implementations, the amount by which to increase scores can be learned by first obtaining a large set, e.g., 100-100,000, of ambiguous queries followed by related unambiguous queries. For each such query pair, the technique calculates the difference between the score of the highest ranked search result for the unambiguous query and the score attributed to the same search result for the ambiguous query. The technique then takes the average of these differences as the amount by which to increase search result scores.
Another technique for learning the amount to add utilizes a large set, e.g., 100-100,000, of ambiguous/unambiguous query pairs together with background audio for each pair. This technique determines the amount of score increase, denoted X, that maximizes the proportion of the large set of query pairs for which the following two conditions are satisfied: (1) the highest ranked ambiguous query search result does not appear in the top N unambiguous query search results, where N is fixed at any number from 1 to 25, in an example implementation, and (2) adding X to the score of each of the ambiguous query search results that contain terms related to the background audio causes the highest ranked ambiguous query search result to change as a result of such addition(s). The technique selects the value of X that maximizes the proportion of the query pairs for which conditions (1) and (2) are satisfied. To select such an X, the technique can use, for example, an exhaustive search, a gradient descent, or another methodology.
In implementations that rely on learning to set or adjust the score increase amount, the learning process can be ongoing, for example, as the system receives more and more search queries from users.
At block 260, search system 114 sends the search results to computing device 104 using communications channel 112. Computing device 104 receives the search results and presents them to the user using, for example, display 180 and/or speaker 182.
In general, systems capable of performing the disclosed techniques can take many different forms. Further, the functionality of one portion of the system can be substituted into another portion of the system. Each hardware component can include one or more processors coupled to random access memory operating under control of, or in conjunction with, an operating system. The search system can include network interfaces to connect with clients through a network. Such interfaces can include one or more servers. Further, each hardware component can include persistent storage, such as a hard drive or drive array, which can store program instructions to perform the techniques disclosed herein. That is, such program instructions can serve to perform techniques as disclosed. Other configurations of search system 114, computing device 104, associated network connections, and other hardware, software, and service resources are possible.
The foregoing description is illustrative, and variations in configuration and implementation can occur. Other resources described as singular or integrated can in implementations be plural or distributed, and resources described as multiple or distributed can in implementations be combined. The scope of the present teachings is accordingly intended to be limited only by the following claims.
Claims (18)
1. A computer-implemented method comprising:
receiving (i) a search query including one or more query terms entered into a mobile computing device by a user, the search query being entered by the user speaking the search query during a first period of time during which a voice detection signal is above a threshold volume level, indicating the presence of the user's voice, and (ii) background audio that is not made by the user, and that is produced by a source in an environment surrounding the mobile computing device within a predetermined time of entry of the one or more query terms, the predetermined time of entry being outside of the first time period, and wherein the background audio includes audio that is detected during a second time period that is a fixed time interval and after the first time period during which the voice detection signal is below the threshold volume level, indicating an absence of the user's voice, the background audio being detected during the second time period in response to a determination by the mobile computing device that the voice detection signal has fallen below the threshold volume level;
identifying a known audio segment based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
generating a set of related terms that describe entities that are associated in an entity-relationship model with the known audio segment that is identified based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
modifying the search query to include, as query terms, one or more related terms of the set of related terms that describe entities that are associated in the entity-relationship model with the known audio segment that is identified based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
receiving search results based on the modified search query; and
providing the search results.
2. The method of claim 1 , wherein receiving search results based on the modified search query comprises:
providing the query to a search engine;
receiving scored results from the search engine; and
altering a score for a search result containing at least one of the related terms.
3. The method of claim 2 , further comprising determining an amount to alter the score by using a plurality of training queries.
4. The method of claim 1 , wherein providing the search results comprises providing only search results that satisfy a threshold score.
5. The method of claim 1 , wherein identifying the known audio segment based on the background audio comprises:
recognizing at least a portion of the background audio by matching it to an acoustic fingerprint; and
identifying the known audio segment based on the background audio comprising the known audio segment associated with the acoustic fingerprint.
6. The method of claim 1 , wherein generating the set of terms related to the background audio comprises:
generating the set of terms based on querying a database based on the known audio segment based on the background audio.
7. A system comprising:
one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:
receiving (i) a search query including one or more query terms entered into a mobile computing device by a user, the search query being entered by the user speaking the search query during a first period of time during which a voice detection signal is above a threshold volume level, indicating the presence of the user's voice, and (ii) background audio that is not made by the user, and that is produced by a source in an environment surrounding the mobile computing device within a predetermined time of entry of the one or more query terms, the predetermined time of entry being outside of the first time period, and wherein the background audio includes audio that is detected during a second time period that is a fixed time interval and after the first time period during which the voice detection signal is below the threshold volume level, indicating an absence of the user's voice, the background audio being detected during the second time period in response to a determination by the mobile computing device that the voice detection signal has fallen below the threshold volume level;
identifying a known audio segment based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
generating a set of related terms that describe entities that are associated in an entity-relationship model with the known audio segment that is identified based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
modifying the search query to include, as query terms, one or more related terms of the set of related terms that describe entities that are associated in the entity-relationship model with the known audio segment that is identified based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
receiving search results based on the modified search query; and
providing the search results.
8. The system of claim 7 , wherein receiving search results based on the modified search query comprises:
providing the query to a search engine;
receiving scored results from the search engine; and
altering a score for a search result containing at least one of the related terms.
9. System of claim 8 , wherein the operations further comprise determining an amount to alter the score by using a plurality of training queries.
10. The system of claim 7 , wherein providing the search results comprises providing only search results that satisfy a threshold score.
11. The system of claim 7 , wherein identifying the known audio segment based on the background audio comprises:
recognizing at least a portion of the background audio by matching it to an acoustic fingerprint; and
identifying the known audio segment based on the background audio comprising the known audio segment associated with the acoustic fingerprint.
12. The system of claim 7 , wherein generating the set of terms related to the background audio comprises:
generating the set of terms based on querying a database based on the known audio segment based on the background audio.
13. A non-transitory computer-readable storage device storing software comprising instructions executable by one or more computers which, upon such execution, cause the one or more computers to perform operations comprising:
receiving (i) a search query including one or more query terms entered into a mobile computing device by a user, the search query being entered by the user speaking the search query during a first period of time during which a voice detection signal is above a threshold volume level, indicating the presence of the user's voice, and (ii) background audio that is not made by the user, and that is produced by a source in an environment surrounding the mobile computing device within a predetermined time of entry of the one or more query terms, the predetermined time of entry being outside of the first time period, and wherein the background audio includes audio that is detected during a second time period that is a fixed time interval and after the first time period during which the voice detection signal is below the threshold volume level, indicating an absence of the user's voice, the background audio being detected during the second time period in response to a determination by the mobile computing device that the voice detection signal has fallen below the threshold volume level;
identifying a known audio segment based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
generating a set of related terms that describe entities that are associated in an entity-relationship model with the known audio segment that is identified based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
modifying the search query to include, as query terms, one or more related terms of the set of related terms that describe entities that are associated in the entity-relationship model with the known audio segment that is identified based on the background audio that is not made by the user, and that is produced by the source in the environment surrounding the mobile computing device within the predetermined time of the entry of the one or more query terms;
receiving search results based on the modified search query; and
providing the search results.
14. The storage device of claim 13 , wherein receiving search results based on the modified search query comprises:
providing the query to a search engine;
receiving scored results from the search engine; and
altering a score for a search result containing at least one of the related terms.
15. The storage device of claim 14 , wherein the operations further comprise determining an amount to alter the score by using a plurality of training queries.
16. The storage device of claim 13 , wherein providing the search results comprises providing only search results that satisfy a threshold score.
17. The storage device of claim 13 , wherein identifying the known audio segment based on the background audio comprises:
recognizing at least a portion of the background audio by matching it to an acoustic fingerprint; and
identifying the known audio segment based on the background audio comprising the known audio segment associated with the acoustic fingerprint.
18. The storage device of claim 13 wherein generating the set of terms related to the background audio comprises:
generating the set of terms based on querying a database based on the known audio segment based on the background audio.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/334,378 US11640426B1 (en) | 2012-06-01 | 2021-05-28 | Background audio identification for query disambiguation |
Applications Claiming Priority (6)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261654518P | 2012-06-01 | 2012-06-01 | |
US201261654387P | 2012-06-01 | 2012-06-01 | |
US201261654407P | 2012-06-01 | 2012-06-01 | |
US201313795153A | 2013-03-12 | 2013-03-12 | |
US16/244,366 US11023520B1 (en) | 2012-06-01 | 2019-01-10 | Background audio identification for query disambiguation |
US17/334,378 US11640426B1 (en) | 2012-06-01 | 2021-05-28 | Background audio identification for query disambiguation |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/244,366 Continuation US11023520B1 (en) | 2012-06-01 | 2019-01-10 | Background audio identification for query disambiguation |
Publications (1)
Publication Number | Publication Date |
---|---|
US11640426B1 true US11640426B1 (en) | 2023-05-02 |
Family
ID=76094464
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/244,366 Active 2033-05-17 US11023520B1 (en) | 2012-06-01 | 2019-01-10 | Background audio identification for query disambiguation |
US17/334,378 Active US11640426B1 (en) | 2012-06-01 | 2021-05-28 | Background audio identification for query disambiguation |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/244,366 Active 2033-05-17 US11023520B1 (en) | 2012-06-01 | 2019-01-10 | Background audio identification for query disambiguation |
Country Status (1)
Country | Link |
---|---|
US (2) | US11023520B1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220392435A1 (en) * | 2021-06-08 | 2022-12-08 | Comcast Cable Communications, Llc | Processing Voice Commands |
Citations (48)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5721902A (en) | 1995-09-15 | 1998-02-24 | Infonautics Corporation | Restricted expansion of query terms using part of speech tagging |
US6006622A (en) | 1997-05-26 | 1999-12-28 | Bayerische Motoren Werke Aktiengesellschaft | Transmission gear arrangement for compensating play at the tooth flanks |
US6345252B1 (en) | 1999-04-09 | 2002-02-05 | International Business Machines Corporation | Methods and apparatus for retrieving audio information using content and speaker information |
US6415258B1 (en) | 1999-10-06 | 2002-07-02 | Microsoft Corporation | Background audio recovery system |
US6454520B1 (en) | 2000-05-16 | 2002-09-24 | Delphi Technologies, Inc. | Enhanced v-blade impeller design for a regenerative turbine |
US6584439B1 (en) * | 1999-05-21 | 2003-06-24 | Winbond Electronics Corporation | Method and apparatus for controlling voice controlled devices |
US20040225650A1 (en) | 2000-03-06 | 2004-11-11 | Avaya Technology Corp. | Personal virtual assistant |
AU2005200340A1 (en) | 1999-03-05 | 2005-02-24 | Canon Kabushiki Kaisha | Database annotation and retrieval |
US20050154713A1 (en) | 2004-01-14 | 2005-07-14 | Nec Laboratories America, Inc. | Systems and methods for determining document relationship and automatic query expansion |
US20060074883A1 (en) | 2004-10-05 | 2006-04-06 | Microsoft Corporation | Systems, methods, and interfaces for providing personalized search and information access |
US7027987B1 (en) | 2001-02-07 | 2006-04-11 | Google Inc. | Voice interface for a search engine |
US7089188B2 (en) | 2002-03-27 | 2006-08-08 | Hewlett-Packard Development Company, L.P. | Method to expand inputs for word or document searching |
US20060200431A1 (en) | 2005-03-01 | 2006-09-07 | Microsoft Corporation | Private clustering and statistical queries while analyzing a large database |
US20060253427A1 (en) | 2005-05-04 | 2006-11-09 | Jun Wu | Suggesting and refining user input based on original user input |
WO2007012120A1 (en) | 2005-07-26 | 2007-02-01 | Redfern International Enterprises Pty Ltd | Enhanced searching using a thesaurus |
US20070071206A1 (en) | 2005-06-24 | 2007-03-29 | Gainsboro Jay L | Multi-party conversation analyzer & logger |
WO2007064640A2 (en) | 2005-11-29 | 2007-06-07 | Google Inc. | Detecting repeating content in broadcast media |
US20080005076A1 (en) | 2006-06-28 | 2008-01-03 | Microsoft Corporation | Entity-specific search model |
US20080005068A1 (en) | 2006-06-28 | 2008-01-03 | Microsoft Corporation | Context-based search, retrieval, and awareness |
US20080162471A1 (en) * | 2005-01-24 | 2008-07-03 | Bernard David E | Multimodal natural language query system for processing and analyzing voice and proximity-based queries |
US20080215597A1 (en) | 2005-06-21 | 2008-09-04 | Hidetsugu Nanba | Information processing apparatus, information processing system, and program |
US20080244675A1 (en) | 2007-04-02 | 2008-10-02 | Sony Corporation | Imaged image data processing apparatus, viewing information creating apparatus, viewing information creating system, imaged image data processing method and viewing information creating method |
US20080250011A1 (en) | 2007-04-09 | 2008-10-09 | Alexander Haubold | Method and apparatus for query expansion based on multimodal cross-vocabulary mapping |
US20090006294A1 (en) | 2007-06-28 | 2009-01-01 | Microsoft Corporation | Identification of events of search queries |
US20090018898A1 (en) | 2007-06-29 | 2009-01-15 | Lawrence Genen | Method or apparatus for purchasing one or more media based on a recommendation |
US20090157383A1 (en) * | 2007-12-18 | 2009-06-18 | Samsung Electronics Co., Ltd. | Voice query extension method and system |
US20090228439A1 (en) | 2008-03-07 | 2009-09-10 | Microsoft Corporation | Intent-aware search |
US20100076996A1 (en) | 2005-03-24 | 2010-03-25 | The Mitre Corporation | System and method for audio hot spotting |
US20100145971A1 (en) | 2008-12-08 | 2010-06-10 | Motorola, Inc. | Method and apparatus for generating a multimedia-based query |
US20100204986A1 (en) | 2002-06-03 | 2010-08-12 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US7783620B1 (en) | 2007-06-29 | 2010-08-24 | Emc Corporation | Relevancy scoring using query structure and data structure for federated search |
EP2228737A2 (en) | 2009-03-05 | 2010-09-15 | Edward Michael Carroll | Improving search effectiveness |
US20110035382A1 (en) | 2008-02-05 | 2011-02-10 | Dolby Laboratories Licensing Corporation | Associating Information with Media Content |
US20110153324A1 (en) | 2009-12-23 | 2011-06-23 | Google Inc. | Language Model Selection for Speech-to-Text Conversion |
US20110257974A1 (en) | 2010-04-14 | 2011-10-20 | Google Inc. | Geotagged environmental audio for enhanced speech recognition accuracy |
US20110273213A1 (en) | 2010-05-06 | 2011-11-10 | Qualcomm Incorporated | Method and apparatus to dynamically adjust a clock rate in a mobile device |
US20110295590A1 (en) | 2010-05-26 | 2011-12-01 | Google Inc. | Acoustic model adaptation using geographic information |
US20110307253A1 (en) | 2010-06-14 | 2011-12-15 | Google Inc. | Speech and Noise Models for Speech Recognition |
US20120034904A1 (en) | 2010-08-06 | 2012-02-09 | Google Inc. | Automatically Monitoring for Voice Input Based on Context |
US8131716B2 (en) | 2007-06-29 | 2012-03-06 | Emc Corporation | Tuning of relevancy ranking for federated search |
US20120117051A1 (en) | 2010-11-05 | 2012-05-10 | Microsoft Corporation | Multi-modal approach to search query input |
US20120179465A1 (en) | 2011-01-10 | 2012-07-12 | International Business Machines Corporation | Real time generation of audio content summaries |
US20120239175A1 (en) | 2010-07-29 | 2012-09-20 | Keyvan Mohajer | System and method for matching a query against a broadcast stream |
US20120253802A1 (en) | 2011-03-31 | 2012-10-04 | Microsoft Corporation | Location-Based Conversational Understanding |
US20120296938A1 (en) | 2011-05-18 | 2012-11-22 | Microsoft Corporation | Query and Matching for Content Recognition |
US20120296458A1 (en) | 2011-05-18 | 2012-11-22 | Microsoft Corporation | Background Audio Listening for Content Recognition |
US8352467B1 (en) | 2006-05-09 | 2013-01-08 | Google Inc. | Search result ranking based on trust |
US9947333B1 (en) | 2012-02-10 | 2018-04-17 | Amazon Technologies, Inc. | Voice interaction architecture with intelligent background noise cancellation |
-
2019
- 2019-01-10 US US16/244,366 patent/US11023520B1/en active Active
-
2021
- 2021-05-28 US US17/334,378 patent/US11640426B1/en active Active
Patent Citations (49)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5721902A (en) | 1995-09-15 | 1998-02-24 | Infonautics Corporation | Restricted expansion of query terms using part of speech tagging |
US6006622A (en) | 1997-05-26 | 1999-12-28 | Bayerische Motoren Werke Aktiengesellschaft | Transmission gear arrangement for compensating play at the tooth flanks |
AU2005200340A1 (en) | 1999-03-05 | 2005-02-24 | Canon Kabushiki Kaisha | Database annotation and retrieval |
US6345252B1 (en) | 1999-04-09 | 2002-02-05 | International Business Machines Corporation | Methods and apparatus for retrieving audio information using content and speaker information |
US6584439B1 (en) * | 1999-05-21 | 2003-06-24 | Winbond Electronics Corporation | Method and apparatus for controlling voice controlled devices |
US6415258B1 (en) | 1999-10-06 | 2002-07-02 | Microsoft Corporation | Background audio recovery system |
US20040225650A1 (en) | 2000-03-06 | 2004-11-11 | Avaya Technology Corp. | Personal virtual assistant |
US6454520B1 (en) | 2000-05-16 | 2002-09-24 | Delphi Technologies, Inc. | Enhanced v-blade impeller design for a regenerative turbine |
US7027987B1 (en) | 2001-02-07 | 2006-04-11 | Google Inc. | Voice interface for a search engine |
US7089188B2 (en) | 2002-03-27 | 2006-08-08 | Hewlett-Packard Development Company, L.P. | Method to expand inputs for word or document searching |
US20100204986A1 (en) | 2002-06-03 | 2010-08-12 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US20050154713A1 (en) | 2004-01-14 | 2005-07-14 | Nec Laboratories America, Inc. | Systems and methods for determining document relationship and automatic query expansion |
US20060074883A1 (en) | 2004-10-05 | 2006-04-06 | Microsoft Corporation | Systems, methods, and interfaces for providing personalized search and information access |
US20080162471A1 (en) * | 2005-01-24 | 2008-07-03 | Bernard David E | Multimodal natural language query system for processing and analyzing voice and proximity-based queries |
US20060200431A1 (en) | 2005-03-01 | 2006-09-07 | Microsoft Corporation | Private clustering and statistical queries while analyzing a large database |
US20100076996A1 (en) | 2005-03-24 | 2010-03-25 | The Mitre Corporation | System and method for audio hot spotting |
US20060253427A1 (en) | 2005-05-04 | 2006-11-09 | Jun Wu | Suggesting and refining user input based on original user input |
US20080215597A1 (en) | 2005-06-21 | 2008-09-04 | Hidetsugu Nanba | Information processing apparatus, information processing system, and program |
US20070071206A1 (en) | 2005-06-24 | 2007-03-29 | Gainsboro Jay L | Multi-party conversation analyzer & logger |
WO2007012120A1 (en) | 2005-07-26 | 2007-02-01 | Redfern International Enterprises Pty Ltd | Enhanced searching using a thesaurus |
WO2007064640A2 (en) | 2005-11-29 | 2007-06-07 | Google Inc. | Detecting repeating content in broadcast media |
US8352467B1 (en) | 2006-05-09 | 2013-01-08 | Google Inc. | Search result ranking based on trust |
US20080005076A1 (en) | 2006-06-28 | 2008-01-03 | Microsoft Corporation | Entity-specific search model |
US20080005068A1 (en) | 2006-06-28 | 2008-01-03 | Microsoft Corporation | Context-based search, retrieval, and awareness |
US20080244675A1 (en) | 2007-04-02 | 2008-10-02 | Sony Corporation | Imaged image data processing apparatus, viewing information creating apparatus, viewing information creating system, imaged image data processing method and viewing information creating method |
US20080250011A1 (en) | 2007-04-09 | 2008-10-09 | Alexander Haubold | Method and apparatus for query expansion based on multimodal cross-vocabulary mapping |
US20090006294A1 (en) | 2007-06-28 | 2009-01-01 | Microsoft Corporation | Identification of events of search queries |
US20090018898A1 (en) | 2007-06-29 | 2009-01-15 | Lawrence Genen | Method or apparatus for purchasing one or more media based on a recommendation |
US8131716B2 (en) | 2007-06-29 | 2012-03-06 | Emc Corporation | Tuning of relevancy ranking for federated search |
US7783620B1 (en) | 2007-06-29 | 2010-08-24 | Emc Corporation | Relevancy scoring using query structure and data structure for federated search |
US20090157383A1 (en) * | 2007-12-18 | 2009-06-18 | Samsung Electronics Co., Ltd. | Voice query extension method and system |
US20110035382A1 (en) | 2008-02-05 | 2011-02-10 | Dolby Laboratories Licensing Corporation | Associating Information with Media Content |
US20090228439A1 (en) | 2008-03-07 | 2009-09-10 | Microsoft Corporation | Intent-aware search |
US20100145971A1 (en) | 2008-12-08 | 2010-06-10 | Motorola, Inc. | Method and apparatus for generating a multimedia-based query |
EP2228737A2 (en) | 2009-03-05 | 2010-09-15 | Edward Michael Carroll | Improving search effectiveness |
US20110153324A1 (en) | 2009-12-23 | 2011-06-23 | Google Inc. | Language Model Selection for Speech-to-Text Conversion |
US20110257974A1 (en) | 2010-04-14 | 2011-10-20 | Google Inc. | Geotagged environmental audio for enhanced speech recognition accuracy |
US20110273213A1 (en) | 2010-05-06 | 2011-11-10 | Qualcomm Incorporated | Method and apparatus to dynamically adjust a clock rate in a mobile device |
US20110295590A1 (en) | 2010-05-26 | 2011-12-01 | Google Inc. | Acoustic model adaptation using geographic information |
US20110307253A1 (en) | 2010-06-14 | 2011-12-15 | Google Inc. | Speech and Noise Models for Speech Recognition |
US20120239175A1 (en) | 2010-07-29 | 2012-09-20 | Keyvan Mohajer | System and method for matching a query against a broadcast stream |
WO2012019020A1 (en) | 2010-08-06 | 2012-02-09 | Google Inc. | Automatically monitoring for voice input based on context |
US20120034904A1 (en) | 2010-08-06 | 2012-02-09 | Google Inc. | Automatically Monitoring for Voice Input Based on Context |
US20120117051A1 (en) | 2010-11-05 | 2012-05-10 | Microsoft Corporation | Multi-modal approach to search query input |
US20120179465A1 (en) | 2011-01-10 | 2012-07-12 | International Business Machines Corporation | Real time generation of audio content summaries |
US20120253802A1 (en) | 2011-03-31 | 2012-10-04 | Microsoft Corporation | Location-Based Conversational Understanding |
US20120296938A1 (en) | 2011-05-18 | 2012-11-22 | Microsoft Corporation | Query and Matching for Content Recognition |
US20120296458A1 (en) | 2011-05-18 | 2012-11-22 | Microsoft Corporation | Background Audio Listening for Content Recognition |
US9947333B1 (en) | 2012-02-10 | 2018-04-17 | Amazon Technologies, Inc. | Voice interaction architecture with intelligent background noise cancellation |
Non-Patent Citations (6)
Title |
---|
Checkik et al., "Large-scale content-based audio retrieval from text queries", Proceedings of the 1st ACM international conference on Multimedia information retrieval, pp. 106-112, 2008. |
Egozi et al., "Concept-Based Feature Generation and Selection for Information Retrieval", Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pp. 1132-1137, 2008. |
Haus et al., "An Audio front end for query-by-humming systems", Proceedings of International Sympsium on Music Information Retrieval, 2001. |
Mobile Image Search With Multimodal Context-Aware Queries, Yang et al, IEEE Computer Society Conference on Computer Vision and Pattern Recognition—Workshops, p. 25-32, Jun. 2010. |
Multimodal Photo Annotation and Retrieval on a Mobile Phone, Anguera et al, 2008. |
Natsev et al., Semantic concept-based query expansion and re-ranking for multimedia retrieval, Proceedings of the 15th International Conference on Multimedia, pp. 991-1000, 2007. |
Also Published As
Publication number | Publication date |
---|---|
US11023520B1 (en) | 2021-06-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11557280B2 (en) | Background audio identification for speech disambiguation | |
US11960526B2 (en) | Query response using media consumption history | |
US20220214775A1 (en) | Method for extracting salient dialog usage from live data | |
US10417344B2 (en) | Exemplar-based natural language processing | |
US9679257B2 (en) | Method and apparatus for adapting a context model at least partially based upon a context-related search criterion | |
US9123330B1 (en) | Large-scale speaker identification | |
US10838746B2 (en) | Identifying parameter values and determining features for boosting rankings of relevant distributable digital assistant operations | |
US10698654B2 (en) | Ranking and boosting relevant distributable digital assistant operations | |
US10768954B2 (en) | Personalized digital assistant device and related methods | |
US11537651B2 (en) | Descriptive media content search | |
US20140379346A1 (en) | Video analysis based language model adaptation | |
US9224385B1 (en) | Unified recognition of speech and music | |
WO2014039106A1 (en) | Answering questions using environmental context | |
US20200403816A1 (en) | Utilizing volume-based speaker attribution to associate meeting attendees with digital meeting content | |
US11640426B1 (en) | Background audio identification for query disambiguation | |
JP6322125B2 (en) | Speech recognition apparatus, speech recognition method, and speech recognition program | |
EP2706470A1 (en) | Answering questions using environmental context | |
US11983217B2 (en) | Responding to queries with voice recordings | |
US20230009983A1 (en) | Responding to queries with voice recordings |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |