CN110679151B9 - Method and apparatus for video coding using parameterized motion models - Google Patents
Method and apparatus for video coding using parameterized motion models Download PDFInfo
- Publication number
- CN110679151B9 CN110679151B9 CN201880035742.4A CN201880035742A CN110679151B9 CN 110679151 B9 CN110679151 B9 CN 110679151B9 CN 201880035742 A CN201880035742 A CN 201880035742A CN 110679151 B9 CN110679151 B9 CN 110679151B9
- Authority
- CN
- China
- Prior art keywords
- block
- encoded
- frame
- current block
- pixel
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/597—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding specially adapted for multi-view video sequence encoding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/182—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a pixel
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/513—Processing of motion vectors
- H04N19/521—Processing of motion vectors for estimating the reliability of the determined motion vectors or motion vector field, e.g. for smoothing the motion vector field or for correcting motion vectors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
- H04N19/51—Motion estimation or motion compensation
- H04N19/56—Motion estimation with initialisation of the vector search, e.g. estimating a good candidate to initiate a search
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/105—Selection of the reference unit for prediction within a chosen coding or prediction mode, e.g. adaptive choice of position and number of pixels used for prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
- H04N19/137—Motion inside a coding unit, e.g. average field, frame or block difference
- H04N19/139—Analysis of motion vectors, e.g. their magnitude, direction, variance or reliability
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/187—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a scalable video layer
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/70—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by syntax aspects related to video coding, e.g. related to compression standards
Abstract
Video blocks of a stereoscopic or non-stereoscopic video sequence are coded using a parameterized motion model. Encoding a current block of a stereoscopic video sequence may include: a block-level disparity between a first frame and a second frame is determined and a plane normal candidate within the current block of the first frame is identified based on the block-level disparity. One of the plane normal candidates is selected based on a rate distortion value, and a warp parameter for predicting motion within the current block is determined using the selected plane normal candidate. The current block is encoded using a reference block generated by applying the warp parameter. Decoding the encoded block may include: the method includes receiving a bitstream representing an encoded video sequence, determining warp parameters for predicting motion within the encoded block based on syntax elements encoded into the bitstream, and decoding the encoded block using a reference block generated by applying the warp parameters.
Description
Technical Field
The present application relates generally to methods and apparatus for video coding using parameterized motion models.
Background
The digital video stream may represent video using a series of frames or still images. Digital video may be used in a variety of applications including, for example, video conferencing, high definition video entertainment, video advertising, or sharing user-generated video. Digital video streams may contain large amounts of data and consume large amounts of computing or communication resources of a computing device to process, transmit, or store video data. In order to reduce the amount of data in a video stream, various methods have been proposed, including encoding or decoding techniques.
Disclosure of Invention
A method for encoding blocks of frames of a stereoscopic video sequence according to an embodiment of the present disclosure includes: a block level disparity between a first frame of the stereoscopic video sequence and a second frame of the stereoscopic video sequence is determined. The method further comprises the steps of: a plane normal candidate within a current block of the first frame is identified based on the block level disparity. The method further comprises the steps of: one of the plane normal candidates is selected based on rate distortion values associated with a plurality of the plane normal candidates. The method further comprises the steps of: the selected plane normal candidates are used to determine warp parameters for predicting motion within the current block. The method further comprises the steps of: the current block is encoded according to the warp parameter.
An apparatus for encoding blocks of frames of a stereoscopic video sequence according to an embodiment of the present disclosure includes a processor configured to execute instructions stored in a non-transitory storage medium. The instructions include instructions for determining block-level disparities between a first frame of the stereoscopic video sequence and a second frame of the stereoscopic video sequence. The instructions also include instructions for identifying a plane normal candidate within a current block of the first frame based on the block level disparity. The instructions further include instructions for selecting one of the plane normal candidates based on rate distortion values associated with a plurality of the plane normal candidates. The instructions further include instructions for determining warp parameters for predicting motion within the current block using the selected plane normal candidates. The instructions further include instructions for encoding the current block according to the warp parameter.
A method for decoding an encoded block of an encoded frame of an encoded stereoscopic video sequence according to an embodiment of the present disclosure includes: a bitstream representing an encoded stereoscopic video sequence is received. The method further comprises the steps of: a warp parameter for predicting motion within a coding block of a first coded frame of the coded stereoscopic video sequence is determined based on one or more syntax elements coded to the bitstream using the selected plane normal candidates. The selected plane normal candidate represents a current pixel of the encoded block and is selected based on block level disparity between a first encoded frame and a second encoded frame of the encoded video sequence. The method further comprises the steps of: the encoded block is decoded according to the warp parameter.
An apparatus for decoding an encoded block of an encoded frame of an encoded stereoscopic video sequence according to an embodiment of the present disclosure includes a processor configured to execute instructions stored in a non-transitory storage medium. The instructions include instructions for: a bitstream representing an encoded stereoscopic video sequence is received and warp parameters for predicting motion within a coding block of a first encoded frame of the encoded stereoscopic video sequence are determined based on one or more syntax elements encoded to the bitstream using the selected plane normal candidates. The selected plane normal candidate represents a current pixel of the encoded block and is selected based on block level disparity between a first encoded frame and a second encoded frame of the encoded video sequence. The instructions further include instructions for decoding the encoded block according to the warp parameter.
These and other aspects of the disclosure are disclosed in the following detailed description of the embodiments, the appended claims and the accompanying drawings.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views.
Fig. 1 is a schematic diagram of a video encoding and decoding system.
Fig. 2 is a block diagram of an example of a computing device that may implement a sending station or a receiving station.
Fig. 3 is a schematic diagram of a typical video stream to be encoded and subsequently decoded.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
FIG. 5 is a block diagram of a decoder according to an embodiment of the present disclosure;
fig. 6 is a diagram of corresponding left and right views shown within an associated frame of a stereoscopic video sequence.
Fig. 7 is a flow chart of an example of a technique for encoding video blocks using a parameterized motion model.
Fig. 8 is a flow chart of an example of a technique for decoding an encoded block using a parameterized motion model.
Detailed Description
The video compression scheme may include: the respective image or frame is decomposed into smaller portions (such as blocks) and an output bitstream is generated using techniques for restricting the information included in the output for the respective block. The encoded bitstream may be decoded to recreate the source image from the limited information. Typical video compression and decompression techniques use periodic motion compensation that uses inter-frame redundancy to predict motion based on temporal similarity between video frames or inter-frame redundancy to predict motion based on spatial similarity within individual video frames.
In some cases, motion compensation may also be performed using redundancy other than inter-or intra-frame redundancy. For example, in the context of stereoscopic video (e.g., as used in virtual reality video applications), inter-view redundancy may be used to determine similarity between left and right views. Stereoscopic video includes left and right views reflecting images respectively seen through left and right eyes of a stereoscopic video user. Each of the left view and the right view may be represented by a frame of a stereoscopic video sequence. Thus, at any given time during stereoscopic video presentation, two frames are displayed to the user. These frames show different perspectives of the same portion of video and thus may include many similarities. Because these frames may share similarity, one of these frames may be used as a reference for predicting motion within the other frames.
However, periodic motion compensation of stereoscopic and non-stereoscopic video sequences uses pure translational motion between or within blocks to predict motion within blocks of a frame to be encoded or decoded. However, not all motions within a block can be described using translational motion models, which can only perform rectangular transformations. For example, some of the motions may include scaling, shearing, or rotational motions, either alone or in combination with translational motions. In the context of stereoscopic video sequences, for example, translational motion models may not accurately reflect the similarity between video frames representing left and right views.
Some techniques for performing motion compensation, including determining non-pure translational motion, may include: warp parameters for predicting motion are determined using motion vectors from video blocks adjacent to a current block to be encoded or decoded and using a function (e.g., least squares fit). These warp parameters may be parameters of an affine motion model, for example. However, these techniques do not always provide accurate predictions for stereoscopic video sequences, for example, because these warp parameters may not accurately reflect motion in the stereoscopic video sequence.
Embodiments of the present disclosure include: video blocks of a stereoscopic or non-stereoscopic video sequence are encoded and decoded using a parameterized motion model reflecting multiple motion types within a video frame. The parameterized motion model includes warp parameters that indicate how to warp (e.g., scale, rotate, or otherwise move) pixels of a video block to generate a prediction block for encoding or decoding such video block. Embodiments of the present disclosure may be used to predict motion based on inter-frame redundancy or inter-view redundancy.
In one example, encoding a current block of a stereoscopic video sequence according to an embodiment of the present disclosure may include: a block-level disparity between a first frame and a second frame of a stereoscopic video sequence is determined, and a plane normal candidate within a current block of the second frame is identified based on the block-level disparity. One of the plane normal candidates is selected based on the rate distortion value, and a warp parameter for predicting motion within the current block is determined using the selected plane normal candidate. The current block is then encoded according to the warp parameter. Decoding the encoded block according to embodiments of the present disclosure may include: a method includes receiving a bitstream representing an encoded stereoscopic video sequence and determining warp parameters for predicting motion within an encoded block based on one or more syntax elements encoded to the bitstream. The encoded block is then decoded according to the warp parameter.
Additional details of techniques for video coding using parameterized motion models are initially described herein with respect to systems capable of implementing these techniques. Fig. 1 is a schematic diagram of a video encoding and decoding system 100. The transmitting station 102 may be, for example, a computer having a hardware internal configuration such as the one described in fig. 2. However, other implementations of the sending station 102 are also possible. For example, the processing of the sending station 102 may be distributed across multiple devices.
The network 104 may connect a transmitting station 102 and a receiving station 106 for encoding and decoding video streams. In particular, the video stream may be encoded in the sending station 102 and the encoded video stream may be decoded in the receiving station 106. The network 104 may be, for example, the internet. Network 104 may also be a Local Area Network (LAN), wide Area Network (WAN), virtual Private Network (VPN), cellular telephone network, or any other manner of transmitting video streams from sending station 102 to receiving station 106 in this example.
In one example, the receiving station 106 may be a computer having a hardware internal configuration such as the one depicted in fig. 2. However, other suitable implementations of the receiving station 106 are also possible. For example, the processing of the receiving station 106 may be distributed across multiple devices.
Other implementations of the video encoding and decoding system 100 are also possible. For example, embodiments may omit network 104. In another embodiment, the video stream may be encoded and then stored for transmission to the receiving station 106 at a later time or any other device having memory. In one embodiment, the receiving station 106 receives the encoded video stream (e.g., via the network 104, a computer bus, and/or some communication path) and stores the video stream for later decoding. In an example embodiment, real-time transport protocol (RTP) is used to transport encoded video over network 104. In another embodiment, a transport protocol other than RTP may be used, such as a hypertext transfer protocol (HTTP) based video streaming protocol.
When used in a video conferencing system, for example, the sending station 102 and/or the receiving station 106 may include the following capabilities to encode and decode video streams. For example, the receiving station 106 may be a video conference participant that receives the encoded video bitstream from the video conference server (e.g., the sending station 102) to decode and view and further encodes and sends its own video bitstream to the video conference server to decode and view by other participants.
Fig. 2 is a block diagram of an example of a computing device 200 that may implement a sending station or a receiving station. For example, computing device 200 may implement one or both of transmitting station 102 and receiving station 106 of fig. 1. The computing device 200 may be in the form of a computing system including multiple computing devices or in the form of one computing device, such as a mobile phone, tablet computer, laptop computer, notebook computer, desktop computer, or the like.
In an embodiment, the memory 204 in the computing device 200 may be a Read Only Memory (ROM) device or a Random Access Memory (RAM) device. However, other suitable types of storage devices may also be used as memory 204. Memory 204 may include code and data 206 that is accessed by processor 202 using bus 212. Memory 204 may further include an operating system 208 and an application 210, application 210 including at least one program that allows processor 202 to perform the techniques described herein. For example, application 210 may include applications 1 through N, which applications 1 through N further include video encoding applications that perform the techniques described herein. Computing device 200 may also include secondary storage 214, which secondary storage 214 may be, for example, a memory card for use with a mobile computing device. Because video communication sessions may contain a large amount of information, they may be stored in whole or in part in secondary storage 214 and loaded into memory 204 for processing as desired.
The computing device 200 may also include or communicate with an image sensing device 220 (e.g., a camera) or any other image sensing device 220 capable of sensing the present or later developed of an image, such as an image of a user operating the computing device 200. The image sensing device 220 may be positioned such that it is directed to a user operating the computing device 200. In an example, the position and optical axis of the image sensing device 220 may be configured such that the field of view includes an area directly adjacent to the display 218 and from which the display 218 is visible.
The computing device 200 may also include or be in communication with a sound sensing device 222 (e.g., a microphone) or any other sound sensing device capable of sensing the present or later developed sound in the vicinity of the computing device 200. The sound sensing device 222 can be positioned such that it is directed to a user operating the computing device 200 and can be configured to receive sound, such as speech or other utterances uttered by the user while operating the computing device 200.
Although fig. 2 depicts the processor 202 and memory 204 of the computing device 200 as being integrated as one unit, other configurations may be utilized. The operations of processor 202 may be distributed across multiple machines (where individual machines may have one or more processors) or local area networks or other networks that can be directly coupled. Memory 204 may be allocated to multiple machines, such as network-based memory or storage in multiple machines performing the operations of computing device 200. Although depicted as a single bus, the bus 212 of the computing device 200 may be comprised of multiple buses. Further, secondary storage 214 may be directly coupled to other components of computing device 200 or may be accessible via a network and may include an integrated unit, such as a memory card or multiple units, such as multiple memory cards. The computing device 200 may thus be implemented in a variety of configurations.
Fig. 3 is a schematic diagram of an example of a video stream 300 to be encoded and subsequently decoded. Video stream 300 includes video sequence 302. At the next level, the video sequence 302 includes a plurality of adjacent frames 304. Although three frames are depicted as adjacent frames 304, the video sequence 302 may include any number of adjacent frames 304. The adjacent frames 304 may then be further subdivided into individual frames, e.g., frame 306. At the next level, the frame 306 may be divided into a series of planes or segments 308. For example, segment 308 may be a subset of frames that allow parallel processing. Segment 308 may also be a subset of frames that are capable of dividing video data into separate colors. For example, a frame 306 of color video data may include a luminance plane and two chrominance planes. Segment 308 may be sampled at different resolutions.
Whether or not frame 306 is divided into segments 308, frame 306 may be further subdivided into blocks 310, which blocks 310 may contain data corresponding to, for example, 16 x 16 pixels in frame 306. Block 310 may also be configured to include data from one or more segments 308 of pixel data. The block 310 may also have any other suitable size, such as 4 x 4 pixels, 8 x 8 pixels, 16 x 8 pixels, 8 x 16 pixels, 16 x 16 pixels, or larger. The terms block and macroblock are used interchangeably herein unless otherwise indicated.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. As described above, the encoder 400 may be implemented in the sending station 102, such as by providing a computer software program stored in a memory (e.g., memory 204). The computer software program may include machine instructions that, when executed by a processor (such as processor 202), cause sending station 102 to encode video data in the manner described in fig. 4. Encoder 400 may also be implemented as dedicated hardware included in, for example, transmitting station 102. In a particularly preferred embodiment, encoder 400 is a hardware encoder.
The encoder 400 has the following stages for performing various functions in the forward path (shown by solid connecting lines) using the video stream 300 as input to produce an encoded or compressed bitstream 420: an intra/inter prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy encoding stage 408. The encoder 400 may also include a reconstruction path (shown by the dashed connection) for reconstructing frames encoding future blocks. In fig. 4, the encoder 400 has the following stages for performing various functions in the reconstruction path: a dequantization stage 410, an inverse transformation stage 412, a reconstruction stage 414, and a loop filtering stage 416. Other structural variations of encoder 400 may also be used to encode video stream 300.
When video stream 300 is presented for encoding, corresponding adjacent frames 304 (such as frames 306) may be processed in units of blocks. In the intra/inter prediction stage 402, the corresponding block may be encoded using intra prediction (also referred to as intra prediction) or inter prediction (also referred to as inter prediction). In any case, a prediction block may be formed. In the case of intra prediction, a prediction block may be formed from samples in the current frame that have been previously encoded and reconstructed. In the case of inter prediction, a prediction block may be formed from samples in one or more reference frames that were previously reconstructed.
Next, the prediction block may be subtracted from the current block at the intra/inter stage 402 to produce a residual block (also referred to as a residual). The transform stage 404 transforms the residual using a block-based transform into transform coefficients, e.g., in the frequency domain. The quantization stage 406 converts the transform coefficients into discrete quantized values, referred to as quantized transform coefficients, using a quantizer value or quantization stage. For example, the transform coefficients may be divided by the quantizer values and truncated.
The quantized transform coefficients are then entropy encoded in an entropy encoding stage 408. The entropy encoded coefficients are then output to the compressed bitstream 420 along with other information for decoding the block (which information may include, for example, syntax elements, such as information indicating the type of prediction used, the type of transform, motion vectors, quantizer values, etc.). The compressed bitstream 420 may be formatted using various techniques, such as Variable Length Coding (VLC) or arithmetic coding. The compressed bitstream 420 may also be referred to as an encoded video stream or an encoded video bitstream, and the terms are used interchangeably herein.
The reconstruction path (shown by the dashed connection) may be used to ensure that the encoder 400 and decoder 500 (described below with respect to fig. 5) decode the compressed bitstream 420 using the same reference frame. The reconstruction path performs functions similar to those that occur during the decoding process (described below with respect to fig. 5), including dequantizing quantized transform coefficients in dequantization stage 410 and inverse transforming the dequantized transform coefficients in inverse transformation stage 412 to produce a derivative residual block (also referred to as a derivative residual). In reconstruction stage 414, the predicted block predicted in intra/inter prediction stage 402 may be added to the derivative residual to produce a reconstructed block. Loop filtering stage 416 may be applied to the reconstructed block to reduce distortion, such as block artifacts.
Other variations of encoder 400 may also be used to encode compressed bit stream 420. In some implementations, the non-transform based encoder may directly quantize the residual signal without the transform stage 404 for certain blocks or frames. In some implementations, the encoder can have a quantization stage 406 and a dequantization stage 410 combined into a common stage.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. Decoder 500 may be implemented in receiving station 106, for example, by providing a computer software program stored in memory 204. The computer software program may include machine instructions that, when executed by a processor (such as processor 202), cause the receiving station 106 to decode video data in the manner described in fig. 5. Decoder 500 may also be implemented in hardware included in, for example, transmitting station 102 or receiving station 106.
Similar to the reconstruction path of encoder 400 described above, in one example, decoder 500 includes the following stages for performing various functions to produce output video stream 516 from compressed bitstream 420: entropy decoding stage 502, dequantization stage 504, inverse transform stage 506, intra/inter prediction stage 508, reconstruction stage 510, loop filtering stage 512, and deblocking filtering stage 514. Other structural variations of decoder 500 may also be used to decode compressed bit stream 420.
When the compressed bitstream 420 is presented for decoding, data elements within the compressed bitstream 420 may be decoded in an entropy decoding stage 502 to produce a set of quantized transform coefficients. Dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients with quantizer values), and inverse transform stage 506 inverse transforms the dequantized transform coefficients to produce derivative residuals that may be the same as the derivative residuals produced by inverse transform stage 412 in encoder 400. Using header information decoded from compressed bitstream 420, decoder 500 may generate the same prediction block as that generated in encoder 400 (e.g., at intra/inter prediction stage 402) using intra/inter prediction stage 508.
In the reconstruction stage 510, the prediction block may be added to the derivative residual to produce a reconstructed block. Loop filtering stage 512 may be applied to the reconstructed block to reduce block artifacts. Other filtering may also be applied to the reconstruction block. In this example, deblocking filtering stage 514 is applied to reconstructed blocks to reduce block distortion, and the results are output as output video stream 516. The output video stream 516 may also be referred to as a decoded video stream, and the terms are used interchangeably herein. Other variations of decoder 500 may also be used to decode compressed bit stream 420. In some implementations, decoder 500 may generate output video stream 516 without deblocking filtering stage 514.
Fig. 6 is an illustration of respective left and right views displayed within an associated frame of a stereoscopic video sequence. As shown, a first frame 600 represents a left view and a second frame 602 represents a right view; however, based on the arrangement of frames within the stereoscopic video sequence, the first frame 600 may in some cases represent a right view, while the second frame 602 in this case represents a left view. The first frame 600 and the second frame 602 are displayed simultaneously in the output video stream to a user, for example, wearing a virtual reality helmet or otherwise using a device configured for stereoscopic video viewing. The first frame 600 and the second frame 602 may include the same video object. For example, each of the first frame 600 and the second frame 602 may include objects displayed in the first frame 600 at block 604A and in the second frame at block 604B. That is, the pixels of each of blocks 604A and 604B may have the same luminance, chrominance, or other value.
However, the location of block 604A in the first frame 600 is different from the location of block 604B in the second frame 602. This may be due, for example, to the perspectives associated with the left and right views for viewing the objects shown at blocks 604A and 604B. The corresponding block 606 reflects the juxtaposition of the block 604A within the second frame 602.
The difference in position between the block 604B and the corresponding block 606 within the second frame 602 is referred to as a disparity 608. The disparity 608 reflects the horizontal position difference between the block 604B and the corresponding block 606 because the first frame 600 and the second frame 602 are two views (e.g., as left and right views) within the stereoscopic video sequence and follow epipolar geometry. In at least some cases, the first frame 600 and the second frame 602 are considered to be different views of a single stereoscopic video frame.
The disparity 608 may be determined by performing a disparity search on the blocks of the first frame 600 and the second frame 602. For example, performing the parallax search may include: juxtaposition blocks 606 are identified based on the locations of blocks 604A within first frame 600 and then a diamond search is performed starting from the locations of juxtaposition blocks 606 within second frame 602. For example, the diamond search may be a one-dimensional diamond search. This is because the parallax 608 to be determined reflects a one-dimensional translation. The parallax search area 610 reflects an area of the second frame 602 in which parallax search is performed.
Techniques for encoding or decoding video blocks are now described with respect to fig. 7 and 8. Fig. 7 is a flow chart of an example of a technique 700 for encoding video blocks using a parameterized motion model. Fig. 8 is a flow chart of an example of a technique 800 for decoding an encoded block using a parameterized motion model. For example, one or both of techniques 700 or 800 may be implemented as a software program that may be executed by a computing device (such as transmitting station 102 or receiving station 106). For example, a software program may include machine-readable instructions that may be stored in a memory (such as memory 204 or secondary storage 214) and that, when executed by a processor (such as processor 202), may cause a computing device to perform technique 700 and/or technique 800. One or both of techniques 700 or 800 may be implemented using dedicated hardware or firmware. As described above, some computing devices may have multiple memories or processors, and the operations described in one or both of techniques 700 or 800 may be allocated using multiple processors, memories, or both.
For ease of explanation, the techniques 700 and 800 are depicted and described as a series of steps or operations, respectively. However, steps or operations according to the present disclosure may occur in various orders and/or concurrently. In addition, other steps or operations not presented and described herein may be utilized. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter.
Referring first to fig. 7, a flow chart of an example of a technique 700 for encoding video blocks using a parameterized motion model is shown. The parameterized motion model is used to warp the reference frames towards the video block. In the context of a stereoscopic video sequence, for example, to better match video blocks in one stereoscopic frame, corresponding reference blocks in another stereoscopic frame are warped.
In 702, block level disparity between a first frame of a stereoscopic video sequence and a second frame of the stereoscopic video sequence is determined. In an example, the first frame and the second frame may represent left and right views, respectively, of a stereoscopic video sequence; however, in another example, the first frame and the second frame may represent a right view and a left view of the stereoscopic video sequence, respectively. The block level disparity is a disparity indicating a difference in position between one or more blocks of a first frame and one or more blocks of a second frame. For example, block-level disparity may reflect a horizontal position difference between a video block to be encoded (e.g., a current block) within a first frame and a reference video block within a second frame. Determining block level disparity may include: disparity searching is performed on blocks of the first frame and blocks of the second frame (e.g., the current block and the reference video block). For example, the parallax search may be a one-dimensional diamond search.
That is, the left and right views of a stereoscopic video sequence follow epipolar geometry such that pixel values located in one of the views will be disposed along the same epipolar (e.g., horizontal) line as the same pixel values within the other of the views. Thus, the disparity between the current block and the reference video block ultimately reflects a one-dimensional translation. Then, block level disparities may be determined based on the results of the disparity search. For example, if the position of the current block within the first frame is (x, y), the position of the reference block within the second frame is (x+d, y), where D represents the disparity determined by performing the disparity search.
In 704, a plane normal candidate is identified within the current block. Plane normal candidates are identified based on block level disparities. Identifying plane normal candidates includes: a normal search is performed on pixels of the current block based on the locations of sample pixels of the current block and block-level disparities. The locations of respective ones of the pixels within the current block are then identified based on the results of the normal search. Then, a pixel-level disparity is determined based on the position of a corresponding pixel among pixels within the current block. These pixel level disparities are used to identify plane normal candidates.
For example, a three-dimensional linear plane may be used to simulate one-dimensional parallax motion associated with block-level parallax. The pixel level disparity for a given pixel may be defined as d (x, y) =a x+b x y+c, where d (x, y) represents a disparity function with respect to the position of the pixel within the current block; x and y represent the locations of pixels within the current block; a and b are warp parameters for modeling a linear plane of one-dimensional disparity motion associated with block-level disparities and common to all pixels in the current block; and c is a block level disparity that is also common to all pixels in the current block. The values of x and y may be initially defined as the origin of the current block (e.g., the center of the current block) such that the values of x and y are zero, respectively (e.g., such that the position of the representation is (0, 0)). Thus, the pixel level disparity at the center of the current block is represented by its block level disparity.
Thus, a discovery search is performed for the pixels of the current block based on the initial position (0, 0) and the block-level disparity. Performing the normal search includes: values of normal vectors (nx, ny, nz) which are normal vectors of a three-dimensional linear plane for simulating one-dimensional parallax motions associated with block-level parallaxes are determined. For example, the above equation for pixel level parallax for a given pixel may be reset to nx (x-x 0) +ny (y-y 0) +nz (z-z 0) =0, where x and y reflect the position of the given pixel and z reflect the pixel level parallax for that pixel, (x 0, y0, z 0) reflect sample points on a three-dimensional plane, where x0=y0=0 and z0=d (x 0, y 0) =d (0, 0), such that the value of z0 is defined as block level parallax.
The locations of pixels within the current block are determined based on the results of the normal search. For example, the length of the normal vectors (nx, ny, nz) should be 1, where nx and ny are both greater than-1 and nz is greater than 0 and less than or equal to 1. The particular values of nx and ny may be searched within a predefined set of discrete values. For example, the predefined set of discrete values may be [ -0.7, -0.525, -0.35, -0.175, 0.0, 0.175, 0.35, 0.525]. Other values than the indicated values may also or instead be included in the predefined set of discrete values. A particular value of nz may be defined as nz=sqrt (l-nx-ny).
By using the respective values of nx and ny, the value of nz can be determined such that these values of normal vectors (nx, ny, nz) can be used to determine normal values for the locations of pixels within the current block. These normal values are used as plane normal candidates. Further, by inserting individual ones of these pixel positions into the above-described reset equation, the pixel-level parallax can be determined for the corresponding one of the plane normal candidates.
In 706, one of the plane normal candidates is selected based on the rate distortion values associated with the plurality of plane normal candidates. Selecting one of the plane normal candidates includes: a rate distortion value resulting from predicting motion within the current block is determined for a plurality of plane normal candidates from among the plane normal candidates using a respective one of the pixel level disparities. The rate-distortion value refers to a ratio that balances the degree of distortion (e.g., loss of video quality) with the rate (e.g., number of bits) used to code the block or other video component. After determining the rate distortion values for the plane normal candidates, the plane normal candidates associated with the pixel-level disparities that result in the smallest of the rate distortion values are selected.
For example, the rate distortion value may be defined as RD = d + λ x (dr + nr), where RD is the rate distortion value determined for a given plane normal candidate, d is the distortion that measures the prediction error, which is the sum of absolute pixel disparities between a warped video block and the current block, λ is the lagrangian multiplier, dr is the rate required to store disparities and is based on the block-level disparity rate, and nr is the normal rate, which represents the rate required to form a strong normal and is based on the values of nx and ny used to determine the given plane normal candidate. To reduce overhead, instead of storing actual values of nx and ny, an index of values of nx and ny is stored.
At 708, a warp parameter for predicting motion within the current block is determined using the selected plane normal candidate. The warp parameters are determined based on normals (nx, ny, nz) associated with the selected plane normal position candidates and pixel level disparities. For example, the warp parameter may be a warp parameter of a portion of the operation previously determined to be used to identify the plane normal candidate. For example, determining the warp parameters may include: warp parameters are selected from a list of warp parameters. Alternatively, the warp parameter may be determined based on the result of the operation for identifying the plane normal candidates and the result of the operation for selecting one of the plane normal candidates.
The warp parameter is used to describe the motion of the pixels of the current block. For example, warp parameters may be used to indicate translation, rotation, scaling, aspect ratio changes, shearing, or similar movements of the pixels, or combinations thereof. However, in another example, the warp parameter may only be used to indicate only the translation of these pixels. These warp parameters may be parameters of an affine motion model.
In 710, the current block is encoded according to the warp parameter. Encoding the current block according to the warp parameter includes: a prediction block is generated by warping a current pixel within a current block according to a motion of a corresponding pixel within a corresponding block of a first frame. The motion of the corresponding pixel is determined based on the reference frame of the first frame. The current block is then encoded using the prediction block.
In some implementations, the technique 700 includes: one or more syntax elements are encoded to the bitstream to which the current block is encoded, the one or more syntax elements indicating at least one of block-level disparity or warp parameters. For example, the plurality of bits may be included in a frame header of a first frame within the bitstream. The plurality of bits may indicate block level disparity, warp parameters, or both. The plurality of bits may later be decoded from the frame header, such as by hardware or software for decoding the bitstream and outputting an encoded stereoscopic video sequence including the encoded current block for display. Alternatively, a plurality of bits may be included in the bitstream of the current block of the first frame to indicate one or both of the block level disparity or warp parameters thereof. A plurality of such bits may later be decoded from the bitstream to decode the encoded current block.
In some implementations, the technique 700 includes: at least some of the first frames are partitioned based on the rate distortion values determined for the plane normal candidates. For example, a depth-first partition search may be performed for the current block. Block level disparities may be obtained at each partition block level, which is used to perform a normal search on a three-dimensional linear plane. Then, partitioning is performed based on the rate distortion value determined as a result of the normal search and based on the block-level disparity. More accurate disparity approximations and larger video blocks (e.g., with larger partition sizes within the first frame) are generated by partitioning based on the rate distortion values and normals (nx, ny, nz). Syntax elements indicating the partition may be encoded to the bitstream to which the current block is encoded.
In some implementations, the warp parameters used to encode the current block at 710 may be parameters of a non-affine motion model. For example, the warp parameters may be parameters of homographic motion models, similar motion models, translational motion models, and the like. Thus, the plurality of parameters included in the warp parameters may be different based on the particular parameterized motion model used to encode the current block.
The technique 700 shown in fig. 7 and described with respect to fig. 7 reflects a technique for encoding video blocks of a stereoscopic video sequence. However, in some implementations, the technique 700 may be performed to encode video blocks of a non-stereoscopic video sequence. In an example, technique 700 may be performed to predict affine motion of a current block. Thus, affine motion models support translation, rotation, scaling, aspect ratio variation, and shearing. For example, a rotation-only scaling affine motion model for mimicking scaling and rotation motion may use 4 parameters to project pixels of the current block to the warp reference region. Thus, these 4 parameters defining the affine motion model may be used to project pixels of the current block to warped reference regions in a second frame, a reference frame used to encode one or more blocks of the first frame, and so on.
Affine projection between two spaces is defined as u (x, y) =a x+b x+c and v (x, y) =b x+a x+y+f, where (u (x, y), v (x, y)) is the motion vector MV at the pixel at (x, y) within the current block; and wherein a, b, c and f are unknown parameters of the affine motion model. A given pixel of the current block may be represented in a local coordinate system, where the origin is the center of the current block (e.g., (0, 0)). Block-level motion estimation may be performed for respective blocks of a video frame to determine block-level motion vectors for the blocks. These block-level motion vectors can then be used to represent the motion vector MV of the center pixel of the current block.
The 4 parametric affine projections described above can be reset to-a (x-x 0) -b (y-y 0) + (u-u 0) =0 and b (x-x 0) -a (y-y 0) + (v-v 0) =0, where (u, v) is used to represent the motion vector instead of the simple signed functional form (u (x, y), v (x, y)). Dividing the reset equations by h=sqrt (a+b+1) yields the equations-a/h (x-x 0) -b/h (y-y 0) +l/h (u-u 0) =0 and b/h (x-x 0) -a/h (y-y 0) +l/h (v-v 0) =0. Variables can then be defined and substituted into these reset equations. For example, a may represent a value of-a/h, B may represent a value of-B/h, and C may represent a value of-1/h. Thus, the reset equation can be rewritten as a (x-x 0) +b (y-y 0) +c (u-u 0) =0 and-B (x-x 0) +a (y-y 0) +c (v-v 0) =0.
Thus, 4 affine parameters a, b, C, f can be estimated by determining the values of A, B and C. (A, B, C) is a normal vector, wherein a+b+b+c=1. A. The values of B and C are limited to-1 < A < 1, -1 < B < 1 and 0 < C.ltoreq.1. Candidate values for A, B and C may be determined, such as by performing the operations described above in 704. Then, the motion vectors at the corresponding pixels in the current block are calculated, and one such motion vector is selected to determine the warp parameters of the current block, e.g., as described above in 706 and 708. These warp parameters are then used to warp the reference frame to the current block (e.g., the second frame or a reference block within the reference frame is used to encode one or more blocks of the first frame) to encode the current block.
Referring next to fig. 8, a flow chart of an example of a technique 800 for decoding an encoded block using a parameterized motion model is shown. The technique 800 may be performed to decode encoded blocks of an encoded stereoscopic video sequence, for example, where a first encoded frame and a second encoded frame of the encoded stereoscopic video sequence represent left and right views of the stereoscopic video sequence. Alternatively, the technique 800 may be performed to decode encoded blocks that encode a non-stereoscopic video sequence, e.g., where the first encoded frame and the second encoded frame are consecutive frames of the video sequence.
In 802, a bitstream representing an encoded stereoscopic video sequence is received. The encoded video sequence includes a first encoded frame and a second encoded frame. For purposes of describing technique 800, it may be considered that the second encoded frame has been decoded such that it serves as a reference frame for decoding one or more encoded blocks of the first encoded frame. The first encoded frame includes an encoded block to be decoded. The bitstream may be received from an encoder (e.g., encoder 400 shown in fig. 4), a device implementing the encoder (e.g., transmitting station 102 shown in fig. 1), and so on.
At 804, warp parameters are determined from the bitstream. The bitstream includes syntax elements indicating the encoded video sequence. For example, the syntax element may represent data indicating the encoded blocks and encoded frames to be output for display as an output video stream. In another example, the syntax element may include other data (such as warp parameters) for encoding the encoded blocks and encoded frames and may also be used to decode the encoded blocks and encoded frames. Thus, determining the warp parameters may include: one or more syntax elements indicating warp parameters are decoded from the bitstream.
Warp parameters are determined from a bitstream used to predict motion within a coded block to be decoded using selected plane normal candidates. The selected plane normal candidate represents the current pixel of the encoded block. The selected plane normal candidates are selected, such as during an operation for encoding encoded blocks, based on block-level disparities between a first encoded frame and a second encoded frame of the encoded video sequence. The warp parameter is a parameter of an affine plane that includes the current pixel and a corresponding pixel of the current pixel within the encoded block of the second encoded frame. Other information (e.g., block level disparity or partition information) may also be determined by decoding syntax elements from the bitstream.
At 806, the encoded block is decoded according to the warp parameter. Decoding the encoded block according to the warp parameter may include: the prediction block is generated by warping the reference block of the second frame. The coded block is then decoded using the prediction block.
In some implementations, the warp parameters used to decode the encoded block at 806 may be parameters of a non-affine motion model. For example, the parameterized motion model reflected by the warp parameters may be a homography motion model, a similarity motion model, a translational motion model, and so on. Thus, the plurality of parameters included in the warp parameters may be different based on the particular parameterized motion model used to decode the encoded block.
The above-described encoding and decoding aspects illustrate some examples of encoding and decoding techniques. However, it should be understood that encoding and decoding, as those terms are used in the claims, may refer to compression, decompression, transformation, or any other processing or change of data.
The word "example" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" is intended to specifically present concepts. As used in this application, the term "or" is intended to mean including "or" rather than not including "or". That is, unless specified otherwise or the context clearly indicates otherwise, the statement "X includes A or B" is intended to mean any of its natural inclusive permutations. That is, if X includes A, X includes B, or X includes a and B, then "X includes a or B" is satisfied under any of the foregoing examples. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or the context clearly dictates otherwise. Furthermore, throughout this disclosure, the use of the term "an embodiment" or the term "one embodiment" is not intended to refer to the same example or embodiment unless so described.
Embodiments of the sending station 102 and/or the receiving station 106 (and algorithms, methods, instructions, etc., stored thereon and/or executed by the same, including by the encoder 400 and decoder 500) may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, a microcontroller, a server, a microprocessor, a digital signal processor, or any other suitable circuit. In the claims, the term "processor" should be understood to encompass any of the foregoing hardware, either alone or in combination. The terms "signal" and "data" are used interchangeably. Further, portions of the sending station 102 and the receiving station 106 are not necessarily implemented in the same manner.
Further, in one aspect, for example, the sending station 102 or the receiving station 106 may be implemented using a general-purpose computer or general-purpose processor with a computer program that, when executed, performs any of the corresponding methods, algorithms, and/or instructions described herein. Additionally or alternatively, for example, a special purpose computer/processor may be utilized that may contain dedicated hardware for carrying out any of the respective methods, algorithms, and/or instructions described herein.
The sending station 102 and the receiver 106 may be implemented, for example, on a computer in a video conferencing system. Alternatively, the sending station 102 may be implemented on a server and the receiving station 106 may be implemented on a device separate from the server (such as a handheld communication device). In this case, the transmitting station 102 may encode the content into an encoded video signal using the encoder 400 and transmit the encoded video signal to the communication apparatus. Instead, the communication device may then decode the encoded video signal using decoder 500. Alternatively, the communication device may decode content stored locally on the communication device (e.g., content not transmitted by the sending station 102). Other suitable transmit and receive implementations are also available. For example, the receiving station 106 may be a generally stationary personal computer rather than a portable communication device, and/or the device including the encoder 400 may also include the decoder 500.
Further, all or part of the embodiments of the present disclosure may take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium. A computer-usable or computer-readable medium may be any apparatus that can, for example, tangibly contain, store, communicate, or transport a program for use by or in connection with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic or semiconductor device. Other suitable media are also useful.
The foregoing examples, embodiments and aspects have been described in order to facilitate an understanding of the present disclosure, and are not limiting of the present disclosure. On the contrary, the disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent arrangements as is permitted under the law.
Claims (19)
1. A method for encoding blocks of frames of a stereoscopic video sequence, the method comprising:
determining a block-level disparity between a first frame of the stereoscopic video sequence and a second frame of the stereoscopic video sequence;
identifying a plane normal candidate within a current block of the first frame based on the block-level disparity, wherein identifying the plane normal candidate within the current block of the first frame based on the block-level disparity comprises:
performing a normal search for pixels of the current block based on the locations of sample pixels of the current block and the block-level disparity, wherein performing the normal search comprises: determining a value of a normal vector (nx, ny, nz) being a normal vector for simulating a three-dimensional linear plane of one-dimensional parallax motion associated with the block-level parallax, wherein the length of the normal vector (nx, ny, nz) is equal to 1, and the value of the normal vector (nx, ny, nz) is used to determine a normal value of a position of a pixel within the current block, the normal value being used as the plane normal candidate;
Identifying a location of a respective one of the pixels within the current block based on a result of the normal search; and
determining a pixel-level disparity based on the location of the respective one of the pixels within the current block;
selecting one of the plane normal candidates based on rate distortion values associated with a plurality of the plane normal candidates;
determining a warp parameter for predicting motion within the current block using the selected plane normal candidate; and
the current block is encoded according to the warp parameter.
2. The method of claim 1, wherein determining the block-level disparity between the first frame of the stereoscopic video sequence and the second frame of the stereoscopic video sequence comprises:
a disparity search is performed for the block of the first frame and the block of the second frame.
3. The method of claim 2, wherein the disparity search is a one-dimensional diamond search.
4. The method of claim 1, wherein selecting the one of the surface normal candidates based on the rate-distortion values associated with the plurality of the surface normal candidates comprises:
Determining, for a plurality of the plane normal candidates, the rate distortion value due to predicting the motion within the current block using a respective one of the pixel-level disparities; and
the one of the plane normal candidates associated with the one of the pixel-level disparities that results in the lowest of the rate-distortion values is selected.
5. The method of claim 1, wherein the warp parameter is a parameter of an affine plane that includes a current pixel corresponding to the selected plane normal candidate and a corresponding pixel of the current pixel within a block of the first frame.
6. The method of claim 5, wherein encoding the current block according to the warp parameter comprises:
generating a prediction block by warping the current pixel according to a motion of the corresponding pixel, the motion of the corresponding pixel being determined based on a reference frame of the second frame; and
the current block is encoded using the prediction block.
7. The method of any one of claims 1 to 6, further comprising:
One or more syntax elements are encoded to the bitstream to which the current block is encoded, the one or more syntax elements indicating at least one of the block-level disparity, the selected plane normal candidate, or the warp parameter.
8. An apparatus for encoding blocks of frames of a stereoscopic video sequence, the apparatus comprising:
a processor configured to execute instructions stored in a non-transitory storage medium to:
determining a block-level disparity between a first frame of the stereoscopic video sequence and a second frame of the stereoscopic video sequence;
identifying a plane normal candidate within a current block of the first frame based on the block-level disparity, wherein identifying the plane normal candidate within the current block of the first frame based on the block-level disparity comprises:
performing a normal search for pixels of the current block based on the locations of sample pixels of the current block and the block-level disparity, wherein performing the normal search comprises: determining a value of a normal vector (nx, ny, nz) being a normal vector for simulating a three-dimensional linear plane of one-dimensional parallax motion associated with the block-level parallax, wherein the length of the normal vector (nx, ny, nz) is equal to 1, and the value of the normal vector (nx, ny, nz) is used to determine a normal value of a position of a pixel within the current block, the normal value being used as the plane normal candidate;
Identifying a location of a respective one of the pixels within the current block based on a result of the normal search; and
determining a pixel-level disparity based on the location of the respective one of the pixels within the current block;
selecting one of the plane normal candidates based on rate distortion values associated with a plurality of the plane normal candidates;
determining a warp parameter for predicting motion within the current block using the selected plane normal candidate; and
the current block is encoded according to the warp parameter.
9. The device of claim 8, wherein the instructions to determine the block-level disparity between the first frame of the stereoscopic video sequence and the second frame of the stereoscopic video sequence comprise instructions to:
a disparity search is performed for the block of the first frame and the block of the second frame.
10. The apparatus of claim 9, wherein the disparity search is a one-dimensional diamond search.
11. The apparatus of claim 8, wherein the instructions to select the one of the surface normal candidates based on the rate-distortion values associated with the plurality of the surface normal candidates comprise instructions to:
Determining, for a plurality of the plane normal candidates, the rate distortion value due to predicting the motion within the current block using a respective one of the pixel-level disparities; and
the one of the plane normal candidates associated with the one of the pixel-level disparities that results in the lowest of the rate-distortion values is selected.
12. The apparatus of claim 8, wherein the warp parameter is a parameter of an affine plane that includes a current pixel corresponding to the selected plane normal candidate and a corresponding pixel of the current pixel within a block of the first frame.
13. The apparatus of claim 12, wherein the instructions to encode the current block according to the warp parameter comprise instructions to:
generating a prediction block by warping the current pixel according to a motion of the corresponding pixel, the motion of the corresponding pixel being determined based on a reference frame of the second frame; and
the current block is encoded using the prediction block.
14. The apparatus of any of claims 8 to 13, wherein the instructions comprise instructions to:
One or more syntax elements are encoded to the bitstream to which the current block is encoded, the one or more syntax elements indicating the block-level disparity, the selected plane normal candidate, or the warp parameter, or any combination of the block-level disparity, the selected plane normal candidate, and the warp parameter.
15. A method for decoding an encoded block of an encoded frame of an encoded stereoscopic video sequence encoded according to the method of any one of claims 1 to 7, the method comprising:
receiving a bitstream representing the encoded stereoscopic video sequence;
determining a warp parameter based on one or more syntax elements encoded to the bitstream using a selected plane normal candidate, the warp parameter being used to predict motion within an encoded block of a first encoded frame of the encoded stereoscopic video sequence, the selected plane normal candidate representing a current pixel of the encoded block and being selected based on block-level disparity between the first encoded frame and a second encoded frame of the encoded stereoscopic video sequence; and
the encoded block is decoded according to the warp parameter.
16. The method of claim 15, wherein determining the warp parameter using the selected plane normal candidate for predicting the motion within the encoded block of the first encoded frame of the encoded stereoscopic video sequence comprises:
One or more syntax elements indicating the warp parameter are decoded from the bitstream.
17. The method of claim 15 or 16, wherein the warp parameter is a parameter of an affine plane comprising the current pixel and corresponding pixels of the current pixel within a coded block of the second coded frame.
18. The method of claim 17, wherein decoding the encoded block according to the warp parameter comprises:
generating a prediction block by warping the current pixel according to a motion of the corresponding pixel, the motion of the corresponding pixel being determined based on a reference frame of the second encoded frame; and
the prediction block is used to decode the encoded block.
19. An apparatus for decoding an encoded block of an encoded frame of an encoded stereoscopic video sequence, the apparatus comprising:
a processor configured to execute instructions stored in a non-transitory storage medium to perform the method of any one of claims 15 to 18.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/727,894 | 2017-10-09 | ||
US15/727,894 US10645417B2 (en) | 2017-10-09 | 2017-10-09 | Video coding using parameterized motion model |
PCT/US2018/040797 WO2019074559A1 (en) | 2017-10-09 | 2018-07-03 | Video coding using parameterized motion model |
Publications (3)
Publication Number | Publication Date |
---|---|
CN110679151A CN110679151A (en) | 2020-01-10 |
CN110679151B CN110679151B (en) | 2023-06-06 |
CN110679151B9 true CN110679151B9 (en) | 2023-06-30 |
Family
ID=63036350
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880035742.4A Active CN110679151B9 (en) | 2017-10-09 | 2018-07-03 | Method and apparatus for video coding using parameterized motion models |
Country Status (4)
Country | Link |
---|---|
US (1) | US10645417B2 (en) |
EP (1) | EP3695607A1 (en) |
CN (1) | CN110679151B9 (en) |
WO (1) | WO2019074559A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3979649A4 (en) | 2019-06-14 | 2023-06-07 | Hyundai Motor Company | Method and device for coding and decoding video using inter-prediction |
WO2020251325A1 (en) * | 2019-06-14 | 2020-12-17 | 현대자동차주식회사 | Method and device for coding and decoding video using inter-prediction |
CN115134534B (en) * | 2022-09-02 | 2022-11-18 | 深圳前海鹏影数字软件运营有限公司 | Video uploading method, device, equipment and storage medium based on e-commerce platform |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102282838A (en) * | 2009-01-19 | 2011-12-14 | 夏普株式会社 | Methods and Systems for Enhanced Dynamic Range Images and Video from Multiple Exposures |
CN104521236A (en) * | 2012-07-27 | 2015-04-15 | 联发科技股份有限公司 | Method of constrain disparity vector derivation in 3D video coding |
CN105230022A (en) * | 2013-05-31 | 2016-01-06 | 高通股份有限公司 | Use based on the disparity vector of neighbor derive for 3D video coding and derivation disparity vector of passing through |
CN106134197A (en) * | 2013-12-27 | 2016-11-16 | 日本电信电话株式会社 | Method for video coding, video encoding/decoding method, video coding apparatus, video decoder, video coding program and video decoding program |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DE10105423C1 (en) * | 2001-01-31 | 2002-07-04 | Hertz Inst Heinrich | Correlation analysis method for image characteristics in corresponding real-time video images uses hybrid recursive technique for providing disparity vector field used as correlation vector field |
US8284237B2 (en) * | 2009-09-09 | 2012-10-09 | Nokia Corporation | Rendering multiview content in a 3D video system |
US8711204B2 (en) | 2009-11-11 | 2014-04-29 | Disney Enterprises, Inc. | Stereoscopic editing for video production, post-production and display adaptation |
US9066110B2 (en) * | 2011-03-08 | 2015-06-23 | Texas Instruments Incorporated | Parsing friendly and error resilient merge flag coding in video coding |
CN104025153B (en) | 2011-12-30 | 2017-09-15 | 英特尔公司 | It is thick to arrive thin multiple parallax candidate Stereo matchings |
EP2807827A4 (en) | 2012-01-25 | 2015-03-04 | Lumenco Llc | Conversion of a digital stereo image into multiple views with parallax for 3d viewing without glasses |
US9998726B2 (en) * | 2012-06-20 | 2018-06-12 | Nokia Technologies Oy | Apparatus, a method and a computer program for video coding and decoding |
US8867826B2 (en) | 2012-11-26 | 2014-10-21 | Mitusbishi Electric Research Laboratories, Inc. | Disparity estimation for misaligned stereo image pairs |
KR102378459B1 (en) * | 2014-06-30 | 2022-03-24 | 한국전자통신연구원 | Apparatus And Method For Eliminating Redundancy Of View Synthesis Prediction Candidate In Merge Mode |
-
2017
- 2017-10-09 US US15/727,894 patent/US10645417B2/en active Active
-
2018
- 2018-07-03 EP EP18746375.7A patent/EP3695607A1/en not_active Withdrawn
- 2018-07-03 WO PCT/US2018/040797 patent/WO2019074559A1/en unknown
- 2018-07-03 CN CN201880035742.4A patent/CN110679151B9/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102282838A (en) * | 2009-01-19 | 2011-12-14 | 夏普株式会社 | Methods and Systems for Enhanced Dynamic Range Images and Video from Multiple Exposures |
CN104521236A (en) * | 2012-07-27 | 2015-04-15 | 联发科技股份有限公司 | Method of constrain disparity vector derivation in 3D video coding |
CN105230022A (en) * | 2013-05-31 | 2016-01-06 | 高通股份有限公司 | Use based on the disparity vector of neighbor derive for 3D video coding and derivation disparity vector of passing through |
CN106134197A (en) * | 2013-12-27 | 2016-11-16 | 日本电信电话株式会社 | Method for video coding, video encoding/decoding method, video coding apparatus, video decoder, video coding program and video decoding program |
Non-Patent Citations (2)
Title |
---|
"HEVC-Compatible Extensions for Advanced Coding of 3D and Multiview Video";Anthony Vetro等;《2015 Data Compression Conference》;20150706;全文 * |
"立体视频深度图提取及深度序列编码技术研究";高凯;《中国博士学位论文全文数据库》;20130815;全文 * |
Also Published As
Publication number | Publication date |
---|---|
CN110679151A (en) | 2020-01-10 |
WO2019074559A1 (en) | 2019-04-18 |
US20190110075A1 (en) | 2019-04-11 |
US10645417B2 (en) | 2020-05-05 |
EP3695607A1 (en) | 2020-08-19 |
CN110679151B (en) | 2023-06-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20200366921A1 (en) | Constrained motion field estimation for hardware efficiency | |
US10555000B2 (en) | Multi-level compound prediction | |
CN100512431C (en) | Method and apparatus for encoding and decoding stereoscopic video | |
CN107027032B (en) | Method and device for partitioning motion vector of last frame | |
CN112005551B (en) | Video image prediction method and device | |
CN107205149B (en) | Motion vector reference selection via reference frame buffer tracking | |
US20240098298A1 (en) | Segmentation-based parameterized motion models | |
JP7279154B2 (en) | Motion vector prediction method and apparatus based on affine motion model | |
WO2015098948A1 (en) | Video coding method, video decoding method, video coding device, video decoding device, video coding program, and video decoding program | |
US20220007046A1 (en) | Inter Prediction Method and Related Apparatus | |
CN110741641B (en) | Method and apparatus for video compression | |
CN111801944B (en) | Video image encoder, decoder and corresponding motion information encoding method | |
US11343528B2 (en) | Compound prediction for video coding | |
CN110679151B (en) | Method and apparatus for video coding using parameterized motion models | |
CN110741638A (en) | Motion vector coding using residual block energy distribution | |
WO2019036080A1 (en) | Constrained motion field estimation for inter prediction | |
WO2015083742A1 (en) | Video encoding device and method, video decoding device and method, and program therefor | |
US20220337872A1 (en) | Point cloud data transmission device, point cloud data transmission method, point cloud data reception device, and point cloud data reception method | |
EP3926953A1 (en) | Inter-frame prediction method and related device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant | ||
CI03 | Correction of invention patent |
Correction item: ClaimsCorrect: Claims 1-19 submitted on March 17, 2023False: Claims 1-19 submitted on February 24, 2023Number: 23-01Page: ??Volume: 39 |
|
CI03 | Correction of invention patent |