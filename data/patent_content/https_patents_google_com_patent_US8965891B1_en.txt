US8965891B1 - Training scoring models optimized for highly-ranked results - Google Patents
Training scoring models optimized for highly-ranked results Download PDFInfo
- Publication number
- US8965891B1 US8965891B1 US14/083,043 US201314083043A US8965891B1 US 8965891 B1 US8965891 B1 US 8965891B1 US 201314083043 A US201314083043 A US 201314083043A US 8965891 B1 US8965891 B1 US 8965891B1
- Authority
- US
- United States
- Prior art keywords
- images
- image
- query
- selecting
- candidate
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G06K9/66—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2457—Query processing with adaptation to user needs
- G06F16/24578—Query processing with adaptation to user needs using ranking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
-
- G06K9/6267—
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/912—Applications of a database
- Y10S707/913—Multimedia
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10—TECHNICAL SUBJECTS COVERED BY FORMER USPC
- Y10S—TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y10S707/00—Data processing: database and file management or data structures
- Y10S707/912—Applications of a database
- Y10S707/913—Multimedia
- Y10S707/915—Image
Definitions
- This specification relates to ranking search results using scoring models.
- Internet search engines provide information about Internet accessible resources, e.g., Web pages, images, text documents, and multimedia content, that are responsive to a user's search query and present information about the resources in a manner intended to be useful to the user.
- Internet search engines return a set of search results in response to a user submitted query, e.g., as a ranked (i.e., ordered) list of results.
- the search results can be ranked in an order according to scores for the search results.
- the scores can be determined by applying a scoring model to each of the search results. Different scoring models assign different scores to the search results.
- Scoring models for scoring resources can be trained using machine learning techniques that train the model using positive and negative resources for the query.
- Conventional techniques for training scoring models focus on training scoring models to be good overall, for all positive and negative resources for the query.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of storing data identifying a plurality of training images for a query, wherein each of the training images is classified as being in a positive group of images for the query or a negative group of images for the query according to a respective query-specific preference measure for the image; selecting a first image from either the positive group of images or the negative group of images, and applying a scoring model to the first image to determine a score for the first image; selecting a plurality of candidate images from the other group of images; applying the scoring model to each of the candidate images to determine a respective score for each candidate image, and then selecting a second image from the candidate images, the second image having a highest score; and determining that the scores for the first image and the second image fail to satisfy a criterion, wherein the criterion requires that a result of the score of the image selected from the positive group of images minus the score of the image selected from the negative group of images exceeds a threshold, updating the scoring
- Selecting the first image can include selecting a plurality of first candidate images from either the positive group of images or the negative group of images; applying the scoring model to each of the first candidate images to determine a respective score for each first candidate image; and selecting as the first image an image having a highest score from among the first candidate images.
- the first image can be selected from the positive group of images and the plurality of candidate images can be selected from the negative group of images.
- the first image can be selected from the negative group of images and the plurality of candidate images can be selected from the positive group of images.
- the respective query-specific preference measure for each image can be derived from a number of times users select the image in response to being presented with a search result for the query that includes the image.
- the threshold can be greater than zero.
- the method can further include in response to receiving the query through a search interface, identifying a plurality of images responsive to the query; applying the scoring model to each of the plurality of images to determine a respective score for each image; and presenting images from the plurality of in the search interface, wherein the images are presented in an order according to the respective score for each image.
- the scoring models can be query specific, and the operations can further comprise updating and storing query-specific scoring models for a plurality of queries.
- the method can further include repeating selecting the first image, selecting the second image, and updating the scoring model until a training condition is satisfied.
- the candidate images can include one or more previously-considered images each having a score that satisfies a threshold.
- Scoring models can be learned for individual queries. Scoring models can be optimized so that they provide the most accurate scores for the highest-ranked queries. Scoring models can be trained using larger training sets. Scoring models can be trained on small training sets and applied to larger training sets.
- FIG. 1 illustrates an example system that uses scoring models trained using methods described in this specification.
- FIG. 2 illustrates an example method for training a query-specific scoring model.
- FIG. 3 illustrates an example architecture of a training engine.
- FIG. 4 illustrates an example method for using a query-specific scoring model to score images responsive to a query.
- FIG. 1 illustrates an example system 100 that uses scoring models trained using methods described in this specification.
- the system includes a search system 102 and a user device 106 .
- the search system 102 receives queries 104 from a user device 106 and presents search results 108 to the user device, for example, through a network.
- the user device 106 submits queries 104 to the search system 102 , receives search results 108 from the search system 102 , and presents the search results to a user.
- the search system 102 includes a search engine 110 and a scoring model engine 112 .
- the search engine 110 receives the query 104 from the user device 106 .
- the search engine 110 identifies images indexed in its image index 114 that are responsive to the query 104 . Images include, for example, still images, video, and other visual content.
- the image index 114 associates an identifier for each image with data for the image.
- the data for the image includes, for example, keywords used to determine when the image is relevant to queries, details on where to obtain the image, and optionally image features extracted from the image. Image features are described in more detail below, with reference to FIG.
- the search engine 110 identifies a number of images that are responsive to the query 104 . This can be done using conventional techniques, for example, based on how well text associated with the images matches the query.
- the search engine 110 then ranks the responsive images using a ranking engine 116 that applies a query-specific scoring model 118 to the features for each image.
- the ranking engine 116 can receive the features for the responsive images 120 from the image index 114 . Alternatively, the ranking engine 116 can receive the responsive images themselves and extract the features for each image.
- the ranking engine 116 can receive the responsive images, for example, from an image cache maintained by the search system 102 , or from the original source of the image.
- the scoring model 118 is specific to the query 104 , and specifies a weight for each of a number of image features.
- the scoring model is implemented as a passive-aggressive model for image retrieval (“PAMIR”).
- PAMIR passive-aggressive model for image retrieval
- a PAMIR is a linear model that can be applied to non-linear decisions.
- a PAMIR is implemented as a vector of weights, with each weight corresponding to a distinct image feature. The model is trained by iteratively computing dot products of the vector of weights representing the model and feature vectors representing the images, and appropriately updating the weights after each iteration.
- the ranking engine 118 receives the query-specific scoring model 122 from a store of query-specific data 124 maintained by the scoring model engine 112 .
- the query-specific data 124 includes, for each of a number of queries, a scoring model for the query, and positive and negative images for the query.
- the query-specific data 124 is used and updated by a training engine 126 .
- the query-specific data 124 and training engine 126 are described in more detail below with reference to FIGS. 2 and 3 .
- the ranking engine 118 applies the model to the image features, for example, by taking the dot product of a vector of the image features and a vector of the weights for the image features.
- the user device 104 submits search a query 106 to the search system 104 , receives the search results page 108 with the images responsive to the query, and renders the search results page 108 for presentation to a user.
- a user clicks e.g., selects with a mouse or other input device
- an image in the search results page 108 the user device 106 sends data about the selection to the search system 102 .
- the scoring model engine 112 uses this data to identify positive and negative images for the query, as will be described in more detail below with reference to FIG. 2 .
- FIG. 2 illustrates an example method 200 for training a query-specific scoring model.
- the system can be, for example, the training engine 126 described above with reference to FIG. 1 .
- the system optimizes the scoring model during training, so that the scoring model is most accurate at scoring the most highly ranked images.
- the system will be described as training a single query-specific scoring model. However, the system can train multiple query-specific scoring models, either in sequence or in parallel.
- the system stores data identifying a plurality of training images for a query, where each training image is classified as being in a positive group of images or a negative group of images for the query ( 202 ).
- the training images are images that are responsive to the query.
- the images are divided into a positive group of images and a negative group of images according to a query-specific user preference measure for each image. Images that have a query-specific preference measure that exceeds a first threshold are classified as positive training images and images that have a query-specific preference measure that is below a second threshold are classified as negative training images.
- the first and second threshold can be the same, or different, thresholds.
- the first and second thresholds are determined, for example, empirically.
- the query-specific preference measure is derived from user behavior data, and estimates whether users find the images relevant to the query.
- An example query-specific preference measure is a click-through-rate for the image for the query.
- the click-through-rate is the number of times users selected the image when it was presented as a search result for the query divided by the total number of times the image was presented as a search result for the query.
- Each training image has one or more associated features that represent visual characteristics of the image.
- Example features include color, texture, edges, and other characteristics of the image.
- the features can be stored in the index, as described above with reference to FIG. 1 , can be stored in a separate feature cache, or can be extracted from the images as needed. The features can be extracted, for example, when a search system crawls web pages to extract content for its index, or at a later time.
- Example processes for extracting features include scale-invariant feature transform, edge detection, and corner detection. Example features are described in more detail below.
- image can mean the image itself, or features extracted from the image, depending on whether the system pre-extracts the features from the images, or extracts them as needed to apply the scoring model.
- the system initializes a scoring model for the query ( 204 ).
- the system initializes the scoring model by assigning default values to the weights in the scoring model.
- the system can determine appropriate default values, for example, according to values of weights for a model that was trained for a similar query, or based on the values of weights for a model that was trained using similar training images. Alternatively, the system can assign each of the weights a default value of zero.
- the system then begins the training process. Ideally, the system would rank all of the images according to their scores, and train the model by comparing the highest ranked positive images with the highest ranked negative images. However, ranking the images is very time consuming. The system approximates the full ranking process by selecting multiple images from either the positive group of images or the negative group of images, and then selecting the highest scoring image. Because the system selects multiple images, and then selects the highest-ranked image, the probability of any given image being used in the comparison changes from a uniform probability to a probability that is roughly proportional to the image's rank raised to the number of candidate images minus one.
- the system repeatedly iterates through the steps of the training process described below until a training condition is satisfied.
- the training condition is described in more detail below.
- the system selects a first image from either the positive group of images or the negative group of images, and applies a scoring model for the query to the first image to determine a score for the first image ( 206 ).
- the system applies the scoring model, for example, as described above with reference to FIG. 1 .
- the system selects candidate images from the other group of images ( 208 ). For example, when the system selected the first image from the positive group of images, the system selects the candidate images from the negative group of images. Similarly, when the system selected the first image from the negative group of images, the system selects the candidate images from the positive group of images.
- the system selects the candidate images, for example, by arbitrarily selecting images. For example, the system can randomly select candidate images according to a uniform distribution.
- the system can select any number of candidate images. The number is generally greater than one and less than the total number of images in the other group of images. The number of images is chosen to balance the number of images that the system can reasonably handle without a significant decrease in speed with the benefits that come from the additional emphasis put on higher-ranked images when more images are chosen. The number can be, for example, three, four, or five.
- the system applies the scoring model to each of the candidate images, and then selects the candidate image having the highest score ( 210 ).
- the system applies the scoring model to each of the candidate images, for example, as described above with reference to FIG. 1 . Selecting the candidate image with the highest score allows the system to compare the best image from the group of candidate images to the first image. This results in a scoring model that is optimized to give the most accurate scores to the most highly ranked images.
- the system stores data identifying one or more previously-considered images from the same group of images that the candidate images are selected from.
- Previously-considered images are images that were considered in a prior iteration of the training process for the query.
- Data identifying a previously-considered image is stored when the image has a particularly high score for the query.
- the system can store data identifying one or more previously-considered images having a score that exceeds a threshold.
- the candidate images can include both these previously-considered images and other images selected from the group of images. This introduces higher scoring images into the group of candidate images, and therefore can further increase the likelihood that better-ranked images are selected.
- the system then subtracts the score of the image selected from the negative group of images from the score of the image selected from the positive group of images, and compares the difference to a threshold ( 212 ).
- the threshold can be empirically chosen. In some implementations, the threshold is greater than zero.
- the scoring model is updated to increase the difference between the positive image score and the negative image score by adjusting the vector of weights.
- the system updates the scoring model by modifying the vector of weights of the scoring model such that each weight is proportional to a minimum difference between the feature vectors for the images.
- the vector of weights can also be adjusted such that each weight is proportional to a maximum of a specified aggressiveness constant and the distance between the positive and negative scores divided by and an Euclidean norm of a vector representing the difference between the feature vectors for the images.
- the specified aggressiveness constant is a constant that controls a tradeoff between minimizing expected error of future iterations and maintaining low vector weights.
- ⁇ i min ⁇ ⁇ c , l ⁇ ( w i - 1 ; ( p i + , p i - ) ) ⁇ p i + , p i - ⁇ 2 ⁇ ,
- c is a parameter chosen to control the trade-off between maximizing the margin between positive and negative images and satisfying the other training constraints
- q i is a representation of the query on which the model is being trained
- p i+ is a representation of the current positive image being considered
- p i ⁇ is a representation of the current negative image being considered
- l(w, p+, p ⁇ ) is determined as described below.
- the system repeats the steps of the training process until a training condition is satisfied.
- the training condition can require that the average number of adjustments of the vector of weights over a given number of iterations be below a threshold.
- the training condition can specify that a trained model be a model that has been adjusted following no more than 10 of the last 10,000 iterations. In this example, when a model having a vector of weights that has only been adjusted 9 out of the last 10,000 iterations, the training condition is satisfied.
- the training condition can specify that the average value adjustment of the vector of weights be less than a threshold amount. For example, the difference between the weights of the vector prior to an iteration and following the iteration can be computed. The average difference over a given number of previous iterations can be computed. When the average difference is less than the threshold amount, the training condition is satisfied.
- the system selects the first image much as the system selects the candidate image from the other group of images, e.g., by selecting multiple images from the other group of images, applying the scoring model to each of the images to obtain a score for the image, and then selecting the image with the highest score as the first image. Selecting both the positive and the negative image in this way can further result in a model that determines more accurate scores for more highly ranked images.
- the features for an image include features extracted from the image at different scales. For example, a first set of features may be extracted from the full scale of the image as it is presented to a user device, while other sets of features may be extracted at 75%, 50%, and 25% of the full scale of the image.
- the features are indexed according to feature categories.
- Feature categories are a predefined set of reference features with which individual features that are sufficiently similar can be categorized, even though the individual features may vary slightly. For example, similar features can be categorized in the same feature category even though slight variations exist in the illumination or color of the images from which the features were extracted.
- the features of images can be indexed by feature categories based on a frequency with which the features occur in various scaled versions of the image.
- the features can be indexed by feature category without reference to spatial relation of the features in the image. Indexing the features without a reference to the spatial relation of the features in the image results in a “bag of features” for the image.
- the bag of features for the image includes a frequency of features that satisfy each feature category without considering where each of the features occurs in the image. Therefore, each image is characterized by a frequency distribution of the feature categories in which the features are categorized. Indexing features of images as “a bag of features” facilitates characterization of an image without regard to the scale of the image.
- the feature categories can be defined based on a sparse vector of discrete features.
- the discrete features are learned, for example, based on k-means clustering of features that are extracted from a set of training images. For example, edge histograms, e.g., texture features, and color histograms, e.g., color features, can be generated for a top 10,000 most selected images.
- the features can be extracted from each of a plurality of overlapping square portions of each image.
- the edge histograms are based on differences in intensities of circular portions of the image that are centered at each pixel, referred to as a center pixel, of the square portions.
- an intensity of the center pixel is compared to interpolated intensities of pixels that are located at eight equally spaced locations on an outer edge of the circular portion.
- Each of the pixels is determined to have a lower or greater intensity than the center pixel, and is assigned a respective 0 or 1 binary value based on the determination.
- the eight binary values provide an eight-bit sequence that represents the texture of the circular portion.
- This eight-bit binary sequence can be represented as a 256-bin histogram.
- the bins corresponding to non-uniform sequences i.e., sequences having more than two 1 to 0 or 0 to 1 transitions, can be merged, thereby allowing each of the 256 bin histograms to be mapped to 59 bin histograms, and resulting in a sparse representation of the texture content feature value.
- the color histograms for each pixel are generated based on k-means clustering.
- a color codebook is learned from red-green-blue pixels of training images, and a color histogram for a square portion of the image is determined by mapping each pixel in the square portion to the color to which the pixel most closely corresponds.
- the edge histograms and color histograms for an image can be represented as a sparse vector of features by mapping each vector of image features to a discrete index, i.e., feature category, referred to as a visual term.
- a visual term i.e., feature category
- concatenated vectors are generated by concatenating each pair of vectors that represent the edge and color histograms, respectively.
- each concatenated vector for an image is mapped to a corresponding visual term.
- the visual term to which a concatenated vector corresponds is determined, for example, based on relationships that have been identified between the concatenated vectors visual terms.
- Relationships between the concatenated vectors and the visual terms are identified, for example, by using a machine learning algorithm to learn the relationships from the concatenated vectors of content feature values for the training images based on k-means clustering.
- the weight of each visual term can be determined, for example, based on the following relationship:
- p i is the weight of the visual term i in an image p
- f i is the frequency of the visual term i in an image p
- f j is the frequency of the visual term j in the image p;
- idf i is the inverse document frequency of the visual term i, defined as ⁇ log(r i ), where r i is a fraction of training images containing at least one occurrence of visual term i;
- idf j is the inverse document frequency of visual term j, defined as ⁇ log(r j ), where r j is a fraction of training images containing at least one occurrence of visual term j;
- C is the number of visual terms.
- the above relationship provides visual term weights that normalize the sparse vector that results from mapping each of the concatenated vectors for an image to visual terms.
- the normalization emphasizes concatenated vector values that correspond to rare visual terms, i.e., visual terms that appear in less than a threshold portion of all images.
- the normalization also de-emphasizes concatenated vector values that correspond to common visual terms, i.e., visual terms that appear in more than a threshold portion of all images.
- FIG. 3 illustrates an example architecture of the training engine 126 .
- the training engine 126 trains query-specific scoring models, for example, using the method described above with reference to FIG. 2 .
- the specific training engine illustrated in FIG. 3 selects the first image from the group of positive images 302 and the candidate images from the group of negative images 304 ; however, other training engines can select the candidate images from the positive images 302 , or select candidate images from both the positive and negative images, as described above with reference to FIG. 2 .
- the positive 302 and negative 304 images can be the images themselves or features extracted from the images.
- the training engine 126 includes a positive image selector/scorer 306 , a negative image selector/scorer 308 , a score comparator 310 , and a scoring model modifier 312 .
- the training engine 126 uses these components to train a query-specific scoring model 314 .
- the positive image selector/scorer 306 selects a positive image from the positive training images 302 for the query, and applies the scoring model 314 to the features of the positive image to generate a score 316 for the positive image, for example, as described above with reference to FIG. 2 .
- the negative image selector/scorer 308 includes a candidate engine 318 and a candidate selector/scorer 320 .
- the candidate engine 318 selects candidate images 322 from the negative images 304 for the query, for example, as described above with reference to FIG. 2 .
- the candidate selector/scorer 320 scores each image and selects the image having the highest score, for example, as described above with reference to FIG. 2 .
- the score comparator 310 receives the score for the positive image 316 and the score for the negative image 324 , calculates the difference between the score for the positive and the negative image, and determines whether the difference exceeds a threshold, for example, as described above with reference to FIG. 2 . If so, the score comparator 310 instructs the scoring model modifier 312 to update the scoring model.
- the scoring model modifier 312 updates the scoring model based on the features of the negative image 326 and the features of the positive image 328 , for example, as described above with reference to FIG. 2 .
- w i-1 is the weight vector from the previous iteration
- ⁇ i and v i are determined according to the below equations.
- ⁇ i min ⁇ ⁇ c , l ⁇ ( w i - 1 ; ( q i , p i + , p i - ) ) ⁇ v i ⁇ 2 ⁇ ,
- c is a parameter chosen to control the trade-off between maximizing the margin between positive and negative images and satisfying the other training constraints
- q i is a representation of the query on which the model is being trained
- p i+ is a representation of the current positive image being considered
- p i ⁇ is a representation of the current negative image being considered
- l(w, q, p+, p ⁇ ) and v i are determined as described below.
- ⁇ (q,p) is the vector (q 1 p, . . . q T p), where the terms of the query are (q 1 . . . q T ).
- FIG. 4 illustrates an example method 400 for using a query-specific scoring model to score images responsive to a query.
- the system can be, for example, the search system 102 described above with reference to FIG. 1 .
- the system stores trained query-specific scoring models for queries ( 402 ), for example, as described above with reference to FIG. 1 .
- the system receives a query and identifies images responsive to the query ( 404 ), for example, as described above with reference to FIG. 1 .
- the system obtains the query-specific scoring model for the query and applies the query-specific scoring model to each image responsive to the query to obtain a query-specific score for each image ( 406 ), for example, as described above with reference to FIG. 1 .
- the system presents the images responsive to the query in an order according to the query-specific score for each image ( 408 ), for example, as described above with reference to FIG. 1 .
- a non query-specific scoring model While the above describes using a query-specific scoring model to score images responsive to a query, a non query-specific scoring model.
- the same non query-specific scoring model is used for multiple queries; to determine the score for an image and a query, the model processes the image and the terms in the query.
- the non query-specific scoring model can be trained as described above.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a computer storage medium for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on a propagated signal that is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA, or field programmable gate array, or an ASIC, or application-specific integrated circuit.
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program also known as a program, software, software application, script, or code
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub-programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA, or field programmable gate array, or an ASIC, or application-specific integrated circuit.
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
score=W·X,
where W is a vector storing the weights for the query-
w i =w i-1+τi(p i+ −p i−),
l(w,p + ,p −)=max(0,1−w·(p + +p −)).
w i =w i-1+τi v i,
l(w,q,p + ,p −)=max(0,1−w·γ(q,p +)+w·γ(q,p −),
v i=γ(q i ,p i+)−γ(q i ,p i−),
Claims (18)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/083,043 US8965891B1 (en) | 2009-11-23 | 2013-11-18 | Training scoring models optimized for highly-ranked results |
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/624,001 US8131786B1 (en) | 2009-11-23 | 2009-11-23 | Training scoring models optimized for highly-ranked results |
US13/342,532 US8429212B1 (en) | 2009-11-23 | 2012-01-03 | Training scoring models optimized for highly-ranked results |
US13/616,108 US8589457B1 (en) | 2009-11-23 | 2012-09-14 | Training scoring models optimized for highly-ranked results |
US14/083,043 US8965891B1 (en) | 2009-11-23 | 2013-11-18 | Training scoring models optimized for highly-ranked results |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/616,108 Continuation US8589457B1 (en) | 2009-11-23 | 2012-09-14 | Training scoring models optimized for highly-ranked results |
Publications (1)
Publication Number | Publication Date |
---|---|
US8965891B1 true US8965891B1 (en) | 2015-02-24 |
Family
ID=45758023
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/624,001 Active 2030-07-28 US8131786B1 (en) | 2009-11-23 | 2009-11-23 | Training scoring models optimized for highly-ranked results |
US13/342,532 Expired - Fee Related US8429212B1 (en) | 2009-11-23 | 2012-01-03 | Training scoring models optimized for highly-ranked results |
US13/616,108 Expired - Fee Related US8589457B1 (en) | 2009-11-23 | 2012-09-14 | Training scoring models optimized for highly-ranked results |
US14/083,043 Active US8965891B1 (en) | 2009-11-23 | 2013-11-18 | Training scoring models optimized for highly-ranked results |
Family Applications Before (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/624,001 Active 2030-07-28 US8131786B1 (en) | 2009-11-23 | 2009-11-23 | Training scoring models optimized for highly-ranked results |
US13/342,532 Expired - Fee Related US8429212B1 (en) | 2009-11-23 | 2012-01-03 | Training scoring models optimized for highly-ranked results |
US13/616,108 Expired - Fee Related US8589457B1 (en) | 2009-11-23 | 2012-09-14 | Training scoring models optimized for highly-ranked results |
Country Status (1)
Country | Link |
---|---|
US (4) | US8131786B1 (en) |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160350264A1 (en) * | 2015-05-26 | 2016-12-01 | Hon Hai Precision Industry Co., Ltd. | Server and method for extracting content for commodity |
EP3712818A1 (en) * | 2019-03-19 | 2020-09-23 | Sap Se | Image search and training system |
US20200411164A1 (en) * | 2016-11-25 | 2020-12-31 | Contextflow Gmbh | Method and a system for creating a medical image database by means of a convolutional neural network |
US11361191B2 (en) | 2018-05-22 | 2022-06-14 | Ebay Inc. | Adversarial learning for finegrained image search |
CN115631388A (en) * | 2022-12-21 | 2023-01-20 | 第六镜科技(成都)有限公司 | Image classification method and device, electronic equipment and storage medium |
Families Citing this family (37)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8396325B1 (en) * | 2009-04-27 | 2013-03-12 | Google Inc. | Image enhancement through discrete patch optimization |
US8611695B1 (en) | 2009-04-27 | 2013-12-17 | Google Inc. | Large scale patch search |
US8391634B1 (en) | 2009-04-28 | 2013-03-05 | Google Inc. | Illumination estimation for images |
US8385662B1 (en) | 2009-04-30 | 2013-02-26 | Google Inc. | Principal component analysis based seed generation for clustering analysis |
US8812519B1 (en) * | 2010-09-06 | 2014-08-19 | Jonathan Binnings Bent | Face photograph-based dating recommendation system |
US8798393B2 (en) | 2010-12-01 | 2014-08-05 | Google Inc. | Removing illumination variation from images |
US8787454B1 (en) | 2011-07-13 | 2014-07-22 | Google Inc. | Method and apparatus for data compression using content-based features |
EP2783303A4 (en) * | 2011-11-21 | 2015-09-30 | Microsoft Technology Licensing Llc | Prototype-based re-ranking of search results |
US8938119B1 (en) | 2012-05-01 | 2015-01-20 | Google Inc. | Facade illumination removal |
US8707201B1 (en) | 2012-06-27 | 2014-04-22 | Google Inc. | Systems and methods for prioritizing notifications on mobile devices |
US9560332B2 (en) * | 2012-09-10 | 2017-01-31 | Google Inc. | Media summarization |
JP5883821B2 (en) * | 2013-03-28 | 2016-03-15 | 富士フイルム株式会社 | Image search apparatus, operation control method thereof, and image search server |
US9679224B2 (en) * | 2013-06-28 | 2017-06-13 | Cognex Corporation | Semi-supervised method for training multiple pattern recognition and registration tool models |
US9286549B1 (en) * | 2013-07-15 | 2016-03-15 | Google Inc. | Sublinear time classification via feature padding and hashing |
US11288346B1 (en) * | 2014-03-03 | 2022-03-29 | Charles Schwab & Co., Inc. | System and method for authenticating users using weak authentication techniques, with differences for different features |
US9916328B1 (en) | 2014-07-11 | 2018-03-13 | Google Llc | Providing user assistance from interaction understanding |
US9552549B1 (en) * | 2014-07-28 | 2017-01-24 | Google Inc. | Ranking approach to train deep neural nets for multilabel image annotation |
US9965559B2 (en) | 2014-08-21 | 2018-05-08 | Google Llc | Providing automatic actions for mobile onscreen content |
US9715639B2 (en) | 2015-06-18 | 2017-07-25 | The Boeing Company | Method and apparatus for detecting targets |
US9727785B2 (en) * | 2015-06-18 | 2017-08-08 | The Boeing Company | Method and apparatus for tracking targets |
US9971940B1 (en) * | 2015-08-10 | 2018-05-15 | Google Llc | Automatic learning of a video matching system |
US10970646B2 (en) | 2015-10-01 | 2021-04-06 | Google Llc | Action suggestions for user-selected content |
US10055390B2 (en) | 2015-11-18 | 2018-08-21 | Google Llc | Simulated hyperlinks on a mobile device based on user intent and a centered selection of text |
US10949770B2 (en) * | 2016-01-28 | 2021-03-16 | Shutterstock, Inc. | Identification of synthetic examples for improving search rankings |
US10671895B2 (en) * | 2016-06-30 | 2020-06-02 | Microsoft Technology Licensing, Llc | Automated selection of subjectively best image frames from burst captured image sequences |
US10685070B2 (en) * | 2016-06-30 | 2020-06-16 | Facebook, Inc. | Dynamic creative optimization for effectively delivering content |
US10535005B1 (en) | 2016-10-26 | 2020-01-14 | Google Llc | Providing contextual actions for mobile onscreen content |
US11237696B2 (en) | 2016-12-19 | 2022-02-01 | Google Llc | Smart assist for repeated actions |
US10922713B2 (en) | 2017-01-03 | 2021-02-16 | Facebook, Inc. | Dynamic creative optimization rule engine for effective content delivery |
US10572908B2 (en) | 2017-01-03 | 2020-02-25 | Facebook, Inc. | Preview of content items for dynamic creative optimization |
US11373086B2 (en) * | 2017-02-17 | 2022-06-28 | Google Llc | Cooperatively training and/or using separate input and response neural network models for determining response(s) for electronic communications |
US11188824B2 (en) | 2017-02-17 | 2021-11-30 | Google Llc | Cooperatively training and/or using separate input and subsequent content neural networks for information retrieval |
US10408852B2 (en) | 2017-04-26 | 2019-09-10 | Lawrence Livermore National Security, Llc | Automated control of microfluidic devices based on machine learning |
US11250433B2 (en) | 2017-11-02 | 2022-02-15 | Microsoft Technologly Licensing, LLC | Using semi-supervised label procreation to train a risk determination model |
CN110750548B (en) * | 2018-07-05 | 2024-04-05 | 深圳Tcl数字技术有限公司 | Problem evaluation method based on neural network, storage medium and application server |
US11574004B2 (en) * | 2019-11-26 | 2023-02-07 | Dash Hudson | Visual image search using text-based search engines |
CN113132752B (en) | 2019-12-30 | 2023-02-24 | 阿里巴巴集团控股有限公司 | Video processing method and device |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7546293B2 (en) * | 2001-03-30 | 2009-06-09 | Microsoft Corporation | Relevance maximizing, iteration minimizing, relevance-feedback, content-based image retrieval (CBIR) |
-
2009
- 2009-11-23 US US12/624,001 patent/US8131786B1/en active Active
-
2012
- 2012-01-03 US US13/342,532 patent/US8429212B1/en not_active Expired - Fee Related
- 2012-09-14 US US13/616,108 patent/US8589457B1/en not_active Expired - Fee Related
-
2013
- 2013-11-18 US US14/083,043 patent/US8965891B1/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7546293B2 (en) * | 2001-03-30 | 2009-06-09 | Microsoft Corporation | Relevance maximizing, iteration minimizing, relevance-feedback, content-based image retrieval (CBIR) |
Non-Patent Citations (18)
Title |
---|
Burges et al., Learning to Rank using Gradient Descent, Proceedings of the 22nd International Conference on Machine Learning, 2005, pp. 89-96. |
Burges et al., Learning to Rank with Nonsmooth Cost Functions, NIPS, 2006, 8 pages. |
Burges, Learning to Rank for Web Search: Some New Direction, Web Learning Group Microsoft Research, SIGIR Ranking Workshop, Jul. 27, 2007, 88 pages. |
Cao et al, Learning to Rank: From Pairwise Approach to Listwise Approach, Proceedings of the 24th International Conference on Machine Learning, 2007, pp. 129-13. |
Caruana et al., Using the Future to "Sort Out" the Present: Rankprop and Multitask Learning for Medical Risk Evaluation, Advances in Neural Information Processing Systems 8, Proceedings of the 1995 Conference, 1995, pp. 959-965. |
Crammer and Singer, Pranking with Ranking, Advances in Neural Information Processing Systems 14, vol. 1, Proceedings of the 2002 Conference, 2002, pp. 641-647. |
Freund et al., An Efficient Boosting Algorithm for Combining Preferences, 2003, J of Machine Learning Research, vol. 4, pp. 933-969. |
Friedman, 1999 Reitz Lecture, Greedy Function Approximation: A Gradient Boosting Machine, 2001, vol. 29, No. 5, pp. 1189-1232. |
Grangier and Bengio, A Discriminative Kernel-based Model to Rank Images from Text Queries, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, vol. 30, No. 8, pp. 1-14. |
Harrington, Online Ranking/Collaborative filtering using the Perceptron Algorithm, Proceedings of the Twentieth International Conference on Machine Learning, ICML-2003, pp. 250-257. |
Herbrich et al., Support Vector Learning for Ordinal Regression, Artificial Neural Networks, Sep. 7-10, 1999, Conference Publication No. 470, pp. 97-102. |
Joachims, Optimizing Search Engines using Clickthrough Data, Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), ACM, 2002, pp. 133-142. |
Le et al., Optimization of Ranking Measures, J of Machine Learning Research 1 (2999) 1-48, 2010, pp. 1-29. |
Li et al., Learning to Rank Using Classification and Gradient Boosting, NIPS, 2007, pp. 1-10. |
Spall, Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation, IEEE Transactions on Automatic Control, 1992, vol. 37, No. 3, pp. 332-341. |
Tsai et al., FRank: A Ranking Method with Fidelity Loss, SIGIR 2007 Proceedings, Session 16: Learning to Rank II, 2007, pp. 383-390. |
Xu and Li, AdaRank: A Boosting Algorithm for Information Retrieval, SIGIR 2007 Proceedings, Session 16: Learning to Rank II, 2007, pp. 391-398. |
Yue et al., A Support Vector Method for Optimizing Average Precision, SIGIR 2007 Proceedings, Session 12: Learning to Rank I, 2007, pp. 271-278. |
Cited By (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160350264A1 (en) * | 2015-05-26 | 2016-12-01 | Hon Hai Precision Industry Co., Ltd. | Server and method for extracting content for commodity |
US9906588B2 (en) * | 2015-05-26 | 2018-02-27 | Hon Hai Precision Industry Co., Ltd. | Server and method for extracting content for commodity |
US20200411164A1 (en) * | 2016-11-25 | 2020-12-31 | Contextflow Gmbh | Method and a system for creating a medical image database by means of a convolutional neural network |
US11915464B2 (en) * | 2016-11-25 | 2024-02-27 | Contextflow Gmbh | Method and a system for creating a medical image database by means of a convolutional neural network |
US11361191B2 (en) | 2018-05-22 | 2022-06-14 | Ebay Inc. | Adversarial learning for finegrained image search |
EP3712818A1 (en) * | 2019-03-19 | 2020-09-23 | Sap Se | Image search and training system |
US10885385B2 (en) | 2019-03-19 | 2021-01-05 | Sap Se | Image search and training system |
CN115631388A (en) * | 2022-12-21 | 2023-01-20 | 第六镜科技(成都)有限公司 | Image classification method and device, electronic equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US8589457B1 (en) | 2013-11-19 |
US8131786B1 (en) | 2012-03-06 |
US8429212B1 (en) | 2013-04-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8965891B1 (en) | Training scoring models optimized for highly-ranked results | |
US9176988B2 (en) | Image relevance model | |
US8370282B1 (en) | Image quality measures | |
US9183226B2 (en) | Image classification | |
US9589277B2 (en) | Search service advertisement selection | |
US8566746B2 (en) | Parameterization of a categorizer for adjusting image categorization and retrieval | |
US7111002B2 (en) | Relevance maximizing, iteration minimizing, relevance-feedback, content-based image retrieval (CBIR) | |
US8805812B1 (en) | Learning semantic image similarity | |
US10311096B2 (en) | Online image analysis | |
US8189963B2 (en) | Matching advertisements to visual media objects | |
US20230205813A1 (en) | Training Image and Text Embedding Models | |
US8301638B2 (en) | Automated feature selection based on rankboost for ranking | |
CN112119388A (en) | Training image embedding model and text embedding model | |
US9110923B2 (en) | Ranking over hashes | |
US20210125108A1 (en) | Training a ranking model | |
US10565265B2 (en) | Accounting for positional bias in a document retrieval system using machine learning | |
US20140250115A1 (en) | Prototype-Based Re-Ranking of Search Results | |
US11636164B2 (en) | Search system for providing web crawling query prioritization based on classification operation performance | |
US20100121844A1 (en) | Image relevance by identifying experts | |
US11403339B2 (en) | Techniques for identifying color profiles for textual queries | |
CN110851705A (en) | Project-based collaborative storage recommendation method and recommendation device thereof | |
Piras et al. | Passive-aggressive online learning for relevance feedback in content based image retrieval | |
US11308099B2 (en) | Method of and system for ranking digital objects based on objective characteristic associated therewith |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BENGIO, SAMY;CHECHIK, GAL;IOFFE, SERGEY;AND OTHERS;SIGNING DATES FROM 20091113 TO 20091115;REEL/FRAME:031769/0132 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |