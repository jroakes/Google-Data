CN114846521A - Learning lighting from various portraits - Google Patents
Learning lighting from various portraits Download PDFInfo
- Publication number
- CN114846521A CN114846521A CN202080089261.9A CN202080089261A CN114846521A CN 114846521 A CN114846521 A CN 114846521A CN 202080089261 A CN202080089261 A CN 202080089261A CN 114846521 A CN114846521 A CN 114846521A
- Authority
- CN
- China
- Prior art keywords
- images
- image
- illumination
- prediction engine
- faces
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/50—Lighting effects
- G06T15/506—Illumination models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/60—Extraction of image or video features relating to illumination properties, e.g. using a reflectance or lighting model
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/61—Control of cameras or camera modules based on recognised objects
- H04N23/611—Control of cameras or camera modules based on recognised objects where the recognised objects include parts of the human body
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/70—Circuitry for compensating brightness variation in the scene
- H04N23/741—Circuitry for compensating brightness variation in the scene by increasing the dynamic range of the image compared to the dynamic range of the electronic image sensors
Abstract
Techniques to estimate illumination from a portrait include generating illumination estimates from a single image of a face based on a Machine Learning (ML) system using a plurality of Bidirectional Reflectance Distribution Functions (BRDFs) as loss functions. In some implementations, the ML system is trained using an image of a face formed with HDR illumination computed from an LDR imagery. The technical solution includes training an illumination estimation model using a data set of a portrait and its corresponding ground truth lighting in a supervised manner.
Description
Cross Reference to Related Applications
This application is a non-provisional application entitled "LEARNING ILLUMINATION FROM portrait" filed on 20/5/2020, and claiming priority, the disclosure of which is hereby incorporated by reference in its entirety.
Technical Field
The present disclosure relates to determining lighting from a portrait for use in, for example, augmented reality applications.
Background
A problem in both still photo and video applications is matching the lighting of the real-world scene so that the rendered virtual content matches the appearance of the scene as if it were. For example, a lighting scheme may be designed for an Augmented Reality (AR) use case with a world-facing camera, as in a rear camera of a mobile device, where one may want to render a synthetic object, such as a piece of furniture, into a live camera feed of a real-world scene.
Disclosure of Invention
Embodiments disclosed herein provide learning-based techniques for estimating High Dynamic Range (HDR) omnidirectional illumination from a single Low Dynamic Range (LDR) portrait image captured under any indoor or outdoor lighting conditions. Such techniques include training a model using portrait photos paired with ground true (ground true) ambient lighting of the portrait photos. The training includes generating a rich set of such photos by: the reflex fields and alfa masks of 70 various subjects of various expressions were recorded using a lighting stage (light stage), then the subjects were relighted using image-based relighting with a database of one million HDR lighting environments, composited onto paired high resolution background imagery recorded during the light acquisition. Training of the illumination estimation model uses a rendering-based loss function, and in addition, in some cases, multi-scale countermeasures losses, to estimate plausible high-frequency illumination details. This learning-based technique reliably handles inherent ambiguities between overall illumination intensity and surface albedo, restoring similarly dimensioned illumination for subjects with various skin natural colors. The technique further allows virtual objects and numeric characters to be added to a portrait photograph with consistent illumination. The lighting estimation can be run in real-time on a smartphone, enabling realistic rendering and compositing of virtual objects into live video for Augmented Reality (AR) applications.
In one general aspect, a method can include: receiving image training data representing a plurality of images, each of the plurality of images comprising at least one of a plurality of human faces, each of the plurality of human faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each of the plurality of illumination sources having been in a respective one of a plurality of orientations within the physical or virtual environment. The method can further include: a prediction engine is generated based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
In another general aspect, a computer program product includes a non-transitory storage medium, the computer program product including code that, when executed by processing circuitry of a computing device, causes the processing circuitry to perform a method. The method can include: receiving image training data representing a plurality of images, each of the plurality of images comprising at least one of a plurality of human faces, each of the plurality of human faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each of the plurality of illumination sources having been in a respective one of a plurality of orientations within the physical or virtual environment. The method can further include: a prediction engine is generated based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
In another general aspect, an electronic device includes: a memory; and a control circuit coupled to the memory. The control circuit can be configured to: receiving image training data representing a plurality of images, each of the plurality of images comprising at least one of a plurality of human faces, each of the plurality of human faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each of the plurality of illumination sources having been in a respective one of a plurality of orientations within the physical or virtual environment. The control circuit can also be configured to: a prediction engine is generated based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 is a schematic diagram illustrating an example electronic environment in which the improved techniques described herein may be implemented.
FIG. 2 is a flow diagram illustrating an example method of estimating illumination from a portrait in accordance with a disclosed embodiment.
FIG. 3 is a schematic diagram illustrating an example system configured to estimate illumination from a portrait in accordance with disclosed embodiments.
Fig. 4 is a schematic diagram illustrating an example Convolutional Neural Network (CNN) within the example system illustrated in fig. 3.
FIG. 5 is a schematic diagram illustrating an example arbiter within the example system illustrated in FIG. 3.
FIG. 6 is a schematic diagram illustrating an example of a computer device and a mobile computer device that can be used to implement the described techniques.
Detailed Description
One challenge in video applications such as Augmented Reality (AR) involves rendering a synthetic object into a real scene so that the object appears as if it were in the scene. One problem is to match the lighting of a real-world scene so that the rendered virtual content matches the appearance of the scene plausibly. For example, a lighting scheme may be designed for an AR use case with a world-facing camera, as in a rear-facing camera of a mobile device, where someone may want to render a synthetic object (such as a piece of furniture) into a live camera feed of a real-world scene.
However, such an illumination scheme for world-oriented camera design may be different from an illumination scheme for front camera design (e.g., for self-portrait images). For example, in portrait photographs, lighting affects the look and feel of a given shot. Photographers illuminate their subjects to convey specific aesthetics and emotional mood. One method used by film visual effects practitioners to capture real-world lighting schemes involves recording the color and intensity of the omnidirectional illumination by taking a mirror ball using multiple exposures. The result of this conventional approach is an HDR "image-based lighting" (IBL) environment for realistically rendering virtual content into real-world photographs.
AR shares the goal of realistically fusing visual content with real-world imagery with movie visual effects. However, in real-time AR, the illumination measurements from specialized capture hardware are not available because the acquisition is impractical for ordinary mobile phone or headset users. Similarly, for late visual effects in movies, a stadium lighting measurement is not always available, however, the light engineer still has to use cues in the scene to deduce the lighting.
Thus, the challenge is to determine the lighting scheme for the front camera given an image of a face within a lighting environment. Some concepts have utilized strong geometry and reflection priors from the face to solve for the illumination from the portrait. Over the years since some researchers have introduced portrait backlighting, most of these techniques have attempted to recover both facial geometry and low frequency approximations of distant scene lighting, typically using up to a second order Spherical Harmonic (SH) base representation. A reasonable reason for this approximation is that the skin reflectance is mainly diffuse (lambertian) and therefore acts as a low pass filter for the incident illumination. For a diffuse reflective material, the irradiance does very closely approach the nine-dimensional subspace well represented by the basis.
However, the illumination at the time of capture may reveal itself not only by diffuse reflection of the skin but also by the direction and extent of the drop shadow and the intensity and location of the mirror highlight. Inspired by these cues, some methods train neural networks to perform back-illumination from a portrait to estimate the omnidirectional HDR illumination without assuming any specific skin reflection model. Such an approach may produce higher frequency lighting that can be used to convincingly render novel objects into real-world portraits, applied in both visual effects and AR when offline lighting measurements are not available.
Conventional methods of estimating illumination given an LDR image of a face include generating such illumination estimates based on a modeled Bidirectional Reflectance Distribution Function (BRDF) that defines a relationship between incident optical illumination and reflected optical radiation on the face. BDRF can be expressed as the differential of the optical radiation or the ratio of the power per unit solid angle with respect to the direction of the incident ray orthogonal to the projected area per unit of the ray to the differential of the intensity of the exiting optical radiation or the power per unit surface area.
A technical problem with the above-described conventional methods of estimating illumination from images of faces is that they base illumination estimation on a single reflection function, such as a lambertian or Phong model, which can limit the robustness of illumination estimation when there are: skin reflection is extremely complex, involving sub-surface scattering as well as asperities and fresnel reflections, for example in the presence of varying skin natural colors. In addition, the inherent ambiguity between light source intensity and surface albedo prevents simple restoration of the correct scale of illumination for subjects of various skin tones, even though a simple lambertian model can accurately predict skin reflectance.
According to embodiments described herein, a solution to the above technical problem includes generating a lighting estimate from a single image of a face based on a Machine Learning (ML) system using a plurality of Bidirectional Reflectance Distribution Functions (BRDFs) as loss functions. In some embodiments, the ML system is trained using images of faces formed with HDR illumination captured using an LDR illumination acquisition method. The technical solution includes training an illumination estimation model using a data set of a portrait and its corresponding ground truth lighting in a supervised manner. In an example dataset, 70 various subjects were photographed in an illuminated stage system as illuminated by 331 directional light sources forming the basis on a ball, so that the captured subjects could be re-illuminated to appear as if they were in any scene with image-based re-illumination. Although several databases of real world lighting environments captured using traditional HDR panorama capture techniques are publicly available, the LDR lighting collection techniques employed in some embodiments have been extended to instead capture about 1 million indoor and outdoor lighting environments, upgrading them to HDR via a novel non-negative least squares solver formula before using them for relighting.
A technical advantage of the disclosed embodiments is that the ML system produces substantially the same illumination estimate at the correct scale or exposure value, regardless of the skin natural color of the face in the input image. Any attempt at illumination estimation is complicated by the inherent ambiguity between surface reflection (albedo) and light source intensity. In other words, a shadow of a pixel is rendered unchanged if its albedo is halved when the light source intensity is doubled. The improved technique described above explicitly evaluates the performance of the model on a wide variety of subjects with different natural skin colors. For a given lighting condition, the improved techniques are capable of restoring lighting on a similar scale for a wide variety of subjects.
In addition, the ML system is able to estimate the HDR illumination even when trained on LDR portrait images generated using the HDR illumination. Several recent efforts have attempted to recover lighting from portraits without relying on a low frequency lighting base or BRDF model, including deep learning methods for arbitrary scenes and for outdoor scenes containing only the sun. The technical problem described herein outperforms both of these approaches and generalizes to any indoor or outdoor scenario. These models rely on computer-generated humanoid models as training data and are therefore not generalized to the likeness of a real natural environment when inferred.
Fig. 1 is a schematic diagram of an example electronic environment 100 in which the above-described aspects may be implemented. The computer 120 is configured to train and operate a prediction engine configured to estimate illumination from the portrait.
In some implementations, one or more of the components of the computer 120 can be or can include a processor (e.g., the processing unit 124) configured to process instructions stored in the memory 126. Examples of such instructions as depicted in fig. 1 include an image acquisition manager 130 and a prediction engine training manager 140. In addition, as shown in FIG. 1, the memory 126 is configured to store various data, which is described with respect to respective management using such data.
In some embodiments, image acquisition manager 130 is further configured to crop and resize face images from image training data 131 to produce a standard-sized portrait. By cropping the image and resizing it to a standard size, the training of the ML system is made more robust.
The image training data 131 represents a set of likelihoods of faces taken with various lighting arrangements. In some implementations, the image training data 131 includes an image or portrait of a face formed with HDR illumination recovered from Low Dynamic Range (LDR) illumination environment capture. As shown in fig. 1, the image training data 131 includes a plurality of images 132(1), … 132(M), where M is the number of images in the image training data 131. Each image, e.g., image 132(1), includes light direction data 134(1) and pose data 135 (1).
Light direction data 134(1 … M) represents one of a specified number of directions (e.g., 331) from which a face is illuminated for a portrait used in image training data 131. In some embodiments, the light direction data 134(1) includes polar and azimuthal angles, i.e., coordinates on a unit sphere. In some embodiments, the light direction data 134(1) includes a triplet of direction cosines. In some embodiments, the light direction data 134(1) includes a set of euler angles. In the examples described above and in some embodiments, the angular configuration represented by the light direction data 134(1) is one of 331 configurations for training the ML system.
Pose data 135(1.. M) represents one of a plurality (e.g., 9) of specified poses in accordance with which an image of a face is captured. In some implementations, the gestures include facial expressions. In some embodiments, there are a fixed number of facial expressions (e.g., 3, 6, 9, 12, or higher).
The four-dimensional reflected field R (θ, Φ, x, y) may represent a subject illuminated from any illumination direction (θ, Φ) for each image pixel (x, y) according to the light direction data 134(1.. M). It has been demonstrated that the point product of this reflected field and the HDR illumination environment similarly parameterized by (θ, φ) re-illuminates subjects to appear as if they were in the scene. To photograph the subject's reflected field, a computer controlled ball of white LED light sources is used with the light spaced 12 ° apart at the equator. In such embodiments, the reflected field is formed by a collection of reflected base images, capturing the subject as each of the directional LED light sources is individually turned on within the ball device, one at a time. In some implementations, these one light at a time (OLAT) images are captured for multiple camera viewpoints. In some embodiments, 331 OLAT images are captured for each subject using six color machine vision cameras with 12 megapixel resolution placed 1.7 meters from the subject, although in some embodiments these values and the number of OLAT images and the type of camera used may be different. In some embodiments, the cameras are positioned approximately in front of the subject, with five cameras with a 35mm lens capturing the upper body of the subject from different angles, and one additional camera with a 50mm lens capturing a close-up image of the face with a more rigid construct.
In some embodiments, for the reflected fields of 70 various subjects, each subject performs nine different facial expressions from pose data 135(1.. M) and wearing different accessories, resulting in about 630 sets of OLAT sequences from six different camera viewpoints, totaling 3780 unique OLAT sequences. Other amounts of sets of OLAT sequences may be used. Subjects spanning a wide range of natural skin colors are photographed.
Since it takes some time, e.g. about six seconds, to acquire a complete OLAT sequence for a subject, there may be some slight subject motion between frames. In some embodiments, optical flow techniques are used to align images, occasionally (e.g., every 11 OLAT frames) interspersed with one additional "tracking" frame with uniform illumination to ensure that the constant brightness constraint for optical flow is satisfied. This step may preserve the sharpness of the image features when performing a re-illumination operation that linearly combines aligned OLAT images.
To re-illuminate subjects with a shot reflectance field, in some embodiments, a large database of HDR lighting environments is used in which no light sources are cropped. While there are several such databases containing on the order of thousands of indoor panoramas or upper hemispheres of outdoor panoramas, deep learning models are typically enhanced with a greater amount of training data. Thus, approximately 1 million indoor and outdoor lighting environments are collected. In some embodiments, using a mobile phone capture device enables the simultaneous capture of an auto-exposed and white-balanced LDR video of a high resolution background image and the corresponding LDR appearance of three balls of different reflectivity (diffuse, specular, matte silver with rough specular reflection). These three balls reveal different clues about the scene lighting. Specular spheres reflect all-around high frequency illumination, but since bright light sources are often cropped in a single exposure image, their intensity and color will be incorrect. In contrast, the near-lambertian BRDF of a diffuse reflecting sphere acts as a low pass filter with respect to the incident illumination, thereby capturing a blurred but relatively complete record of the total scene radiation.
Embodiments herein can have a real HDR recording of scene lighting for re-illuminating subjects after explicitly lifting these three ball appearances into an approximate HDR illumination environment.
The reference object data 136 represents a reference object, such as a ball of different reflectivity. Such reference objects are used to provide ground truth illumination in ML systems. As shown in fig. 1, the reference object data 136 comprises a plurality of reference sets 137(1), …,137(N), where N is the number of HDR illumination environments considered. Each of the reference sets 137(1 … N), e.g., reference set 137(1), includes BRDF data for the specular surfaces 138(1), matte silver 139(1), and diffuse reflective gray 141 (1). In some embodiments, the BRDF data 138(1), 139(1), and 141(1) comprise an array of BRDF values. In some embodiments, BRDF data 138(1), 139(1), and 141(1) comprise SH expanded sets of coefficients.
To train a model for estimating illumination from image training data 131 in a supervised manner, in some embodiments, the likeness represented by image training data 131 is tagged with ground truth lighting, such as reference object data 136. In some embodiments, portraits using data-driven techniques of image-based relighting are synthesized, in some cases shown to produce photographic real-world relighting results for human faces, to properly capture complex light transmission phenomena for human skin and hair, such as sub-surface and rough scattering and fresnel reflections. Such compositing is in contrast to rendering of 3D models of faces, which often cannot represent these complex phenomena.
The prediction engine training manager 140 is configured to generate prediction engine data 150, the prediction engine data 150 representing the above-described ML system for estimating illumination from a portrait. As shown in fig. 1, the prediction engine training manager 140 includes an encoder 142, a decoder 143, and a discriminator 144.
Returning to reference object data 136, given a captured image of three reflective spheres that may have cropped pixels, some embodiments solve for HDR illumination that can already plausibly produce the appearance of these three spheres. In some casesIn an embodiment, the illumination stage system can be used again first to capture the reflected field for the diffuse and matte silver balls. Some embodiments convert the reflectance base image into the same correlated radiation measurement space normalized based on the incident light source color. Some embodiments then project the reflected base image into a specular sphere map (lambert's equal-product projection) to accumulate energy from the input image for each new illumination direction (θ, φ) on, for example, a 32 × 32 image of a specular sphere as in some embodiments, to form a reflected field R (θ, φ, x, y) or to be sliced into individual pixels R x，y (θ，φ)。
For illumination directions (θ, φ) in a captured specular ball image that is not clipped for color channel c, some embodiments recover scene illumination L by simply scaling the specular ball image pixel values by the inverse (82.7%) of the measured specular ball reflectivity c (theta, phi). Some embodiments set the pixel value to 1.0 for the illumination direction (θ, φ) with the clipped pixels in the original mirror sphere image, scaling this by the inverse of the measured reflectance, resulting in scene illumination L c (theta, phi) and then solving for the residual lost illumination intensity U using a non-negative least squares solver formulation c (theta, phi). Given an index k for BRDF (e.g., diffuse reflective or matte silver), a color channel c, and a measured reflection field R x，y，c，k Original image pixel value p of (theta, phi) x，y，c，k Due to the principle of superposition of light, the following equation is satisfied:
equation (1) represents a set of m equations for each BRDF k and color channel c, equal to the number of spherical pixels in the reflection base image, where n is the unknown residual light intensity. For light direction without clipping, U c (θ, Φ) ═ 0. For each color channel, where km > n, the unknown U can be solved using non-negative least squares c (θ, φ) value, thereby ensuring that light is only added and not removed. Fruit of Chinese wolfberryIndeed, some embodiments exclude the pruned pixel p from the solution x，y，c，k . Some methods have restored the clipped light source intensity by comparing pixel values from the shot diffuse reflectance sphere to the diffuse reflectance convolution of the clipped panorama, but these embodiments use the shot reflectance basis and multiple BRDFs for the first time.
In some embodiments, it is observed that when solving for U c When each color channel is independently processed in (θ, φ), red, green, and blue light sources of bright hue are produced, typically in a geometrically near illumination direction, rather than a single light source with greater intensity on all three color channels. To recover the results of light sources with more plausible neutral shading, some embodiments add cross-color-channel regularization based on the following insight: color of photographed diffusely reflecting gray spheres reveals the average color balance (R) of bright light sources in a scene avg ，G avg ，B avg ). Some embodiments add a new set of linear equations with a weight λ 0.5 to the system of equations:
these regularization terms penalize the recovery of light sources of intense hue in color balance that are different from the target diffuse reflecting sphere. Some embodiments add a regularization term to encourage similar intensities for geometrically nearby lighting directions, but this will not necessarily prevent restoration of strongly-shaded light. Some embodiments use a Ceres solver to recover U c (θ, φ), thereby promoting the appearance of one million captured balls to HDR illumination. Since the LDR image from this video rate data collection method is 8-bit and encoded as sRGB, possibly with local native color mapping, some embodiments first linearize the ball image, assuming a gamma value γ of 2.2, as included for linear systemsAs is the formula.
Using the captured reflectance field and HDR boosted illumination for each subject, some embodiments generate a relighting portrait with ground truth lighting to be used as training data. Some embodiments again convert the reflected base image into the same correlated radiation measurement space, calibrated based on the incident light source color. Since the illumination environment is represented as, for example, a 32 x 32 specular sphere image, some embodiments project the reflected field onto this basis, again accumulating energy from the input image for each new illumination direction (θ, φ) as in some embodiments. Each new base image is a linear combination of the original 331 ola images.
The illumination capture technique also produces a high resolution background image corresponding to the appearance of the three balls. Since even arbitrary images contain useful clues for extracting lighting estimates, some embodiments compound the relighting subjects onto the background rather than onto the black box as in some embodiments. Since the background image may be an 8-bit sRGB, some embodiments crop and apply this transfer function to the relighted subject prior to compositing. Since natural environment portraits may contain cropped pixels (especially for 8-bit live video for moving ARs), some implementations discard HDR data for relighting subjects to match the expected inference-time input.
Although background imagery may provide contextual cues to aid in lighting estimation, some implementations compute a face bounding box for each input, and during training and inference, some implementations crop each image, enlarging the bounding box by 25%. During training, some embodiments add slight cropping zone variations, randomly changing their position and extent.
The components (e.g., modules, processing unit 124) of user device 120 can be configured to operate based on one or more platforms (e.g., one or more similar or different platforms) that can include one or more types of hardware, software, firmware, operating systems, runtime libraries, and so forth. In some implementations, the components of the computer 120 can be configured to operate within a cluster of devices (e.g., a server farm). In such an embodiment, the functions and processing of the components of computer 120 can be distributed to several devices of a cluster of devices.
The components of computer 120 can be or include any type of hardware and/or software configured to process attributes. In some implementations, one or more portions of the components shown in the components of computer 120 in fig. 1 can be or can include a hardware-based module (e.g., a Digital Signal Processor (DSP), a Field Programmable Gate Array (FPGA), a memory), a firmware module, and/or a software-based module (e.g., a module of computer code, a set of computer-readable instructions capable of being executed at a computer). For example, in some embodiments, one or more portions of the components of computer 120 can be or can include software modules configured for execution by at least one processor (not shown). In some embodiments, the functionality of the components can be included in different modules and/or different components than those shown in fig. 1, including combining the functionality illustrated as two components into a single component.
Although not shown, in some embodiments, the components of computer 120 (or portions thereof) can be configured to operate, for example, within a data center (e.g., a cloud computing environment), a computer system, one or more servers/host devices, and/or the like. In some implementations, the components of computer 120 (or portions thereof) can be configured to operate within a network. Accordingly, the components of computer 120 (or portions thereof) can be configured to operate within various types of network environments that can include one or more devices and/or one or more server devices. For example, the network can be or can include a Local Area Network (LAN), a Wide Area Network (WAN), and the like. The network can be or can include a wireless network and/or a wireless network implemented using, for example, gateway devices, bridges, switches, and so forth. The network can include one or more segments and/or can have portions based on protocols such as Internet Protocol (IP) and/or proprietary protocols. The network can include at least a portion of the internet.
In some implementations, one or more portions of the components of the computer 120 can be or can include a processor configured to process instructions stored in a memory. For example, the image acquisition manager 130 (and/or portions thereof) and the predictive image training manager 140 (and/or portions thereof) can be a combination of a processor and a memory configured to execute instructions related to a process that implements one or more functions.
In some implementations, the memory 126 can be any type of memory, such as random access memory, disk drive memory, flash memory, and so forth. In some implementations, the memory 126 can be implemented as more than one memory component (e.g., more than one RAM component or disk drive memory) associated with a component of the VR server computer 120. In some implementations, the memory 126 can be a database memory. In some implementations, the memory 126 can be or can include non-local memory. For example, the memory 126 can be or can include a memory shared by multiple devices (not shown). In some implementations, the memory 126 can be associated with a server device (not shown) within the network and configured to serve components of the computer 120. As shown in fig. 1, memory 126 is configured to store various data, including image training data 131, reference object data 136, and prediction engine data 150.
FIG. 2 is a flow diagram depicting an example method 200 of performing a visual search in accordance with the improved techniques described above. Method 200 may be performed by a software structure described in connection with fig. 1 that resides in memory 126 of computer 120 and is run by a collection of processing units 124.
At 202, the image acquisition manager 130 receives a plurality of images (e.g., image training data 131) of a plurality of faces in a physical environment. Each of the plurality of faces is illuminated by at least one of a plurality of illumination sources oriented within the physical environment according to at least one of a plurality of orientations (e.g., light direction data 134(1.. M)).
At 204, the prediction engine training manager 140 generates a prediction engine (e.g., prediction engine data 150) configured to generate a predicted lighting profile based on a plurality of images of a plurality of human faces. The prediction engine is configured to generate a predicted illumination profile based on the input image data. The input image data represents at least one human face. The prediction engine includes a cost function (e.g., the arbiter 144 and the cost function data 154) based on a plurality of Bidirectional Reflectance Distribution Functions (BRDFs) corresponding to each of the reference objects (e.g., the reference object data 136). The predicted illumination profile represents the spatial distribution of illumination incident on the body of the portrait. An example representation of the predicted lighting includes coefficients of spherical harmonic expansions of the angular illumination function. Another example representation of predicted lighting includes a grid of pixels, each pixel having a value of an illumination function of a solid angle.
FIG. 3 is a schematic diagram illustrating an example ML system 300 configured to estimate illumination from a portrait. As shown in fig. 3, ML system 300 includes a generator network 314 and an auxiliary confrontation arbiter 312. The input to the generator network 314 is an sRGB encoded LDR image, e.g., LDR portrait 302, with a crop 306 of the face region of each image detected by the face detector 304, sized to 256 x 256 input resolution, and normalized to a range of [ -0.5, 0.5 ]. As shown in fig. 3, the generator network 314 has an encoder/decoder architecture including an encoder 142 and a decoder 143, with a potential vector representation of log-space HDR illumination of size 1024 at the bottleneck. In some embodiments, encoder 142 and decoder 143 are implemented as Convolutional Neural Networks (CNNs). The final output of the generator network 314 comprises a 32 x 32 HDR image representing a mirrored sphere with log-space omnidirectional illumination. Further details regarding encoder 142 and decoder 143 are shown with reference to fig. 4; further details regarding the secondary confrontation arbiter 312 are shown with reference to fig. 5.
Fig. 4 is a schematic diagram illustrating example details for the encoder 142 and decoder 143. As shown in fig. 4, the encoder 142 includes five 3 x 3 convolutions, each convolution followed by a blur pooling operation, with successive filter depths of 16, 32, 64, 128, and 256, followed by one final convolution with a filter size of 8 x 8 and a depth of 256, and finally a fully connected layer. Decoder 143 includes three sets of 3 x 3 convolutions with filter depths of 64, 32 and 16, each followed by a bilinear upsampling operation.
Fig. 5 is a schematic diagram illustrating an example secondary confrontation arbiter 312. The secondary confrontation arbiter 312 is configured to provide a confrontation loss term to enforce the estimation of the plausible high frequency illumination. As shown in fig. 5, the secondary confrontation arbiter 312 takes as input the cropped image of the ground truth and the predicted illumination from the main model and attempts to discriminate between the truth examples and the generation examples. The discriminator has an encoder comprising three 3 x 3 convolutions, each 3 x 3 convolution followed by a max pooling operation with successive filter depths of 64, 128 and 256, followed by a fully connected layer of size 1024 prior to the final output layer. Since the decoder of the main network includes several upsampling operations, the network implicitly learns the information at multiple scales. Some embodiments utilize this multi-scale output to provide input to the discriminator, not only the full resolution 32 x 32 cropped illumination image, but also the multi-scale gradient technique using MSG-GAN: illumination images on 4 × 4, 8 × 8, and 16 × 16. Since the lower resolution feature map produced by the generator network has more than 3 channels, some embodiments add the convolution operation at each scale as an extra branch of the network, producing multiple scales of 3-channel illumination images to supply to the discriminator.
Returning to FIG. 3, the generator network 314 and the secondary confrontation arbiter 312 employ various cost functions to build the prediction engine and discriminate between real and generated lighting estimates. Some embodiments describe methods and systems for training a network to estimate HDR illumination from unconstrained images
since in some embodiments the network similarly outputs a signal having a pixel value Q c (θ, φ) HDR illuminated log space image Q, so the ball image is rendered as
Using binary masks masking the corners of each ball
Wherein the binary operator |, indicates element-by-element multiplication.
Not for use in videoLDR ball image captured in rate data collection as reference image I k Some embodiments instead render the sphere with HDR illumination recovered from a linear solver (e.g., equation (1)), gamma encoding the rendering with γ ═ 2.2. This can ensure that the same illumination is used to render the "ground truth" ball as the input portrait, preventing residual propagation from HDR illumination recovery to the model training phase.
Some embodiments eventually add an additional convolution branch to convert the multi-scale feature map of the decoder into a 3-channel image representing logarithmic space HDR illumination on a continuous scale. Some embodiments then extend the rendering loss function (equation (6)) of some embodiments to the multi-scale domain, rendering specular, matte silver, and diffuse reflective spheres during training in sizes 4 × 4, 8 × 8, 16 × 16, and 32 × 32. Using the sum of the scale indices denoted by s as λ s For each of the selectable weights, the multi-scale image reconstruction loss is written as
Recent work in unconstrained illumination estimation has shown that the term of countermeasures to loss improves the recovery of high frequency information compared to using only image reconstruction loss. Thus, some embodiments utilize a weight λ as in some embodiments adv An anti-loss term is added. However, in contrast to this technique, some embodiments use a multi-scale GAN architecture that flows gradients from the discriminators to the generator network over multiple scales, providing discriminators with real and generated cropped mirror sphere images of different sizes.
Some embodiments use Tensorflow and have beta 1 ＝0.9、β 2 An ADAM optimizer with 0.999 learning rate of 0.00015 is used for the generator network and as is common, a 100 times lower learning rate is used for the discriminator network, alternating between training the generators and discriminators. Some embodiments set λ for specular, diffuse and matte silver BRDF, respectively k λ is set to 0.2, 0.6, 0.2 s 1 to apply equally to all image scalesDegree is weighted, setting lambda adv 0.004 and a batch size of 32 was used. Since the number of lighting environments may be orders of magnitude greater than the number of subjects, for some embodiments, stopping at 1.2 rounds of the early point prevents overfitting to subjects in the training set. Some embodiments use a ReLU activation function for the generator network and an ELU activation function for the arbiter. To enhance the data set, some embodiments flip both the input image and the lighting environment across the vertical axis. Some embodiments enhance the dataset with a slight image rotation (+/-15 degrees) of the input image in the image plane.
Some embodiments split 70 subjects into two groups: 63 were used for training and 7 for evaluation, ensuring that all expressions and camera views for a given subject belong to the same subset. Some embodiments include manually selecting 7 subjects to include various skin natural colors. In summary, for each of 1 million lighting environments, some embodiments include randomly selecting 8 OLAT sequences from a training set to relight (across subjects, facial expressions, and camera views) to generate a training data set of 8 million portraits with ground truth lighting. Using the same approach, some embodiments capture the lighting environment in both indoor and outdoor locations not seen in training for evaluation, pairing only these with the evaluation subjects.
Accurately estimated illumination should correctly render objects with arbitrary reflectance properties, so the performance of the model uses L rec To test. This metric compares the appearance of three balls (diffuse, matte silver, and specular) as rendered with ground truth and estimated lighting.
This model outperforms some implementations for diffuse reflective and matte silver spheres for LDR image reconstruction loss. However, some embodiments can outperform this embodiment for a mirror ball. The second-order SH approximation of the ground real lighting can outperform L for diffuse reflecting spheres rec Because the low frequency representation of the illumination is sufficient for rendering lambertian material. However, this embodiment is able to outperform for matte silver and mirrors with non-lambertian BRDFTargeting of both balls rec 2 nd order SH decomposition. This shows that the illumination produced by this embodiment is better suited for rendering various materials.
Some embodiments add a loss function based on cross-subject consistency based on a difference between a first predicted lighting profile from a first face and a second predicted lighting profile from a second face. Such a loss function may provide a measure of the consistency of the illumination for various skin natural colors and head poses.
FIG. 6 illustrates an example of a general-purpose computer device 600 and a general-purpose mobile computer device 650 that can be used with the techniques described herein. Computer device 600 is one example configuration of computer 120 of fig. 1 and 2.
As shown in FIG. 6, computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Computing device 650 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, and other similar computing devices. The components shown herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit embodiments of the inventions described and/or claimed in this document.
The memory 604 stores information within the computing device 600. In one implementation, the memory 604 is a volatile memory unit or units. In another implementation, the memory 604 is a non-volatile memory unit or units. The memory 604 may also be another form of computer-readable medium, such as a magnetic or optical disk.
The storage device 606 can provide mass storage for the computing device 600. In one implementation, the storage device 606 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. The computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions which, when executed, perform one or more methods such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 604, the storage device 606, or memory on processor 602.
The high-speed controller 608 manages bandwidth-intensive operations for the computing device 600, while the low-speed controller 612 manages lower bandwidth-intensive operations. Such allocation of functions is merely exemplary. In one embodiment, the high-speed controller 608 is coupled to memory 604, display 616 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 610, which may accept various expansion cards (not shown). In an embodiment, low-speed controller 612 is coupled to storage device 506 and low-speed expansion port 614. The low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices such as a keyboard, a pointing device, a scanner, or a network device such as a switch or router, for example, through a network adapter.
Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation of one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor (which may be special or general purpose) coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium," "computer-readable medium" refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
The systems and techniques described here can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the internet.
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the specification.
It will also be understood that when an element is referred to as being "on," "connected to," "electrically connected to," coupled to, "or electrically coupled to" another element, it can be directly on, connected or coupled to the other element or one or more intervening elements may be present. In contrast, when an element is referred to as being directly on, directly connected to, or directly coupled to another element, there are no intervening elements present. Elements shown as directly on. The claims of the present application may be modified to recite the exemplary relationships described in the specification or shown in the figures.
While certain features of the described embodiments have been illustrated as described herein, many modifications, substitutions, changes and equivalents will now occur to those skilled in the art. It is, therefore, to be understood that the appended claims are intended to cover all such modifications and changes as fall within the scope of the embodiments. It is to be understood that they have been presented by way of example only, and not limitation, and various changes in form and details may be made. Any portions of the apparatus and/or methods described herein may be combined in any combination, except mutually exclusive combinations. The facts described herein can include various combinations and/or sub-combinations of the functions, elements and/or features of the different embodiments described.
In addition, the logic flows depicted in the various figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are within the scope of the following claims.
In the following, some examples are described.
Example 1: a method, comprising:
receiving image training data representing a plurality of images, each image of the plurality of images comprising at least one face of a plurality of faces, each face of the plurality of faces having been formed by combining images of one or more faces as illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each illumination source of the plurality of illumination sources having been in a respective orientation of a plurality of orientations within the physical or virtual environment; and
generating a prediction engine based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
Example 2: the method of example 1, further comprising:
combining the images of the one or more faces illuminated by the at least one of a plurality of illumination sources to synthetically render each face of the plurality of faces to appear as illuminated by a high dynamic range HDR illumination environment.
Example 3: the method of example 2, wherein combining the images comprises:
generating the HDR illumination environment based on low dynamic range LDR images of a set of reference objects, each reference object of the set of reference objects having a respective bidirectional reflection distribution function BRDF.
Example 4: the method of example 3, wherein the set of reference objects comprises specular spheres, matte silver spheres, and gray diffuse reflective spheres.
Example 5: the method of example 1, wherein generating the prediction engine comprises:
performing micro-renderable of a set of reference objects using the predicted lighting profile to produce a rendered image of the set of reference objects, each reference object of the set of reference objects having a respective bi-directional reflection distribution function, BRDF; and
generating a difference between the rendered image of the set of reference objects and a ground truth image in the set of reference objects as a function of a cost of the prediction engine.
Example 6: the method of example 5, wherein the cost function comprises a BRDF weighted L1 penalty on the rendered images of the reference object set.
Example 7: the method of example 5, wherein the cost function is a first cost function, and
wherein the prediction engine comprises a second cost function that is based on an antagonistic loss function from a high frequency mirror reflection of the specular sphere.
Example 8: the method of example 5, wherein the micro-renderable is performed using image-based relighting (IBRL) to produce a High Dynamic Range (HDR) illumination image.
Example 9: the method of example 5, wherein the cost function is a first cost function, and
wherein the prediction engine comprises a second cost function that is a cross-subject consistency based loss function based on a difference between a first predicted lighting profile from a first face and a second predicted lighting profile from a second face.
Example 10: the method of example 1, wherein generating the prediction engine comprises:
during the generation of the prediction engine, performing facial keypoint detection operations on image training data to produce facial keypoint identifiers that identify facial keypoints.
Example 11: the method of example 1, wherein generating the prediction engine comprises:
projecting each pixel of an image of a face of the plurality of faces into a common UV space.
Example 12: the method of example 1, wherein each image of the plurality of images is gamma encoded.
Example 13: a computer program product comprising a non-transitory storage medium, the computer program product comprising code that, when executed by processing circuitry of a computer, causes the processing circuitry to perform a method comprising:
receiving image training data representing a plurality of images, each image of the plurality of images comprising at least one face of a plurality of faces, each face of the plurality of faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each illumination source of the plurality of illumination sources having been in a respective orientation of a plurality of orientations within the physical or virtual environment; and
generating a prediction engine based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
Example 14: the computer program product of example 13, wherein generating the prediction engine comprises:
combining the images of the one or more faces as illuminated by the at least one of a plurality of illumination sources to synthetically render each face of the plurality of faces to appear as illuminated by a high dynamic range HDR illumination environment.
Example 15: the computer program product of example 14, wherein combining the images comprises:
generating the HDR illumination environment based on low dynamic range LDR images in a set of reference objects, each reference object in the set of reference objects having a respective bidirectional reflection distribution function BRDF.
Example 16: the computer program product of example 15, wherein the set of reference objects comprises specular spheres, matte silver spheres, and gray diffuse reflective spheres.
Example 17: the computer program product of example 13, wherein generating the prediction engine comprises:
performing micro-renderable of a set of reference objects using the predicted lighting profile to produce rendered images of the set of reference objects, each reference object of the set of reference objects having a respective bidirectional reflectance distribution function, BRDF; and
generating a difference between the rendered image in the set of reference objects and a ground truth image in the set of reference objects as a function of a cost of the prediction engine.
Example 18: the computer program product of example 14, wherein the cost function comprises a BRDF weighted L1 penalty on the rendered images in the set of reference objects.
Example 19: the computer program product of example 18, wherein the micro-renderable is performed using image-based re-illumination, IBRL, to produce a high dynamic range, HDR, illumination image.
Example 20: an electronic device, the electronic device comprising:
a memory; and
processing circuitry coupled to the memory, the processing circuitry configured to:
image training data representing a plurality of images, each image of the plurality of images comprising at least one face of a plurality of faces, each face of the plurality of faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each illumination source of the plurality of illumination sources having been in a respective orientation of a plurality of orientations within the physical or virtual environment; and is
Generating a prediction engine based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
Claims (20)
1. A method, comprising:
receiving image training data representing a plurality of images, each image of the plurality of images comprising at least one face of a plurality of faces, each face of the plurality of faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each illumination source of the plurality of illumination sources having been in a respective orientation of a plurality of orientations within the physical or virtual environment; and
generating a prediction engine based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
2. The method of claim 1, further comprising:
combining the images of the one or more faces illuminated by the at least one of a plurality of illumination sources to synthetically render each face of the plurality of faces to appear as illuminated by a high dynamic range HDR illumination environment.
3. The method of claim 2, wherein combining the images comprises:
generating the HDR illumination environment based on low dynamic range LDR images in a set of reference objects, each reference object in the set of reference objects having a respective bidirectional reflection distribution function BRDF.
4. The method of claim 3, wherein the set of reference objects comprises specular, matte silver, and gray diffuse reflective spheres.
5. The method of claim 1, wherein generating the prediction engine comprises:
performing micro-renderable of a set of reference objects using the predicted lighting profile to produce rendered images of the set of reference objects, each reference object of the set of reference objects having a respective bidirectional reflectance distribution function, BRDF; and
generating a difference between the rendered image in the set of reference objects and a ground truth image in the set of reference objects as a function of a cost of the prediction engine.
6. The method of claim 5, wherein the cost function comprises a BRDF weighted L1 penalty on the rendered images in the set of reference objects.
7. The method of claim 5, wherein the cost function is a first cost function, and
wherein the prediction engine comprises a second cost function that is based on an antagonistic loss function from a specular sphere's high frequency mirror reflection.
8. The method of claim 5, wherein the micro-renderable is performed using image-based relighting (IBRL) to produce a High Dynamic Range (HDR) illumination image.
9. The method of claim 5, wherein the cost function is a first cost function, and
wherein the prediction engine comprises a second cost function that is a cross-subject consistency based loss function based on a difference between a first predicted lighting profile from a first face and a second predicted lighting profile from a second face.
10. The method of claim 1, wherein generating the prediction engine comprises:
during generation of the prediction engine, a facial keypoint detection operation is performed on the image training data to produce facial keypoint identifiers that identify facial keypoints.
11. The method of claim 1, wherein generating the prediction engine comprises:
projecting each pixel of an image of a face of the plurality of faces into a common UV space.
12. The method of claim 1, wherein each image of the plurality of images is gamma-encoded.
13. A computer program product comprising a non-transitory storage medium, the computer program product comprising code that, when executed by processing circuitry of a server computing device, causes the processing circuitry to perform a method comprising:
receiving image training data representing a plurality of images, each image of the plurality of images comprising at least one face of a plurality of faces, each face of the plurality of faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each illumination source of the plurality of illumination sources having been in a respective orientation of a plurality of orientations within the physical or virtual environment; and
generating a prediction engine based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
14. The computer program product of claim 13, wherein generating the prediction engine comprises:
combining the images of the one or more faces illuminated by the at least one of a plurality of illumination sources to synthetically render each face of the plurality of faces to appear as illuminated by a high dynamic range HDR illumination environment.
15. The computer program product of claim 14, wherein combining the images comprises:
generating the HDR illumination environment based on low dynamic range LDR images in a set of reference objects, each reference object in the set of reference objects having a respective bidirectional reflection distribution function BRDF.
16. The computer program product of claim 15, wherein the set of reference objects comprises specular spheres, matte silver spheres, and gray diffuse reflective spheres.
17. The computer program product of claim 13, wherein generating the prediction engine comprises:
performing micro-renderable of a set of reference objects using the predicted lighting profile to produce rendered images of the set of reference objects, each reference object of the set of reference objects having a respective bidirectional reflectance distribution function, BRDF; and
generating a difference between the rendered image in the set of reference objects and a ground truth image in the set of reference objects as a function of a cost of the prediction engine.
18. The computer program product of claim 17, wherein the cost function comprises a BRDF weighted L1 penalty on the rendered images in the set of reference objects.
19. The computer program product of claim 17, wherein the micro-renderable is performed using image-based relighting (IBRL) to produce a High Dynamic Range (HDR) illumination image.
20. An electronic device, the electronic device comprising:
a memory; and
control circuitry coupled to the memory, the control circuitry configured to:
receiving image training data representing a plurality of images, each image of the plurality of images comprising at least one face of a plurality of faces, each face of the plurality of faces having been formed by combining images of one or more faces illuminated by at least one of a plurality of illumination sources in a physical or virtual environment, each illumination source of the plurality of illumination sources having been in a respective orientation of a plurality of orientations within the physical or virtual environment; and
generating a prediction engine based on the plurality of images, the prediction engine configured to produce a predicted lighting profile from input image data, the input image data representing an input human face.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062704657P | 2020-05-20 | 2020-05-20 | |
US62/704,657 | 2020-05-20 | ||
PCT/US2020/070558 WO2021236175A1 (en) | 2020-05-20 | 2020-09-21 | Learning illumination from diverse portraits |
Publications (1)
Publication Number | Publication Date |
---|---|
CN114846521A true CN114846521A (en) | 2022-08-02 |
Family
ID=72752560
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080089261.9A Pending CN114846521A (en) | 2020-05-20 | 2020-09-21 | Learning lighting from various portraits |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220027659A1 (en) |
EP (1) | EP3939011A1 (en) |
JP (1) | JP2023521270A (en) |
KR (1) | KR20220117324A (en) |
CN (1) | CN114846521A (en) |
WO (1) | WO2021236175A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2020102771A1 (en) * | 2018-11-15 | 2020-05-22 | Google Llc | Deep light design |
CN112132832B (en) * | 2020-08-21 | 2021-09-28 | 苏州浪潮智能科技有限公司 | Method, system, device and medium for enhancing image instance segmentation |
US20220108434A1 (en) * | 2020-10-07 | 2022-04-07 | National Technology & Engineering Solutions Of Sandia, Llc | Deep learning for defect detection in high-reliability components |
US11908233B2 (en) * | 2020-11-02 | 2024-02-20 | Pinscreen, Inc. | Normalization of facial images using deep neural networks |
CN115294263B (en) * | 2022-10-08 | 2023-02-03 | 武汉大学 | Illumination estimation method and system |
Family Cites Families (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10417824B2 (en) * | 2014-03-25 | 2019-09-17 | Apple Inc. | Method and system for representing a virtual object in a view of a real environment |
US9860453B2 (en) * | 2014-11-26 | 2018-01-02 | Disney Enterprises, Inc. | Systems and methods for estimating sky light probes for outdoor images |
US10430978B2 (en) * | 2017-03-02 | 2019-10-01 | Adobe Inc. | Editing digital images utilizing a neural network with an in-network rendering layer |
US10609286B2 (en) * | 2017-06-13 | 2020-03-31 | Adobe Inc. | Extrapolating lighting conditions from a single digital image |
CN109427080A (en) * | 2017-08-31 | 2019-03-05 | 爱唯秀股份有限公司 | The method for quickly generating large amount of complex light source facial image |
WO2019171124A1 (en) * | 2018-03-06 | 2019-09-12 | Omron Corporation | Method, device, system and program for setting lighting condition and storage medium |
US10692276B2 (en) * | 2018-05-03 | 2020-06-23 | Adobe Inc. | Utilizing an object relighting neural network to generate digital images illuminated from a target lighting direction |
WO2020036782A2 (en) * | 2018-08-10 | 2020-02-20 | University Of Connecticut | Methods and systems for object recognition in low illumination conditions |
US10936909B2 (en) * | 2018-11-12 | 2021-03-02 | Adobe Inc. | Learning to estimate high-dynamic range outdoor lighting parameters |
CN111541840B (en) * | 2019-02-06 | 2022-03-22 | 佳能株式会社 | Information processing apparatus, method and storage medium for determining illumination effect candidates |
JP7292905B2 (en) * | 2019-03-06 | 2023-06-19 | キヤノン株式会社 | Image processing device, image processing method, and imaging device |
US11538216B2 (en) * | 2019-09-03 | 2022-12-27 | Adobe Inc. | Dynamically estimating light-source-specific parameters for digital images using a neural network |
CN110765923A (en) * | 2019-10-18 | 2020-02-07 | 腾讯科技（深圳）有限公司 | Face living body detection method, device, equipment and storage medium |
KR20210126934A (en) * | 2020-04-13 | 2021-10-21 | 삼성전자주식회사 | Method and apparatus of outputting light source information |
KR102441171B1 (en) * | 2020-05-26 | 2022-09-08 | 한국전자통신연구원 | Apparatus and Method for Monitoring User based on Multi-View Face Image |
US11330196B2 (en) * | 2020-10-12 | 2022-05-10 | Microsoft Technology Licensing, Llc | Estimating illumination in an environment based on an image of a reference object |
CN112529097B (en) * | 2020-12-23 | 2024-03-26 | 北京百度网讯科技有限公司 | Sample image generation method and device and electronic equipment |
US20220207819A1 (en) * | 2020-12-31 | 2022-06-30 | Snap Inc. | Light estimation using neural networks |
-
2020
- 2020-09-21 EP EP20786421.6A patent/EP3939011A1/en active Pending
- 2020-09-21 KR KR1020227025292A patent/KR20220117324A/en not_active Application Discontinuation
- 2020-09-21 WO PCT/US2020/070558 patent/WO2021236175A1/en unknown
- 2020-09-21 US US17/309,171 patent/US20220027659A1/en active Pending
- 2020-09-21 JP JP2022544338A patent/JP2023521270A/en active Pending
- 2020-09-21 CN CN202080089261.9A patent/CN114846521A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20220117324A (en) | 2022-08-23 |
US20220027659A1 (en) | 2022-01-27 |
EP3939011A1 (en) | 2022-01-19 |
WO2021236175A1 (en) | 2021-11-25 |
JP2023521270A (en) | 2023-05-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
LeGendre et al. | Deeplight: Learning illumination for unconstrained mobile mixed reality | |
Pandey et al. | Total relighting: learning to relight portraits for background replacement. | |
Deschaintre et al. | Single-image svbrdf capture with a rendering-aware deep network | |
Song et al. | Neural illumination: Lighting prediction for indoor environments | |
Hold-Geoffroy et al. | Deep sky modeling for single image outdoor lighting estimation | |
Garon et al. | Fast spatially-varying indoor lighting estimation | |
Nestmeyer et al. | Learning physics-guided face relighting under directional light | |
Gardner et al. | Learning to predict indoor illumination from a single image | |
US20220027659A1 (en) | Learning illumination from diverse portraits | |
CN110148204B (en) | Method and system for representing virtual objects in a view of a real environment | |
Mandl et al. | Learning lightprobes for mixed reality illumination | |
CN110910486B (en) | Indoor scene illumination estimation model, method and device, storage medium and rendering method | |
Calian et al. | From faces to outdoor light probes | |
LeGendre et al. | Learning illumination from diverse portraits | |
CN114972617A (en) | Scene illumination and reflection modeling method based on conductive rendering | |
Lalonde | Deep learning for augmented reality | |
EP3485464A1 (en) | Computer system and method for improved gloss representation in digital images | |
US20230368459A1 (en) | Systems and methods for rendering virtual objects using editable light-source parameter estimation | |
Chalmers et al. | Reconstructing reflection maps using a stacked-cnn for mixed reality rendering | |
Zhang et al. | Neilf++: Inter-reflectable light fields for geometry and material estimation | |
Song et al. | Real-time shadow-aware portrait relighting in virtual backgrounds for realistic telepresence | |
Li et al. | Guided selfies using models of portrait aesthetics | |
Boss et al. | Single image brdf parameter estimation with a conditional adversarial network | |
Lu et al. | Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry from Sparse Low Dynamic Range Panoramic Images | |
Li et al. | A SVBRDF modeling pipeline using pixel clustering |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |