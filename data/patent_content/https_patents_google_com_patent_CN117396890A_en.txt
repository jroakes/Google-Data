CN117396890A - Efficient hardware accelerator configuration exploration - Google Patents
Efficient hardware accelerator configuration exploration Download PDFInfo
- Publication number
- CN117396890A CN117396890A CN202280035469.1A CN202280035469A CN117396890A CN 117396890 A CN117396890 A CN 117396890A CN 202280035469 A CN202280035469 A CN 202280035469A CN 117396890 A CN117396890 A CN 117396890A
- Authority
- CN
- China
- Prior art keywords
- hardware configuration
- hardware
- training
- neural network
- accelerator
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012549 training Methods 0.000 claims abstract description 119
- 238000013528 artificial neural network Methods 0.000 claims abstract description 100
- 238000000034 method Methods 0.000 claims abstract description 58
- 238000010801 machine learning Methods 0.000 claims abstract description 37
- 230000008569 process Effects 0.000 claims abstract description 34
- 238000003860 storage Methods 0.000 claims abstract description 10
- 238000013461 design Methods 0.000 claims description 35
- 238000012545 processing Methods 0.000 claims description 34
- 230000015654 memory Effects 0.000 claims description 16
- 230000006870 function Effects 0.000 claims description 14
- 238000005457 optimization Methods 0.000 claims description 14
- 238000005070 sampling Methods 0.000 claims description 6
- 230000000007 visual effect Effects 0.000 claims description 5
- 238000004422 calculation algorithm Methods 0.000 claims description 3
- 241000254158 Lampyridae Species 0.000 claims description 2
- 230000004913 activation Effects 0.000 claims description 2
- 230000001934 delay Effects 0.000 claims description 2
- 238000004590 computer program Methods 0.000 abstract description 14
- 238000004088 simulation Methods 0.000 abstract description 10
- 230000036541 health Effects 0.000 description 8
- 230000009471 action Effects 0.000 description 7
- 239000003795 chemical substances by application Substances 0.000 description 7
- 238000004891 communication Methods 0.000 description 5
- 238000004519 manufacturing process Methods 0.000 description 5
- 230000004044 response Effects 0.000 description 5
- 238000002474 experimental method Methods 0.000 description 3
- 239000012634 fragment Substances 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 239000004065 semiconductor Substances 0.000 description 3
- 108091028043 Nucleic acid sequence Proteins 0.000 description 2
- 238000013459 approach Methods 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 230000008451 emotion Effects 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000026676 system process Effects 0.000 description 2
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 2
- 238000013518 transcription Methods 0.000 description 2
- 230000035897 transcription Effects 0.000 description 2
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000002457 bidirectional effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 230000003750 conditioning effect Effects 0.000 description 1
- 238000012938 design process Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000003745 diagnosis Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000007613 environmental effect Effects 0.000 description 1
- 238000011156 evaluation Methods 0.000 description 1
- 238000003709 image segmentation Methods 0.000 description 1
- 238000009533 lab test Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000011987 methylation Effects 0.000 description 1
- 238000007069 methylation reaction Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 229910052710 silicon Inorganic materials 0.000 description 1
- 239000010703 silicon Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013526 transfer learning Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 238000009528 vital sign measurement Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/20—Design optimisation, verification or simulation
- G06F30/27—Design optimisation, verification or simulation using machine learning, e.g. artificial intelligence, neural networks, support vector machines [SVM] or training a model
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/30—Circuit design
- G06F30/34—Circuit design for reconfigurable circuits, e.g. field programmable gate arrays [FPGA] or programmable logic devices [PLD]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/12—Computing arrangements based on biological models using genetic models
- G06N3/126—Evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/01—Dynamic search techniques; Heuristics; Dynamic trees; Branch-and-bound
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training a proxy neural network configured to determine a predicted performance metric of a hardware accelerator having a target hardware configuration on a target application. In addition to, or instead of, hardware simulation, training instances of proxy neural networks may be used during the search process to determine the hardware configuration of an application-specific hardware accelerator (i.e., a hardware accelerator on which one or more neural networks may be deployed to perform one or more target machine learning tasks).
Description
Cross Reference to Related Applications
The present application claims priority from U.S. provisional application No. 63/216,925 filed on 6/30 of 2021. The disclosure of this prior application is considered to be part of the disclosure of the present application and is incorporated by reference into the disclosure of the present application.
Background
The present description relates to determining a configuration of a hardware accelerator.
Hardware accelerators are computing devices, such as graphics processing units ("GPUs"), field programmable gate arrays ("FGPAs"), and application specific integrated circuits ("ASICs"), including tensor processing units ("TPUs"), having specialized hardware configured to perform specialized computations.
Disclosure of Invention
The specification describes systems implemented as computer programs on one or more computers in one or more locations that determine a hardware configuration of an application-specific hardware accelerator on which one or more machine learning models can be deployed to perform one or more machine learning tasks.
A hardware accelerator is a computing device that includes specialized hardware for performing certain types of operations, such as matrix multiplication, that is more efficient than non-specialized or "general-purpose" computing devices. Different hardware accelerators may have different hardware characteristics, such as different computations, memory, bandwidth, etc.
A key feature of an application specific hardware accelerator is that it is primarily oriented towards the needs of one or more specific applications. When compared to the application of general purpose computing devices in these applications, it has the advantages of smaller size, lower power consumption, higher reliability, higher performance, and reduced cost. For example, an application-specific hardware accelerator for a machine learning application may have hardware characteristics that are not only oriented to the specifics of the machine learning task (e.g., data modality, data size, etc.) but also to the computational requirements (e.g., number of computational operations (e.g., convolution, deep convolution, feed forward, etc.), model parameter size, instruction size, etc.) of the various types of machine learning models deployed on the application-specific hardware accelerator to perform these tasks.
The application specific hardware accelerator may be part of a target computing device that includes one or more hardware accelerators. As one example, the target computing device that includes one or more hardware accelerators may be a single specific edge device, such as a mobile phone, a smart speaker, or another embedded computing device, or other edge device. As a specific example, the edge device may be a mobile phone or other device having a particular type of hardware accelerator or other computer chip on which the neural network is to be deployed.
As another example, the target computing device including one or more hardware accelerators may be a set of multiple hardware accelerator devices, such as an ASIC, FPGA, or Tensor Processing Unit (TPU) on a real world agent (agent) such as a vehicle, such as an automated driving car or robot.
As yet another example, the target computing device that includes one or more hardware accelerators may be a set of hardware accelerators in a data center.
The subject matter described in this specification can be implemented in specific embodiments to realize one or more of the following advantages.
Hardware accelerators are specialized hardware configured to perform specialized calculations and are generally more computationally efficient than their general-purpose counterparts, but are also generally associated with higher operating costs because energy is required to power and maintain the accelerator. Using neural networks deployed on hardware accelerators to efficiently perform machine learning tasks, such as visual tasks, text tasks, audio tasks, or other tasks that require near real-time responses to be provided to a user, requires specially designed hardware accelerator configurations, i.e., configurations that have been customized for the machine learning task, the neural network, or both.
Existing accelerator configuration design methods typically involve exploring in a large multimodal candidate configuration space to determine corresponding values of hardware parameters and then evaluating the performance of a hardware accelerator whose configuration has been defined by the determined parameter values. For most modern hardware accelerators, such as GPUs or TPUs, this requires a set of parameter values to be selected from a large discrete design space, an integer design space, an ordinal design space, a radix design space, or a combination thereof. Repeatedly performing this search process is computationally intensive and consumes a significant amount of computational resources because of the exhaustive nature of the parameter value sampling steps and the cost of running expensive simulators or other analytical models for hardware performance assessment.
Furthermore, accelerator design space is often characterized by a narrow manifold (narrow manifold) of possible accelerators within a massive infeasible point. While some of these infeasible points may be identified via simple rules, most infeasible points correspond to failures during compilation or hardware simulation. These infeasible points are often not directly formulated as optimization problems.
Furthermore, the optimization objective may exhibit high sensitivity to small changes in some architectural parameters in some areas of the design space, but remain relatively insensitive in other parts, resulting in a complex optimization scenario. This suggests that optimization algorithms based on local parameter updates (e.g., gradient ramp-up, evolution schemes, etc.) may have the challenging task of traversing nearly flat target scenarios, which may result in poor performance.
The present specification describes a data-driven offline optimization framework that utilizes recorded simulation data from a priori design tasks or experiments. The described framework allows configuration data from a recorded dataset to be utilized in this way: the values of configuration data used in determining an optimized hardware accelerator design under various different design constraints are increased. In particular, the described framework may learn a proxy neural network configured to predict performance metrics of a hardware accelerator from a one-time collected offline data set and automatically search for high performance hardware accelerators (e.g., application specific hardware accelerators) by using the proxy neural network to optimize hardware parameter values defining the hardware configuration.
The described framework saves the recurring costs of large-scale hardware simulation scanning because any additional queries to expensive hardware simulators for evaluating the various candidate configurations generated during the design process are no longer required. Instead, only more efficient forward-pass computation of candidate configuration data through the proxy neural network is required at a time in order to determine the predicted performance metrics of the candidate hardware accelerator. Furthermore, the proxy neural network trained using the described framework is more robust than existing approaches such as black box optimization schemes, and can largely avoid impractical optimistic performance metric predictions for any given hardware configuration, even including hardware configurations defined by hardware parameter values that are rarely seen or outside of the distribution. Thus, the described framework may be used to design a hardware accelerator configuration that has comparable performance to a hardware accelerator configuration designed by using the most advanced online approach, albeit with a significantly reduced amount of time, amount of computing resources, or both, as compared to the prior art.
The details of one or more implementations of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example neural network training system and an example hardware configuration determination system.
FIG. 2 is a flow chart of an example process for training a proxy neural network.
Fig. 3 is an illustration of an example training proxy neural network.
FIG. 4 is a flow chart of an example process for determining a final hardware configuration of a hardware accelerator.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
FIG. 1 illustrates an example neural network training system 100 and an example hardware configuration determination system 150. The neural network training system 100 and the hardware configuration determination system 150 are examples of systems implemented as computer programs on one or more computers at one or more locations, in which the systems, components, and techniques described below may be implemented.
The neural network training system 100 is a system that trains the neural network 110 ("proxy neural network") to determine training values of parameters of the proxy neural network 110, referred to herein as proxy network parameters, from initial values of the proxy network parameters.
The proxy neural network 110 is a neural network configured to receive input data specifying a target hardware configuration and process the input data according to proxy network parameters to generate output data specifying predicted performance metrics of a hardware accelerator having the target hardware configuration.
In some embodiments, the input data specifying the target hardware configuration may include a corresponding plurality of hardware parameters that collectively define the computational characteristics of the hardware accelerator having the target hardware configuration, as will be further described below with reference to table 1.
In some implementations, the predictive performance metrics may include one or more of the following: predicted run-time delays of a machine learning model deployed on a hardware accelerator having a target hardware configuration, or predicted power consumption of the hardware accelerator. The run-time delay measures the time required to perform reasoning on a batch of one or more inputs, i.e., process each input in a batch of inputs using the machine learning model, e.g., in milliseconds, when the machine learning model is deployed on a hardware accelerator having a target hardware configuration.
In some implementations, the input data may also include a description of the target application represented as a context vector. That is, in some embodiments, the predictive performance metric is conditioned on the target application. For example, the context vector may include one or more context variables that specify a set of attributes of the target application. The attributes of the target application may include attributes of the machine learning task and attributes of a machine learning model to be deployed on the hardware accelerator to perform the task. In some examples, the context variables may include one-hot (one-hot) encoding variables that identify which of a plurality of machine learning tasks across various domains (e.g., visual domain, text domain, and audio domain) the machine learning model is configured to perform. The context variables may also include integer or floating point variables that define the number of computations (e.g., convolutions, deep convolutions, feedforward, etc.) of the machine learning model, model parameter sizes, instruction sizes, number of computational operations, etc.
The proxy neural network 110 may be implemented with any suitable neural network architecture that enables it to perform its described functions. In various examples, the proxy neural network 110 may be a fully connected neural network, i.e., including one or more fully connected neural network layers, or may alternatively be a recurrent neural network, e.g., including one or more Long Short Term Memory (LSTM) neural network layers.
As a particular example, the proxy neural network 110 may be an attention neural network that includes a plurality of attention layers. Examples of the configuration of the attention neural network are described in more detail in Vaswani et al, "Attention Is All You Need", arXiv:1706.03762, raffel et al, "Exploring the Limits of Transfer Learning with a Unified Text-to-Text transducer", arXiv:1910.10683 and Devlin et al, "BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding", arXiv: 1810.04805.
To efficiently train the proxy neural network 110, the neural network training system 100 maintains a hardware configuration training data set 120 and trains the proxy neural network 110 using the training data set 120 to determine training values for proxy network parameters based on an optimized proxy objective function. For example, training data set 120 may include offline training inputs that have been generated or otherwise determined from recorded simulation data or from a priori hardware design tasks or experiments, and may optionally include random training inputs generated by random sampling from the space of candidate hardware accelerator configurations. The training process is described in more detail below with reference to fig. 2-3.
Specifically, training data set 120 includes a plurality of first training inputs specifying a viable hardware configuration 126. The training data set 120 also includes a plurality of second training inputs specifying infeasible hardware configurations 128. During training, by training the proxy neural network 110 to generate more conservative predictive performance metrics (i.e., underestimated performance metrics) for these second training inputs specifying infeasible hardware configurations 128, the neural network training system 100 may increase the robustness of performance metric predictions to be generated by the proxy neural network 110 relative to unusual hardware configurations (i.e., hardware configurations defined by rarely seen or out-of-distribution hardware parameter values).
The feasibility of the hardware configuration may be determined by evaluating against a set of feasibility rules or criteria, and may additionally or alternatively be determined by compiling or hardware simulation. The feasibility of the hardware configuration may depend on the target software, the underlying hardware, or both. For example, a hardware configuration may not be viable because it specifies hardware that is not configurable on silicon. As another example, a hardware configuration may not be feasible because it specifies hardware on which target software (the source code of the neural network) cannot be compiled, for example, due to an insufficient number of memory locations of the hardware accelerator.
After training, the neural network training system 100 may then output data specifying a training instance of the proxy neural network 110 to the hardware configuration determination system 150, for example, via a wired or wireless network. For example, the data may include architecture data specifying an architecture of the proxy neural network, and parameter data specifying training values for proxy network parameters.
The hardware configuration determination system 150 is a system that deploys a training instance of the proxy neural network 110 and uses the training instance of the proxy neural network 110 to determine a hardware configuration (or architecture) of a dedicated hardware accelerator, i.e., a hardware accelerator on which one or more machine learning models (including one or more neural networks) can be deployed to perform one or more target machine learning tasks.
Depending on the task(s), the neural network to be deployed on the hardware accelerator may be configured to receive any kind of digital data input and generate any kind of score, classification, or regression output based on the input.
In some cases, the neural network is configured to perform an image processing task, i.e., a network output that receives an input image and processes the input image to generate the input image. In this specification, processing an input image refers to processing intensity values of image pixels using a neural network. For example, a task may be image classification, and the output generated by the neural network for a given image may be a score for each of a set of object categories, each score representing an estimated likelihood that the image contains images of objects belonging to that category. As another example, the task may be image embedding generation and the output generated by the neural network may be digital embedding of the input image. As another example, a task may be object detection and an output generated by a neural network may identify a location in an input image depicting a particular type of object. As yet another example, the task may be image segmentation and the output generated by the neural network may define for each pixel of the input image which of a plurality of categories the pixel belongs to.
As another example, if the input to the neural network is an internet resource (e.g., a web page), a document or document portion, or a feature extracted from an internet resource, document or document portion, the task may be to categorize the resource or document, i.e., the output generated by the neural network for a given internet resource, document or document portion may be a score for each of a set of topics, each score representing an estimated likelihood that the internet resource, document or document portion is related to a topic.
As another example, if the input to the neural network is a characteristic of the impression context of a particular advertisement, the output generated by the neural network may be a score representing an estimated likelihood that the particular advertisement will be clicked.
As another example, if the input to the neural network is a characteristic of a personalized recommendation for the user, e.g., a characteristic that characterizes the context of the recommendation, e.g., a characteristic that characterizes an action previously taken by the user, the output generated by the neural network may be a score for each of a set of content items, each score representing an estimated likelihood that the user will respond favorably to the recommended content item.
As another example, if the input to the neural network is a sequence of text in one language, the output generated by the neural network may be a score for each of a set of text segments in another language, each score representing an estimated likelihood that the text segment in the other language is a correct translation of the input text into the other language.
As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a score for each of a set of text segments, each score representing an estimated likelihood that the text segment is a correct transcription of the utterance.
As another example, the task may be an audio processing task. For example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a segment of text that is a predicted correct transcription of the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may indicate whether a particular word or phrase ("hotword") was spoken in the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may identify the natural language in which the utterance was spoken.
As another example, a task may be a natural language processing or understanding task, where the input is a text sequence in natural language and the output is a natural language processing or understanding output. One example of such a task is an implication (entailment) task in which an input includes a plurality of natural language sentences, and implications between instruction sentences are output. Another example is a paraphrase task, where the input is a natural language sequence and the output identifies another natural language sequence that has a similar meaning to the input sequence. Another example is a text similarity task, where the input is a plurality of natural language sequences and the output indicates how similar, e.g., semantically similar, the input sequences are. Another example is an emotion task, where the input is a natural language sequence and the output characterizes the emotion of the input sequence. Another example is a sentence completion task, where the input is a natural language sequence and the output identifies another natural language sequence as the completion of the input sequence. Another example is a summarization task, where the input is an input natural language text sequence and the output is a summary natural language sequence that is shorter than the input sequence, but the input sequence is summarized, i.e., representing the most important or relevant information within the input sequence. In some cases, the summarization task is a summarization task in which the output sequence is a proper subset of the input sequence, i.e., consists of text from the input sequence. In some other cases, the summary task is an abstract summary task, where the output is a new sequence, which may contain different text than the input sequence. Another example is a grammatical task, where the input is an input natural language text sequence and the output characterizes the grammatical nature of the input sequence, i.e. the grammatical correctness of the input sequence.
As another example, the task may be a text-to-speech task, where the input is text in natural language or text features in natural language, and the network output is a spectrogram, waveform, or other data defining audio of text spoken in natural language.
As another example, the task may be a health prediction task, wherein the input is a sequence derived from electronic health record data of the patient and the output is a prediction related to future health of the patient. In this example, the electronic health record data of the patient may include a plurality of features representing health events in the electronic health record of the patient, each of the plurality of features belonging to a vocabulary of possible features. One example type of possible feature is a digital feature, which includes laboratory test results or patient vital sign measurements, i.e. measurements of vital signs captured by a medical device, both of which can take many possible different values. Another example type of possible feature is a discrete feature that includes a binary feature, such as whether a patient is admitted to receive treatment within a given period of time, or a category feature, such as program code assigned to a particular program or event. For a health prediction task, the output may include a predicted treatment that should be prescribed for the patient, a likelihood that the patient will experience an adverse health event, or a predicted diagnosis of the patient.
As another example, the task may be a text generation task, where the input is a text sequence and the output is another text sequence, e.g., completion of the input text sequence, a response to a question posed in the input sequence, or a text sequence on a topic specified by the first text sequence. As another example, the input to the text generation task may be an input other than text, such as an image, and the output sequence may be text describing the input. As yet another example, the input to the text generation task may include both input and text from different modalities, and the output sequence may be text responsive to the input. For example, the task may be a visual question answering task, and the input may include one or more images and text questions about the one or more images, and the output sequence may be an answer to the text questions.
As another example, the task may be an image generation task, where the input is a conditioning (adjustment) input and the output is a sequence of intensity value inputs for the image pixels. The adjustment input may include one or more of, for example, a class label identifying a desired class of objects that should be depicted in the image, a text sequence describing desired content of the image, or another image, for example, an image of an object that should be included in a new image or a lower resolution image that should be augmented (upscaled) to a higher resolution to generate the new image.
As another example, the task may be an agent control task, where the input is an observation characterizing an environmental state and the output defines an action to be performed by the agent in response to the observation. The agents may be, for example, real world or simulated robots, control systems for industrial facilities, or control systems controlling agents of different types. The observations may include sensor data captured by sensors measuring the environment (e.g., camera sensors, lidar sensors, temperature sensors, humidity sensors, etc.).
As another example, a task may be a genomics task, where inputs are sequences representing fragments of DNA sequences or other molecular sequences, and outputs are embeddings of fragments for use in downstream tasks, such as by utilizing unsupervised learning techniques on a dataset of DNA sequence fragments, or outputs of downstream tasks. Examples of downstream tasks include promoter (promoter) site prediction, methylation analysis, prediction of the functional effects of non-coding variants, and the like.
In some cases, the machine learning task is a combination of multiple individual machine learning tasks, i.e., the neural network is configured to perform multiple different individual machine learning tasks, such as two or more machine learning tasks described above. For example, the neural network may be configured to perform a plurality of separate natural language understanding tasks. Alternatively, the network input may include an identifier of a separate natural language understanding task to be performed on the network input. As another example, the neural network may be configured to perform a plurality of separate image processing or computer vision tasks, i.e., to generate the output of a plurality of different separate image processing tasks in parallel by processing a single input image.
The hardware configuration determination system 150 obtains data describing the accelerator configuration search space 160 and walks through the accelerator configuration search space 160 to determine a final hardware configuration of the hardware accelerator, such as the final hardware configuration 182a, on which one or more machine learning models may be deployed to perform one or more target machine learning tasks.
In some implementations, the accelerator configuration search space 160, which generally defines candidate hardware accelerator configuration spaces, may be parameterized by a set of adjustable hardware parameters, each associated with one or more values (e.g., discrete values). Table 1 below shows an example accelerator configuration search space 160 in which "PE" refers to a processing element capable of performing matrix multiplication in a Single Instruction Multiple Data (SIMD) paradigm, e.g., "PEs-X" refers to the number of processing elements along the horizontal dimension of a hardware accelerator.
TABLE 1
In the example of table 1, the plurality of hardware parameters included in the input data to the proxy neural network 110 that collectively define the computational characteristics of the hardware accelerator having the target hardware configuration may include two or more of the following: the number of Processing Elements (PEs) along one dimension, the size of the PE memory, the size of the core memory, the size of the instruction memory, the size of the activation memory, the number of cores, the number of compute channels, the size of the parameter memory, and the DRAM bandwidth.
Specifically, the hardware configuration determination system 150 may determine the performance of the machine learning task(s) that support approximately specified performance metrics, e.g., in terms of run-time delay or power consumption, while meeting hardware design constraints, e.g., the final hardware configuration 182a of the hardware accelerator in terms of area budget. For example, hardware design constraints may be specified by a user of the system or automatically determined by the system based on the application.
To determine the final hardware configuration 182a, the system 150 utilizes one or more hardware design policies 172a-k and the proxy neural network 110. Example hardware design strategies including random strategies, regularized evolution search strategies, model-based optimization strategies, bayesian optimization strategies, and population-based black box optimization strategies are described in PCT patent application No. PCT/US2021/054359, which is incorporated herein by reference.
By utilizing the hardware design policies 172a-k, the system 150 repeatedly, i.e., each of a plurality of iterations of the search process, generates a different candidate hardware configuration 162 from the accelerator configuration search space 160, and for each candidate hardware configuration 162, determines the predicted performance metrics 164 for a hardware accelerator having the candidate hardware configuration 162 on the target application, e.g., using the proxy neural network 110, rather than relying solely on expensive hardware simulations.
Unlike expensive hardware performance simulators or other analytical models that typically take up to an hour or more, only the performance metrics of a single hardware accelerator with candidate architecture configurations are evaluated, and the proxy neural network 110 is faster and more resource efficient when used to determine the performance metrics. By incorporating the proxy neural network 110 into the search process, the system 150 reduces the amount of computing resources consumed by hardware simulation, as the need for repeated expensive queries to the hardware performance simulator is eliminated. Thus, the system 150 may perform the search process with reduced latency and reduced computing resource consumption while still being able to determine a final hardware configuration that meets the desired performance metrics and hardware design constraints.
After the search process has been completed, for example, once a predetermined number of search iterations have been performed or a certain amount of time has elapsed, the hardware configuration determination system 150 may select the candidate hardware configuration(s) having the best performance metric, best meeting the various hardware design constraints, or both, as the final hardware configuration 182a of the hardware accelerator. Alternatively or additionally, the system 100 may generate new candidate hardware configurations by using the updated hardware design policies and use the new configurations as the final hardware configuration for the hardware accelerator.
The hardware architecture search system 100 may then generate as output hardware configuration data specifying the configuration of the hardware accelerator, such as data specifying the layout of processing elements on the hardware accelerator, the number of compute channels, the size of the various memories provided by the hardware accelerator, and the DRAM bandwidth.
For example, the hardware configuration determination system 150 may output hardware configuration data to a user that provides hardware design constraints. As another example, the hardware configuration determination system 150 may output hardware configuration data to a semiconductor manufacturing facility housing semiconductor manufacturing equipment that may be used to manufacture a hardware accelerator having a final hardware configuration, for example, through a wired or wireless network.
In some implementations, the hardware configuration determination system 150 may be included as part of a software tool (e.g., an Electronic Design Automation (EDA) tool) for designing and/or analyzing an integrated circuit and then the hardware configuration data may be provided to another component of the tool for further refinement or evaluation prior to manufacturing the hardware accelerator.
FIG. 2 is a flow chart of an example process 200 for training a proxy neural network. For convenience, process 200 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed training system, such as training system 100 of FIG. 1, may perform process 200.
The system may repeatedly perform iterations of process 200 to repeatedly update the network parameters until termination criteria have been met, e.g., until a threshold number of iterations of process 200 have been performed, until a threshold amount of wall clock time has elapsed, or until the value of the proxy network parameter has converged.
The system maintains a hardware configuration training dataset. The hardware configuration training data set includes a plurality of first hardware configuration training inputs, each first hardware configuration training input specifying a respective predetermined feasible hardware configuration, and for each predetermined feasible hardware configuration, a respective target performance metric. The hardware configuration training data set further includes a plurality of second hardware configuration training inputs, each second hardware configuration training input specifying a respective predetermined infeasible hardware configuration.
The hardware configuration training dataset may generally include offline training inputs that have been generated or otherwise determined from recorded simulation data or from a priori hardware design tasks or experiments, and may additionally include random training inputs generated by random sampling from the space of the candidate hardware accelerator configuration space in some embodiments.
In these embodiments, the system may generate random training inputs by receiving data defining an accelerator configuration search space and generating different hardware configurations from the accelerator configuration search space based on random sampling. That is, the random training input may include a plurality of hardware parameters, each having a value randomly selected from a corresponding set of possible values. For each random training input, the system may determine, through hardware simulation, whether a hardware configuration defined by a plurality of hardware parameters included therein is viable, and if so, a corresponding target performance metric for a hardware accelerator having the hardware configuration.
In some embodiments, the hardware configuration training data set is fixed during the training process, while in other embodiments, the training data set expands as iterations of the training process are performed, e.g., a third hardware configuration training input, each specifying a new hardware configuration generated as part of the training process, may be added to the training data set, as described below.
The system selects a set of training inputs from the hardware configuration training dataset that includes (i) one or more first hardware configuration training inputs and (ii) one or more second hardware configuration training inputs (step 202). The system will typically obtain different hardware configuration training inputs at different iterations, for example, by sampling a fixed number of training inputs from the maintained training dataset at each iteration.
Each hardware configuration training input includes a respective plurality of hardware parameters defining a respective predetermined hardware configuration specified by the hardware configuration training input, and optionally, a context vector specifying a set of attributes of a corresponding target application of the hardware accelerator. For each first training input, a respective target performance metric identifies a base true (ground) performance metric for the first training input that should be generated by the proxy neural network by processing the first training input.
The system processes each first hardware configuration training input using the proxy neural network based on the current values of the plurality of proxy network parameters to determine a respective predicted performance metric for each predetermined viable hardware configuration specified in the one or more first hardware configuration training inputs (step 204).
In some implementations, the predictive performance metric may be a run-time delay of a machine learning model deployed on the hardware accelerator on the target application, e.g., to perform a target machine learning task. The runtime delay may be the delay of the (trained) machine learning model performing the target machine learning task, i.e. the delay for processing the new input of the target task by using the machine learning model deployed on the hardware accelerator after the hardware configuration of the hardware accelerator has been determined. In some implementations, the predicted performance metric may be the power (or energy) consumption of the hardware accelerator on the target application, i.e., the power (or energy) consumption when supporting execution of the machine learning model on the target task.
The system processes each second hardware configuration training input using the proxy neural network based on the current values of the plurality of proxy network parameters to determine a respective predicted performance metric for each predetermined infeasible hardware configuration specified in the one or more second hardware configuration training inputs (step 206).
Optionally, in some embodiments, during the training process, the system may repeatedly generate unseen hardware configurations that were not previously used (or rarely used) in training the proxy neural network by traversing the space of candidate architectures in order to generate new training inputs that specify the unseen hardware configurations.
That is, the system optionally generates one or more third hardware configuration training inputs, each specifying a new hardware configuration (step 208). In particular, the system may apply one or more hardware design policies to the accelerator configuration search space to generate a new hardware configuration that, when applied to a hardware accelerator, results in improved performance metrics for the hardware accelerator.
In addition to or instead of the example hardware design policies described above, the system may identify each new hardware configuration that maximizes the predicted performance metrics of the new hardware configuration determined by using the proxy neural network, for example, using a generative optimization strategy or an evolutionary optimization strategy, including an optimization strategy that runs a firefly optimization algorithm. Additionally or alternatively, the system may do this based on proactively modifying or altering existing hardware configurations as specified in the offline training dataset. In some such examples, to allow a hardware design strategy to gradually generate new hardware configurations with improved performance metrics during training, the system may provide predicted performance metrics of the already generated hardware configurations determined by using the proxy neural network as input to the hardware design strategy.
In these embodiments, the system optionally processes each third hardware configuration training input using the proxy neural network in accordance with the current values of the plurality of proxy network parameters to determine a respective predicted performance metric for each new hardware configuration specified in the one or more third hardware configuration training inputs (step 210).
The system determines a gradient of the proxy objective function with respect to the plurality of proxy network parameters (step 212) and determines an update to the current values of the plurality of proxy network parameters based on the gradient. For example, the system may determine the gradient by back-propagation and may determine the update by applying an update rule to the gradient, such as a random gradient descent update rule, adam optimizer update rule, or rmssprop update rule.
Fig. 3 is an illustration of an example training proxy neural network.
As illustrated, the proxy objective function includes a first term 310 that is specific to each predetermined viable hardware configuration x specified in one or more first hardware configuration training inputs i Metrology target performance metrics y i Predictive performance metricsDifferences between them. The first term metric agent neural network predicts an accuracy of a performance metric of a viable hardware configuration relative to a corresponding target performance metric included in the training dataset. For example, the first term 310 may be the mean square error between the target performance metric and the predicted performance metric, e.g., +. >Where the expected value is taken from a viable sample. It should be understood that the first item 310 may alternatively have other forms.
The proxy objective function optionally includes a second term 320 that specifies, for each predetermined infeasible hardware configuration x specified in the one or more second hardware configuration training inputs j Measuring a value of a predicted performance metric of a predetermined infeasible hardware configurationThis optional additional term depends on the predicted performance metrics generated by using the proxy neural network for infeasible hardware configurations. For example, the second item 320 may comprise an expected value of the predicted performance on a hardware configuration that is not feasible, e.g., +.>It should be understood that the first item 310 may alternatively have other forms.
The proxy objective function optionally includes a third term 330 for each new hardware configuration specified in the one or more third hardware configuration training inputsMetric value of predictive Performance metric of New hardware configuration +.>This further optional additional term depends on the predicted performance metrics generated by using the proxy neural network for the unseen hardware configuration. For example, the third item 330 may comprise the expected value of the predicted performance on the negative sample, e.g., ->It should be understood that the first item 310 may alternatively have other forms.
Specifically, the proxy objective function minimizes the first term 310, maximizes the second term 320 and the third term 330, i.e., in the case where either or both terms are included therein. Maximizing the second and third terms encourages the proxy neural network to generate worse predicted performance metrics (i.e., underestimated performance metrics) for infeasible or unseen hardware configurations. That is, the additional term minimizes the metric when a higher metric indicates better hardware performance, and maximizes the metric when a lower metric (e.g., lower latency) indicates better hardware performance.
In some embodiments, only the parameters of the proxy neural network are updated according to the gradient of the proxy objective function, rather than the hardware design strategy (for generating new hardware configurations during training). This may be achieved by applying a "stop gradient" operator to the hardware design strategy when performing the training operation, i.e., by stopping the gradient reflow into the hardware design strategy.
FIG. 4 is a flow chart of an example process 400 for determining a final hardware configuration of a hardware accelerator. For convenience, process 400 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed system, such as hardware configuration determination system 150 of FIG. 1, may perform process 400.
The system generates a plurality of different candidate hardware configurations from the accelerator configuration search space (step 402) and determines a corresponding predicted performance metric for each of the different candidate hardware configurations using the proxy neural network that has been trained by performing process 200 (step 404).
In particular, the system may use any of the hardware design strategies described above to traverse the accelerator configuration search space to repeatedly, i.e., in each of multiple iterations of the search process, determine each different candidate hardware configuration that minimizes the predicted performance metrics of the different candidate hardware configurations determined by using training instances of the proxy neural network on the target application subject to one or more hardware design constraints. For example, the one or more hardware design constraints may include an area constraint of the hardware accelerator that defines a maximum allowable area of the hardware accelerator, e.g., measured in square millimeters.
After the search process has been completed, for example, once a predetermined number of search iterations have been performed or a certain amount of time has elapsed, the system selects the candidate hardware configuration with the best predicted performance metric as the hardware configuration of the application-specific hardware accelerator from among the different candidate hardware configurations and based on the corresponding predicted performance metrics (step 406).
The term "configured" is used in this specification in connection with systems and computer program components. For a system of one or more computers configured to perform a particular operation or action, it is meant that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. For one or more computer programs configured to perform particular operations or actions, it is meant that the one or more programs comprise instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operations or actions.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program can also be called or described as a program, software application, app, module, software module, script, or code, which can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" is used broadly to refer to any collection of data: the data need not be structured in any particular way, or structured at all, and it may be stored on a storage device in one or more locations. Thus, for example, an index database may include multiple sets of data, each of which may be organized and accessed differently.
Similarly, in this specification, the term "engine" is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines may be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or combination of, special purpose logic circuitry (e.g., an FPGA or ASIC) and one or more programmed computers.
A computer adapted to execute a computer program may be based on a general purpose or a special purpose microprocessor or both, or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for carrying out or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such a device. Furthermore, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, such as a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on a user device in response to a request received from the web browser. Further, the computer may interact with the user by sending a text message or other form of message to a personal device (e.g., a smart phone running a messaging application), and then receiving a response message from the user.
The data processing means for implementing the machine learning model may also comprise, for example, dedicated hardware accelerator units for handling general and computationally intensive parts of machine learning training or production, i.e. reasoning, workload.
The machine learning model may be implemented and deployed using a machine learning framework, such as a TensorFlow framework, microsoft cognitive toolkit framework, apache Single framework, or Apache MXNet framework.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), such as the internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data, such as HTML pages, to the user device, for example for the purpose of displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, such as the results of a user interaction, may be received at the server from the device.
While this specification contains many specifics of embodiment, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, and in the claims, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (19)
1. A method for training a neural network having a plurality of network parameters and for determining a predicted performance metric for a hardware accelerator having a target hardware configuration on a target application, the method comprising:
maintaining a hardware configuration training dataset comprising (i) a plurality of first hardware configuration training inputs, each first hardware configuration training input specifying a respective predetermined feasible hardware configuration, (ii) a respective target performance metric for each respective predetermined feasible hardware configuration, and (iii) a plurality of second hardware configuration training inputs, each second hardware configuration training input specifying a respective predetermined infeasible hardware configuration; and
repeatedly performing training operations, including:
selecting a set of training inputs from the hardware configuration training dataset, the set of training inputs comprising (i) one or more first hardware configuration training inputs and (ii) one or more second hardware configuration training inputs;
processing each first hardware configuration training input using the neural network in accordance with the current values of the plurality of network parameters to determine a respective predicted performance metric for each predetermined viable hardware configuration specified in the one or more first hardware configuration training inputs;
Processing each second hardware configuration training input using the neural network in accordance with the current values of the plurality of network parameters to determine a respective predicted performance metric for each predetermined infeasible hardware configuration specified in the one or more second hardware configuration training inputs; and
determining a gradient of a proxy objective function with respect to a plurality of network parameters, the proxy objective function comprising (i) a first term that measures a difference between a target performance metric and a predicted performance metric for each predetermined feasible hardware configuration specified in the one or more first hardware configuration training inputs, and (ii) a second term that measures a value of the predicted performance metric for the predetermined infeasible hardware configuration for each predetermined infeasible hardware configuration specified in the one or more second hardware configuration training inputs.
2. The method of claim 1, wherein the training operation further comprises:
generating one or more third hardware configuration training inputs, each third hardware configuration training input specifying a new hardware configuration;
processing each third hardware configuration training input using the neural network in accordance with the current values of the plurality of network parameters to determine a predicted performance metric for the new hardware configuration; and
Wherein the proxy objective function includes, for each new hardware configuration specified in the one or more third hardware configuration training inputs, a third term that measures a value of a predicted performance metric for the new hardware configuration.
3. The method of any of claims 1-2, wherein the proxy objective function minimizes the first term and maximizes the second and third terms.
4. The method of claim 2, wherein generating the third hardware configuration training input specifying the new hardware configuration comprises:
an optimizer is applied to the accelerator configuration search space to identify each new hardware configuration that maximizes a predicted performance metric for the new hardware configuration determined by using the neural network.
5. The method of claim 4, wherein an optimizer is configured to receive a predicted performance metric of a new hardware configuration determined by using the neural network.
6. The method of claim 5, further comprising applying a stop gradient operator to the optimizer when performing the training operation.
7. The method of any of claims 4-6, wherein the optimizer comprises a generative optimizer or evolutionary optimizer, including an optimizer running a firefly optimization algorithm.
8. The method of any of claims 1-7, wherein the neural network is further configured to process a context vector specifying a set of attributes for a corresponding target application when processing the first hardware configuration training input, the second hardware configuration training input, or the third hardware configuration training input, and wherein the context vector specifies a different set of attributes for each different target application.
9. The method of any of claims 1-8, wherein each hardware configuration training input in a hardware configuration data set comprises a respective plurality of hardware parameters defining a respective predetermined hardware configuration specified by the hardware configuration training input.
10. The method of claim 9, wherein the plurality of hardware parameters comprises two or more of: the number of Processing Elements (PEs) along one dimension, the size of the PE memory, the size of the core memory, the size of the instruction memory, the size of the activation memory, the number of cores, the number of compute channels, the size of the parameter memory, or the DRAM bandwidth.
11. The method of any of claims 1-10, wherein the predicted performance metrics of the hardware configuration of the hardware accelerator having the target hardware configuration include one or more of:
A machine learning model deployed on the hardware accelerator delays operation on the target application, or
The power consumption of the hardware accelerator.
12. The method of claim 11, wherein the target application comprises a visual machine learning application.
13. The method of any of claims 1-12, further comprising generating the hardware configuration training dataset, the generating comprising:
receiving data defining an accelerator configuration search space;
generating different hardware configurations from the accelerator configuration search space based on random sampling; and
a hardware simulator is used to determine (i) whether each different hardware configuration is viable or not viable and (ii) the corresponding target performance metric for any of the different hardware configurations determined to be viable.
14. A method of determining a hardware configuration for an application specific hardware accelerator, the method comprising:
generating a plurality of different candidate hardware configurations;
determining a respective predicted performance metric for each different candidate hardware configuration using a neural network that has been trained by performing the respective operations of any preceding claim; and
from the different candidate hardware configurations and based on the respective predicted performance metrics, the candidate hardware configuration with the best predicted performance metric is selected as the hardware configuration of the application-specific hardware accelerator.
15. The method of claim 14, wherein generating the plurality of different candidate hardware configurations comprises traversing the accelerator configuration search space using the optimizer of any one of claims 4-7 to determine each different candidate hardware configuration that minimizes a predicted performance metric for the different candidate hardware configuration determined by using the neural network on the target application subject to one or more hardware design constraints.
16. The method of claim 15, wherein the one or more hardware design constraints comprise an area constraint of the hardware accelerator.
17. An application specific hardware accelerator whose hardware configuration has predicted performance metrics determined by performing a process comprising the respective operations of any of claims 14-16.
18. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the operations of the respective method of any one of claims 1-16.
19. One or more computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform the operations of the respective method of any one of claims 1-16.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163216925P | 2021-06-30 | 2021-06-30 | |
US63/216,925 | 2021-06-30 | ||
PCT/US2022/035740 WO2023278712A1 (en) | 2021-06-30 | 2022-06-30 | Efficient hardware accelerator configuration exploration |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117396890A true CN117396890A (en) | 2024-01-12 |
Family
ID=82742963
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280035469.1A Pending CN117396890A (en) | 2021-06-30 | 2022-06-30 | Efficient hardware accelerator configuration exploration |
Country Status (3)
Country | Link |
---|---|
EP (1) | EP4315180A1 (en) |
CN (1) | CN117396890A (en) |
WO (1) | WO2023278712A1 (en) |
-
2022
- 2022-06-30 WO PCT/US2022/035740 patent/WO2023278712A1/en active Application Filing
- 2022-06-30 CN CN202280035469.1A patent/CN117396890A/en active Pending
- 2022-06-30 EP EP22748150.4A patent/EP4315180A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
EP4315180A1 (en) | 2024-02-07 |
WO2023278712A1 (en) | 2023-01-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11568207B2 (en) | Learning observation representations by predicting the future in latent space | |
US20240062062A1 (en) | Device placement optimization with reinforcement learning | |
CN110651280B (en) | Projection neural network | |
CN111279362B (en) | Capsule neural network | |
US20230049747A1 (en) | Training machine learning models using teacher annealing | |
CN110476173B (en) | Hierarchical device placement with reinforcement learning | |
CN111652378B (en) | Learning to select vocabulary for category features | |
CN110663049A (en) | Neural network optimizer search | |
WO2021195095A1 (en) | Neural architecture search with weight sharing | |
CN111666416A (en) | Method and apparatus for generating semantic matching model | |
US20220335209A1 (en) | Systems, apparatus, articles of manufacture, and methods to generate digitized handwriting with user style adaptations | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
WO2023158881A1 (en) | Computationally efficient distillation using generative neural networks | |
US20240005129A1 (en) | Neural architecture and hardware accelerator search | |
US20220019856A1 (en) | Predicting neural network performance using neural network gaussian process | |
US20230063686A1 (en) | Fine-grained stochastic neural architecture search | |
WO2023059811A1 (en) | Constrained device placement using neural networks | |
CN117396890A (en) | Efficient hardware accelerator configuration exploration | |
US20230376664A1 (en) | Efficient hardware accelerator architecture exploration | |
US20230145129A1 (en) | Generating neural network outputs by enriching latent embeddings using self-attention and cross-attention operations | |
EP4302237A1 (en) | Generating neural network outputs by cross attention of query embeddings over a set of latent embeddings | |
CN116109449A (en) | Data processing method and related equipment | |
CN118043818A (en) | Self-attention-based neural network for processing network metrics from multiple modalities |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |